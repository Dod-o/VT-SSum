{
    "id": "bowdmmbb7pcu3kfndbna2qqnzhyypdqi",
    "title": "Expectation-Maximization for Sparse and Non-Negative PCA",
    "info": {
        "author": [
            "Christian David Sigg, Institute of Computational Science, ETH Zurich"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_sigg_ems/",
    "segmentation": [
        [
            "OK, this is another talk about PCA this time including constraint sparsity and non negativity and the idea is to derive an algorithm which is fast enough for many features.",
            "Let's say 10,000 or more."
        ],
        [
            "A sorta with the problem definition.",
            "I guess we're all familiar with PCA.",
            "There are two formulations, either the variance maximization formulation where basically PCA maximizes the variance of the projected data or the reconstruction formulation where the PC subspace is actually L2 optimal.",
            "So minimal Euclidean reconstruction error so.",
            "So this is the optimization problem in the variance maximization formulation, so.",
            "It's a it's a convex maximization problem, so it's convex.",
            "But unfortunately we maximize and we need this constraint here in order to have the problem well post.",
            "And.",
            "Yes, we know we can.",
            "We can successively find further components just by again maximising under the orthogonality constraint and what we do here is consider this problem under two additional constraints, sparsity, so limiting the number of features.",
            "Let's say at most K features.",
            "The second constraint is non negativity, meaning that it's a linear combination where we have just equal signs, so no differences between features considering."
        ],
        [
            "Yeah, why do we want to do this?",
            "The idea is that the constraints allow us to.",
            "They facilitate the tradeoff.",
            "So basically we still want to maximize variance, so that's the criterion of interest, but.",
            "Due to the constraints that say we might want to interpret the loadings of the PC components, and of course if we.",
            "I mean, it's well known that that for practical data, what usually happens is that PCA composition sort of compresses the data in few components, right?",
            "So you have the knee in the eigenvalue spectrum, but usually one of these loadings for one component or still full, so it's kind of hard to say.",
            "Unless the situation is very clear, we have few high loadings and the rest are small, or maybe all of them are equal.",
            "Also, there might be applications where the constraints might make sense, so if due to the underlying physical process, let's say.",
            "Maybe it's.",
            "Sort of.",
            "It's finding non negative influence so non negative combination of influence might be what we are looking for.",
            "Also you could think of examples where actually including a feature has an associated cost and in that setting it might make sense to reduce the variance a little in order to use fewer features.",
            "Now sparse PCA.",
            "I think that the motivation sorting into.",
            "Mid 90s was from an interpretation of the PC loadings, but it has done also been applied to unsupervised gene selection and ranking.",
            "Non negative sparse PCA is.",
            "I'm sorry the green is.",
            "I hope you can read it is a 2006 nips paper by slash Assassin's Shashua on what you see here is is a visualization of on the left side.",
            "We have the full loadings of the first principle components for and here are non negative sparse loadings and I guess it's just illustrates that.",
            "I mean there are sparse so there are few few dimensions included in here and.",
            "It's kind of we see natural parts.",
            "Maybe like eyebrows or or eyes."
        ],
        [
            "OK, how how to attack the problem?",
            "If you write out the quadratic form as an explicit some, we immediately see that if we set a feature to zero, what this means is that we we delete columns and rows in the.",
            "Covariance matrix, so for a given sparsity pattern, actually it's easy for sparse PCA to compute the weights.",
            "So it's really about the pattern.",
            "So combinatorial search is 1 approach.",
            "An exact French in palm with it by McAdam, for which works well for smaller T and also a greedy algorithm.",
            "Forward and backward search, which was then improved too.",
            "D square for step complexity less, use acnl because it's in MP."
        ],
        [
            "See the problem?",
            "We can verify it.",
            "In colonial time, all those are the continuous approximation, so we do an L1 relaxation if we just add the L1 penalty to the original problem.",
            "So here is a visualization we have the bowl.",
            "It's shaped like this because it's positive definite and it's constraint on the unit circle.",
            "Then what happens is that we cut away parts of that circle and So what we have here is we introduce local minimum.",
            "And there are various continuous algorithms based on iterative L1 regression, convex SV STP approximation which has very nice results but is kind of expensive and also at last years I CML there was a DC minimization algorithm with cubed."
        ],
        [
            "Complexity preparation.",
            "OK."
        ],
        [
            "OK, So what do we do?",
            "The algorithm is motivated from probabilistic PCA.",
            "The EM algorithm for probabilistic PCA.",
            "So if you've attended the.",
            "The tutorial on Saturday by Neil Lawrence.",
            "You're already familiar, so this is just.",
            "The very basic, so it's a generative model.",
            "We have the latent latent variable in the PC subspace and say its center Gaussian and then the conditioned observation on that latent variable is a linear motion model.",
            "And we have here the noise.",
            "So basically we sample here and then.",
            "Given the position of Y in the subspace we sampled, the accent is then gives.",
            "These are distribution."
        ],
        [
            "Now I'm.",
            "You also provided an EM algorithm for that will not give the equations here.",
            "What we can do is we can simplify it.",
            "Three things, so I take the non probabilistic limit so Sigma 20, considering L1L equals one subspace.",
            "So just the first component and I normalize just to unity in order to simplify notation and then the EM algorithm is very simple so you see that the eastep amounts to orthogonal projection of the data onto the current estimate of the subspace and then the M step is just minimization of reconstruction error.",
            "Between the original data point and this reconstruction and then we re normalize and iterate.",
            "So here is a.",
            "Here is an illustration from the PR Mail book from Bishop.",
            "So sample data here are the two principal axis.",
            "We choose a random direction.",
            "First project the data orthogonally.",
            "So we get.",
            "The wise and then we optimize the W in order to minimize.",
            "Erotic distance and we iterate this and we see that already after like 2 1/2 iterations.",
            "We basically turn and have identified the first principle."
        ],
        [
            "Points.",
            "Now if we have these settings, so we have split the.",
            "So.",
            "The original problem.",
            "I mean we have approximated of course it's just a local method, but we split it in two parts and we can easily add the constraints to the second.",
            "So we just rewrite that formula here.",
            "I mean, it's just a change of notation.",
            "So we see you have an isotropic QP.",
            "So basically the Hessian is just the scale identity matrix.",
            "H is positive, so it's it's convex, and so therefore we can efficiently apply the L1 bound.",
            "And if we want the non negativity.",
            "So what this means is because the there are no interaction terms.",
            "Is that all?",
            "This means is just we minimize L2 distance."
        ],
        [
            "So the unconstrained optimum so.",
            "So if this here is the optimum and this is the L1 boundary, then that's the point we're looking for.",
            "So to motivate what do we do?",
            "I mean, it's not expensive to solve that, so after transformation into the non negative earth and so we just remember the sign structure, we start at the origin and then we take axis aligned gradient decent steps.",
            "So small step in the direction of the largest element of the negative gradient and we continue until we arrive at that point.",
            "This allows us to XX not specify the boundary B, but actually just directly the sparsity K because due to monitor monotonicity so.",
            "Features enter the solution.",
            "One of the other and.",
            "Yeah, that's nice.",
            "We also.",
            "I mean we don't have to explicitly do this.",
            "Do these steps.",
            "I mean we can directly just compute the point where another feature enters the solution.",
            "So basically all this amounts to is.",
            "Is to sort the elements of W star, which is D log D and then we add up.",
            "Until we arrive at that point, W circle here."
        ],
        [
            "OK, how do we get multiple components here?",
            "We just do iterative deflation, so we compute the first sparse principal component and then we just protect it out.",
            "Projected out of the data and repeat the procedure.",
            "Now it's immediately obvious that if we also include non negativity constraints like orthogonality and non negativity is quite a strong requirement and you can only be orthogonal if a feature and to only one of the loading.",
            "So if feature I is included in PCL you cannot include it anymore and this might be a two strong requirement.",
            "So what we can do is we can just enforce quasi orthogonality, meaning that we just enforce the minimum angle between components which for the non negative setting just amounts to a an additional linear constraint."
        ],
        [
            "OK that."
        ],
        [
            "It's the method now.",
            "Experiments and results.",
            "So we've compared our method to three algorithms.",
            "As I said before, there are more, but basically why did we choose these three?",
            "SPCA Beisu and others in 2004?",
            "Is is an iterative method as ours.",
            "It also has these L1 penalty at some point and.",
            "To reduce the data matrix instead of the covariance matrix, so it's sufficient for the D very much larger than case paths.",
            "PCA is this combinatorial greedy algorithm is just a greedy forward search and SPCA is the only non negative sparse PCA algorithm I am aware of and it has this quasi orthogonality formulation.",
            "Here we have results for two datasets.",
            "So for the larger the case from the CCL faces data set, so about 2400 images 1919 pixels, which is cut.",
            "The prominent set in data set in the NMF literature, and for the much larger than in case we look at any gene expression data set leukemia data.",
            "So 72 expression profiles in about 12,500 genes.",
            "We are.",
            "We, in the formulation we assume that the empirical mean is 0, so that's what we do here standardize to 0 mean.",
            "Also we have here unit variance by dimension, but the results don't really change if we do this."
        ],
        [
            "OK, variance versus cardinality.",
            "On the left side, what you see here is is.",
            "In Gray, the crosses here is our method and SPCA.",
            "Before we recompute the weights for the for the sparsity pattern, and so this renormalization step is really necessary in order to get the performance.",
            "So read here is again SPCA off to renormalization and here for this state actually.",
            "Arson past PCA.",
            "Achieve the same result on the gene expression data.",
            "So this is now all after renormalization, which is not actually necessary for past PCB 'cause this is optimal.",
            "The weights are optimal by construction.",
            "We have also added here like.",
            "Sort of the baseline, so thresholding you just take pick the K largest elements of the first to the loadings of the first principle component.",
            "And as you see, it actually performs quite well in these.",
            "Of course, defense method considered in here.",
            "The simple thresholding.",
            "Yes yes, of course, yeah.",
            "Yes, it's it's not very good."
        ],
        [
            "OK, now to the complexity.",
            "So we have we have this.",
            "It depends on how you look at it.",
            "Either this sorting complexity or maximum dsquared.",
            "Now of course the question is it's an iterative method.",
            "How many iterations do we need?",
            "And of course if the iterations in fact depend on the dimensionality then the analysis is a bit more complex.",
            "So for the faces data here you see dimensionality versus cardinality and the color denotes the number of.",
            "Yeah, iterations that we need until convergence and what you see is there is in fact a dependency on the dimensionality.",
            "But it's fairly weak and I would say sub linear and as you see here also the total number of yam iterations for that precision is actually low.",
            "There is also like.",
            "So something like a hard regime here, so for for sparse solutions.",
            "Actually our algorithm has to do more than the others.",
            "So what you see here is.",
            "OK sorry this is very bad.",
            "The runtime in Matlab, so just reference implementations.",
            "I guess the I mean the axis offset.",
            "It's not really important, it's more about the behavior for increasing the cardinality.",
            "So what we have here?",
            "But you probably cannot see is SPCA up there.",
            "We have past PCA here and this is our algorithm and again you see that there is a small bump here in that sort of let's say K between.",
            "10 and 40 features, but overall on this data set.",
            "I mean there is not much of a dependence on the cardinality.",
            "There are others, but usually I mean the complexity still stays very low."
        ],
        [
            "OK, non negative sparse PCA.",
            "Best result of the 10 random restarts.",
            "So so we just use random initialization.",
            "You see here in black MPC or algorithm and this NSA algorithm which has 2 two tuning parameters.",
            "Sparsity parameter beta, which we just determined by bisection search so we can plot the cardinality, and a Alpha is an orthonormal orthonormality penalty and you see that actually the behavior strongly depends on Alpha.",
            "So if Alpha is low, we get good performance.",
            "But unfortunately we cannot really sweep the whole regularization path.",
            "If you turn up the Alpha, we end up down here so there seem."
        ],
        [
            "It will be a problem with the sparsity formulation in that algorithm.",
            "Multiple principal components in the non negative case.",
            "As I said before, we have a problem if the first negative loadings do negative loadings lying, then negative orthant.",
            "So because then there is no second orthogonal.",
            "Component, So what can we do?",
            "We can either keep the orthogonality requirement and just have sparse components, or just require a minimum angle instead, and that's what we have here.",
            "So orthogonal sparse loadings.",
            "Our algorithm in reference, the circles here or actually without the non negativity.",
            "So for that data set doesn't make much of a difference and we see again here.",
            "The 10 SPCA has some problems so this is for the features total.",
            "And here on the right side for the quasi orthogonal case, and what you actually can observe here is the difference between the sequential variance maximization formulation of our algorithm in the deflation setting.",
            "So it starts out high and then it flattens off because it already I mean.",
            "Where is Ennis PCA actually considers cumulative variance for the number of components, so in the end we achieve better cumulative variance, so I would say I mean depending on the setting.",
            "Either of these algorithms could be."
        ],
        [
            "Preferable.",
            "As a last experiment.",
            "An unsupervised gene selection tasks.",
            "So what we did is compare the feature selection used by NPC to a criterion.",
            "Proposed PowerShell version of Ski in 2006 in bioinformatics.",
            "So basically what they do is they do a leave one out comparison.",
            "So remove a feature, do the SVD of the data matrix and look at the.",
            "At the distribution of the singular value spectrum.",
            "And.",
            "Depending on the largest difference, they it's just a simple ranking.",
            "This is the the only scheme which is actually efficient enough to compute on this large data set.",
            "So what do we do?",
            "We choose a gene subset.",
            "Based on the result of the algorithm and then we just do here, just K means clustering of the samples where the number of classes is equal to the labels 100 restarts, and then we compare the clustering assignment to the label and compute shakar scores, which basically I mean score of 1 up there would be perfect agreement between clustering and labels and hear the solid line is is the result without any feature selection so.",
            ".544 including all genes and you see here that actually for the non negative variance of the MPC algorithm we get significantly better results at about 80 features."
        ],
        [
            "OK, that's it.",
            "So in summary, we have presented a disk where constraint PCA algorithm.",
            "I mean that the square is so much hand waving because we also have the ZM iterations in there.",
            "It's sparse, non negative constraints, so you can choose which ones you want to have strict in quasi orthogonality between components and it's sufficient for both the end, larger D&D much larger than case and we have competitive variants for sparse PCA and actually for some settings.",
            "We are superior in the non negative sparse PCA case and could improve on the efficiency and which is what is nice.",
            "We can specify the number of features directly instead of the bound.",
            "OK, that's it, thank."
        ],
        [
            "My.",
            "No, actually I have not added a let's say a classification.",
            "Powered into the pipeline.",
            "Basically it was just about about the unsupervised part of the method.",
            "That's why we have this unsupervised teen selection task in there.",
            "Come back and make this decorate the inside the city, so not the widely used method, yeah.",
            "So.",
            "A few words about mean whatever really distinctive features of the non legacy, a faster than normal.",
            "Yeah, I mean many of these methods are basically just.",
            "Or a factorization of the data matrix right into two parts.",
            "And here in the PCA setting, what you what you require is non negativity on the basis matrix but not on the loadings on the loadings for not on the basis various various in F you have more constraints, you require the data matrix itself to be non negative and so on.",
            "So I guess in the end many of these methods it's just a change of."
        ],
        [
            "Constraints maybe?",
            "You can go to this slide so so in the end this is what most of these methods do, right?",
            "This is the data matrix and the factorization, and in the end it's just about which constraints.",
            "So here in the PCA negative PCA you just have to constraints on W, Whereas in MF you would have them do non negativity on all three matrices.",
            "Any other questions?",
            "Since we have time out last one, so use your PC.",
            "It works by you specify the K that you want beforehand.",
            "Yeah, so when you want to do full path, how much of that computation would be amortized so you don't have to restart for every K?",
            "Yeah exactly, so this is the case where paths at the greedy methods are really efficient because you just add 1K to the solution.",
            "I mean you can of course.",
            "Usually you wouldn't do random initialization again.",
            "So, but just keep the W you had add increase K and rerun it again so I don't have the the results here.",
            "In the end it depends on on the.",
            "It depends on the shape of the data matrix, whether the greedy approach or the M approaches is.",
            "Is actually faster in the end would."
        ],
        [
            "I guess.",
            "The point is, I would say that the greedy methods really are.",
            "Very competitive for sparse solutions because.",
            "In the end, if you if you go let's say beyond maybe 100 genes or 100 pixels or so on.",
            "Because you have to actually run up.",
            "Takes more OK. Great, let's thank the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is another talk about PCA this time including constraint sparsity and non negativity and the idea is to derive an algorithm which is fast enough for many features.",
                    "label": 0
                },
                {
                    "sent": "Let's say 10,000 or more.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A sorta with the problem definition.",
                    "label": 0
                },
                {
                    "sent": "I guess we're all familiar with PCA.",
                    "label": 0
                },
                {
                    "sent": "There are two formulations, either the variance maximization formulation where basically PCA maximizes the variance of the projected data or the reconstruction formulation where the PC subspace is actually L2 optimal.",
                    "label": 0
                },
                {
                    "sent": "So minimal Euclidean reconstruction error so.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization problem in the variance maximization formulation, so.",
                    "label": 1
                },
                {
                    "sent": "It's a it's a convex maximization problem, so it's convex.",
                    "label": 1
                },
                {
                    "sent": "But unfortunately we maximize and we need this constraint here in order to have the problem well post.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yes, we know we can.",
                    "label": 1
                },
                {
                    "sent": "We can successively find further components just by again maximising under the orthogonality constraint and what we do here is consider this problem under two additional constraints, sparsity, so limiting the number of features.",
                    "label": 0
                },
                {
                    "sent": "Let's say at most K features.",
                    "label": 0
                },
                {
                    "sent": "The second constraint is non negativity, meaning that it's a linear combination where we have just equal signs, so no differences between features considering.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, why do we want to do this?",
                    "label": 0
                },
                {
                    "sent": "The idea is that the constraints allow us to.",
                    "label": 0
                },
                {
                    "sent": "They facilitate the tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So basically we still want to maximize variance, so that's the criterion of interest, but.",
                    "label": 0
                },
                {
                    "sent": "Due to the constraints that say we might want to interpret the loadings of the PC components, and of course if we.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's well known that that for practical data, what usually happens is that PCA composition sort of compresses the data in few components, right?",
                    "label": 0
                },
                {
                    "sent": "So you have the knee in the eigenvalue spectrum, but usually one of these loadings for one component or still full, so it's kind of hard to say.",
                    "label": 0
                },
                {
                    "sent": "Unless the situation is very clear, we have few high loadings and the rest are small, or maybe all of them are equal.",
                    "label": 0
                },
                {
                    "sent": "Also, there might be applications where the constraints might make sense, so if due to the underlying physical process, let's say.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "It's finding non negative influence so non negative combination of influence might be what we are looking for.",
                    "label": 0
                },
                {
                    "sent": "Also you could think of examples where actually including a feature has an associated cost and in that setting it might make sense to reduce the variance a little in order to use fewer features.",
                    "label": 0
                },
                {
                    "sent": "Now sparse PCA.",
                    "label": 0
                },
                {
                    "sent": "I think that the motivation sorting into.",
                    "label": 0
                },
                {
                    "sent": "Mid 90s was from an interpretation of the PC loadings, but it has done also been applied to unsupervised gene selection and ranking.",
                    "label": 1
                },
                {
                    "sent": "Non negative sparse PCA is.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry the green is.",
                    "label": 0
                },
                {
                    "sent": "I hope you can read it is a 2006 nips paper by slash Assassin's Shashua on what you see here is is a visualization of on the left side.",
                    "label": 1
                },
                {
                    "sent": "We have the full loadings of the first principle components for and here are non negative sparse loadings and I guess it's just illustrates that.",
                    "label": 0
                },
                {
                    "sent": "I mean there are sparse so there are few few dimensions included in here and.",
                    "label": 0
                },
                {
                    "sent": "It's kind of we see natural parts.",
                    "label": 0
                },
                {
                    "sent": "Maybe like eyebrows or or eyes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, how how to attack the problem?",
                    "label": 0
                },
                {
                    "sent": "If you write out the quadratic form as an explicit some, we immediately see that if we set a feature to zero, what this means is that we we delete columns and rows in the.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrix, so for a given sparsity pattern, actually it's easy for sparse PCA to compute the weights.",
                    "label": 1
                },
                {
                    "sent": "So it's really about the pattern.",
                    "label": 0
                },
                {
                    "sent": "So combinatorial search is 1 approach.",
                    "label": 0
                },
                {
                    "sent": "An exact French in palm with it by McAdam, for which works well for smaller T and also a greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "Forward and backward search, which was then improved too.",
                    "label": 0
                },
                {
                    "sent": "D square for step complexity less, use acnl because it's in MP.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See the problem?",
                    "label": 0
                },
                {
                    "sent": "We can verify it.",
                    "label": 0
                },
                {
                    "sent": "In colonial time, all those are the continuous approximation, so we do an L1 relaxation if we just add the L1 penalty to the original problem.",
                    "label": 0
                },
                {
                    "sent": "So here is a visualization we have the bowl.",
                    "label": 0
                },
                {
                    "sent": "It's shaped like this because it's positive definite and it's constraint on the unit circle.",
                    "label": 0
                },
                {
                    "sent": "Then what happens is that we cut away parts of that circle and So what we have here is we introduce local minimum.",
                    "label": 0
                },
                {
                    "sent": "And there are various continuous algorithms based on iterative L1 regression, convex SV STP approximation which has very nice results but is kind of expensive and also at last years I CML there was a DC minimization algorithm with cubed.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complexity preparation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what do we do?",
                    "label": 0
                },
                {
                    "sent": "The algorithm is motivated from probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "The EM algorithm for probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "So if you've attended the.",
                    "label": 0
                },
                {
                    "sent": "The tutorial on Saturday by Neil Lawrence.",
                    "label": 0
                },
                {
                    "sent": "You're already familiar, so this is just.",
                    "label": 0
                },
                {
                    "sent": "The very basic, so it's a generative model.",
                    "label": 0
                },
                {
                    "sent": "We have the latent latent variable in the PC subspace and say its center Gaussian and then the conditioned observation on that latent variable is a linear motion model.",
                    "label": 1
                },
                {
                    "sent": "And we have here the noise.",
                    "label": 0
                },
                {
                    "sent": "So basically we sample here and then.",
                    "label": 0
                },
                {
                    "sent": "Given the position of Y in the subspace we sampled, the accent is then gives.",
                    "label": 0
                },
                {
                    "sent": "These are distribution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm.",
                    "label": 0
                },
                {
                    "sent": "You also provided an EM algorithm for that will not give the equations here.",
                    "label": 0
                },
                {
                    "sent": "What we can do is we can simplify it.",
                    "label": 0
                },
                {
                    "sent": "Three things, so I take the non probabilistic limit so Sigma 20, considering L1L equals one subspace.",
                    "label": 0
                },
                {
                    "sent": "So just the first component and I normalize just to unity in order to simplify notation and then the EM algorithm is very simple so you see that the eastep amounts to orthogonal projection of the data onto the current estimate of the subspace and then the M step is just minimization of reconstruction error.",
                    "label": 1
                },
                {
                    "sent": "Between the original data point and this reconstruction and then we re normalize and iterate.",
                    "label": 0
                },
                {
                    "sent": "So here is a.",
                    "label": 0
                },
                {
                    "sent": "Here is an illustration from the PR Mail book from Bishop.",
                    "label": 0
                },
                {
                    "sent": "So sample data here are the two principal axis.",
                    "label": 0
                },
                {
                    "sent": "We choose a random direction.",
                    "label": 0
                },
                {
                    "sent": "First project the data orthogonally.",
                    "label": 0
                },
                {
                    "sent": "So we get.",
                    "label": 0
                },
                {
                    "sent": "The wise and then we optimize the W in order to minimize.",
                    "label": 0
                },
                {
                    "sent": "Erotic distance and we iterate this and we see that already after like 2 1/2 iterations.",
                    "label": 0
                },
                {
                    "sent": "We basically turn and have identified the first principle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Points.",
                    "label": 0
                },
                {
                    "sent": "Now if we have these settings, so we have split the.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The original problem.",
                    "label": 0
                },
                {
                    "sent": "I mean we have approximated of course it's just a local method, but we split it in two parts and we can easily add the constraints to the second.",
                    "label": 0
                },
                {
                    "sent": "So we just rewrite that formula here.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just a change of notation.",
                    "label": 0
                },
                {
                    "sent": "So we see you have an isotropic QP.",
                    "label": 0
                },
                {
                    "sent": "So basically the Hessian is just the scale identity matrix.",
                    "label": 0
                },
                {
                    "sent": "H is positive, so it's it's convex, and so therefore we can efficiently apply the L1 bound.",
                    "label": 0
                },
                {
                    "sent": "And if we want the non negativity.",
                    "label": 0
                },
                {
                    "sent": "So what this means is because the there are no interaction terms.",
                    "label": 0
                },
                {
                    "sent": "Is that all?",
                    "label": 0
                },
                {
                    "sent": "This means is just we minimize L2 distance.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the unconstrained optimum so.",
                    "label": 0
                },
                {
                    "sent": "So if this here is the optimum and this is the L1 boundary, then that's the point we're looking for.",
                    "label": 0
                },
                {
                    "sent": "So to motivate what do we do?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not expensive to solve that, so after transformation into the non negative earth and so we just remember the sign structure, we start at the origin and then we take axis aligned gradient decent steps.",
                    "label": 0
                },
                {
                    "sent": "So small step in the direction of the largest element of the negative gradient and we continue until we arrive at that point.",
                    "label": 0
                },
                {
                    "sent": "This allows us to XX not specify the boundary B, but actually just directly the sparsity K because due to monitor monotonicity so.",
                    "label": 0
                },
                {
                    "sent": "Features enter the solution.",
                    "label": 0
                },
                {
                    "sent": "One of the other and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's nice.",
                    "label": 0
                },
                {
                    "sent": "We also.",
                    "label": 0
                },
                {
                    "sent": "I mean we don't have to explicitly do this.",
                    "label": 0
                },
                {
                    "sent": "Do these steps.",
                    "label": 0
                },
                {
                    "sent": "I mean we can directly just compute the point where another feature enters the solution.",
                    "label": 0
                },
                {
                    "sent": "So basically all this amounts to is.",
                    "label": 0
                },
                {
                    "sent": "Is to sort the elements of W star, which is D log D and then we add up.",
                    "label": 1
                },
                {
                    "sent": "Until we arrive at that point, W circle here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how do we get multiple components here?",
                    "label": 0
                },
                {
                    "sent": "We just do iterative deflation, so we compute the first sparse principal component and then we just protect it out.",
                    "label": 0
                },
                {
                    "sent": "Projected out of the data and repeat the procedure.",
                    "label": 0
                },
                {
                    "sent": "Now it's immediately obvious that if we also include non negativity constraints like orthogonality and non negativity is quite a strong requirement and you can only be orthogonal if a feature and to only one of the loading.",
                    "label": 0
                },
                {
                    "sent": "So if feature I is included in PCL you cannot include it anymore and this might be a two strong requirement.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can just enforce quasi orthogonality, meaning that we just enforce the minimum angle between components which for the non negative setting just amounts to a an additional linear constraint.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the method now.",
                    "label": 0
                },
                {
                    "sent": "Experiments and results.",
                    "label": 0
                },
                {
                    "sent": "So we've compared our method to three algorithms.",
                    "label": 0
                },
                {
                    "sent": "As I said before, there are more, but basically why did we choose these three?",
                    "label": 0
                },
                {
                    "sent": "SPCA Beisu and others in 2004?",
                    "label": 0
                },
                {
                    "sent": "Is is an iterative method as ours.",
                    "label": 0
                },
                {
                    "sent": "It also has these L1 penalty at some point and.",
                    "label": 0
                },
                {
                    "sent": "To reduce the data matrix instead of the covariance matrix, so it's sufficient for the D very much larger than case paths.",
                    "label": 0
                },
                {
                    "sent": "PCA is this combinatorial greedy algorithm is just a greedy forward search and SPCA is the only non negative sparse PCA algorithm I am aware of and it has this quasi orthogonality formulation.",
                    "label": 0
                },
                {
                    "sent": "Here we have results for two datasets.",
                    "label": 0
                },
                {
                    "sent": "So for the larger the case from the CCL faces data set, so about 2400 images 1919 pixels, which is cut.",
                    "label": 0
                },
                {
                    "sent": "The prominent set in data set in the NMF literature, and for the much larger than in case we look at any gene expression data set leukemia data.",
                    "label": 1
                },
                {
                    "sent": "So 72 expression profiles in about 12,500 genes.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "We, in the formulation we assume that the empirical mean is 0, so that's what we do here standardize to 0 mean.",
                    "label": 0
                },
                {
                    "sent": "Also we have here unit variance by dimension, but the results don't really change if we do this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, variance versus cardinality.",
                    "label": 0
                },
                {
                    "sent": "On the left side, what you see here is is.",
                    "label": 0
                },
                {
                    "sent": "In Gray, the crosses here is our method and SPCA.",
                    "label": 0
                },
                {
                    "sent": "Before we recompute the weights for the for the sparsity pattern, and so this renormalization step is really necessary in order to get the performance.",
                    "label": 0
                },
                {
                    "sent": "So read here is again SPCA off to renormalization and here for this state actually.",
                    "label": 0
                },
                {
                    "sent": "Arson past PCA.",
                    "label": 0
                },
                {
                    "sent": "Achieve the same result on the gene expression data.",
                    "label": 0
                },
                {
                    "sent": "So this is now all after renormalization, which is not actually necessary for past PCB 'cause this is optimal.",
                    "label": 0
                },
                {
                    "sent": "The weights are optimal by construction.",
                    "label": 0
                },
                {
                    "sent": "We have also added here like.",
                    "label": 0
                },
                {
                    "sent": "Sort of the baseline, so thresholding you just take pick the K largest elements of the first to the loadings of the first principle component.",
                    "label": 0
                },
                {
                    "sent": "And as you see, it actually performs quite well in these.",
                    "label": 0
                },
                {
                    "sent": "Of course, defense method considered in here.",
                    "label": 0
                },
                {
                    "sent": "The simple thresholding.",
                    "label": 0
                },
                {
                    "sent": "Yes yes, of course, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's it's not very good.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now to the complexity.",
                    "label": 0
                },
                {
                    "sent": "So we have we have this.",
                    "label": 0
                },
                {
                    "sent": "It depends on how you look at it.",
                    "label": 0
                },
                {
                    "sent": "Either this sorting complexity or maximum dsquared.",
                    "label": 0
                },
                {
                    "sent": "Now of course the question is it's an iterative method.",
                    "label": 0
                },
                {
                    "sent": "How many iterations do we need?",
                    "label": 0
                },
                {
                    "sent": "And of course if the iterations in fact depend on the dimensionality then the analysis is a bit more complex.",
                    "label": 0
                },
                {
                    "sent": "So for the faces data here you see dimensionality versus cardinality and the color denotes the number of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, iterations that we need until convergence and what you see is there is in fact a dependency on the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "But it's fairly weak and I would say sub linear and as you see here also the total number of yam iterations for that precision is actually low.",
                    "label": 0
                },
                {
                    "sent": "There is also like.",
                    "label": 0
                },
                {
                    "sent": "So something like a hard regime here, so for for sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Actually our algorithm has to do more than the others.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is.",
                    "label": 0
                },
                {
                    "sent": "OK sorry this is very bad.",
                    "label": 0
                },
                {
                    "sent": "The runtime in Matlab, so just reference implementations.",
                    "label": 0
                },
                {
                    "sent": "I guess the I mean the axis offset.",
                    "label": 0
                },
                {
                    "sent": "It's not really important, it's more about the behavior for increasing the cardinality.",
                    "label": 0
                },
                {
                    "sent": "So what we have here?",
                    "label": 0
                },
                {
                    "sent": "But you probably cannot see is SPCA up there.",
                    "label": 0
                },
                {
                    "sent": "We have past PCA here and this is our algorithm and again you see that there is a small bump here in that sort of let's say K between.",
                    "label": 0
                },
                {
                    "sent": "10 and 40 features, but overall on this data set.",
                    "label": 0
                },
                {
                    "sent": "I mean there is not much of a dependence on the cardinality.",
                    "label": 0
                },
                {
                    "sent": "There are others, but usually I mean the complexity still stays very low.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, non negative sparse PCA.",
                    "label": 0
                },
                {
                    "sent": "Best result of the 10 random restarts.",
                    "label": 1
                },
                {
                    "sent": "So so we just use random initialization.",
                    "label": 0
                },
                {
                    "sent": "You see here in black MPC or algorithm and this NSA algorithm which has 2 two tuning parameters.",
                    "label": 0
                },
                {
                    "sent": "Sparsity parameter beta, which we just determined by bisection search so we can plot the cardinality, and a Alpha is an orthonormal orthonormality penalty and you see that actually the behavior strongly depends on Alpha.",
                    "label": 1
                },
                {
                    "sent": "So if Alpha is low, we get good performance.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately we cannot really sweep the whole regularization path.",
                    "label": 0
                },
                {
                    "sent": "If you turn up the Alpha, we end up down here so there seem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It will be a problem with the sparsity formulation in that algorithm.",
                    "label": 0
                },
                {
                    "sent": "Multiple principal components in the non negative case.",
                    "label": 1
                },
                {
                    "sent": "As I said before, we have a problem if the first negative loadings do negative loadings lying, then negative orthant.",
                    "label": 0
                },
                {
                    "sent": "So because then there is no second orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Component, So what can we do?",
                    "label": 0
                },
                {
                    "sent": "We can either keep the orthogonality requirement and just have sparse components, or just require a minimum angle instead, and that's what we have here.",
                    "label": 0
                },
                {
                    "sent": "So orthogonal sparse loadings.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm in reference, the circles here or actually without the non negativity.",
                    "label": 0
                },
                {
                    "sent": "So for that data set doesn't make much of a difference and we see again here.",
                    "label": 0
                },
                {
                    "sent": "The 10 SPCA has some problems so this is for the features total.",
                    "label": 0
                },
                {
                    "sent": "And here on the right side for the quasi orthogonal case, and what you actually can observe here is the difference between the sequential variance maximization formulation of our algorithm in the deflation setting.",
                    "label": 0
                },
                {
                    "sent": "So it starts out high and then it flattens off because it already I mean.",
                    "label": 0
                },
                {
                    "sent": "Where is Ennis PCA actually considers cumulative variance for the number of components, so in the end we achieve better cumulative variance, so I would say I mean depending on the setting.",
                    "label": 1
                },
                {
                    "sent": "Either of these algorithms could be.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Preferable.",
                    "label": 0
                },
                {
                    "sent": "As a last experiment.",
                    "label": 0
                },
                {
                    "sent": "An unsupervised gene selection tasks.",
                    "label": 1
                },
                {
                    "sent": "So what we did is compare the feature selection used by NPC to a criterion.",
                    "label": 0
                },
                {
                    "sent": "Proposed PowerShell version of Ski in 2006 in bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "So basically what they do is they do a leave one out comparison.",
                    "label": 0
                },
                {
                    "sent": "So remove a feature, do the SVD of the data matrix and look at the.",
                    "label": 0
                },
                {
                    "sent": "At the distribution of the singular value spectrum.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Depending on the largest difference, they it's just a simple ranking.",
                    "label": 0
                },
                {
                    "sent": "This is the the only scheme which is actually efficient enough to compute on this large data set.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 1
                },
                {
                    "sent": "We choose a gene subset.",
                    "label": 0
                },
                {
                    "sent": "Based on the result of the algorithm and then we just do here, just K means clustering of the samples where the number of classes is equal to the labels 100 restarts, and then we compare the clustering assignment to the label and compute shakar scores, which basically I mean score of 1 up there would be perfect agreement between clustering and labels and hear the solid line is is the result without any feature selection so.",
                    "label": 1
                },
                {
                    "sent": ".544 including all genes and you see here that actually for the non negative variance of the MPC algorithm we get significantly better results at about 80 features.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we have presented a disk where constraint PCA algorithm.",
                    "label": 1
                },
                {
                    "sent": "I mean that the square is so much hand waving because we also have the ZM iterations in there.",
                    "label": 0
                },
                {
                    "sent": "It's sparse, non negative constraints, so you can choose which ones you want to have strict in quasi orthogonality between components and it's sufficient for both the end, larger D&D much larger than case and we have competitive variants for sparse PCA and actually for some settings.",
                    "label": 0
                },
                {
                    "sent": "We are superior in the non negative sparse PCA case and could improve on the efficiency and which is what is nice.",
                    "label": 0
                },
                {
                    "sent": "We can specify the number of features directly instead of the bound.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it, thank.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "No, actually I have not added a let's say a classification.",
                    "label": 0
                },
                {
                    "sent": "Powered into the pipeline.",
                    "label": 0
                },
                {
                    "sent": "Basically it was just about about the unsupervised part of the method.",
                    "label": 0
                },
                {
                    "sent": "That's why we have this unsupervised teen selection task in there.",
                    "label": 0
                },
                {
                    "sent": "Come back and make this decorate the inside the city, so not the widely used method, yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A few words about mean whatever really distinctive features of the non legacy, a faster than normal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean many of these methods are basically just.",
                    "label": 0
                },
                {
                    "sent": "Or a factorization of the data matrix right into two parts.",
                    "label": 0
                },
                {
                    "sent": "And here in the PCA setting, what you what you require is non negativity on the basis matrix but not on the loadings on the loadings for not on the basis various various in F you have more constraints, you require the data matrix itself to be non negative and so on.",
                    "label": 0
                },
                {
                    "sent": "So I guess in the end many of these methods it's just a change of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraints maybe?",
                    "label": 0
                },
                {
                    "sent": "You can go to this slide so so in the end this is what most of these methods do, right?",
                    "label": 0
                },
                {
                    "sent": "This is the data matrix and the factorization, and in the end it's just about which constraints.",
                    "label": 0
                },
                {
                    "sent": "So here in the PCA negative PCA you just have to constraints on W, Whereas in MF you would have them do non negativity on all three matrices.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Since we have time out last one, so use your PC.",
                    "label": 0
                },
                {
                    "sent": "It works by you specify the K that you want beforehand.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so when you want to do full path, how much of that computation would be amortized so you don't have to restart for every K?",
                    "label": 0
                },
                {
                    "sent": "Yeah exactly, so this is the case where paths at the greedy methods are really efficient because you just add 1K to the solution.",
                    "label": 0
                },
                {
                    "sent": "I mean you can of course.",
                    "label": 0
                },
                {
                    "sent": "Usually you wouldn't do random initialization again.",
                    "label": 0
                },
                {
                    "sent": "So, but just keep the W you had add increase K and rerun it again so I don't have the the results here.",
                    "label": 0
                },
                {
                    "sent": "In the end it depends on on the.",
                    "label": 0
                },
                {
                    "sent": "It depends on the shape of the data matrix, whether the greedy approach or the M approaches is.",
                    "label": 0
                },
                {
                    "sent": "Is actually faster in the end would.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "The point is, I would say that the greedy methods really are.",
                    "label": 0
                },
                {
                    "sent": "Very competitive for sparse solutions because.",
                    "label": 0
                },
                {
                    "sent": "In the end, if you if you go let's say beyond maybe 100 genes or 100 pixels or so on.",
                    "label": 0
                },
                {
                    "sent": "Because you have to actually run up.",
                    "label": 0
                },
                {
                    "sent": "Takes more OK. Great, let's thank the speaker.",
                    "label": 0
                }
            ]
        }
    }
}