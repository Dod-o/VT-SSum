{
    "id": "b47keqxfdsyilwxfrrwocpajwttv24nw",
    "title": "Time-rescaling Methods for the Estimation and Assessment of Non-Poisson Neural Encoding Models",
    "info": {
        "author": [
            "Jonathan Pillow, University of Texas at Austin"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_pillow_trme/",
    "segmentation": [
        [
            "Alright, well thanks.",
            "Thanks everyone for sticking around for the last talk in the morning I'm going to tell you about some recent work that we've been doing in my lab on the neural coding problem.",
            "So first of all I'd like to just start by setting up the problem for you in neuroscience.",
            "One of the one of the most important questions we were trying to answer is the question of how different natural signals in the world get encoded by."
        ],
        [
            "Spike trains in the brain, so of course brains differ in their statistical properties, but we know something about the types of signals that come in, and some people try to make good models of these natural signals, and the question that I'm interested in is how to how to understand the mapping from those natural signals to the spike trains that come out.",
            "What is the coding relationship that describes how these spikes serve to represent or store information about or provide information about what there is out in the real world?",
            "So this is sometimes called the neural coding problem, and it's roughly the idea of trying to find models that describe the probabilistic relationship.",
            "Between stimuli and spike trains.",
            "OK, so implicitly what we want to find here is an."
        ],
        [
            "Explicit model of the probability over spike times given stimulus and there is a family of models that's been very successful and very popular in addressing this problem recently.",
            "There are sometimes called the point with the mouse.",
            "Let's see."
        ],
        [
            "OK, well I'm just going to use the pointer like this.",
            "OK, the class of models that people have studied a lot in recent years are sometimes called these cascade type models and so they consist of a cascade of stages that two particular operations on the incoming signal.",
            "So the incoming signal X and this particular linear nonlinear prasan model cascade model.",
            "The high dimensional input signal comes in.",
            "There's a linear receptive field which we're going to talk a stimulus filter.",
            "The output of that filter is a single number, which which.",
            "Is a scalar value quantity.",
            "This first stage is doing dimensionality reduction effectively.",
            "Then we have a non linearity that converts that scalar to an instantaneous probability of spiking and the output of the model then is a POS on process.",
            "OK, well there's one thing wrong with this.",
            "OK well so this model of course is very flexible and it's easy to fit to data.",
            "So it's very tractable for the fitting problem.",
            "One problem with the model however, is that we know that real real spike trains are not plus on, so the property of prasan processes that recent spike generation is completely independent of the.",
            "Occurrence or absence of spikes in the previous previous increment, and so we know that in fact neurons have things like refractory effects and they have adaptation and they can do other other history dependent properties like bursting and so.",
            "The focus for today's talk is going to look at two particular strategies for incorporating spike history dependencies into neural models and see if we can say something about the mathematical properties of these different model modeling attempts to incorporate spike history dependence.",
            "OK, so the first model one that I've worked worked with a fair amount in the last couple of years.",
            "Sometimes called the generalized linear model on the output of this model, is a conditional Poisson process, so it's not.",
            "It's not a Poisson process because there is history dependence in the spike generation, so the model looks the same as the previous one.",
            "If you look at the at, the difference between the figures here all we're adding is a post spike filter which comes in it's.",
            "You can think of it as a feedback term such that every time we get a spike out of this neuron we feedback a particular shape, particular waveform or current into the integrator, which influences the subsequent probability of spiking.",
            "So it's a conditional cross on process, and what that means is that.",
            "Conditioned on the spike history up to a particular time, the future spike trains the future spikes will be generated according to a rate varying prasan process where that rate depends on the history of spiking.",
            "OK, alright."
        ],
        [
            "So we can write down the intensity.",
            "This Lambda T is what I'm going to do throughout the talk to describe the instantaneous rate of the process.",
            "So that's roughly you can think of is the spike rate of the neuron, and in this model that's just defined as in terms of the stimulus and the spike train history.",
            "It's actually a linear.",
            "The reason for this name generalized linear model is the fact that we have the stimulus at Time TX and the spike history at time Ty and those were operated on by a pair of linear filters K&H here.",
            "So K is this H is this.",
            "Alright, the properties of this model is that with this age, so this age can take on arbitrary weight arbitrary shape."
        ],
        [
            "In order to get different types of history dependence in the model, so it provides very flexible model of spike history effects and just to show you what a few of those effects are.",
            "Here we can get things like regular spiking.",
            "We can get bursting behavior, we can get adaptation.",
            "All it all in response to a continuous step current input.",
            "All is a function just by changing the shape of this HH correct?",
            "OK, so and more importantly."
        ],
        [
            "Although the parameters for this K in this H live in a very high dimensional parameter space, there's a very nice result due to Liam Peninsula Paper from 2004, but the log likelihood for this model is concave, so it says downward curvature at all places, which precludes any sub optimal local Maxima so we can be guaranteed that just by doing gradient descent of the likelihood function will find the global maximum, which makes maximum likelihood fitting very tractable.",
            "OK, so that's the conditional processing process model.",
            "The other model that people have used, it's actually been used in neuroscience for a bit longer than this generalized linear model.",
            "I'm going to call it a modulated renewal process."
        ],
        [
            "This model OK.",
            "So instead of having a spike history term here, we're going to say is that instead of cross on spiking, we're going to have renewal spiking where this term Q.",
            "Here is an arbitrary renewal density, and I'll come back to what that is in a second to hope to give you a little bit more intuition about how how Q can give you spike history dependencies.",
            "But roughly you can imagine that the shape of Q is what determines when the next spike will occur relative to the time of the last spike, and this Q then gets rescaled by the instantaneous rate Lambda.",
            "OK, so just show you an example of what."
        ],
        [
            "This looks like one example you might think about is a modulated gamma process.",
            "So a gamma distribution.",
            "It could be written like this, which has a shape parameter, Kappa and that shape parameter determines the shape of this of this interspike interval density and the way we could generate samples from the homogeneous gamma process is just to generate each successive spike time from this density, right?",
            "So draw spikes and just lay them down one after the other by drawing from this density.",
            "So if CAP is 1 here, we actually get the exponential distribution and this therefore is a process on process.",
            "So special case of gamma is the process.",
            "Process, but of course gamma can be used to give us different types of history dependence if we let Kappa be greater than one, we have some sort of refractory effect, so there's a very small probability of seeing a spike right after you just spiked and the next bike is likely to be shifted out in time.",
            "So these spike trains out of this model or more regular, and we can make Kappa higher and higher to get increasingly regular interspike intervals out of the model.",
            "OK, so the intensity in this model of course is just a."
        ],
        [
            "Dot product between the stimulus filter K and the spike and the stimulus X and then the the renewal properties of this density Q will be what determined the history dependence of the model output.",
            "OK, now I haven't said anything about the likelihood or the properties of the likelihood of this model, and so one of one of the sort of results of this of our NIPS paper is to prove something about that likelihood.",
            "So I'm going to take you through that in a moment."
        ],
        [
            "So I'd like to give you a little bit of of intuition about the likelihood of the conditional.",
            "Sorry, the inhomogeneous renewal model, So what?"
        ],
        [
            "I described previously this process of generating spikes."
        ],
        [
            "On by drawing each one from a fixed density that will give me a homogeneous renewal process.",
            "How do I get a time varying renewal process with the rate parameter?"
        ],
        [
            "About the variant time I can get that using something called the time rescaling."
        ],
        [
            "Transport, so suppose I have a time varying rate Lambda T that looks like this and I would like to get a spike train which has gamma intervals where the rate is being continuously varied according to that particular shape.",
            "Well, the intuition behind what I want to do is to start with spike times from a homogeneous renewal process so I can just draw my spike times from the gamma distribution with a fixed fixed shape parameter, and then when I affectively what I want to do is to squish time and stretch time in proportion to the amplitude of that of that rate.",
            "So when the rate is high.",
            "Effectively want to take these spikes and push them together, right?",
            "That'll make for shorter interspike intervals, and when the rate is low, I want to stretch those apart.",
            "OK, so that's the idea behind the time we scaling transform mathematically, we can.",
            "We can pick that.",
            "OK, yeah, so if we actually apply that to these spikes."
        ],
        [
            "Times we started with these these homogeneous pipe times.",
            "We stretch time according to the rate here and we get spike times that occur when the rate is high preferentially.",
            "Alright, so mathematically now we can, we can."
        ],
        [
            "Describe that transform is actually the involves the cumulative integral of this intensity function.",
            "So here's the intensity on the spike rate as a function of time.",
            "If we integrate Lambda T, we actually get a function here, which takes us from rescaled time on the vertical axis to real time on the horizontal axis here, and so that process that I just described could be summarized as this to draw spikes from a homogeneous renewal process, I just draw spikes from this particular gamma distribution and those intervals mu U1U2U3U four.",
            "Are the intervals of the homogeneous process IMAP those through this cumulative intensity function, and then when the slope is high, the spikes get bunched up?",
            "The slope is high, of course, where this rate is high and then when the slope is low where the rate of the process is low, I stretched the interspike intervals out OK, so that's the time rescaling transform and we can also use the same key rescaling transform to define the likelihood for the model.",
            "So the likelihood of course in rescaled time these each of these intervals was drawn independently from a fixed distribution.",
            "That's the definition of a renewal process, and so the likelihood over these.",
            "Over these intervals in rescaled time is just the."
        ],
        [
            "Oil density on the product of the renewal density applied to these intervals, right?",
            "So if I want to map map that likelihood into real time, however, I have to do a standard change of variables and when I apply that I end up applying Q2 capital Lambda of T. So the time we scaled intervals.",
            "This is the integral of the process from the previous spike time up to the next fight time inside Q times the intensity at the time of a spike.",
            "That doesn't make sense, it's done.",
            "It's not important for the talk, but I wanted to give you some intuition for how we can get the likelihood function.",
            "Over the spike times in real time, and now we're ready to describe it."
        ],
        [
            "Theoretical result, which is that we can prove a theorem that this log likelihood in the filter parameter.",
            "So remember that's the receptive field have been around is long concave when the renewal density is both nonincreasing and log in cave.",
            "So specifically that rules out densities, where, so to say that again this this the density Q which I'm drawing my renewal process from has to be either constant or decreasing and it has to be decreasing at least as fast as either the minus X. OK, that's what this log concave requirement requires.",
            "And So what this says is it rules out many of the common interspike interval densities that people would like to use to describe neurons as renewal processes.",
            "Usually people settle on gamma with Kappa greater than one, so you have some regularity to the intervals or inverse Gaussian.",
            "Which is the density of 1st first passage density for integrating fire neuron or lognormal distribution which is sort of heavy tailed.",
            "All of these densities have a rising phase which is which violates this requirement of not increasing this OK of course is the exponential density which is a plus on process.",
            "There are many others that are also OK as long as they don't as long as they are monotonically decreasing and don't decrease at least this fast as E to the minus X. OK, so that's one result.",
            "A corollary of this theorem of course is that if we want to do decoding we want to estimate the stimulus given the spike times.",
            "The same requirement on the density holds such that the decoding problem is a convex problem.",
            "OK, so if Q obeys the same properties then we can estimate the map.",
            "We can find the map estimate of the stimulus given the spikes.",
            "Also by doing gradient descent and so the summary of this first this first."
        ],
        [
            "Medical result is just to say if you were unclear about which of these models to use to capture spike dependencies between neurons, the conditional prasan model seems like a better bet.",
            "Just because we have this convex loss function with no restrictions on the shape of age.",
            "Here we can have refractory type affects or bursting type affects.",
            "This age can be anything we want, whereas if we want to have a convex loss function here for the modulated renewal model, we have to restrict Q in a very in a very specific way that rules out refractory effects.",
            "So right, if the renewal density is always decreasing then we can have an initial.",
            "Where it's where there's a zero probability of having a spike.",
            "OK, so that's result one we can actually combine these models.",
            "So one thing we might think about doing, why not have a renewal process?",
            "An arbitrary renewal process.",
            "Q&A Feedback term H in the same model like this sort of an obvious thing to do from."
        ],
        [
            "Mathematical standpoint, we just plug in.",
            "We make a conditional intensity Lambda which depends on the spike history and the stimulus, and we have an arbitrary renewal density Q.",
            "And this is now the Super set of the of the two previous models, right?",
            "So it has somewhat more statistical power and more flexibility in the previous model, and in fact we can write the likelihood in a very nice way."
        ],
        [
            "For this conditional renewal model, I should say I'm calling it a conditional renewal model because the output is a renewal process."
        ],
        [
            "Yes, and inhomogeneous renewal process conditioned on the spike train history up to a single point.",
            "So if I know this by train up till now I have a time varying rate that describes an inhomogeneous renewal process for my subsequent spikes.",
            "OK, so the nice thing about writing this model down this way is that the likelihood can be factored into two terms, one of which is the conditional Poisson likelihood.",
            "So this first term is just the likelihood that we would have obtained if we assume that Q is an exponential density.",
            "So we had just across on process here and then the second term here is actually just going to pick up the slack."
        ],
        [
            "Of whatever was missed by my across an approximation to the to the renewal process.",
            "So this is Q with the rescaled interspike intervals here, here the interspike interval rescaled to be within the unit interval, zero to 1.",
            "So this is actually a copula here.",
            "If you know if you know about that family of models, but it's a density that captures deviations from prasana's.",
            "Alright, so we can actually separately measure the contribution of these two terms to the likelihood.",
            "OK, now we have an implication for something."
        ],
        [
            "Call the time rescaling goodness of fit test, which is a very very cool trick for evaluating different models of neural responses, so this is introduced by Emery Brown's group in 2001.",
            "I'm going to give you a rough idea behind the time we scaling goodness of fit test.",
            "The basic idea is that if you have a model of the neural response, you can take the observed spike times, map them through their cumulative intensity and then the intervals on this.",
            "In this rescaled time axis should should come from a single distribution and the IID distributed from that density, right?",
            "So if we actually had a pass on process and we re scaled our intervals according to this cumulative intensity are histogram here should look like an exponential distribution on each of these intervals should be IID exponential and the idea behind the goodness of fit tests that they used was to say, let's compare."
        ],
        [
            "Quantiles of this empirical distribution for the red red dots against the expected quantiles under the dotted black line here, and the red trace here lies exactly on the diagonal, so if this is the case, statistic is the largest deviation of this red trace from the diagonal, and so you can see if things are along the diagonal, then yes, these rescaled intervals closely obey the assumptions of the model.",
            "If there's a big discrepancy like this blue curve here, then then the intervals don't obey the assumption of cross on this, and so we're doing a bad job, and so I wanted to sound.",
            "A note of caution about this test, which is that it can be easily fooled.",
            "This test, in some sense is only sensitive to the accuracy with which you're modeling the renewal statistics of the process in rescaled time, and there's somewhat indifferent to the accuracy with which you're modeling the rate that I'm going rate parameter itself, and so I think that's easiest to demonstrate by means of a simulated toy example, so it's kind of a fake example.",
            "Here's the claim spelled out.",
            "If we estimate the renewal density Q and a nonparametric way, then we can trivially pass the time rescaling goodness of fit test, so we can always get our model.",
            "By along the diagonal.",
            "Here, OK, what is that?",
            "What?",
            "Here's the toy example?"
        ],
        [
            "Take you through.",
            "So what I'm going to assume is that we have a true a true neuron, which is in fact this simulated neurons within the true neuron has a stimulus filter K, and it has a renewal process which is gamma alright.",
            "So the spike trains come by convolving the stimulus with this filter and then generating spikes according to an inhomogeneous gamma process.",
            "And then I'm in approximate those spike times using 2 sub optimal approximations.",
            "The first one is going to be a model where I assume that the model is plus on and I've somehow estimated the stimulus filter correctly, but I've missed estimated the renewal properties, right?",
            "So I've said.",
            "I'm going to assume it pass on this.",
            "I didn't think to try Gamma on the other model.",
            "Would be to say somehow I missed up.",
            "I messed up my estimate of the stimulus filter here.",
            "K right.",
            "So I actually have the negative inverse, the additive inverse of the true filter, but once I fix that filter that defines a mapping into rescaled time and now I can build a nonparametric estimate of the density of rescaled interspike intervals over here, right?",
            "So this is the density that I would get by mapping into rescaled time and fitting so I can."
        ],
        [
            "Sorry, so if I fit just histogram based estimator to this rescaled interval density, that's what I get here.",
            "In this second model Q. OK so here is."
        ],
        [
            "Related Masters out of the three models and what you can see by eye is that the exponential model gives a does a good job of capturing the time frame rate information, whereas this sub optimal model does a very poor job.",
            "However, if we compute the goodness of fit test under the KS statistic."
        ],
        [
            "Red model, this one with the completely wrong rate rate variation, seems to be right along the diagonal.",
            "Where's this model?",
            "The process on one is badly is doing very badly, so this is all you looked at.",
            "You would conclude that this model is bad in this model is good, but instead we cross validate using log likely."
        ],
        [
            "We can show that there's a lot more information about the original spike times from the blue model.",
            "Then there is about this.",
            "Red model is slightly negative information relative to a homogeneous closson process, so that's the.",
            "Very last pieces, just to show you."
        ],
        [
            "Into data so we can we can think actually have a 2 by 2.",
            "One minute, 2 by two table here, which where we have in the upper left hand corner A is a model which is just a inhomogeneous prasan process and then we can expand that model in two ways.",
            "We can either add a renewal density Q or we can add a spike history term H over here and then we have the conditional renewal bottle in this corner where we've added both a renewal density Q and this recurrent term age and we can now look at."
        ],
        [
            "First, the cash statistique validation.",
            "The red model would seem to be doing worse.",
            "It still fails at 90 at the 95% confidence interval.",
            "But if we look at the changes in the log likelihood cross validation data, the increase from A to B is much smaller, so adding a renewal density Q is much smaller than if we go to from A to C. And unfortunately, if we go from C to adding both terms, see today is only very small improvement, so we're actually not gaining a lot by using this conditional renewal model, but that's something we're hoping to explore with other datasets.",
            "OK, so just to wrap up, so that is to say that the.",
            "It seems that adding a recurrent filter H is giving you more benefit than an obvious."
        ],
        [
            "In order to take you the conclusions are we describe this new Class A new model, the conditional renewal model, which has a very nice likelihood that factorizes into two terms that have Additionally pasan model and the deviations from class.",
            "On this we have a restrictive condition on Q such that the estimation problem is still convex.",
            "The model when it's when Q is estimated nonparametrically, trivially passes the time rescaling goodness of fit test, but unfortunately in data it doesn't seem to be significantly better than the GLM conditionally plus on model.",
            "So I just like to thanks for your attention."
        ],
        [
            "And and thanks to my data collaborators here, who provided the retinal data in that last example, thanks.",
            "So we have time for questions.",
            "Are there any?",
            "Page on so you pointed out for the renewal process model that there was non convexity under certain situations.",
            "That's an interesting result, but one of the kind of themes of NIPS is to fearlessly go in the face of non convexity.",
            "So how much of a problem really is that in practice?",
            "Do you?",
            "Can you show how difficult?",
            "How much the problem that causes?",
            "That's a great."
        ],
        [
            "Question what I would say is, although it's it's not probably convex, there could still be.",
            "We don't know for certain that if Q doesn't have those those properties that there are multiple local Maxima.",
            "So we can just fearlessly go ahead and maximize, but we won't know for sure that we have the global optimum.",
            "The I guess the prescriptive approach that I would suggest taking is to start with the condition we pass on model, so this since this likelihood term factorizes, we can be guaranteed that we have an optimal description under the under the Cross on Model, and then anything leftover is gravy.",
            "We can always improve.",
            "Basically if we look at the KS statistic after fitting this model.",
            "And there's a big discrepancy between the expected those things like power off the diagonal.",
            "Then we can introduce Q and continue to ascend, so we get we get a better model.",
            "We don't know if there's a better global maximum somewhere else, but we're still we're still doing better than this.",
            "You can think of this as a convex relaxation, I guess of the full problem, and in fact this density that I used here I didn't impose any restriction."
        ],
        [
            "For the real data case, yeah, in this model here I didn't impose any kind of restriction on Q, so you can just just.",
            "Estimated using using.",
            "Hope that you get the maximum."
        ],
        [
            "So actually a second question.",
            "So what happens?",
            "I mean, can one do inference easily if you have, for instance, a network of these models?",
            "Is that possible?",
            "So, so that's another case where the this conditional renewal model doesn't actually buy you that much over the so.",
            "So so you're asking about about network interactions, right?",
            "You have multiple such so.",
            "So one nice thing about this.",
            "This model with the spike history terms that we can also add in Spike history terms that couple one neuron."
        ],
        [
            "To the spike trains of despite another neuron so that if one neuron spikes, I have a subsequent influence on the probability.",
            "Speaking of another neuron.",
            "And so this GLM framework is very flexible and that it scales.",
            "It allows you to incorporate network type affects this.",
            "This renewal process model is really only about how the next bike determined depends on the previous spike from that same neuron.",
            "And I don't know there might be some interesting way to think about coupling neurons in rescaled time.",
            "So I suppose that would be what you would you would end up if you try doing network effects.",
            "But based on the application today to, I'd say that's.",
            "But we aren't getting very big improvement.",
            "When we when we try to add Q to our model already.",
            "But it would be something.",
            "Certainly try to write down.",
            "I think it would be interesting to know.",
            "So if there are no more questions, then let's thank Jonathan again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, well thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks everyone for sticking around for the last talk in the morning I'm going to tell you about some recent work that we've been doing in my lab on the neural coding problem.",
                    "label": 0
                },
                {
                    "sent": "So first of all I'd like to just start by setting up the problem for you in neuroscience.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the most important questions we were trying to answer is the question of how different natural signals in the world get encoded by.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spike trains in the brain, so of course brains differ in their statistical properties, but we know something about the types of signals that come in, and some people try to make good models of these natural signals, and the question that I'm interested in is how to how to understand the mapping from those natural signals to the spike trains that come out.",
                    "label": 0
                },
                {
                    "sent": "What is the coding relationship that describes how these spikes serve to represent or store information about or provide information about what there is out in the real world?",
                    "label": 0
                },
                {
                    "sent": "So this is sometimes called the neural coding problem, and it's roughly the idea of trying to find models that describe the probabilistic relationship.",
                    "label": 0
                },
                {
                    "sent": "Between stimuli and spike trains.",
                    "label": 1
                },
                {
                    "sent": "OK, so implicitly what we want to find here is an.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explicit model of the probability over spike times given stimulus and there is a family of models that's been very successful and very popular in addressing this problem recently.",
                    "label": 0
                },
                {
                    "sent": "There are sometimes called the point with the mouse.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well I'm just going to use the pointer like this.",
                    "label": 0
                },
                {
                    "sent": "OK, the class of models that people have studied a lot in recent years are sometimes called these cascade type models and so they consist of a cascade of stages that two particular operations on the incoming signal.",
                    "label": 0
                },
                {
                    "sent": "So the incoming signal X and this particular linear nonlinear prasan model cascade model.",
                    "label": 0
                },
                {
                    "sent": "The high dimensional input signal comes in.",
                    "label": 0
                },
                {
                    "sent": "There's a linear receptive field which we're going to talk a stimulus filter.",
                    "label": 0
                },
                {
                    "sent": "The output of that filter is a single number, which which.",
                    "label": 0
                },
                {
                    "sent": "Is a scalar value quantity.",
                    "label": 0
                },
                {
                    "sent": "This first stage is doing dimensionality reduction effectively.",
                    "label": 0
                },
                {
                    "sent": "Then we have a non linearity that converts that scalar to an instantaneous probability of spiking and the output of the model then is a POS on process.",
                    "label": 0
                },
                {
                    "sent": "OK, well there's one thing wrong with this.",
                    "label": 0
                },
                {
                    "sent": "OK well so this model of course is very flexible and it's easy to fit to data.",
                    "label": 0
                },
                {
                    "sent": "So it's very tractable for the fitting problem.",
                    "label": 0
                },
                {
                    "sent": "One problem with the model however, is that we know that real real spike trains are not plus on, so the property of prasan processes that recent spike generation is completely independent of the.",
                    "label": 1
                },
                {
                    "sent": "Occurrence or absence of spikes in the previous previous increment, and so we know that in fact neurons have things like refractory effects and they have adaptation and they can do other other history dependent properties like bursting and so.",
                    "label": 0
                },
                {
                    "sent": "The focus for today's talk is going to look at two particular strategies for incorporating spike history dependencies into neural models and see if we can say something about the mathematical properties of these different model modeling attempts to incorporate spike history dependence.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first model one that I've worked worked with a fair amount in the last couple of years.",
                    "label": 0
                },
                {
                    "sent": "Sometimes called the generalized linear model on the output of this model, is a conditional Poisson process, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a Poisson process because there is history dependence in the spike generation, so the model looks the same as the previous one.",
                    "label": 0
                },
                {
                    "sent": "If you look at the at, the difference between the figures here all we're adding is a post spike filter which comes in it's.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a feedback term such that every time we get a spike out of this neuron we feedback a particular shape, particular waveform or current into the integrator, which influences the subsequent probability of spiking.",
                    "label": 0
                },
                {
                    "sent": "So it's a conditional cross on process, and what that means is that.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on the spike history up to a particular time, the future spike trains the future spikes will be generated according to a rate varying prasan process where that rate depends on the history of spiking.",
                    "label": 0
                },
                {
                    "sent": "OK, alright.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can write down the intensity.",
                    "label": 0
                },
                {
                    "sent": "This Lambda T is what I'm going to do throughout the talk to describe the instantaneous rate of the process.",
                    "label": 0
                },
                {
                    "sent": "So that's roughly you can think of is the spike rate of the neuron, and in this model that's just defined as in terms of the stimulus and the spike train history.",
                    "label": 0
                },
                {
                    "sent": "It's actually a linear.",
                    "label": 0
                },
                {
                    "sent": "The reason for this name generalized linear model is the fact that we have the stimulus at Time TX and the spike history at time Ty and those were operated on by a pair of linear filters K&H here.",
                    "label": 1
                },
                {
                    "sent": "So K is this H is this.",
                    "label": 0
                },
                {
                    "sent": "Alright, the properties of this model is that with this age, so this age can take on arbitrary weight arbitrary shape.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to get different types of history dependence in the model, so it provides very flexible model of spike history effects and just to show you what a few of those effects are.",
                    "label": 1
                },
                {
                    "sent": "Here we can get things like regular spiking.",
                    "label": 0
                },
                {
                    "sent": "We can get bursting behavior, we can get adaptation.",
                    "label": 0
                },
                {
                    "sent": "All it all in response to a continuous step current input.",
                    "label": 0
                },
                {
                    "sent": "All is a function just by changing the shape of this HH correct?",
                    "label": 0
                },
                {
                    "sent": "OK, so and more importantly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Although the parameters for this K in this H live in a very high dimensional parameter space, there's a very nice result due to Liam Peninsula Paper from 2004, but the log likelihood for this model is concave, so it says downward curvature at all places, which precludes any sub optimal local Maxima so we can be guaranteed that just by doing gradient descent of the likelihood function will find the global maximum, which makes maximum likelihood fitting very tractable.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the conditional processing process model.",
                    "label": 0
                },
                {
                    "sent": "The other model that people have used, it's actually been used in neuroscience for a bit longer than this generalized linear model.",
                    "label": 1
                },
                {
                    "sent": "I'm going to call it a modulated renewal process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This model OK.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a spike history term here, we're going to say is that instead of cross on spiking, we're going to have renewal spiking where this term Q.",
                    "label": 0
                },
                {
                    "sent": "Here is an arbitrary renewal density, and I'll come back to what that is in a second to hope to give you a little bit more intuition about how how Q can give you spike history dependencies.",
                    "label": 0
                },
                {
                    "sent": "But roughly you can imagine that the shape of Q is what determines when the next spike will occur relative to the time of the last spike, and this Q then gets rescaled by the instantaneous rate Lambda.",
                    "label": 1
                },
                {
                    "sent": "OK, so just show you an example of what.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This looks like one example you might think about is a modulated gamma process.",
                    "label": 1
                },
                {
                    "sent": "So a gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "It could be written like this, which has a shape parameter, Kappa and that shape parameter determines the shape of this of this interspike interval density and the way we could generate samples from the homogeneous gamma process is just to generate each successive spike time from this density, right?",
                    "label": 0
                },
                {
                    "sent": "So draw spikes and just lay them down one after the other by drawing from this density.",
                    "label": 0
                },
                {
                    "sent": "So if CAP is 1 here, we actually get the exponential distribution and this therefore is a process on process.",
                    "label": 0
                },
                {
                    "sent": "So special case of gamma is the process.",
                    "label": 0
                },
                {
                    "sent": "Process, but of course gamma can be used to give us different types of history dependence if we let Kappa be greater than one, we have some sort of refractory effect, so there's a very small probability of seeing a spike right after you just spiked and the next bike is likely to be shifted out in time.",
                    "label": 0
                },
                {
                    "sent": "So these spike trains out of this model or more regular, and we can make Kappa higher and higher to get increasingly regular interspike intervals out of the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so the intensity in this model of course is just a.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dot product between the stimulus filter K and the spike and the stimulus X and then the the renewal properties of this density Q will be what determined the history dependence of the model output.",
                    "label": 0
                },
                {
                    "sent": "OK, now I haven't said anything about the likelihood or the properties of the likelihood of this model, and so one of one of the sort of results of this of our NIPS paper is to prove something about that likelihood.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take you through that in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to give you a little bit of of intuition about the likelihood of the conditional.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the inhomogeneous renewal model, So what?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I described previously this process of generating spikes.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On by drawing each one from a fixed density that will give me a homogeneous renewal process.",
                    "label": 0
                },
                {
                    "sent": "How do I get a time varying renewal process with the rate parameter?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the variant time I can get that using something called the time rescaling.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transport, so suppose I have a time varying rate Lambda T that looks like this and I would like to get a spike train which has gamma intervals where the rate is being continuously varied according to that particular shape.",
                    "label": 0
                },
                {
                    "sent": "Well, the intuition behind what I want to do is to start with spike times from a homogeneous renewal process so I can just draw my spike times from the gamma distribution with a fixed fixed shape parameter, and then when I affectively what I want to do is to squish time and stretch time in proportion to the amplitude of that of that rate.",
                    "label": 0
                },
                {
                    "sent": "So when the rate is high.",
                    "label": 0
                },
                {
                    "sent": "Effectively want to take these spikes and push them together, right?",
                    "label": 0
                },
                {
                    "sent": "That'll make for shorter interspike intervals, and when the rate is low, I want to stretch those apart.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea behind the time we scaling transform mathematically, we can.",
                    "label": 0
                },
                {
                    "sent": "We can pick that.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, so if we actually apply that to these spikes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times we started with these these homogeneous pipe times.",
                    "label": 0
                },
                {
                    "sent": "We stretch time according to the rate here and we get spike times that occur when the rate is high preferentially.",
                    "label": 0
                },
                {
                    "sent": "Alright, so mathematically now we can, we can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe that transform is actually the involves the cumulative integral of this intensity function.",
                    "label": 0
                },
                {
                    "sent": "So here's the intensity on the spike rate as a function of time.",
                    "label": 0
                },
                {
                    "sent": "If we integrate Lambda T, we actually get a function here, which takes us from rescaled time on the vertical axis to real time on the horizontal axis here, and so that process that I just described could be summarized as this to draw spikes from a homogeneous renewal process, I just draw spikes from this particular gamma distribution and those intervals mu U1U2U3U four.",
                    "label": 0
                },
                {
                    "sent": "Are the intervals of the homogeneous process IMAP those through this cumulative intensity function, and then when the slope is high, the spikes get bunched up?",
                    "label": 0
                },
                {
                    "sent": "The slope is high, of course, where this rate is high and then when the slope is low where the rate of the process is low, I stretched the interspike intervals out OK, so that's the time rescaling transform and we can also use the same key rescaling transform to define the likelihood for the model.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood of course in rescaled time these each of these intervals was drawn independently from a fixed distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the definition of a renewal process, and so the likelihood over these.",
                    "label": 0
                },
                {
                    "sent": "Over these intervals in rescaled time is just the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oil density on the product of the renewal density applied to these intervals, right?",
                    "label": 0
                },
                {
                    "sent": "So if I want to map map that likelihood into real time, however, I have to do a standard change of variables and when I apply that I end up applying Q2 capital Lambda of T. So the time we scaled intervals.",
                    "label": 0
                },
                {
                    "sent": "This is the integral of the process from the previous spike time up to the next fight time inside Q times the intensity at the time of a spike.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make sense, it's done.",
                    "label": 0
                },
                {
                    "sent": "It's not important for the talk, but I wanted to give you some intuition for how we can get the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Over the spike times in real time, and now we're ready to describe it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theoretical result, which is that we can prove a theorem that this log likelihood in the filter parameter.",
                    "label": 0
                },
                {
                    "sent": "So remember that's the receptive field have been around is long concave when the renewal density is both nonincreasing and log in cave.",
                    "label": 0
                },
                {
                    "sent": "So specifically that rules out densities, where, so to say that again this this the density Q which I'm drawing my renewal process from has to be either constant or decreasing and it has to be decreasing at least as fast as either the minus X. OK, that's what this log concave requirement requires.",
                    "label": 0
                },
                {
                    "sent": "And So what this says is it rules out many of the common interspike interval densities that people would like to use to describe neurons as renewal processes.",
                    "label": 0
                },
                {
                    "sent": "Usually people settle on gamma with Kappa greater than one, so you have some regularity to the intervals or inverse Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Which is the density of 1st first passage density for integrating fire neuron or lognormal distribution which is sort of heavy tailed.",
                    "label": 0
                },
                {
                    "sent": "All of these densities have a rising phase which is which violates this requirement of not increasing this OK of course is the exponential density which is a plus on process.",
                    "label": 0
                },
                {
                    "sent": "There are many others that are also OK as long as they don't as long as they are monotonically decreasing and don't decrease at least this fast as E to the minus X. OK, so that's one result.",
                    "label": 0
                },
                {
                    "sent": "A corollary of this theorem of course is that if we want to do decoding we want to estimate the stimulus given the spike times.",
                    "label": 0
                },
                {
                    "sent": "The same requirement on the density holds such that the decoding problem is a convex problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so if Q obeys the same properties then we can estimate the map.",
                    "label": 0
                },
                {
                    "sent": "We can find the map estimate of the stimulus given the spikes.",
                    "label": 0
                },
                {
                    "sent": "Also by doing gradient descent and so the summary of this first this first.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Medical result is just to say if you were unclear about which of these models to use to capture spike dependencies between neurons, the conditional prasan model seems like a better bet.",
                    "label": 0
                },
                {
                    "sent": "Just because we have this convex loss function with no restrictions on the shape of age.",
                    "label": 1
                },
                {
                    "sent": "Here we can have refractory type affects or bursting type affects.",
                    "label": 0
                },
                {
                    "sent": "This age can be anything we want, whereas if we want to have a convex loss function here for the modulated renewal model, we have to restrict Q in a very in a very specific way that rules out refractory effects.",
                    "label": 0
                },
                {
                    "sent": "So right, if the renewal density is always decreasing then we can have an initial.",
                    "label": 0
                },
                {
                    "sent": "Where it's where there's a zero probability of having a spike.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's result one we can actually combine these models.",
                    "label": 0
                },
                {
                    "sent": "So one thing we might think about doing, why not have a renewal process?",
                    "label": 0
                },
                {
                    "sent": "An arbitrary renewal process.",
                    "label": 0
                },
                {
                    "sent": "Q&A Feedback term H in the same model like this sort of an obvious thing to do from.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mathematical standpoint, we just plug in.",
                    "label": 0
                },
                {
                    "sent": "We make a conditional intensity Lambda which depends on the spike history and the stimulus, and we have an arbitrary renewal density Q.",
                    "label": 1
                },
                {
                    "sent": "And this is now the Super set of the of the two previous models, right?",
                    "label": 0
                },
                {
                    "sent": "So it has somewhat more statistical power and more flexibility in the previous model, and in fact we can write the likelihood in a very nice way.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this conditional renewal model, I should say I'm calling it a conditional renewal model because the output is a renewal process.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, and inhomogeneous renewal process conditioned on the spike train history up to a single point.",
                    "label": 0
                },
                {
                    "sent": "So if I know this by train up till now I have a time varying rate that describes an inhomogeneous renewal process for my subsequent spikes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the nice thing about writing this model down this way is that the likelihood can be factored into two terms, one of which is the conditional Poisson likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this first term is just the likelihood that we would have obtained if we assume that Q is an exponential density.",
                    "label": 0
                },
                {
                    "sent": "So we had just across on process here and then the second term here is actually just going to pick up the slack.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of whatever was missed by my across an approximation to the to the renewal process.",
                    "label": 1
                },
                {
                    "sent": "So this is Q with the rescaled interspike intervals here, here the interspike interval rescaled to be within the unit interval, zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a copula here.",
                    "label": 1
                },
                {
                    "sent": "If you know if you know about that family of models, but it's a density that captures deviations from prasana's.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we can actually separately measure the contribution of these two terms to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have an implication for something.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call the time rescaling goodness of fit test, which is a very very cool trick for evaluating different models of neural responses, so this is introduced by Emery Brown's group in 2001.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you a rough idea behind the time we scaling goodness of fit test.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that if you have a model of the neural response, you can take the observed spike times, map them through their cumulative intensity and then the intervals on this.",
                    "label": 0
                },
                {
                    "sent": "In this rescaled time axis should should come from a single distribution and the IID distributed from that density, right?",
                    "label": 0
                },
                {
                    "sent": "So if we actually had a pass on process and we re scaled our intervals according to this cumulative intensity are histogram here should look like an exponential distribution on each of these intervals should be IID exponential and the idea behind the goodness of fit tests that they used was to say, let's compare.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quantiles of this empirical distribution for the red red dots against the expected quantiles under the dotted black line here, and the red trace here lies exactly on the diagonal, so if this is the case, statistic is the largest deviation of this red trace from the diagonal, and so you can see if things are along the diagonal, then yes, these rescaled intervals closely obey the assumptions of the model.",
                    "label": 0
                },
                {
                    "sent": "If there's a big discrepancy like this blue curve here, then then the intervals don't obey the assumption of cross on this, and so we're doing a bad job, and so I wanted to sound.",
                    "label": 0
                },
                {
                    "sent": "A note of caution about this test, which is that it can be easily fooled.",
                    "label": 0
                },
                {
                    "sent": "This test, in some sense is only sensitive to the accuracy with which you're modeling the renewal statistics of the process in rescaled time, and there's somewhat indifferent to the accuracy with which you're modeling the rate that I'm going rate parameter itself, and so I think that's easiest to demonstrate by means of a simulated toy example, so it's kind of a fake example.",
                    "label": 0
                },
                {
                    "sent": "Here's the claim spelled out.",
                    "label": 0
                },
                {
                    "sent": "If we estimate the renewal density Q and a nonparametric way, then we can trivially pass the time rescaling goodness of fit test, so we can always get our model.",
                    "label": 1
                },
                {
                    "sent": "By along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Here, OK, what is that?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Here's the toy example?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take you through.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to assume is that we have a true a true neuron, which is in fact this simulated neurons within the true neuron has a stimulus filter K, and it has a renewal process which is gamma alright.",
                    "label": 0
                },
                {
                    "sent": "So the spike trains come by convolving the stimulus with this filter and then generating spikes according to an inhomogeneous gamma process.",
                    "label": 0
                },
                {
                    "sent": "And then I'm in approximate those spike times using 2 sub optimal approximations.",
                    "label": 0
                },
                {
                    "sent": "The first one is going to be a model where I assume that the model is plus on and I've somehow estimated the stimulus filter correctly, but I've missed estimated the renewal properties, right?",
                    "label": 0
                },
                {
                    "sent": "So I've said.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume it pass on this.",
                    "label": 0
                },
                {
                    "sent": "I didn't think to try Gamma on the other model.",
                    "label": 0
                },
                {
                    "sent": "Would be to say somehow I missed up.",
                    "label": 0
                },
                {
                    "sent": "I messed up my estimate of the stimulus filter here.",
                    "label": 0
                },
                {
                    "sent": "K right.",
                    "label": 0
                },
                {
                    "sent": "So I actually have the negative inverse, the additive inverse of the true filter, but once I fix that filter that defines a mapping into rescaled time and now I can build a nonparametric estimate of the density of rescaled interspike intervals over here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the density that I would get by mapping into rescaled time and fitting so I can.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, so if I fit just histogram based estimator to this rescaled interval density, that's what I get here.",
                    "label": 0
                },
                {
                    "sent": "In this second model Q. OK so here is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Related Masters out of the three models and what you can see by eye is that the exponential model gives a does a good job of capturing the time frame rate information, whereas this sub optimal model does a very poor job.",
                    "label": 0
                },
                {
                    "sent": "However, if we compute the goodness of fit test under the KS statistic.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Red model, this one with the completely wrong rate rate variation, seems to be right along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Where's this model?",
                    "label": 0
                },
                {
                    "sent": "The process on one is badly is doing very badly, so this is all you looked at.",
                    "label": 0
                },
                {
                    "sent": "You would conclude that this model is bad in this model is good, but instead we cross validate using log likely.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that there's a lot more information about the original spike times from the blue model.",
                    "label": 0
                },
                {
                    "sent": "Then there is about this.",
                    "label": 0
                },
                {
                    "sent": "Red model is slightly negative information relative to a homogeneous closson process, so that's the.",
                    "label": 0
                },
                {
                    "sent": "Very last pieces, just to show you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into data so we can we can think actually have a 2 by 2.",
                    "label": 0
                },
                {
                    "sent": "One minute, 2 by two table here, which where we have in the upper left hand corner A is a model which is just a inhomogeneous prasan process and then we can expand that model in two ways.",
                    "label": 0
                },
                {
                    "sent": "We can either add a renewal density Q or we can add a spike history term H over here and then we have the conditional renewal bottle in this corner where we've added both a renewal density Q and this recurrent term age and we can now look at.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, the cash statistique validation.",
                    "label": 0
                },
                {
                    "sent": "The red model would seem to be doing worse.",
                    "label": 0
                },
                {
                    "sent": "It still fails at 90 at the 95% confidence interval.",
                    "label": 0
                },
                {
                    "sent": "But if we look at the changes in the log likelihood cross validation data, the increase from A to B is much smaller, so adding a renewal density Q is much smaller than if we go to from A to C. And unfortunately, if we go from C to adding both terms, see today is only very small improvement, so we're actually not gaining a lot by using this conditional renewal model, but that's something we're hoping to explore with other datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to wrap up, so that is to say that the.",
                    "label": 0
                },
                {
                    "sent": "It seems that adding a recurrent filter H is giving you more benefit than an obvious.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to take you the conclusions are we describe this new Class A new model, the conditional renewal model, which has a very nice likelihood that factorizes into two terms that have Additionally pasan model and the deviations from class.",
                    "label": 1
                },
                {
                    "sent": "On this we have a restrictive condition on Q such that the estimation problem is still convex.",
                    "label": 0
                },
                {
                    "sent": "The model when it's when Q is estimated nonparametrically, trivially passes the time rescaling goodness of fit test, but unfortunately in data it doesn't seem to be significantly better than the GLM conditionally plus on model.",
                    "label": 0
                },
                {
                    "sent": "So I just like to thanks for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and thanks to my data collaborators here, who provided the retinal data in that last example, thanks.",
                    "label": 0
                },
                {
                    "sent": "So we have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Are there any?",
                    "label": 0
                },
                {
                    "sent": "Page on so you pointed out for the renewal process model that there was non convexity under certain situations.",
                    "label": 0
                },
                {
                    "sent": "That's an interesting result, but one of the kind of themes of NIPS is to fearlessly go in the face of non convexity.",
                    "label": 0
                },
                {
                    "sent": "So how much of a problem really is that in practice?",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Can you show how difficult?",
                    "label": 0
                },
                {
                    "sent": "How much the problem that causes?",
                    "label": 0
                },
                {
                    "sent": "That's a great.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question what I would say is, although it's it's not probably convex, there could still be.",
                    "label": 0
                },
                {
                    "sent": "We don't know for certain that if Q doesn't have those those properties that there are multiple local Maxima.",
                    "label": 0
                },
                {
                    "sent": "So we can just fearlessly go ahead and maximize, but we won't know for sure that we have the global optimum.",
                    "label": 0
                },
                {
                    "sent": "The I guess the prescriptive approach that I would suggest taking is to start with the condition we pass on model, so this since this likelihood term factorizes, we can be guaranteed that we have an optimal description under the under the Cross on Model, and then anything leftover is gravy.",
                    "label": 0
                },
                {
                    "sent": "We can always improve.",
                    "label": 0
                },
                {
                    "sent": "Basically if we look at the KS statistic after fitting this model.",
                    "label": 0
                },
                {
                    "sent": "And there's a big discrepancy between the expected those things like power off the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Then we can introduce Q and continue to ascend, so we get we get a better model.",
                    "label": 0
                },
                {
                    "sent": "We don't know if there's a better global maximum somewhere else, but we're still we're still doing better than this.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as a convex relaxation, I guess of the full problem, and in fact this density that I used here I didn't impose any restriction.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the real data case, yeah, in this model here I didn't impose any kind of restriction on Q, so you can just just.",
                    "label": 0
                },
                {
                    "sent": "Estimated using using.",
                    "label": 0
                },
                {
                    "sent": "Hope that you get the maximum.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually a second question.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "I mean, can one do inference easily if you have, for instance, a network of these models?",
                    "label": 0
                },
                {
                    "sent": "Is that possible?",
                    "label": 0
                },
                {
                    "sent": "So, so that's another case where the this conditional renewal model doesn't actually buy you that much over the so.",
                    "label": 0
                },
                {
                    "sent": "So so you're asking about about network interactions, right?",
                    "label": 0
                },
                {
                    "sent": "You have multiple such so.",
                    "label": 0
                },
                {
                    "sent": "So one nice thing about this.",
                    "label": 0
                },
                {
                    "sent": "This model with the spike history terms that we can also add in Spike history terms that couple one neuron.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the spike trains of despite another neuron so that if one neuron spikes, I have a subsequent influence on the probability.",
                    "label": 0
                },
                {
                    "sent": "Speaking of another neuron.",
                    "label": 0
                },
                {
                    "sent": "And so this GLM framework is very flexible and that it scales.",
                    "label": 0
                },
                {
                    "sent": "It allows you to incorporate network type affects this.",
                    "label": 0
                },
                {
                    "sent": "This renewal process model is really only about how the next bike determined depends on the previous spike from that same neuron.",
                    "label": 0
                },
                {
                    "sent": "And I don't know there might be some interesting way to think about coupling neurons in rescaled time.",
                    "label": 0
                },
                {
                    "sent": "So I suppose that would be what you would you would end up if you try doing network effects.",
                    "label": 0
                },
                {
                    "sent": "But based on the application today to, I'd say that's.",
                    "label": 0
                },
                {
                    "sent": "But we aren't getting very big improvement.",
                    "label": 0
                },
                {
                    "sent": "When we when we try to add Q to our model already.",
                    "label": 0
                },
                {
                    "sent": "But it would be something.",
                    "label": 0
                },
                {
                    "sent": "Certainly try to write down.",
                    "label": 0
                },
                {
                    "sent": "I think it would be interesting to know.",
                    "label": 0
                },
                {
                    "sent": "So if there are no more questions, then let's thank Jonathan again.",
                    "label": 0
                }
            ]
        }
    }
}