{
    "id": "b4zk3zfs42qxoh7glmnmstvnuv37a42r",
    "title": "Exploration Scavenging",
    "info": {
        "author": [
            "Alexander Strehl, Facebook"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_exs_strehl/",
    "segmentation": [
        [
            "Hello, I'm Alex troan.",
            "Today I'm going to talk about exploration, scavenging and this is joint work with John Langford and Jenn Wortman."
        ],
        [
            "So I work at Yahoo and Yahoo.",
            "We care a lot about choosing good ads as well as choosing good search results.",
            "The basic problem that I'm going to discuss today is.",
            "How do you find a better policy for choosing ads given the huge amount of logged data that we have at Yahoo?",
            "So in particular, while Yahoo's policy might not be explicitly randomizing overtime, it does change.",
            "So for example, maybe during one month, when I search for extra dark chocolate, 'cause you know you're going to search for something, it should be chocolate, then you might get some result like this, and then say another month you might get another result like this.",
            "And the reason that it changes it just because new technologies come out.",
            "New ideas are tried and so on and so forth.",
            "So the idea is, can we use this natural variety in the data to somehow learn which ads are better using a machine learning based approach and perhaps find a better policy?",
            "Given this old data, so that's the basic problem that we're trying to solve today."
        ],
        [
            "So now try to formalize that a bit more.",
            "On the setting is that during each stage, or during each trial, an input X is provided from some fixed underlying distribution in the application.",
            "You can think of this as some query that the user types in.",
            "Then the algorithm itself must choose an action which you can think of as an ad or even a set of ads, and then the payoff usually is zero or one.",
            "It's a pleasure signal that you're trying to maximize, and it's generated from an unknown and noisy function of only the input and the action tried.",
            "In our case, we can think of it as being a one.",
            "If the user clicks on it in a 0.",
            "Otherwise because when the user clicks on it, we get paid.",
            "So one thing I should mention is that you don't receive a reward for any of the actions that you didn't try.",
            "This is going to be important.",
            "The problem that we're looking at is the so called offline policy evaluation.",
            "That is, suppose that we've generated some data from an old policy, or equivalently, you have Oracle access to the old policy.",
            "So you have examples where you've seen an input.",
            "You've seen an action chosen, and you've seen a reward.",
            "And that the action was chosen according to some policy currently in place.",
            "And the question is, how do we evaluate a new policy?",
            "Generally we think of deterministic policies using this old data, so without actually implementing it and running it online.",
            "And we say evaluate, we mean compute its value, which is in this case the probability of click an.",
            "In general the expected reward.",
            "So although I've stated this in terms of web search, an advertising, it's actually much more general than that, and it could have potential applications to say medical treatments where the input is patient data and the actions are different types of treatments or robotics, etc.",
            "So in general, anytime we have a lot of historical data that we'd like to take advantage of.",
            "And one more thing, before I go on is.",
            "I say that we're going to evaluate a new policy, but you can clearly see that this leads to solving the problem of finding a better policy, because if we have a class of policy's.",
            "That we're considering, we could simply evaluate each one in the class and then pick the one that has the maximum value.",
            "Yes.",
            "Are expected reward.",
            "One step yes, because we assume that the input is chosen from a fixed distribution that doesn't change based on your action."
        ],
        [
            "One approach that you might try is important waiting and.",
            "Importance weighting is has been used for this before.",
            "It assumes that the current policy is explicitly randomized.",
            "So I'll try to explain that really quickly here.",
            "So what you do is you want to evaluate a new policy given data from an old policy.",
            "So you generate a tuple which is an input X and action A and then a reward using the old policy and then you re weight the reward using this formula.",
            "So you multiply the reward by an indicator function of whether the new policy that you're trying to evaluate would have chosen the action that your old policy evaluated.",
            "If it wouldn't choose the same action, then you can't really use this information in our setting in the importance weighting setting and you divide by the probability that the old policy.",
            "Chose would have chosen this action.",
            "So this is just the very old technique.",
            "It's called important sampling and the value of the new policy is equal to the expected value of this quantity.",
            "The problem that we have with applying this approach in our case is that.",
            "The login policy is that the old policy that Yahoo is currently using is not randomized.",
            "And even if it were randomized, we wouldn't know these probabilities anyway, but the point is that it really is not randomized.",
            "So the question is, what can we do in this case?"
        ],
        [
            "Quick outline of the talk.",
            "So first I'll present a some theoretical theoretical analysis.",
            "I'll provide a way to do policy estimation.",
            "In this case will provide an estimator and our main result in the main result will show that under certain assumptions the estimator will converge rapidly to the value of the new policy.",
            "These assumptions are going to sound very strong and you might actually want to get up and leave the room once you hear the assumption, but I'd like you to stay here.",
            "The next thing I'm going to do is explain how this can be.",
            "Applied to a realistic set in and in some strict form away solve the problem from the 1st first page.",
            "And I'll talk quickly about an impossibility result, which implies that we really need this strong assumption as well.",
            "Finally, the second part we're going to apply this to some data that we acquired from Yahoo in the web advertising problem, and it turns out that there is actually a number of issues that came up when we actually tried to work with real data, so we'll talk about that and we'll have some experiments."
        ],
        [
            "So here's the estimator.",
            "It's very similar to important sampling.",
            "Basically we take our data that we've acquired from the old policy.",
            "Again, it consists of inputs or queries.",
            "Actions are ads and then rewards, which is click information.",
            "And then we.",
            "Evaluate a new policy 8 using this formula H according to this formula, which is just the average of these quantities here, which, like important sampling, it was indicator of whether the old the new policy would have chosen the same action as the old policy.",
            "Chose conditioned on the current input, the reward that was received.",
            "This is just R, so T appear and then divided by the number of times the action was chosen across the whole entire.",
            "Data set, so you can think of this as really approximating importance.",
            "Weighted sampling.",
            "And the theorem is it says that for any policy H and Delta in zero on if each action a sub T in the old and the data is chosen independently of the input X.",
            "So here's the strong assumption.",
            "So action a city is not chosen independently of the current input.",
            "Then with probability high probability, our estimate won't deviate too much from its expected value, which is the value of the new policy by this quantity here.",
            "So I'll try to go quickly through this.",
            "Terms that the sum over all the actions an it's inversely proportional to the square root of the number of times we've tried the action, and it has some union bound terms here.",
            "So the number K is the number of arms.",
            "T is a number of time steps.",
            "So basically if we don't try.",
            "If there's an action that we haven't tried very much, the bound is bad.",
            "But if we try all the actions, then it's it rapidly converges to 0, yes?",
            "These are.",
            "Consider your policy to be just randomize the world, the actions over all the States and so these fractions are essentially estimate of the true probabilities so well.",
            "In some sense, the right intuition and basically we validated that intuition, but it's not as obvious as you might think, because the policy here that chooses these could actually be changing on each step based on all the previous data.",
            "So maybe we saw the 1st three and then we decided to change our policy based on that going forward.",
            "And so it's not exactly clear that you can assume that that's somehow based off of a randomized policy.",
            "It really is deterministic, yes, Java.",
            "Right?",
            "Probably let's just think about it a little bit.",
            "OK."
        ],
        [
            "So now I'm going to try to explain how this actually solves the problem from the first page and then very strictly.",
            "So like I said, the current policy that Yahoo uses or you would expect almost any other company changes overtime so the new technologies come out and new algorithms that are implemented.",
            "And maybe there's some you know just trying out of different techniques to see how they work.",
            "So we can denote this set of historical policies by capital \u03c0.",
            "So you say capital Pi is equal to.",
            "This sets the historical policies that we've tried in the past.",
            "And now what we do is we re define the action space to be pie and we evaluate policies of the Form H which map inputs 2\u03c0.",
            "So we say here is that we notice that these changes that occur overtime occur because of external factors there wasn't.",
            "Do any of the inputs and what we'd like to do is find a new policy that looks at the context and picks the best historical policy.",
            "So this is so in some sense this is going to find a much better policy if you expect these historical policies that sometimes work, or other ones might not work.",
            "So it's a context dependent policy that we're learning.",
            "So this clear.",
            "Is there any questions about this?",
            "'cause this is kind of pretty important.",
            "And of course, it's straightforward to show, just apply the previous theorem that.",
            "With high probability, our estimate or estimator for a new policy of this form.",
            "Is very close to its expected value as long as we've tried each action enough times."
        ],
        [
            "OK, so now I'll try to also justify the assumption even more and the.",
            "Their claim here is that the previous theorem that we showed before is false.",
            "If you allow the actions that depend on input.",
            "What I mean by that is if you get rid of this assumption that the actions do not depend on input, there is absolutely no estimator.",
            "No way to solve the bounds that we had.",
            "So the specific counterexample for that is right here.",
            "It's very simple and obvious.",
            "You have two inputs, 0112 actions, zero on one and our policy.",
            "Our old policy is deterministic, it only chooses action zero on input zero and action one input one, and the new policy that we would like to evaluate does the opposite.",
            "It chooses Action Zero on input one and action one input zero, and the claim, which seems pretty clear, is that you can't evaluate this new policy.",
            "From data over this old policy.",
            "Basically, no matter what you do, you're not going to be able to evaluate this new policy, and the only assumption that we illuminated was the assumption that actions are allowed to depend on input.",
            "Unless the rewards.",
            "Sure.",
            "Yeah, you could make other assumptions an maybe.",
            "Develop techniques for those studied.",
            "The chocolate search engine is always returns chocolate.",
            "I don't know.",
            "I think you should send me a link to that one."
        ],
        [
            "OK, so just quickly talk about how we're getting around this fact that the old policy is not stochastic.",
            "So pretend like the old policy cycles through the action, so if it chooses A1A2A3A1A2A3, it's not really choosing these stochastically.",
            "And so we can't use important sampling in this case, but I guess that will result says, well, do I know was pointing out, as you can really view this, as you can pretend that this is stochastic, an apply important sampling, and the reason that you can do this is fundamentally depends on a fixed relationship between the input and the reward.",
            "That is, the reward only depends on the current input in their current action.",
            "If it wasn't that case, then we wouldn't be able to do this because then an adversary could mess up our rewards.",
            "And bias our samples."
        ],
        [
            "Alright, so we wanted to apply this to some data at Yahoo, but it turned out that that was a little tricky because now instead of a single ad we have a slate of ads and slate of ads means that we had an exponential explosion of ads of actions and so we can't have a problem with using the previous estimator in one case, and other cases that we also had this assumption that the.",
            "Current policy doesn't depend on the input, and so when we're thinking of actions as ads and not historical policies, we also couldn't apply the previous work.",
            "So we kind of developed completely new technique for application to web advertising that's inspired by the previous work in the previous section.",
            "But it is a little bit different, so in some sense are gaining bonus here you're getting like a second paper."
        ],
        [
            "So the Internet advertising application, the input is the web page.",
            "The action is the slate of advertisements shown, so it's a set of advertisements and their reward is.",
            "Perhaps the number of clicks that you received or say the revenue that you get from that sleep.",
            "And like I said, a large number of actions our estimator would, we could apply that, but the accuracy would be very poor.",
            "So we need a way to deal with this."
        ],
        [
            "It's.",
            "We take is a approach that's taken in the past, which is to assume a factoring assumption.",
            "So factorization says that the probability that we receive a click for fixed page or this is the input here and add an.",
            "In position I.",
            "So it's like the if I is equal to three.",
            "It's like the ad that was in the third slot.",
            "So the probability for a click is equal to some position dependent constant.",
            "See survive times an intrinsic probability of click for the ad on the page.",
            "Maybe I might have confused here.",
            "I switched from using a query to page here, so in this application we actually have web pages and we put ads next to the web page.",
            "You could just think of this as the query if you want.",
            "So what this allows us to do is that we can first actually estimate what these position dependent.",
            "Factors are and.",
            "Then we could use those to try to evaluate new policies.",
            "So we did as we developed a particular way to estimate these factors that are particularly robust to low probability events.",
            "So the obvious way that you would think of estimating these factors is simply compute the expected number of clicks that you receive for.",
            "All ads across, say the first position and then also compute the expected number of clicks you would get from the seconds second position and then take the ratio of those two.",
            "That would be kind of the naive approach to this, and it turns out that that's very very bad because generally search engines put better ads at the top, so that puts a bias on these factors.",
            "So it is.",
            "We have a new way to do this and.",
            "Using these factors, we actually use ideas inspired by the previous section to evaluate policies that reorder the slate chosen by the current system, so it doesn't actually choose an entirely new slate.",
            "It first calls the old system, and then it reorders those in somewhere in.",
            "The reordering thing is important to get around this assumption that the old policy dependent depended on the input.",
            "So with this we don't have that assumption anymore."
        ],
        [
            "So first I'll talk about these coefficients that we computed so.",
            "What I have here is I'm applying the position and the actual coefficient so they are decreasing, which means that.",
            "People tend to click on the higher higher ads for more often.",
            "And the naive approach is very is.",
            "Are very low compared to the new method which we have and I'll have time to go into the details of the new method, but please ask him about it.",
            "The poster session.",
            "What we see here is that there are low because they're biased here because it's the case that.",
            "In Yahoo, we actually do put better ads, higher positions, so if you estimate it in the naive way, you get a low estimate for these and I also put down DCG, which is actually another common method that people use to estimate these, and it turns out that it's just equal to the law one over the log of the position plus one, so it really has no justification.",
            "But this is in some sense of justification for those numbers because our numbers actually get very very close to those, so I thought that was interesting."
        ],
        [
            "And finally we use these to evaluate some based policy's on the on the log data.",
            "So again, we were able to evaluate these policies without actually running them, which is the first one that we tried is the best possible feature independent reordering.",
            "So as we took large entire set of ads, and we globally ordered them by their expected number of clicks across all possible pages, and then again this is a reordering policy, So what it does is.",
            "It calls given a query or given a page, it calls the current system to produce a slate, and then it reorders them based on this global ordering of the ads, and we concluded that this would probably result in an 8% approximately 8% increase in the number of clicks.",
            "We also looked at the best possible feature dependent reordering, which is a theoretical thing that you can't.",
            "You can't compute this just to compare these numbers.",
            "This is the maximum amount that you could increase it according to our metric.",
            "And finally we looked at two algorithms based on the machine learn scoring rule.",
            "So we used machine learning to try to order the ads and these are dependent on on the page.",
            "Look at the maximum entry approach and a stochastic gradient descent and they both were fairly good at increasing the number of clicks when you use them to reorder the slate of ads, and so we get 20% and 21%."
        ],
        [
            "To conclude, we provided a new method for offline policy evaluation.",
            "That is the first one that we know of that deals with a huge number of features, so that's not in the table based approach and deals with deterministic initial policy that has no explicit randomization in it.",
            "With additional assumptions, we can show that it provides good estimates of reordering policies and online advertising."
        ],
        [
            "That's all I have here.",
            "Thank you very much.",
            "So how did you?",
            "Hug outline.",
            "Miss you.",
            "How did you compute the performance?",
            "OK, So what you do is you."
        ],
        [
            "So you can.",
            "First you have to estimate these."
        ],
        [
            "These factors.",
            "OK, now."
        ],
        [
            "To do as you see the office and produce a slate of ads, and one of them were clicked.",
            "And the new policy reorders those ads.",
            "So you look at the new position that the ad isn't in the old position, you take a ratio of the respective.",
            "Estimated coefficients which gives you an expected value of how much more probability of click would you have gotten from moving that add up or down.",
            "And then you add that and you average that across the entire data set.",
            "So the reason that this gets around the assumption that the new policy issues in action is different from the old policies.",
            "In some sense it's not.",
            "It's using the same sleep, so.",
            "It is actually OK, yeah, if that's what your question was, I didn't understand it correctly.",
            "At this point we can't.",
            "We can't be sure that that's accurate.",
            "The only assumption that we used was when we estimated these coefficients, and there's there's assumption there which is weaker than the previous assumption, but it's something on the lines of, like the previous assumption, that the actions don't depend on the input would imply this thing.",
            "It's weaker than that.",
            "It probably doesn't hold, so we can't guarantee that we estimated these right.",
            "If you estimate these right then we can prove that our new estimator, using just hope things bound, will converge to.",
            "The right answer, but since we don't have these exactly, we can't be sure, but we do have a pretty."
        ],
        [
            "Strong, especially from this graph."
        ],
        [
            "Very strong indication that these are probably very, very accurate.",
            "Can you comment on how?",
            "The case with some other words as opposed to just one step forward.",
            "So you're saying that the sort of reinforcement learning case where the input on the next step would OK?",
            "Depends on the current action.",
            "Someone thought about that too much.",
            "You go ahead, John.",
            "Entirely from a single.",
            "Once every first step before starting, you can always replacing over vaccines with Norvasc to the timer.",
            "But there is one thing you can do.",
            "Given these actions, it's just thinking."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, I'm Alex troan.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about exploration, scavenging and this is joint work with John Langford and Jenn Wortman.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I work at Yahoo and Yahoo.",
                    "label": 0
                },
                {
                    "sent": "We care a lot about choosing good ads as well as choosing good search results.",
                    "label": 0
                },
                {
                    "sent": "The basic problem that I'm going to discuss today is.",
                    "label": 0
                },
                {
                    "sent": "How do you find a better policy for choosing ads given the huge amount of logged data that we have at Yahoo?",
                    "label": 0
                },
                {
                    "sent": "So in particular, while Yahoo's policy might not be explicitly randomizing overtime, it does change.",
                    "label": 0
                },
                {
                    "sent": "So for example, maybe during one month, when I search for extra dark chocolate, 'cause you know you're going to search for something, it should be chocolate, then you might get some result like this, and then say another month you might get another result like this.",
                    "label": 0
                },
                {
                    "sent": "And the reason that it changes it just because new technologies come out.",
                    "label": 0
                },
                {
                    "sent": "New ideas are tried and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So the idea is, can we use this natural variety in the data to somehow learn which ads are better using a machine learning based approach and perhaps find a better policy?",
                    "label": 0
                },
                {
                    "sent": "Given this old data, so that's the basic problem that we're trying to solve today.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now try to formalize that a bit more.",
                    "label": 0
                },
                {
                    "sent": "On the setting is that during each stage, or during each trial, an input X is provided from some fixed underlying distribution in the application.",
                    "label": 1
                },
                {
                    "sent": "You can think of this as some query that the user types in.",
                    "label": 1
                },
                {
                    "sent": "Then the algorithm itself must choose an action which you can think of as an ad or even a set of ads, and then the payoff usually is zero or one.",
                    "label": 0
                },
                {
                    "sent": "It's a pleasure signal that you're trying to maximize, and it's generated from an unknown and noisy function of only the input and the action tried.",
                    "label": 1
                },
                {
                    "sent": "In our case, we can think of it as being a one.",
                    "label": 1
                },
                {
                    "sent": "If the user clicks on it in a 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise because when the user clicks on it, we get paid.",
                    "label": 0
                },
                {
                    "sent": "So one thing I should mention is that you don't receive a reward for any of the actions that you didn't try.",
                    "label": 0
                },
                {
                    "sent": "This is going to be important.",
                    "label": 1
                },
                {
                    "sent": "The problem that we're looking at is the so called offline policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "That is, suppose that we've generated some data from an old policy, or equivalently, you have Oracle access to the old policy.",
                    "label": 0
                },
                {
                    "sent": "So you have examples where you've seen an input.",
                    "label": 0
                },
                {
                    "sent": "You've seen an action chosen, and you've seen a reward.",
                    "label": 1
                },
                {
                    "sent": "And that the action was chosen according to some policy currently in place.",
                    "label": 1
                },
                {
                    "sent": "And the question is, how do we evaluate a new policy?",
                    "label": 0
                },
                {
                    "sent": "Generally we think of deterministic policies using this old data, so without actually implementing it and running it online.",
                    "label": 0
                },
                {
                    "sent": "And we say evaluate, we mean compute its value, which is in this case the probability of click an.",
                    "label": 0
                },
                {
                    "sent": "In general the expected reward.",
                    "label": 0
                },
                {
                    "sent": "So although I've stated this in terms of web search, an advertising, it's actually much more general than that, and it could have potential applications to say medical treatments where the input is patient data and the actions are different types of treatments or robotics, etc.",
                    "label": 0
                },
                {
                    "sent": "So in general, anytime we have a lot of historical data that we'd like to take advantage of.",
                    "label": 0
                },
                {
                    "sent": "And one more thing, before I go on is.",
                    "label": 0
                },
                {
                    "sent": "I say that we're going to evaluate a new policy, but you can clearly see that this leads to solving the problem of finding a better policy, because if we have a class of policy's.",
                    "label": 1
                },
                {
                    "sent": "That we're considering, we could simply evaluate each one in the class and then pick the one that has the maximum value.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Are expected reward.",
                    "label": 0
                },
                {
                    "sent": "One step yes, because we assume that the input is chosen from a fixed distribution that doesn't change based on your action.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One approach that you might try is important waiting and.",
                    "label": 0
                },
                {
                    "sent": "Importance weighting is has been used for this before.",
                    "label": 0
                },
                {
                    "sent": "It assumes that the current policy is explicitly randomized.",
                    "label": 1
                },
                {
                    "sent": "So I'll try to explain that really quickly here.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you want to evaluate a new policy given data from an old policy.",
                    "label": 1
                },
                {
                    "sent": "So you generate a tuple which is an input X and action A and then a reward using the old policy and then you re weight the reward using this formula.",
                    "label": 1
                },
                {
                    "sent": "So you multiply the reward by an indicator function of whether the new policy that you're trying to evaluate would have chosen the action that your old policy evaluated.",
                    "label": 0
                },
                {
                    "sent": "If it wouldn't choose the same action, then you can't really use this information in our setting in the importance weighting setting and you divide by the probability that the old policy.",
                    "label": 1
                },
                {
                    "sent": "Chose would have chosen this action.",
                    "label": 1
                },
                {
                    "sent": "So this is just the very old technique.",
                    "label": 0
                },
                {
                    "sent": "It's called important sampling and the value of the new policy is equal to the expected value of this quantity.",
                    "label": 0
                },
                {
                    "sent": "The problem that we have with applying this approach in our case is that.",
                    "label": 0
                },
                {
                    "sent": "The login policy is that the old policy that Yahoo is currently using is not randomized.",
                    "label": 1
                },
                {
                    "sent": "And even if it were randomized, we wouldn't know these probabilities anyway, but the point is that it really is not randomized.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what can we do in this case?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quick outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "So first I'll present a some theoretical theoretical analysis.",
                    "label": 0
                },
                {
                    "sent": "I'll provide a way to do policy estimation.",
                    "label": 0
                },
                {
                    "sent": "In this case will provide an estimator and our main result in the main result will show that under certain assumptions the estimator will converge rapidly to the value of the new policy.",
                    "label": 0
                },
                {
                    "sent": "These assumptions are going to sound very strong and you might actually want to get up and leave the room once you hear the assumption, but I'd like you to stay here.",
                    "label": 0
                },
                {
                    "sent": "The next thing I'm going to do is explain how this can be.",
                    "label": 0
                },
                {
                    "sent": "Applied to a realistic set in and in some strict form away solve the problem from the 1st first page.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk quickly about an impossibility result, which implies that we really need this strong assumption as well.",
                    "label": 0
                },
                {
                    "sent": "Finally, the second part we're going to apply this to some data that we acquired from Yahoo in the web advertising problem, and it turns out that there is actually a number of issues that came up when we actually tried to work with real data, so we'll talk about that and we'll have some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the estimator.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to important sampling.",
                    "label": 0
                },
                {
                    "sent": "Basically we take our data that we've acquired from the old policy.",
                    "label": 1
                },
                {
                    "sent": "Again, it consists of inputs or queries.",
                    "label": 0
                },
                {
                    "sent": "Actions are ads and then rewards, which is click information.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                },
                {
                    "sent": "Evaluate a new policy 8 using this formula H according to this formula, which is just the average of these quantities here, which, like important sampling, it was indicator of whether the old the new policy would have chosen the same action as the old policy.",
                    "label": 1
                },
                {
                    "sent": "Chose conditioned on the current input, the reward that was received.",
                    "label": 0
                },
                {
                    "sent": "This is just R, so T appear and then divided by the number of times the action was chosen across the whole entire.",
                    "label": 0
                },
                {
                    "sent": "Data set, so you can think of this as really approximating importance.",
                    "label": 0
                },
                {
                    "sent": "Weighted sampling.",
                    "label": 1
                },
                {
                    "sent": "And the theorem is it says that for any policy H and Delta in zero on if each action a sub T in the old and the data is chosen independently of the input X.",
                    "label": 0
                },
                {
                    "sent": "So here's the strong assumption.",
                    "label": 1
                },
                {
                    "sent": "So action a city is not chosen independently of the current input.",
                    "label": 0
                },
                {
                    "sent": "Then with probability high probability, our estimate won't deviate too much from its expected value, which is the value of the new policy by this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So I'll try to go quickly through this.",
                    "label": 0
                },
                {
                    "sent": "Terms that the sum over all the actions an it's inversely proportional to the square root of the number of times we've tried the action, and it has some union bound terms here.",
                    "label": 0
                },
                {
                    "sent": "So the number K is the number of arms.",
                    "label": 0
                },
                {
                    "sent": "T is a number of time steps.",
                    "label": 1
                },
                {
                    "sent": "So basically if we don't try.",
                    "label": 1
                },
                {
                    "sent": "If there's an action that we haven't tried very much, the bound is bad.",
                    "label": 0
                },
                {
                    "sent": "But if we try all the actions, then it's it rapidly converges to 0, yes?",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 1
                },
                {
                    "sent": "Consider your policy to be just randomize the world, the actions over all the States and so these fractions are essentially estimate of the true probabilities so well.",
                    "label": 0
                },
                {
                    "sent": "In some sense, the right intuition and basically we validated that intuition, but it's not as obvious as you might think, because the policy here that chooses these could actually be changing on each step based on all the previous data.",
                    "label": 0
                },
                {
                    "sent": "So maybe we saw the 1st three and then we decided to change our policy based on that going forward.",
                    "label": 0
                },
                {
                    "sent": "And so it's not exactly clear that you can assume that that's somehow based off of a randomized policy.",
                    "label": 0
                },
                {
                    "sent": "It really is deterministic, yes, Java.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Probably let's just think about it a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to try to explain how this actually solves the problem from the first page and then very strictly.",
                    "label": 0
                },
                {
                    "sent": "So like I said, the current policy that Yahoo uses or you would expect almost any other company changes overtime so the new technologies come out and new algorithms that are implemented.",
                    "label": 1
                },
                {
                    "sent": "And maybe there's some you know just trying out of different techniques to see how they work.",
                    "label": 0
                },
                {
                    "sent": "So we can denote this set of historical policies by capital \u03c0.",
                    "label": 1
                },
                {
                    "sent": "So you say capital Pi is equal to.",
                    "label": 0
                },
                {
                    "sent": "This sets the historical policies that we've tried in the past.",
                    "label": 1
                },
                {
                    "sent": "And now what we do is we re define the action space to be pie and we evaluate policies of the Form H which map inputs 2\u03c0.",
                    "label": 1
                },
                {
                    "sent": "So we say here is that we notice that these changes that occur overtime occur because of external factors there wasn't.",
                    "label": 1
                },
                {
                    "sent": "Do any of the inputs and what we'd like to do is find a new policy that looks at the context and picks the best historical policy.",
                    "label": 0
                },
                {
                    "sent": "So this is so in some sense this is going to find a much better policy if you expect these historical policies that sometimes work, or other ones might not work.",
                    "label": 0
                },
                {
                    "sent": "So it's a context dependent policy that we're learning.",
                    "label": 0
                },
                {
                    "sent": "So this clear.",
                    "label": 1
                },
                {
                    "sent": "Is there any questions about this?",
                    "label": 0
                },
                {
                    "sent": "'cause this is kind of pretty important.",
                    "label": 0
                },
                {
                    "sent": "And of course, it's straightforward to show, just apply the previous theorem that.",
                    "label": 1
                },
                {
                    "sent": "With high probability, our estimate or estimator for a new policy of this form.",
                    "label": 0
                },
                {
                    "sent": "Is very close to its expected value as long as we've tried each action enough times.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'll try to also justify the assumption even more and the.",
                    "label": 0
                },
                {
                    "sent": "Their claim here is that the previous theorem that we showed before is false.",
                    "label": 0
                },
                {
                    "sent": "If you allow the actions that depend on input.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is if you get rid of this assumption that the actions do not depend on input, there is absolutely no estimator.",
                    "label": 0
                },
                {
                    "sent": "No way to solve the bounds that we had.",
                    "label": 0
                },
                {
                    "sent": "So the specific counterexample for that is right here.",
                    "label": 0
                },
                {
                    "sent": "It's very simple and obvious.",
                    "label": 0
                },
                {
                    "sent": "You have two inputs, 0112 actions, zero on one and our policy.",
                    "label": 0
                },
                {
                    "sent": "Our old policy is deterministic, it only chooses action zero on input zero and action one input one, and the new policy that we would like to evaluate does the opposite.",
                    "label": 1
                },
                {
                    "sent": "It chooses Action Zero on input one and action one input zero, and the claim, which seems pretty clear, is that you can't evaluate this new policy.",
                    "label": 1
                },
                {
                    "sent": "From data over this old policy.",
                    "label": 1
                },
                {
                    "sent": "Basically, no matter what you do, you're not going to be able to evaluate this new policy, and the only assumption that we illuminated was the assumption that actions are allowed to depend on input.",
                    "label": 0
                },
                {
                    "sent": "Unless the rewards.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could make other assumptions an maybe.",
                    "label": 0
                },
                {
                    "sent": "Develop techniques for those studied.",
                    "label": 0
                },
                {
                    "sent": "The chocolate search engine is always returns chocolate.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think you should send me a link to that one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just quickly talk about how we're getting around this fact that the old policy is not stochastic.",
                    "label": 0
                },
                {
                    "sent": "So pretend like the old policy cycles through the action, so if it chooses A1A2A3A1A2A3, it's not really choosing these stochastically.",
                    "label": 0
                },
                {
                    "sent": "And so we can't use important sampling in this case, but I guess that will result says, well, do I know was pointing out, as you can really view this, as you can pretend that this is stochastic, an apply important sampling, and the reason that you can do this is fundamentally depends on a fixed relationship between the input and the reward.",
                    "label": 0
                },
                {
                    "sent": "That is, the reward only depends on the current input in their current action.",
                    "label": 0
                },
                {
                    "sent": "If it wasn't that case, then we wouldn't be able to do this because then an adversary could mess up our rewards.",
                    "label": 0
                },
                {
                    "sent": "And bias our samples.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we wanted to apply this to some data at Yahoo, but it turned out that that was a little tricky because now instead of a single ad we have a slate of ads and slate of ads means that we had an exponential explosion of ads of actions and so we can't have a problem with using the previous estimator in one case, and other cases that we also had this assumption that the.",
                    "label": 0
                },
                {
                    "sent": "Current policy doesn't depend on the input, and so when we're thinking of actions as ads and not historical policies, we also couldn't apply the previous work.",
                    "label": 0
                },
                {
                    "sent": "So we kind of developed completely new technique for application to web advertising that's inspired by the previous work in the previous section.",
                    "label": 0
                },
                {
                    "sent": "But it is a little bit different, so in some sense are gaining bonus here you're getting like a second paper.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Internet advertising application, the input is the web page.",
                    "label": 0
                },
                {
                    "sent": "The action is the slate of advertisements shown, so it's a set of advertisements and their reward is.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the number of clicks that you received or say the revenue that you get from that sleep.",
                    "label": 0
                },
                {
                    "sent": "And like I said, a large number of actions our estimator would, we could apply that, but the accuracy would be very poor.",
                    "label": 0
                },
                {
                    "sent": "So we need a way to deal with this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "We take is a approach that's taken in the past, which is to assume a factoring assumption.",
                    "label": 0
                },
                {
                    "sent": "So factorization says that the probability that we receive a click for fixed page or this is the input here and add an.",
                    "label": 0
                },
                {
                    "sent": "In position I.",
                    "label": 0
                },
                {
                    "sent": "So it's like the if I is equal to three.",
                    "label": 0
                },
                {
                    "sent": "It's like the ad that was in the third slot.",
                    "label": 0
                },
                {
                    "sent": "So the probability for a click is equal to some position dependent constant.",
                    "label": 0
                },
                {
                    "sent": "See survive times an intrinsic probability of click for the ad on the page.",
                    "label": 0
                },
                {
                    "sent": "Maybe I might have confused here.",
                    "label": 0
                },
                {
                    "sent": "I switched from using a query to page here, so in this application we actually have web pages and we put ads next to the web page.",
                    "label": 0
                },
                {
                    "sent": "You could just think of this as the query if you want.",
                    "label": 0
                },
                {
                    "sent": "So what this allows us to do is that we can first actually estimate what these position dependent.",
                    "label": 0
                },
                {
                    "sent": "Factors are and.",
                    "label": 0
                },
                {
                    "sent": "Then we could use those to try to evaluate new policies.",
                    "label": 0
                },
                {
                    "sent": "So we did as we developed a particular way to estimate these factors that are particularly robust to low probability events.",
                    "label": 0
                },
                {
                    "sent": "So the obvious way that you would think of estimating these factors is simply compute the expected number of clicks that you receive for.",
                    "label": 0
                },
                {
                    "sent": "All ads across, say the first position and then also compute the expected number of clicks you would get from the seconds second position and then take the ratio of those two.",
                    "label": 0
                },
                {
                    "sent": "That would be kind of the naive approach to this, and it turns out that that's very very bad because generally search engines put better ads at the top, so that puts a bias on these factors.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "We have a new way to do this and.",
                    "label": 0
                },
                {
                    "sent": "Using these factors, we actually use ideas inspired by the previous section to evaluate policies that reorder the slate chosen by the current system, so it doesn't actually choose an entirely new slate.",
                    "label": 0
                },
                {
                    "sent": "It first calls the old system, and then it reorders those in somewhere in.",
                    "label": 0
                },
                {
                    "sent": "The reordering thing is important to get around this assumption that the old policy dependent depended on the input.",
                    "label": 0
                },
                {
                    "sent": "So with this we don't have that assumption anymore.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'll talk about these coefficients that we computed so.",
                    "label": 0
                },
                {
                    "sent": "What I have here is I'm applying the position and the actual coefficient so they are decreasing, which means that.",
                    "label": 0
                },
                {
                    "sent": "People tend to click on the higher higher ads for more often.",
                    "label": 0
                },
                {
                    "sent": "And the naive approach is very is.",
                    "label": 0
                },
                {
                    "sent": "Are very low compared to the new method which we have and I'll have time to go into the details of the new method, but please ask him about it.",
                    "label": 0
                },
                {
                    "sent": "The poster session.",
                    "label": 0
                },
                {
                    "sent": "What we see here is that there are low because they're biased here because it's the case that.",
                    "label": 0
                },
                {
                    "sent": "In Yahoo, we actually do put better ads, higher positions, so if you estimate it in the naive way, you get a low estimate for these and I also put down DCG, which is actually another common method that people use to estimate these, and it turns out that it's just equal to the law one over the log of the position plus one, so it really has no justification.",
                    "label": 0
                },
                {
                    "sent": "But this is in some sense of justification for those numbers because our numbers actually get very very close to those, so I thought that was interesting.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we use these to evaluate some based policy's on the on the log data.",
                    "label": 0
                },
                {
                    "sent": "So again, we were able to evaluate these policies without actually running them, which is the first one that we tried is the best possible feature independent reordering.",
                    "label": 0
                },
                {
                    "sent": "So as we took large entire set of ads, and we globally ordered them by their expected number of clicks across all possible pages, and then again this is a reordering policy, So what it does is.",
                    "label": 0
                },
                {
                    "sent": "It calls given a query or given a page, it calls the current system to produce a slate, and then it reorders them based on this global ordering of the ads, and we concluded that this would probably result in an 8% approximately 8% increase in the number of clicks.",
                    "label": 0
                },
                {
                    "sent": "We also looked at the best possible feature dependent reordering, which is a theoretical thing that you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't compute this just to compare these numbers.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum amount that you could increase it according to our metric.",
                    "label": 0
                },
                {
                    "sent": "And finally we looked at two algorithms based on the machine learn scoring rule.",
                    "label": 0
                },
                {
                    "sent": "So we used machine learning to try to order the ads and these are dependent on on the page.",
                    "label": 0
                },
                {
                    "sent": "Look at the maximum entry approach and a stochastic gradient descent and they both were fairly good at increasing the number of clicks when you use them to reorder the slate of ads, and so we get 20% and 21%.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we provided a new method for offline policy evaluation.",
                    "label": 1
                },
                {
                    "sent": "That is the first one that we know of that deals with a huge number of features, so that's not in the table based approach and deals with deterministic initial policy that has no explicit randomization in it.",
                    "label": 0
                },
                {
                    "sent": "With additional assumptions, we can show that it provides good estimates of reordering policies and online advertising.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all I have here.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So how did you?",
                    "label": 0
                },
                {
                    "sent": "Hug outline.",
                    "label": 0
                },
                {
                    "sent": "Miss you.",
                    "label": 0
                },
                {
                    "sent": "How did you compute the performance?",
                    "label": 0
                },
                {
                    "sent": "OK, So what you do is you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "First you have to estimate these.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These factors.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do as you see the office and produce a slate of ads, and one of them were clicked.",
                    "label": 0
                },
                {
                    "sent": "And the new policy reorders those ads.",
                    "label": 0
                },
                {
                    "sent": "So you look at the new position that the ad isn't in the old position, you take a ratio of the respective.",
                    "label": 0
                },
                {
                    "sent": "Estimated coefficients which gives you an expected value of how much more probability of click would you have gotten from moving that add up or down.",
                    "label": 0
                },
                {
                    "sent": "And then you add that and you average that across the entire data set.",
                    "label": 0
                },
                {
                    "sent": "So the reason that this gets around the assumption that the new policy issues in action is different from the old policies.",
                    "label": 0
                },
                {
                    "sent": "In some sense it's not.",
                    "label": 0
                },
                {
                    "sent": "It's using the same sleep, so.",
                    "label": 0
                },
                {
                    "sent": "It is actually OK, yeah, if that's what your question was, I didn't understand it correctly.",
                    "label": 0
                },
                {
                    "sent": "At this point we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't be sure that that's accurate.",
                    "label": 0
                },
                {
                    "sent": "The only assumption that we used was when we estimated these coefficients, and there's there's assumption there which is weaker than the previous assumption, but it's something on the lines of, like the previous assumption, that the actions don't depend on the input would imply this thing.",
                    "label": 0
                },
                {
                    "sent": "It's weaker than that.",
                    "label": 0
                },
                {
                    "sent": "It probably doesn't hold, so we can't guarantee that we estimated these right.",
                    "label": 0
                },
                {
                    "sent": "If you estimate these right then we can prove that our new estimator, using just hope things bound, will converge to.",
                    "label": 0
                },
                {
                    "sent": "The right answer, but since we don't have these exactly, we can't be sure, but we do have a pretty.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strong, especially from this graph.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very strong indication that these are probably very, very accurate.",
                    "label": 0
                },
                {
                    "sent": "Can you comment on how?",
                    "label": 0
                },
                {
                    "sent": "The case with some other words as opposed to just one step forward.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that the sort of reinforcement learning case where the input on the next step would OK?",
                    "label": 0
                },
                {
                    "sent": "Depends on the current action.",
                    "label": 0
                },
                {
                    "sent": "Someone thought about that too much.",
                    "label": 0
                },
                {
                    "sent": "You go ahead, John.",
                    "label": 0
                },
                {
                    "sent": "Entirely from a single.",
                    "label": 0
                },
                {
                    "sent": "Once every first step before starting, you can always replacing over vaccines with Norvasc to the timer.",
                    "label": 0
                },
                {
                    "sent": "But there is one thing you can do.",
                    "label": 0
                },
                {
                    "sent": "Given these actions, it's just thinking.",
                    "label": 0
                }
            ]
        }
    }
}