{
    "id": "pgmvxpgmpjot2sdvaxojrhhmqlwbzykp",
    "title": "Markov Chain Monte Carlo Methods",
    "info": {
        "author": [
            "Christian P. Robert, Paris Dauphine University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2004",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/mlss04_robert_mcmcm/",
    "segmentation": [
        [
            "Talk to you, learn more about his way of thinking.",
            "This is the Robert different public company.",
            "For those problems in the system.",
            "OK, thank you.",
            "So first I want to thank you either for the invitation.",
            "I'm quite sorry I would give a crash course because I'm only here for two days.",
            "So I'll try to cram everything in this six lectures.",
            "As you can see from the nodes, it's there's a lot of material because it's actually.",
            "Of course I gave in Finland and in April, and I had 12 hours to give it.",
            "So I just picked twice as fast today.",
            "So this is a trying to skip part of it.",
            "For the big inning.",
            "I scale it out of the big inning in the sense that I notice that there is a course by Christophe Andrieu in this book.",
            "That really covers.",
            "As the two first chapters.",
            "At the same level of detail.",
            "So if you lost for the first.",
            "Session you can refer to this chapter because you have it in your bag.",
            "Alright, so.",
            "The talk means."
        ],
        [
            "The course I gave it is based on my book with George Casella and I just want to mention that the new addition just disappeared.",
            "Of course, it's much better than the previous one.",
            "In particular, it covers one of the last chapters that I hope to talk about tomorrow about traversable jump.",
            "And also a new chapter on perfect sampling that presumably I will not have time to talk about.",
            "So."
        ],
        [
            "Today I'll.",
            "I'll try to cover regular mantecado.",
            "That means regular sampling and important sampling.",
            "An I give the basics of Markov chain theory that I need to justify via MCMC algorithms, and presumably I just start tomorrow with the MCM's algorithms.",
            "They have three spots tomorrow, so I'll talk about Metro police testing gives an reversible jump an the rest of the slides is like free material because I won't have time to talk about population Monte Carlo, which is a mix of MCMC.",
            "An important sampling and neither will have time to talk about perfect sampling, which is like the most advanced use of MCMC.",
            "Alright, but I think we have enough to talk about for this.",
            "For this hours.",
            "I have now so I'll start with a very very basic chapter where I tried to motivate the use of Monte Carlo methods.",
            "By the way, I'm a statistician.",
            "So all my examples would be motivated by statistics.",
            "And again, same as a less speaker.",
            "If you have any questions, just feel free to interrupt.",
            "Official because personally I'd go too fast.",
            "OK, so my first example that I will use over and over again because this is one of my preferred problems in statistics, is a mixture model.",
            "So make sure model is simply a convolution of several simple model that can be written as a weighted sum of densities so you have densities FJ an you sum up K densities with weight P1P2P K and the sum of the weights is 1 and that only means that in terms of sampling.",
            "You start by selecting one of the K densities with probability P1P2P K and once you have selected this density you generate from the identity value X. OK, so if you don't observe the selection mechanism, you get observations from this weighted sum, so that's something very SIM."
        ],
        [
            "Apple.",
            "Usually we pick.",
            "The density is F1F2F K as parameterized densities, and we are interested in the parameters of these densities, or.",
            "We can have the densities given in advance and we want to allocate the observations to each of the densities and such, more like a clustering or classification problem, OK?",
            "And when you write down the distribution of a sample of size N, you have a product of N terms, which are all thumbs of K terms.",
            "So it's a complexity is Kate.",
            "I'm North, but as we will see in a few slides, the real complexity is more of order K to the North, which is a number of possibilities of allocating the an observationes to the K densities."
        ],
        [
            "An even in the simplest case is like if you have a mixture of two normals with known weights, .7 and .3, and known violence is 1 and one, so you only have to estimate the two means you wanna new to here is shape.",
            "Of.",
            "The posterior surface of the likelihood.",
            "It's about the same thing, and so you have two modes, which is a funny thing because it actually is a totally identifiable problem.",
            "So you should have one mode before 500 Salvation.",
            "You keep having two modes an this is already a complex problem to find estimator of new one, you too and for instance if you use the standard EM algorithm to estimate both means depending on where you start from there you can see really well from the back but.",
            "The path of the algorithm will depend where you start from and out of five starting points.",
            "Here there are two that two path of EM that stops at the wrong mode, which doesn't mean anything in terms of the real value.",
            "So that's already a complex problem.",
            "Even though you can."
        ],
        [
            "Write down easily the likelihood.",
            "Father, Statistician, and in addition a Bayesian statistician.",
            "So my examples will also be mostly beige and.",
            "An Asian statistic is just starting from regular statistics, so you have model with an unknown parameter.",
            "You have observations X from a distribution indexed by this unknown parameter and all you do is you introduce a tool which is called a prior an, which is usually a distribution of probability distribution on the parameter space and you stopped to work with both these entities F and \u03c0."
        ],
        [
            "And then where you work with it is to use base theorem.",
            "That is, you think that pie is the distribution on Thea and your observable come from a conditional distribution X.",
            "Given Syria and you just supplies the standard probability theorem called Bayes theorem, that is, you can't use a partial distribution of Cedar given X.",
            "So you reverse the order of conditioning the likelihood condition X on theory.",
            "But now you've observed X condition Theta on X, so so that's your.",
            "Quantity you want to work with an use it as a regular probability distribution.",
            "OK, so pile feeder given X is a basic tool for doing Bayesian inference.",
            "Anne."
        ],
        [
            "So it's all over in terms of formal inference, except that of course we will have trouble to use this posterior distribution, so we'll skip this.",
            "This points, which are more like advertising for Bayesian statistics."
        ],
        [
            "So in very special conditions you can work out everything in close term and I took this simple example where you have a binomial.",
            "So if you observe a binomial B&P&P is your unknown parameter and if in addition on P, so it's a probability between zero and one, you use a beta distribution, which is just a Power Distribution on 01.",
            "You can find your estimator of P. Which is a posterior expectation of P conditional on X.",
            "And so if you write down what it means, you take P. You take your posterior, which is a product of your prior P2, Z a -- 1.",
            "One minus speech is a B -- 1 by the likelihood which is P2Z X y -- P to the minus X and bingo you get something new clothes form which looks really like X over your standard estimator, except there is a kind of stabilizing.",
            "Term here Express A and a + B + N. There is a very very special."
        ],
        [
            "Case, which is called a conjugate prior case and conjugate priors are nice because they give you close form solutions, but they are terrible because they restrict the use of prayer information.",
            "They have a specific form, so there are two sensitive to the choice of the prayer parameters and so.",
            "This is a bad situation that.",
            "Were sold by new computational methods in the early 90s.",
            "Because they need when we get out of computational.",
            "Computers only simple priors like conjugate priors.",
            "We have a range of difficulties.",
            "That kind of blocking, the more widespread use of vision techniques, and here are a few examples of computational problems first.",
            "The product of space may be constrained in a very non linear way as we will see later for the air problem.",
            "The sampling model itself.",
            "The likelihood may be complex enough so that we cannot write it down.",
            "We cannot compute explicitly the likelihood that the point.",
            "And again in some missing problems like this statistic volatility model, this is the case.",
            "You cannot write down the likelihood.",
            "The data set itself may be a problem is that if you have several millions of observations, the single computation of the likelihood may take too much time, and of course adding to that which is not a vision problem by itself.",
            "If you start using a complex prior distribution, which may be the result of previous observations that are too numerous to compute the prior, which is a posterior, then had to.",
            "Complexity an yeah.",
            "Lecture for lunch.",
            "We're going to be introductory.",
            "Yeah, this is introduction.",
            "Don't you think?",
            "It makes more sense to you instead of going twice as fast, you entry covered half as much material.",
            "I was trying to go snow there.",
            "You know, I think it'll be a waste of your time.",
            "Waste of a lot of our time if you if we're getting lost, I think.",
            "It's OK, but I think the speed is OK. We just met.",
            "I want to ask you.",
            "Maybe we can just ask.",
            "So yes, questions when you want.",
            "It was more slowly.",
            "Faster.",
            "No one faced with him.",
            "OK, let me ask you questions and it will go slower.",
            "Have you heard of base here already?",
            "No no one.",
            "Yeah, I don't know.",
            "OK, so.",
            "The difficulty when you use base here to do inference is that.",
            "You have a quantity here.",
            "You want to use as your pivot for the inference that the only thing you want to use.",
            "And when I was mentioning these computational problems.",
            "The points that block inference from being easy.",
            "All that first, you work on the parameter space here.",
            "That may be nothing like the regular Euclidean or to the D space.",
            "You may have constraints that are either imposed by kind of very weak prior information or by the model itself.",
            "An basic example we will see later is the air P model autoregressive of order P model.",
            "Where stationality.",
            "Involves a truncation of the space, which is totally nonlinear.",
            "So first you may have to do your inference with a lot of barriers, so when you go around your space you blocked very quickly if you go straight to take an example, the second difficulty is F of X given seta itself, you're given a model.",
            "But the model is a result of several integration of or interpolation or simplification, but actually.",
            "Complexification rather so censoring is 1 example.",
            "You don't observe the data itself, but some bits of the data, and so F of X itself is an integral.",
            "Off censoring menu Delta X, but you know that X is in a positive state, OK?",
            "So F maybe an integral in dimension or whatever, or 100 or 1000.",
            "Without difficulty, when you use this object, is that pie?",
            "Itself.",
            "You pick \u03c0 as a vision, but if you are.",
            "Did invasion you want to use prior information so your information may induce already a complexity in pile feeder.",
            "In the first example was a constraint parameter space.",
            "Your permission tells you that soda is there and not there, but of course you may have several levels of power information and one level of pro information is that you may have earlier observations an your prior observation.",
            "Your prior distribution maybe already.",
            "Posterior and so it may be complex by itself.",
            "Another level of complexity is that you will use this as a true.",
            "Probability distribution, and so you will have to compute quantities based on this distribution.",
            "And so you want to compute, for instance, an expectation based on this ratio and its expectation.",
            "If you are largely mentioned.",
            "For instance, maybe just not computable, because either the numerator is complex or you cannot even compute the denominator.",
            "And this is just.",
            "This is just example, but after if you want to do testing in a Bayesian setting, you use base factors.",
            "This is a basic tool in vision inference and based factors.",
            "If you oppose the the assumption that CD is in Seton, opt to theater in.",
            "This is in Cedar one you have to compute this ratio of the probability that series in sit on at given X over theorems in Cedar one given X.",
            "And that means that you have to do 2 integrals against this partial distribution just mentioned."
        ],
        [
            "OK, so if we go back to this mixture example an I take again and make sure of two normals.",
            "Your observations come from again the weighted sum of two normals P normal of menu, one Violence Sigma, one square and another normal of U2 and Violence Sigma 2 square.",
            "That's your model.",
            "That's something that's reasonably simple.",
            "Now you take a prior on all the parameters.",
            "That means you choose a certain distribution on mean violence weights.",
            "So I took some basic distributions.",
            "You don't need to know what it is.",
            "These are standard distributions, and so the partial distribution on my parameter, which is made of P, new one MU, Two Sigma, One Sigma two.",
            "So that's dimension 5.",
            "I call it's Aida.",
            "Is a product of.",
            "The likelihood by the prior, which is a product of all these distributions, and if I write it down, the simplest possible way it is the likelihood complexity two times an times the prior complexity enough, but actually.",
            "This doesn't mean anything to me.",
            "This product is just a function that they can compute in each.",
            "At each point Syria, but I am in Dimension 5.",
            "I have no intuition where to go for the important bits of this distribution, so I will rewrite rewrite it.",
            "As a double sum, it would be a sum of over all the partitions of my sample.",
            "My cell number is X1, XN in two bits.",
            "The observations that come from the first normal and the observations that come from the second normal an.",
            "If I partition my sample into these two bits, I may have L observations from the first normal an N -- L. From the second normal for my index L, here is the number of observations that go into the first normal and so.",
            "Of course L may run from zero to N and my second index here is all the possible party permutations that allocate the L first observations to the first normal, so that's a bit obscure, but all you need to know is that this double sum.",
            "Involves two to the N terms, which is a number of partitions of N observations and points into 2."
        ],
        [
            "Populations or subpopulations and the nice thing is that now I can start to understand what's going on that if I have a given permutation or given partition of my sample into 2 bits, while then of course I have one group of normal here, another group of normals there, and so you have two normal subsamples an I'm back to standard setting where I can compute the partial distribution if I know the partition as a product of standard distributions.",
            "Again, you don't need to know what it means, but I can make all the computations very easily.",
            "And here you can look at.",
            "Tried on the paper handouts.",
            "It means that I can compute everything just.",
            "Includes four everything is known, but the only difficulty is that online understand."
        ],
        [
            "Precisely, the meaning of all the terms, and I can compute all the weights, the probability of all the partitions.",
            "I just have too many terms.",
            "To use this representation of the posterior distribution or of the posterior expectation of the parameters.",
            "So all this terms are meaningful, the weights can be computed, but there are too many terms.",
            "So this is 1 basic example where.",
            "It's not a toy example in the sense that if I kind of choose an approximation device, I'm stuck an in practice Bayesian inference on mixtures are not made any progress tools of 90s because we had very approximate very crude approximation to this expression."
        ],
        [
            "Another example is the one I wanted to talk about about the difficulty of the parameter space itself.",
            "It is the auto regressive model where you have a signal or time series XD that you auto regressive the send that you make depend X on the past through a linear form of the past observations.",
            "Back to Verizon P. So XD is a weighted sum of the theater IX T -- 1 and you add.",
            "The noise subsidiary Standard Markovian model for time series.",
            "But first there is a difficulty if you impose stationary challenges series, it is that the corresponding polynomial 1 minus some of the seta IX to the I must have all routes outside the unit circle.",
            "So in terms of theorizes, makes very obscure constraint on the season I an in terms of inference.",
            "Usually if you start with a series that you want to.",
            "Plug and error P model.",
            "You don't know the order of the series, so you don't know P. And assume you just interrupted interested in prediction.",
            "So P is part of your parameter, so you have a pair of size, well unknown because it's P citiies plus P by exact P is unknown, so you have a parameter that evolves in an infinite dimensional space and you need to do an integration over this weird space to make your prediction.",
            "You have observed XTX T -- 1.",
            "X1 you are time T so you want to predict X2 plus one and to do the prediction.",
            "Invasion terms.",
            "You want to integrate some running out of power.",
            "There you want to integrate.",
            "Over well, the parameter space which is in CFP space so that makes."
        ],
        [
            "Something complicated because if you condition on P. You have a regular of flighty regular expression there, except it works in a weird space, so you integrate over theater.",
            "But then you have to sum up over all the possible Pisan, so compute the probability that P is the right order.",
            "So in terms of complexity, it's.",
            "One more level than the mixture problem, because actually you are not working with one model but with an Infinity of models.",
            "And so you want on the run to compare all models to compare all peas and your prediction, we just sum up over all the peas.",
            "Taking into account the probability that P is the right order.",
            "So that makes life even harder."
        ],
        [
            "So.",
            "For all this reasons.",
            "OK, so this is what I wanted to say here.",
            "There is an additional complexity, which is that you may have.",
            "Time series that come to you fairly often, so you may want to process.",
            "Either the predictive or supports your distribution rather fast mean.",
            "If you have extras that come every second, you want your approximation method to run in a smaller times and then one second obviously OK and the other point was that the parameters are really different from one model to the other than they have usually no connection between the error P theaters an the error P + 1 theaters, especially in terms of.",
            "Limited space if you look at the Paris peace, the constraints are not the same or you need to use another representation which is the root representation that we will use hopefully later."
        ],
        [
            "OK, so this was just the motivation from the vision statistical point of view.",
            "I have posted this tradition, which is something that sometimes maybe defining close form and sometimes may not be defined in closed form.",
            "But even if we have a closed form, we are facing two general classes of problems.",
            "We run a regular basean estimation.",
            "We will want to have integrals against this person distribution, so we have an integration problem an if we are doing.",
            "Less classical Bayesian inference like MA, P. For instance, we may want to optimize against this distribution, so we have an optimization problem.",
            "So in my examples.",
            "I say less classical because in my example I will mostly use integration examples.",
            "OK, so.",
            "We start with this tool that is the posterior distribution an.",
            "In this first bit.",
            "Recall the basic Montecarlo.",
            "Approximation methods and it's only tomorrow that I will talk about a Markov chain Monte Carlo."
        ],
        [
            "Simulation methods.",
            "OK."
        ],
        [
            "So just again to set.",
            "The notations.",
            "I have a distribution.",
            "F the density against.",
            "A measure of reference and I have a function of interest age and I want to compute the integral of H against F and because F is a density.",
            "This is the expectation of H against F. So.",
            "Depending on the setup, either H of or F may be partially unknown and define in an implicit manner."
        ],
        [
            "Now I want to cover.",
            "So if you notice in the notes, it is Chapter 3 because I won't cover the direct simulation of.",
            "Distributions given the density.",
            "OK, I won't use it directly, but let me recall one.",
            "Approach that has many connections with Metro police testing algorithms, which is the access project method.",
            "Usually it's when once you're given a density.",
            "It's fairly unusual to be able to produce one simulation from this density calling.",
            "1U generator, I mean the standard.",
            "CDF inverse method doesn't work for many densities.",
            "But there exist kind of uniform universal algorithm that's called the accept reject algorithm that can work once you have a functional F that is given up to a normalizing constant.",
            "OK, if you know a density and if you don't know the constant in front of the density.",
            "That's enough on principle to be able to simulate from this density, and to do so.",
            "Of course you don't simulate directly from the density, because this is a problem, but you use another density G that we will call instrumental.",
            "That obviously you know how to simulate from an.",
            "You will use simulation from the wrong density to reach a simulation from the true density, and this is something I wanted to recall because all MCMC methods are based on the same paradox.",
            "You never use the true density and still at the end you simulate from this true density.",
            "And there is something that."
        ],
        [
            "Underlies the principle of access projects that we called fundamental theorem of simulation, which is that.",
            "If you want to simulate from this density F. And F maybe a density or density up to a multiplicative constant that doesn't matter.",
            "Simulating from F is the same thing as simulating a uniform over this sub graph.",
            "I mean, this is an exercise.",
            "You can do later, but if you integrate U in this joint distribution, so if you integrate over U the ice here is.",
            "F of X and therefore marginally X is distributed from F of X, and this justifies not only accept reject but also the slice sampler.",
            "That is a Gibbs MCMC method.",
            "We will talk about later.",
            "OK now one."
        ],
        [
            "You've seen that.",
            "You can understand.",
            "At the intuitive level we accept reject method.",
            "If given F or C * F. You take another density G. Such that you can find.",
            "A constant M which bounds F / G so F is less than M * G. If you think of that graph is like having a hat on top.",
            "Of F you just put a hat.",
            "Over F, something that is bigger than F OK?",
            "Now, if you want to simulate uniformly over this Gray zone, you may start by simulating uniformly over something bigger.",
            "And something bigger means simulating from G. So if you simulate from J. Pica uniform.",
            "Multiply this uniform by M * G of X is like simulating over.",
            "Under MG, so you simulate over the sub graph of M * G and you get a uniform.",
            "Now sometimes you're on top of the grey zone.",
            "And sometime your insides grasm what I say that if you happen to be in the Gray zone, which is the same thing as you, less than F over MG, you stop because you're in the grey zone and you've just produced a uniform simulation out of the grazer.",
            "If you're not why you start again till you hit the grazer, so this is."
        ],
        [
            "Something that validates the algorithm.",
            "If you simulate uniform over larger space and restrict your output to the smaller space, you've produced a uniform or a small smaller space.",
            "Here is another example."
        ],
        [
            "Um?",
            "That should be more visible than it is, but I picked.",
            "Heart, which is MG.",
            "This is my function F which is like a normal multiplied by a sinus to get this wiggly shape an I simulate a lot of points uniformly under that yellow normal density.",
            "Multiplied by M and I only kept the points that were inside the Blue Zone an.",
            "As you can see, there are uniformly over the Blues, but the idea that really validates this method is that you just wait till you hit the inner surface and then it's uniformly distributed over the inner surface.",
            "So you can with just one assumption, which is that assumption.",
            "You can produce something from F without simulating from F. OK, and if you want to prove mathematical proof is just a stopping rule principle that because the user stopping rule to pick the X or so, you have a flow of simulations from X because you use a stopping rule, you get a random index on your flow and therefore you modify the distribution which is truly F and not G. Alright, thank you.",
            "OK, so."
        ],
        [
            "This is something.",
            "That's one of the bases of the random generators.",
            "For the standard distributions.",
            "The nice point that makes it useful for Bayesian inference is that you don't need to know F exactly.",
            "So if you have a prior that is close form and the likelihood that is closed for the product is still closed form, so you can compute Pi Theorem F of X given Syria at a given CR, even though you have no idea of the shape an if you can.",
            "Find a G such that Pi Theta F of X given she is bounded by M * G, Then simulating from G is enough.",
            "OK and there are a few properties that I won't mention.",
            "Right now the only important point is that F."
        ],
        [
            "Origi you have to pick a G such that F of G is bounded, so that implies that G must have heavy tails.",
            "If details are two small, you cannot simulate from this small tail distribution.",
            "So for instance you cannot simulate Akashi using a normal but the reverse."
        ],
        [
            "Is true if you pick a Koshy audio proposal distribution and you want to see more than normal because the ratio is bounded, you get a simulation method that works for instance.",
            "In that case is sqrt, 2\u03c0, / E. That means."
        ],
        [
            "That because 1 / M is a probability of accepting one random simulation.",
            "That means that you have an average acceptance of 1 / 152.",
            "So two times out of three you will accept your simulation.",
            "But"
        ],
        [
            "I want go more.",
            "Over that.",
            "Just the principle of having a G to simulate from F is something I want you to remember because this is something that we will use for MCMC and also the ratio F / G bounded is a property that we will meet again with MCMC method.",
            "An even earlier with important sampling, because when F / G is not bounded over methods, we can think of kind of fell at some level run over.",
            "So now we're back to this Chapter 3 on Monte Carlo.",
            "Integration.",
            "OK. As you presumably know, the base of Monte Carlo integration is just a load of large numbers that you can produce sample large enough from a given distribution F using the standard empirical average leads to an approximation that is converging in probability and usually with the central limit theorem to the right quantity.",
            "That is, with an expectation of absolute value of.",
            "H Finite this standard average converges in probability and almost surely to the expectation.",
            "So now the expectation is just one representation of the integral and therefore as N goes to Infinity, this approximation converges with probability one choose integral, we want to approximate.",
            "So if you have enough power and enough time you can get as close as you want to the true quantity."
        ],
        [
            "Now you can because it's a standard IID set up, you have a standard IID sample from a given distribution.",
            "You can use all the approximations to even evaluate the time or the power you need to approximate your integral in that you can also use standard violence estimators.",
            "Anderson to limit Theorem, assuming that the expectation of a square is finite.",
            "To say that your approximation compared with the truth divided by this approximation of the violence root of the variance is approximately normal.",
            "Oh OK, so you may not.",
            "Only you have convergence, but you have an ID of the error you make in the approximation."
        ],
        [
            "OK, so if I go back to my.",
            "Simulation example I know take.",
            "A true Bayesian inference problem I have say one observation from a normal fear one, and the mean is unknown and I put a Koshi prior on the mean, which either way of saying I don't know anything about domain because the Koshi has very heavy tails.",
            "So I'd like to Koshi standard Koshi 1, / \u03c0 one over 1 plus Theater Square as my prior.",
            "OK so this is my prior, this is my.",
            "Likelihood and therefore the poster distribution is something like y -- X -- y ^2 / 2 likelihoods 1 + 0 square prior.",
            "Anne.",
            "I'm missing a constant which is this integral over Thiere Ann.",
            "I want to compute the posterior mean.",
            "I mean, this is a toy example because I could do everything directly.",
            "But here is what I want to compute and I will use Monte Carlo to approximate this quantity.",
            "OK, so for instance because in here I have an expectation again, something I don't really control which is this product of the Koshy and the normal.",
            "So I can try to pick.",
            "One density that I like in this integral and for instance I can see E to the minus X minus square root 2, which looks like normal in sealed.",
            "So I can produce."
        ],
        [
            "A sample of normals with mean X 01 statistic M and use this sample to approximate both integrals at once.",
            "OK, the upper and the lower integral, the numerator and the denominator, or both integrals against a normal with mean X, they just use two different functions, seed over one procedure square and 1 / 1 plus Cedar Square.",
            "So if I use my sample I have.",
            "Simultaneously, an approximation of the numerator and the denominator using the same sample.",
            "I just scrap the 1 / M here, but you can see that both numerator and denominator converge to both integrals and therefore ratio also converges in probability.",
            "OK, and just to show you is the central theorem in action here I have my thousand iterations of the methods I produce up to 1000 normals with mean X.",
            "And I just replicated this simulation experiment."
        ],
        [
            "100 times and what shown here is the range of my estimations.",
            "OK, so.",
            "And there is nothing to see inside really, except the true value which is.",
            "Which isn't a meal, but why you can see that the range of.",
            "My estimation so that means that each of the 100 times I produce 1000 normals with mean X, my estimators went wiggling inside this range, and actually the open down is hit by one or several estimation sequences.",
            "But so while you can see that the central limit theorem is really.",
            "In action there in that both shapes of the upper and the lower limit of this route and decrease predicted by the central limit theorem and so in the end here, which is if you have run 1000 iterations, that's all you're interested in.",
            "This interval is the prediction interval from zero, and that is I.",
            "Plus or minus 2 standard deviations.",
            "For 1000 iterations.",
            "OK."
        ],
        [
            "Now you may have noticed something.",
            "Here.",
            "Which is that I picked the normal.",
            "X1.",
            "Of my simulation method it is, I said well.",
            "I spot in this integral normal, so therefore this is integrals that function against a normal density.",
            "And I run my simulation method using a normal sample with mean X and violence one.",
            "But I had an infinite amount of choice there because I could have chosen instead another distribution.",
            "For instance, Akashi distribution, Akashi 01 distribution.",
            "And I could have produced instead another sample from Akashi.",
            "01 and used another function to do my Monte Carlo approximation instead of Syria over one plus Cedar Square.",
            "I would have then used theater times E -- X -- Y ^2 and the method would have been equally valid in terms of principles, but I would have had two different types of samples, one with fat tails, a cushy one within tells the normal an.",
            "We could.",
            "Have had discussion over what is the best of the two and this is really the introduction to the important sampling method, which is that you can see two ways.",
            "The first way is to say that one when you have an expectation to compute against a density F. Simulating from F is not necessarily the best choice.",
            "Actually, it is never the best choice in the sense that you can reinterpret your integral of another expectation against another density.",
            "That is, if I start from FI, can pick practically any G and say that integral of H * F is equal of H. Times F / G * G. Because G / G is.",
            "Usually one.",
            "OK, but if I just flip the order it is integral of H * F of G * G, so it is an integral against G and so my expectations in terms of F are also expectations in terms of G. And if particularly any possible choices, OK, so the 1st way of seeing it, and the 3rd way is seeing that once you have an integral.",
            "Of a function.",
            "Against a density.",
            "It is.",
            "You can pick whatever density you wish to make the integration, and I think given that I'm a stop here, an I'll talk about Tim Horn sampling.",
            "In the first hour this afternoon.",
            "OK.",
            "But if you have questions.",
            "You're not hungry."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk to you, learn more about his way of thinking.",
                    "label": 0
                },
                {
                    "sent": "This is the Robert different public company.",
                    "label": 0
                },
                {
                    "sent": "For those problems in the system.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So first I want to thank you either for the invitation.",
                    "label": 0
                },
                {
                    "sent": "I'm quite sorry I would give a crash course because I'm only here for two days.",
                    "label": 0
                },
                {
                    "sent": "So I'll try to cram everything in this six lectures.",
                    "label": 0
                },
                {
                    "sent": "As you can see from the nodes, it's there's a lot of material because it's actually.",
                    "label": 0
                },
                {
                    "sent": "Of course I gave in Finland and in April, and I had 12 hours to give it.",
                    "label": 0
                },
                {
                    "sent": "So I just picked twice as fast today.",
                    "label": 0
                },
                {
                    "sent": "So this is a trying to skip part of it.",
                    "label": 0
                },
                {
                    "sent": "For the big inning.",
                    "label": 0
                },
                {
                    "sent": "I scale it out of the big inning in the sense that I notice that there is a course by Christophe Andrieu in this book.",
                    "label": 0
                },
                {
                    "sent": "That really covers.",
                    "label": 0
                },
                {
                    "sent": "As the two first chapters.",
                    "label": 0
                },
                {
                    "sent": "At the same level of detail.",
                    "label": 0
                },
                {
                    "sent": "So if you lost for the first.",
                    "label": 0
                },
                {
                    "sent": "Session you can refer to this chapter because you have it in your bag.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "The talk means.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The course I gave it is based on my book with George Casella and I just want to mention that the new addition just disappeared.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's much better than the previous one.",
                    "label": 0
                },
                {
                    "sent": "In particular, it covers one of the last chapters that I hope to talk about tomorrow about traversable jump.",
                    "label": 0
                },
                {
                    "sent": "And also a new chapter on perfect sampling that presumably I will not have time to talk about.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll try to cover regular mantecado.",
                    "label": 0
                },
                {
                    "sent": "That means regular sampling and important sampling.",
                    "label": 0
                },
                {
                    "sent": "An I give the basics of Markov chain theory that I need to justify via MCMC algorithms, and presumably I just start tomorrow with the MCM's algorithms.",
                    "label": 0
                },
                {
                    "sent": "They have three spots tomorrow, so I'll talk about Metro police testing gives an reversible jump an the rest of the slides is like free material because I won't have time to talk about population Monte Carlo, which is a mix of MCMC.",
                    "label": 0
                },
                {
                    "sent": "An important sampling and neither will have time to talk about perfect sampling, which is like the most advanced use of MCMC.",
                    "label": 0
                },
                {
                    "sent": "Alright, but I think we have enough to talk about for this.",
                    "label": 0
                },
                {
                    "sent": "For this hours.",
                    "label": 0
                },
                {
                    "sent": "I have now so I'll start with a very very basic chapter where I tried to motivate the use of Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "By the way, I'm a statistician.",
                    "label": 0
                },
                {
                    "sent": "So all my examples would be motivated by statistics.",
                    "label": 0
                },
                {
                    "sent": "And again, same as a less speaker.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions, just feel free to interrupt.",
                    "label": 0
                },
                {
                    "sent": "Official because personally I'd go too fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so my first example that I will use over and over again because this is one of my preferred problems in statistics, is a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So make sure model is simply a convolution of several simple model that can be written as a weighted sum of densities so you have densities FJ an you sum up K densities with weight P1P2P K and the sum of the weights is 1 and that only means that in terms of sampling.",
                    "label": 0
                },
                {
                    "sent": "You start by selecting one of the K densities with probability P1P2P K and once you have selected this density you generate from the identity value X. OK, so if you don't observe the selection mechanism, you get observations from this weighted sum, so that's something very SIM.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "Usually we pick.",
                    "label": 0
                },
                {
                    "sent": "The density is F1F2F K as parameterized densities, and we are interested in the parameters of these densities, or.",
                    "label": 0
                },
                {
                    "sent": "We can have the densities given in advance and we want to allocate the observations to each of the densities and such, more like a clustering or classification problem, OK?",
                    "label": 0
                },
                {
                    "sent": "And when you write down the distribution of a sample of size N, you have a product of N terms, which are all thumbs of K terms.",
                    "label": 0
                },
                {
                    "sent": "So it's a complexity is Kate.",
                    "label": 0
                },
                {
                    "sent": "I'm North, but as we will see in a few slides, the real complexity is more of order K to the North, which is a number of possibilities of allocating the an observationes to the K densities.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An even in the simplest case is like if you have a mixture of two normals with known weights, .7 and .3, and known violence is 1 and one, so you only have to estimate the two means you wanna new to here is shape.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "The posterior surface of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "It's about the same thing, and so you have two modes, which is a funny thing because it actually is a totally identifiable problem.",
                    "label": 0
                },
                {
                    "sent": "So you should have one mode before 500 Salvation.",
                    "label": 0
                },
                {
                    "sent": "You keep having two modes an this is already a complex problem to find estimator of new one, you too and for instance if you use the standard EM algorithm to estimate both means depending on where you start from there you can see really well from the back but.",
                    "label": 0
                },
                {
                    "sent": "The path of the algorithm will depend where you start from and out of five starting points.",
                    "label": 0
                },
                {
                    "sent": "Here there are two that two path of EM that stops at the wrong mode, which doesn't mean anything in terms of the real value.",
                    "label": 0
                },
                {
                    "sent": "So that's already a complex problem.",
                    "label": 0
                },
                {
                    "sent": "Even though you can.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Write down easily the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Father, Statistician, and in addition a Bayesian statistician.",
                    "label": 0
                },
                {
                    "sent": "So my examples will also be mostly beige and.",
                    "label": 0
                },
                {
                    "sent": "An Asian statistic is just starting from regular statistics, so you have model with an unknown parameter.",
                    "label": 0
                },
                {
                    "sent": "You have observations X from a distribution indexed by this unknown parameter and all you do is you introduce a tool which is called a prior an, which is usually a distribution of probability distribution on the parameter space and you stopped to work with both these entities F and \u03c0.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then where you work with it is to use base theorem.",
                    "label": 0
                },
                {
                    "sent": "That is, you think that pie is the distribution on Thea and your observable come from a conditional distribution X.",
                    "label": 0
                },
                {
                    "sent": "Given Syria and you just supplies the standard probability theorem called Bayes theorem, that is, you can't use a partial distribution of Cedar given X.",
                    "label": 0
                },
                {
                    "sent": "So you reverse the order of conditioning the likelihood condition X on theory.",
                    "label": 0
                },
                {
                    "sent": "But now you've observed X condition Theta on X, so so that's your.",
                    "label": 0
                },
                {
                    "sent": "Quantity you want to work with an use it as a regular probability distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so pile feeder given X is a basic tool for doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's all over in terms of formal inference, except that of course we will have trouble to use this posterior distribution, so we'll skip this.",
                    "label": 0
                },
                {
                    "sent": "This points, which are more like advertising for Bayesian statistics.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in very special conditions you can work out everything in close term and I took this simple example where you have a binomial.",
                    "label": 0
                },
                {
                    "sent": "So if you observe a binomial B&P&P is your unknown parameter and if in addition on P, so it's a probability between zero and one, you use a beta distribution, which is just a Power Distribution on 01.",
                    "label": 0
                },
                {
                    "sent": "You can find your estimator of P. Which is a posterior expectation of P conditional on X.",
                    "label": 0
                },
                {
                    "sent": "And so if you write down what it means, you take P. You take your posterior, which is a product of your prior P2, Z a -- 1.",
                    "label": 0
                },
                {
                    "sent": "One minus speech is a B -- 1 by the likelihood which is P2Z X y -- P to the minus X and bingo you get something new clothes form which looks really like X over your standard estimator, except there is a kind of stabilizing.",
                    "label": 0
                },
                {
                    "sent": "Term here Express A and a + B + N. There is a very very special.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case, which is called a conjugate prior case and conjugate priors are nice because they give you close form solutions, but they are terrible because they restrict the use of prayer information.",
                    "label": 0
                },
                {
                    "sent": "They have a specific form, so there are two sensitive to the choice of the prayer parameters and so.",
                    "label": 0
                },
                {
                    "sent": "This is a bad situation that.",
                    "label": 0
                },
                {
                    "sent": "Were sold by new computational methods in the early 90s.",
                    "label": 0
                },
                {
                    "sent": "Because they need when we get out of computational.",
                    "label": 0
                },
                {
                    "sent": "Computers only simple priors like conjugate priors.",
                    "label": 0
                },
                {
                    "sent": "We have a range of difficulties.",
                    "label": 0
                },
                {
                    "sent": "That kind of blocking, the more widespread use of vision techniques, and here are a few examples of computational problems first.",
                    "label": 0
                },
                {
                    "sent": "The product of space may be constrained in a very non linear way as we will see later for the air problem.",
                    "label": 0
                },
                {
                    "sent": "The sampling model itself.",
                    "label": 0
                },
                {
                    "sent": "The likelihood may be complex enough so that we cannot write it down.",
                    "label": 0
                },
                {
                    "sent": "We cannot compute explicitly the likelihood that the point.",
                    "label": 0
                },
                {
                    "sent": "And again in some missing problems like this statistic volatility model, this is the case.",
                    "label": 0
                },
                {
                    "sent": "You cannot write down the likelihood.",
                    "label": 0
                },
                {
                    "sent": "The data set itself may be a problem is that if you have several millions of observations, the single computation of the likelihood may take too much time, and of course adding to that which is not a vision problem by itself.",
                    "label": 0
                },
                {
                    "sent": "If you start using a complex prior distribution, which may be the result of previous observations that are too numerous to compute the prior, which is a posterior, then had to.",
                    "label": 0
                },
                {
                    "sent": "Complexity an yeah.",
                    "label": 0
                },
                {
                    "sent": "Lecture for lunch.",
                    "label": 0
                },
                {
                    "sent": "We're going to be introductory.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is introduction.",
                    "label": 0
                },
                {
                    "sent": "Don't you think?",
                    "label": 0
                },
                {
                    "sent": "It makes more sense to you instead of going twice as fast, you entry covered half as much material.",
                    "label": 0
                },
                {
                    "sent": "I was trying to go snow there.",
                    "label": 0
                },
                {
                    "sent": "You know, I think it'll be a waste of your time.",
                    "label": 0
                },
                {
                    "sent": "Waste of a lot of our time if you if we're getting lost, I think.",
                    "label": 0
                },
                {
                    "sent": "It's OK, but I think the speed is OK. We just met.",
                    "label": 0
                },
                {
                    "sent": "I want to ask you.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can just ask.",
                    "label": 0
                },
                {
                    "sent": "So yes, questions when you want.",
                    "label": 0
                },
                {
                    "sent": "It was more slowly.",
                    "label": 0
                },
                {
                    "sent": "Faster.",
                    "label": 0
                },
                {
                    "sent": "No one faced with him.",
                    "label": 0
                },
                {
                    "sent": "OK, let me ask you questions and it will go slower.",
                    "label": 0
                },
                {
                    "sent": "Have you heard of base here already?",
                    "label": 0
                },
                {
                    "sent": "No no one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The difficulty when you use base here to do inference is that.",
                    "label": 0
                },
                {
                    "sent": "You have a quantity here.",
                    "label": 0
                },
                {
                    "sent": "You want to use as your pivot for the inference that the only thing you want to use.",
                    "label": 0
                },
                {
                    "sent": "And when I was mentioning these computational problems.",
                    "label": 0
                },
                {
                    "sent": "The points that block inference from being easy.",
                    "label": 0
                },
                {
                    "sent": "All that first, you work on the parameter space here.",
                    "label": 0
                },
                {
                    "sent": "That may be nothing like the regular Euclidean or to the D space.",
                    "label": 0
                },
                {
                    "sent": "You may have constraints that are either imposed by kind of very weak prior information or by the model itself.",
                    "label": 0
                },
                {
                    "sent": "An basic example we will see later is the air P model autoregressive of order P model.",
                    "label": 0
                },
                {
                    "sent": "Where stationality.",
                    "label": 0
                },
                {
                    "sent": "Involves a truncation of the space, which is totally nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So first you may have to do your inference with a lot of barriers, so when you go around your space you blocked very quickly if you go straight to take an example, the second difficulty is F of X given seta itself, you're given a model.",
                    "label": 0
                },
                {
                    "sent": "But the model is a result of several integration of or interpolation or simplification, but actually.",
                    "label": 0
                },
                {
                    "sent": "Complexification rather so censoring is 1 example.",
                    "label": 0
                },
                {
                    "sent": "You don't observe the data itself, but some bits of the data, and so F of X itself is an integral.",
                    "label": 0
                },
                {
                    "sent": "Off censoring menu Delta X, but you know that X is in a positive state, OK?",
                    "label": 0
                },
                {
                    "sent": "So F maybe an integral in dimension or whatever, or 100 or 1000.",
                    "label": 0
                },
                {
                    "sent": "Without difficulty, when you use this object, is that pie?",
                    "label": 0
                },
                {
                    "sent": "Itself.",
                    "label": 0
                },
                {
                    "sent": "You pick \u03c0 as a vision, but if you are.",
                    "label": 0
                },
                {
                    "sent": "Did invasion you want to use prior information so your information may induce already a complexity in pile feeder.",
                    "label": 0
                },
                {
                    "sent": "In the first example was a constraint parameter space.",
                    "label": 0
                },
                {
                    "sent": "Your permission tells you that soda is there and not there, but of course you may have several levels of power information and one level of pro information is that you may have earlier observations an your prior observation.",
                    "label": 0
                },
                {
                    "sent": "Your prior distribution maybe already.",
                    "label": 0
                },
                {
                    "sent": "Posterior and so it may be complex by itself.",
                    "label": 0
                },
                {
                    "sent": "Another level of complexity is that you will use this as a true.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution, and so you will have to compute quantities based on this distribution.",
                    "label": 0
                },
                {
                    "sent": "And so you want to compute, for instance, an expectation based on this ratio and its expectation.",
                    "label": 0
                },
                {
                    "sent": "If you are largely mentioned.",
                    "label": 0
                },
                {
                    "sent": "For instance, maybe just not computable, because either the numerator is complex or you cannot even compute the denominator.",
                    "label": 0
                },
                {
                    "sent": "And this is just.",
                    "label": 0
                },
                {
                    "sent": "This is just example, but after if you want to do testing in a Bayesian setting, you use base factors.",
                    "label": 0
                },
                {
                    "sent": "This is a basic tool in vision inference and based factors.",
                    "label": 0
                },
                {
                    "sent": "If you oppose the the assumption that CD is in Seton, opt to theater in.",
                    "label": 0
                },
                {
                    "sent": "This is in Cedar one you have to compute this ratio of the probability that series in sit on at given X over theorems in Cedar one given X.",
                    "label": 0
                },
                {
                    "sent": "And that means that you have to do 2 integrals against this partial distribution just mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we go back to this mixture example an I take again and make sure of two normals.",
                    "label": 0
                },
                {
                    "sent": "Your observations come from again the weighted sum of two normals P normal of menu, one Violence Sigma, one square and another normal of U2 and Violence Sigma 2 square.",
                    "label": 0
                },
                {
                    "sent": "That's your model.",
                    "label": 0
                },
                {
                    "sent": "That's something that's reasonably simple.",
                    "label": 0
                },
                {
                    "sent": "Now you take a prior on all the parameters.",
                    "label": 0
                },
                {
                    "sent": "That means you choose a certain distribution on mean violence weights.",
                    "label": 0
                },
                {
                    "sent": "So I took some basic distributions.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know what it is.",
                    "label": 0
                },
                {
                    "sent": "These are standard distributions, and so the partial distribution on my parameter, which is made of P, new one MU, Two Sigma, One Sigma two.",
                    "label": 0
                },
                {
                    "sent": "So that's dimension 5.",
                    "label": 0
                },
                {
                    "sent": "I call it's Aida.",
                    "label": 0
                },
                {
                    "sent": "Is a product of.",
                    "label": 0
                },
                {
                    "sent": "The likelihood by the prior, which is a product of all these distributions, and if I write it down, the simplest possible way it is the likelihood complexity two times an times the prior complexity enough, but actually.",
                    "label": 0
                },
                {
                    "sent": "This doesn't mean anything to me.",
                    "label": 0
                },
                {
                    "sent": "This product is just a function that they can compute in each.",
                    "label": 0
                },
                {
                    "sent": "At each point Syria, but I am in Dimension 5.",
                    "label": 0
                },
                {
                    "sent": "I have no intuition where to go for the important bits of this distribution, so I will rewrite rewrite it.",
                    "label": 0
                },
                {
                    "sent": "As a double sum, it would be a sum of over all the partitions of my sample.",
                    "label": 0
                },
                {
                    "sent": "My cell number is X1, XN in two bits.",
                    "label": 0
                },
                {
                    "sent": "The observations that come from the first normal and the observations that come from the second normal an.",
                    "label": 0
                },
                {
                    "sent": "If I partition my sample into these two bits, I may have L observations from the first normal an N -- L. From the second normal for my index L, here is the number of observations that go into the first normal and so.",
                    "label": 0
                },
                {
                    "sent": "Of course L may run from zero to N and my second index here is all the possible party permutations that allocate the L first observations to the first normal, so that's a bit obscure, but all you need to know is that this double sum.",
                    "label": 0
                },
                {
                    "sent": "Involves two to the N terms, which is a number of partitions of N observations and points into 2.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Populations or subpopulations and the nice thing is that now I can start to understand what's going on that if I have a given permutation or given partition of my sample into 2 bits, while then of course I have one group of normal here, another group of normals there, and so you have two normal subsamples an I'm back to standard setting where I can compute the partial distribution if I know the partition as a product of standard distributions.",
                    "label": 0
                },
                {
                    "sent": "Again, you don't need to know what it means, but I can make all the computations very easily.",
                    "label": 0
                },
                {
                    "sent": "And here you can look at.",
                    "label": 0
                },
                {
                    "sent": "Tried on the paper handouts.",
                    "label": 0
                },
                {
                    "sent": "It means that I can compute everything just.",
                    "label": 0
                },
                {
                    "sent": "Includes four everything is known, but the only difficulty is that online understand.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Precisely, the meaning of all the terms, and I can compute all the weights, the probability of all the partitions.",
                    "label": 1
                },
                {
                    "sent": "I just have too many terms.",
                    "label": 0
                },
                {
                    "sent": "To use this representation of the posterior distribution or of the posterior expectation of the parameters.",
                    "label": 1
                },
                {
                    "sent": "So all this terms are meaningful, the weights can be computed, but there are too many terms.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 basic example where.",
                    "label": 0
                },
                {
                    "sent": "It's not a toy example in the sense that if I kind of choose an approximation device, I'm stuck an in practice Bayesian inference on mixtures are not made any progress tools of 90s because we had very approximate very crude approximation to this expression.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example is the one I wanted to talk about about the difficulty of the parameter space itself.",
                    "label": 0
                },
                {
                    "sent": "It is the auto regressive model where you have a signal or time series XD that you auto regressive the send that you make depend X on the past through a linear form of the past observations.",
                    "label": 0
                },
                {
                    "sent": "Back to Verizon P. So XD is a weighted sum of the theater IX T -- 1 and you add.",
                    "label": 0
                },
                {
                    "sent": "The noise subsidiary Standard Markovian model for time series.",
                    "label": 0
                },
                {
                    "sent": "But first there is a difficulty if you impose stationary challenges series, it is that the corresponding polynomial 1 minus some of the seta IX to the I must have all routes outside the unit circle.",
                    "label": 0
                },
                {
                    "sent": "So in terms of theorizes, makes very obscure constraint on the season I an in terms of inference.",
                    "label": 0
                },
                {
                    "sent": "Usually if you start with a series that you want to.",
                    "label": 0
                },
                {
                    "sent": "Plug and error P model.",
                    "label": 0
                },
                {
                    "sent": "You don't know the order of the series, so you don't know P. And assume you just interrupted interested in prediction.",
                    "label": 0
                },
                {
                    "sent": "So P is part of your parameter, so you have a pair of size, well unknown because it's P citiies plus P by exact P is unknown, so you have a parameter that evolves in an infinite dimensional space and you need to do an integration over this weird space to make your prediction.",
                    "label": 0
                },
                {
                    "sent": "You have observed XTX T -- 1.",
                    "label": 0
                },
                {
                    "sent": "X1 you are time T so you want to predict X2 plus one and to do the prediction.",
                    "label": 0
                },
                {
                    "sent": "Invasion terms.",
                    "label": 0
                },
                {
                    "sent": "You want to integrate some running out of power.",
                    "label": 0
                },
                {
                    "sent": "There you want to integrate.",
                    "label": 0
                },
                {
                    "sent": "Over well, the parameter space which is in CFP space so that makes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something complicated because if you condition on P. You have a regular of flighty regular expression there, except it works in a weird space, so you integrate over theater.",
                    "label": 0
                },
                {
                    "sent": "But then you have to sum up over all the possible Pisan, so compute the probability that P is the right order.",
                    "label": 0
                },
                {
                    "sent": "So in terms of complexity, it's.",
                    "label": 0
                },
                {
                    "sent": "One more level than the mixture problem, because actually you are not working with one model but with an Infinity of models.",
                    "label": 0
                },
                {
                    "sent": "And so you want on the run to compare all models to compare all peas and your prediction, we just sum up over all the peas.",
                    "label": 0
                },
                {
                    "sent": "Taking into account the probability that P is the right order.",
                    "label": 0
                },
                {
                    "sent": "So that makes life even harder.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For all this reasons.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what I wanted to say here.",
                    "label": 0
                },
                {
                    "sent": "There is an additional complexity, which is that you may have.",
                    "label": 0
                },
                {
                    "sent": "Time series that come to you fairly often, so you may want to process.",
                    "label": 0
                },
                {
                    "sent": "Either the predictive or supports your distribution rather fast mean.",
                    "label": 0
                },
                {
                    "sent": "If you have extras that come every second, you want your approximation method to run in a smaller times and then one second obviously OK and the other point was that the parameters are really different from one model to the other than they have usually no connection between the error P theaters an the error P + 1 theaters, especially in terms of.",
                    "label": 0
                },
                {
                    "sent": "Limited space if you look at the Paris peace, the constraints are not the same or you need to use another representation which is the root representation that we will use hopefully later.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was just the motivation from the vision statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "I have posted this tradition, which is something that sometimes maybe defining close form and sometimes may not be defined in closed form.",
                    "label": 0
                },
                {
                    "sent": "But even if we have a closed form, we are facing two general classes of problems.",
                    "label": 0
                },
                {
                    "sent": "We run a regular basean estimation.",
                    "label": 0
                },
                {
                    "sent": "We will want to have integrals against this person distribution, so we have an integration problem an if we are doing.",
                    "label": 0
                },
                {
                    "sent": "Less classical Bayesian inference like MA, P. For instance, we may want to optimize against this distribution, so we have an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So in my examples.",
                    "label": 0
                },
                {
                    "sent": "I say less classical because in my example I will mostly use integration examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We start with this tool that is the posterior distribution an.",
                    "label": 0
                },
                {
                    "sent": "In this first bit.",
                    "label": 0
                },
                {
                    "sent": "Recall the basic Montecarlo.",
                    "label": 0
                },
                {
                    "sent": "Approximation methods and it's only tomorrow that I will talk about a Markov chain Monte Carlo.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simulation methods.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just again to set.",
                    "label": 0
                },
                {
                    "sent": "The notations.",
                    "label": 0
                },
                {
                    "sent": "I have a distribution.",
                    "label": 0
                },
                {
                    "sent": "F the density against.",
                    "label": 0
                },
                {
                    "sent": "A measure of reference and I have a function of interest age and I want to compute the integral of H against F and because F is a density.",
                    "label": 0
                },
                {
                    "sent": "This is the expectation of H against F. So.",
                    "label": 0
                },
                {
                    "sent": "Depending on the setup, either H of or F may be partially unknown and define in an implicit manner.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I want to cover.",
                    "label": 0
                },
                {
                    "sent": "So if you notice in the notes, it is Chapter 3 because I won't cover the direct simulation of.",
                    "label": 0
                },
                {
                    "sent": "Distributions given the density.",
                    "label": 0
                },
                {
                    "sent": "OK, I won't use it directly, but let me recall one.",
                    "label": 0
                },
                {
                    "sent": "Approach that has many connections with Metro police testing algorithms, which is the access project method.",
                    "label": 0
                },
                {
                    "sent": "Usually it's when once you're given a density.",
                    "label": 0
                },
                {
                    "sent": "It's fairly unusual to be able to produce one simulation from this density calling.",
                    "label": 0
                },
                {
                    "sent": "1U generator, I mean the standard.",
                    "label": 0
                },
                {
                    "sent": "CDF inverse method doesn't work for many densities.",
                    "label": 0
                },
                {
                    "sent": "But there exist kind of uniform universal algorithm that's called the accept reject algorithm that can work once you have a functional F that is given up to a normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "OK, if you know a density and if you don't know the constant in front of the density.",
                    "label": 1
                },
                {
                    "sent": "That's enough on principle to be able to simulate from this density, and to do so.",
                    "label": 1
                },
                {
                    "sent": "Of course you don't simulate directly from the density, because this is a problem, but you use another density G that we will call instrumental.",
                    "label": 0
                },
                {
                    "sent": "That obviously you know how to simulate from an.",
                    "label": 0
                },
                {
                    "sent": "You will use simulation from the wrong density to reach a simulation from the true density, and this is something I wanted to recall because all MCMC methods are based on the same paradox.",
                    "label": 0
                },
                {
                    "sent": "You never use the true density and still at the end you simulate from this true density.",
                    "label": 0
                },
                {
                    "sent": "And there is something that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Underlies the principle of access projects that we called fundamental theorem of simulation, which is that.",
                    "label": 0
                },
                {
                    "sent": "If you want to simulate from this density F. And F maybe a density or density up to a multiplicative constant that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Simulating from F is the same thing as simulating a uniform over this sub graph.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is an exercise.",
                    "label": 0
                },
                {
                    "sent": "You can do later, but if you integrate U in this joint distribution, so if you integrate over U the ice here is.",
                    "label": 0
                },
                {
                    "sent": "F of X and therefore marginally X is distributed from F of X, and this justifies not only accept reject but also the slice sampler.",
                    "label": 0
                },
                {
                    "sent": "That is a Gibbs MCMC method.",
                    "label": 0
                },
                {
                    "sent": "We will talk about later.",
                    "label": 0
                },
                {
                    "sent": "OK now one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You've seen that.",
                    "label": 0
                },
                {
                    "sent": "You can understand.",
                    "label": 0
                },
                {
                    "sent": "At the intuitive level we accept reject method.",
                    "label": 0
                },
                {
                    "sent": "If given F or C * F. You take another density G. Such that you can find.",
                    "label": 0
                },
                {
                    "sent": "A constant M which bounds F / G so F is less than M * G. If you think of that graph is like having a hat on top.",
                    "label": 0
                },
                {
                    "sent": "Of F you just put a hat.",
                    "label": 0
                },
                {
                    "sent": "Over F, something that is bigger than F OK?",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to simulate uniformly over this Gray zone, you may start by simulating uniformly over something bigger.",
                    "label": 0
                },
                {
                    "sent": "And something bigger means simulating from G. So if you simulate from J. Pica uniform.",
                    "label": 0
                },
                {
                    "sent": "Multiply this uniform by M * G of X is like simulating over.",
                    "label": 0
                },
                {
                    "sent": "Under MG, so you simulate over the sub graph of M * G and you get a uniform.",
                    "label": 0
                },
                {
                    "sent": "Now sometimes you're on top of the grey zone.",
                    "label": 0
                },
                {
                    "sent": "And sometime your insides grasm what I say that if you happen to be in the Gray zone, which is the same thing as you, less than F over MG, you stop because you're in the grey zone and you've just produced a uniform simulation out of the grazer.",
                    "label": 0
                },
                {
                    "sent": "If you're not why you start again till you hit the grazer, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that validates the algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you simulate uniform over larger space and restrict your output to the smaller space, you've produced a uniform or a small smaller space.",
                    "label": 0
                },
                {
                    "sent": "Here is another example.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That should be more visible than it is, but I picked.",
                    "label": 0
                },
                {
                    "sent": "Heart, which is MG.",
                    "label": 0
                },
                {
                    "sent": "This is my function F which is like a normal multiplied by a sinus to get this wiggly shape an I simulate a lot of points uniformly under that yellow normal density.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by M and I only kept the points that were inside the Blue Zone an.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there are uniformly over the Blues, but the idea that really validates this method is that you just wait till you hit the inner surface and then it's uniformly distributed over the inner surface.",
                    "label": 0
                },
                {
                    "sent": "So you can with just one assumption, which is that assumption.",
                    "label": 0
                },
                {
                    "sent": "You can produce something from F without simulating from F. OK, and if you want to prove mathematical proof is just a stopping rule principle that because the user stopping rule to pick the X or so, you have a flow of simulations from X because you use a stopping rule, you get a random index on your flow and therefore you modify the distribution which is truly F and not G. Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is something.",
                    "label": 0
                },
                {
                    "sent": "That's one of the bases of the random generators.",
                    "label": 0
                },
                {
                    "sent": "For the standard distributions.",
                    "label": 0
                },
                {
                    "sent": "The nice point that makes it useful for Bayesian inference is that you don't need to know F exactly.",
                    "label": 0
                },
                {
                    "sent": "So if you have a prior that is close form and the likelihood that is closed for the product is still closed form, so you can compute Pi Theorem F of X given Syria at a given CR, even though you have no idea of the shape an if you can.",
                    "label": 0
                },
                {
                    "sent": "Find a G such that Pi Theta F of X given she is bounded by M * G, Then simulating from G is enough.",
                    "label": 0
                },
                {
                    "sent": "OK and there are a few properties that I won't mention.",
                    "label": 0
                },
                {
                    "sent": "Right now the only important point is that F.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Origi you have to pick a G such that F of G is bounded, so that implies that G must have heavy tails.",
                    "label": 0
                },
                {
                    "sent": "If details are two small, you cannot simulate from this small tail distribution.",
                    "label": 0
                },
                {
                    "sent": "So for instance you cannot simulate Akashi using a normal but the reverse.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is true if you pick a Koshy audio proposal distribution and you want to see more than normal because the ratio is bounded, you get a simulation method that works for instance.",
                    "label": 0
                },
                {
                    "sent": "In that case is sqrt, 2\u03c0, / E. That means.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That because 1 / M is a probability of accepting one random simulation.",
                    "label": 0
                },
                {
                    "sent": "That means that you have an average acceptance of 1 / 152.",
                    "label": 0
                },
                {
                    "sent": "So two times out of three you will accept your simulation.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want go more.",
                    "label": 0
                },
                {
                    "sent": "Over that.",
                    "label": 0
                },
                {
                    "sent": "Just the principle of having a G to simulate from F is something I want you to remember because this is something that we will use for MCMC and also the ratio F / G bounded is a property that we will meet again with MCMC method.",
                    "label": 0
                },
                {
                    "sent": "An even earlier with important sampling, because when F / G is not bounded over methods, we can think of kind of fell at some level run over.",
                    "label": 0
                },
                {
                    "sent": "So now we're back to this Chapter 3 on Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Integration.",
                    "label": 0
                },
                {
                    "sent": "OK. As you presumably know, the base of Monte Carlo integration is just a load of large numbers that you can produce sample large enough from a given distribution F using the standard empirical average leads to an approximation that is converging in probability and usually with the central limit theorem to the right quantity.",
                    "label": 0
                },
                {
                    "sent": "That is, with an expectation of absolute value of.",
                    "label": 0
                },
                {
                    "sent": "H Finite this standard average converges in probability and almost surely to the expectation.",
                    "label": 0
                },
                {
                    "sent": "So now the expectation is just one representation of the integral and therefore as N goes to Infinity, this approximation converges with probability one choose integral, we want to approximate.",
                    "label": 0
                },
                {
                    "sent": "So if you have enough power and enough time you can get as close as you want to the true quantity.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can because it's a standard IID set up, you have a standard IID sample from a given distribution.",
                    "label": 0
                },
                {
                    "sent": "You can use all the approximations to even evaluate the time or the power you need to approximate your integral in that you can also use standard violence estimators.",
                    "label": 0
                },
                {
                    "sent": "Anderson to limit Theorem, assuming that the expectation of a square is finite.",
                    "label": 0
                },
                {
                    "sent": "To say that your approximation compared with the truth divided by this approximation of the violence root of the variance is approximately normal.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so you may not.",
                    "label": 0
                },
                {
                    "sent": "Only you have convergence, but you have an ID of the error you make in the approximation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if I go back to my.",
                    "label": 0
                },
                {
                    "sent": "Simulation example I know take.",
                    "label": 0
                },
                {
                    "sent": "A true Bayesian inference problem I have say one observation from a normal fear one, and the mean is unknown and I put a Koshi prior on the mean, which either way of saying I don't know anything about domain because the Koshi has very heavy tails.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to Koshi standard Koshi 1, / \u03c0 one over 1 plus Theater Square as my prior.",
                    "label": 0
                },
                {
                    "sent": "OK so this is my prior, this is my.",
                    "label": 0
                },
                {
                    "sent": "Likelihood and therefore the poster distribution is something like y -- X -- y ^2 / 2 likelihoods 1 + 0 square prior.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I'm missing a constant which is this integral over Thiere Ann.",
                    "label": 0
                },
                {
                    "sent": "I want to compute the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a toy example because I could do everything directly.",
                    "label": 0
                },
                {
                    "sent": "But here is what I want to compute and I will use Monte Carlo to approximate this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance because in here I have an expectation again, something I don't really control which is this product of the Koshy and the normal.",
                    "label": 0
                },
                {
                    "sent": "So I can try to pick.",
                    "label": 0
                },
                {
                    "sent": "One density that I like in this integral and for instance I can see E to the minus X minus square root 2, which looks like normal in sealed.",
                    "label": 0
                },
                {
                    "sent": "So I can produce.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sample of normals with mean X 01 statistic M and use this sample to approximate both integrals at once.",
                    "label": 0
                },
                {
                    "sent": "OK, the upper and the lower integral, the numerator and the denominator, or both integrals against a normal with mean X, they just use two different functions, seed over one procedure square and 1 / 1 plus Cedar Square.",
                    "label": 0
                },
                {
                    "sent": "So if I use my sample I have.",
                    "label": 0
                },
                {
                    "sent": "Simultaneously, an approximation of the numerator and the denominator using the same sample.",
                    "label": 0
                },
                {
                    "sent": "I just scrap the 1 / M here, but you can see that both numerator and denominator converge to both integrals and therefore ratio also converges in probability.",
                    "label": 0
                },
                {
                    "sent": "OK, and just to show you is the central theorem in action here I have my thousand iterations of the methods I produce up to 1000 normals with mean X.",
                    "label": 0
                },
                {
                    "sent": "And I just replicated this simulation experiment.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "100 times and what shown here is the range of my estimations.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And there is nothing to see inside really, except the true value which is.",
                    "label": 0
                },
                {
                    "sent": "Which isn't a meal, but why you can see that the range of.",
                    "label": 0
                },
                {
                    "sent": "My estimation so that means that each of the 100 times I produce 1000 normals with mean X, my estimators went wiggling inside this range, and actually the open down is hit by one or several estimation sequences.",
                    "label": 0
                },
                {
                    "sent": "But so while you can see that the central limit theorem is really.",
                    "label": 0
                },
                {
                    "sent": "In action there in that both shapes of the upper and the lower limit of this route and decrease predicted by the central limit theorem and so in the end here, which is if you have run 1000 iterations, that's all you're interested in.",
                    "label": 0
                },
                {
                    "sent": "This interval is the prediction interval from zero, and that is I.",
                    "label": 0
                },
                {
                    "sent": "Plus or minus 2 standard deviations.",
                    "label": 0
                },
                {
                    "sent": "For 1000 iterations.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you may have noticed something.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Which is that I picked the normal.",
                    "label": 0
                },
                {
                    "sent": "X1.",
                    "label": 0
                },
                {
                    "sent": "Of my simulation method it is, I said well.",
                    "label": 0
                },
                {
                    "sent": "I spot in this integral normal, so therefore this is integrals that function against a normal density.",
                    "label": 0
                },
                {
                    "sent": "And I run my simulation method using a normal sample with mean X and violence one.",
                    "label": 0
                },
                {
                    "sent": "But I had an infinite amount of choice there because I could have chosen instead another distribution.",
                    "label": 0
                },
                {
                    "sent": "For instance, Akashi distribution, Akashi 01 distribution.",
                    "label": 0
                },
                {
                    "sent": "And I could have produced instead another sample from Akashi.",
                    "label": 0
                },
                {
                    "sent": "01 and used another function to do my Monte Carlo approximation instead of Syria over one plus Cedar Square.",
                    "label": 0
                },
                {
                    "sent": "I would have then used theater times E -- X -- Y ^2 and the method would have been equally valid in terms of principles, but I would have had two different types of samples, one with fat tails, a cushy one within tells the normal an.",
                    "label": 0
                },
                {
                    "sent": "We could.",
                    "label": 0
                },
                {
                    "sent": "Have had discussion over what is the best of the two and this is really the introduction to the important sampling method, which is that you can see two ways.",
                    "label": 0
                },
                {
                    "sent": "The first way is to say that one when you have an expectation to compute against a density F. Simulating from F is not necessarily the best choice.",
                    "label": 0
                },
                {
                    "sent": "Actually, it is never the best choice in the sense that you can reinterpret your integral of another expectation against another density.",
                    "label": 0
                },
                {
                    "sent": "That is, if I start from FI, can pick practically any G and say that integral of H * F is equal of H. Times F / G * G. Because G / G is.",
                    "label": 0
                },
                {
                    "sent": "Usually one.",
                    "label": 0
                },
                {
                    "sent": "OK, but if I just flip the order it is integral of H * F of G * G, so it is an integral against G and so my expectations in terms of F are also expectations in terms of G. And if particularly any possible choices, OK, so the 1st way of seeing it, and the 3rd way is seeing that once you have an integral.",
                    "label": 0
                },
                {
                    "sent": "Of a function.",
                    "label": 0
                },
                {
                    "sent": "Against a density.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "You can pick whatever density you wish to make the integration, and I think given that I'm a stop here, an I'll talk about Tim Horn sampling.",
                    "label": 0
                },
                {
                    "sent": "In the first hour this afternoon.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But if you have questions.",
                    "label": 0
                },
                {
                    "sent": "You're not hungry.",
                    "label": 0
                }
            ]
        }
    }
}