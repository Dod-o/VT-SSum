{
    "id": "swsdwf4ezn4wv5cwonjrjzxldr3migqc",
    "title": "Kernel Methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss2011_scholkopf_kernel/",
    "segmentation": [
        [
            "So I come from the empirical inference Department."
        ],
        [
            "Which I think is a nice term for what we are doing in machine learning.",
            "It refers to the process of drawing conclusions from empirical data from observational data, and one example of such conclusions is something we do as scientists will be doing scientific inference, so we measure some data points.",
            "Let's say we have two observables X&Y.",
            "We have two observables.",
            "We measure pairs of these observables.",
            "Let's say we find this kind of relationships.",
            "Then we might be tempted to say it looks like there's a linear law connecting these two observables.",
            "But lately it's already pointed out the philosopher, mathematician Leibnitz, that even if you.",
            "I mean, in this case you can maybe come up with a linear law, but even if you randomly produce points on a sheet of paper, for instance by taking an ink quill pen and just shaking it so you get some random points, you will also find some mathematical expression that somehow more or less explains the data points.",
            "So if you have these random points here, you might come up with a more complicated solution.",
            "Might even be a kernel expansion that we will talk about.",
            "More in this lecture and then the question comes up, why should we trust the one solution more than the other?",
            "Is there anything that allows us to justify where we trust something?",
            "And are there any infinite any general principles how to arrive at that sort of such solutions?"
        ],
        [
            "Now I mentioned this is from scientific inference, and of course scientists do this all the time and they have their own ways of handling it.",
            "This is a quote from the physicist Rutherford, who once said if your experiment needs statistics, you ought to have done a better experiment.",
            "So this is the enemy of machine learning.",
            "First, machine learning didn't exist at that time, so he's basically saying that data points should be such that the inference what is the law from the data point should be trivial.",
            "So the real problem is producing the right data, not producing inference from data.",
            "And of course we are specializing in inference from data, so we want to convince people that nowadays this is different.",
            "I need a sense, even from our point of view of the interesting part of this."
        ],
        [
            "How to do the inference from data?",
            "Now here's a second example of empirically inference.",
            "This is from the field of perception, so this is also something that we do all the time as animals.",
            "And here are some visual patterns that we have no problem recognizing.",
            "This is one of the favorite datasets of machine learning.",
            "People.",
            "the US Postal Service said of handwritten digits and.",
            "So if you see these dishes, you have no problem assigning them to the correct class.",
            "However, just to show you that this is actually not such a trivial inference problem.",
            "If I apply a permutation to the digits, and this is the same fixed permutation, so I'm just changing the pixels.",
            "Play the same fixed permutation everywhere.",
            "I get this kind of patterns that have the same information.",
            "But now it's difficult to assign them to the classes.",
            "Maybe you can still see that the zeros look a bit different from the threes, but it's it's no longer trivial to recognize these patterns.",
            "So this means that well or one of the explanations for this.",
            "Or maybe the main explanation is that this appears trivial to us because we've been trained on this kind of patterns for a long time, so we've been trained on contiguous smooth patterns.",
            "In general, and also specifically on handwritten digits, we've been trained in lots of.",
            "For us, it's easy to recognize these, but it's not so trivial to recognize these.",
            "Even if we were given a sizable training set.",
            "And for a machine learning algorithm, unless you do something very clever and try to add prior knowledge, these problems are in principle the same because these images are normally represented as vectors.",
            "So for machine learning algorithm, this is as hard as this one, and we can tell this one isn't isn't actually that easy to begin with.",
            "And there's a famous neuro scientist.",
            "Actually, once said the brain is nothing but a statistical decision Oregon.",
            "So we have different organs.",
            "The heart is a pump and the brain is our statistics engine.",
            "So from that point of view, I think we had a neuro scientist here.",
            "I think it's very good to have neuro scientists here, because if we want to understand what the brain is doing and if it's true that the brain is a statistical decision organ, then we need to understand statistics and inference."
        ],
        [
            "So.",
            "So this so this second example was an example of a relatively hard inference problem, and I'll say a bit more about what I mean by this.",
            "Before I show you another hard inference program, which is a more standard machine learning problem nowadays from bioinformatics.",
            "So in this case the task is to.",
            "Classification task on human DNA sequences.",
            "So you can use portions of the DNA sequence to predict or classify certain things.",
            "In this case, it's a 3 way classification problem.",
            "Maybe you don't have to worry about the battle details, but it's a classification problem for which we can get labeled data so we can perform biological experiments to check which of the three classes our piece of DNA sequence belongs to, and we can.",
            "Produce a large training set and then train some machine learning system on it and it doesn't matter.",
            "So for the point that I want to make more system it is.",
            "It also doesn't matter exactly what error measure we use or what correctness measure we use.",
            "The only point I want to make here is this is a problem where we have up to 15 million training points over here and if we use few training points where few means let's say 1000 or less than our performances.",
            "More or less you also were more or less than chance level.",
            "This is a certain kind of error measure.",
            "And if we use a lot of training points and we can be very accurate, so this is a kind of regularity out there in the world, which we wouldn't see from smallest datasets, or which we wouldn't see as humans, because there's no way for us to look at 15 million sequences of DNA locations.",
            "So if we just look at the data as biologists, we wouldn't see the regularity.",
            "So this is an example of such a hard inference problem that we as humans cannot solve because we didn't grow up with this kind of data, so we grew up with visual patterns when we didn't grow up with DNA sequences.",
            "So that's where machine learning is really interesting nowadays, so this is an example of a hard inference problem by this.",
            "Roughly speaking, me high dimensional problem.",
            "So we have to consider many factors simultaneously to find the regularity's.",
            "And these are factors that might interact in a non linear way.",
            "It's a relatively complex regularity, so we have to use nonlinear functions.",
            "Sometimes the problems might even be nonstationary.",
            "It's a problem where we have relatively little prior knowledge, so in particular this kind of problem from biology.",
            "It involves the splicing mechanism, which is not understood in detail.",
            "It's very complicated.",
            "We have some idea of what parts of the cells are involved.",
            "Involved in it.",
            "But we don't know how it works in detail and as a consequence of these three issues, we typically need large datasets to analyze such problems.",
            "And of course as a consequence of that we need computers.",
            "We need automatic inference methods.",
            "And also as a aside remark, maybe this is also the reason why what Rutherford did at the time when you said we shouldn't.",
            "We shouldn't do complicated inference.",
            "We should produce data that makes the inference trivial.",
            "Maybe that made sense at the time because we didn't have computers we didn't have automatic inference methods to deal with the data, so maybe it was a healthy selection bias that he had.",
            "So this is a hard inference."
        ],
        [
            "Problems, and maybe just to drive this point home, and I think it's quite remarkable and maybe in retrospect it would be trivial.",
            "But or maybe for you it's already trivial.",
            "You grew up in this world already, but I think it's remarkable that we are now at a point where we can solve with machines nontrivial scientific inference problems that cannot be solved by humans, so we can see structure in the world using machine learning methods that humans.",
            "Cannot see and I haven't said why we can't do that yet.",
            "I haven't talked about methods at all, but even if this were just because of the data set sizes that we can process or the dimensionality, the size in the other direction, I think it's a.",
            "It's a quantum leap in the application of machine learning in the world."
        ],
        [
            "So let me say a little bit about the main issues of inference in machine learning.",
            "So suppose we see this.",
            "I should also say.",
            "Of course you can interrupt at anytime.",
            "This is summer school, so if you have the feeling I go to maybe I've been doing too slowly so far.",
            "If you have the feeling I should speed up a little bit, give me some signal, throw something at me if I'm.",
            "Going too fast, which I probably am not, also give me some signal.",
            "So main idea, so generalization.",
            "So suppose we see this number sequence.",
            "1247 What's the next number?",
            "Any cases?",
            "1112 who wants 1314?",
            "OK, so let's see so 11 I think is the first thing people said.",
            "That means you have recognized this law here.",
            "Which is a particularly simple law which is caused the lazy caterer's sequence, because turns out if you have a piece of cake and you want to cut it into maximum number of pieces, then within cuts then this is what you get.",
            "So you just have to make sure that every new cut intersects or previous cuts and then the new cut adds an additional pieces if we have.",
            "If you have any previous cuts.",
            "In the next cut gives you anymore, so we have a N + N. So that's a nice sequence.",
            "Some people said 12.",
            "This is also a simple law that generates 12, but it continues differently.",
            "Some said 13 is the so called triple Nachi sequence.",
            "Each number is the sum of the previous three.",
            "Think someone said 14.",
            "I don't know whether it was serious or just a joke, but of course it's serious.",
            "In this case, it's a set of devices of 28, so we could even say in this case the sequence ends here with 28.",
            "Which adds another dimension.",
            "Previous key when suggested the next number should be one, because obviously this is the decimal expansions of \u03a0 and E interleaved.",
            "So Mary, it's a particularly compact expression for a mathematician, and I think you're getting the point.",
            "There are many different ways of continuing this.",
            "There's even a website where you can enter these numbers, and you then get possible continuations of this sequence.",
            "You get lots of them in the question.",
            "Of course, it's hard to tell."
        ],
        [
            "So which continuation is correct, which generalizes to the future?",
            "And this is a problem that has kept philosophers busy for a very long time.",
            "And of course, they haven't solved it.",
            "There's no way to tell, so this is the so called induction problem problem that was for instance studied by Hume but also by others.",
            "Even from ancient times, so this is difficult and we cannot solve that problem.",
            "But in statistical learning theory, which I find fascinating that we are dealing with more or less the same problem but in a slightly simpler, there's no statistical learning theory has found a way to rephrase this problem such that I believe it can be solved, but of course it's an easier problem, a different problem.",
            "Another question is not which continuation is correct, but it's a methodological question.",
            "What kind of procedures should we follow?",
            "Such that we come up with laws that are probably correct and I should say more accurately that we come up with laws that are probably as correct on the test data as there are the training data.",
            "So we can give some methodological procedures such that if you follow these procedures and you find out that not only have you followed the procedures, but you have also ended up with an explanation that works well on the training data.",
            "Then you can guarantee that with high probability you will do also relatively well on the test data.",
            "So this is kind of surprising when one first understands it, and I would like to explain this to you in this first lecture how this works.",
            "I think most of you will not work on statistical learning theory.",
            "In fact, I also haven't worked on it in front of for awhile, but I think it's.",
            "It's a nice.",
            "It's a nice theory and everybody who works in machine learning should know something about it, even if you don't apply it in your own work, because this is a surprising thing, so it's.",
            "If you want a formalization of a certain philosophical problem and such, we should all know about."
        ],
        [
            "Soon.",
            "So let's look at this particular problem, which maybe is the simplest problem of machine learning.",
            "Is the problem of two class classification.",
            "So in two class classification we learn functions that have two possible outputs plus minus one.",
            "So they take some input from some domain, X produce outputs plus minus one, and we want to learn these functions from M observations of inputs and outputs.",
            "We assume that these M observations, so each one has an input and output generated by a random experiment independently, so they are generated by an M fold application of this random experiment.",
            "In the random experiment is generate is characterized by some.",
            "Probability distribution joint distribution of X&Y that somehow captures what other dependencies between X&Y.",
            "So these are some regularity out there in the world, and we draw M times from this regularity to get these data points and our goal is to minimize the expected error or the risk, which means in the.",
            "Can you hear me now OK?",
            "So this means we are averaging over this probability distribution.",
            "So we imagine we draw more and more points from this distribution and every time we have drawn a point to point consisting of X&Y, we evaluate this so called loss function and then we average this loss function over all these points.",
            "This is nothing by that, and this is the quantity that they want that we want to minimize.",
            "And maybe you wonder why this loss function takes this particular form.",
            "So let's look at it.",
            "Take a look at this for a minute.",
            "This quantity here.",
            "So remember why is our outputs.",
            "They are either plus one or minus one.",
            "Likewise, the function F takes value of plus one or minus one, so this things here.",
            "This difference is either zero.",
            "It's a plus two or minus minus two.",
            "We take the modulus so it's zero or two.",
            "We multiply with 1/2, it's zero or one.",
            "So that's the so called 01 loss function, so it's zero whenever F of X is equal to Y, it's one whenever F of X is not equal to Y. OK, maybe this is better anyway.",
            "OK, so so we have our 01 loss function here.",
            "And we would like to minimize this quantity.",
            "Now the problem in learning, of course is we are given this training set.",
            "We want to estimate this function.",
            "We are not given this probability distribution, so we only have indirect knowledge about this distribution or about the law generating the data.",
            "We have indirect knowledge through the data, but we don't know half the law itself.",
            "If we had this law itself.",
            "We wouldn't be doing statistics, we would just be doing probability theory.",
            "So from this law you can.",
            "You can derive everything that you want to predict about this random experiment.",
            "In statistics, we don't have the law, but we have the data and we want to infer properties of this law, and this risk is 1 property of that law.",
            "What is minimal risk?",
            "OK so problem is this distribution is unknown, so we cannot compute this quantity, let alone minimize it.",
            "So we cannot find the function that has the minimal test error.",
            "The minimal future error.",
            "So we need some prescription, some procedure for finding something close to this function based on the training set, and this is quite an induction principle and the simplest induction principle.",
            "And the one that Muchnick in German increase will studying in their PhD thesis in Russia in the 60s is called empirical risk minimization.",
            "It consists of minimizing the training error over some class of functions.",
            "So the training error is the same thing as this, only that we replaced this average over the unknown distribution by an average over the training set.",
            "So that means we take a class of functions and we pick the function that has the lowest error on the training set and.",
            "And we can ask the question, does this lead to the the correct results in some sense, in which since it's a standard construction from statistics, we look at the question of consistency, which roughly speaking means if we had infinitely many training points X?",
            "Why would this procedure of always picking the function that has the minimum training error in the limit lead us to the best possible result?",
            "So would we?",
            "Would we find a function that really minimizes this thing in the limit?",
            "And I think Chairman is studied.",
            "That and it turns out that a lot of the structure of the whole room influence from studying this, and the result is."
        ],
        [
            "Kind of surprising.",
            "Surprising, surprising.",
            "So first one could think there's this standards law from statistics.",
            "The law of large numbers, which tells us that under fairly general conditions, but we only look at our particular case tells us for our particular case that the training error this empirical risk will converge towards the test error.",
            "So I."
        ],
        [
            "Hope I'm not confusing you with this, so this is the law of large numbers tells us that this thing here so provided these points XYI samples from this distribution, then this empirical average will converge to its expectations to it."
        ],
        [
            "Expectation G, which means the probability that this deviation between the empirical average and its expectation probability that this division is larger than epsilon.",
            "Will go to zero no matter how small.",
            "This positive constant epsilon is.",
            "So that's nice.",
            "Yeah, this thing converges to this money in probability.",
            "Unfortunately this is not enough for what we want.",
            "This doesn't imply that our procedure of minimizing this quantity in order to find a function that also minimizes this one.",
            "It is consistent, so this doesn't follow from the law of large numbers, and it turns out one needs a stronger law of large numbers when he's in the law of large numbers, which is uniform over the whole function class that we're working with.",
            "That's where suddenly the function class becomes relevant, 'cause it will turn out in order to get uniform convergence over a large function class, then one has to look at certain properties of the function class and then issues that complexity in capacity and so on come up.",
            "So I'll tell you about this in the next hour.",
            "So what we need turns out I'm just saying this now, but I'll tell you a bit more about it.",
            "We need a law which is uniform, so we need this kind of uniform convergence.",
            "We need that the worst function, in a sense, their function where.",
            "Training and test are as different as it can be even for that function we want the training error to converge to the test error.",
            "So."
        ],
        [
            "Let me try to give you a bit of an intuition for that.",
            "So let's look at this drawing here.",
            "So here we have the risk or test error training error.",
            "So this is the actual risk the tester or this is the empirical risk the training error.",
            "And here we have the function class.",
            "Of course, in reality it's not a 1 dimensional class might be infinite dimensional, but let's just draw one dimension.",
            "So I told you the law of large numbers tells us whenever we have some function F. Then its empirical error will converge in probability to its actual risk.",
            "So that means whatever slice we take in this direction, the point on the empirical error curve this curve will change as we see more and more training points.",
            "So this point will converge towards this one in probabilities, or the probability that these two points are more than epsilon part will go to zero.",
            "Now what we want is something a little bit different.",
            "What we want is we went to empirical risk mutation which means we take this curve.",
            "We find the minimizer of the curve.",
            "We then take this value of this parameter values or take this function and plug it into this this risk and then the we ask the question, is this going to be the minimum of that curve as well?",
            "So roughly speaking with a minimum of this curve converge to the minimum of this curve provided then we have this pointwise convergence and it turns out the answer is no and I think if you have said some mathematics training in analysis.",
            "You might have heard this concept of uniform convergence, which roughly speaking means that.",
            "The whole curve converges to the other one at the same speed, and if you have that then it turns out this is a sufficient condition for the consistency.",
            "So this is where the uniformity comes from.",
            "And of course then the question is.",
            "If we want a uniform convergence over the whole function class, this involves the function class and what function class should be choosing.",
            "So we might be tempted to say, well, we just should.",
            "We should, if we choose from, we shouldn't make a restriction a priority.",
            "We should take all possible functions.",
            "So all functions that take our input domain in map it into plus minus one 'cause we don't know before in which one is the right one.",
            "So we have training data.",
            "We use the class of all functions and we let the rest we let machine learning to the rest, or we did our empirical inference, empirical risk minimization principle, do the rest, and it turns out that cannot work, and it's easy to see."
        ],
        [
            "That this cannot work.",
            "And I'm going to switch to my latex slides for that.",
            "OK.",
            "So I think these are the slides you have."
        ],
        [
            "And we are now.",
            "We've now arrived here.",
            "So we're talking about the size of the function loss.",
            "You give me one second.",
            "OK, so why don't we take all functions?",
            "Well, so let's assume we have some training set in same names as before and someone gives me some test points.",
            "And for simplicity, let's assume the test points are disjoined from the turn training points.",
            "We don't want to see exactly the same point again, but if we have some continuous scenario, it's it's basically the probability that this happens is 0.",
            "So let's assume this is not going to happen.",
            "And then I claim that if you give me a function F. So suppose you I give you the training points, you give me a function F. You tell me.",
            "I reckon this is a good solution for my problem.",
            "Then I'll give you a second function F star, which has exactly the same outputs on all the training points, but which says the opposite on all the test points, and it's easy to construct this function because we are allowing all functions from X 2 + -- 1.",
            "So I'll just define my function to do this and.",
            "And therefore my function says the opposite and all the test points.",
            "Now the for the uniform convergence we are interested in the values of the functions everywhere, and if we compute the risk then it's obvious to you.",
            "I think I don't have to go through the details that if for this function the risk on training and test set is similar to each other, then for this function because this function says the opposite on the test set for, then for this function the risks on the training and test set can be very different from each other.",
            "So these two functions have the F&F star have the same risk on the training set.",
            "But they say the opposite on the test set, so they might have very different risks on the test set.",
            "So if if for one of these functions.",
            "If one of these functions that training in the test, there are very near to each other, then for the other function there might be very different from each other and we have no restriction.",
            "So so roughly speaking in this picture, but I'm telling you if you if you give me function that's here for which these two risks are closed, I can construct another function which is over here for which the two risks are very different.",
            "So it's not possible that these curves are close to each other everywhere and simultaneously.",
            "So why is it not possible?",
            "It's not possible because we were using the class of all functions from X 2 + -- 1.",
            "So we need a restriction.",
            "So this is by the way, this is called no free lunch theorem.",
            "Well, this has.",
            "This was always known to people in some people in statistics and machine learning, and it was later called the No Free Lunch theorem and so we need a restriction on the class of functions."
        ],
        [
            "And we need this everywhere in machine learning.",
            "In learning theory, the restriction is done in terms of the capacity of a class of functions.",
            "But also in other areas, like in the basin community, we need to place prior distributions over the class of functions, so we haven't somehow have to restrict the class of functions either in a hard way or in a software.",
            "And then, ideally, at least in learning theory, we would like to make statements about how strong is restriction.",
            "Is this?",
            "What kind of guarantees does it allow us to make?"
        ],
        [
            "Two so to analyze this a bit.",
            "Let's look at some details and I don't want to.",
            "Go into all details because you will have a course next week by any coaches at Yankee who I think will also cover some of this.",
            "But I think I want to spend maybe half an hour or a bit more on this because it's also useful as basis for some aspects of of kernel methods.",
            "And anyway, it's if you don't know this stuff, I think it's good to maybe here in the main ideas twice.",
            "Let's see, so let's not spend ages on it, but I think it's sufficiently interesting so.",
            "So let's define our or let's take define this shorthand as I for our loss the loss of point X iy I remember this was R 01 loss function.",
            "Then the first observation is, since this is XIYI drawn independently from the underlying distribution P. So that's our random experiments, like throwing a dice only that when we throw our days we get X&Y.",
            "So all these are independent trials, so this is.",
            "These are trials of independent random variable an.",
            "Actually in this case is the so-called only random variable taking values only zero or one is like tossing a coin.",
            "So these things are.",
            "These losses are like coin tosses and our empirical mean.",
            "Our training error is nothing but summing up how many times we toss heads as opposed to tails.",
            "And our actual risk is the expected value of this.",
            "So in the limit, if we do this infinitely many times."
        ],
        [
            "Now there's a nice bones by.",
            "The American statistician Herman shown off who I believe is still alive, at least a few years ago.",
            "He still wants, so this is also nice in our field.",
            "Heroes are still around.",
            "The field is not sold.",
            "And this bond basically tells us for this particularly nice situation, these independent trials.",
            "How fast does the law of large numbers work?",
            "And his bond tells us this probability of a deviation between this empirical average and its expectation.",
            "Deviation larger than epsilon is bounded by this exponential quantity, so that's nice exponentially.",
            "It goes to zero as the exponentially fast as the number of observations M goes to Infinity.",
            "And this probability.",
            "Just to be explicit.",
            "This is the probability that we obtain a sample, say one through 'cause.",
            "I am so excited that these are the losses of our training points.",
            "With the property that this this thing here is larger than website on so this is a probability over this enfold random experiment is so called product measure.",
            "And there's a consequence that we're going to use the consequence of this which is that.",
            "If we instead of taking the empirical quantity, needs expectation if we just do this empirical thing twice.",
            "So we draw two endpoints, they are independent of each other and you can imagine well they're both going to be close to their expectations, so they will also be close to each other, just a little bit further, and you can see that in this bond we have here a factor of 4 instead of two, and here it's also a little bit different.",
            "So basically the epsilon this replaced by epsilon over 2, then this number 2 moves down in the denominator.",
            "So so if we have a coin and we do too in trials and we take the average over the first 10 average of the 2nd and then this tells us how likely it is that these averages are very different and you can try it out tonight with the client if you want."
        ],
        [
            "To see if this works, if we translate it back just to be explicit into machine learning, this means the probability of getting a training set where training or intestinal different by more than epsilon is bounded by this quantity.",
            "That's nice, but as already mentioned before, this is a pointwise statement as opposed to a U1 pointwise meaning it refers to one fixed function F. And we cannot just say, well, let's let's now plug in the function that our learning machine returns.",
            "So suppose you have a neural network training in data.",
            "You it.",
            "In the end you have your parameter value, so you have a function F. You neural network implements and function.",
            "You might be tempted to say, well, let's take this function and plug it in here and then I know that for my function that I have obtained by training on my training set, the probability that I'm that my training or is far from the test error is upper bounded by this thing.",
            "Maybe I have seen 10,000 test examples than this quantity will be basically be 0.",
            "Unfortunately that's not allowed Becausw.",
            "If we looked at the data before choosing this function, which of course we do.",
            "If we trained on the data.",
            "Then we cannot apply the channel bond because our penalty trials become dependent because if we remember from before.",
            "So these quantities, these losses, they depend on the function, of course, and if this function basically knows something about all training points X one through XM, then even though the xiy are independent, that IR is no longer will be independent, so that's not allowed.",
            "So."
        ],
        [
            "We have to look at uniform convergence.",
            "There's no way to avoid it, so this is the one of the main theorems of learning theory and.",
            "This is we're not going to prove this, but I think I tried to give you a bit of intuition for this and we will prove something else.",
            "So this theorem tells us that the necessary and sufficient conditions for.",
            "A certain form of consistency, consistency, meaning we get the right result in the limit for certain consistency of empiricism.",
            "Isation of this procedure of choosing the function that minimizes the training error.",
            "Is that we have this kind of uniform convergence 1 sided so I don't have a modulus in here.",
            "This is a slightly weaker condition.",
            "Uniform overall functions that the learning machine can implement, and.",
            "If you want, you can put a modulus here.",
            "In that case you would get a condition for it.",
            "Consistency of both empirical risk minimization and empirical risk maximization, but normally we are not interested in empirical risk maximization.",
            "We don't choose the function that maximizes the training error, so we need this property to be true.",
            "This kind of uniform convergence.",
            "Another question is, are there properties of the learning machine properties of this class of function which will ensure that this uniform conversions can take place?"
        ],
        [
            "And this will lead us to capacity concepts and things like the VC dimension and I think it's nice to see this to see these ones, even if it's just that you're not scared of VC dimension.",
            "So let's take a look at this quantity.",
            "So this probability that the training error and the test error very different.",
            "So which means that if you look at the training data with your learning machine, you look how well you're doing, this might mislead you about the future performance.",
            "Now first of all, if our function class has only one point only one function.",
            "Then of course we can use the standard Journal font because in that case.",
            "Uniform convergence is the same as pointwise convergence.",
            "There's nothing to choose from, we just choose the one function that we have.",
            "In this case, our loss vectors or our losses are not dependent, and where you can apply Chernov in that Channel found as I stated before.",
            "Tells us that in this case we have this nice exponentially fast convergence.",
            "The next step will be if we have finitely many functions.",
            "We basically do more or less the same.",
            "We use a certain trick.",
            "We lose a bit here.",
            "We get another factor in here, but it doesn't cost us much because we still have this exponential thing.",
            "Now, the most interesting cases if we have infinitely many functions.",
            "So if you have infinitely many, then of course UU statement is much stronger than a point by a statement.",
            "In that case, the main trick will be that whenever we have only finitely many training points, even though the function shots might be infinite, if we have finitely many training points, then effectively it looks as if we only have finitely many functions, because the functions can only take outputs plus minus one.",
            "We have finitely many points.",
            "There are finitely many possible output vectors that we can generate and therefore the function class looks finite.",
            "So what we will be interested in is how large does a function class look on a certain number of training points that will be the crucial thing form to measure the."
        ],
        [
            "Acity of function class.",
            "But let's first, let's first do the case of finitely many functions.",
            "Even starting with just two functions.",
            "So if we have two functions.",
            "Column F1F2 so remember we were interested in this thing happening.",
            "We were interested in when.",
            "How likely is it that the training error is very different from the test error?",
            "In that case, well then and there is a supremum over the function class.",
            "Well, if the function has two functions, then of course this event here is the same as the union of two events where the 1st event describes the that.",
            "For the first function training error test, there are very different.",
            "The 2nd event is just drives the event where for the second function training tests are very different and events I told you before this is a product measure over drawing the training set.",
            "So events always means that someone gives me a training set with a certain property.",
            "So this is the event that's the training set is such that for the first functions training and test error are very different.",
            "So my training error.",
            "I always mean this is empirical risk here, and likewise for the second function and.",
            "So we rewrite all probabilities like this and then we can see that this probability can be first described, like the probability that sums of the probability of the two events minus the intersection is elementary probability theory.",
            "And since all probabilities are non negative, we can upper bound this by the sum of these two events.",
            "So we lose a little bit when both of them are misleading, but we have an upper bound and therefore we can upper bound this by two probabilities which now refer to only one function each.",
            "We know how to deal with one function, we use the Journal Font, so we get this exponential decay as before this quantity, only with the factor of two here.",
            "OK so far.",
            "So I think you can if you have understood this, you can only imagine what happens if we take in functions for infant, since we do exactly the same.",
            "So any fine."
        ],
        [
            "Add a number of functions.",
            "We re write this probability as the probability of this union of any events.",
            "Each of these events can be, well.",
            "First, we upper bound these union by some of the probabilities.",
            "This is again the so called union bound.",
            "We use the Journal for inequality for each term, and in this case we get an extra factor on and off in the right hands.",
            "Now the difficult case and this extra extra factor of NI should say it doesn't cost."
        ],
        [
            "As much because.",
            "If we have any instead of two here, but this thing here goes to 0 so fast that this is basically we can ignore that factor of in.",
            "So as long as we don't have anything here that goes up exponentially with them and we're in business.",
            "Of course, in the infinite case we might have things that go up exponentially with him and let's get to this case."
        ],
        [
            "No.",
            "So the basic idea, and so it's clear, so we cannot.",
            "Just we cannot do."
        ],
        [
            "The same triggers before, right?",
            "If we have infinite number over here, then we are lost.",
            "So we have to."
        ],
        [
            "Do some tricks and as mentioned before, the trick will be that the Imperial risks.",
            "The training only refers to endpoints and our functions taken values, output values plus minus one.",
            "Small functions on these endpoints can take at most 2 to the power of N values.",
            "So on endpoints.",
            "The function class has effectively maximum size of two to the power of M. There's a question.",
            "Indicates that previous example sets are independent or."
        ],
        [
            "So this is these are the different sets they could.",
            "They could overlap.",
            "They don't have to be disjoint.",
            "I guess that's what you mean, it's like."
        ],
        [
            "For.",
            "So if there are these, if they are not disjoint, then this quantity here is non zero, which means when we are doing this upper bound we're losing something.",
            "If they were disjoint, then we would be losing less than our boy bound in the end would be sharper, but with so we might lose a lot.",
            "If these events overlap."
        ],
        [
            "Yeah, so I mean sort of disjointness of events is something that's similar to what you mean by independence, right, independence, property, property of probability measures?",
            "If we just look at the events we would normally be talking about whether they are disjoint or mutually exclusive.",
            "So.",
            "OK, so we use a trick to reduce."
        ],
        [
            "The infinite case to the finite case, and this is the so.",
            "Cause Symmetrization lemma public in Germany in keys and is.",
            "I'm not going to prove this but I think you will believe me when you hear it.",
            "This lemma says that the probability that.",
            "Training on test error.",
            "Differ by more than epsilon can be upper bounded by the twice the probability that two different training errors differ from each other by more than epsilon over 2.",
            "So if these two things if by drawing two sets of training points.",
            "If you if by doing this you cannot get.",
            "Training errors that are very different from each other.",
            "Then we can also guarantee that the training error on the test error cannot be very different.",
            "It's again some kind of triangle inequality type argument.",
            "So which means now we need a training set of twice the size.",
            "And maybe maybe we over here ignore this down here.",
            "Now this is just for the details.",
            "If you want to look at it afterwards again.",
            "But basically it means rather than.",
            "Having this quantity here, which is the integral over the whole probability distribution that we don't know if we're only interested in upper bounding this quantity, we can first upper bounded by this thing here.",
            "And the nice thing about this is that this is a term that only involves endpoints.",
            "This is a term that only involves endpoints, so if we can somehow upper bound these guys by using some tricks that rely on the function class being effectively finite, we can do it over here because this.",
            "Quantities were filled to finitely many points, and as I mentioned before, in that case, the function classes are effectively."
        ],
        [
            "Final engine.",
            "So what we are therefore interested in is the maximum size of our function class on 2M points.",
            "And this is sometimes called the shattering coefficient.",
            "We denoted by this calligraphic in.",
            "So this can be also generalized to the case of real valued outputs, functions and people look at things like covering numbers in that case, but we only look at classification here and at this shattering coefficient, and let me remind you what it means.",
            "So the size of a function Class A + -- 1 valued function class.",
            "On two endpoints is the maximum number of different outputs that the function class can generate on two endpoints.",
            "He said we will consider two functions that are that give the same outputs on our two endpoints.",
            "We will consider them equivalent.",
            "There is no way for us to distinguish them, and therefore we only interested in how many such functions there are.",
            "And if you think of the output vectors of the output vectors are the entries are plus minus one there plus one whenever we have.",
            "Correctly separated the points, there are minus one.",
            "If we have done it the other way round.",
            "So if we have assigned a point to the other class, sorry there plus one or no.",
            "Actually this that's true there plus one or minus one, so I was not sorry I have to take back what I just said so I could also do this in terms of the loss functions, but it's not done like this here.",
            "So these are just plus 1 -- 1.",
            "How many output vectors can we generate on two endpoints?",
            "In other words, how many?",
            "How many different ways?",
            "Can we wait ways?",
            "Can we classify the points into classes?",
            "And of course, each point could go either in the one class on the other one, and there is an upper bound.",
            "This quantity, which is 2 to the power of two him.",
            "And if this upper bound is actually attained, then the function class is set to shatter the two endpoints.",
            "It means we can realize all possible class assignments using functions from our class, so intense the function class has maximum richness or maximum."
        ],
        [
            "City on sets of points of size to him.",
            "So now we put everything together to get our bond.",
            "So we first use the symmetrization so this symmetrization lemma to get rid of this test error.",
            "This quantity that depends on the unknown probability measure.",
            "That's an integral over lips probability measure.",
            "To get two quantities that are both only depending on endpoints.",
            "We then say another cheating a tiny little bit here, and if you want to know details, I can either give you something to read or I can tell you afterwards, but I don't want to go into great detail here.",
            "Then I'm basically saying well now my function class on these two endpoints has size calligraphic in this shattering coefficient.",
            "So that means I have a colleague drive I can effectively the size of the function classes this thing here.",
            "So I pick functions F1 through F sub N. I rewrite this as a union of all these events.",
            "Then I apply the Union bounds so each of these things.",
            "Basically upper bound, this by a sum of probabilities, where now I have one probability for each of these terms and it's a sum over calligraphic in.",
            "Such probabilities."
        ],
        [
            "And then I can use the general font for each term.",
            "So this is this second form of the channel found that was referring to two empirical quantities.",
            "How far they deviate from each other.",
            "For plug this in, then what I get is I have this shattering coefficient.",
            "I still have some that goes over this number of terms and for each term I get something like this and if I put this together I get this kind of found.",
            "What does this tell us?",
            "Then?",
            "This tells us that if the shattering coefficient if this somehow is well behaved, in particular if it doesn't grow exponentially in EM.",
            "Then the second term will always win, because this decays decays exponentially fast.",
            "So in that case the right hand side will go to 0.",
            "Well, I'm speaking differently now.",
            "Maybe I'm getting too emotional that the bound is closed.",
            "This is the so called Optic Shevlin English time inequality, so this is a people called this a tail bound in statistics and.",
            "Maybe just to discuss it a little bit.",
            "So there are two types of randomness in here.",
            "One is that this P over here.",
            "Refers to drawing training.",
            "Examples were drawing training examples from our unknown underlying probability distribution, so this is.",
            "This tells us how likely is it that we get a training set for which this is the case.",
            "And the second randomness is that here on these risks these are an expectation over the same probability distribution.",
            "Well, OK, this one is a product measure as a product of these distributions.",
            "This is the distribution itself.",
            "So here we also have we're using this probability distribution.",
            "We compute this expectation and here of course this is the risk.",
            "So is the expectation over the whole distribution.",
            "In other words, we think of this as the test error.",
            "And we can also rewrite this bound if we want.",
            "So now here in this form the bound tells us that is very unlikely for training error and test error to be very different epsilon different through unlikely.",
            "This term will be small for large trains and sizes."
        ],
        [
            "We can rewrite this.",
            "By specifying the probability with which we want the risks to be closed through the training error and then solving for epsilon.",
            "Maybe this is a nice exercise if you want to play around a bit with it, and we can then rewrite this bond into lines.",
            "And then in that case the bound will tell us that with the probability of at least one minus Delta this.",
            "And test error is upper bounded by the training error plus some quantity that depends on this shattering coefficient.",
            "And here you can also see this thing here will go to zero as N goes to Infinity and if this shattering coefficient does not grow exponentially in M, this thing here will always win against this, because here we have a logarithm and this bond is.",
            "This is not a typical learning theory bond.",
            "It's kind of surprising that something like this is true.",
            "It holds independent of the function, so this holds uniformly over the whole function class.",
            "In particular, it holds for the function minimizing the risks.",
            "So if we choose our function by minimizing some kind of training error, then this bone still holds because it holds for all functions.",
            "And.",
            "Yeah, maybe let's see.",
            "Maybe I should discuss this little bit actually, but actually, let's first talk a bit more about capacity concepts, and I think in the end I will have will stay there."
        ],
        [
            "Something like this bound again.",
            "So this is not the best possible bound.",
            "You can give.",
            "People have looked at constants and so on in detail and more sophisticated ways of doing things.",
            "That's one comment.",
            "The other comment is we cannot minimize the bound over the function, so that's something that maybe we would like to do.",
            "'cause in the end we want a function that generalizes well.",
            "So we might say, well, OK, you might say, OK, you've convinced me that I shouldn't be minimizing just the training error, so I gave you an example where we use the class of all possible functions.",
            "We are in the training error is basically meaningless.",
            "The tester can be arbitrarily different even though the training errors are the same.",
            "So maybe you're convinced the training error is not the whole story, but you might say, well, in that case let's just minimize this right hand side.",
            "And of course we would like to do something like that.",
            "We cannot do it directly, because OK, this is a property of the function here, but this over here is not a property of the function, it's a property of the function class they were choosing from.",
            "So it doesn't matter.",
            "What is the solution that you came up with at the end?",
            "But it matters what set of solutions you were choosing it from.",
            "So if you give someone on training, set some machine learning guy and he comes back and says here I have a solution for you then you cannot assess whether this is a good solution or not.",
            "You should you should first ask him well which solutions did you choose from and he said I chose from a huge class for solutions and you could say, well that's probably rubbish.",
            "What you're telling me if he chose from a small set of solutions.",
            "Apriori solutions to a small set of functions that he has chosen a prior before looking at the training data.",
            "And nevertheless from that small set he can give you a particular function which does well on the training data.",
            "Then you're in business.",
            "Then it looks like this looks, and then it's likely.",
            "High probability that the tester would also be good.",
            "So we have to do this in a more sophisticated way if we want to minimize this over functions, we have to somehow simultaneously control the size of the function class and minimize within the classes, and this would be called structural risk minimisation."
        ],
        [
            "Now I just want to briefly mention.",
            "And the main capacity measures from statistical learning theory.",
            "So I already told you that about this shattering coefficient.",
            "That's not the whole story, and the rest of us are the most accurate capacity measure which is rarely used is called the VC entropy.",
            "So you're probably not going to use it versus, but it's good to know that this exists because people sometimes.",
            "Think VC theory is identical to this kind of worst case theory where it has to hold uniform over all possible probability distributions, and that's not necessarily the case, so we can also give quantities that depend on probabilities distributions only that those are difficult to handle in practice.",
            "So remember again we have this sign which is our loss on an example XY loss.",
            "01 So now this is in terms of the loss vectors.",
            "If we have a sample of training points and we cycle through all our functions in the function class, we get a whole set of loss vectors that we can obtain and the cardinality of this set is denoted as this color graphic in and the VC entropy.",
            "So now here we don't have any supremum maximum.",
            "In shattering coefficient we had a supremum.",
            "The VC entropy is now defined as the expectation of the logarithm of that number, so this is the expectation taken over generating this M sample by an info experiment according to our underlying regularity P. So this is sort of like the expected complexity or the expected log size of the function class, and it turns out that.",
            "This thing here, well that uniform convergence of the risks and therefore consistency, is actually equivalent to this thing, growing subexponential sublinearly.",
            "So.",
            "So if this VC entropy divided by M goes to 0, this isn't if and only if conditioned for uniform convergence."
        ],
        [
            "Risks.",
            "Now if we loosen things a little bit, we can exchange the expectation and the logarithm so."
        ],
        [
            "Which.",
            "If we explain exchange this expectation in the logarithm, remember the logarithm is a convex function or concave depends.",
            "Which way you define it anyway, you will get an upper bound in that case."
        ],
        [
            "Ace and.",
            "Therefore this this thing here going to 0 is a stronger condition than the other one, 'cause this upper bounds is so called annealed entropy upper bounds.",
            "The VC entropy, and it turns out that this will be even only condition for exponentially fast uniform convergence.",
            "So it turns out in the other case you could get uniform convergence.",
            "That's arbitrarily slow, which they didn't practice might also be useless because.",
            "For every given training set size, you might still be very far away from having converged.",
            "OK, so then the next step of making things loser getting an upper bound again is to take the maximum instead of the expectation, in which case you get something called in growth function, which is essentially this shattering coefficient that I mentioned before.",
            "So he said logarithm of the shattering coefficient, and then this condition that the growth function growth grows sublinearly in the limit is equivalent to exponential convergence, independent of the underlying distribution.",
            "So that means no matter.",
            "What distribution generated my training and test points?",
            "It will be the case that I get uniform convergence of.",
            "For risks of training error towards test error.",
            "That's nice, because remember, we need the uniform convergence for consistency.",
            "So consistency was this question whether our procedure or minimizing the training error leads to the correct result in the limit.",
            "So we would like that to be true.",
            "And of course we would like it to be true.",
            "Normally we would like to be true independent of the underlying probability distribution, because we don't know the probability distribution.",
            "That's the whole point about learning.",
            "We only have a sample from the distribution, so we would like these statements to be independent, and in that case we could use this.",
            "Growth function.",
            "If we can compute it for a different class of functions or learning machine.",
            "Which will ensure this.",
            "Now remember that I mentioned before this shattering coefficient in the worst case, if the function is maximally, region will grow exponentially.",
            "Growth function is used logarithm, so if we translate this into growth functional language, it means the growth function will grow linearly, which means just to remind you briefly because we need it on the next slide.",
            "It means that we can generate all loss vectors or in other words we can.",
            "We can find points no matter what is M we can find endpoints such that by using functions of the learning machine we can generate all those vectors or that by using functions of learning machine we can separate them in all two to the M possible ways."
        ],
        [
            "Now there's this surprising result also due to buffering in Germany and keys.",
            "But also this has been proven by others by some on a little bit later and by Sheila.",
            "So this is sometimes called Summers lemma, even though it was first proven by happening in German Yankees and.",
            "It's a company tutorial lemma about the structure of this growth function and the surprising thing is that.",
            "It turns out the growth function, so I told you before in the worst possible case, or in the case where the function class has maximum richness, the growth function grows linearly even in the limits.",
            "So for all M. But it turns out that if this doesn't happen, then we have this nice upper bar.",
            "This nice look arhythmic upper bound, so there's nothing in between linear and logarithmically can only take one of these two behaviors.",
            "So either it has full richness.",
            "Or this function grows linearly for awhile, so up to some maximum.",
            "This number is called Missy dimension, and then afterwards suddenly we have this.",
            "It's very slow growth afterwards, so suddenly after that it's only logarithmically him, so it starts linear in M and then it becomes logarithmic.",
            "So this growth function will typically in the case that we are interested in.",
            "Of course we in the cases where we don't have maximum complexity.",
            "To see if we have maximum complexity, we cannot guarantee generalization, so we are interested in this second case, and the surprising thing is that in this second case, growth function grows linearly up to a certain point, and then suddenly we have this local rhythmic upper bound.",
            "So then the complexity of the function class grows very slowly suddenly.",
            "So that's.",
            "Kind of surprising.",
            "So up to a certain size of the training set, it looks like the function class is very rich and afterwards it's suddenly small and that's the regime in which we can."
        ],
        [
            "Generalize.",
            "So let me give you an example of this so called VC dimension.",
            "So remember I said the VC dimension at that point up to which the growth function grows linearly.",
            "So up to which the complexity of function task is maximum.",
            "So let's take this function class, which is the which are half spaces in R2.",
            "So they separated the space into an area where the value is a plus one.",
            "An area where the value is minus one.",
            "To specify these last functions, we need 3 parameters.",
            "And we can ask the question, how rich is this function class so up to which number of points is this maximally rich?",
            "Maybe I'll drink a bit while you meditate on this issue.",
            "So let's let's try it out.",
            "Let's take three points.",
            "So if we take three points, we put them in general position.",
            "And now we try to realize different separations of these points, so we can assign these points in different classes.",
            "Three points.",
            "We have 2 * 2 * 2 possibilities overall, because each point can be plus one or minus one, so we have eight different ways and I can hear I can show you 8 different ways of classifying these three points.",
            "And you can see that I can realize all these 8 three different separations.",
            "So this is a graphical proof that the VC dimension is at least three because for three points I still have maximum richness.",
            "Now the question is what happens if we have 4 points and that I I don't don't prove, but I think you're going to believe me.",
            "Or you can try it out if you want.",
            "But if I put four points, there will always be a situation where there's two points that I cannot separate from the other tool by using a straight line.",
            "No matter how you put them, there will always be a class assignment where you would like to use a done linear function which we are not allowed to do in this case.",
            "Because we're doing the pieces I mentioned of this class of linear functions where we call this linear functions because the separation lines are linear.",
            "So in this case there for the Visit Dimension is 3, which is noteworthy also because in this case it's identical to the number of parameters of the system.",
            "So this has sometimes misled people in thinking that the reason I mention is equal to the number of parameters or maybe also there used to be this this belief in statistics that the number of parameters is somehow a meaningful.",
            "Meaningful way of measuring the complexity of a set of functions, and maybe sometimes this is the case, but there are cases where the number of parameters is very small and still the complexity is large and vegetables are there cases where the number of parameters is very large and the complexity is small, so it's not really the right quantity."
        ],
        [
            "OK, so if we now plug in the VC dimension in our bounds, remember the VC dimension gave us this upper bound on the growth function."
        ],
        [
            "So.",
            "We have this other volunteer in terms of the museum mention age.",
            "On the growth function and we could we have we had before about and that was essentially using the growth function or actually exponential of the growth function this chat."
        ],
        [
            "In coefficient, if we now plug it in the VC dimension, we get up on in terms of the VC dimension, which looks like this.",
            "So you can see here if the VC dimension is finite, which is the case that we are interested in, then here on the right hand side this goes to zero as 1 / M and this grows logarithmically in M. But of course love Arhythmic growth is not as strong as going to 0 and 1 / M. So whenever age is finite this quantity here will go to 0.",
            "Ender maybe it's a it's worth thinking about this bound for a minute.",
            "What does it tell us?",
            "So it tells us that.",
            "If we can achieve training error training error, let's small by using class of functions that has a small BC dimension then we can guarantee that the test error is small and so is it something that's does it sound like it's logically impossible?",
            "That sounds like it's solving the induction problem where it's not.",
            "It's not really solving the induction problem, because if we if we saw if we try to learn something that cannot be learned.",
            "So there's a nice example of taking the telephone directory.",
            "If you take that elephant director of some city.",
            "And you try to predict the telephone number of a person from their name.",
            "So you have a large training set.",
            "If you take a big city.",
            "And you can try to train a huge neural network on this.",
            "If you take a large enough neural network, maybe it will work such that you have a small training error, but in that case you will find.",
            "So there's more training or Boost Mobile.",
            "In that case we will find that the network is so large that effectively its complexity will be or you will have to.",
            "You will have to work with such a huge class of networks to begin with.",
            "That VC dimension will be very large, so then this second term will blow up and you don't know whether you can generalize.",
            "And so, and on the other hand, if you take a function class, whether we see that mention is small, and then you will probably find that with that small function class you won't find a function that gives you a small training error.",
            "So in that case the first term will blow up, so you will never be able to guarantee something that's unreasonable like this telephone directory thing, or a second example is to try to predict the character of a person from their birth date.",
            "Maybe that's more controversial."
        ],
        [
            "OK, so this is.",
            "Now the picture I mentioned already before structure is musician.",
            "So if we want to minimize the right hand sides so 5 minutes to the breaker, or actually you're being polite is 0 minutes.",
            "But I think maybe let me see if this is a good point.",
            "Actually is a perfect point, so I have just two more slides and we stop.",
            "So a structure minimization.",
            "We had these two terms."
        ],
        [
            "This term, this time we went to have both of them all at the same time.",
            "To make this more, we just minimize over class of functions, find them when it hits the smallest training error.",
            "To make this small, we somehow have to vary the size of the function class.",
            "And one way to do it."
        ],
        [
            "Is is this procedure cost structures minimization which introduces this nested set of sets on the function class.",
            "So you could say for instance this is new, all neural networks with N -- 1 hidden nodes or something like this, and this is the class of networks that have up to N hidden nodes.",
            "So this is some way of increasing the size of the class of functions and then for each of these we compute the VC dimension and then the prescription is.",
            "But in each of these we should also find the empirical risk minimizer, and then we always sum up the training error that we get for the risk minimizer.",
            "Plus this second term that involves the VC dimension and then we will have this tradeoff between the training error that goes to 0 minimum training or goes to zero as the complexity increases.",
            "This second term will grow logarithmically and then we sum up these two things.",
            "Infrastructure is memorization.",
            "Tells us this is the point where we just should be working, so this is the VC dimension we should choose.",
            "And in that class we should take the the."
        ],
        [
            "Function that minimizes the visit minimizes the training error.",
            "OK, so maybe just two to get to a conclusion.",
            "I tried to explain to you that for in order to guarantee generalization and guaranteeing generalization, always means guaranteeing that the training error does not mislead us about the test error.",
            "So by in order to guarantee generalization, we need to take into account the size of a function class.",
            "And to take into account the size of function task, we need the right measures to measure the size of the function class and these measures are typically combinatorial quantities that measure how rich the function classes depending on the number of points we look at.",
            "So generalized, we need to control these quantities.",
            "We looked at an example where the VC dimension in two dimensions for separating elements was three.",
            "In N dimensions, it will turn out that we see dimension is N + 1.",
            "So if we have a high dimensional problems dimension could be quite large and in the support vector machines we will work with very high dimensional cases, maybe even infinite dimensional cases, and in that case we will need ways to bond the VC dimension.",
            "Other than using this trivial bound and I will tell you something about these kinds of function classes later on, I will also show you how to give an upper bound and the VC dimension of function classes that could even be in infinite dimensional spaces, but where we take into account in additional quantity called the margin of separation, and this will fit together nicely with kernel functions be kernel, because kernels are a way of generating such function classes in high dimensional spaces, or kernels are way of.",
            "Generating function classes that are not nonlinear, but that correspond to linear function classes in very high dimensional spaces.",
            "And then we're going to do the theoretical analysis in these high dimensional spaces where the functions are linear.",
            "We can handle the linear function classes, but in practice it will give us nonlinear function classes which we like to use for real world problems, because remember in real world problems in the bounds we always have two terms, one is the complexity and one is the.",
            "Empirical error and if we have a complicated problem, we won't be able to achieve a low empirical error using just linear function classes, at least often.",
            "We won't be, so this is the kind of goals for the."
        ],
        [
            "Next few lectures and now I should really let you have your coffee.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I come from the empirical inference Department.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which I think is a nice term for what we are doing in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It refers to the process of drawing conclusions from empirical data from observational data, and one example of such conclusions is something we do as scientists will be doing scientific inference, so we measure some data points.",
                    "label": 1
                },
                {
                    "sent": "Let's say we have two observables X&Y.",
                    "label": 0
                },
                {
                    "sent": "We have two observables.",
                    "label": 0
                },
                {
                    "sent": "We measure pairs of these observables.",
                    "label": 0
                },
                {
                    "sent": "Let's say we find this kind of relationships.",
                    "label": 0
                },
                {
                    "sent": "Then we might be tempted to say it looks like there's a linear law connecting these two observables.",
                    "label": 0
                },
                {
                    "sent": "But lately it's already pointed out the philosopher, mathematician Leibnitz, that even if you.",
                    "label": 0
                },
                {
                    "sent": "I mean, in this case you can maybe come up with a linear law, but even if you randomly produce points on a sheet of paper, for instance by taking an ink quill pen and just shaking it so you get some random points, you will also find some mathematical expression that somehow more or less explains the data points.",
                    "label": 0
                },
                {
                    "sent": "So if you have these random points here, you might come up with a more complicated solution.",
                    "label": 0
                },
                {
                    "sent": "Might even be a kernel expansion that we will talk about.",
                    "label": 0
                },
                {
                    "sent": "More in this lecture and then the question comes up, why should we trust the one solution more than the other?",
                    "label": 0
                },
                {
                    "sent": "Is there anything that allows us to justify where we trust something?",
                    "label": 0
                },
                {
                    "sent": "And are there any infinite any general principles how to arrive at that sort of such solutions?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I mentioned this is from scientific inference, and of course scientists do this all the time and they have their own ways of handling it.",
                    "label": 0
                },
                {
                    "sent": "This is a quote from the physicist Rutherford, who once said if your experiment needs statistics, you ought to have done a better experiment.",
                    "label": 1
                },
                {
                    "sent": "So this is the enemy of machine learning.",
                    "label": 0
                },
                {
                    "sent": "First, machine learning didn't exist at that time, so he's basically saying that data points should be such that the inference what is the law from the data point should be trivial.",
                    "label": 0
                },
                {
                    "sent": "So the real problem is producing the right data, not producing inference from data.",
                    "label": 0
                },
                {
                    "sent": "And of course we are specializing in inference from data, so we want to convince people that nowadays this is different.",
                    "label": 0
                },
                {
                    "sent": "I need a sense, even from our point of view of the interesting part of this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to do the inference from data?",
                    "label": 0
                },
                {
                    "sent": "Now here's a second example of empirically inference.",
                    "label": 0
                },
                {
                    "sent": "This is from the field of perception, so this is also something that we do all the time as animals.",
                    "label": 0
                },
                {
                    "sent": "And here are some visual patterns that we have no problem recognizing.",
                    "label": 0
                },
                {
                    "sent": "This is one of the favorite datasets of machine learning.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "the US Postal Service said of handwritten digits and.",
                    "label": 0
                },
                {
                    "sent": "So if you see these dishes, you have no problem assigning them to the correct class.",
                    "label": 0
                },
                {
                    "sent": "However, just to show you that this is actually not such a trivial inference problem.",
                    "label": 0
                },
                {
                    "sent": "If I apply a permutation to the digits, and this is the same fixed permutation, so I'm just changing the pixels.",
                    "label": 0
                },
                {
                    "sent": "Play the same fixed permutation everywhere.",
                    "label": 0
                },
                {
                    "sent": "I get this kind of patterns that have the same information.",
                    "label": 0
                },
                {
                    "sent": "But now it's difficult to assign them to the classes.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can still see that the zeros look a bit different from the threes, but it's it's no longer trivial to recognize these patterns.",
                    "label": 0
                },
                {
                    "sent": "So this means that well or one of the explanations for this.",
                    "label": 0
                },
                {
                    "sent": "Or maybe the main explanation is that this appears trivial to us because we've been trained on this kind of patterns for a long time, so we've been trained on contiguous smooth patterns.",
                    "label": 0
                },
                {
                    "sent": "In general, and also specifically on handwritten digits, we've been trained in lots of.",
                    "label": 0
                },
                {
                    "sent": "For us, it's easy to recognize these, but it's not so trivial to recognize these.",
                    "label": 0
                },
                {
                    "sent": "Even if we were given a sizable training set.",
                    "label": 0
                },
                {
                    "sent": "And for a machine learning algorithm, unless you do something very clever and try to add prior knowledge, these problems are in principle the same because these images are normally represented as vectors.",
                    "label": 0
                },
                {
                    "sent": "So for machine learning algorithm, this is as hard as this one, and we can tell this one isn't isn't actually that easy to begin with.",
                    "label": 0
                },
                {
                    "sent": "And there's a famous neuro scientist.",
                    "label": 0
                },
                {
                    "sent": "Actually, once said the brain is nothing but a statistical decision Oregon.",
                    "label": 1
                },
                {
                    "sent": "So we have different organs.",
                    "label": 0
                },
                {
                    "sent": "The heart is a pump and the brain is our statistics engine.",
                    "label": 0
                },
                {
                    "sent": "So from that point of view, I think we had a neuro scientist here.",
                    "label": 0
                },
                {
                    "sent": "I think it's very good to have neuro scientists here, because if we want to understand what the brain is doing and if it's true that the brain is a statistical decision organ, then we need to understand statistics and inference.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this so this second example was an example of a relatively hard inference problem, and I'll say a bit more about what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "Before I show you another hard inference program, which is a more standard machine learning problem nowadays from bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "So in this case the task is to.",
                    "label": 0
                },
                {
                    "sent": "Classification task on human DNA sequences.",
                    "label": 1
                },
                {
                    "sent": "So you can use portions of the DNA sequence to predict or classify certain things.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a 3 way classification problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't have to worry about the battle details, but it's a classification problem for which we can get labeled data so we can perform biological experiments to check which of the three classes our piece of DNA sequence belongs to, and we can.",
                    "label": 0
                },
                {
                    "sent": "Produce a large training set and then train some machine learning system on it and it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So for the point that I want to make more system it is.",
                    "label": 0
                },
                {
                    "sent": "It also doesn't matter exactly what error measure we use or what correctness measure we use.",
                    "label": 0
                },
                {
                    "sent": "The only point I want to make here is this is a problem where we have up to 15 million training points over here and if we use few training points where few means let's say 1000 or less than our performances.",
                    "label": 0
                },
                {
                    "sent": "More or less you also were more or less than chance level.",
                    "label": 0
                },
                {
                    "sent": "This is a certain kind of error measure.",
                    "label": 0
                },
                {
                    "sent": "And if we use a lot of training points and we can be very accurate, so this is a kind of regularity out there in the world, which we wouldn't see from smallest datasets, or which we wouldn't see as humans, because there's no way for us to look at 15 million sequences of DNA locations.",
                    "label": 0
                },
                {
                    "sent": "So if we just look at the data as biologists, we wouldn't see the regularity.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of such a hard inference problem that we as humans cannot solve because we didn't grow up with this kind of data, so we grew up with visual patterns when we didn't grow up with DNA sequences.",
                    "label": 0
                },
                {
                    "sent": "So that's where machine learning is really interesting nowadays, so this is an example of a hard inference problem by this.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, me high dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "So we have to consider many factors simultaneously to find the regularity's.",
                    "label": 1
                },
                {
                    "sent": "And these are factors that might interact in a non linear way.",
                    "label": 0
                },
                {
                    "sent": "It's a relatively complex regularity, so we have to use nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the problems might even be nonstationary.",
                    "label": 0
                },
                {
                    "sent": "It's a problem where we have relatively little prior knowledge, so in particular this kind of problem from biology.",
                    "label": 0
                },
                {
                    "sent": "It involves the splicing mechanism, which is not understood in detail.",
                    "label": 0
                },
                {
                    "sent": "It's very complicated.",
                    "label": 0
                },
                {
                    "sent": "We have some idea of what parts of the cells are involved.",
                    "label": 0
                },
                {
                    "sent": "Involved in it.",
                    "label": 0
                },
                {
                    "sent": "But we don't know how it works in detail and as a consequence of these three issues, we typically need large datasets to analyze such problems.",
                    "label": 0
                },
                {
                    "sent": "And of course as a consequence of that we need computers.",
                    "label": 1
                },
                {
                    "sent": "We need automatic inference methods.",
                    "label": 0
                },
                {
                    "sent": "And also as a aside remark, maybe this is also the reason why what Rutherford did at the time when you said we shouldn't.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't do complicated inference.",
                    "label": 0
                },
                {
                    "sent": "We should produce data that makes the inference trivial.",
                    "label": 0
                },
                {
                    "sent": "Maybe that made sense at the time because we didn't have computers we didn't have automatic inference methods to deal with the data, so maybe it was a healthy selection bias that he had.",
                    "label": 0
                },
                {
                    "sent": "So this is a hard inference.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems, and maybe just to drive this point home, and I think it's quite remarkable and maybe in retrospect it would be trivial.",
                    "label": 0
                },
                {
                    "sent": "But or maybe for you it's already trivial.",
                    "label": 0
                },
                {
                    "sent": "You grew up in this world already, but I think it's remarkable that we are now at a point where we can solve with machines nontrivial scientific inference problems that cannot be solved by humans, so we can see structure in the world using machine learning methods that humans.",
                    "label": 1
                },
                {
                    "sent": "Cannot see and I haven't said why we can't do that yet.",
                    "label": 0
                },
                {
                    "sent": "I haven't talked about methods at all, but even if this were just because of the data set sizes that we can process or the dimensionality, the size in the other direction, I think it's a.",
                    "label": 1
                },
                {
                    "sent": "It's a quantum leap in the application of machine learning in the world.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me say a little bit about the main issues of inference in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So suppose we see this.",
                    "label": 0
                },
                {
                    "sent": "I should also say.",
                    "label": 0
                },
                {
                    "sent": "Of course you can interrupt at anytime.",
                    "label": 0
                },
                {
                    "sent": "This is summer school, so if you have the feeling I go to maybe I've been doing too slowly so far.",
                    "label": 0
                },
                {
                    "sent": "If you have the feeling I should speed up a little bit, give me some signal, throw something at me if I'm.",
                    "label": 0
                },
                {
                    "sent": "Going too fast, which I probably am not, also give me some signal.",
                    "label": 0
                },
                {
                    "sent": "So main idea, so generalization.",
                    "label": 0
                },
                {
                    "sent": "So suppose we see this number sequence.",
                    "label": 0
                },
                {
                    "sent": "1247 What's the next number?",
                    "label": 0
                },
                {
                    "sent": "Any cases?",
                    "label": 0
                },
                {
                    "sent": "1112 who wants 1314?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see so 11 I think is the first thing people said.",
                    "label": 0
                },
                {
                    "sent": "That means you have recognized this law here.",
                    "label": 0
                },
                {
                    "sent": "Which is a particularly simple law which is caused the lazy caterer's sequence, because turns out if you have a piece of cake and you want to cut it into maximum number of pieces, then within cuts then this is what you get.",
                    "label": 1
                },
                {
                    "sent": "So you just have to make sure that every new cut intersects or previous cuts and then the new cut adds an additional pieces if we have.",
                    "label": 0
                },
                {
                    "sent": "If you have any previous cuts.",
                    "label": 0
                },
                {
                    "sent": "In the next cut gives you anymore, so we have a N + N. So that's a nice sequence.",
                    "label": 0
                },
                {
                    "sent": "Some people said 12.",
                    "label": 0
                },
                {
                    "sent": "This is also a simple law that generates 12, but it continues differently.",
                    "label": 0
                },
                {
                    "sent": "Some said 13 is the so called triple Nachi sequence.",
                    "label": 0
                },
                {
                    "sent": "Each number is the sum of the previous three.",
                    "label": 0
                },
                {
                    "sent": "Think someone said 14.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether it was serious or just a joke, but of course it's serious.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a set of devices of 28, so we could even say in this case the sequence ends here with 28.",
                    "label": 1
                },
                {
                    "sent": "Which adds another dimension.",
                    "label": 0
                },
                {
                    "sent": "Previous key when suggested the next number should be one, because obviously this is the decimal expansions of \u03a0 and E interleaved.",
                    "label": 1
                },
                {
                    "sent": "So Mary, it's a particularly compact expression for a mathematician, and I think you're getting the point.",
                    "label": 0
                },
                {
                    "sent": "There are many different ways of continuing this.",
                    "label": 0
                },
                {
                    "sent": "There's even a website where you can enter these numbers, and you then get possible continuations of this sequence.",
                    "label": 0
                },
                {
                    "sent": "You get lots of them in the question.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's hard to tell.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So which continuation is correct, which generalizes to the future?",
                    "label": 1
                },
                {
                    "sent": "And this is a problem that has kept philosophers busy for a very long time.",
                    "label": 0
                },
                {
                    "sent": "And of course, they haven't solved it.",
                    "label": 1
                },
                {
                    "sent": "There's no way to tell, so this is the so called induction problem problem that was for instance studied by Hume but also by others.",
                    "label": 0
                },
                {
                    "sent": "Even from ancient times, so this is difficult and we cannot solve that problem.",
                    "label": 0
                },
                {
                    "sent": "But in statistical learning theory, which I find fascinating that we are dealing with more or less the same problem but in a slightly simpler, there's no statistical learning theory has found a way to rephrase this problem such that I believe it can be solved, but of course it's an easier problem, a different problem.",
                    "label": 0
                },
                {
                    "sent": "Another question is not which continuation is correct, but it's a methodological question.",
                    "label": 0
                },
                {
                    "sent": "What kind of procedures should we follow?",
                    "label": 0
                },
                {
                    "sent": "Such that we come up with laws that are probably correct and I should say more accurately that we come up with laws that are probably as correct on the test data as there are the training data.",
                    "label": 1
                },
                {
                    "sent": "So we can give some methodological procedures such that if you follow these procedures and you find out that not only have you followed the procedures, but you have also ended up with an explanation that works well on the training data.",
                    "label": 0
                },
                {
                    "sent": "Then you can guarantee that with high probability you will do also relatively well on the test data.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of surprising when one first understands it, and I would like to explain this to you in this first lecture how this works.",
                    "label": 0
                },
                {
                    "sent": "I think most of you will not work on statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "In fact, I also haven't worked on it in front of for awhile, but I think it's.",
                    "label": 0
                },
                {
                    "sent": "It's a nice.",
                    "label": 0
                },
                {
                    "sent": "It's a nice theory and everybody who works in machine learning should know something about it, even if you don't apply it in your own work, because this is a surprising thing, so it's.",
                    "label": 0
                },
                {
                    "sent": "If you want a formalization of a certain philosophical problem and such, we should all know about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Soon.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this particular problem, which maybe is the simplest problem of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Is the problem of two class classification.",
                    "label": 0
                },
                {
                    "sent": "So in two class classification we learn functions that have two possible outputs plus minus one.",
                    "label": 0
                },
                {
                    "sent": "So they take some input from some domain, X produce outputs plus minus one, and we want to learn these functions from M observations of inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "We assume that these M observations, so each one has an input and output generated by a random experiment independently, so they are generated by an M fold application of this random experiment.",
                    "label": 0
                },
                {
                    "sent": "In the random experiment is generate is characterized by some.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution joint distribution of X&Y that somehow captures what other dependencies between X&Y.",
                    "label": 0
                },
                {
                    "sent": "So these are some regularity out there in the world, and we draw M times from this regularity to get these data points and our goal is to minimize the expected error or the risk, which means in the.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me now OK?",
                    "label": 0
                },
                {
                    "sent": "So this means we are averaging over this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So we imagine we draw more and more points from this distribution and every time we have drawn a point to point consisting of X&Y, we evaluate this so called loss function and then we average this loss function over all these points.",
                    "label": 0
                },
                {
                    "sent": "This is nothing by that, and this is the quantity that they want that we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "And maybe you wonder why this loss function takes this particular form.",
                    "label": 0
                },
                {
                    "sent": "So let's look at it.",
                    "label": 0
                },
                {
                    "sent": "Take a look at this for a minute.",
                    "label": 0
                },
                {
                    "sent": "This quantity here.",
                    "label": 0
                },
                {
                    "sent": "So remember why is our outputs.",
                    "label": 0
                },
                {
                    "sent": "They are either plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "Likewise, the function F takes value of plus one or minus one, so this things here.",
                    "label": 0
                },
                {
                    "sent": "This difference is either zero.",
                    "label": 0
                },
                {
                    "sent": "It's a plus two or minus minus two.",
                    "label": 0
                },
                {
                    "sent": "We take the modulus so it's zero or two.",
                    "label": 0
                },
                {
                    "sent": "We multiply with 1/2, it's zero or one.",
                    "label": 0
                },
                {
                    "sent": "So that's the so called 01 loss function, so it's zero whenever F of X is equal to Y, it's one whenever F of X is not equal to Y. OK, maybe this is better anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we have our 01 loss function here.",
                    "label": 0
                },
                {
                    "sent": "And we would like to minimize this quantity.",
                    "label": 0
                },
                {
                    "sent": "Now the problem in learning, of course is we are given this training set.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate this function.",
                    "label": 0
                },
                {
                    "sent": "We are not given this probability distribution, so we only have indirect knowledge about this distribution or about the law generating the data.",
                    "label": 0
                },
                {
                    "sent": "We have indirect knowledge through the data, but we don't know half the law itself.",
                    "label": 0
                },
                {
                    "sent": "If we had this law itself.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't be doing statistics, we would just be doing probability theory.",
                    "label": 0
                },
                {
                    "sent": "So from this law you can.",
                    "label": 0
                },
                {
                    "sent": "You can derive everything that you want to predict about this random experiment.",
                    "label": 0
                },
                {
                    "sent": "In statistics, we don't have the law, but we have the data and we want to infer properties of this law, and this risk is 1 property of that law.",
                    "label": 0
                },
                {
                    "sent": "What is minimal risk?",
                    "label": 0
                },
                {
                    "sent": "OK so problem is this distribution is unknown, so we cannot compute this quantity, let alone minimize it.",
                    "label": 1
                },
                {
                    "sent": "So we cannot find the function that has the minimal test error.",
                    "label": 0
                },
                {
                    "sent": "The minimal future error.",
                    "label": 0
                },
                {
                    "sent": "So we need some prescription, some procedure for finding something close to this function based on the training set, and this is quite an induction principle and the simplest induction principle.",
                    "label": 0
                },
                {
                    "sent": "And the one that Muchnick in German increase will studying in their PhD thesis in Russia in the 60s is called empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "It consists of minimizing the training error over some class of functions.",
                    "label": 1
                },
                {
                    "sent": "So the training error is the same thing as this, only that we replaced this average over the unknown distribution by an average over the training set.",
                    "label": 0
                },
                {
                    "sent": "So that means we take a class of functions and we pick the function that has the lowest error on the training set and.",
                    "label": 0
                },
                {
                    "sent": "And we can ask the question, does this lead to the the correct results in some sense, in which since it's a standard construction from statistics, we look at the question of consistency, which roughly speaking means if we had infinitely many training points X?",
                    "label": 0
                },
                {
                    "sent": "Why would this procedure of always picking the function that has the minimum training error in the limit lead us to the best possible result?",
                    "label": 0
                },
                {
                    "sent": "So would we?",
                    "label": 0
                },
                {
                    "sent": "Would we find a function that really minimizes this thing in the limit?",
                    "label": 0
                },
                {
                    "sent": "And I think Chairman is studied.",
                    "label": 0
                },
                {
                    "sent": "That and it turns out that a lot of the structure of the whole room influence from studying this, and the result is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of surprising.",
                    "label": 0
                },
                {
                    "sent": "Surprising, surprising.",
                    "label": 0
                },
                {
                    "sent": "So first one could think there's this standards law from statistics.",
                    "label": 0
                },
                {
                    "sent": "The law of large numbers, which tells us that under fairly general conditions, but we only look at our particular case tells us for our particular case that the training error this empirical risk will converge towards the test error.",
                    "label": 1
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hope I'm not confusing you with this, so this is the law of large numbers tells us that this thing here so provided these points XYI samples from this distribution, then this empirical average will converge to its expectations to it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expectation G, which means the probability that this deviation between the empirical average and its expectation probability that this division is larger than epsilon.",
                    "label": 0
                },
                {
                    "sent": "Will go to zero no matter how small.",
                    "label": 0
                },
                {
                    "sent": "This positive constant epsilon is.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this thing converges to this money in probability.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately this is not enough for what we want.",
                    "label": 0
                },
                {
                    "sent": "This doesn't imply that our procedure of minimizing this quantity in order to find a function that also minimizes this one.",
                    "label": 0
                },
                {
                    "sent": "It is consistent, so this doesn't follow from the law of large numbers, and it turns out one needs a stronger law of large numbers when he's in the law of large numbers, which is uniform over the whole function class that we're working with.",
                    "label": 1
                },
                {
                    "sent": "That's where suddenly the function class becomes relevant, 'cause it will turn out in order to get uniform convergence over a large function class, then one has to look at certain properties of the function class and then issues that complexity in capacity and so on come up.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you about this in the next hour.",
                    "label": 0
                },
                {
                    "sent": "So what we need turns out I'm just saying this now, but I'll tell you a bit more about it.",
                    "label": 0
                },
                {
                    "sent": "We need a law which is uniform, so we need this kind of uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "We need that the worst function, in a sense, their function where.",
                    "label": 0
                },
                {
                    "sent": "Training and test are as different as it can be even for that function we want the training error to converge to the test error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me try to give you a bit of an intuition for that.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this drawing here.",
                    "label": 0
                },
                {
                    "sent": "So here we have the risk or test error training error.",
                    "label": 0
                },
                {
                    "sent": "So this is the actual risk the tester or this is the empirical risk the training error.",
                    "label": 0
                },
                {
                    "sent": "And here we have the function class.",
                    "label": 0
                },
                {
                    "sent": "Of course, in reality it's not a 1 dimensional class might be infinite dimensional, but let's just draw one dimension.",
                    "label": 0
                },
                {
                    "sent": "So I told you the law of large numbers tells us whenever we have some function F. Then its empirical error will converge in probability to its actual risk.",
                    "label": 0
                },
                {
                    "sent": "So that means whatever slice we take in this direction, the point on the empirical error curve this curve will change as we see more and more training points.",
                    "label": 0
                },
                {
                    "sent": "So this point will converge towards this one in probabilities, or the probability that these two points are more than epsilon part will go to zero.",
                    "label": 0
                },
                {
                    "sent": "Now what we want is something a little bit different.",
                    "label": 0
                },
                {
                    "sent": "What we want is we went to empirical risk mutation which means we take this curve.",
                    "label": 0
                },
                {
                    "sent": "We find the minimizer of the curve.",
                    "label": 0
                },
                {
                    "sent": "We then take this value of this parameter values or take this function and plug it into this this risk and then the we ask the question, is this going to be the minimum of that curve as well?",
                    "label": 0
                },
                {
                    "sent": "So roughly speaking with a minimum of this curve converge to the minimum of this curve provided then we have this pointwise convergence and it turns out the answer is no and I think if you have said some mathematics training in analysis.",
                    "label": 0
                },
                {
                    "sent": "You might have heard this concept of uniform convergence, which roughly speaking means that.",
                    "label": 0
                },
                {
                    "sent": "The whole curve converges to the other one at the same speed, and if you have that then it turns out this is a sufficient condition for the consistency.",
                    "label": 0
                },
                {
                    "sent": "So this is where the uniformity comes from.",
                    "label": 0
                },
                {
                    "sent": "And of course then the question is.",
                    "label": 0
                },
                {
                    "sent": "If we want a uniform convergence over the whole function class, this involves the function class and what function class should be choosing.",
                    "label": 0
                },
                {
                    "sent": "So we might be tempted to say, well, we just should.",
                    "label": 0
                },
                {
                    "sent": "We should, if we choose from, we shouldn't make a restriction a priority.",
                    "label": 0
                },
                {
                    "sent": "We should take all possible functions.",
                    "label": 0
                },
                {
                    "sent": "So all functions that take our input domain in map it into plus minus one 'cause we don't know before in which one is the right one.",
                    "label": 0
                },
                {
                    "sent": "So we have training data.",
                    "label": 0
                },
                {
                    "sent": "We use the class of all functions and we let the rest we let machine learning to the rest, or we did our empirical inference, empirical risk minimization principle, do the rest, and it turns out that cannot work, and it's easy to see.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this cannot work.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to switch to my latex slides for that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I think these are the slides you have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we are now.",
                    "label": 0
                },
                {
                    "sent": "We've now arrived here.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about the size of the function loss.",
                    "label": 0
                },
                {
                    "sent": "You give me one second.",
                    "label": 0
                },
                {
                    "sent": "OK, so why don't we take all functions?",
                    "label": 0
                },
                {
                    "sent": "Well, so let's assume we have some training set in same names as before and someone gives me some test points.",
                    "label": 0
                },
                {
                    "sent": "And for simplicity, let's assume the test points are disjoined from the turn training points.",
                    "label": 0
                },
                {
                    "sent": "We don't want to see exactly the same point again, but if we have some continuous scenario, it's it's basically the probability that this happens is 0.",
                    "label": 0
                },
                {
                    "sent": "So let's assume this is not going to happen.",
                    "label": 0
                },
                {
                    "sent": "And then I claim that if you give me a function F. So suppose you I give you the training points, you give me a function F. You tell me.",
                    "label": 0
                },
                {
                    "sent": "I reckon this is a good solution for my problem.",
                    "label": 0
                },
                {
                    "sent": "Then I'll give you a second function F star, which has exactly the same outputs on all the training points, but which says the opposite on all the test points, and it's easy to construct this function because we are allowing all functions from X 2 + -- 1.",
                    "label": 0
                },
                {
                    "sent": "So I'll just define my function to do this and.",
                    "label": 0
                },
                {
                    "sent": "And therefore my function says the opposite and all the test points.",
                    "label": 0
                },
                {
                    "sent": "Now the for the uniform convergence we are interested in the values of the functions everywhere, and if we compute the risk then it's obvious to you.",
                    "label": 0
                },
                {
                    "sent": "I think I don't have to go through the details that if for this function the risk on training and test set is similar to each other, then for this function because this function says the opposite on the test set for, then for this function the risks on the training and test set can be very different from each other.",
                    "label": 0
                },
                {
                    "sent": "So these two functions have the F&F star have the same risk on the training set.",
                    "label": 0
                },
                {
                    "sent": "But they say the opposite on the test set, so they might have very different risks on the test set.",
                    "label": 0
                },
                {
                    "sent": "So if if for one of these functions.",
                    "label": 0
                },
                {
                    "sent": "If one of these functions that training in the test, there are very near to each other, then for the other function there might be very different from each other and we have no restriction.",
                    "label": 0
                },
                {
                    "sent": "So so roughly speaking in this picture, but I'm telling you if you if you give me function that's here for which these two risks are closed, I can construct another function which is over here for which the two risks are very different.",
                    "label": 0
                },
                {
                    "sent": "So it's not possible that these curves are close to each other everywhere and simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So why is it not possible?",
                    "label": 0
                },
                {
                    "sent": "It's not possible because we were using the class of all functions from X 2 + -- 1.",
                    "label": 0
                },
                {
                    "sent": "So we need a restriction.",
                    "label": 0
                },
                {
                    "sent": "So this is by the way, this is called no free lunch theorem.",
                    "label": 0
                },
                {
                    "sent": "Well, this has.",
                    "label": 0
                },
                {
                    "sent": "This was always known to people in some people in statistics and machine learning, and it was later called the No Free Lunch theorem and so we need a restriction on the class of functions.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we need this everywhere in machine learning.",
                    "label": 0
                },
                {
                    "sent": "In learning theory, the restriction is done in terms of the capacity of a class of functions.",
                    "label": 0
                },
                {
                    "sent": "But also in other areas, like in the basin community, we need to place prior distributions over the class of functions, so we haven't somehow have to restrict the class of functions either in a hard way or in a software.",
                    "label": 0
                },
                {
                    "sent": "And then, ideally, at least in learning theory, we would like to make statements about how strong is restriction.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "What kind of guarantees does it allow us to make?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two so to analyze this a bit.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some details and I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Go into all details because you will have a course next week by any coaches at Yankee who I think will also cover some of this.",
                    "label": 0
                },
                {
                    "sent": "But I think I want to spend maybe half an hour or a bit more on this because it's also useful as basis for some aspects of of kernel methods.",
                    "label": 0
                },
                {
                    "sent": "And anyway, it's if you don't know this stuff, I think it's good to maybe here in the main ideas twice.",
                    "label": 0
                },
                {
                    "sent": "Let's see, so let's not spend ages on it, but I think it's sufficiently interesting so.",
                    "label": 0
                },
                {
                    "sent": "So let's define our or let's take define this shorthand as I for our loss the loss of point X iy I remember this was R 01 loss function.",
                    "label": 0
                },
                {
                    "sent": "Then the first observation is, since this is XIYI drawn independently from the underlying distribution P. So that's our random experiments, like throwing a dice only that when we throw our days we get X&Y.",
                    "label": 0
                },
                {
                    "sent": "So all these are independent trials, so this is.",
                    "label": 0
                },
                {
                    "sent": "These are trials of independent random variable an.",
                    "label": 0
                },
                {
                    "sent": "Actually in this case is the so-called only random variable taking values only zero or one is like tossing a coin.",
                    "label": 0
                },
                {
                    "sent": "So these things are.",
                    "label": 0
                },
                {
                    "sent": "These losses are like coin tosses and our empirical mean.",
                    "label": 0
                },
                {
                    "sent": "Our training error is nothing but summing up how many times we toss heads as opposed to tails.",
                    "label": 0
                },
                {
                    "sent": "And our actual risk is the expected value of this.",
                    "label": 0
                },
                {
                    "sent": "So in the limit, if we do this infinitely many times.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's a nice bones by.",
                    "label": 0
                },
                {
                    "sent": "The American statistician Herman shown off who I believe is still alive, at least a few years ago.",
                    "label": 0
                },
                {
                    "sent": "He still wants, so this is also nice in our field.",
                    "label": 0
                },
                {
                    "sent": "Heroes are still around.",
                    "label": 0
                },
                {
                    "sent": "The field is not sold.",
                    "label": 0
                },
                {
                    "sent": "And this bond basically tells us for this particularly nice situation, these independent trials.",
                    "label": 0
                },
                {
                    "sent": "How fast does the law of large numbers work?",
                    "label": 0
                },
                {
                    "sent": "And his bond tells us this probability of a deviation between this empirical average and its expectation.",
                    "label": 0
                },
                {
                    "sent": "Deviation larger than epsilon is bounded by this exponential quantity, so that's nice exponentially.",
                    "label": 0
                },
                {
                    "sent": "It goes to zero as the exponentially fast as the number of observations M goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And this probability.",
                    "label": 0
                },
                {
                    "sent": "Just to be explicit.",
                    "label": 0
                },
                {
                    "sent": "This is the probability that we obtain a sample, say one through 'cause.",
                    "label": 0
                },
                {
                    "sent": "I am so excited that these are the losses of our training points.",
                    "label": 0
                },
                {
                    "sent": "With the property that this this thing here is larger than website on so this is a probability over this enfold random experiment is so called product measure.",
                    "label": 0
                },
                {
                    "sent": "And there's a consequence that we're going to use the consequence of this which is that.",
                    "label": 0
                },
                {
                    "sent": "If we instead of taking the empirical quantity, needs expectation if we just do this empirical thing twice.",
                    "label": 0
                },
                {
                    "sent": "So we draw two endpoints, they are independent of each other and you can imagine well they're both going to be close to their expectations, so they will also be close to each other, just a little bit further, and you can see that in this bond we have here a factor of 4 instead of two, and here it's also a little bit different.",
                    "label": 0
                },
                {
                    "sent": "So basically the epsilon this replaced by epsilon over 2, then this number 2 moves down in the denominator.",
                    "label": 0
                },
                {
                    "sent": "So so if we have a coin and we do too in trials and we take the average over the first 10 average of the 2nd and then this tells us how likely it is that these averages are very different and you can try it out tonight with the client if you want.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To see if this works, if we translate it back just to be explicit into machine learning, this means the probability of getting a training set where training or intestinal different by more than epsilon is bounded by this quantity.",
                    "label": 1
                },
                {
                    "sent": "That's nice, but as already mentioned before, this is a pointwise statement as opposed to a U1 pointwise meaning it refers to one fixed function F. And we cannot just say, well, let's let's now plug in the function that our learning machine returns.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have a neural network training in data.",
                    "label": 0
                },
                {
                    "sent": "You it.",
                    "label": 0
                },
                {
                    "sent": "In the end you have your parameter value, so you have a function F. You neural network implements and function.",
                    "label": 0
                },
                {
                    "sent": "You might be tempted to say, well, let's take this function and plug it in here and then I know that for my function that I have obtained by training on my training set, the probability that I'm that my training or is far from the test error is upper bounded by this thing.",
                    "label": 0
                },
                {
                    "sent": "Maybe I have seen 10,000 test examples than this quantity will be basically be 0.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately that's not allowed Becausw.",
                    "label": 0
                },
                {
                    "sent": "If we looked at the data before choosing this function, which of course we do.",
                    "label": 0
                },
                {
                    "sent": "If we trained on the data.",
                    "label": 0
                },
                {
                    "sent": "Then we cannot apply the channel bond because our penalty trials become dependent because if we remember from before.",
                    "label": 0
                },
                {
                    "sent": "So these quantities, these losses, they depend on the function, of course, and if this function basically knows something about all training points X one through XM, then even though the xiy are independent, that IR is no longer will be independent, so that's not allowed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have to look at uniform convergence.",
                    "label": 1
                },
                {
                    "sent": "There's no way to avoid it, so this is the one of the main theorems of learning theory and.",
                    "label": 0
                },
                {
                    "sent": "This is we're not going to prove this, but I think I tried to give you a bit of intuition for this and we will prove something else.",
                    "label": 0
                },
                {
                    "sent": "So this theorem tells us that the necessary and sufficient conditions for.",
                    "label": 0
                },
                {
                    "sent": "A certain form of consistency, consistency, meaning we get the right result in the limit for certain consistency of empiricism.",
                    "label": 1
                },
                {
                    "sent": "Isation of this procedure of choosing the function that minimizes the training error.",
                    "label": 0
                },
                {
                    "sent": "Is that we have this kind of uniform convergence 1 sided so I don't have a modulus in here.",
                    "label": 0
                },
                {
                    "sent": "This is a slightly weaker condition.",
                    "label": 0
                },
                {
                    "sent": "Uniform overall functions that the learning machine can implement, and.",
                    "label": 0
                },
                {
                    "sent": "If you want, you can put a modulus here.",
                    "label": 1
                },
                {
                    "sent": "In that case you would get a condition for it.",
                    "label": 0
                },
                {
                    "sent": "Consistency of both empirical risk minimization and empirical risk maximization, but normally we are not interested in empirical risk maximization.",
                    "label": 0
                },
                {
                    "sent": "We don't choose the function that maximizes the training error, so we need this property to be true.",
                    "label": 0
                },
                {
                    "sent": "This kind of uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "Another question is, are there properties of the learning machine properties of this class of function which will ensure that this uniform conversions can take place?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this will lead us to capacity concepts and things like the VC dimension and I think it's nice to see this to see these ones, even if it's just that you're not scared of VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at this quantity.",
                    "label": 0
                },
                {
                    "sent": "So this probability that the training error and the test error very different.",
                    "label": 0
                },
                {
                    "sent": "So which means that if you look at the training data with your learning machine, you look how well you're doing, this might mislead you about the future performance.",
                    "label": 0
                },
                {
                    "sent": "Now first of all, if our function class has only one point only one function.",
                    "label": 0
                },
                {
                    "sent": "Then of course we can use the standard Journal font because in that case.",
                    "label": 0
                },
                {
                    "sent": "Uniform convergence is the same as pointwise convergence.",
                    "label": 0
                },
                {
                    "sent": "There's nothing to choose from, we just choose the one function that we have.",
                    "label": 0
                },
                {
                    "sent": "In this case, our loss vectors or our losses are not dependent, and where you can apply Chernov in that Channel found as I stated before.",
                    "label": 0
                },
                {
                    "sent": "Tells us that in this case we have this nice exponentially fast convergence.",
                    "label": 0
                },
                {
                    "sent": "The next step will be if we have finitely many functions.",
                    "label": 0
                },
                {
                    "sent": "We basically do more or less the same.",
                    "label": 0
                },
                {
                    "sent": "We use a certain trick.",
                    "label": 0
                },
                {
                    "sent": "We lose a bit here.",
                    "label": 0
                },
                {
                    "sent": "We get another factor in here, but it doesn't cost us much because we still have this exponential thing.",
                    "label": 0
                },
                {
                    "sent": "Now, the most interesting cases if we have infinitely many functions.",
                    "label": 0
                },
                {
                    "sent": "So if you have infinitely many, then of course UU statement is much stronger than a point by a statement.",
                    "label": 0
                },
                {
                    "sent": "In that case, the main trick will be that whenever we have only finitely many training points, even though the function shots might be infinite, if we have finitely many training points, then effectively it looks as if we only have finitely many functions, because the functions can only take outputs plus minus one.",
                    "label": 0
                },
                {
                    "sent": "We have finitely many points.",
                    "label": 0
                },
                {
                    "sent": "There are finitely many possible output vectors that we can generate and therefore the function class looks finite.",
                    "label": 0
                },
                {
                    "sent": "So what we will be interested in is how large does a function class look on a certain number of training points that will be the crucial thing form to measure the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acity of function class.",
                    "label": 0
                },
                {
                    "sent": "But let's first, let's first do the case of finitely many functions.",
                    "label": 1
                },
                {
                    "sent": "Even starting with just two functions.",
                    "label": 0
                },
                {
                    "sent": "So if we have two functions.",
                    "label": 0
                },
                {
                    "sent": "Column F1F2 so remember we were interested in this thing happening.",
                    "label": 1
                },
                {
                    "sent": "We were interested in when.",
                    "label": 0
                },
                {
                    "sent": "How likely is it that the training error is very different from the test error?",
                    "label": 0
                },
                {
                    "sent": "In that case, well then and there is a supremum over the function class.",
                    "label": 0
                },
                {
                    "sent": "Well, if the function has two functions, then of course this event here is the same as the union of two events where the 1st event describes the that.",
                    "label": 1
                },
                {
                    "sent": "For the first function training error test, there are very different.",
                    "label": 0
                },
                {
                    "sent": "The 2nd event is just drives the event where for the second function training tests are very different and events I told you before this is a product measure over drawing the training set.",
                    "label": 0
                },
                {
                    "sent": "So events always means that someone gives me a training set with a certain property.",
                    "label": 0
                },
                {
                    "sent": "So this is the event that's the training set is such that for the first functions training and test error are very different.",
                    "label": 0
                },
                {
                    "sent": "So my training error.",
                    "label": 0
                },
                {
                    "sent": "I always mean this is empirical risk here, and likewise for the second function and.",
                    "label": 0
                },
                {
                    "sent": "So we rewrite all probabilities like this and then we can see that this probability can be first described, like the probability that sums of the probability of the two events minus the intersection is elementary probability theory.",
                    "label": 0
                },
                {
                    "sent": "And since all probabilities are non negative, we can upper bound this by the sum of these two events.",
                    "label": 0
                },
                {
                    "sent": "So we lose a little bit when both of them are misleading, but we have an upper bound and therefore we can upper bound this by two probabilities which now refer to only one function each.",
                    "label": 0
                },
                {
                    "sent": "We know how to deal with one function, we use the Journal Font, so we get this exponential decay as before this quantity, only with the factor of two here.",
                    "label": 1
                },
                {
                    "sent": "OK so far.",
                    "label": 0
                },
                {
                    "sent": "So I think you can if you have understood this, you can only imagine what happens if we take in functions for infant, since we do exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So any fine.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add a number of functions.",
                    "label": 0
                },
                {
                    "sent": "We re write this probability as the probability of this union of any events.",
                    "label": 0
                },
                {
                    "sent": "Each of these events can be, well.",
                    "label": 0
                },
                {
                    "sent": "First, we upper bound these union by some of the probabilities.",
                    "label": 0
                },
                {
                    "sent": "This is again the so called union bound.",
                    "label": 0
                },
                {
                    "sent": "We use the Journal for inequality for each term, and in this case we get an extra factor on and off in the right hands.",
                    "label": 0
                },
                {
                    "sent": "Now the difficult case and this extra extra factor of NI should say it doesn't cost.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As much because.",
                    "label": 0
                },
                {
                    "sent": "If we have any instead of two here, but this thing here goes to 0 so fast that this is basically we can ignore that factor of in.",
                    "label": 0
                },
                {
                    "sent": "So as long as we don't have anything here that goes up exponentially with them and we're in business.",
                    "label": 0
                },
                {
                    "sent": "Of course, in the infinite case we might have things that go up exponentially with him and let's get to this case.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea, and so it's clear, so we cannot.",
                    "label": 0
                },
                {
                    "sent": "Just we cannot do.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same triggers before, right?",
                    "label": 0
                },
                {
                    "sent": "If we have infinite number over here, then we are lost.",
                    "label": 0
                },
                {
                    "sent": "So we have to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do some tricks and as mentioned before, the trick will be that the Imperial risks.",
                    "label": 0
                },
                {
                    "sent": "The training only refers to endpoints and our functions taken values, output values plus minus one.",
                    "label": 0
                },
                {
                    "sent": "Small functions on these endpoints can take at most 2 to the power of N values.",
                    "label": 0
                },
                {
                    "sent": "So on endpoints.",
                    "label": 0
                },
                {
                    "sent": "The function class has effectively maximum size of two to the power of M. There's a question.",
                    "label": 0
                },
                {
                    "sent": "Indicates that previous example sets are independent or.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is these are the different sets they could.",
                    "label": 0
                },
                {
                    "sent": "They could overlap.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be disjoint.",
                    "label": 0
                },
                {
                    "sent": "I guess that's what you mean, it's like.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "So if there are these, if they are not disjoint, then this quantity here is non zero, which means when we are doing this upper bound we're losing something.",
                    "label": 0
                },
                {
                    "sent": "If they were disjoint, then we would be losing less than our boy bound in the end would be sharper, but with so we might lose a lot.",
                    "label": 0
                },
                {
                    "sent": "If these events overlap.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so I mean sort of disjointness of events is something that's similar to what you mean by independence, right, independence, property, property of probability measures?",
                    "label": 0
                },
                {
                    "sent": "If we just look at the events we would normally be talking about whether they are disjoint or mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so we use a trick to reduce.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The infinite case to the finite case, and this is the so.",
                    "label": 0
                },
                {
                    "sent": "Cause Symmetrization lemma public in Germany in keys and is.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to prove this but I think you will believe me when you hear it.",
                    "label": 0
                },
                {
                    "sent": "This lemma says that the probability that.",
                    "label": 0
                },
                {
                    "sent": "Training on test error.",
                    "label": 0
                },
                {
                    "sent": "Differ by more than epsilon can be upper bounded by the twice the probability that two different training errors differ from each other by more than epsilon over 2.",
                    "label": 0
                },
                {
                    "sent": "So if these two things if by drawing two sets of training points.",
                    "label": 0
                },
                {
                    "sent": "If you if by doing this you cannot get.",
                    "label": 0
                },
                {
                    "sent": "Training errors that are very different from each other.",
                    "label": 0
                },
                {
                    "sent": "Then we can also guarantee that the training error on the test error cannot be very different.",
                    "label": 0
                },
                {
                    "sent": "It's again some kind of triangle inequality type argument.",
                    "label": 0
                },
                {
                    "sent": "So which means now we need a training set of twice the size.",
                    "label": 0
                },
                {
                    "sent": "And maybe maybe we over here ignore this down here.",
                    "label": 0
                },
                {
                    "sent": "Now this is just for the details.",
                    "label": 0
                },
                {
                    "sent": "If you want to look at it afterwards again.",
                    "label": 0
                },
                {
                    "sent": "But basically it means rather than.",
                    "label": 0
                },
                {
                    "sent": "Having this quantity here, which is the integral over the whole probability distribution that we don't know if we're only interested in upper bounding this quantity, we can first upper bounded by this thing here.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about this is that this is a term that only involves endpoints.",
                    "label": 0
                },
                {
                    "sent": "This is a term that only involves endpoints, so if we can somehow upper bound these guys by using some tricks that rely on the function class being effectively finite, we can do it over here because this.",
                    "label": 0
                },
                {
                    "sent": "Quantities were filled to finitely many points, and as I mentioned before, in that case, the function classes are effectively.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Final engine.",
                    "label": 0
                },
                {
                    "sent": "So what we are therefore interested in is the maximum size of our function class on 2M points.",
                    "label": 0
                },
                {
                    "sent": "And this is sometimes called the shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "We denoted by this calligraphic in.",
                    "label": 0
                },
                {
                    "sent": "So this can be also generalized to the case of real valued outputs, functions and people look at things like covering numbers in that case, but we only look at classification here and at this shattering coefficient, and let me remind you what it means.",
                    "label": 0
                },
                {
                    "sent": "So the size of a function Class A + -- 1 valued function class.",
                    "label": 0
                },
                {
                    "sent": "On two endpoints is the maximum number of different outputs that the function class can generate on two endpoints.",
                    "label": 0
                },
                {
                    "sent": "He said we will consider two functions that are that give the same outputs on our two endpoints.",
                    "label": 0
                },
                {
                    "sent": "We will consider them equivalent.",
                    "label": 0
                },
                {
                    "sent": "There is no way for us to distinguish them, and therefore we only interested in how many such functions there are.",
                    "label": 0
                },
                {
                    "sent": "And if you think of the output vectors of the output vectors are the entries are plus minus one there plus one whenever we have.",
                    "label": 0
                },
                {
                    "sent": "Correctly separated the points, there are minus one.",
                    "label": 0
                },
                {
                    "sent": "If we have done it the other way round.",
                    "label": 0
                },
                {
                    "sent": "So if we have assigned a point to the other class, sorry there plus one or no.",
                    "label": 1
                },
                {
                    "sent": "Actually this that's true there plus one or minus one, so I was not sorry I have to take back what I just said so I could also do this in terms of the loss functions, but it's not done like this here.",
                    "label": 0
                },
                {
                    "sent": "So these are just plus 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "How many output vectors can we generate on two endpoints?",
                    "label": 0
                },
                {
                    "sent": "In other words, how many?",
                    "label": 0
                },
                {
                    "sent": "How many different ways?",
                    "label": 0
                },
                {
                    "sent": "Can we wait ways?",
                    "label": 0
                },
                {
                    "sent": "Can we classify the points into classes?",
                    "label": 1
                },
                {
                    "sent": "And of course, each point could go either in the one class on the other one, and there is an upper bound.",
                    "label": 0
                },
                {
                    "sent": "This quantity, which is 2 to the power of two him.",
                    "label": 0
                },
                {
                    "sent": "And if this upper bound is actually attained, then the function class is set to shatter the two endpoints.",
                    "label": 0
                },
                {
                    "sent": "It means we can realize all possible class assignments using functions from our class, so intense the function class has maximum richness or maximum.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City on sets of points of size to him.",
                    "label": 0
                },
                {
                    "sent": "So now we put everything together to get our bond.",
                    "label": 0
                },
                {
                    "sent": "So we first use the symmetrization so this symmetrization lemma to get rid of this test error.",
                    "label": 0
                },
                {
                    "sent": "This quantity that depends on the unknown probability measure.",
                    "label": 0
                },
                {
                    "sent": "That's an integral over lips probability measure.",
                    "label": 0
                },
                {
                    "sent": "To get two quantities that are both only depending on endpoints.",
                    "label": 0
                },
                {
                    "sent": "We then say another cheating a tiny little bit here, and if you want to know details, I can either give you something to read or I can tell you afterwards, but I don't want to go into great detail here.",
                    "label": 0
                },
                {
                    "sent": "Then I'm basically saying well now my function class on these two endpoints has size calligraphic in this shattering coefficient.",
                    "label": 1
                },
                {
                    "sent": "So that means I have a colleague drive I can effectively the size of the function classes this thing here.",
                    "label": 0
                },
                {
                    "sent": "So I pick functions F1 through F sub N. I rewrite this as a union of all these events.",
                    "label": 0
                },
                {
                    "sent": "Then I apply the Union bounds so each of these things.",
                    "label": 0
                },
                {
                    "sent": "Basically upper bound, this by a sum of probabilities, where now I have one probability for each of these terms and it's a sum over calligraphic in.",
                    "label": 0
                },
                {
                    "sent": "Such probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I can use the general font for each term.",
                    "label": 0
                },
                {
                    "sent": "So this is this second form of the channel found that was referring to two empirical quantities.",
                    "label": 0
                },
                {
                    "sent": "How far they deviate from each other.",
                    "label": 0
                },
                {
                    "sent": "For plug this in, then what I get is I have this shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "I still have some that goes over this number of terms and for each term I get something like this and if I put this together I get this kind of found.",
                    "label": 0
                },
                {
                    "sent": "What does this tell us?",
                    "label": 0
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "This tells us that if the shattering coefficient if this somehow is well behaved, in particular if it doesn't grow exponentially in EM.",
                    "label": 1
                },
                {
                    "sent": "Then the second term will always win, because this decays decays exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "So in that case the right hand side will go to 0.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm speaking differently now.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm getting too emotional that the bound is closed.",
                    "label": 0
                },
                {
                    "sent": "This is the so called Optic Shevlin English time inequality, so this is a people called this a tail bound in statistics and.",
                    "label": 0
                },
                {
                    "sent": "Maybe just to discuss it a little bit.",
                    "label": 0
                },
                {
                    "sent": "So there are two types of randomness in here.",
                    "label": 0
                },
                {
                    "sent": "One is that this P over here.",
                    "label": 0
                },
                {
                    "sent": "Refers to drawing training.",
                    "label": 0
                },
                {
                    "sent": "Examples were drawing training examples from our unknown underlying probability distribution, so this is.",
                    "label": 0
                },
                {
                    "sent": "This tells us how likely is it that we get a training set for which this is the case.",
                    "label": 0
                },
                {
                    "sent": "And the second randomness is that here on these risks these are an expectation over the same probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, this one is a product measure as a product of these distributions.",
                    "label": 0
                },
                {
                    "sent": "This is the distribution itself.",
                    "label": 0
                },
                {
                    "sent": "So here we also have we're using this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We compute this expectation and here of course this is the risk.",
                    "label": 0
                },
                {
                    "sent": "So is the expectation over the whole distribution.",
                    "label": 0
                },
                {
                    "sent": "In other words, we think of this as the test error.",
                    "label": 0
                },
                {
                    "sent": "And we can also rewrite this bound if we want.",
                    "label": 0
                },
                {
                    "sent": "So now here in this form the bound tells us that is very unlikely for training error and test error to be very different epsilon different through unlikely.",
                    "label": 0
                },
                {
                    "sent": "This term will be small for large trains and sizes.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can rewrite this.",
                    "label": 0
                },
                {
                    "sent": "By specifying the probability with which we want the risks to be closed through the training error and then solving for epsilon.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is a nice exercise if you want to play around a bit with it, and we can then rewrite this bond into lines.",
                    "label": 0
                },
                {
                    "sent": "And then in that case the bound will tell us that with the probability of at least one minus Delta this.",
                    "label": 0
                },
                {
                    "sent": "And test error is upper bounded by the training error plus some quantity that depends on this shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "And here you can also see this thing here will go to zero as N goes to Infinity and if this shattering coefficient does not grow exponentially in M, this thing here will always win against this, because here we have a logarithm and this bond is.",
                    "label": 1
                },
                {
                    "sent": "This is not a typical learning theory bond.",
                    "label": 0
                },
                {
                    "sent": "It's kind of surprising that something like this is true.",
                    "label": 0
                },
                {
                    "sent": "It holds independent of the function, so this holds uniformly over the whole function class.",
                    "label": 0
                },
                {
                    "sent": "In particular, it holds for the function minimizing the risks.",
                    "label": 0
                },
                {
                    "sent": "So if we choose our function by minimizing some kind of training error, then this bone still holds because it holds for all functions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe let's see.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should discuss this little bit actually, but actually, let's first talk a bit more about capacity concepts, and I think in the end I will have will stay there.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something like this bound again.",
                    "label": 1
                },
                {
                    "sent": "So this is not the best possible bound.",
                    "label": 0
                },
                {
                    "sent": "You can give.",
                    "label": 0
                },
                {
                    "sent": "People have looked at constants and so on in detail and more sophisticated ways of doing things.",
                    "label": 0
                },
                {
                    "sent": "That's one comment.",
                    "label": 1
                },
                {
                    "sent": "The other comment is we cannot minimize the bound over the function, so that's something that maybe we would like to do.",
                    "label": 1
                },
                {
                    "sent": "'cause in the end we want a function that generalizes well.",
                    "label": 0
                },
                {
                    "sent": "So we might say, well, OK, you might say, OK, you've convinced me that I shouldn't be minimizing just the training error, so I gave you an example where we use the class of all possible functions.",
                    "label": 0
                },
                {
                    "sent": "We are in the training error is basically meaningless.",
                    "label": 0
                },
                {
                    "sent": "The tester can be arbitrarily different even though the training errors are the same.",
                    "label": 0
                },
                {
                    "sent": "So maybe you're convinced the training error is not the whole story, but you might say, well, in that case let's just minimize this right hand side.",
                    "label": 0
                },
                {
                    "sent": "And of course we would like to do something like that.",
                    "label": 0
                },
                {
                    "sent": "We cannot do it directly, because OK, this is a property of the function here, but this over here is not a property of the function, it's a property of the function class they were choosing from.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "What is the solution that you came up with at the end?",
                    "label": 0
                },
                {
                    "sent": "But it matters what set of solutions you were choosing it from.",
                    "label": 0
                },
                {
                    "sent": "So if you give someone on training, set some machine learning guy and he comes back and says here I have a solution for you then you cannot assess whether this is a good solution or not.",
                    "label": 0
                },
                {
                    "sent": "You should you should first ask him well which solutions did you choose from and he said I chose from a huge class for solutions and you could say, well that's probably rubbish.",
                    "label": 0
                },
                {
                    "sent": "What you're telling me if he chose from a small set of solutions.",
                    "label": 0
                },
                {
                    "sent": "Apriori solutions to a small set of functions that he has chosen a prior before looking at the training data.",
                    "label": 0
                },
                {
                    "sent": "And nevertheless from that small set he can give you a particular function which does well on the training data.",
                    "label": 0
                },
                {
                    "sent": "Then you're in business.",
                    "label": 0
                },
                {
                    "sent": "Then it looks like this looks, and then it's likely.",
                    "label": 0
                },
                {
                    "sent": "High probability that the tester would also be good.",
                    "label": 0
                },
                {
                    "sent": "So we have to do this in a more sophisticated way if we want to minimize this over functions, we have to somehow simultaneously control the size of the function class and minimize within the classes, and this would be called structural risk minimisation.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I just want to briefly mention.",
                    "label": 0
                },
                {
                    "sent": "And the main capacity measures from statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "So I already told you that about this shattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "That's not the whole story, and the rest of us are the most accurate capacity measure which is rarely used is called the VC entropy.",
                    "label": 0
                },
                {
                    "sent": "So you're probably not going to use it versus, but it's good to know that this exists because people sometimes.",
                    "label": 0
                },
                {
                    "sent": "Think VC theory is identical to this kind of worst case theory where it has to hold uniform over all possible probability distributions, and that's not necessarily the case, so we can also give quantities that depend on probabilities distributions only that those are difficult to handle in practice.",
                    "label": 0
                },
                {
                    "sent": "So remember again we have this sign which is our loss on an example XY loss.",
                    "label": 0
                },
                {
                    "sent": "01 So now this is in terms of the loss vectors.",
                    "label": 0
                },
                {
                    "sent": "If we have a sample of training points and we cycle through all our functions in the function class, we get a whole set of loss vectors that we can obtain and the cardinality of this set is denoted as this color graphic in and the VC entropy.",
                    "label": 0
                },
                {
                    "sent": "So now here we don't have any supremum maximum.",
                    "label": 0
                },
                {
                    "sent": "In shattering coefficient we had a supremum.",
                    "label": 0
                },
                {
                    "sent": "The VC entropy is now defined as the expectation of the logarithm of that number, so this is the expectation taken over generating this M sample by an info experiment according to our underlying regularity P. So this is sort of like the expected complexity or the expected log size of the function class, and it turns out that.",
                    "label": 0
                },
                {
                    "sent": "This thing here, well that uniform convergence of the risks and therefore consistency, is actually equivalent to this thing, growing subexponential sublinearly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if this VC entropy divided by M goes to 0, this isn't if and only if conditioned for uniform convergence.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Risks.",
                    "label": 0
                },
                {
                    "sent": "Now if we loosen things a little bit, we can exchange the expectation and the logarithm so.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "If we explain exchange this expectation in the logarithm, remember the logarithm is a convex function or concave depends.",
                    "label": 0
                },
                {
                    "sent": "Which way you define it anyway, you will get an upper bound in that case.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ace and.",
                    "label": 0
                },
                {
                    "sent": "Therefore this this thing here going to 0 is a stronger condition than the other one, 'cause this upper bounds is so called annealed entropy upper bounds.",
                    "label": 0
                },
                {
                    "sent": "The VC entropy, and it turns out that this will be even only condition for exponentially fast uniform convergence.",
                    "label": 1
                },
                {
                    "sent": "So it turns out in the other case you could get uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "That's arbitrarily slow, which they didn't practice might also be useless because.",
                    "label": 0
                },
                {
                    "sent": "For every given training set size, you might still be very far away from having converged.",
                    "label": 0
                },
                {
                    "sent": "OK, so then the next step of making things loser getting an upper bound again is to take the maximum instead of the expectation, in which case you get something called in growth function, which is essentially this shattering coefficient that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "So he said logarithm of the shattering coefficient, and then this condition that the growth function growth grows sublinearly in the limit is equivalent to exponential convergence, independent of the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "So that means no matter.",
                    "label": 0
                },
                {
                    "sent": "What distribution generated my training and test points?",
                    "label": 0
                },
                {
                    "sent": "It will be the case that I get uniform convergence of.",
                    "label": 0
                },
                {
                    "sent": "For risks of training error towards test error.",
                    "label": 0
                },
                {
                    "sent": "That's nice, because remember, we need the uniform convergence for consistency.",
                    "label": 0
                },
                {
                    "sent": "So consistency was this question whether our procedure or minimizing the training error leads to the correct result in the limit.",
                    "label": 0
                },
                {
                    "sent": "So we would like that to be true.",
                    "label": 0
                },
                {
                    "sent": "And of course we would like it to be true.",
                    "label": 0
                },
                {
                    "sent": "Normally we would like to be true independent of the underlying probability distribution, because we don't know the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point about learning.",
                    "label": 0
                },
                {
                    "sent": "We only have a sample from the distribution, so we would like these statements to be independent, and in that case we could use this.",
                    "label": 0
                },
                {
                    "sent": "Growth function.",
                    "label": 0
                },
                {
                    "sent": "If we can compute it for a different class of functions or learning machine.",
                    "label": 0
                },
                {
                    "sent": "Which will ensure this.",
                    "label": 0
                },
                {
                    "sent": "Now remember that I mentioned before this shattering coefficient in the worst case, if the function is maximally, region will grow exponentially.",
                    "label": 0
                },
                {
                    "sent": "Growth function is used logarithm, so if we translate this into growth functional language, it means the growth function will grow linearly, which means just to remind you briefly because we need it on the next slide.",
                    "label": 0
                },
                {
                    "sent": "It means that we can generate all loss vectors or in other words we can.",
                    "label": 0
                },
                {
                    "sent": "We can find points no matter what is M we can find endpoints such that by using functions of the learning machine we can generate all those vectors or that by using functions of learning machine we can separate them in all two to the M possible ways.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's this surprising result also due to buffering in Germany and keys.",
                    "label": 0
                },
                {
                    "sent": "But also this has been proven by others by some on a little bit later and by Sheila.",
                    "label": 0
                },
                {
                    "sent": "So this is sometimes called Summers lemma, even though it was first proven by happening in German Yankees and.",
                    "label": 0
                },
                {
                    "sent": "It's a company tutorial lemma about the structure of this growth function and the surprising thing is that.",
                    "label": 0
                },
                {
                    "sent": "It turns out the growth function, so I told you before in the worst possible case, or in the case where the function class has maximum richness, the growth function grows linearly even in the limits.",
                    "label": 0
                },
                {
                    "sent": "So for all M. But it turns out that if this doesn't happen, then we have this nice upper bar.",
                    "label": 0
                },
                {
                    "sent": "This nice look arhythmic upper bound, so there's nothing in between linear and logarithmically can only take one of these two behaviors.",
                    "label": 0
                },
                {
                    "sent": "So either it has full richness.",
                    "label": 0
                },
                {
                    "sent": "Or this function grows linearly for awhile, so up to some maximum.",
                    "label": 0
                },
                {
                    "sent": "This number is called Missy dimension, and then afterwards suddenly we have this.",
                    "label": 0
                },
                {
                    "sent": "It's very slow growth afterwards, so suddenly after that it's only logarithmically him, so it starts linear in M and then it becomes logarithmic.",
                    "label": 0
                },
                {
                    "sent": "So this growth function will typically in the case that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "Of course we in the cases where we don't have maximum complexity.",
                    "label": 0
                },
                {
                    "sent": "To see if we have maximum complexity, we cannot guarantee generalization, so we are interested in this second case, and the surprising thing is that in this second case, growth function grows linearly up to a certain point, and then suddenly we have this local rhythmic upper bound.",
                    "label": 0
                },
                {
                    "sent": "So then the complexity of the function class grows very slowly suddenly.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Kind of surprising.",
                    "label": 0
                },
                {
                    "sent": "So up to a certain size of the training set, it looks like the function class is very rich and afterwards it's suddenly small and that's the regime in which we can.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generalize.",
                    "label": 0
                },
                {
                    "sent": "So let me give you an example of this so called VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So remember I said the VC dimension at that point up to which the growth function grows linearly.",
                    "label": 1
                },
                {
                    "sent": "So up to which the complexity of function task is maximum.",
                    "label": 0
                },
                {
                    "sent": "So let's take this function class, which is the which are half spaces in R2.",
                    "label": 0
                },
                {
                    "sent": "So they separated the space into an area where the value is a plus one.",
                    "label": 0
                },
                {
                    "sent": "An area where the value is minus one.",
                    "label": 0
                },
                {
                    "sent": "To specify these last functions, we need 3 parameters.",
                    "label": 0
                },
                {
                    "sent": "And we can ask the question, how rich is this function class so up to which number of points is this maximally rich?",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll drink a bit while you meditate on this issue.",
                    "label": 0
                },
                {
                    "sent": "So let's let's try it out.",
                    "label": 0
                },
                {
                    "sent": "Let's take three points.",
                    "label": 0
                },
                {
                    "sent": "So if we take three points, we put them in general position.",
                    "label": 0
                },
                {
                    "sent": "And now we try to realize different separations of these points, so we can assign these points in different classes.",
                    "label": 0
                },
                {
                    "sent": "Three points.",
                    "label": 0
                },
                {
                    "sent": "We have 2 * 2 * 2 possibilities overall, because each point can be plus one or minus one, so we have eight different ways and I can hear I can show you 8 different ways of classifying these three points.",
                    "label": 0
                },
                {
                    "sent": "And you can see that I can realize all these 8 three different separations.",
                    "label": 0
                },
                {
                    "sent": "So this is a graphical proof that the VC dimension is at least three because for three points I still have maximum richness.",
                    "label": 0
                },
                {
                    "sent": "Now the question is what happens if we have 4 points and that I I don't don't prove, but I think you're going to believe me.",
                    "label": 0
                },
                {
                    "sent": "Or you can try it out if you want.",
                    "label": 0
                },
                {
                    "sent": "But if I put four points, there will always be a situation where there's two points that I cannot separate from the other tool by using a straight line.",
                    "label": 0
                },
                {
                    "sent": "No matter how you put them, there will always be a class assignment where you would like to use a done linear function which we are not allowed to do in this case.",
                    "label": 0
                },
                {
                    "sent": "Because we're doing the pieces I mentioned of this class of linear functions where we call this linear functions because the separation lines are linear.",
                    "label": 0
                },
                {
                    "sent": "So in this case there for the Visit Dimension is 3, which is noteworthy also because in this case it's identical to the number of parameters of the system.",
                    "label": 0
                },
                {
                    "sent": "So this has sometimes misled people in thinking that the reason I mention is equal to the number of parameters or maybe also there used to be this this belief in statistics that the number of parameters is somehow a meaningful.",
                    "label": 0
                },
                {
                    "sent": "Meaningful way of measuring the complexity of a set of functions, and maybe sometimes this is the case, but there are cases where the number of parameters is very small and still the complexity is large and vegetables are there cases where the number of parameters is very large and the complexity is small, so it's not really the right quantity.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we now plug in the VC dimension in our bounds, remember the VC dimension gave us this upper bound on the growth function.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have this other volunteer in terms of the museum mention age.",
                    "label": 0
                },
                {
                    "sent": "On the growth function and we could we have we had before about and that was essentially using the growth function or actually exponential of the growth function this chat.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In coefficient, if we now plug it in the VC dimension, we get up on in terms of the VC dimension, which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So you can see here if the VC dimension is finite, which is the case that we are interested in, then here on the right hand side this goes to zero as 1 / M and this grows logarithmically in M. But of course love Arhythmic growth is not as strong as going to 0 and 1 / M. So whenever age is finite this quantity here will go to 0.",
                    "label": 0
                },
                {
                    "sent": "Ender maybe it's a it's worth thinking about this bound for a minute.",
                    "label": 0
                },
                {
                    "sent": "What does it tell us?",
                    "label": 0
                },
                {
                    "sent": "So it tells us that.",
                    "label": 0
                },
                {
                    "sent": "If we can achieve training error training error, let's small by using class of functions that has a small BC dimension then we can guarantee that the test error is small and so is it something that's does it sound like it's logically impossible?",
                    "label": 0
                },
                {
                    "sent": "That sounds like it's solving the induction problem where it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not really solving the induction problem, because if we if we saw if we try to learn something that cannot be learned.",
                    "label": 0
                },
                {
                    "sent": "So there's a nice example of taking the telephone directory.",
                    "label": 0
                },
                {
                    "sent": "If you take that elephant director of some city.",
                    "label": 0
                },
                {
                    "sent": "And you try to predict the telephone number of a person from their name.",
                    "label": 0
                },
                {
                    "sent": "So you have a large training set.",
                    "label": 0
                },
                {
                    "sent": "If you take a big city.",
                    "label": 0
                },
                {
                    "sent": "And you can try to train a huge neural network on this.",
                    "label": 0
                },
                {
                    "sent": "If you take a large enough neural network, maybe it will work such that you have a small training error, but in that case you will find.",
                    "label": 0
                },
                {
                    "sent": "So there's more training or Boost Mobile.",
                    "label": 0
                },
                {
                    "sent": "In that case we will find that the network is so large that effectively its complexity will be or you will have to.",
                    "label": 0
                },
                {
                    "sent": "You will have to work with such a huge class of networks to begin with.",
                    "label": 0
                },
                {
                    "sent": "That VC dimension will be very large, so then this second term will blow up and you don't know whether you can generalize.",
                    "label": 0
                },
                {
                    "sent": "And so, and on the other hand, if you take a function class, whether we see that mention is small, and then you will probably find that with that small function class you won't find a function that gives you a small training error.",
                    "label": 0
                },
                {
                    "sent": "So in that case the first term will blow up, so you will never be able to guarantee something that's unreasonable like this telephone directory thing, or a second example is to try to predict the character of a person from their birth date.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's more controversial.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Now the picture I mentioned already before structure is musician.",
                    "label": 0
                },
                {
                    "sent": "So if we want to minimize the right hand sides so 5 minutes to the breaker, or actually you're being polite is 0 minutes.",
                    "label": 0
                },
                {
                    "sent": "But I think maybe let me see if this is a good point.",
                    "label": 0
                },
                {
                    "sent": "Actually is a perfect point, so I have just two more slides and we stop.",
                    "label": 0
                },
                {
                    "sent": "So a structure minimization.",
                    "label": 0
                },
                {
                    "sent": "We had these two terms.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This term, this time we went to have both of them all at the same time.",
                    "label": 0
                },
                {
                    "sent": "To make this more, we just minimize over class of functions, find them when it hits the smallest training error.",
                    "label": 1
                },
                {
                    "sent": "To make this small, we somehow have to vary the size of the function class.",
                    "label": 0
                },
                {
                    "sent": "And one way to do it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is is this procedure cost structures minimization which introduces this nested set of sets on the function class.",
                    "label": 0
                },
                {
                    "sent": "So you could say for instance this is new, all neural networks with N -- 1 hidden nodes or something like this, and this is the class of networks that have up to N hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "So this is some way of increasing the size of the class of functions and then for each of these we compute the VC dimension and then the prescription is.",
                    "label": 0
                },
                {
                    "sent": "But in each of these we should also find the empirical risk minimizer, and then we always sum up the training error that we get for the risk minimizer.",
                    "label": 0
                },
                {
                    "sent": "Plus this second term that involves the VC dimension and then we will have this tradeoff between the training error that goes to 0 minimum training or goes to zero as the complexity increases.",
                    "label": 0
                },
                {
                    "sent": "This second term will grow logarithmically and then we sum up these two things.",
                    "label": 0
                },
                {
                    "sent": "Infrastructure is memorization.",
                    "label": 0
                },
                {
                    "sent": "Tells us this is the point where we just should be working, so this is the VC dimension we should choose.",
                    "label": 0
                },
                {
                    "sent": "And in that class we should take the the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function that minimizes the visit minimizes the training error.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe just two to get to a conclusion.",
                    "label": 0
                },
                {
                    "sent": "I tried to explain to you that for in order to guarantee generalization and guaranteeing generalization, always means guaranteeing that the training error does not mislead us about the test error.",
                    "label": 1
                },
                {
                    "sent": "So by in order to guarantee generalization, we need to take into account the size of a function class.",
                    "label": 0
                },
                {
                    "sent": "And to take into account the size of function task, we need the right measures to measure the size of the function class and these measures are typically combinatorial quantities that measure how rich the function classes depending on the number of points we look at.",
                    "label": 0
                },
                {
                    "sent": "So generalized, we need to control these quantities.",
                    "label": 0
                },
                {
                    "sent": "We looked at an example where the VC dimension in two dimensions for separating elements was three.",
                    "label": 0
                },
                {
                    "sent": "In N dimensions, it will turn out that we see dimension is N + 1.",
                    "label": 0
                },
                {
                    "sent": "So if we have a high dimensional problems dimension could be quite large and in the support vector machines we will work with very high dimensional cases, maybe even infinite dimensional cases, and in that case we will need ways to bond the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Other than using this trivial bound and I will tell you something about these kinds of function classes later on, I will also show you how to give an upper bound and the VC dimension of function classes that could even be in infinite dimensional spaces, but where we take into account in additional quantity called the margin of separation, and this will fit together nicely with kernel functions be kernel, because kernels are a way of generating such function classes in high dimensional spaces, or kernels are way of.",
                    "label": 0
                },
                {
                    "sent": "Generating function classes that are not nonlinear, but that correspond to linear function classes in very high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do the theoretical analysis in these high dimensional spaces where the functions are linear.",
                    "label": 0
                },
                {
                    "sent": "We can handle the linear function classes, but in practice it will give us nonlinear function classes which we like to use for real world problems, because remember in real world problems in the bounds we always have two terms, one is the complexity and one is the.",
                    "label": 0
                },
                {
                    "sent": "Empirical error and if we have a complicated problem, we won't be able to achieve a low empirical error using just linear function classes, at least often.",
                    "label": 0
                },
                {
                    "sent": "We won't be, so this is the kind of goals for the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next few lectures and now I should really let you have your coffee.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}