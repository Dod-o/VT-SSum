{
    "id": "6xyzbpjinmktlznxkcrcqn3dg5gyx36k",
    "title": "Pairwise Interaction Tensor Factorization for Personalized Tag Recommendation",
    "info": {
        "author": [
            "Steffen Rendle, Google, Inc."
        ],
        "published": "April 12, 2010",
        "recorded": "February 2010",
        "category": [
            "Top->Computer Science->Semantic Web->Annotation"
        ]
    },
    "url": "http://videolectures.net/wsdm2010_rendle_pit/",
    "segmentation": [
        [
            "So I will talk about personalized tech recommendation on how to do it with this pairwise interaction tensor vectorized."
        ],
        [
            "So Robert has already introduced the problem of personalized tech recommendation, so I will only go shortly about this and what I will talk about is how to optimize models in these settings.",
            "And then the main part of this talk is about how to model here with these pairwise interaction tensor factorization model."
        ],
        [
            "So to start the problem of personalized tech recommendation is imagine a tagging website like last FM where users can check for example artists or songs.",
            "Antec recommenders should faciliate these taking process, so they should, for example, like in this image.",
            "They suggest me some text that I might want to use so my personalized text and this is an easier for me to for the checking process.",
            "And in contrast to things that are implant implemented already in last FM that suggest the same text for the same artists all the time, we want to do this personalized because people used to use different tags for the same things."
        ],
        [
            "So first some formalization of the problem.",
            "So we have basically three types of things.",
            "Here we have users, we have items and we have text and it can be seen as a tensor, and we observe for interaction between these three things.",
            "These are the observed text.",
            "So for example, in this example we have seen that the user one has checked the second item with the 1st and the 3rd tab.",
            "And one other concept that I will use in this talk also is the concept of posts that that are combinations of Houston items so.",
            "Uses are we at the end?",
            "We want to recommend tags for posts superuser item combinations.",
            "We want to recommend some kind of rank list of texts."
        ],
        [
            "So if you look at this, we have one basic problem.",
            "When we want to apply machine learning techniques here is that all observations are positive, so we only see what the user wants to give us A tag.",
            "But he never says what he doesn't want to give us A tag.",
            "So one could say OK, everything he hasn't given as A tag is negative feedback, but that's not true.",
            "For example, it's easy to see in this.",
            "Tom first item for the first user.",
            "Yes, it given any take yet.",
            "But this doesn't mean that every tag is bad for this user, which just means you have impacted you.",
            "So an approach where were used there that's uses negative feedback for everything that has not been observed yet is obviously not doing the right thing.",
            "And there are also other problems because more less we want to rank the Texan, not classify them.",
            "And also if we would fill them with negative, we would have a very very high sparsity, like not sparsity but imbalance like 99.999% of the text would be negative, so that's obviously not a good thing."
        ],
        [
            "YouTube.",
            "So what we do instead, because we have a ranking problem, we.",
            "We come get some kind of ranking constraints from these data.",
            "So the idea's ranking is more or less a comparison between two things to rank.",
            "In our case comparison about two tags.",
            "Say we want to ring given the post the text.",
            "So for each post that we have, we look at the text that they use has already given, and we say OK.",
            "These yes already given.",
            "He seems to prefer over the non given text.",
            "And this is.",
            "Several implications, so first of all, look for example at least Blue column with theater.",
            "The post with user one and item one.",
            "So we haven't observed anything, so we cannot.",
            "Infer any pairwise training data from this, so this actually what we want to do, because we cannot say anything about this.",
            "So we also don't learn about from it in Congress to this this second column, this red one, for example.",
            "Here we can say we think that the user, like the first tech better than the second or so, and.",
            "In this.",
            "Middle row you can see them these these constraints that can be inferred.",
            "Come from this and the advantages of this interpretation of the data.",
            "First of all, the things that we want to recommend in the future, for example, for this blue column for this post, we don't learn on this because that's the target that we want to do.",
            "And and Secondly.",
            "Only within the things where we have observations only within the post that we have observations, we infer any rank."
        ],
        [
            "OK, so the idea is here to keep this core tensor in some kind of way, fixed here to make it diagonal so.",
            "The effect of this is that all the interaction that are not in the main diagonal, they vanish and their model equation guards linear.",
            "So we have only one son.",
            "Um?",
            "Yeah, and and so we have now a better runtime.",
            "Rip"
        ],
        [
            "OK, so one might think, OK, that's it an and we and we are done.",
            "But there's one more problem in that this with the sparsity that we have in the data.",
            "So our approach is different.",
            "Then we go.",
            "One further step is instead of modeling here like with this parallel factor analyze this model, we have some over three multiplications about over three factors and instead of doing this we make it simpler.",
            "We say we model directly the pairwise interactions.",
            "This is for getting a more denser.",
            "Model.",
            "So the idea is here to have to model directly two way interactions.",
            "And one can see this in this framework of tensor factorization by fixing some parts of these factor models to one.",
            "So for example, half of the user feature matrix is fixed to 1 and half of the item matrix is fixed to one and then you get this model equation at the bottom and there you directly factorize the interactions between users and text and another interaction between item in text.",
            "So again, here the equation is linear in K, but the difference here is that it's more restrictive than on this pair effect model because we don't Model 3 way interaction but aware interaction."
        ],
        [
            "So.",
            "One word about the expressiveness.",
            "So we have seen that these targeted composition subsumes this Canonical decomposition, which in turn subsumes this pairwise interaction models, and we have also seen that the advantage of these CD model and these pairwise interaction model over tedious to linear runtime complexity.",
            "So one might ask, why do we need this pairwise interaction model at all?",
            "Because the other one subsumes it, and we already have linear runtime.",
            "And theoretically, if we have a pairwise interaction model, every time it can be represented by Canonical decomposition.",
            "But if you have sparse settings like we have, then it might help to initialize the meaningful structure in your tensors.",
            "Two to get a better estimation of the.",
            "Defectors?",
            "And yeah, I will show this now in."
        ],
        [
            "Weekly."
        ],
        [
            "Um?",
            "OK, so we evaluated this on three datasets first.",
            "2 one are from pips.",
            "Enemy ships anamesa, very small data set here and from last FM and 3rd we emulate this on this year's PPD challenge.",
            "So we compared to page rank folk rank, higher order SVD and this RTF."
        ],
        [
            "The composition.",
            "And if you look at the quality so for this small groups enemy data set, these pairwise interaction model and this academic composition are almost as good as folk rank.",
            "That's one of the state of the art methods.",
            "But if you go to a much sparser problem like last FM.",
            "There you can see that these factorization models outperform these other baselines like programming and patron and so on.",
            "An interesting Lee here is like that.",
            "This pairwise interaction model even outperforms these other mods that subsume it like this Canonical decomposition or these targeted composition, and the reason is that here you have sparse settings you cannot learn from any values, so it makes sense if you have a reasonable model structure in advance to initialize it.",
            "So to keep your model structure fixed."
        ],
        [
            "OK, now something else about the runtime, so this is not so.",
            "An astonishing is that you see.",
            "How good the quality gets after training and for certain time?",
            "And as you can see, this target decomposition needs about 30 days on this last FM data set to converge, and in contrast to this, if you look at these linear models there quite much faster and on the right side you see assuming this left picture and you see for example, it's pairwise interaction model converts already after 20 minutes instead of 30 days.",
            "And also this Canonical decomposition only needs about 40 minutes.",
            "What is interesting that is Canonical decomposition.",
            "It's at the beginning sometime to converge and we think this is the time that it needs to find a basic structure.",
            "And only after this it can converge because there's in this Canonical decomposition or structures given in advance."
        ],
        [
            "OK. And now to last evaluation.",
            "So this was from this year's ECM LPD Discovery challenge.",
            "This is task 2 about tech recommendation, and they're also we submitted our EPA air care wise interaction model and also there it generated the best results.",
            "One word about this thing we submitted here was slightly improved with some kind of post processing.",
            "Because of this challenge, you could gain from recommending less than five tabs, so we had some kind of post processing to estimate how much text we should give out.",
            "But even if you don't use this, this is the second line we didn't submit this, but even with this we would get the best score, but I think this improvement also other.",
            "Other participant use."
        ],
        [
            "OK. Now."
        ],
        [
            "To come to the conclusion.",
            "So whatever presented here is a tensor factorization model based on directly modeling pairwise interactions.",
            "And in our case directly modeling the pairwise interaction between you syntax an item and text, and we have seen that theoretically this CD model and targeting composition both subsumed this pairwise interaction model.",
            "But I've also argued that this does not mean that they are guaranteed to be.",
            "To generate better recommendations, especially on the sparsity, because if you don't have enough data to estimate then your estimations are not that good.",
            "An empirically we have shown that this pairwise interaction model outperforms the other approaches on these sparse last FM data set and also and at these ECM LP, Katie discovered terms from this year.",
            "OK, so that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will talk about personalized tech recommendation on how to do it with this pairwise interaction tensor vectorized.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Robert has already introduced the problem of personalized tech recommendation, so I will only go shortly about this and what I will talk about is how to optimize models in these settings.",
                    "label": 0
                },
                {
                    "sent": "And then the main part of this talk is about how to model here with these pairwise interaction tensor factorization model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to start the problem of personalized tech recommendation is imagine a tagging website like last FM where users can check for example artists or songs.",
                    "label": 0
                },
                {
                    "sent": "Antec recommenders should faciliate these taking process, so they should, for example, like in this image.",
                    "label": 0
                },
                {
                    "sent": "They suggest me some text that I might want to use so my personalized text and this is an easier for me to for the checking process.",
                    "label": 0
                },
                {
                    "sent": "And in contrast to things that are implant implemented already in last FM that suggest the same text for the same artists all the time, we want to do this personalized because people used to use different tags for the same things.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first some formalization of the problem.",
                    "label": 0
                },
                {
                    "sent": "So we have basically three types of things.",
                    "label": 0
                },
                {
                    "sent": "Here we have users, we have items and we have text and it can be seen as a tensor, and we observe for interaction between these three things.",
                    "label": 0
                },
                {
                    "sent": "These are the observed text.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this example we have seen that the user one has checked the second item with the 1st and the 3rd tab.",
                    "label": 0
                },
                {
                    "sent": "And one other concept that I will use in this talk also is the concept of posts that that are combinations of Houston items so.",
                    "label": 0
                },
                {
                    "sent": "Uses are we at the end?",
                    "label": 0
                },
                {
                    "sent": "We want to recommend tags for posts superuser item combinations.",
                    "label": 0
                },
                {
                    "sent": "We want to recommend some kind of rank list of texts.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you look at this, we have one basic problem.",
                    "label": 0
                },
                {
                    "sent": "When we want to apply machine learning techniques here is that all observations are positive, so we only see what the user wants to give us A tag.",
                    "label": 1
                },
                {
                    "sent": "But he never says what he doesn't want to give us A tag.",
                    "label": 0
                },
                {
                    "sent": "So one could say OK, everything he hasn't given as A tag is negative feedback, but that's not true.",
                    "label": 0
                },
                {
                    "sent": "For example, it's easy to see in this.",
                    "label": 0
                },
                {
                    "sent": "Tom first item for the first user.",
                    "label": 0
                },
                {
                    "sent": "Yes, it given any take yet.",
                    "label": 0
                },
                {
                    "sent": "But this doesn't mean that every tag is bad for this user, which just means you have impacted you.",
                    "label": 0
                },
                {
                    "sent": "So an approach where were used there that's uses negative feedback for everything that has not been observed yet is obviously not doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "And there are also other problems because more less we want to rank the Texan, not classify them.",
                    "label": 0
                },
                {
                    "sent": "And also if we would fill them with negative, we would have a very very high sparsity, like not sparsity but imbalance like 99.999% of the text would be negative, so that's obviously not a good thing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "YouTube.",
                    "label": 0
                },
                {
                    "sent": "So what we do instead, because we have a ranking problem, we.",
                    "label": 0
                },
                {
                    "sent": "We come get some kind of ranking constraints from these data.",
                    "label": 1
                },
                {
                    "sent": "So the idea's ranking is more or less a comparison between two things to rank.",
                    "label": 0
                },
                {
                    "sent": "In our case comparison about two tags.",
                    "label": 0
                },
                {
                    "sent": "Say we want to ring given the post the text.",
                    "label": 0
                },
                {
                    "sent": "So for each post that we have, we look at the text that they use has already given, and we say OK.",
                    "label": 0
                },
                {
                    "sent": "These yes already given.",
                    "label": 0
                },
                {
                    "sent": "He seems to prefer over the non given text.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "Several implications, so first of all, look for example at least Blue column with theater.",
                    "label": 0
                },
                {
                    "sent": "The post with user one and item one.",
                    "label": 0
                },
                {
                    "sent": "So we haven't observed anything, so we cannot.",
                    "label": 0
                },
                {
                    "sent": "Infer any pairwise training data from this, so this actually what we want to do, because we cannot say anything about this.",
                    "label": 1
                },
                {
                    "sent": "So we also don't learn about from it in Congress to this this second column, this red one, for example.",
                    "label": 0
                },
                {
                    "sent": "Here we can say we think that the user, like the first tech better than the second or so, and.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "Middle row you can see them these these constraints that can be inferred.",
                    "label": 0
                },
                {
                    "sent": "Come from this and the advantages of this interpretation of the data.",
                    "label": 1
                },
                {
                    "sent": "First of all, the things that we want to recommend in the future, for example, for this blue column for this post, we don't learn on this because that's the target that we want to do.",
                    "label": 0
                },
                {
                    "sent": "And and Secondly.",
                    "label": 0
                },
                {
                    "sent": "Only within the things where we have observations only within the post that we have observations, we infer any rank.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the idea is here to keep this core tensor in some kind of way, fixed here to make it diagonal so.",
                    "label": 0
                },
                {
                    "sent": "The effect of this is that all the interaction that are not in the main diagonal, they vanish and their model equation guards linear.",
                    "label": 0
                },
                {
                    "sent": "So we have only one son.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and and so we have now a better runtime.",
                    "label": 0
                },
                {
                    "sent": "Rip",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one might think, OK, that's it an and we and we are done.",
                    "label": 0
                },
                {
                    "sent": "But there's one more problem in that this with the sparsity that we have in the data.",
                    "label": 0
                },
                {
                    "sent": "So our approach is different.",
                    "label": 0
                },
                {
                    "sent": "Then we go.",
                    "label": 0
                },
                {
                    "sent": "One further step is instead of modeling here like with this parallel factor analyze this model, we have some over three multiplications about over three factors and instead of doing this we make it simpler.",
                    "label": 0
                },
                {
                    "sent": "We say we model directly the pairwise interactions.",
                    "label": 1
                },
                {
                    "sent": "This is for getting a more denser.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So the idea is here to have to model directly two way interactions.",
                    "label": 0
                },
                {
                    "sent": "And one can see this in this framework of tensor factorization by fixing some parts of these factor models to one.",
                    "label": 1
                },
                {
                    "sent": "So for example, half of the user feature matrix is fixed to 1 and half of the item matrix is fixed to one and then you get this model equation at the bottom and there you directly factorize the interactions between users and text and another interaction between item in text.",
                    "label": 0
                },
                {
                    "sent": "So again, here the equation is linear in K, but the difference here is that it's more restrictive than on this pair effect model because we don't Model 3 way interaction but aware interaction.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One word about the expressiveness.",
                    "label": 0
                },
                {
                    "sent": "So we have seen that these targeted composition subsumes this Canonical decomposition, which in turn subsumes this pairwise interaction models, and we have also seen that the advantage of these CD model and these pairwise interaction model over tedious to linear runtime complexity.",
                    "label": 0
                },
                {
                    "sent": "So one might ask, why do we need this pairwise interaction model at all?",
                    "label": 0
                },
                {
                    "sent": "Because the other one subsumes it, and we already have linear runtime.",
                    "label": 0
                },
                {
                    "sent": "And theoretically, if we have a pairwise interaction model, every time it can be represented by Canonical decomposition.",
                    "label": 0
                },
                {
                    "sent": "But if you have sparse settings like we have, then it might help to initialize the meaningful structure in your tensors.",
                    "label": 1
                },
                {
                    "sent": "Two to get a better estimation of the.",
                    "label": 0
                },
                {
                    "sent": "Defectors?",
                    "label": 0
                },
                {
                    "sent": "And yeah, I will show this now in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weekly.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so we evaluated this on three datasets first.",
                    "label": 0
                },
                {
                    "sent": "2 one are from pips.",
                    "label": 0
                },
                {
                    "sent": "Enemy ships anamesa, very small data set here and from last FM and 3rd we emulate this on this year's PPD challenge.",
                    "label": 0
                },
                {
                    "sent": "So we compared to page rank folk rank, higher order SVD and this RTF.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The composition.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the quality so for this small groups enemy data set, these pairwise interaction model and this academic composition are almost as good as folk rank.",
                    "label": 0
                },
                {
                    "sent": "That's one of the state of the art methods.",
                    "label": 0
                },
                {
                    "sent": "But if you go to a much sparser problem like last FM.",
                    "label": 0
                },
                {
                    "sent": "There you can see that these factorization models outperform these other baselines like programming and patron and so on.",
                    "label": 0
                },
                {
                    "sent": "An interesting Lee here is like that.",
                    "label": 0
                },
                {
                    "sent": "This pairwise interaction model even outperforms these other mods that subsume it like this Canonical decomposition or these targeted composition, and the reason is that here you have sparse settings you cannot learn from any values, so it makes sense if you have a reasonable model structure in advance to initialize it.",
                    "label": 0
                },
                {
                    "sent": "So to keep your model structure fixed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now something else about the runtime, so this is not so.",
                    "label": 0
                },
                {
                    "sent": "An astonishing is that you see.",
                    "label": 0
                },
                {
                    "sent": "How good the quality gets after training and for certain time?",
                    "label": 0
                },
                {
                    "sent": "And as you can see, this target decomposition needs about 30 days on this last FM data set to converge, and in contrast to this, if you look at these linear models there quite much faster and on the right side you see assuming this left picture and you see for example, it's pairwise interaction model converts already after 20 minutes instead of 30 days.",
                    "label": 0
                },
                {
                    "sent": "And also this Canonical decomposition only needs about 40 minutes.",
                    "label": 0
                },
                {
                    "sent": "What is interesting that is Canonical decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's at the beginning sometime to converge and we think this is the time that it needs to find a basic structure.",
                    "label": 0
                },
                {
                    "sent": "And only after this it can converge because there's in this Canonical decomposition or structures given in advance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And now to last evaluation.",
                    "label": 0
                },
                {
                    "sent": "So this was from this year's ECM LPD Discovery challenge.",
                    "label": 0
                },
                {
                    "sent": "This is task 2 about tech recommendation, and they're also we submitted our EPA air care wise interaction model and also there it generated the best results.",
                    "label": 0
                },
                {
                    "sent": "One word about this thing we submitted here was slightly improved with some kind of post processing.",
                    "label": 0
                },
                {
                    "sent": "Because of this challenge, you could gain from recommending less than five tabs, so we had some kind of post processing to estimate how much text we should give out.",
                    "label": 0
                },
                {
                    "sent": "But even if you don't use this, this is the second line we didn't submit this, but even with this we would get the best score, but I think this improvement also other.",
                    "label": 0
                },
                {
                    "sent": "Other participant use.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To come to the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So whatever presented here is a tensor factorization model based on directly modeling pairwise interactions.",
                    "label": 0
                },
                {
                    "sent": "And in our case directly modeling the pairwise interaction between you syntax an item and text, and we have seen that theoretically this CD model and targeting composition both subsumed this pairwise interaction model.",
                    "label": 0
                },
                {
                    "sent": "But I've also argued that this does not mean that they are guaranteed to be.",
                    "label": 1
                },
                {
                    "sent": "To generate better recommendations, especially on the sparsity, because if you don't have enough data to estimate then your estimations are not that good.",
                    "label": 0
                },
                {
                    "sent": "An empirically we have shown that this pairwise interaction model outperforms the other approaches on these sparse last FM data set and also and at these ECM LP, Katie discovered terms from this year.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                }
            ]
        }
    }
}