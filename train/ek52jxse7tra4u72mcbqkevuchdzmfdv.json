{
    "id": "ek52jxse7tra4u72mcbqkevuchdzmfdv",
    "title": "Learning the Parameters of Probabilistic Logic Programs from Interpretations",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Ingo Thon, KU Leuven"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Logic",
            "Top->Computer Science->Software and Tools"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_thon_problog/",
    "segmentation": [
        [
            "So my name is Inga tone and I'm here to present the paper learning the parameters of probabilistic logic programs from interpretation, which is joint work with Ben Goodman and looked at who is our supervisor.",
            "We are all from the Catholic University learn.",
            "And we're working through the details."
        ],
        [
            "My talk consists of five parts.",
            "I start to motivate all back after what they give you some background information.",
            "Then I will enter choose our contribution which is the algorithm called alibi problem.",
            "And before I conclude, I will present some experimental results."
        ],
        [
            "OK, for a moment consider you an ambulance dispatcher and here in a big city and some big katastrophy happened like an earthquake and so on.",
            "And your basic task.",
            "But you have to perform now is to decide from which hole.",
            "Which hospital you send the ambulance to which?",
            "Call you can imagine that some parts are blocked and not every location was reachable now."
        ],
        [
            "We are all not.",
            "Ambulance dispatch up machine learner.",
            "We see this as a challenge and the first question one has.",
            "How can we model that?",
            "In love you back a lot on so-called probabilistic programming languages, and specifically on the probabilistic programming language problem.",
            "What is the idea of probabilistic programming?",
            "So the basic idea is you combine normal programming language.",
            "More cases prologue with.",
            "Some random variables in product.",
            "That's probabilistic fact, and that gives you the distribution of our programs.",
            "And with this distribution of a program, you have a distribution over answers of the program.",
            "So to give an example, for example, the probabilistic fact in the ambulance example might be in a probabilistic graph where edges are, for example, there is probability 08.",
            "Or are they?",
            "The program then, is the pathfinding algorithm.",
            "And if you combine these tools so you have a graph represented in your program.",
            "Or one of these graphs?",
            "You get a probability distribution used for finding away from the for the ambulance to the."
        ],
        [
            "But problem is not only restricted to passes in graphs, you can do much more.",
            "So for example, and I'll P. 2009 we had combined soft evidence that our observations when you only have the certainty of say 80% that you observe something together is a ruler and he got a probabilistic rule."
        ],
        [
            "Or you can take probably craft with Prof Minor and you still get craft patterns out of the probabilistic graph."
        ],
        [
            "Or if you want, you can even implement Lisp interpreter and roll off and then use simple probabilistic function and then you would have a probabilistic list if you would like to."
        ],
        [
            "But I."
        ],
        [
            "And roller guys, I'm using prob log and but we actually wanted to do in this paper.",
            "We wanted to learn this parameter so we don't provide the power meter affront, but we want to learn them from data."
        ],
        [
            "So let me continue on my small example, so let's say that's one of the graph you get from listening to the ambulance, so the ambulance might tell you.",
            "OK, sorry guys, we could pass from this on the street from 8 to eat.",
            "Sweets are blue edges, but we found another path from HD an from the level of the road free to see.",
            "So you also have the information that there is a path from ages.",
            "Problem people come from my LP's background and I see people differenciate between two learning stuff."
        ],
        [
            "So.",
            "The first learning setting is called learning it up from interpretation for something done setting is called learning from entailment.",
            "To exaggerate a little bit in learning from interpretation, you can use this full partial draw description, which ago in learning from entailment you only can sort of say the end of the process which is there is a path from A to C, But all the intermediate information you cannot use.",
            "I omitted here a few details, but I hope it gives you the idea that learn from it and it was a very restricted learning setting."
        ],
        [
            "So if we now look at the so called Alphabet soup, so as well as provided a lot of methods, a lot of models and so on.",
            "One can do a very interesting observation.",
            "The social payment.",
            "See methods with message which basically take the program and transform it into a graphical model like a Bayesian network.",
            "Or a Markov network in the case of Michael Logic.",
            "Are all basically learning from interpretations method and that is a very simple reason.",
            "Learning Center naturally applied in those graphical models is learning from.",
            "Interpretation, on the other hand, this sentence.",
            "This probabilistic program, typically based on the distribution semantics by Soto, are already from intense containment algorithms, so they have very limited."
        ],
        [
            "And what we did in our paper, we contributed televi problem, which is a programming language based on such as distribution, semantics and us.",
            "The full running from interpretations."
        ],
        [
            "So let me tell you our contribution.",
            "So we contributed Roblox, the first parameter learning approach for.",
            "Learning probabilistic logic programs from partial interpretation.",
            "The cool feature about this is that it works in a full generated setting.",
            "So in the learning from interpreting will often have to drive your program you model your domain such that it fits the learning from entailment setting which is removed, and I will explain the generator Secondly, 2nd.",
            "It uses partial interpretations of the domain, does not have to be fully observable, and the task is then applicable to a lot of the other is less than applicable to a lot of tasks like collective classification, clustering.",
            "Actually, even did a small algorithm very picked out the right rules for domain."
        ],
        [
            "Again, my little toy example and now represent it also improved problem.",
            "So the pink stuff on the left are so called probabilistic fact.",
            "They assumed to be independent and the green stuff is the program.",
            "How do I generate the possible world from this?"
        ],
        [
            "Very simple, I look at the first edge.",
            "I sample value.",
            "So this is true.",
            "This probability 0.1 that's the right number.",
            "But there.",
            "And let's say I sampled."
        ],
        [
            "Also, I added as far as to the."
        ],
        [
            "Then I look at the next edge.",
            "Is the 1 two.",
            "It has probability 0.6."
        ],
        [
            "So maybe I had a screw to the program."
        ],
        [
            "The Boston, so I sent all the probabilistic things and then I looked at the problem program, which consists of so called clauses which you can easily read read like if the stuff on the right hand side just threw the stuff on the left hand side is true as well.",
            "So HAB is true for Edge 1, three at fault for anyone, sweets or fast month which is for the moment for AP is true for Edge 1 two so pass 1 two is."
        ],
        [
            "So as well so we can all those cream passes which are available.",
            "What is interesting here is that for the moment in the 1st.",
            "Step past 1 Four is full, but then there's the second clause, so I do a second iteration, apply it, look with cloth."
        ],
        [
            "I can apply again Lisa second Clause on going first.",
            "The edge from one to two and then I take the path which exists are from 2 to."
        ],
        [
            "Four and can also have this action, and I continue this process until I have.",
            "Completely generated my interpretation of the script."
        ],
        [
            "And obviously I can sample."
        ],
        [
            "Only one interpretation, but I can send interpretation of that application until I've made about them and normally don't sample them by ourselves.",
            "But basically the training data provided to learn the model.",
            "And that's what we want to do here.",
            "We want to learn the power meter so we."
        ],
        [
            "Want to calculate them for observation and."
        ],
        [
            "There's a very simple base case which is."
        ],
        [
            "If you observe all the probabilistic factors, just counting like normal, so you count the number of times the probabilistic factors true divided by the number of terms Salvation in that case 1/2 and you add it to the."
        ],
        [
            "Grand back type of thing."
        ],
        [
            "But fully observable learning is often very, very boring, boring so.",
            "Yeah, I mean, in many applications you don't observe some things, so one wants to do learning from partial interpretations.",
            "There are many settings imaginable.",
            "Abundance maybe just popped up.",
            "The problem is the fact that still easy but more natural is when you only observe the outcome of the clauses.",
            "Or maybe a mix thing like in the example I had in the beginning by the ambulance.",
            "And if you learn in a generator setting what people typically do, I think in machine learning is using it.",
            "Yeah algorithm, so we have to replace in the EM algorithm.",
            "The accounts.",
            "I had this.",
            "Division by so called expected console the number of times you expect affectively true in an example.",
            "Anne."
        ],
        [
            "This brings me to alchemy algorithm.",
            "Which is a problem.",
            "And I promise on the very high Level 2 step procedure.",
            "So you input into the program to enter the algorithm.",
            "The set of data, example and approach of problem program.",
            "And then we convert this into a set of support probabilistic CNF and then we do any EM algorithm on the set of broader spaces.",
            "Probabilistic"
        ],
        [
            "This idea has been done before, especially the convert into probabilistic CN F that has been done by hand average.",
            "For example for Bayesian network in his guide transference has this yield done something very similar to what we did actually work together on that for prologue.",
            "And I did myself something in this conversion direction based on the.",
            "Language was also probabilistic programming languages, but tailored towards signal.",
            "But let me know."
        ],
        [
            "Take a look at this.",
            "Procedure in detail.",
            "So the first step of the algorithm itself is a four step procedure and the basic goal is to get at the very end.",
            "As I said, this probabilistic seems.",
            "The first step is trying to compute finite draming of the program.",
            "So if you have the evidence, they observation that there's a path from one to three in the past from two to three.",
            "Do you immediately see in the directed graph that all edges going to four are irrelevant because you cannot come back?",
            "And that's especially important problem, cause the theoretical grounding the complete crowning of a program might be infinitely large, but given a query it is necessary to do the semantic to be fine.",
            "So we first have to compute the finite subset of things which are relevant."
        ],
        [
            "And we're basically doing this by running Sodu interpreter, which is then not in logic programming and collect the Crown facts which are used during this.",
            "As of the interpreter run."
        ],
        [
            "And once you have done this, you can basically take all the clauses.",
            "Anne Rivers add convert them in a set of ground clauses.",
            "So for the path ABH maybe I can basically look OK which passes through, so there's 1 two so I can replace part.",
            "This one by this one.",
            "Then I need to add that one and so on and so forth until I've changed rated all clauses which are relevant to calculate the probabilities."
        ],
        [
            "Now I have a ground program and what I can then do is use the algorithm called Clarks completion which takes this.",
            "Set of long claws and transfers it into a CNF which preserves the semantics of the long claws and the reason why that is a little bit is necessary.",
            "Is that problem lies the closed world example assumption, which means that if something is not proven by a horn clause, it's false.",
            "It does not exist, and that means, for example, for past 1, three past 1, three can only be true if any of these clauses generated.",
            "So if parts on three is true.",
            "Then either edge once we need to be true or H12 and pasta 3 needs to be true.",
            "Or maybe both.",
            "And this gives this.",
            "Send the logical equivalence formula and this formula does not look like a CNF, but it can be easily transferred into, which is straightforward."
        ],
        [
            "The last step is we want to simplify the formula for a lot of facts.",
            "We observed already that they are true.",
            "So for example, apart frankly I know it's true."
        ],
        [
            "So I can basically replace it in the formula by truth and after I've done that I can do another iteration of unit propagation because now I know that H23."
        ],
        [
            "Needs to be true.",
            "And we can also replace it later.",
            "But importantly, about edge 2.",
            "Three is that it's probabilistic fact, so I now know that it was true in the observation.",
            "So I need to store that and basically adding one to the count for this problem."
        ],
        [
            "In my first step procedure this admittedly a fifth step, which is splitting of the formula so.",
            "OK, I will not elaborate on that into detail because it's maybe a bit complicated.",
            "And but the idea is basically there is some independence is which can now be exploited to make split.",
            "The form language simplifies the later steps."
        ],
        [
            "And the second step is a yam on the probabilistic CN."
        ],
        [
            "And again, their related bug.",
            "We only want to just Moody's Shiota who did the simplified version of our algorithm trust community, so we just say, OK, we can do any M on these four programs in it, but it was too restrictive to apply it to the formula."
        ],
        [
            "Maximize the probability I explained it in the beginning.",
            "It's very easy, you just have to replace the counts by expected counts.",
            "How you could get fixed."
        ],
        [
            "Account.",
            "And that's what I also don't go into.",
            "I recommend to it the paper for that overcome yesterday holster.",
            "It's basically an inside outside algorithm life you do for PC FGS.",
            "But on the deals which represent this formula."
        ],
        [
            "OK, that brings me already to my experimental results.",
            "So we ask the question, is elephant problem competitive with existing methods based on the old frameworks, which is for example, Markov logic networks?",
            "Is LFI problem sensitive to the initial probabilities?",
            "So we used in the M schemes, so we need to initialize the probabilities and this might roughly algorithm into a local air into the local Maxima.",
            "And finally, is a Teletype problem able to recover?",
            "The original power meters from a limited.",
            "Set."
        ],
        [
            "And the answer is very easy, yes."
        ],
        [
            "First experiment loss on debt could be, I guess everyone knows, the data set is basically the link profit University and you need to classify for each page is that the students pages professor page is a project and so on.",
            "Anne.",
            "The black half is the one of our algorithm.",
            "The pink one is the one of Markov logic.",
            "And as you can see, we are better, but we need a little bit more time.",
            "So Mark of logic is in a way also converting into CNF.",
            "In that case it's much easier, but.",
            "Yeah, he gets the better result though the trade off time is probably reasonable."
        ],
        [
            "The second question was, is our algorithm sensitive to the initial probabilities?",
            "So the model is encoded in a way that the probabilities in the model for Vectibix are very close to 0, so.",
            "The.",
            "Set the sampling for the initial step for values which are close to the true values which are actually somewhere at 0.001 or even smaller.",
            "That's the black curve.",
            "Then we have the greenish curve which sampled uniformly there between 0 to planet 0.9.",
            "And you can see Even so that Even so 0.9 can be very far away from the true probabilities, which are very low.",
            "It still converges after not much more time to the same result.",
            "So I think at least for web, maybe we can argue that the results are."
        ],
        [
            "And the last experiment was on smokers I think.",
            "Simplified, I remove from 50% of the information from each interpretation.",
            "I only need twice as much interpretations.",
            "So I think that's reasonable.",
            "If I say if I remove from every training sample, 50% of the information I need twice as much.",
            "Examples."
        ],
        [
            "OK, that's let me conclude, so I presented here LFI problem and if my problem is the first algorithm which is able to learn probabilistic logic programming language from partial interpretation.",
            "It blocks in the generative setting, which is a very natural setting, and actually it's the more natural setting.",
            "Then learn from entetainment.",
            "I.",
            "Presented some empirical results which shows that it works in practice and even sounds better than state of the art in essence, and the last part of our contributions actually are implementation which you can download from all that page.",
            "You can try it out.",
            "Problem contains also other interesting inference algorithm for decision making for other learning methods for optimization.",
            "Methods for finding the most likely state of the world and so on and so forth and.",
            "Just later the depo.",
            "Thank you very much.",
            "So the inference algorithm we presented requires rounding everything out.",
            "Is it possible to lift your algorithms to the 1st order so we don't have to fully ground things and potentially do it more efficiently?",
            "The answer is obviously maybe.",
            "I don't know.",
            "I mean if I would know I would write a paper.",
            "There are certain steps, but will come next year.",
            "Yeah, I want to come next year so that I can tell you now.",
            "So there are certain sub steps which are liftable.",
            "So for example clocks completion is also described in the lifted setting, but we needed the Crown formula so we can not.",
            "I mean we could have first through the listed class completion and grounded afterwards, but what does it fire?",
            "There are also people working on kind of listed on the algorithm for lift.",
            "For first order CNF and this might be then applied, but I'm not sure how it works.",
            "Can you tell us more about the methodology for good kidney?",
            "What was partially observed there?"
        ],
        [
            "The amount of available data, no.",
            "So in the actually in that can be bought was observed was the.",
            "But it was the link for the class for every web page.",
            "And the link structure and the words for every page.",
            "What is an observed is basically if one looks at the model it's.",
            "Trying to find.",
            "Sorry.",
            "And what is an observed is?",
            "Basically you have the words and.",
            "The model is encoded that a verb kind of causes the Backpage to ever serve in class.",
            "So for example, if you have the words.",
            "Professor appearing on the word.",
            "Professors Page an which blocks basically caused the web page to have sunglasses on, so it's pretty much the settings a actually exactly the setting.",
            "People Dominguez than enter loads used in the book and in the paper on.",
            "I think it was in Chicago, so we got their data set, used different one from the one described on the web page.",
            "That's very important.",
            "We are running out of time so thank you English."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my name is Inga tone and I'm here to present the paper learning the parameters of probabilistic logic programs from interpretation, which is joint work with Ben Goodman and looked at who is our supervisor.",
                    "label": 1
                },
                {
                    "sent": "We are all from the Catholic University learn.",
                    "label": 0
                },
                {
                    "sent": "And we're working through the details.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk consists of five parts.",
                    "label": 0
                },
                {
                    "sent": "I start to motivate all back after what they give you some background information.",
                    "label": 0
                },
                {
                    "sent": "Then I will enter choose our contribution which is the algorithm called alibi problem.",
                    "label": 0
                },
                {
                    "sent": "And before I conclude, I will present some experimental results.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for a moment consider you an ambulance dispatcher and here in a big city and some big katastrophy happened like an earthquake and so on.",
                    "label": 1
                },
                {
                    "sent": "And your basic task.",
                    "label": 0
                },
                {
                    "sent": "But you have to perform now is to decide from which hole.",
                    "label": 0
                },
                {
                    "sent": "Which hospital you send the ambulance to which?",
                    "label": 0
                },
                {
                    "sent": "Call you can imagine that some parts are blocked and not every location was reachable now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are all not.",
                    "label": 0
                },
                {
                    "sent": "Ambulance dispatch up machine learner.",
                    "label": 0
                },
                {
                    "sent": "We see this as a challenge and the first question one has.",
                    "label": 0
                },
                {
                    "sent": "How can we model that?",
                    "label": 0
                },
                {
                    "sent": "In love you back a lot on so-called probabilistic programming languages, and specifically on the probabilistic programming language problem.",
                    "label": 1
                },
                {
                    "sent": "What is the idea of probabilistic programming?",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is you combine normal programming language.",
                    "label": 0
                },
                {
                    "sent": "More cases prologue with.",
                    "label": 1
                },
                {
                    "sent": "Some random variables in product.",
                    "label": 1
                },
                {
                    "sent": "That's probabilistic fact, and that gives you the distribution of our programs.",
                    "label": 0
                },
                {
                    "sent": "And with this distribution of a program, you have a distribution over answers of the program.",
                    "label": 0
                },
                {
                    "sent": "So to give an example, for example, the probabilistic fact in the ambulance example might be in a probabilistic graph where edges are, for example, there is probability 08.",
                    "label": 0
                },
                {
                    "sent": "Or are they?",
                    "label": 0
                },
                {
                    "sent": "The program then, is the pathfinding algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if you combine these tools so you have a graph represented in your program.",
                    "label": 0
                },
                {
                    "sent": "Or one of these graphs?",
                    "label": 0
                },
                {
                    "sent": "You get a probability distribution used for finding away from the for the ambulance to the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But problem is not only restricted to passes in graphs, you can do much more.",
                    "label": 0
                },
                {
                    "sent": "So for example, and I'll P. 2009 we had combined soft evidence that our observations when you only have the certainty of say 80% that you observe something together is a ruler and he got a probabilistic rule.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you can take probably craft with Prof Minor and you still get craft patterns out of the probabilistic graph.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or if you want, you can even implement Lisp interpreter and roll off and then use simple probabilistic function and then you would have a probabilistic list if you would like to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And roller guys, I'm using prob log and but we actually wanted to do in this paper.",
                    "label": 0
                },
                {
                    "sent": "We wanted to learn this parameter so we don't provide the power meter affront, but we want to learn them from data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me continue on my small example, so let's say that's one of the graph you get from listening to the ambulance, so the ambulance might tell you.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry guys, we could pass from this on the street from 8 to eat.",
                    "label": 0
                },
                {
                    "sent": "Sweets are blue edges, but we found another path from HD an from the level of the road free to see.",
                    "label": 0
                },
                {
                    "sent": "So you also have the information that there is a path from ages.",
                    "label": 0
                },
                {
                    "sent": "Problem people come from my LP's background and I see people differenciate between two learning stuff.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first learning setting is called learning it up from interpretation for something done setting is called learning from entailment.",
                    "label": 1
                },
                {
                    "sent": "To exaggerate a little bit in learning from interpretation, you can use this full partial draw description, which ago in learning from entailment you only can sort of say the end of the process which is there is a path from A to C, But all the intermediate information you cannot use.",
                    "label": 0
                },
                {
                    "sent": "I omitted here a few details, but I hope it gives you the idea that learn from it and it was a very restricted learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we now look at the so called Alphabet soup, so as well as provided a lot of methods, a lot of models and so on.",
                    "label": 0
                },
                {
                    "sent": "One can do a very interesting observation.",
                    "label": 0
                },
                {
                    "sent": "The social payment.",
                    "label": 0
                },
                {
                    "sent": "See methods with message which basically take the program and transform it into a graphical model like a Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "Or a Markov network in the case of Michael Logic.",
                    "label": 0
                },
                {
                    "sent": "Are all basically learning from interpretations method and that is a very simple reason.",
                    "label": 1
                },
                {
                    "sent": "Learning Center naturally applied in those graphical models is learning from.",
                    "label": 1
                },
                {
                    "sent": "Interpretation, on the other hand, this sentence.",
                    "label": 0
                },
                {
                    "sent": "This probabilistic program, typically based on the distribution semantics by Soto, are already from intense containment algorithms, so they have very limited.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we did in our paper, we contributed televi problem, which is a programming language based on such as distribution, semantics and us.",
                    "label": 0
                },
                {
                    "sent": "The full running from interpretations.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me tell you our contribution.",
                    "label": 0
                },
                {
                    "sent": "So we contributed Roblox, the first parameter learning approach for.",
                    "label": 1
                },
                {
                    "sent": "Learning probabilistic logic programs from partial interpretation.",
                    "label": 0
                },
                {
                    "sent": "The cool feature about this is that it works in a full generated setting.",
                    "label": 0
                },
                {
                    "sent": "So in the learning from interpreting will often have to drive your program you model your domain such that it fits the learning from entailment setting which is removed, and I will explain the generator Secondly, 2nd.",
                    "label": 0
                },
                {
                    "sent": "It uses partial interpretations of the domain, does not have to be fully observable, and the task is then applicable to a lot of the other is less than applicable to a lot of tasks like collective classification, clustering.",
                    "label": 1
                },
                {
                    "sent": "Actually, even did a small algorithm very picked out the right rules for domain.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, my little toy example and now represent it also improved problem.",
                    "label": 0
                },
                {
                    "sent": "So the pink stuff on the left are so called probabilistic fact.",
                    "label": 0
                },
                {
                    "sent": "They assumed to be independent and the green stuff is the program.",
                    "label": 0
                },
                {
                    "sent": "How do I generate the possible world from this?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple, I look at the first edge.",
                    "label": 0
                },
                {
                    "sent": "I sample value.",
                    "label": 0
                },
                {
                    "sent": "So this is true.",
                    "label": 0
                },
                {
                    "sent": "This probability 0.1 that's the right number.",
                    "label": 0
                },
                {
                    "sent": "But there.",
                    "label": 0
                },
                {
                    "sent": "And let's say I sampled.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, I added as far as to the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I look at the next edge.",
                    "label": 0
                },
                {
                    "sent": "Is the 1 two.",
                    "label": 0
                },
                {
                    "sent": "It has probability 0.6.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe I had a screw to the program.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Boston, so I sent all the probabilistic things and then I looked at the problem program, which consists of so called clauses which you can easily read read like if the stuff on the right hand side just threw the stuff on the left hand side is true as well.",
                    "label": 0
                },
                {
                    "sent": "So HAB is true for Edge 1, three at fault for anyone, sweets or fast month which is for the moment for AP is true for Edge 1 two so pass 1 two is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as well so we can all those cream passes which are available.",
                    "label": 0
                },
                {
                    "sent": "What is interesting here is that for the moment in the 1st.",
                    "label": 0
                },
                {
                    "sent": "Step past 1 Four is full, but then there's the second clause, so I do a second iteration, apply it, look with cloth.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can apply again Lisa second Clause on going first.",
                    "label": 0
                },
                {
                    "sent": "The edge from one to two and then I take the path which exists are from 2 to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Four and can also have this action, and I continue this process until I have.",
                    "label": 0
                },
                {
                    "sent": "Completely generated my interpretation of the script.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And obviously I can sample.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only one interpretation, but I can send interpretation of that application until I've made about them and normally don't sample them by ourselves.",
                    "label": 0
                },
                {
                    "sent": "But basically the training data provided to learn the model.",
                    "label": 0
                },
                {
                    "sent": "And that's what we want to do here.",
                    "label": 0
                },
                {
                    "sent": "We want to learn the power meter so we.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to calculate them for observation and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a very simple base case which is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you observe all the probabilistic factors, just counting like normal, so you count the number of times the probabilistic factors true divided by the number of terms Salvation in that case 1/2 and you add it to the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grand back type of thing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But fully observable learning is often very, very boring, boring so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, in many applications you don't observe some things, so one wants to do learning from partial interpretations.",
                    "label": 1
                },
                {
                    "sent": "There are many settings imaginable.",
                    "label": 0
                },
                {
                    "sent": "Abundance maybe just popped up.",
                    "label": 0
                },
                {
                    "sent": "The problem is the fact that still easy but more natural is when you only observe the outcome of the clauses.",
                    "label": 0
                },
                {
                    "sent": "Or maybe a mix thing like in the example I had in the beginning by the ambulance.",
                    "label": 0
                },
                {
                    "sent": "And if you learn in a generator setting what people typically do, I think in machine learning is using it.",
                    "label": 0
                },
                {
                    "sent": "Yeah algorithm, so we have to replace in the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "The accounts.",
                    "label": 0
                },
                {
                    "sent": "I had this.",
                    "label": 0
                },
                {
                    "sent": "Division by so called expected console the number of times you expect affectively true in an example.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This brings me to alchemy algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which is a problem.",
                    "label": 0
                },
                {
                    "sent": "And I promise on the very high Level 2 step procedure.",
                    "label": 0
                },
                {
                    "sent": "So you input into the program to enter the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The set of data, example and approach of problem program.",
                    "label": 0
                },
                {
                    "sent": "And then we convert this into a set of support probabilistic CNF and then we do any EM algorithm on the set of broader spaces.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This idea has been done before, especially the convert into probabilistic CN F that has been done by hand average.",
                    "label": 0
                },
                {
                    "sent": "For example for Bayesian network in his guide transference has this yield done something very similar to what we did actually work together on that for prologue.",
                    "label": 0
                },
                {
                    "sent": "And I did myself something in this conversion direction based on the.",
                    "label": 0
                },
                {
                    "sent": "Language was also probabilistic programming languages, but tailored towards signal.",
                    "label": 0
                },
                {
                    "sent": "But let me know.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look at this.",
                    "label": 0
                },
                {
                    "sent": "Procedure in detail.",
                    "label": 0
                },
                {
                    "sent": "So the first step of the algorithm itself is a four step procedure and the basic goal is to get at the very end.",
                    "label": 0
                },
                {
                    "sent": "As I said, this probabilistic seems.",
                    "label": 0
                },
                {
                    "sent": "The first step is trying to compute finite draming of the program.",
                    "label": 0
                },
                {
                    "sent": "So if you have the evidence, they observation that there's a path from one to three in the past from two to three.",
                    "label": 0
                },
                {
                    "sent": "Do you immediately see in the directed graph that all edges going to four are irrelevant because you cannot come back?",
                    "label": 0
                },
                {
                    "sent": "And that's especially important problem, cause the theoretical grounding the complete crowning of a program might be infinitely large, but given a query it is necessary to do the semantic to be fine.",
                    "label": 0
                },
                {
                    "sent": "So we first have to compute the finite subset of things which are relevant.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're basically doing this by running Sodu interpreter, which is then not in logic programming and collect the Crown facts which are used during this.",
                    "label": 0
                },
                {
                    "sent": "As of the interpreter run.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you have done this, you can basically take all the clauses.",
                    "label": 0
                },
                {
                    "sent": "Anne Rivers add convert them in a set of ground clauses.",
                    "label": 0
                },
                {
                    "sent": "So for the path ABH maybe I can basically look OK which passes through, so there's 1 two so I can replace part.",
                    "label": 0
                },
                {
                    "sent": "This one by this one.",
                    "label": 0
                },
                {
                    "sent": "Then I need to add that one and so on and so forth until I've changed rated all clauses which are relevant to calculate the probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I have a ground program and what I can then do is use the algorithm called Clarks completion which takes this.",
                    "label": 0
                },
                {
                    "sent": "Set of long claws and transfers it into a CNF which preserves the semantics of the long claws and the reason why that is a little bit is necessary.",
                    "label": 0
                },
                {
                    "sent": "Is that problem lies the closed world example assumption, which means that if something is not proven by a horn clause, it's false.",
                    "label": 0
                },
                {
                    "sent": "It does not exist, and that means, for example, for past 1, three past 1, three can only be true if any of these clauses generated.",
                    "label": 0
                },
                {
                    "sent": "So if parts on three is true.",
                    "label": 0
                },
                {
                    "sent": "Then either edge once we need to be true or H12 and pasta 3 needs to be true.",
                    "label": 0
                },
                {
                    "sent": "Or maybe both.",
                    "label": 0
                },
                {
                    "sent": "And this gives this.",
                    "label": 0
                },
                {
                    "sent": "Send the logical equivalence formula and this formula does not look like a CNF, but it can be easily transferred into, which is straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last step is we want to simplify the formula for a lot of facts.",
                    "label": 0
                },
                {
                    "sent": "We observed already that they are true.",
                    "label": 0
                },
                {
                    "sent": "So for example, apart frankly I know it's true.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I can basically replace it in the formula by truth and after I've done that I can do another iteration of unit propagation because now I know that H23.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Needs to be true.",
                    "label": 0
                },
                {
                    "sent": "And we can also replace it later.",
                    "label": 0
                },
                {
                    "sent": "But importantly, about edge 2.",
                    "label": 0
                },
                {
                    "sent": "Three is that it's probabilistic fact, so I now know that it was true in the observation.",
                    "label": 0
                },
                {
                    "sent": "So I need to store that and basically adding one to the count for this problem.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my first step procedure this admittedly a fifth step, which is splitting of the formula so.",
                    "label": 0
                },
                {
                    "sent": "OK, I will not elaborate on that into detail because it's maybe a bit complicated.",
                    "label": 0
                },
                {
                    "sent": "And but the idea is basically there is some independence is which can now be exploited to make split.",
                    "label": 0
                },
                {
                    "sent": "The form language simplifies the later steps.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second step is a yam on the probabilistic CN.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, their related bug.",
                    "label": 0
                },
                {
                    "sent": "We only want to just Moody's Shiota who did the simplified version of our algorithm trust community, so we just say, OK, we can do any M on these four programs in it, but it was too restrictive to apply it to the formula.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maximize the probability I explained it in the beginning.",
                    "label": 0
                },
                {
                    "sent": "It's very easy, you just have to replace the counts by expected counts.",
                    "label": 1
                },
                {
                    "sent": "How you could get fixed.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Account.",
                    "label": 0
                },
                {
                    "sent": "And that's what I also don't go into.",
                    "label": 0
                },
                {
                    "sent": "I recommend to it the paper for that overcome yesterday holster.",
                    "label": 0
                },
                {
                    "sent": "It's basically an inside outside algorithm life you do for PC FGS.",
                    "label": 0
                },
                {
                    "sent": "But on the deals which represent this formula.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that brings me already to my experimental results.",
                    "label": 0
                },
                {
                    "sent": "So we ask the question, is elephant problem competitive with existing methods based on the old frameworks, which is for example, Markov logic networks?",
                    "label": 0
                },
                {
                    "sent": "Is LFI problem sensitive to the initial probabilities?",
                    "label": 1
                },
                {
                    "sent": "So we used in the M schemes, so we need to initialize the probabilities and this might roughly algorithm into a local air into the local Maxima.",
                    "label": 1
                },
                {
                    "sent": "And finally, is a Teletype problem able to recover?",
                    "label": 0
                },
                {
                    "sent": "The original power meters from a limited.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the answer is very easy, yes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First experiment loss on debt could be, I guess everyone knows, the data set is basically the link profit University and you need to classify for each page is that the students pages professor page is a project and so on.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The black half is the one of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The pink one is the one of Markov logic.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, we are better, but we need a little bit more time.",
                    "label": 0
                },
                {
                    "sent": "So Mark of logic is in a way also converting into CNF.",
                    "label": 0
                },
                {
                    "sent": "In that case it's much easier, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, he gets the better result though the trade off time is probably reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second question was, is our algorithm sensitive to the initial probabilities?",
                    "label": 1
                },
                {
                    "sent": "So the model is encoded in a way that the probabilities in the model for Vectibix are very close to 0, so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Set the sampling for the initial step for values which are close to the true values which are actually somewhere at 0.001 or even smaller.",
                    "label": 0
                },
                {
                    "sent": "That's the black curve.",
                    "label": 0
                },
                {
                    "sent": "Then we have the greenish curve which sampled uniformly there between 0 to planet 0.9.",
                    "label": 0
                },
                {
                    "sent": "And you can see Even so that Even so 0.9 can be very far away from the true probabilities, which are very low.",
                    "label": 0
                },
                {
                    "sent": "It still converges after not much more time to the same result.",
                    "label": 0
                },
                {
                    "sent": "So I think at least for web, maybe we can argue that the results are.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last experiment was on smokers I think.",
                    "label": 0
                },
                {
                    "sent": "Simplified, I remove from 50% of the information from each interpretation.",
                    "label": 0
                },
                {
                    "sent": "I only need twice as much interpretations.",
                    "label": 0
                },
                {
                    "sent": "So I think that's reasonable.",
                    "label": 0
                },
                {
                    "sent": "If I say if I remove from every training sample, 50% of the information I need twice as much.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's let me conclude, so I presented here LFI problem and if my problem is the first algorithm which is able to learn probabilistic logic programming language from partial interpretation.",
                    "label": 1
                },
                {
                    "sent": "It blocks in the generative setting, which is a very natural setting, and actually it's the more natural setting.",
                    "label": 0
                },
                {
                    "sent": "Then learn from entetainment.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Presented some empirical results which shows that it works in practice and even sounds better than state of the art in essence, and the last part of our contributions actually are implementation which you can download from all that page.",
                    "label": 1
                },
                {
                    "sent": "You can try it out.",
                    "label": 0
                },
                {
                    "sent": "Problem contains also other interesting inference algorithm for decision making for other learning methods for optimization.",
                    "label": 0
                },
                {
                    "sent": "Methods for finding the most likely state of the world and so on and so forth and.",
                    "label": 0
                },
                {
                    "sent": "Just later the depo.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So the inference algorithm we presented requires rounding everything out.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to lift your algorithms to the 1st order so we don't have to fully ground things and potentially do it more efficiently?",
                    "label": 0
                },
                {
                    "sent": "The answer is obviously maybe.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean if I would know I would write a paper.",
                    "label": 0
                },
                {
                    "sent": "There are certain steps, but will come next year.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I want to come next year so that I can tell you now.",
                    "label": 0
                },
                {
                    "sent": "So there are certain sub steps which are liftable.",
                    "label": 0
                },
                {
                    "sent": "So for example clocks completion is also described in the lifted setting, but we needed the Crown formula so we can not.",
                    "label": 1
                },
                {
                    "sent": "I mean we could have first through the listed class completion and grounded afterwards, but what does it fire?",
                    "label": 0
                },
                {
                    "sent": "There are also people working on kind of listed on the algorithm for lift.",
                    "label": 0
                },
                {
                    "sent": "For first order CNF and this might be then applied, but I'm not sure how it works.",
                    "label": 0
                },
                {
                    "sent": "Can you tell us more about the methodology for good kidney?",
                    "label": 0
                },
                {
                    "sent": "What was partially observed there?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The amount of available data, no.",
                    "label": 0
                },
                {
                    "sent": "So in the actually in that can be bought was observed was the.",
                    "label": 0
                },
                {
                    "sent": "But it was the link for the class for every web page.",
                    "label": 0
                },
                {
                    "sent": "And the link structure and the words for every page.",
                    "label": 0
                },
                {
                    "sent": "What is an observed is basically if one looks at the model it's.",
                    "label": 0
                },
                {
                    "sent": "Trying to find.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And what is an observed is?",
                    "label": 0
                },
                {
                    "sent": "Basically you have the words and.",
                    "label": 0
                },
                {
                    "sent": "The model is encoded that a verb kind of causes the Backpage to ever serve in class.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have the words.",
                    "label": 0
                },
                {
                    "sent": "Professor appearing on the word.",
                    "label": 0
                },
                {
                    "sent": "Professors Page an which blocks basically caused the web page to have sunglasses on, so it's pretty much the settings a actually exactly the setting.",
                    "label": 0
                },
                {
                    "sent": "People Dominguez than enter loads used in the book and in the paper on.",
                    "label": 0
                },
                {
                    "sent": "I think it was in Chicago, so we got their data set, used different one from the one described on the web page.",
                    "label": 0
                },
                {
                    "sent": "That's very important.",
                    "label": 0
                },
                {
                    "sent": "We are running out of time so thank you English.",
                    "label": 0
                }
            ]
        }
    }
}