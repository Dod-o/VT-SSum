{
    "id": "qdblbmmmvq4gyr4kn2kpfknwyixbiw32",
    "title": "Deep Language Classification for Relabeling of Financial News and its application in Stock Price Forecasting",
    "info": {
        "author": [
            "Miha Torkar, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 14, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2019_torkar_stock_price_forecasting/",
    "segmentation": [
        [
            "Today I'll be presenting the joint work with Julia about using so deep natural language processing for forecasting time series, and in this case will be predicting the stock prices and this."
        ],
        [
            "Basically all fits into the literature in terms of if we're actually estimating if the efficient market hypothesis, which basically stays at all information, is immediately incorporated into the price actually holds.",
            "There are three different levels, or I would say strengths, or how much do you actually believe this?",
            "Believe this from week where basically it says that old pressed information is priced into securities, whereas.",
            "The strong one or the most.",
            "Basically, I guess conservative one would say that yeah, there is no way you could actually do this.",
            "Everything is already there as soon as something new comes up, it's reflected in the price and you're done.",
            "All right?"
        ],
        [
            "So the framework that basically we came up with is something like this and I'll go.",
            "There are four different steps and I'll go through everything.",
            "So basically we have two different datasets.",
            "In the phase one, so this is the two different datasets from use and we will in phase one just basically take the labels from our labeled news data source one and use that as the.",
            "Basically the features in our final model.",
            "Then we also have the second news source that unfortunately does not have all the same labels.",
            "So the idea here was to basically take the first data source that already has this nice labels trainer classifier on top of it and then basically.",
            "Use this classifier to assign new labels to the new data set, and this is basically the Step 2 where we basically input our train our classifier.",
            "There's a we tried a lot of them.",
            "I guess all the new fancy methodology here, and then we managed to obtain the labels for our second data set.",
            "I'll go into details what these datasets, how they look like in all this.",
            "Then once we basically had these labels, we were able to basically again form the same set of features as from the first data set as from the second one as well and then together with the stock prices we basically builds two different regression models that basically ended up with forecasting the final stock price.",
            "So once again, the two datasets we first train the classifier, got two different labels, input that into the final model and then got the forecasts.",
            "Labels are so labels from basically two different datasets and the main thing is that basically the two datasets sort of complement each other, so one has labels whereas the other one is more rich and I guess has all these additional information and I'm guessing you already know which which one the second one is."
        ],
        [
            "So the first data set was basically the Raven packs database, and they have a lot of high frequency news, but that's more or less everything they have and then they have all these different hand.",
            "Almost all they claim it's algorithmic, but when you look down I think it's someone actually does this by hand as well so they have nice labels for every new story.",
            "And we actually have approximately 3 million data points.",
            "But to basically make the field more even and to train our classifier's a bit more nicely, we basically took a more balanced subset and we took the 20 most frequent ones because basically all the other ones are less frequent and then they don't really carry that much importance.",
            "So at the end we ended up and then they still basically account for majority of our data set.",
            "So this was still 24.",
            "Million data points which we finally split into the data validation and test set on the."
        ],
        [
            "Second hand we have the event registry.",
            "Also, high frequency news data on events here we basically limited ourselves to a smaller set of companies to make to make this more basically computationally feasible, we here we took 74 of them.",
            "For those 74 we actually get almost 1,000,000 news events.",
            "The only problem is here that these events are not all labeled, so that's why basically we use them as a target for labeling.",
            "And."
        ],
        [
            "Classification task was basically done in the following manner.",
            "So first we basically obtained the word embeddings from the first database.",
            "Then we trained our models on top of this word embeddings and then we use the train models to again attribute basically the labels to the corpus be here, so the word."
        ],
        [
            "That we tried for fasttext word, two VEC and Doctor Vic.",
            "There are various different algorithms.",
            "Basically how you can actually train this.",
            "And at the bottom you can actually see some specifications that were used.",
            "On the next slide, I'll show basically the performance comparison between."
        ],
        [
            "Love these guys.",
            "So this is basically the embeddings, different embeddings and then for different basically models of classification, right?",
            "And then the average precision was basically more or less in the 70s.",
            "Bird, I think is a bit higher than everyone else.",
            "The best classes I think over different embeddings were usually like exploracion or credit, so that basically some things do repeat across different embedding, so there's not that much difference, whereas for the worst performing classes we actually get similar or similar results, whereas I guess this class the insider trading it appeared more often than not, had fewer prominent words, or something that could be classifier could detect.",
            "Right, so now that we have our classifier, we try to attribute the labels to introduce to your second data set because we there actually don't have some systematic way how to actually assess how good this labels are to the given text, we actually calculate the cosine similarity between the top words in each label and then compare them between each other to see basically how semantically similar is the label to the top words, basically.",
            "In those groups.",
            "So yeah.",
            "So the so we compute the empirical similarity quartiles for this class is and the maximum is then reported.",
            "I'll show you on the next slide, so here what I'm basically showing is the similarity between the quartiles of all the classes for all the models on the second data set and the last two basically columns X that I guess is a baseline for showing the.",
            "The similarity, so we can basically see how well the models perform versus say here where this is actually the labeled data set.",
            "So we can actually see that the bird actually performs better than on the unlabeled data.",
            "Then actual, or I guess the similarity measures on the labeled ones.",
            "So this basically just means that our model didn't manage to catch.",
            "I guess the right labels for the right.",
            "Stopwords, obviously.",
            "The ones that perform worse have really bad.",
            "Label words associated to it.",
            "So then we go ahead and we basically choose the birth attributed labels for the see the event registry database, so the second one.",
            "I've already mentioned the first point, this, I guess just points to the.",
            "Strength or the capabilities of bird that actually manages to capture multiple levels of similarity.",
            "And yeah.",
            "Right, so now that we have this labels right from the two databases, how do we actually come from that to the final features that we are using our models here?",
            "We didn't really complicate that much, so we basically just this stage just used accounts from both of these data sources, so we counted how many times each label appeared in both data sources and so for each feature.",
            "Basically, there's actually 20 of them, the same number as our.",
            "Categories.",
            "For the, for the first data set, we took the original ones.",
            "For the second one, or the ones that weren't associated to it, so.",
            "Then we basically continue to do our final stage where we basically predict.",
            "I guess the stock price movement here.",
            "You can see basically we took the training data set from January 2014 until the end of 2016.",
            "Then we tested it on the remainder of the 2000 or 2017.",
            "Here we chose two models, mostly because the first model allowed us to basically so this time series regression allowed us to basically check for.",
            "Label significance or what are they actually?",
            "Actual coefficients in R model, which labels were the most important ones and the second one is, I guess, more black box type of model where you basically don't really know what the most important feature was, but the performance was actually a lot better.",
            "The so we did the one step ahead prediction in both cases, and here are some of the model specifications as well.",
            "I guess these are the results for, so the mean average error for the two when we include the news and when we don't for each one separately.",
            "So we can actually see that basically re labeling our second data data source, improved the predictions instead of just basically taking the first data source in, including that on the basically right hand side here.",
            "You can see how many times or how many features were basically the number of stocks for which the.",
            "These labels actually prevailed or were actually used inside the models.",
            "And to show some plots, basically this is how the predictions from the first time series model look like.",
            "Obviously they.",
            "The actual line is the red one, the blue one is the prediction one, and the light blue one is the one from the second data source.",
            "So basically we have the original one.",
            "If we use the first data source for labels, or if we use the second one for a data source, this does not really look promising, But what we basically wanted to get from this model, most mostly which labels actually came to be significant.",
            "And I guess this is in line with what you would expect as well, since I guess the most important one here was the acquisitions and mergers, which means that there is a massive moving the price, whereas in the other data set we had revenues or earnings.",
            "So this is when companies announce how much profit or loss they made from last quarter and then obviously the price moves.",
            "For the second, basically modeled, I guess the predictions are a bit better.",
            "Not in all cases.",
            "Sometimes you can see this straight line.",
            "This is where basically the model got completely, I guess.",
            "Stuck in some local minimum and didn't even manage to basically perform really well, so the overall performance.",
            "Is again not really established for all the cases and basically let me just conclude here we basically show which news we perform or the best and the worst, for we can discuss this more into detail in the last slide.",
            "Basically tells us this story that for the STM so the long short term memory model if the original prediction is already quite good, including the news will actually improve.",
            "The entire model where if the initial prediction is not that good and actually the including news, will not benefit.",
            "The model will still basically perform not that well, but it is the case.",
            "Basically more often than not when we include the new stories.",
            "This basically getting stuck in the local minima doesn't occur.",
            "So that will be all.",
            "Thank you for your time and.",
            "I'm open for questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'll be presenting the joint work with Julia about using so deep natural language processing for forecasting time series, and in this case will be predicting the stock prices and this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically all fits into the literature in terms of if we're actually estimating if the efficient market hypothesis, which basically stays at all information, is immediately incorporated into the price actually holds.",
                    "label": 1
                },
                {
                    "sent": "There are three different levels, or I would say strengths, or how much do you actually believe this?",
                    "label": 0
                },
                {
                    "sent": "Believe this from week where basically it says that old pressed information is priced into securities, whereas.",
                    "label": 1
                },
                {
                    "sent": "The strong one or the most.",
                    "label": 0
                },
                {
                    "sent": "Basically, I guess conservative one would say that yeah, there is no way you could actually do this.",
                    "label": 0
                },
                {
                    "sent": "Everything is already there as soon as something new comes up, it's reflected in the price and you're done.",
                    "label": 0
                },
                {
                    "sent": "All right?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the framework that basically we came up with is something like this and I'll go.",
                    "label": 0
                },
                {
                    "sent": "There are four different steps and I'll go through everything.",
                    "label": 0
                },
                {
                    "sent": "So basically we have two different datasets.",
                    "label": 0
                },
                {
                    "sent": "In the phase one, so this is the two different datasets from use and we will in phase one just basically take the labels from our labeled news data source one and use that as the.",
                    "label": 0
                },
                {
                    "sent": "Basically the features in our final model.",
                    "label": 0
                },
                {
                    "sent": "Then we also have the second news source that unfortunately does not have all the same labels.",
                    "label": 0
                },
                {
                    "sent": "So the idea here was to basically take the first data source that already has this nice labels trainer classifier on top of it and then basically.",
                    "label": 0
                },
                {
                    "sent": "Use this classifier to assign new labels to the new data set, and this is basically the Step 2 where we basically input our train our classifier.",
                    "label": 0
                },
                {
                    "sent": "There's a we tried a lot of them.",
                    "label": 0
                },
                {
                    "sent": "I guess all the new fancy methodology here, and then we managed to obtain the labels for our second data set.",
                    "label": 0
                },
                {
                    "sent": "I'll go into details what these datasets, how they look like in all this.",
                    "label": 0
                },
                {
                    "sent": "Then once we basically had these labels, we were able to basically again form the same set of features as from the first data set as from the second one as well and then together with the stock prices we basically builds two different regression models that basically ended up with forecasting the final stock price.",
                    "label": 0
                },
                {
                    "sent": "So once again, the two datasets we first train the classifier, got two different labels, input that into the final model and then got the forecasts.",
                    "label": 0
                },
                {
                    "sent": "Labels are so labels from basically two different datasets and the main thing is that basically the two datasets sort of complement each other, so one has labels whereas the other one is more rich and I guess has all these additional information and I'm guessing you already know which which one the second one is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first data set was basically the Raven packs database, and they have a lot of high frequency news, but that's more or less everything they have and then they have all these different hand.",
                    "label": 0
                },
                {
                    "sent": "Almost all they claim it's algorithmic, but when you look down I think it's someone actually does this by hand as well so they have nice labels for every new story.",
                    "label": 0
                },
                {
                    "sent": "And we actually have approximately 3 million data points.",
                    "label": 0
                },
                {
                    "sent": "But to basically make the field more even and to train our classifier's a bit more nicely, we basically took a more balanced subset and we took the 20 most frequent ones because basically all the other ones are less frequent and then they don't really carry that much importance.",
                    "label": 1
                },
                {
                    "sent": "So at the end we ended up and then they still basically account for majority of our data set.",
                    "label": 0
                },
                {
                    "sent": "So this was still 24.",
                    "label": 0
                },
                {
                    "sent": "Million data points which we finally split into the data validation and test set on the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second hand we have the event registry.",
                    "label": 0
                },
                {
                    "sent": "Also, high frequency news data on events here we basically limited ourselves to a smaller set of companies to make to make this more basically computationally feasible, we here we took 74 of them.",
                    "label": 1
                },
                {
                    "sent": "For those 74 we actually get almost 1,000,000 news events.",
                    "label": 0
                },
                {
                    "sent": "The only problem is here that these events are not all labeled, so that's why basically we use them as a target for labeling.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification task was basically done in the following manner.",
                    "label": 1
                },
                {
                    "sent": "So first we basically obtained the word embeddings from the first database.",
                    "label": 0
                },
                {
                    "sent": "Then we trained our models on top of this word embeddings and then we use the train models to again attribute basically the labels to the corpus be here, so the word.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we tried for fasttext word, two VEC and Doctor Vic.",
                    "label": 0
                },
                {
                    "sent": "There are various different algorithms.",
                    "label": 0
                },
                {
                    "sent": "Basically how you can actually train this.",
                    "label": 0
                },
                {
                    "sent": "And at the bottom you can actually see some specifications that were used.",
                    "label": 0
                },
                {
                    "sent": "On the next slide, I'll show basically the performance comparison between.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love these guys.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the embeddings, different embeddings and then for different basically models of classification, right?",
                    "label": 0
                },
                {
                    "sent": "And then the average precision was basically more or less in the 70s.",
                    "label": 0
                },
                {
                    "sent": "Bird, I think is a bit higher than everyone else.",
                    "label": 0
                },
                {
                    "sent": "The best classes I think over different embeddings were usually like exploracion or credit, so that basically some things do repeat across different embedding, so there's not that much difference, whereas for the worst performing classes we actually get similar or similar results, whereas I guess this class the insider trading it appeared more often than not, had fewer prominent words, or something that could be classifier could detect.",
                    "label": 0
                },
                {
                    "sent": "Right, so now that we have our classifier, we try to attribute the labels to introduce to your second data set because we there actually don't have some systematic way how to actually assess how good this labels are to the given text, we actually calculate the cosine similarity between the top words in each label and then compare them between each other to see basically how semantically similar is the label to the top words, basically.",
                    "label": 0
                },
                {
                    "sent": "In those groups.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So the so we compute the empirical similarity quartiles for this class is and the maximum is then reported.",
                    "label": 0
                },
                {
                    "sent": "I'll show you on the next slide, so here what I'm basically showing is the similarity between the quartiles of all the classes for all the models on the second data set and the last two basically columns X that I guess is a baseline for showing the.",
                    "label": 0
                },
                {
                    "sent": "The similarity, so we can basically see how well the models perform versus say here where this is actually the labeled data set.",
                    "label": 0
                },
                {
                    "sent": "So we can actually see that the bird actually performs better than on the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Then actual, or I guess the similarity measures on the labeled ones.",
                    "label": 0
                },
                {
                    "sent": "So this basically just means that our model didn't manage to catch.",
                    "label": 0
                },
                {
                    "sent": "I guess the right labels for the right.",
                    "label": 0
                },
                {
                    "sent": "Stopwords, obviously.",
                    "label": 0
                },
                {
                    "sent": "The ones that perform worse have really bad.",
                    "label": 0
                },
                {
                    "sent": "Label words associated to it.",
                    "label": 0
                },
                {
                    "sent": "So then we go ahead and we basically choose the birth attributed labels for the see the event registry database, so the second one.",
                    "label": 0
                },
                {
                    "sent": "I've already mentioned the first point, this, I guess just points to the.",
                    "label": 0
                },
                {
                    "sent": "Strength or the capabilities of bird that actually manages to capture multiple levels of similarity.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so now that we have this labels right from the two databases, how do we actually come from that to the final features that we are using our models here?",
                    "label": 0
                },
                {
                    "sent": "We didn't really complicate that much, so we basically just this stage just used accounts from both of these data sources, so we counted how many times each label appeared in both data sources and so for each feature.",
                    "label": 0
                },
                {
                    "sent": "Basically, there's actually 20 of them, the same number as our.",
                    "label": 0
                },
                {
                    "sent": "Categories.",
                    "label": 0
                },
                {
                    "sent": "For the, for the first data set, we took the original ones.",
                    "label": 0
                },
                {
                    "sent": "For the second one, or the ones that weren't associated to it, so.",
                    "label": 0
                },
                {
                    "sent": "Then we basically continue to do our final stage where we basically predict.",
                    "label": 0
                },
                {
                    "sent": "I guess the stock price movement here.",
                    "label": 0
                },
                {
                    "sent": "You can see basically we took the training data set from January 2014 until the end of 2016.",
                    "label": 0
                },
                {
                    "sent": "Then we tested it on the remainder of the 2000 or 2017.",
                    "label": 0
                },
                {
                    "sent": "Here we chose two models, mostly because the first model allowed us to basically so this time series regression allowed us to basically check for.",
                    "label": 0
                },
                {
                    "sent": "Label significance or what are they actually?",
                    "label": 0
                },
                {
                    "sent": "Actual coefficients in R model, which labels were the most important ones and the second one is, I guess, more black box type of model where you basically don't really know what the most important feature was, but the performance was actually a lot better.",
                    "label": 0
                },
                {
                    "sent": "The so we did the one step ahead prediction in both cases, and here are some of the model specifications as well.",
                    "label": 0
                },
                {
                    "sent": "I guess these are the results for, so the mean average error for the two when we include the news and when we don't for each one separately.",
                    "label": 0
                },
                {
                    "sent": "So we can actually see that basically re labeling our second data data source, improved the predictions instead of just basically taking the first data source in, including that on the basically right hand side here.",
                    "label": 0
                },
                {
                    "sent": "You can see how many times or how many features were basically the number of stocks for which the.",
                    "label": 0
                },
                {
                    "sent": "These labels actually prevailed or were actually used inside the models.",
                    "label": 0
                },
                {
                    "sent": "And to show some plots, basically this is how the predictions from the first time series model look like.",
                    "label": 0
                },
                {
                    "sent": "Obviously they.",
                    "label": 0
                },
                {
                    "sent": "The actual line is the red one, the blue one is the prediction one, and the light blue one is the one from the second data source.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the original one.",
                    "label": 0
                },
                {
                    "sent": "If we use the first data source for labels, or if we use the second one for a data source, this does not really look promising, But what we basically wanted to get from this model, most mostly which labels actually came to be significant.",
                    "label": 0
                },
                {
                    "sent": "And I guess this is in line with what you would expect as well, since I guess the most important one here was the acquisitions and mergers, which means that there is a massive moving the price, whereas in the other data set we had revenues or earnings.",
                    "label": 0
                },
                {
                    "sent": "So this is when companies announce how much profit or loss they made from last quarter and then obviously the price moves.",
                    "label": 0
                },
                {
                    "sent": "For the second, basically modeled, I guess the predictions are a bit better.",
                    "label": 0
                },
                {
                    "sent": "Not in all cases.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you can see this straight line.",
                    "label": 0
                },
                {
                    "sent": "This is where basically the model got completely, I guess.",
                    "label": 0
                },
                {
                    "sent": "Stuck in some local minimum and didn't even manage to basically perform really well, so the overall performance.",
                    "label": 0
                },
                {
                    "sent": "Is again not really established for all the cases and basically let me just conclude here we basically show which news we perform or the best and the worst, for we can discuss this more into detail in the last slide.",
                    "label": 0
                },
                {
                    "sent": "Basically tells us this story that for the STM so the long short term memory model if the original prediction is already quite good, including the news will actually improve.",
                    "label": 0
                },
                {
                    "sent": "The entire model where if the initial prediction is not that good and actually the including news, will not benefit.",
                    "label": 0
                },
                {
                    "sent": "The model will still basically perform not that well, but it is the case.",
                    "label": 0
                },
                {
                    "sent": "Basically more often than not when we include the new stories.",
                    "label": 0
                },
                {
                    "sent": "This basically getting stuck in the local minima doesn't occur.",
                    "label": 0
                },
                {
                    "sent": "So that will be all.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your time and.",
                    "label": 0
                },
                {
                    "sent": "I'm open for questions.",
                    "label": 0
                }
            ]
        }
    }
}