{
    "id": "und74naecg2ltlrn2ytgcuatzwofjc6e",
    "title": "Reading Tea Leaves: How Humans Interpret Topic Models",
    "info": {
        "author": [
            "Jordan Boyd-Graber, University of Maryland"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/nips09_boyd_graber_rtl/",
    "segmentation": [
        [
            "OK, thank you for the introduction.",
            "OK, I just want to."
        ],
        [
            "Again, by giving a very brief overview of what topic models are and how in the literature they've traditionally been evaluated.",
            "So a topic model takes as input a corpus just a collection of documents and the raw words inside them."
        ],
        [
            "You then get his output a decomposition of the vocabulary that was present in the corpus into a set of topics.",
            "So for example, here we have a topic about technology, a topic about business and an arts topic, and this is the latent space of the model that is in covered through postira inference.",
            "And so, let's say you have a couple of models that you've learned from the corpus.",
            "How do you know which one is the best?"
        ],
        [
            "One so typically what's done is you have some held out data and then you ask your models what is the probability of generating held out."
        ],
        [
            "ETA that you've never seen before, and so this shows how well the model generalized and you get a number for each one, and you take the highest number and that's the winner.",
            "And so what's interesting about this is this is a measure of the predictive power of these models.",
            "It doesn't look at the latent structure at all.",
            "It doesn't care what the topics are.",
            "Does this mean that people don't care about the topics that you get out of a topic model?",
            "I think the answer to that is pretty definitively know if you've ever read a topic modeling paper.",
            "You're going to see a chow."
        ],
        [
            "Right in there that looks like."
        ],
        [
            "Or this and this shows that these models are capturing the intuitions that went into the model.",
            "Is doing the right thing.",
            "It makes sense."
        ],
        [
            "Looks cool, people do this, even if they're working with a non English course."
        ],
        [
            "This, even if they're working with a non natural."
        ],
        [
            "Language and people put so much stock and the latent space that they even use it to draw conclusions like the influence of statistical methods in computational English."
        ],
        [
            "And this isn't just in academic papers, there are search engines where along with the results you see the topics that are most relevant to your query, and this wouldn't happen if there wasn't someone who said hey, people would like to see topics along with these queries, it'll be useful."
        ],
        [
            "It'll be interesting an there are other search engines that allow you to refine and focus your query based on."
        ],
        [
            "Topics.",
            "And so in the rest of this talk, we're going to give an alternative to this qualitative evaluation of the latent space.",
            "We're going to actually measure how interpretable these topics are.",
            "That is, how much sense do these topics make to a human being?",
            "And so we're going to directly, quantitatively measure this.",
            "And we're going to test this on different models and corpora and at the end of the talk hopefully will show that there's a disconnect with what's typically done."
        ],
        [
            "Measurement and that what we often care about isn't what we're measuring by valuating with likelihood."
        ],
        [
            "OK, so we want to measure something that is based on human judgment, and so there's no getting around having a human in the loop somewhere.",
            "And So what we're going to do is we're going to design experiments that are quick that people don't mind doing too much, and that gives us consistent results.",
            "In order to ask how interpretable is."
        ],
        [
            "Latent space.",
            "And so to do this, we're going to turn to a service called Amazon Mechanical Turk.",
            "This is an online marketplace where people can post short little jobs that get farmed out to hundreds or thousands of people around the world.",
            "They do them.",
            "You get the results, and you pay them a little bit of money, and we're going to propose two tasks, word intrusion and topic into."
        ],
        [
            "OK, so task number one word intrusion.",
            "This is going to measure how well the topic model associate's word."
        ],
        [
            "What the topics.",
            "OK, So what we're going to do is we're going to take one of the topics that we've discovered and take the words with the highest probability in that topic.",
            "And just."
        ],
        [
            "Lump them together into a set.",
            "Now we're going to take a word from a different topic that had high probability in that topic and throw it into that set.",
            "We call this word an intruder.",
            "And So what we do is we're going to display this set of."
        ],
        [
            "It's with an intruder to people on Mechanical Turk.",
            "And so if these Turkers the people on Mechanical Turk can click on the word that doesn't belong when we tell them to click on the word that doesn't belong, then that means in our hypothesis that the users are able to find the same sort of associations that the topic is making and that the topic model is making an that the topic model is doing something that matches."
        ],
        [
            "Human intuition.",
            "And this is what the task looks like when we display to people in an HTML page.",
            "So each of these boxes corresponds to a topic with an intruder in there somewhere and they have to click on the word that they."
        ],
        [
            "It doesn't belong.",
            "And that's what that looks like.",
            "So we shuffled the order of the words in a topic, and we also randomized what intruder was put in there, and we measured how often people clicked on the true intruder, and we called this measurement model."
        ],
        [
            "Vision.",
            "OK task #2 measures how well the topic model associated.",
            "Topics to documents and so you can think about what the topic model is doing as assigning each document to a point in the topic simplex and so.",
            "In the upper left you have the topic that concerns technology and you can have a document that's entirely about technology and one such document is entitled red Light Green Light, a 2 tone led to simplify screens, and then as you move to the right along this topic simplex, the documents become more and more about business and so you have a document like Internet portals.",
            "Portholes begin to distinguish themselves as shopping malls.",
            "That's about 5050 technology and business, and then you can get.",
            "More and more entertainment as you move down in this simplex and you have a document like forget the bootleg.",
            "Just download the movie legally as a document that has all three topics.",
            "And So what we're going to do is we're going to see if human intuitions about the relationship between topics and documents match what the model."
        ],
        [
            "Is giving us.",
            "And So what we're going to do here is similar to what we did before, and So what we're going to do is we're going to display the title of the document and the beginning of that document, and we're going to ask the user to click on the set of words, IE the topic."
        ],
        [
            "That doesn't belong.",
            "And so again, our hypothesis is, if the user can click on a topic that doesn't belong that we selected randomly from topics that aren't related to the document, then it is making the same that sorry he or she is making the same Association."
        ],
        [
            "As our topic model.",
            "OK, and this is what the task looks like.",
            "You have the title, you have the beginning of the document and then each of these rows is one of the topics and we ask them to click on the topic that doesn't belong.",
            "Three of them were associated by our model to the topic.",
            "Sorry to the document and one of them was chosen random."
        ],
        [
            "Way from the other topics.",
            "And that's what they look, and that's what it."
        ],
        [
            "Like when they click on it.",
            "OK, and so this is what it looks like.",
            "Visually.",
            "This is the per document distribution over top."
        ],
        [
            "Sorted by probability.",
            "And So what we always do is we take the three most probable topics associated with that document and we always did."
        ],
        [
            "Play that.",
            "We then choose one topic from the remainder of the topics and this is our intruding topic.",
            "And now what we want to do is we want to measure how consistent the user selection is with what the model thought and so we do this with the measure."
        ],
        [
            "That we call the topic log odds and this is the log of the probability of the intruder topic over the probability of the topic that the user clicked on.",
            "So by definition, if the user clicked on the true intruder, the topic log odds score will be 0.",
            "That's the best."
        ],
        [
            "What you can do?",
            "If you click on another topic that didn't have that high of a probability, then you're going to get less than zero, but not too much."
        ],
        [
            "Plus and then, if you click on higher and higher probability topics, the topic log odds will go down and down, and so as it gets lower and lower, the user is saying that hey what the model thought was relevant."
        ],
        [
            "Isn't relevant.",
            "OK, so we have a way of evaluating topic models.",
            "Now we need some topic models, so we're going to take three different topic models and these make different assumptions about the per document topic distribution.",
            "So if it's a free parameter, you get something that's like PSSI probabilistic latent semantic indexing.",
            "It isn't exactly that.",
            "The details are in the paper.",
            "You can look that up, but we're going to call it P LSI anyway.",
            "If you assume that it comes from a garishly distribution, then you get latent garishly allocation.",
            "And if it comes from a multivariate normal and then you renormalize it to make a multinomial distribution, that's the correlated topic model."
        ],
        [
            "OK. We now fit these three topic models to two different corpora.",
            "the New York Times and a subset of Wiki P."
        ],
        [
            "Yeah, we chose these corpora because they're well structured.",
            "Each document has a title and the beginning of the document should give you a good idea of what's in the document.",
            "These are both relevant to the real world.",
            "Millions of people look at these and read these everyday and there are many different themes in this corpus that hopefully are."
        ],
        [
            "Topic bottles will uncover.",
            "OK, so we fit these three topic models to these two corpora with either 5000 or 150 topics and we presented 50 topics Anna 100 documents from each of these conditions to 8 users."
        ],
        [
            "On mechanical Turk.",
            "OK, so we have our evaluation.",
            "We have our models.",
            "We have our data.",
            "Let's put them together and see what we get.",
            "OK so this is a histogram of the model precision.",
            "This is our first task.",
            "That word intrusion task and it goes from on the right.",
            "You have topics where people were always able to find the intruder.",
            "You have artistic words an whenever an intruder went in there.",
            "People were able to see it didn't belong, they clicked on it.",
            "Everything matched up with what the topic.",
            "Model said as he moved to the left, the model precision gets worse and worse.",
            "People were clicking on words other than the true intruder.",
            "And So what you see there is you have this topic with a bunch of.",
            "Political words and then taxis.",
            "And no matter what intruder appeared there, people thought that that intruder fit with the political words better than taxis did, and so that topic wasn't very."
        ],
        [
            "Charitable OK, we can now compare against multiple corpora, an multiple topics, and so the corpora are the rows in this chart and the topics are the columns, so 50 topics.",
            "100 topics 150 topics left to right and you can see that the correlated topic model in Red did a little bit worse than LDAP.",
            "LSI in green and blue respectively."
        ],
        [
            "OK, our second task.",
            "We can do something similar.",
            "This is again a histogram and this is topic log.",
            "Odds recall that that is simply how well the human view of the intruder meshed with what the user clicked on and so on the right.",
            "You have 0 the best score and here we have Wikipedia articles and so the Wikipedia article about Lindy Hop had a very clear Association with a topic.",
            "It's about dance.",
            "And nothing else and uses variable to figure that out and they clicked on low probability topics that were associated with that document.",
            "Which is good, but then as you move left, people are clicking on things that the topic model thought were associated with the document.",
            "So for example, the Wikipedia article about book people weren't able to decide what was truly associated with books, since books could be related to just about anything."
        ],
        [
            "OK, so we can now do the same comparison across models incorpora.",
            "So again we have 5000 hundred and 50 topics, New York Times and Wikipedia and we see the same thing that we saw before the correlated topic model and Red did a little bit worse and LDAP LSI which did."
        ],
        [
            "Out the same.",
            "Remember that our goal was to compare these measures against likelihood, so we computed the held out likelihood for these models and we got results that are fairly consistent with the literature that correlated topic model did best in about half of the conditions LDN 2 and PLS IN-1."
        ],
        [
            "OK, here is what it looks like when you plot model precision versus held out likelihood recall, but model precision is how often people clicked on the word intruder.",
            "So this is seeing whether the Association of words to topics made sense and so imagine that you're using a topic model and you need to figure out how many topics you're going to use, and so each of these models is a different color.",
            "So red is correlated topic model.",
            "Green is LDA.",
            "And blue is PLSI.",
            "As you increase the number of topics, the likelihood improves going from left to right.",
            "And that's what you would expect.",
            "However, what's what's happening with the model precision?",
            "As you increase the number of topics, even though the likelihood is increasing, which is what you'd likely be measuring, the model precision seems to be going down.",
            "And so it doesn't necessarily mean that improving the likelihood is improving how humans interpret the topic model, which we think is pretty surprising."
        ],
        [
            "OK, so let's look at our other evaluation.",
            "The topic log odds.",
            "So this is measuring the Association of topics to documents.",
            "And now let's pretend that you have a bunch of different topic models and you need to figure out which one to use.",
            "So you have the correlated topic model and it's blowing everything else out of the water in terms of the likelihood.",
            "It's further to the right, much more so than LDA or P LSI.",
            "But if you look at how these models are performing in terms of associating documents to topics correlated topic models is doing substantially worse than LDA or PLSI.",
            "So if you're choosing among topic models, you also have this effect of likelihood, not necessarily meaning that you're doing better."
        ],
        [
            "And this is what I want to leave you with that there seems to be a disconnect between evaluation of topic models and how they're actually used, and so to investigate this, we developed a means of testing and unsupervised method.",
            "And very specifically for topic models.",
            "We showed that there was a way that you could actually ask people in a relatively fast and efficient manner how effective the topic models were at capturing human intuition.",
            "And we found a surprising relationship between interpretability and likelihood.",
            "They weren't moving in lockstep.",
            "And I think the moral of the story is that one likelihood isn't necessarily as intuitive as you would like it to be, and two that you should measure what you care about.",
            "And if you care about the interpretability of topic models, you should directly measure that, and here's."
        ],
        [
            "Here's some things we'd like to look at in the future.",
            "There's been a lot of work recently looking at how inference techniques and hyperparameter optimization affect topic models, and it would be interesting to know if the findings that have happened there in terms of likelihood evaluation also carry over here and in this work we examined a pretty narrow range of models and settings of the various parameters.",
            "And in this space we found a really odd relationship between likelihood interpret ability, and I think it would be really interesting to know how this trades off more generally, and what I think would be really, really exciting is can you capture the same things that we're finding in these evaluation metrics in the model itself?",
            "And can we actually capture human intuition inside the model?"
        ],
        [
            "And that's all I have, except for one shameless plug for our workshop.",
            "The applications for topic models, sex and beyond on Friday.",
            "Thank you very much.",
            "So."
        ],
        [
            "Time for questions.",
            "Come come.",
            "OK so I have two quick questions.",
            "One, how much does it cost to get that sort of evaluation on Mechanical Turk?",
            "It also related to that.",
            "How complex is it?",
            "Or I know that they would involve some sort of construction that you have to design the tasks and go through some sort of process for getting that sort of evaluation, but how likely do you think it could be that you know other other people could try applying this to other topic models?",
            "OK, so all told we spent about $230 to get this data and that included a lot of false starts and experiments that really didn't pan out.",
            "I think you could probably do it for around $100 if you knew what you were doing from the from the beginning, which we didn't in terms of the complexity of constructing these tasks, it's pretty simple.",
            "So basically you create an HTML template and you create a CSV file and you send both of these things to Amazon Mechanical Turk.",
            "And it uses that HTML template to fill in the holes that you left in your CSV file, and so it's really, really easy.",
            "Probably someone could have created a Mechanical Turk talk task during this talk and they would get results by the time this oral session is over.",
            "Thank you.",
            "One more.",
            "We have that.",
            "Question.",
            "Yes, she does come over.",
            "So how can you control for the fact that the users on the Mechanical Turk could be randomly clicking and not really trying to solve the task?",
            "So that's a great question, and there's a lot of art and science to excluding users who are trying to scam the system.",
            "And there are various things you can do.",
            "You can put in things for which you know the answer, or you can create gates that that sort of pre qualify users, and then if afterwards if you find people who are at scanning the system you can exclude them and say I don't trust this person anymore, but what I like to do is put in.",
            "Things for which you already know the answer and that you're pretty confident you know what people will click on.",
            "If 20 people have clicked on this one answer already, you want to make sure that people in the future click on that.",
            "You're clued anybody who doesn't, and so that sort of gives you some groupthink, but that's probably more valuable than letting scammers and people clicking randomly in.",
            "And so with our checks we had about .8 agreement across people.",
            "So in the long run it might be expensive and slow to run human experiments in all kinds of data, so I wonder if you have suggestions on creating automatic evaluation metric that can be computed.",
            "That's imperfect, but somehow correlated with the kind of human measurement.",
            "Yeah, I think that would be great.",
            "We looked at some things.",
            "We looked at their various measures of lexical similarity derived from corpus statistics, and also from things like word net.",
            "And we couldn't find anything that correlated well with what we had.",
            "I think that would be great.",
            "I hope somebody figures out how to do that and tells us.",
            "OK please thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "OK, I just want to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, by giving a very brief overview of what topic models are and how in the literature they've traditionally been evaluated.",
                    "label": 0
                },
                {
                    "sent": "So a topic model takes as input a corpus just a collection of documents and the raw words inside them.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You then get his output a decomposition of the vocabulary that was present in the corpus into a set of topics.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we have a topic about technology, a topic about business and an arts topic, and this is the latent space of the model that is in covered through postira inference.",
                    "label": 0
                },
                {
                    "sent": "And so, let's say you have a couple of models that you've learned from the corpus.",
                    "label": 0
                },
                {
                    "sent": "How do you know which one is the best?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One so typically what's done is you have some held out data and then you ask your models what is the probability of generating held out.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ETA that you've never seen before, and so this shows how well the model generalized and you get a number for each one, and you take the highest number and that's the winner.",
                    "label": 0
                },
                {
                    "sent": "And so what's interesting about this is this is a measure of the predictive power of these models.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look at the latent structure at all.",
                    "label": 0
                },
                {
                    "sent": "It doesn't care what the topics are.",
                    "label": 0
                },
                {
                    "sent": "Does this mean that people don't care about the topics that you get out of a topic model?",
                    "label": 0
                },
                {
                    "sent": "I think the answer to that is pretty definitively know if you've ever read a topic modeling paper.",
                    "label": 0
                },
                {
                    "sent": "You're going to see a chow.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right in there that looks like.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or this and this shows that these models are capturing the intuitions that went into the model.",
                    "label": 0
                },
                {
                    "sent": "Is doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "It makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks cool, people do this, even if they're working with a non English course.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, even if they're working with a non natural.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language and people put so much stock and the latent space that they even use it to draw conclusions like the influence of statistical methods in computational English.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this isn't just in academic papers, there are search engines where along with the results you see the topics that are most relevant to your query, and this wouldn't happen if there wasn't someone who said hey, people would like to see topics along with these queries, it'll be useful.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It'll be interesting an there are other search engines that allow you to refine and focus your query based on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topics.",
                    "label": 0
                },
                {
                    "sent": "And so in the rest of this talk, we're going to give an alternative to this qualitative evaluation of the latent space.",
                    "label": 1
                },
                {
                    "sent": "We're going to actually measure how interpretable these topics are.",
                    "label": 0
                },
                {
                    "sent": "That is, how much sense do these topics make to a human being?",
                    "label": 0
                },
                {
                    "sent": "And so we're going to directly, quantitatively measure this.",
                    "label": 0
                },
                {
                    "sent": "And we're going to test this on different models and corpora and at the end of the talk hopefully will show that there's a disconnect with what's typically done.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measurement and that what we often care about isn't what we're measuring by valuating with likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we want to measure something that is based on human judgment, and so there's no getting around having a human in the loop somewhere.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is we're going to design experiments that are quick that people don't mind doing too much, and that gives us consistent results.",
                    "label": 0
                },
                {
                    "sent": "In order to ask how interpretable is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Latent space.",
                    "label": 0
                },
                {
                    "sent": "And so to do this, we're going to turn to a service called Amazon Mechanical Turk.",
                    "label": 1
                },
                {
                    "sent": "This is an online marketplace where people can post short little jobs that get farmed out to hundreds or thousands of people around the world.",
                    "label": 0
                },
                {
                    "sent": "They do them.",
                    "label": 0
                },
                {
                    "sent": "You get the results, and you pay them a little bit of money, and we're going to propose two tasks, word intrusion and topic into.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so task number one word intrusion.",
                    "label": 0
                },
                {
                    "sent": "This is going to measure how well the topic model associate's word.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What the topics.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're going to do is we're going to take one of the topics that we've discovered and take the words with the highest probability in that topic.",
                    "label": 1
                },
                {
                    "sent": "And just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lump them together into a set.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to take a word from a different topic that had high probability in that topic and throw it into that set.",
                    "label": 1
                },
                {
                    "sent": "We call this word an intruder.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is we're going to display this set of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's with an intruder to people on Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "And so if these Turkers the people on Mechanical Turk can click on the word that doesn't belong when we tell them to click on the word that doesn't belong, then that means in our hypothesis that the users are able to find the same sort of associations that the topic is making and that the topic model is making an that the topic model is doing something that matches.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Human intuition.",
                    "label": 0
                },
                {
                    "sent": "And this is what the task looks like when we display to people in an HTML page.",
                    "label": 0
                },
                {
                    "sent": "So each of these boxes corresponds to a topic with an intruder in there somewhere and they have to click on the word that they.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It doesn't belong.",
                    "label": 0
                },
                {
                    "sent": "And that's what that looks like.",
                    "label": 0
                },
                {
                    "sent": "So we shuffled the order of the words in a topic, and we also randomized what intruder was put in there, and we measured how often people clicked on the true intruder, and we called this measurement model.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vision.",
                    "label": 0
                },
                {
                    "sent": "OK task #2 measures how well the topic model associated.",
                    "label": 0
                },
                {
                    "sent": "Topics to documents and so you can think about what the topic model is doing as assigning each document to a point in the topic simplex and so.",
                    "label": 0
                },
                {
                    "sent": "In the upper left you have the topic that concerns technology and you can have a document that's entirely about technology and one such document is entitled red Light Green Light, a 2 tone led to simplify screens, and then as you move to the right along this topic simplex, the documents become more and more about business and so you have a document like Internet portals.",
                    "label": 1
                },
                {
                    "sent": "Portholes begin to distinguish themselves as shopping malls.",
                    "label": 1
                },
                {
                    "sent": "That's about 5050 technology and business, and then you can get.",
                    "label": 0
                },
                {
                    "sent": "More and more entertainment as you move down in this simplex and you have a document like forget the bootleg.",
                    "label": 1
                },
                {
                    "sent": "Just download the movie legally as a document that has all three topics.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is we're going to see if human intuitions about the relationship between topics and documents match what the model.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is giving us.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do here is similar to what we did before, and So what we're going to do is we're going to display the title of the document and the beginning of that document, and we're going to ask the user to click on the set of words, IE the topic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That doesn't belong.",
                    "label": 0
                },
                {
                    "sent": "And so again, our hypothesis is, if the user can click on a topic that doesn't belong that we selected randomly from topics that aren't related to the document, then it is making the same that sorry he or she is making the same Association.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As our topic model.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is what the task looks like.",
                    "label": 0
                },
                {
                    "sent": "You have the title, you have the beginning of the document and then each of these rows is one of the topics and we ask them to click on the topic that doesn't belong.",
                    "label": 0
                },
                {
                    "sent": "Three of them were associated by our model to the topic.",
                    "label": 0
                },
                {
                    "sent": "Sorry to the document and one of them was chosen random.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way from the other topics.",
                    "label": 0
                },
                {
                    "sent": "And that's what they look, and that's what it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like when they click on it.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "Visually.",
                    "label": 0
                },
                {
                    "sent": "This is the per document distribution over top.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorted by probability.",
                    "label": 0
                },
                {
                    "sent": "And So what we always do is we take the three most probable topics associated with that document and we always did.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play that.",
                    "label": 0
                },
                {
                    "sent": "We then choose one topic from the remainder of the topics and this is our intruding topic.",
                    "label": 0
                },
                {
                    "sent": "And now what we want to do is we want to measure how consistent the user selection is with what the model thought and so we do this with the measure.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we call the topic log odds and this is the log of the probability of the intruder topic over the probability of the topic that the user clicked on.",
                    "label": 0
                },
                {
                    "sent": "So by definition, if the user clicked on the true intruder, the topic log odds score will be 0.",
                    "label": 1
                },
                {
                    "sent": "That's the best.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you can do?",
                    "label": 0
                },
                {
                    "sent": "If you click on another topic that didn't have that high of a probability, then you're going to get less than zero, but not too much.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus and then, if you click on higher and higher probability topics, the topic log odds will go down and down, and so as it gets lower and lower, the user is saying that hey what the model thought was relevant.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isn't relevant.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a way of evaluating topic models.",
                    "label": 0
                },
                {
                    "sent": "Now we need some topic models, so we're going to take three different topic models and these make different assumptions about the per document topic distribution.",
                    "label": 1
                },
                {
                    "sent": "So if it's a free parameter, you get something that's like PSSI probabilistic latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "It isn't exactly that.",
                    "label": 0
                },
                {
                    "sent": "The details are in the paper.",
                    "label": 0
                },
                {
                    "sent": "You can look that up, but we're going to call it P LSI anyway.",
                    "label": 0
                },
                {
                    "sent": "If you assume that it comes from a garishly distribution, then you get latent garishly allocation.",
                    "label": 1
                },
                {
                    "sent": "And if it comes from a multivariate normal and then you renormalize it to make a multinomial distribution, that's the correlated topic model.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. We now fit these three topic models to two different corpora.",
                    "label": 0
                },
                {
                    "sent": "the New York Times and a subset of Wiki P.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, we chose these corpora because they're well structured.",
                    "label": 1
                },
                {
                    "sent": "Each document has a title and the beginning of the document should give you a good idea of what's in the document.",
                    "label": 0
                },
                {
                    "sent": "These are both relevant to the real world.",
                    "label": 0
                },
                {
                    "sent": "Millions of people look at these and read these everyday and there are many different themes in this corpus that hopefully are.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Topic bottles will uncover.",
                    "label": 0
                },
                {
                    "sent": "OK, so we fit these three topic models to these two corpora with either 5000 or 150 topics and we presented 50 topics Anna 100 documents from each of these conditions to 8 users.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have our evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have our models.",
                    "label": 0
                },
                {
                    "sent": "We have our data.",
                    "label": 0
                },
                {
                    "sent": "Let's put them together and see what we get.",
                    "label": 0
                },
                {
                    "sent": "OK so this is a histogram of the model precision.",
                    "label": 1
                },
                {
                    "sent": "This is our first task.",
                    "label": 1
                },
                {
                    "sent": "That word intrusion task and it goes from on the right.",
                    "label": 0
                },
                {
                    "sent": "You have topics where people were always able to find the intruder.",
                    "label": 0
                },
                {
                    "sent": "You have artistic words an whenever an intruder went in there.",
                    "label": 0
                },
                {
                    "sent": "People were able to see it didn't belong, they clicked on it.",
                    "label": 1
                },
                {
                    "sent": "Everything matched up with what the topic.",
                    "label": 0
                },
                {
                    "sent": "Model said as he moved to the left, the model precision gets worse and worse.",
                    "label": 0
                },
                {
                    "sent": "People were clicking on words other than the true intruder.",
                    "label": 0
                },
                {
                    "sent": "And So what you see there is you have this topic with a bunch of.",
                    "label": 0
                },
                {
                    "sent": "Political words and then taxis.",
                    "label": 0
                },
                {
                    "sent": "And no matter what intruder appeared there, people thought that that intruder fit with the political words better than taxis did, and so that topic wasn't very.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Charitable OK, we can now compare against multiple corpora, an multiple topics, and so the corpora are the rows in this chart and the topics are the columns, so 50 topics.",
                    "label": 1
                },
                {
                    "sent": "100 topics 150 topics left to right and you can see that the correlated topic model in Red did a little bit worse than LDAP.",
                    "label": 1
                },
                {
                    "sent": "LSI in green and blue respectively.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, our second task.",
                    "label": 0
                },
                {
                    "sent": "We can do something similar.",
                    "label": 0
                },
                {
                    "sent": "This is again a histogram and this is topic log.",
                    "label": 0
                },
                {
                    "sent": "Odds recall that that is simply how well the human view of the intruder meshed with what the user clicked on and so on the right.",
                    "label": 0
                },
                {
                    "sent": "You have 0 the best score and here we have Wikipedia articles and so the Wikipedia article about Lindy Hop had a very clear Association with a topic.",
                    "label": 0
                },
                {
                    "sent": "It's about dance.",
                    "label": 0
                },
                {
                    "sent": "And nothing else and uses variable to figure that out and they clicked on low probability topics that were associated with that document.",
                    "label": 0
                },
                {
                    "sent": "Which is good, but then as you move left, people are clicking on things that the topic model thought were associated with the document.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Wikipedia article about book people weren't able to decide what was truly associated with books, since books could be related to just about anything.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can now do the same comparison across models incorpora.",
                    "label": 0
                },
                {
                    "sent": "So again we have 5000 hundred and 50 topics, New York Times and Wikipedia and we see the same thing that we saw before the correlated topic model and Red did a little bit worse and LDAP LSI which did.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out the same.",
                    "label": 0
                },
                {
                    "sent": "Remember that our goal was to compare these measures against likelihood, so we computed the held out likelihood for these models and we got results that are fairly consistent with the literature that correlated topic model did best in about half of the conditions LDN 2 and PLS IN-1.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is what it looks like when you plot model precision versus held out likelihood recall, but model precision is how often people clicked on the word intruder.",
                    "label": 1
                },
                {
                    "sent": "So this is seeing whether the Association of words to topics made sense and so imagine that you're using a topic model and you need to figure out how many topics you're going to use, and so each of these models is a different color.",
                    "label": 0
                },
                {
                    "sent": "So red is correlated topic model.",
                    "label": 0
                },
                {
                    "sent": "Green is LDA.",
                    "label": 0
                },
                {
                    "sent": "And blue is PLSI.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of topics, the likelihood improves going from left to right.",
                    "label": 0
                },
                {
                    "sent": "And that's what you would expect.",
                    "label": 0
                },
                {
                    "sent": "However, what's what's happening with the model precision?",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of topics, even though the likelihood is increasing, which is what you'd likely be measuring, the model precision seems to be going down.",
                    "label": 1
                },
                {
                    "sent": "And so it doesn't necessarily mean that improving the likelihood is improving how humans interpret the topic model, which we think is pretty surprising.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at our other evaluation.",
                    "label": 0
                },
                {
                    "sent": "The topic log odds.",
                    "label": 0
                },
                {
                    "sent": "So this is measuring the Association of topics to documents.",
                    "label": 0
                },
                {
                    "sent": "And now let's pretend that you have a bunch of different topic models and you need to figure out which one to use.",
                    "label": 0
                },
                {
                    "sent": "So you have the correlated topic model and it's blowing everything else out of the water in terms of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "It's further to the right, much more so than LDA or P LSI.",
                    "label": 0
                },
                {
                    "sent": "But if you look at how these models are performing in terms of associating documents to topics correlated topic models is doing substantially worse than LDA or PLSI.",
                    "label": 0
                },
                {
                    "sent": "So if you're choosing among topic models, you also have this effect of likelihood, not necessarily meaning that you're doing better.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what I want to leave you with that there seems to be a disconnect between evaluation of topic models and how they're actually used, and so to investigate this, we developed a means of testing and unsupervised method.",
                    "label": 0
                },
                {
                    "sent": "And very specifically for topic models.",
                    "label": 1
                },
                {
                    "sent": "We showed that there was a way that you could actually ask people in a relatively fast and efficient manner how effective the topic models were at capturing human intuition.",
                    "label": 0
                },
                {
                    "sent": "And we found a surprising relationship between interpretability and likelihood.",
                    "label": 1
                },
                {
                    "sent": "They weren't moving in lockstep.",
                    "label": 0
                },
                {
                    "sent": "And I think the moral of the story is that one likelihood isn't necessarily as intuitive as you would like it to be, and two that you should measure what you care about.",
                    "label": 0
                },
                {
                    "sent": "And if you care about the interpretability of topic models, you should directly measure that, and here's.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's some things we'd like to look at in the future.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work recently looking at how inference techniques and hyperparameter optimization affect topic models, and it would be interesting to know if the findings that have happened there in terms of likelihood evaluation also carry over here and in this work we examined a pretty narrow range of models and settings of the various parameters.",
                    "label": 1
                },
                {
                    "sent": "And in this space we found a really odd relationship between likelihood interpret ability, and I think it would be really interesting to know how this trades off more generally, and what I think would be really, really exciting is can you capture the same things that we're finding in these evaluation metrics in the model itself?",
                    "label": 1
                },
                {
                    "sent": "And can we actually capture human intuition inside the model?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's all I have, except for one shameless plug for our workshop.",
                    "label": 0
                },
                {
                    "sent": "The applications for topic models, sex and beyond on Friday.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for questions.",
                    "label": 0
                },
                {
                    "sent": "Come come.",
                    "label": 0
                },
                {
                    "sent": "OK so I have two quick questions.",
                    "label": 0
                },
                {
                    "sent": "One, how much does it cost to get that sort of evaluation on Mechanical Turk?",
                    "label": 0
                },
                {
                    "sent": "It also related to that.",
                    "label": 0
                },
                {
                    "sent": "How complex is it?",
                    "label": 0
                },
                {
                    "sent": "Or I know that they would involve some sort of construction that you have to design the tasks and go through some sort of process for getting that sort of evaluation, but how likely do you think it could be that you know other other people could try applying this to other topic models?",
                    "label": 0
                },
                {
                    "sent": "OK, so all told we spent about $230 to get this data and that included a lot of false starts and experiments that really didn't pan out.",
                    "label": 0
                },
                {
                    "sent": "I think you could probably do it for around $100 if you knew what you were doing from the from the beginning, which we didn't in terms of the complexity of constructing these tasks, it's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "So basically you create an HTML template and you create a CSV file and you send both of these things to Amazon Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "And it uses that HTML template to fill in the holes that you left in your CSV file, and so it's really, really easy.",
                    "label": 0
                },
                {
                    "sent": "Probably someone could have created a Mechanical Turk talk task during this talk and they would get results by the time this oral session is over.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "One more.",
                    "label": 0
                },
                {
                    "sent": "We have that.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yes, she does come over.",
                    "label": 0
                },
                {
                    "sent": "So how can you control for the fact that the users on the Mechanical Turk could be randomly clicking and not really trying to solve the task?",
                    "label": 0
                },
                {
                    "sent": "So that's a great question, and there's a lot of art and science to excluding users who are trying to scam the system.",
                    "label": 0
                },
                {
                    "sent": "And there are various things you can do.",
                    "label": 0
                },
                {
                    "sent": "You can put in things for which you know the answer, or you can create gates that that sort of pre qualify users, and then if afterwards if you find people who are at scanning the system you can exclude them and say I don't trust this person anymore, but what I like to do is put in.",
                    "label": 0
                },
                {
                    "sent": "Things for which you already know the answer and that you're pretty confident you know what people will click on.",
                    "label": 0
                },
                {
                    "sent": "If 20 people have clicked on this one answer already, you want to make sure that people in the future click on that.",
                    "label": 0
                },
                {
                    "sent": "You're clued anybody who doesn't, and so that sort of gives you some groupthink, but that's probably more valuable than letting scammers and people clicking randomly in.",
                    "label": 0
                },
                {
                    "sent": "And so with our checks we had about .8 agreement across people.",
                    "label": 0
                },
                {
                    "sent": "So in the long run it might be expensive and slow to run human experiments in all kinds of data, so I wonder if you have suggestions on creating automatic evaluation metric that can be computed.",
                    "label": 0
                },
                {
                    "sent": "That's imperfect, but somehow correlated with the kind of human measurement.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that would be great.",
                    "label": 0
                },
                {
                    "sent": "We looked at some things.",
                    "label": 0
                },
                {
                    "sent": "We looked at their various measures of lexical similarity derived from corpus statistics, and also from things like word net.",
                    "label": 0
                },
                {
                    "sent": "And we couldn't find anything that correlated well with what we had.",
                    "label": 0
                },
                {
                    "sent": "I think that would be great.",
                    "label": 0
                },
                {
                    "sent": "I hope somebody figures out how to do that and tells us.",
                    "label": 0
                },
                {
                    "sent": "OK please thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}