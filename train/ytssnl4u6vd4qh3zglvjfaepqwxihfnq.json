{
    "id": "ytssnl4u6vd4qh3zglvjfaepqwxihfnq",
    "title": "Deep learning for computational chemistry: compound representation, ADMET profiles and automatic optimization",
    "info": {
        "author": [
            "Floriane Montanari, Bayer AG"
        ],
        "published": "June 28, 2019",
        "recorded": "May 2019",
        "category": [
            "Top->Computer Science",
            "Top->Data Science",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/icgeb_montanari_computational_chemistry/",
    "segmentation": [
        [
            "So I actually joined by your two years ago for a kind of exploratory project.",
            "There is set up to evaluate how deep learning could affect positively different aspects of drug discovery, and my focus was on chem informatics computational chemistry, and I just want to give you an overview today and there will be 3 parts that are kind of independent, but they are also very related.",
            "You will see at the end why.",
            "So if you want you can ask question whenever you feel like.",
            "Also at the end of every subpart, if that's what."
        ],
        [
            "So a very brief introduction of computational chemistry in in a pharma company.",
            "Where do computational chemists help?",
            "At which stage of the project?"
        ],
        [
            "So you have to think first the target identification is something that more bioinformatics or biologists and pharmacology STS would come up with.",
            "But once you have your protein target that then the search for a compound that addressed his target.",
            "This is something where computational chemistry can help.",
            "So we had this morning and talk from Andreas and he mentioned how considerations on how to design libraries on which you will screen later on.",
            "Is very important, so there are computational chemistry can help.",
            "Also, once you have a list of hits you need somehow to make a selection.",
            "Or maybe you want to expand your space by using some similarity.",
            "So there again computational chemistry can help and later in later phases when you already have a candidate, maybe you want to refine it or understand how the relationship between structure and activity for your target.",
            "Works and also there you can have docking studies or molecular dynamics to understand how the compound binds to the pocket, for example.",
            "And also later you also want good admit properties for your compounds."
        ],
        [
            "So by now I think you are all experts on how to describe compounds because we have seen many, many talks that address this.",
            "You can have pictures.",
            "That's something that chemists are very comfort table with using because they draw their compounds on paper.",
            "We have those famous circular fingerprints that were mentioned many times and there are also 3D descriptors, but those require that you know the confirmation of your small molecule, which is something that is hard to know in advance.",
            "And there is also a kind of new approach that would be learning the fingerprints graph.",
            "Convolutions can help there, and I will talk a bit more after after that.",
            "And finally you have line notations, so this is like text that represents your molecule.",
            "One very important is the smiles notation which encodes your molecular graph by letters like a C would be a carbon.",
            "Whether it's capital letter or small means it's aromatic or aliphatic and so on.",
            "So you can actually really encode the structure by it's just a string of characters.",
            "And this we will use later on."
        ],
        [
            "So why do we want to apply deep learning and computational chemistry?",
            "The story started about 2012, when Merck published several bioactivity datasets and launched a competition in the platform Cargle, where a lot of machine learning guys compete to try to find the best models and the winning team.",
            "It's listed there the names I guess you recognize my video for Hinton, so those are big names.",
            "In deep learning they actually have no knowledge or not much knowledge of chemistry, but they want this competition and their their method was a mix of single task networks, multi task networks and gradient boosting methods."
        ],
        [
            "So this kind of sparked the interest of the community towards applying deep learning to different applications.",
            "So after this you had regularly publications on toxicity prediction or bioactivity prediction using deep neural networks, and right now I think the slide would be covered with titles because every week or every second week new paper is published."
        ],
        [
            "Yeah.",
            "So in the first part I have, I'm talking about a paper that was published in chemical science.",
            "It's called learning continues and data driven molecular descriptors by translating between equivalent chemical representations and we call them C DDD, that's continuous data driven descriptor.",
            "We are not very good with names, so that's what we came up with.",
            "So that I."
        ],
        [
            "Idea is borrowed from text translation, so when you try to use Google Translate currently what runs behind is a neural networks that has been trained to translate between 2 languages.",
            "And that's also what we try to do with our chemical representations.",
            "So we use the line notations, the string and that way we have text and we can use the same approaches as the text translation.",
            "Methods and we have an auto encoder that network that contains the first part that is an encoder that takes your input string that represents your compound and try to go towards a kind of bottleneck which becomes your embedding.",
            "So we have learned a lot about embedding in the previous lecture.",
            "This is one of them and then the second part of the network is a decoder.",
            "The decoder takes an embedding and try to reproduce.",
            "The same string, or in our case, translate the string to a different representation of the same compound.",
            "So in this case I took the name but actually in smiles notations depending on at which Atom you start you get different string for the same compound and that's what we use to be able to do the translation.",
            "So on the one side you would have one variant of the smiles for that compound and at the output you try to predict the Canonical version which is canonicalized by your software.",
            "So that's.",
            "Unique mapping between a smile and a structure.",
            "Yes, so the idea is that once you have train your model, the embedding represents the compound because that's what is common between the two.",
            "Representation is like the semantic.",
            "If you would say between two sentences in two different languages, that's what you need to keep in your embedding, and we hope that this would be a good molecular descriptor."
        ],
        [
            "So how we did it?",
            "The lucky part is that for training such embeddings, you don't need any specific data, you just need chemical structures.",
            "We have a lot of them, so we took everything in zinc and pubchem.",
            "We applied some cleaning to keep things a bit drug like, or at least not too crazy in terms of chemistry, and we ended up with 72 million structures.",
            "We use the Public Library all the kit to generate the smiles, which is what we need to train our embedding and we also use it to compute some very simple molecular descriptors that you can obtain for any structure.",
            "I will show you later why.",
            "Um?"
        ],
        [
            "Yeah, now in case you're not super familiar with text mining methods, I just want to quickly talk to you about how do you give the input to the model so it's called tokenization and it's actually trying to manage your string of characters into something that the computer can read easily.",
            "So what you do is that you have your whole database of characters and you enumerate them, so we have a C. We have a one.",
            "And so on.",
            "So you have a big lookup table of characters or tokens actually for bromine or chlorine.",
            "It's 2 characters, that's why it's not exactly a character, it's a token, and in the end what you do for representing a token is 1 hot encoding where you basically look up at which index is your token.",
            "That's where you will have a one, and you will have zeros everywhere else.",
            "And that's the input feeder.",
            "Yeah, the input feature."
        ],
        [
            "OK, so the encoder is a recurrent neural network because we are dealing with sequences of tokens and that that's three layers of gated recurrent units that feed a fully connected layer."
        ],
        [
            "We then have this embedding or latent space, that's the that's where the compression of information takes place, so it's a little bottleneck of 512 neurons."
        ],
        [
            "And for the decoder, we have exactly a mirror of the architecture of the encoder and what we output then is a probability distribution on the tokens, so that's how you can sample the sequence by sampling from this distribution.",
            "And now we have a little extra trick that proved very useful to create a good features.",
            "It is too at the same time as you are learning this autoencoder or translation encoder.",
            "You also try to learn a classifier using is embedding and this classifier is trying to predict those chemical features you computed with our ticket.",
            "So log P molecular weight.",
            "This kind of things.",
            "So we have nine of them.",
            "And this helps driving the learning towards a feature representation that's helpful for downstream tasks, which is what we actually care about.",
            "We don't really care about creating smiles."
        ],
        [
            "Yes.",
            "So in this slide we see both the translation accuracy, so that's how good we are at recreating the proper Canonical smiles given smiles version of a compound.",
            "And the other one is a downstream tasks.",
            "So in this case we would extract the embedding at different stage of the learning and we use the embedding to learn a simple machine learning method like SVM, in this case to predict lipophilicity.",
            "So we have a smaller data set where we know lipophilicity.",
            "We use our embedding too as a starting descriptor and well overall what we see is while training.",
            "So we get better translation accuracy and also this usually helps having a good downstream task performance, so that's kind of a sanity check and then we have different versions, so you see the red curve is the highest in terms of accuracy of translation, and that's the case when we start from a Canonical smiles and we reproduce the same Canonical smiles, so there is no translation is just a real autoencoder and we see that these performs.",
            "Quite poorly in the downstream tasks, so the idea of translating is really helping.",
            "So our.",
            "Yes, the orange one is the one where we don't have the helper task, so trying to predict those few descriptors as a fully connected network while training the embedding.",
            "So these performs a little bit worse compared to our final method, which is the blue one where we start from a smiles we predict the Canonical smiles and we also train to predict those few molecular descriptors."
        ],
        [
            "OK, so now we want to check whether our knew molecular descriptors perform well on downstream tasks and downstream tasks.",
            "Here are predicting biological activity, so we we collected public datasets, both classification and regression, different sizes and we compare the performance of our descriptors called SDD.",
            "Different versions of circular fingerprints and also.",
            "The graph convolutional approach and for the first two we can also choose which algorithm to plug in, and we choose between random forests, supervector machine or gradient boosting.",
            "Whatever works best."
        ],
        [
            "So these are the results we obtained for those data in black you have the circular fingerprints in Gray, the graph convolution, and in orange our descriptors and what you can just see is that overall there is no winner.",
            "We are all performing similarly and these are hyper parameterized optimized performances.",
            "On the left side you have random split cross validation versus cluster split cross validation.",
            "You see the performance drops in cluster splits, which is something you would expect and just maybe.",
            "One thing to note, for solubility the graph convolution worked quite well in the random split, but then the performance dropped in the cluster, cross validation split and our hypothesis is that these data set is very small and graph convolution are actually overfitting.",
            "So in the random case and then this doesn't work well in cross in cluster cross validation.",
            "Yeah, so overall we were quite happy that our learned descriptors seem to be as performant as the state of the art."
        ],
        [
            "Easy FP finger."
        ],
        [
            "We also tried to use it in a virtual screening benchmark, so we heard about virtual screening with Andreas today.",
            "So the idea is that you take a known active and you then rank database by similarity to the non active using your descriptors and those two database are actually benchmarks where you have sets of real actives and sets of compounds that are called decoys.",
            "That you assume are negatives because they have no reported activity, but they look kind of similar to your actives.",
            "So the idea is to try to compete with different descriptors and computing similarities from your set of actives and seeing if you can rank your real actives before the decoys.",
            "So that's the overall task and we are comparing with many different descriptors But in this picture we only show the best order descriptors and.",
            "Powers so in black it's our method.",
            "OK. And for the Directory of Useful Decoys we see that our method really outperforms the others significantly, and for the disorder it's also actually significant, but not as strongly.",
            "So what we conclude here."
        ],
        [
            "Or is that actually computing distances or similarities in our space seems to be nicely correlated with biological activity, so it's also a good point that our descriptors can be useful for computational chemistry."
        ],
        [
            "So overall for this part we have learned in an unsupervised way an embedding for compounds by using translation between 2 string representation of the compounds and we can then extract this bottleneck to use as descriptor and we if you use it in combination with support vector machine.",
            "In our experience that works quite well for Qsar models, and it's also good for virtual screening and then.",
            "Something that you should keep in mind is that we have a decoder.",
            "That means that given an embedding, we are able to go back to a compound, which is something we will exploit later, and it's it's something really cool, because if you have circular fingerprints, for example, you cannot deduce a structure from the fingerprint.",
            "The code is available online.",
            "You can just use it if you feel like trying it.",
            "Yes."
        ],
        [
            "So the idea is that we want to use as many chemical structures as possible, so we have their 72 million and if there is no biological activity available for 72 million compounds.",
            "Yeah, we didn't try that.",
            "What is the dimensionality of your encoding space?",
            "500, twelve, 512.",
            "And did you experiment with different dimensions?",
            "Yes, so actually I think this published version was later refined and we also have good performance on a smaller space.",
            "I think 128 but I'm I cannot guarantee I I'm not sure.",
            "Approach that is Linda.",
            "In the next part I talk a lot about it.",
            "Then you can ask more questions.",
            "Your principles.",
            "Is it for the IT was it so we folded them to three different sizes and we tried three different diameters and then we let them choose whichever algorithm work best for which data set?",
            "So that's our final bar in the Bartlett.",
            "OK, so now we switch gears a little bit and we talk about admit prediction using multitask approach."
        ],
        [
            "So I admit for those who are not super familiar with this is absorption, distribution, metabolism, excretion and toxicity, and it actually tried to explain the fate of a drug.",
            "Once you have taken it.",
            "So how fast does it reach the target organ?",
            "How much of it reached the target organ?",
            "How quickly is it metabolised?",
            "Is the expression going through the kidneys or through the Vine?",
            "Does it reach the brain?",
            "And do you want it to reach the brain or not?",
            "So all these kind of questions are actually independent of the drug Discovery project.",
            "They have to be addressed, so we have quite a lot of data."
        ],
        [
            "So this is the data we have 0.5 million and 70 endpoints.",
            "And if you just say those numbers you think, well this is super great.",
            "But then you you plot the distribution of how many compounds are actually measured in every endpoint and you already see my big problem that there are a few essays like log D where we have lots of compounds and then when we go towards more complicated assays or more.",
            "Expensive or assays that you tried not to do because they are human, not human, but animal assays.",
            "Then you have just a few hundred data points, so this is something that's that has to be."
        ],
        [
            "Taken into account.",
            "So in this project we thought that we should compare how multitask networks perform compared to single task networks compared to baseline, because we already have quite some models running in the company that we are not using deep learning.",
            "So if we want to switch to deep learning, it should be because we have a significant improvement and we also compare a graph convolutional not."
        ],
        [
            "Just a very small side on multitask learning, so there are a lot of multi something in machine learning.",
            "One is multi class, that's when you have different categories.",
            "On your examples have to be categorized in one of them.",
            "So for example in the Super famous Iris data set you have three possibilities for a new iris flower to be classified.",
            "You also have the multi label aspect.",
            "That's when you have different categories, but your new examples can be assigned different categories like a movie can be both an action movie on a science fiction movie, but we don't treat any of those.",
            "We are trying to do multi task and in multi task we try to learn jointly different tasks and using a shared representation."
        ],
        [
            "Yeah, so why do we think that this could help so you can see a part of our metrics of admit assays and you can see already that some of them are correlated.",
            "You also know from talking to the experimentalist that some of them are biologically related, or maybe they were measuring different species, so it makes sense maybe to learn them together.",
            "And there are also experiments endpoints that are obtained from the exact same experiment.",
            "But just different readouts maybe?",
            "So what we expect as a benefit is that we could exploit those correlation between endpoints and also that we would benefit from an overall larger training set if you combine those large endpoints with smaller endpoints, then maybe you learn representations that are richer and that will benefit the smaller endpoint.",
            "And overall it's also a regularization method because you are sharing your parameters across more tasks."
        ],
        [
            "So how it works in practice is that you combine different datasets into a larger data set that contains as X you have the union of all the compounds that were tested in any of those three ourselves, and then you're wise or your target now becomes a vector containing those three tasks, and if you don't have a measurement for combination of component ask, then you have a missing value we have talked.",
            "Already about this.",
            "So in theory there is no requirement to have overlaps of compounds in different assays that you stick together, but in practice."
        ],
        [
            "It's helpful.",
            "So how do you train a neural network with this kind of multi task?",
            "So you have to have an output notes for your end tasks and then you compute a loss for each of those tasks.",
            "That means you only in your mini batch you look at the data where you have a measurement for that particular task and you compute the mean squared error over the mini batch.",
            "So that gives you in this case 3 different tasks.",
            "Specific losses and then you combine those into a global loss.",
            "That's a weighted average of those task specific losses.",
            "As a consequence, and this was important for me because I was working on regression, you need to have outputs in the same range, otherwise you might have weird things happening very high.",
            "Mean squared error for values that are between 200 and 600, and then if you combine that with PIC 50s you would have problems when you do your weighted average.",
            "So always standardize your tasks.",
            "And then just one little consideration on how you do your weighted average.",
            "You could just say OK, all tasks are equivalent, they all get the same weight one out of N. But then maybe you have cases where some tasks are really big, sometimes are really small, and then in your meeting, but you would have very few example of this small task and then maybe you are not learning very well because the losses diluted by all the.",
            "Training set of the larger task.",
            "So in this case you could decide to artificially increase the weight you give to the smaller task to work compared to the large task.",
            "In practice you should just try out what works for you.",
            "It depends on the cases.",
            "OK, now."
        ],
        [
            "I will answer my recession specials, question graph, convolutions.",
            "So the concept of graph convolution, and I'm pretty sure my income and I will talk a lot about it, but it's too to learn representation from a graph.",
            "So you you think of your molecule as a graph where the nodes are the atoms and bonds are the edges between those atoms.",
            "So you have the adjacency matrix that says item one is linked to at home.",
            "Three for example.",
            "And then you kind of propagate this Atom representation using this matrix, so the overall network architecture is like this.",
            "So we have two steps of graph convolutions.",
            "Then we have one fully connected layer and we have our output.",
            "So this is a case of end to end learning.",
            "You start from no features and then you learn features for your atoms that are very good for your task.",
            "So you need actually a lot of data to do this.",
            "I'm I'm using the library called Deep Cam for running this and they have a very specific implementation of graph convolutions that is a little bit complex.",
            "I'm just going quickly over it, but there are other software out there that can do it with different approaches, so it's just one example.",
            "So let's imagine our input graph looks like this, so the color of the items could be just simply different item types and the first step is actually too.",
            "Compute features for those atoms.",
            "If you don't know anything, you can use one hot encoding.",
            "That would be the case of America's first networks we saw today, but in this case, since we are in chemistry, we actually can quickly compute some features using our ticket.",
            "That will be our safe input descriptor space.",
            "The first step is to actually look at yourself and at your neighbors, and we have some of the neighborhood for every Atom that is then propagated well enough.",
            "In propagation, those are learned parameters that are shared across the older atoms, so we have one set of parameters for the Atom themselves, and one set of parameters for the neighborhoods.",
            "And this operation occurs simultaneously on all atoms.",
            "We then sum these on.",
            "This becomes the new representation for the at all."
        ],
        [
            "We have then a non linearity operation on every Atom."
        ],
        [
            "And after this we again look at the neighbors and we do a graph pooling operation, which is kind of similar to the pooling operation in normal convolutional networks that you are maybe more familiar with.",
            "So basically you concatenate your features with the ones of your neighbors and you take the Max.",
            "That's your new representation, and again this occurs on all the items simultaneously."
        ],
        [
            "OK, so this was one step of graph convolution.",
            "This happens in our case 2 times and then we have the fully connected layer which still occurs at the Atom level.",
            "So we are simply applying a fully connected operation there with again learn parameters for every Atom."
        ],
        [
            "OK, but we don't want to have a prediction per Atom.",
            "We actually want a prediction per molecule.",
            "So now we need to gather all this information at the molecule level and in deep came the strategy they took is to take the average.",
            "Overall the Atom features also to take the Max, do a concatenation and then hyperbolic tangent operation and this becomes then your representation of your molecule.",
            "That you have learned given your endpoint."
        ],
        [
            "OK, so for our model evaluation we use both cluster cross validation for all the reasons we already heard today of analog bias in your training set.",
            "We also have random cross validation and we also do time split.",
            "So in time splits you choose a test date, for example 2016 and everything earlier than this test date is your training set everything later on.",
            "Is your test set and these kind of simulates how good your model would be in a new project?",
            "Somehow it's hard to always simulate the real use case, and since we're doing regression's metrics we are looking at our square Spiderman.",
            "Spiderman is an evaluation of the ranking capability of your model."
        ],
        [
            "An I'm talking more precisely on a subset of those 70 endpoints, because that would be really long to go over today, so I'm only talking about those more physical chemical endpoints like log D, solubility, melting point, membrane affinity, and serum albumin binding and four solubility.",
            "You can see we have many different assays and actually only two of them, the DMSO and the powder are of interest for the company right now because that's the.",
            "It says that are in place and the orders is just data we have collected and we use as helper task to see what happens in a multi task setting.",
            "You also see we have a quite large amount of compounds for all those endpoints."
        ],
        [
            "So the first comparison is our baseline model, which is a random forest based on fingerprints with fully connected network, but single task network also based on the same descriptors and this is the evaluation on cluster cross validation and red line would be there is no difference between our baseline and the single test network.",
            "Everything on top of the baseline is the network is better.",
            "And everything under the the red line is the random forest is better.",
            "So we see that, at least for sparmann, we have actually quite a nice improvement when using your networks compared to the baseline, and also for our square, except for the melting point task.",
            "But overall it seems that since we have a lot of data, it seems to be beneficial to try neural networks.",
            "The different boxes of eternal targets, yes, exactly so these are.",
            "These are a log D solubility in DMSO.",
            "So we did powder melting point, another log D membrane affinity, and your human serum albumin.",
            "These seven together no.",
            "This is single task.",
            "We are comparing random forest with single task neural networks."
        ],
        [
            "OK, the next step was to compare.",
            "We are still in single task.",
            "We want to see whether single task fully connected network.",
            "How do they perform compared to graph convolutional single task networks and we saw that the Violet one is the fully connected, the green one is the graph convolutional network.",
            "We see that for all endpoints except one, which is this one.",
            "We have an improvement when using graph convolutional networks.",
            "So what happens for this task?",
            "It's the celebrity from powder and this is our smallest task.",
            "We have only 2000 data points there, so we assume that graph convolutions are not able to learn features or maybe overfitting, because this is again cluster split validation sets.",
            "But overall, for this type of data, it seems that graph convolutional networks work very well."
        ],
        [
            "And then the final model, the one that we have now deployed, is a multi task version of this.",
            "So we use graph convolutional networks and we have all those endpoints as the output, including the extra solubility tasks that are not of interest for prediction but are used as helper tasks.",
            "And these are the performance we obtain in cluster cross validation.",
            "So those were really high performance for us.",
            "We were quite pleased and we did a final comparison.",
            "That's a comparison between our final model in pink and what was previously in place in the system and used by the chemist that would be the violent one and his time split performance.",
            "So this actually justified move from the previous method to this method that is now used by our chemists."
        ],
        [
            "Small consideration of what is helpful and what hurts for different tasks.",
            "So sticking all the endpoints in multi task is improving performance for all those endpoints not really affecting this endpoint and hurting that endpoint.",
            "And this one is our largest task with more than 200,000 compounds and it has already been reported in literature that sometimes multitask.",
            "Affects negatively your larger endpoints because they would be actually quite happy to be alone predicted in a single test network, you have enough data to learn.",
            "And then whether we use or not, these solubility helper tasks, which were the other assays that also helps the solubility.",
            "That's a nice thing.",
            "That means that those related assays actually help your final prediction.",
            "It also helped some of those other tasks.",
            "And it also hurt the large task, which, yeah, that's probably the same effect as these multi task versus single terms."
        ],
        [
            "So that's the end of the second part.",
            "We saw that with the current amount of data we have admired for admit properties, it's actually worth it to try deep learning.",
            "It brings some performance improvement and the graph convolutional network worked very well for these physical chemical like endpoints.",
            "Multi task learning also improved performance, but it might hurt your largest task so you have to keep an eye on this.",
            "And yeah, you have always to take care of the output ranges.",
            "Or if you want to do multi task, always try to standardize your output and also check whether you should benefit by over emphasizing the loss on the smaller tasks versus the large tasks.",
            "Yes, I also tried.",
            "This approach is on all the other endpoints in my data set and graph convolutions is not a silver bullet, so it also doesn't work for all endpoints, so you have to try out basically."
        ],
        [
            "OK, now is the last part of the talk.",
            "Yes, sure.",
            "So I."
        ],
        [
            "I still do not quite understand which targets that you're trying to predict together.",
            "You get 7."
        ],
        [
            "Yes.",
            "No.",
            "Substance."
        ],
        [
            "So for this study, we are only looking at these endpoints, plus the three extra solubility assays we had.",
            "One of the reasons for not using all the 70 at once is that I observed that I could not find hyperparameters that would be good for all those 70, and I actually need to build good models for all those 70.",
            "It's not that I want an average good.",
            "I want a good for everyone.",
            "So in this case, sticking together those tasks was really beneficial and that's the final model.",
            "WS.",
            "Yes, of no.",
            "In the end no.",
            "For this case now.",
            "Yep.",
            "Yeah, you mentioned that some overlap between the tasks datasets helps.",
            "And I was wondering if you could give us a bit more details that I didn't do.",
            "A real study of removing overlapping and checking how.",
            "So I cannot really answer you in detail for that so, but I think there is a publication studying these exactly where they really aggressively remove overlaps and see how the performance just there is there.",
            "Just wondering, yeah, similar conclusions to mine, but I just assume that's the case.",
            "Thank you.",
            "Are the data available?",
            "No.",
            "I'm.",
            "Not.",
            "Via sources, yes, this is all internal data on.",
            "Yep.",
            "Degenerate.",
            "So they had an automatic process for the previous model.",
            "That was a PLS.",
            "So they yeah they just retrain it every week or every two weeks training the graph convolution is actually quite long on that amount of data.",
            "If you don't do any cross validation, just do a final training.",
            "I would say you need about one day.",
            "Yeah, so currently there is no retraining implemented, but it's in my task list, so it will be feasible later.",
            "We have more technical problems because you need to send your data to the GPU machine so that the training performs, so it's more technical than not feasible, right?",
            "It's just finding the.",
            "The solutions in our buyer infrastructure.",
            "So that you made the most just learning model.",
            "This.",
            "Best learning model.",
            "Reading.",
            "All of the.",
            "You learn or you tried like, for example, early stop being so you you had that all of the stuff you saved and then we say OK but I don't want the average so let's use the effort like 21 for anything.",
            "You'll only be cause for lung condition.",
            "This was the best and subsequently yeah I think actually Gunther is doing something like this, but I know I'm not doing this so I I trained for a fixed number of.",
            "The box actually not so many because of computational times.",
            "Yeah, I just did a little study of what happens if I remove one of those endpoints and I did the whole leave one end point out to check how the performance fluctuates and there is not much fluctuation actually."
        ],
        [
            "So if there are no more questions, the last part is about."
        ],
        [
            "Optimizing your compounds.",
            "OK, so you if you paid attention at the beginning, you recognize the slide.",
            "That's our autoencoder that learns the embedding and this has two parts, so the embedding is now a continuous space of 512 dimensions that you can navigate and the.",
            "Crucial component here now is the decoder that is able to give your compound given a position in the space."
        ],
        [
            "Yeah, so for example, we start with aspirin because we are a buyer.",
            "We encoded using the encoder.",
            "We get position in the space and then we can apply a small perturbation on this space and we get a new vector.",
            "And when we use the decoder on this we get a new compound.",
            "And this we did a bit more systematically, moving always smaller steps in One Direction or in the opposite direction.",
            "And in this case, this direction is the principle.",
            "The first principle component of the whole Campbell data set in that space, and what we see is that the molecules get bigger in that direction or smaller in the other direction.",
            "So it gives us a proof of concept that we are able to actually move in the space.",
            "And we can translate back to molecules that are valid molecules, and yeah, so that's kind of naive way to move in this space."
        ],
        [
            "No, we don't want to move naively in the space we want to steer our movements towards regions of the space that are of interest to us of interest.",
            "That could be a given biological activity.",
            "For example, there was a previous publication on that topic, but they use a slightly different method than us.",
            "I just took the picture because it's really nice showing how you can.",
            "Move in the molecular space and also improve your your property.",
            "So how could you do this?",
            "In theory you can apply a kind of brute force approach where you would enumerate as many compounds as possible and then use your property function prediction and prioritize this huge library and pick from there what you want.",
            "You can also use a generative model and fine tune it so that instead of representing components from very wide distribution like Campbell, you actually want now to find units that heat will generate compounds that are looking like actives toward your target, for example.",
            "So this requires retraining, or at least fine tuning your generative network.",
            "And then there are also many publications on reinforcement learning where you train a model to take actions that will trigger reward on.",
            "In this case the reward would be my final function got a higher score for example.",
            "Um?"
        ],
        [
            "What we want actually is to be able to use our embedding without retraining it.",
            "Ever so now it has been trained on 72 million compounds.",
            "We think it works nicely.",
            "We don't want to retrain it, and we also have the fact that reward functions can really vary across different stages of the project.",
            "Maybe at the beginning you only worry about your biological activity, but later on you also want to make sure you have a good solubility.",
            "No metabolic problems and if every time you would have to retrain something, that would be really time consuming and not convenient.",
            "And also we don't want to pre any merit compounds.",
            "So yeah, So what we decided to use is a."
        ],
        [
            "I thought called particle swarm optimization.",
            "It is a heuristic that tries to find to explore and find the best position in space given a reward function.",
            "So how it works is that you take starting position in your 512 space and you decide on how many particles you would like to have in your swarm.",
            "For example 200.",
            "So you draw randomly velocities and you apply this to your origonal starting point.",
            "By just adding the velocities, so now you have 200 particles that are kind of randomly spread on your space and you can compute your reward function.",
            "So maybe that's just a qsar model that predicts biological activity.",
            "And given this reward, you can update the position of the particles and the position of the particle depends on the previous velocity.",
            "So that's a kind of inertia.",
            "So if the particle was going very fast in that direction, it will keep going kind of maybe slower, but it will keep this overall direction.",
            "It also keeps its own best position from the past iterations.",
            "So how far am I from my best position?",
            "And how far am I from the overall best particle that we have observed until now?",
            "And once this is done, you update again the position of the particles and you go again the new cycle.",
            "So how does this work?",
            "First of all, it's kind of fast because the only slow step actually is the calculation of the reward function.",
            "So if you have a simple random forest model, it's going to be very fast if you have to go through many different neural networks, it gets a bit slower, obviously, but you don't have to retrain the generator or the autoencoder."
        ],
        [
            "So we compared it with different generative models.",
            "The comparison is not very extensive, we just took the the report from the paper.",
            "We didn't redo ourselves the implementation.",
            "And I just want to show you for this target EGFR.",
            "We collected data from Campbell.",
            "We build a qsar model using our embedding as descriptor, and we try to do the particle swarm optimization to maximize the predicted activity on this target.",
            "And this is how it looks like across 100 steps of optimization.",
            "So we start from a random Campbell point or from benzene.",
            "These are different.",
            "And we see that overall we are able to increase the predictive potency.",
            "And these are some of the compounds that were generated in the final swarm, so they yes.",
            "I don't know if you are an expert on EGFR and you can say something about it.",
            "I can't.",
            "Epidermal growth factor.",
            "So this is kind of a proof of concept that our molecule swarm optimization is able to optimize a starting point in the space.",
            "Given a reward function."
        ],
        [
            "Now we actually want something a bit more precise, because when you work in a drug discovery project, your chemist works on a given chemical series.",
            "And if you start proposing him compounds that have nothing to do with his chemical series, he will never want to test your compound because it's a lot of work to come up with a new synthesis route.",
            "So we want to be able to re strain our optimization for a given scaffold for example.",
            "To test this, we went through the literature and we picked that paper, which is a normal medicinal chemistry paper where the authors had a given scaffold and they tried to optimize BA C1 inhibition so they enumerate different R groups and they tested, or at least they reported in the paper 14 different combinations of those are groups.",
            "There are many more.",
            "I didn't put them.",
            "Untested them and look at activity, so that's their best compound.",
            "They measured it at 11 animal are quite potent.",
            "That's the result of their paper, so we thought, can we also do something like this?"
        ],
        [
            "So we would like to be able to reward the the swarm if the particle contains the scaffold.",
            "So we took the exact same scaffold as the paper.",
            "We also build ABAC E1 qsar model using Campbell data, taking care of removing all the compounds of the paper because otherwise it would be quite unfair.",
            "And we added a few chemistry health checkups we don't want extremely big compounds.",
            "We don't want toxic moieties, and we don't want a rare substructures which are substructures that almost never occur in Campbell.",
            "That's how we define them.",
            "So we're overall rewards function is a combination of those ingredients."
        ],
        [
            "And these are our three best compounds.",
            "After the optimization, we first see that the scaffold is present.",
            "We have the two aromatic rings that were required.",
            "We have in our group and this ring is present.",
            "So in this regard we have fulfilled our expectations.",
            "We also have good predicted P AC1 activity according to our qsar model, and if you compare with the.",
            "Paper compound you see some similarities.",
            "So why did we not find exactly this compound?",
            "While it's a heuristic, you are a randomly walking in the space and another point is that our qsar model doesn't think that this is so much active, so it thinks it has 100 nanomolar IC 50 and our candidates have a better predicted activity so.",
            "That's why they're probably considered better options by our reward function.",
            "Please compose that you suggested in testing.",
            "No.",
            "And what is the actual activity of this one?",
            "This one is 11 anomala.",
            "So that yeah.",
            "That was more of a proof of concept to say that yes, we can actually restrict our chemical space in the out."
        ],
        [
            "And now we want a final use case.",
            "That's a hard case because there we have two different biological activity that we want to.",
            "Maybe we want one to be high and want to be low.",
            "And we also want a good admit profile that means high solubility, good metabolic stability, good cell permeability.",
            "And these elements can enter into conflict, right?",
            "So you can have better biological activity by having poor solubility and also cell permeability relies on not being too soluble.",
            "So this is a typical use case.",
            "It's also hard for chemists to have this in their mind, so typically they would tell you, oh, I don't need your model to know that this compound will have this activity, because I have all my experience in this project.",
            "But when you try to sink in all those dimensions, I think the human brain cannot really cope so well.",
            "So this is a use case that would be useful for the chemists.",
            "So in total we had 10 individual objectives to fulfill and you can you have to scale all those objectives between zero and one.",
            "As always, because you are merging them and you can also put different weights which is very flexible if the chemist tells you for me the most important now is solubility, then you can put more weight on this part of the reward function."
        ],
        [
            "So these are our results after 200 steps of optimization.",
            "That's the overall score, so it's the aggregation of the 10 individual components of the reward function.",
            "Then we have the potency on the targets.",
            "That is, following our constraints, we wanted high potency on one and low potency under order, and here are the admit properties optimization.",
            "So we also were kind of able to keep them in good ranges.",
            "These are the compounds that were generated.",
            "They also look like normal compounds are nothing too crazy."
        ],
        [
            "So as a wrap up we have now the possibility to use our pre trained autoencoder that represents our compounds in a 512 continuous space and we can combine this with the reward function and the decoder to improve on starting point in context of drug discovery project for example.",
            "We can combine complex optimization by combining different individual scores, and it's very flexible because you can just switch the reward functions as you need and you don't need to retrain anything special.",
            "But of course, the main point here is that you rely on good proxy functions for your reward.",
            "So if your qsar model that predicts EGFR activity is completely off, then you are optimizing towards something that will be completely off.",
            "So in that sense it's very important to have good models when you put them together, so the paper has been written and published in Chem Archive and we are under submission for.",
            "Medical science."
        ],
        [
            "So final."
        ],
        [
            "Are we getting there that you just need to press a key and you get a drug?",
            "Maybe that's the dream of our CEO, but no, we are not there.",
            "Actually we we are trying to help so we have provided new representation of compounds we have also now better ultimate models that will be very helpful and we are able also to propose optimization solutions to our chemist so hopefully they are.",
            "Happier now."
        ],
        [
            "With this I want to mention Robin Winter.",
            "He's our PhD student and he did really a lot of the work I presented today.",
            "Okok levered is our supervisor and lava and tongue are working computational chemistry and are working together closely with me.",
            "So thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I actually joined by your two years ago for a kind of exploratory project.",
                    "label": 0
                },
                {
                    "sent": "There is set up to evaluate how deep learning could affect positively different aspects of drug discovery, and my focus was on chem informatics computational chemistry, and I just want to give you an overview today and there will be 3 parts that are kind of independent, but they are also very related.",
                    "label": 1
                },
                {
                    "sent": "You will see at the end why.",
                    "label": 0
                },
                {
                    "sent": "So if you want you can ask question whenever you feel like.",
                    "label": 0
                },
                {
                    "sent": "Also at the end of every subpart, if that's what.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a very brief introduction of computational chemistry in in a pharma company.",
                    "label": 1
                },
                {
                    "sent": "Where do computational chemists help?",
                    "label": 0
                },
                {
                    "sent": "At which stage of the project?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have to think first the target identification is something that more bioinformatics or biologists and pharmacology STS would come up with.",
                    "label": 0
                },
                {
                    "sent": "But once you have your protein target that then the search for a compound that addressed his target.",
                    "label": 0
                },
                {
                    "sent": "This is something where computational chemistry can help.",
                    "label": 1
                },
                {
                    "sent": "So we had this morning and talk from Andreas and he mentioned how considerations on how to design libraries on which you will screen later on.",
                    "label": 0
                },
                {
                    "sent": "Is very important, so there are computational chemistry can help.",
                    "label": 0
                },
                {
                    "sent": "Also, once you have a list of hits you need somehow to make a selection.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you want to expand your space by using some similarity.",
                    "label": 0
                },
                {
                    "sent": "So there again computational chemistry can help and later in later phases when you already have a candidate, maybe you want to refine it or understand how the relationship between structure and activity for your target.",
                    "label": 0
                },
                {
                    "sent": "Works and also there you can have docking studies or molecular dynamics to understand how the compound binds to the pocket, for example.",
                    "label": 0
                },
                {
                    "sent": "And also later you also want good admit properties for your compounds.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by now I think you are all experts on how to describe compounds because we have seen many, many talks that address this.",
                    "label": 0
                },
                {
                    "sent": "You can have pictures.",
                    "label": 0
                },
                {
                    "sent": "That's something that chemists are very comfort table with using because they draw their compounds on paper.",
                    "label": 0
                },
                {
                    "sent": "We have those famous circular fingerprints that were mentioned many times and there are also 3D descriptors, but those require that you know the confirmation of your small molecule, which is something that is hard to know in advance.",
                    "label": 1
                },
                {
                    "sent": "And there is also a kind of new approach that would be learning the fingerprints graph.",
                    "label": 0
                },
                {
                    "sent": "Convolutions can help there, and I will talk a bit more after after that.",
                    "label": 0
                },
                {
                    "sent": "And finally you have line notations, so this is like text that represents your molecule.",
                    "label": 0
                },
                {
                    "sent": "One very important is the smiles notation which encodes your molecular graph by letters like a C would be a carbon.",
                    "label": 0
                },
                {
                    "sent": "Whether it's capital letter or small means it's aromatic or aliphatic and so on.",
                    "label": 0
                },
                {
                    "sent": "So you can actually really encode the structure by it's just a string of characters.",
                    "label": 0
                },
                {
                    "sent": "And this we will use later on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we want to apply deep learning and computational chemistry?",
                    "label": 1
                },
                {
                    "sent": "The story started about 2012, when Merck published several bioactivity datasets and launched a competition in the platform Cargle, where a lot of machine learning guys compete to try to find the best models and the winning team.",
                    "label": 0
                },
                {
                    "sent": "It's listed there the names I guess you recognize my video for Hinton, so those are big names.",
                    "label": 1
                },
                {
                    "sent": "In deep learning they actually have no knowledge or not much knowledge of chemistry, but they want this competition and their their method was a mix of single task networks, multi task networks and gradient boosting methods.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this kind of sparked the interest of the community towards applying deep learning to different applications.",
                    "label": 0
                },
                {
                    "sent": "So after this you had regularly publications on toxicity prediction or bioactivity prediction using deep neural networks, and right now I think the slide would be covered with titles because every week or every second week new paper is published.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So in the first part I have, I'm talking about a paper that was published in chemical science.",
                    "label": 0
                },
                {
                    "sent": "It's called learning continues and data driven molecular descriptors by translating between equivalent chemical representations and we call them C DDD, that's continuous data driven descriptor.",
                    "label": 1
                },
                {
                    "sent": "We are not very good with names, so that's what we came up with.",
                    "label": 0
                },
                {
                    "sent": "So that I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea is borrowed from text translation, so when you try to use Google Translate currently what runs behind is a neural networks that has been trained to translate between 2 languages.",
                    "label": 0
                },
                {
                    "sent": "And that's also what we try to do with our chemical representations.",
                    "label": 0
                },
                {
                    "sent": "So we use the line notations, the string and that way we have text and we can use the same approaches as the text translation.",
                    "label": 0
                },
                {
                    "sent": "Methods and we have an auto encoder that network that contains the first part that is an encoder that takes your input string that represents your compound and try to go towards a kind of bottleneck which becomes your embedding.",
                    "label": 0
                },
                {
                    "sent": "So we have learned a lot about embedding in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "This is one of them and then the second part of the network is a decoder.",
                    "label": 0
                },
                {
                    "sent": "The decoder takes an embedding and try to reproduce.",
                    "label": 0
                },
                {
                    "sent": "The same string, or in our case, translate the string to a different representation of the same compound.",
                    "label": 0
                },
                {
                    "sent": "So in this case I took the name but actually in smiles notations depending on at which Atom you start you get different string for the same compound and that's what we use to be able to do the translation.",
                    "label": 0
                },
                {
                    "sent": "So on the one side you would have one variant of the smiles for that compound and at the output you try to predict the Canonical version which is canonicalized by your software.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Unique mapping between a smile and a structure.",
                    "label": 0
                },
                {
                    "sent": "Yes, so the idea is that once you have train your model, the embedding represents the compound because that's what is common between the two.",
                    "label": 0
                },
                {
                    "sent": "Representation is like the semantic.",
                    "label": 0
                },
                {
                    "sent": "If you would say between two sentences in two different languages, that's what you need to keep in your embedding, and we hope that this would be a good molecular descriptor.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how we did it?",
                    "label": 0
                },
                {
                    "sent": "The lucky part is that for training such embeddings, you don't need any specific data, you just need chemical structures.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of them, so we took everything in zinc and pubchem.",
                    "label": 0
                },
                {
                    "sent": "We applied some cleaning to keep things a bit drug like, or at least not too crazy in terms of chemistry, and we ended up with 72 million structures.",
                    "label": 0
                },
                {
                    "sent": "We use the Public Library all the kit to generate the smiles, which is what we need to train our embedding and we also use it to compute some very simple molecular descriptors that you can obtain for any structure.",
                    "label": 0
                },
                {
                    "sent": "I will show you later why.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, now in case you're not super familiar with text mining methods, I just want to quickly talk to you about how do you give the input to the model so it's called tokenization and it's actually trying to manage your string of characters into something that the computer can read easily.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that you have your whole database of characters and you enumerate them, so we have a C. We have a one.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So you have a big lookup table of characters or tokens actually for bromine or chlorine.",
                    "label": 0
                },
                {
                    "sent": "It's 2 characters, that's why it's not exactly a character, it's a token, and in the end what you do for representing a token is 1 hot encoding where you basically look up at which index is your token.",
                    "label": 0
                },
                {
                    "sent": "That's where you will have a one, and you will have zeros everywhere else.",
                    "label": 0
                },
                {
                    "sent": "And that's the input feeder.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the input feature.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the encoder is a recurrent neural network because we are dealing with sequences of tokens and that that's three layers of gated recurrent units that feed a fully connected layer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then have this embedding or latent space, that's the that's where the compression of information takes place, so it's a little bottleneck of 512 neurons.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the decoder, we have exactly a mirror of the architecture of the encoder and what we output then is a probability distribution on the tokens, so that's how you can sample the sequence by sampling from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And now we have a little extra trick that proved very useful to create a good features.",
                    "label": 0
                },
                {
                    "sent": "It is too at the same time as you are learning this autoencoder or translation encoder.",
                    "label": 0
                },
                {
                    "sent": "You also try to learn a classifier using is embedding and this classifier is trying to predict those chemical features you computed with our ticket.",
                    "label": 0
                },
                {
                    "sent": "So log P molecular weight.",
                    "label": 0
                },
                {
                    "sent": "This kind of things.",
                    "label": 0
                },
                {
                    "sent": "So we have nine of them.",
                    "label": 0
                },
                {
                    "sent": "And this helps driving the learning towards a feature representation that's helpful for downstream tasks, which is what we actually care about.",
                    "label": 0
                },
                {
                    "sent": "We don't really care about creating smiles.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So in this slide we see both the translation accuracy, so that's how good we are at recreating the proper Canonical smiles given smiles version of a compound.",
                    "label": 1
                },
                {
                    "sent": "And the other one is a downstream tasks.",
                    "label": 0
                },
                {
                    "sent": "So in this case we would extract the embedding at different stage of the learning and we use the embedding to learn a simple machine learning method like SVM, in this case to predict lipophilicity.",
                    "label": 1
                },
                {
                    "sent": "So we have a smaller data set where we know lipophilicity.",
                    "label": 1
                },
                {
                    "sent": "We use our embedding too as a starting descriptor and well overall what we see is while training.",
                    "label": 0
                },
                {
                    "sent": "So we get better translation accuracy and also this usually helps having a good downstream task performance, so that's kind of a sanity check and then we have different versions, so you see the red curve is the highest in terms of accuracy of translation, and that's the case when we start from a Canonical smiles and we reproduce the same Canonical smiles, so there is no translation is just a real autoencoder and we see that these performs.",
                    "label": 0
                },
                {
                    "sent": "Quite poorly in the downstream tasks, so the idea of translating is really helping.",
                    "label": 1
                },
                {
                    "sent": "So our.",
                    "label": 0
                },
                {
                    "sent": "Yes, the orange one is the one where we don't have the helper task, so trying to predict those few descriptors as a fully connected network while training the embedding.",
                    "label": 0
                },
                {
                    "sent": "So these performs a little bit worse compared to our final method, which is the blue one where we start from a smiles we predict the Canonical smiles and we also train to predict those few molecular descriptors.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we want to check whether our knew molecular descriptors perform well on downstream tasks and downstream tasks.",
                    "label": 0
                },
                {
                    "sent": "Here are predicting biological activity, so we we collected public datasets, both classification and regression, different sizes and we compare the performance of our descriptors called SDD.",
                    "label": 1
                },
                {
                    "sent": "Different versions of circular fingerprints and also.",
                    "label": 0
                },
                {
                    "sent": "The graph convolutional approach and for the first two we can also choose which algorithm to plug in, and we choose between random forests, supervector machine or gradient boosting.",
                    "label": 1
                },
                {
                    "sent": "Whatever works best.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the results we obtained for those data in black you have the circular fingerprints in Gray, the graph convolution, and in orange our descriptors and what you can just see is that overall there is no winner.",
                    "label": 0
                },
                {
                    "sent": "We are all performing similarly and these are hyper parameterized optimized performances.",
                    "label": 0
                },
                {
                    "sent": "On the left side you have random split cross validation versus cluster split cross validation.",
                    "label": 0
                },
                {
                    "sent": "You see the performance drops in cluster splits, which is something you would expect and just maybe.",
                    "label": 0
                },
                {
                    "sent": "One thing to note, for solubility the graph convolution worked quite well in the random split, but then the performance dropped in the cluster, cross validation split and our hypothesis is that these data set is very small and graph convolution are actually overfitting.",
                    "label": 0
                },
                {
                    "sent": "So in the random case and then this doesn't work well in cross in cluster cross validation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so overall we were quite happy that our learned descriptors seem to be as performant as the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy FP finger.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also tried to use it in a virtual screening benchmark, so we heard about virtual screening with Andreas today.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that you take a known active and you then rank database by similarity to the non active using your descriptors and those two database are actually benchmarks where you have sets of real actives and sets of compounds that are called decoys.",
                    "label": 0
                },
                {
                    "sent": "That you assume are negatives because they have no reported activity, but they look kind of similar to your actives.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to try to compete with different descriptors and computing similarities from your set of actives and seeing if you can rank your real actives before the decoys.",
                    "label": 0
                },
                {
                    "sent": "So that's the overall task and we are comparing with many different descriptors But in this picture we only show the best order descriptors and.",
                    "label": 0
                },
                {
                    "sent": "Powers so in black it's our method.",
                    "label": 0
                },
                {
                    "sent": "OK. And for the Directory of Useful Decoys we see that our method really outperforms the others significantly, and for the disorder it's also actually significant, but not as strongly.",
                    "label": 0
                },
                {
                    "sent": "So what we conclude here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or is that actually computing distances or similarities in our space seems to be nicely correlated with biological activity, so it's also a good point that our descriptors can be useful for computational chemistry.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall for this part we have learned in an unsupervised way an embedding for compounds by using translation between 2 string representation of the compounds and we can then extract this bottleneck to use as descriptor and we if you use it in combination with support vector machine.",
                    "label": 1
                },
                {
                    "sent": "In our experience that works quite well for Qsar models, and it's also good for virtual screening and then.",
                    "label": 0
                },
                {
                    "sent": "Something that you should keep in mind is that we have a decoder.",
                    "label": 0
                },
                {
                    "sent": "That means that given an embedding, we are able to go back to a compound, which is something we will exploit later, and it's it's something really cool, because if you have circular fingerprints, for example, you cannot deduce a structure from the fingerprint.",
                    "label": 0
                },
                {
                    "sent": "The code is available online.",
                    "label": 0
                },
                {
                    "sent": "You can just use it if you feel like trying it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is that we want to use as many chemical structures as possible, so we have their 72 million and if there is no biological activity available for 72 million compounds.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we didn't try that.",
                    "label": 0
                },
                {
                    "sent": "What is the dimensionality of your encoding space?",
                    "label": 0
                },
                {
                    "sent": "500, twelve, 512.",
                    "label": 0
                },
                {
                    "sent": "And did you experiment with different dimensions?",
                    "label": 0
                },
                {
                    "sent": "Yes, so actually I think this published version was later refined and we also have good performance on a smaller space.",
                    "label": 0
                },
                {
                    "sent": "I think 128 but I'm I cannot guarantee I I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Approach that is Linda.",
                    "label": 0
                },
                {
                    "sent": "In the next part I talk a lot about it.",
                    "label": 0
                },
                {
                    "sent": "Then you can ask more questions.",
                    "label": 0
                },
                {
                    "sent": "Your principles.",
                    "label": 0
                },
                {
                    "sent": "Is it for the IT was it so we folded them to three different sizes and we tried three different diameters and then we let them choose whichever algorithm work best for which data set?",
                    "label": 0
                },
                {
                    "sent": "So that's our final bar in the Bartlett.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we switch gears a little bit and we talk about admit prediction using multitask approach.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I admit for those who are not super familiar with this is absorption, distribution, metabolism, excretion and toxicity, and it actually tried to explain the fate of a drug.",
                    "label": 1
                },
                {
                    "sent": "Once you have taken it.",
                    "label": 0
                },
                {
                    "sent": "So how fast does it reach the target organ?",
                    "label": 1
                },
                {
                    "sent": "How much of it reached the target organ?",
                    "label": 0
                },
                {
                    "sent": "How quickly is it metabolised?",
                    "label": 0
                },
                {
                    "sent": "Is the expression going through the kidneys or through the Vine?",
                    "label": 1
                },
                {
                    "sent": "Does it reach the brain?",
                    "label": 0
                },
                {
                    "sent": "And do you want it to reach the brain or not?",
                    "label": 0
                },
                {
                    "sent": "So all these kind of questions are actually independent of the drug Discovery project.",
                    "label": 0
                },
                {
                    "sent": "They have to be addressed, so we have quite a lot of data.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the data we have 0.5 million and 70 endpoints.",
                    "label": 1
                },
                {
                    "sent": "And if you just say those numbers you think, well this is super great.",
                    "label": 0
                },
                {
                    "sent": "But then you you plot the distribution of how many compounds are actually measured in every endpoint and you already see my big problem that there are a few essays like log D where we have lots of compounds and then when we go towards more complicated assays or more.",
                    "label": 0
                },
                {
                    "sent": "Expensive or assays that you tried not to do because they are human, not human, but animal assays.",
                    "label": 0
                },
                {
                    "sent": "Then you have just a few hundred data points, so this is something that's that has to be.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taken into account.",
                    "label": 0
                },
                {
                    "sent": "So in this project we thought that we should compare how multitask networks perform compared to single task networks compared to baseline, because we already have quite some models running in the company that we are not using deep learning.",
                    "label": 0
                },
                {
                    "sent": "So if we want to switch to deep learning, it should be because we have a significant improvement and we also compare a graph convolutional not.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a very small side on multitask learning, so there are a lot of multi something in machine learning.",
                    "label": 1
                },
                {
                    "sent": "One is multi class, that's when you have different categories.",
                    "label": 0
                },
                {
                    "sent": "On your examples have to be categorized in one of them.",
                    "label": 0
                },
                {
                    "sent": "So for example in the Super famous Iris data set you have three possibilities for a new iris flower to be classified.",
                    "label": 0
                },
                {
                    "sent": "You also have the multi label aspect.",
                    "label": 0
                },
                {
                    "sent": "That's when you have different categories, but your new examples can be assigned different categories like a movie can be both an action movie on a science fiction movie, but we don't treat any of those.",
                    "label": 0
                },
                {
                    "sent": "We are trying to do multi task and in multi task we try to learn jointly different tasks and using a shared representation.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so why do we think that this could help so you can see a part of our metrics of admit assays and you can see already that some of them are correlated.",
                    "label": 0
                },
                {
                    "sent": "You also know from talking to the experimentalist that some of them are biologically related, or maybe they were measuring different species, so it makes sense maybe to learn them together.",
                    "label": 0
                },
                {
                    "sent": "And there are also experiments endpoints that are obtained from the exact same experiment.",
                    "label": 1
                },
                {
                    "sent": "But just different readouts maybe?",
                    "label": 0
                },
                {
                    "sent": "So what we expect as a benefit is that we could exploit those correlation between endpoints and also that we would benefit from an overall larger training set if you combine those large endpoints with smaller endpoints, then maybe you learn representations that are richer and that will benefit the smaller endpoint.",
                    "label": 1
                },
                {
                    "sent": "And overall it's also a regularization method because you are sharing your parameters across more tasks.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how it works in practice is that you combine different datasets into a larger data set that contains as X you have the union of all the compounds that were tested in any of those three ourselves, and then you're wise or your target now becomes a vector containing those three tasks, and if you don't have a measurement for combination of component ask, then you have a missing value we have talked.",
                    "label": 0
                },
                {
                    "sent": "Already about this.",
                    "label": 0
                },
                {
                    "sent": "So in theory there is no requirement to have overlaps of compounds in different assays that you stick together, but in practice.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's helpful.",
                    "label": 0
                },
                {
                    "sent": "So how do you train a neural network with this kind of multi task?",
                    "label": 0
                },
                {
                    "sent": "So you have to have an output notes for your end tasks and then you compute a loss for each of those tasks.",
                    "label": 0
                },
                {
                    "sent": "That means you only in your mini batch you look at the data where you have a measurement for that particular task and you compute the mean squared error over the mini batch.",
                    "label": 0
                },
                {
                    "sent": "So that gives you in this case 3 different tasks.",
                    "label": 1
                },
                {
                    "sent": "Specific losses and then you combine those into a global loss.",
                    "label": 0
                },
                {
                    "sent": "That's a weighted average of those task specific losses.",
                    "label": 0
                },
                {
                    "sent": "As a consequence, and this was important for me because I was working on regression, you need to have outputs in the same range, otherwise you might have weird things happening very high.",
                    "label": 0
                },
                {
                    "sent": "Mean squared error for values that are between 200 and 600, and then if you combine that with PIC 50s you would have problems when you do your weighted average.",
                    "label": 0
                },
                {
                    "sent": "So always standardize your tasks.",
                    "label": 0
                },
                {
                    "sent": "And then just one little consideration on how you do your weighted average.",
                    "label": 0
                },
                {
                    "sent": "You could just say OK, all tasks are equivalent, they all get the same weight one out of N. But then maybe you have cases where some tasks are really big, sometimes are really small, and then in your meeting, but you would have very few example of this small task and then maybe you are not learning very well because the losses diluted by all the.",
                    "label": 0
                },
                {
                    "sent": "Training set of the larger task.",
                    "label": 0
                },
                {
                    "sent": "So in this case you could decide to artificially increase the weight you give to the smaller task to work compared to the large task.",
                    "label": 1
                },
                {
                    "sent": "In practice you should just try out what works for you.",
                    "label": 0
                },
                {
                    "sent": "It depends on the cases.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will answer my recession specials, question graph, convolutions.",
                    "label": 0
                },
                {
                    "sent": "So the concept of graph convolution, and I'm pretty sure my income and I will talk a lot about it, but it's too to learn representation from a graph.",
                    "label": 1
                },
                {
                    "sent": "So you you think of your molecule as a graph where the nodes are the atoms and bonds are the edges between those atoms.",
                    "label": 0
                },
                {
                    "sent": "So you have the adjacency matrix that says item one is linked to at home.",
                    "label": 0
                },
                {
                    "sent": "Three for example.",
                    "label": 0
                },
                {
                    "sent": "And then you kind of propagate this Atom representation using this matrix, so the overall network architecture is like this.",
                    "label": 0
                },
                {
                    "sent": "So we have two steps of graph convolutions.",
                    "label": 0
                },
                {
                    "sent": "Then we have one fully connected layer and we have our output.",
                    "label": 0
                },
                {
                    "sent": "So this is a case of end to end learning.",
                    "label": 0
                },
                {
                    "sent": "You start from no features and then you learn features for your atoms that are very good for your task.",
                    "label": 0
                },
                {
                    "sent": "So you need actually a lot of data to do this.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm using the library called Deep Cam for running this and they have a very specific implementation of graph convolutions that is a little bit complex.",
                    "label": 0
                },
                {
                    "sent": "I'm just going quickly over it, but there are other software out there that can do it with different approaches, so it's just one example.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine our input graph looks like this, so the color of the items could be just simply different item types and the first step is actually too.",
                    "label": 0
                },
                {
                    "sent": "Compute features for those atoms.",
                    "label": 0
                },
                {
                    "sent": "If you don't know anything, you can use one hot encoding.",
                    "label": 0
                },
                {
                    "sent": "That would be the case of America's first networks we saw today, but in this case, since we are in chemistry, we actually can quickly compute some features using our ticket.",
                    "label": 0
                },
                {
                    "sent": "That will be our safe input descriptor space.",
                    "label": 0
                },
                {
                    "sent": "The first step is to actually look at yourself and at your neighbors, and we have some of the neighborhood for every Atom that is then propagated well enough.",
                    "label": 0
                },
                {
                    "sent": "In propagation, those are learned parameters that are shared across the older atoms, so we have one set of parameters for the Atom themselves, and one set of parameters for the neighborhoods.",
                    "label": 1
                },
                {
                    "sent": "And this operation occurs simultaneously on all atoms.",
                    "label": 0
                },
                {
                    "sent": "We then sum these on.",
                    "label": 0
                },
                {
                    "sent": "This becomes the new representation for the at all.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have then a non linearity operation on every Atom.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And after this we again look at the neighbors and we do a graph pooling operation, which is kind of similar to the pooling operation in normal convolutional networks that you are maybe more familiar with.",
                    "label": 0
                },
                {
                    "sent": "So basically you concatenate your features with the ones of your neighbors and you take the Max.",
                    "label": 0
                },
                {
                    "sent": "That's your new representation, and again this occurs on all the items simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was one step of graph convolution.",
                    "label": 1
                },
                {
                    "sent": "This happens in our case 2 times and then we have the fully connected layer which still occurs at the Atom level.",
                    "label": 0
                },
                {
                    "sent": "So we are simply applying a fully connected operation there with again learn parameters for every Atom.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but we don't want to have a prediction per Atom.",
                    "label": 0
                },
                {
                    "sent": "We actually want a prediction per molecule.",
                    "label": 0
                },
                {
                    "sent": "So now we need to gather all this information at the molecule level and in deep came the strategy they took is to take the average.",
                    "label": 0
                },
                {
                    "sent": "Overall the Atom features also to take the Max, do a concatenation and then hyperbolic tangent operation and this becomes then your representation of your molecule.",
                    "label": 0
                },
                {
                    "sent": "That you have learned given your endpoint.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for our model evaluation we use both cluster cross validation for all the reasons we already heard today of analog bias in your training set.",
                    "label": 0
                },
                {
                    "sent": "We also have random cross validation and we also do time split.",
                    "label": 1
                },
                {
                    "sent": "So in time splits you choose a test date, for example 2016 and everything earlier than this test date is your training set everything later on.",
                    "label": 1
                },
                {
                    "sent": "Is your test set and these kind of simulates how good your model would be in a new project?",
                    "label": 0
                },
                {
                    "sent": "Somehow it's hard to always simulate the real use case, and since we're doing regression's metrics we are looking at our square Spiderman.",
                    "label": 0
                },
                {
                    "sent": "Spiderman is an evaluation of the ranking capability of your model.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An I'm talking more precisely on a subset of those 70 endpoints, because that would be really long to go over today, so I'm only talking about those more physical chemical endpoints like log D, solubility, melting point, membrane affinity, and serum albumin binding and four solubility.",
                    "label": 1
                },
                {
                    "sent": "You can see we have many different assays and actually only two of them, the DMSO and the powder are of interest for the company right now because that's the.",
                    "label": 0
                },
                {
                    "sent": "It says that are in place and the orders is just data we have collected and we use as helper task to see what happens in a multi task setting.",
                    "label": 0
                },
                {
                    "sent": "You also see we have a quite large amount of compounds for all those endpoints.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first comparison is our baseline model, which is a random forest based on fingerprints with fully connected network, but single task network also based on the same descriptors and this is the evaluation on cluster cross validation and red line would be there is no difference between our baseline and the single test network.",
                    "label": 0
                },
                {
                    "sent": "Everything on top of the baseline is the network is better.",
                    "label": 0
                },
                {
                    "sent": "And everything under the the red line is the random forest is better.",
                    "label": 0
                },
                {
                    "sent": "So we see that, at least for sparmann, we have actually quite a nice improvement when using your networks compared to the baseline, and also for our square, except for the melting point task.",
                    "label": 0
                },
                {
                    "sent": "But overall it seems that since we have a lot of data, it seems to be beneficial to try neural networks.",
                    "label": 0
                },
                {
                    "sent": "The different boxes of eternal targets, yes, exactly so these are.",
                    "label": 0
                },
                {
                    "sent": "These are a log D solubility in DMSO.",
                    "label": 0
                },
                {
                    "sent": "So we did powder melting point, another log D membrane affinity, and your human serum albumin.",
                    "label": 1
                },
                {
                    "sent": "These seven together no.",
                    "label": 0
                },
                {
                    "sent": "This is single task.",
                    "label": 0
                },
                {
                    "sent": "We are comparing random forest with single task neural networks.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the next step was to compare.",
                    "label": 0
                },
                {
                    "sent": "We are still in single task.",
                    "label": 0
                },
                {
                    "sent": "We want to see whether single task fully connected network.",
                    "label": 1
                },
                {
                    "sent": "How do they perform compared to graph convolutional single task networks and we saw that the Violet one is the fully connected, the green one is the graph convolutional network.",
                    "label": 0
                },
                {
                    "sent": "We see that for all endpoints except one, which is this one.",
                    "label": 0
                },
                {
                    "sent": "We have an improvement when using graph convolutional networks.",
                    "label": 1
                },
                {
                    "sent": "So what happens for this task?",
                    "label": 0
                },
                {
                    "sent": "It's the celebrity from powder and this is our smallest task.",
                    "label": 0
                },
                {
                    "sent": "We have only 2000 data points there, so we assume that graph convolutions are not able to learn features or maybe overfitting, because this is again cluster split validation sets.",
                    "label": 0
                },
                {
                    "sent": "But overall, for this type of data, it seems that graph convolutional networks work very well.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the final model, the one that we have now deployed, is a multi task version of this.",
                    "label": 0
                },
                {
                    "sent": "So we use graph convolutional networks and we have all those endpoints as the output, including the extra solubility tasks that are not of interest for prediction but are used as helper tasks.",
                    "label": 0
                },
                {
                    "sent": "And these are the performance we obtain in cluster cross validation.",
                    "label": 0
                },
                {
                    "sent": "So those were really high performance for us.",
                    "label": 0
                },
                {
                    "sent": "We were quite pleased and we did a final comparison.",
                    "label": 0
                },
                {
                    "sent": "That's a comparison between our final model in pink and what was previously in place in the system and used by the chemist that would be the violent one and his time split performance.",
                    "label": 0
                },
                {
                    "sent": "So this actually justified move from the previous method to this method that is now used by our chemists.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Small consideration of what is helpful and what hurts for different tasks.",
                    "label": 0
                },
                {
                    "sent": "So sticking all the endpoints in multi task is improving performance for all those endpoints not really affecting this endpoint and hurting that endpoint.",
                    "label": 1
                },
                {
                    "sent": "And this one is our largest task with more than 200,000 compounds and it has already been reported in literature that sometimes multitask.",
                    "label": 1
                },
                {
                    "sent": "Affects negatively your larger endpoints because they would be actually quite happy to be alone predicted in a single test network, you have enough data to learn.",
                    "label": 0
                },
                {
                    "sent": "And then whether we use or not, these solubility helper tasks, which were the other assays that also helps the solubility.",
                    "label": 1
                },
                {
                    "sent": "That's a nice thing.",
                    "label": 0
                },
                {
                    "sent": "That means that those related assays actually help your final prediction.",
                    "label": 0
                },
                {
                    "sent": "It also helped some of those other tasks.",
                    "label": 0
                },
                {
                    "sent": "And it also hurt the large task, which, yeah, that's probably the same effect as these multi task versus single terms.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the end of the second part.",
                    "label": 0
                },
                {
                    "sent": "We saw that with the current amount of data we have admired for admit properties, it's actually worth it to try deep learning.",
                    "label": 1
                },
                {
                    "sent": "It brings some performance improvement and the graph convolutional network worked very well for these physical chemical like endpoints.",
                    "label": 0
                },
                {
                    "sent": "Multi task learning also improved performance, but it might hurt your largest task so you have to keep an eye on this.",
                    "label": 1
                },
                {
                    "sent": "And yeah, you have always to take care of the output ranges.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to do multi task, always try to standardize your output and also check whether you should benefit by over emphasizing the loss on the smaller tasks versus the large tasks.",
                    "label": 1
                },
                {
                    "sent": "Yes, I also tried.",
                    "label": 0
                },
                {
                    "sent": "This approach is on all the other endpoints in my data set and graph convolutions is not a silver bullet, so it also doesn't work for all endpoints, so you have to try out basically.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now is the last part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Yes, sure.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I still do not quite understand which targets that you're trying to predict together.",
                    "label": 0
                },
                {
                    "sent": "You get 7.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Substance.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this study, we are only looking at these endpoints, plus the three extra solubility assays we had.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons for not using all the 70 at once is that I observed that I could not find hyperparameters that would be good for all those 70, and I actually need to build good models for all those 70.",
                    "label": 0
                },
                {
                    "sent": "It's not that I want an average good.",
                    "label": 0
                },
                {
                    "sent": "I want a good for everyone.",
                    "label": 0
                },
                {
                    "sent": "So in this case, sticking together those tasks was really beneficial and that's the final model.",
                    "label": 0
                },
                {
                    "sent": "WS.",
                    "label": 0
                },
                {
                    "sent": "Yes, of no.",
                    "label": 0
                },
                {
                    "sent": "In the end no.",
                    "label": 0
                },
                {
                    "sent": "For this case now.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you mentioned that some overlap between the tasks datasets helps.",
                    "label": 0
                },
                {
                    "sent": "And I was wondering if you could give us a bit more details that I didn't do.",
                    "label": 0
                },
                {
                    "sent": "A real study of removing overlapping and checking how.",
                    "label": 0
                },
                {
                    "sent": "So I cannot really answer you in detail for that so, but I think there is a publication studying these exactly where they really aggressively remove overlaps and see how the performance just there is there.",
                    "label": 0
                },
                {
                    "sent": "Just wondering, yeah, similar conclusions to mine, but I just assume that's the case.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Are the data available?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "Via sources, yes, this is all internal data on.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Degenerate.",
                    "label": 0
                },
                {
                    "sent": "So they had an automatic process for the previous model.",
                    "label": 0
                },
                {
                    "sent": "That was a PLS.",
                    "label": 0
                },
                {
                    "sent": "So they yeah they just retrain it every week or every two weeks training the graph convolution is actually quite long on that amount of data.",
                    "label": 0
                },
                {
                    "sent": "If you don't do any cross validation, just do a final training.",
                    "label": 0
                },
                {
                    "sent": "I would say you need about one day.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so currently there is no retraining implemented, but it's in my task list, so it will be feasible later.",
                    "label": 0
                },
                {
                    "sent": "We have more technical problems because you need to send your data to the GPU machine so that the training performs, so it's more technical than not feasible, right?",
                    "label": 0
                },
                {
                    "sent": "It's just finding the.",
                    "label": 0
                },
                {
                    "sent": "The solutions in our buyer infrastructure.",
                    "label": 0
                },
                {
                    "sent": "So that you made the most just learning model.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Best learning model.",
                    "label": 0
                },
                {
                    "sent": "Reading.",
                    "label": 0
                },
                {
                    "sent": "All of the.",
                    "label": 0
                },
                {
                    "sent": "You learn or you tried like, for example, early stop being so you you had that all of the stuff you saved and then we say OK but I don't want the average so let's use the effort like 21 for anything.",
                    "label": 0
                },
                {
                    "sent": "You'll only be cause for lung condition.",
                    "label": 0
                },
                {
                    "sent": "This was the best and subsequently yeah I think actually Gunther is doing something like this, but I know I'm not doing this so I I trained for a fixed number of.",
                    "label": 0
                },
                {
                    "sent": "The box actually not so many because of computational times.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I just did a little study of what happens if I remove one of those endpoints and I did the whole leave one end point out to check how the performance fluctuates and there is not much fluctuation actually.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if there are no more questions, the last part is about.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimizing your compounds.",
                    "label": 0
                },
                {
                    "sent": "OK, so you if you paid attention at the beginning, you recognize the slide.",
                    "label": 0
                },
                {
                    "sent": "That's our autoencoder that learns the embedding and this has two parts, so the embedding is now a continuous space of 512 dimensions that you can navigate and the.",
                    "label": 0
                },
                {
                    "sent": "Crucial component here now is the decoder that is able to give your compound given a position in the space.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so for example, we start with aspirin because we are a buyer.",
                    "label": 0
                },
                {
                    "sent": "We encoded using the encoder.",
                    "label": 0
                },
                {
                    "sent": "We get position in the space and then we can apply a small perturbation on this space and we get a new vector.",
                    "label": 0
                },
                {
                    "sent": "And when we use the decoder on this we get a new compound.",
                    "label": 0
                },
                {
                    "sent": "And this we did a bit more systematically, moving always smaller steps in One Direction or in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "And in this case, this direction is the principle.",
                    "label": 0
                },
                {
                    "sent": "The first principle component of the whole Campbell data set in that space, and what we see is that the molecules get bigger in that direction or smaller in the other direction.",
                    "label": 1
                },
                {
                    "sent": "So it gives us a proof of concept that we are able to actually move in the space.",
                    "label": 0
                },
                {
                    "sent": "And we can translate back to molecules that are valid molecules, and yeah, so that's kind of naive way to move in this space.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, we don't want to move naively in the space we want to steer our movements towards regions of the space that are of interest to us of interest.",
                    "label": 1
                },
                {
                    "sent": "That could be a given biological activity.",
                    "label": 0
                },
                {
                    "sent": "For example, there was a previous publication on that topic, but they use a slightly different method than us.",
                    "label": 0
                },
                {
                    "sent": "I just took the picture because it's really nice showing how you can.",
                    "label": 0
                },
                {
                    "sent": "Move in the molecular space and also improve your your property.",
                    "label": 0
                },
                {
                    "sent": "So how could you do this?",
                    "label": 0
                },
                {
                    "sent": "In theory you can apply a kind of brute force approach where you would enumerate as many compounds as possible and then use your property function prediction and prioritize this huge library and pick from there what you want.",
                    "label": 0
                },
                {
                    "sent": "You can also use a generative model and fine tune it so that instead of representing components from very wide distribution like Campbell, you actually want now to find units that heat will generate compounds that are looking like actives toward your target, for example.",
                    "label": 0
                },
                {
                    "sent": "So this requires retraining, or at least fine tuning your generative network.",
                    "label": 0
                },
                {
                    "sent": "And then there are also many publications on reinforcement learning where you train a model to take actions that will trigger reward on.",
                    "label": 1
                },
                {
                    "sent": "In this case the reward would be my final function got a higher score for example.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want actually is to be able to use our embedding without retraining it.",
                    "label": 0
                },
                {
                    "sent": "Ever so now it has been trained on 72 million compounds.",
                    "label": 0
                },
                {
                    "sent": "We think it works nicely.",
                    "label": 0
                },
                {
                    "sent": "We don't want to retrain it, and we also have the fact that reward functions can really vary across different stages of the project.",
                    "label": 0
                },
                {
                    "sent": "Maybe at the beginning you only worry about your biological activity, but later on you also want to make sure you have a good solubility.",
                    "label": 0
                },
                {
                    "sent": "No metabolic problems and if every time you would have to retrain something, that would be really time consuming and not convenient.",
                    "label": 0
                },
                {
                    "sent": "And also we don't want to pre any merit compounds.",
                    "label": 0
                },
                {
                    "sent": "So yeah, So what we decided to use is a.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I thought called particle swarm optimization.",
                    "label": 1
                },
                {
                    "sent": "It is a heuristic that tries to find to explore and find the best position in space given a reward function.",
                    "label": 1
                },
                {
                    "sent": "So how it works is that you take starting position in your 512 space and you decide on how many particles you would like to have in your swarm.",
                    "label": 0
                },
                {
                    "sent": "For example 200.",
                    "label": 0
                },
                {
                    "sent": "So you draw randomly velocities and you apply this to your origonal starting point.",
                    "label": 0
                },
                {
                    "sent": "By just adding the velocities, so now you have 200 particles that are kind of randomly spread on your space and you can compute your reward function.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's just a qsar model that predicts biological activity.",
                    "label": 0
                },
                {
                    "sent": "And given this reward, you can update the position of the particles and the position of the particle depends on the previous velocity.",
                    "label": 1
                },
                {
                    "sent": "So that's a kind of inertia.",
                    "label": 1
                },
                {
                    "sent": "So if the particle was going very fast in that direction, it will keep going kind of maybe slower, but it will keep this overall direction.",
                    "label": 0
                },
                {
                    "sent": "It also keeps its own best position from the past iterations.",
                    "label": 0
                },
                {
                    "sent": "So how far am I from my best position?",
                    "label": 0
                },
                {
                    "sent": "And how far am I from the overall best particle that we have observed until now?",
                    "label": 0
                },
                {
                    "sent": "And once this is done, you update again the position of the particles and you go again the new cycle.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "First of all, it's kind of fast because the only slow step actually is the calculation of the reward function.",
                    "label": 0
                },
                {
                    "sent": "So if you have a simple random forest model, it's going to be very fast if you have to go through many different neural networks, it gets a bit slower, obviously, but you don't have to retrain the generator or the autoencoder.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compared it with different generative models.",
                    "label": 0
                },
                {
                    "sent": "The comparison is not very extensive, we just took the the report from the paper.",
                    "label": 0
                },
                {
                    "sent": "We didn't redo ourselves the implementation.",
                    "label": 0
                },
                {
                    "sent": "And I just want to show you for this target EGFR.",
                    "label": 0
                },
                {
                    "sent": "We collected data from Campbell.",
                    "label": 0
                },
                {
                    "sent": "We build a qsar model using our embedding as descriptor, and we try to do the particle swarm optimization to maximize the predicted activity on this target.",
                    "label": 0
                },
                {
                    "sent": "And this is how it looks like across 100 steps of optimization.",
                    "label": 0
                },
                {
                    "sent": "So we start from a random Campbell point or from benzene.",
                    "label": 0
                },
                {
                    "sent": "These are different.",
                    "label": 0
                },
                {
                    "sent": "And we see that overall we are able to increase the predictive potency.",
                    "label": 0
                },
                {
                    "sent": "And these are some of the compounds that were generated in the final swarm, so they yes.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you are an expert on EGFR and you can say something about it.",
                    "label": 0
                },
                {
                    "sent": "I can't.",
                    "label": 0
                },
                {
                    "sent": "Epidermal growth factor.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a proof of concept that our molecule swarm optimization is able to optimize a starting point in the space.",
                    "label": 1
                },
                {
                    "sent": "Given a reward function.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we actually want something a bit more precise, because when you work in a drug discovery project, your chemist works on a given chemical series.",
                    "label": 0
                },
                {
                    "sent": "And if you start proposing him compounds that have nothing to do with his chemical series, he will never want to test your compound because it's a lot of work to come up with a new synthesis route.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to re strain our optimization for a given scaffold for example.",
                    "label": 0
                },
                {
                    "sent": "To test this, we went through the literature and we picked that paper, which is a normal medicinal chemistry paper where the authors had a given scaffold and they tried to optimize BA C1 inhibition so they enumerate different R groups and they tested, or at least they reported in the paper 14 different combinations of those are groups.",
                    "label": 0
                },
                {
                    "sent": "There are many more.",
                    "label": 0
                },
                {
                    "sent": "I didn't put them.",
                    "label": 0
                },
                {
                    "sent": "Untested them and look at activity, so that's their best compound.",
                    "label": 0
                },
                {
                    "sent": "They measured it at 11 animal are quite potent.",
                    "label": 0
                },
                {
                    "sent": "That's the result of their paper, so we thought, can we also do something like this?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we would like to be able to reward the the swarm if the particle contains the scaffold.",
                    "label": 1
                },
                {
                    "sent": "So we took the exact same scaffold as the paper.",
                    "label": 1
                },
                {
                    "sent": "We also build ABAC E1 qsar model using Campbell data, taking care of removing all the compounds of the paper because otherwise it would be quite unfair.",
                    "label": 0
                },
                {
                    "sent": "And we added a few chemistry health checkups we don't want extremely big compounds.",
                    "label": 1
                },
                {
                    "sent": "We don't want toxic moieties, and we don't want a rare substructures which are substructures that almost never occur in Campbell.",
                    "label": 0
                },
                {
                    "sent": "That's how we define them.",
                    "label": 0
                },
                {
                    "sent": "So we're overall rewards function is a combination of those ingredients.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are our three best compounds.",
                    "label": 0
                },
                {
                    "sent": "After the optimization, we first see that the scaffold is present.",
                    "label": 0
                },
                {
                    "sent": "We have the two aromatic rings that were required.",
                    "label": 0
                },
                {
                    "sent": "We have in our group and this ring is present.",
                    "label": 0
                },
                {
                    "sent": "So in this regard we have fulfilled our expectations.",
                    "label": 0
                },
                {
                    "sent": "We also have good predicted P AC1 activity according to our qsar model, and if you compare with the.",
                    "label": 1
                },
                {
                    "sent": "Paper compound you see some similarities.",
                    "label": 0
                },
                {
                    "sent": "So why did we not find exactly this compound?",
                    "label": 0
                },
                {
                    "sent": "While it's a heuristic, you are a randomly walking in the space and another point is that our qsar model doesn't think that this is so much active, so it thinks it has 100 nanomolar IC 50 and our candidates have a better predicted activity so.",
                    "label": 0
                },
                {
                    "sent": "That's why they're probably considered better options by our reward function.",
                    "label": 0
                },
                {
                    "sent": "Please compose that you suggested in testing.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "And what is the actual activity of this one?",
                    "label": 0
                },
                {
                    "sent": "This one is 11 anomala.",
                    "label": 0
                },
                {
                    "sent": "So that yeah.",
                    "label": 0
                },
                {
                    "sent": "That was more of a proof of concept to say that yes, we can actually restrict our chemical space in the out.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we want a final use case.",
                    "label": 0
                },
                {
                    "sent": "That's a hard case because there we have two different biological activity that we want to.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want one to be high and want to be low.",
                    "label": 0
                },
                {
                    "sent": "And we also want a good admit profile that means high solubility, good metabolic stability, good cell permeability.",
                    "label": 1
                },
                {
                    "sent": "And these elements can enter into conflict, right?",
                    "label": 0
                },
                {
                    "sent": "So you can have better biological activity by having poor solubility and also cell permeability relies on not being too soluble.",
                    "label": 1
                },
                {
                    "sent": "So this is a typical use case.",
                    "label": 0
                },
                {
                    "sent": "It's also hard for chemists to have this in their mind, so typically they would tell you, oh, I don't need your model to know that this compound will have this activity, because I have all my experience in this project.",
                    "label": 0
                },
                {
                    "sent": "But when you try to sink in all those dimensions, I think the human brain cannot really cope so well.",
                    "label": 1
                },
                {
                    "sent": "So this is a use case that would be useful for the chemists.",
                    "label": 0
                },
                {
                    "sent": "So in total we had 10 individual objectives to fulfill and you can you have to scale all those objectives between zero and one.",
                    "label": 1
                },
                {
                    "sent": "As always, because you are merging them and you can also put different weights which is very flexible if the chemist tells you for me the most important now is solubility, then you can put more weight on this part of the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are our results after 200 steps of optimization.",
                    "label": 0
                },
                {
                    "sent": "That's the overall score, so it's the aggregation of the 10 individual components of the reward function.",
                    "label": 1
                },
                {
                    "sent": "Then we have the potency on the targets.",
                    "label": 0
                },
                {
                    "sent": "That is, following our constraints, we wanted high potency on one and low potency under order, and here are the admit properties optimization.",
                    "label": 0
                },
                {
                    "sent": "So we also were kind of able to keep them in good ranges.",
                    "label": 0
                },
                {
                    "sent": "These are the compounds that were generated.",
                    "label": 0
                },
                {
                    "sent": "They also look like normal compounds are nothing too crazy.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a wrap up we have now the possibility to use our pre trained autoencoder that represents our compounds in a 512 continuous space and we can combine this with the reward function and the decoder to improve on starting point in context of drug discovery project for example.",
                    "label": 0
                },
                {
                    "sent": "We can combine complex optimization by combining different individual scores, and it's very flexible because you can just switch the reward functions as you need and you don't need to retrain anything special.",
                    "label": 1
                },
                {
                    "sent": "But of course, the main point here is that you rely on good proxy functions for your reward.",
                    "label": 0
                },
                {
                    "sent": "So if your qsar model that predicts EGFR activity is completely off, then you are optimizing towards something that will be completely off.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's very important to have good models when you put them together, so the paper has been written and published in Chem Archive and we are under submission for.",
                    "label": 0
                },
                {
                    "sent": "Medical science.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So final.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are we getting there that you just need to press a key and you get a drug?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's the dream of our CEO, but no, we are not there.",
                    "label": 0
                },
                {
                    "sent": "Actually we we are trying to help so we have provided new representation of compounds we have also now better ultimate models that will be very helpful and we are able also to propose optimization solutions to our chemist so hopefully they are.",
                    "label": 0
                },
                {
                    "sent": "Happier now.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this I want to mention Robin Winter.",
                    "label": 1
                },
                {
                    "sent": "He's our PhD student and he did really a lot of the work I presented today.",
                    "label": 0
                },
                {
                    "sent": "Okok levered is our supervisor and lava and tongue are working computational chemistry and are working together closely with me.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 1
                }
            ]
        }
    }
}