{
    "id": "mrqki7traqv5zvc4jw6piy5gvykvgv3t",
    "title": "Duality Between Estimation and Control",
    "info": {
        "author": [
            "Sanjoy K. Mitter, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Oct. 16, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Physics->Statistical Physics",
            "Top->Mathematics->Control Theory"
        ]
    },
    "url": "http://videolectures.net/cyberstat2012_mitter_duality/",
    "segmentation": [
        [
            "Thank you for inviting me to speak here.",
            "So."
        ],
        [
            "See.",
            "So I've been interested in the last few years on on.",
            "So they're investigating.",
            "The relationships between filtering theory or estimation problems.",
            "In more general terms, information theory answer typical mechanics and.",
            "So if so this, so the broad idea was this interesting book called.",
            "Gives measures.",
            "And phase transitions.",
            "By georgy.",
            "It works better.",
            "Thanks anyway yeah.",
            "And I believe it's Chapter 16.",
            "Which is devoted to the Gibbs variational principle.",
            "And so this chapter essentially gives you a characterization of translation invariant Gibbs measures in terms of minimizing a certain free energy.",
            "And behind it is this this.",
            "I would say the devolution view where instead of if you think of the color of extension theorem where you look at marginal densities, Ann and consistency as you look at the measures over a larger time interval and projected down to a lower smaller time interval.",
            "They should coincide.",
            "The same argument is carried over for conditional distributions rather than marginal distribution.",
            "And in this case there there maybe there isn't a single extension.",
            "If you like their whole convex compact set of Gibbs measures OK, and what I'm going to talk about is, is what I will not talk about is the recent work that I've done jointly with Nigel Newton on on giving a proof of the noisy channel coding theorem as a problem in statistical mechanics as a problem?",
            "Gives variational principle.",
            "I could talk about it informally later on what I will talk about is to see Bayesian inference as as a problem of free energy minimization, and that also is related to this duality between estimation and control.",
            "Password.",
            "OK, so the first part is now standard material in nonlinear filtering.",
            "And so the I'll quickly go over that.",
            "So the idea of a filter if you like.",
            "Or an estimator?",
            "That you have some observations, say Whitey.",
            "The grades that are equal to 0 and perhaps some initial state X0, which is random an.",
            "So the idea of a filter is so observations are coming in to the filter an.",
            "And what the filter produces is.",
            "So there's a notion of a state XD.",
            "And what the filter produces is the conditional distribution of XT given.",
            "Observation say from Delta T. So the filter is really a map from the space of observations and perhaps initial state into the space of conditional distribution, so those are measured on XT an.",
            "If XD were say a Markov diffusion process, then the several descriptions of that markup diffusion process as a stochastic differential equation perhaps, but also in terms of if there's a density than the focal Planck equation which describes the evolution of XD.",
            "But when we have.",
            "Observations we're interested in conditional distribution, so we have to describe how the conditional distribution evolves in time.",
            "So if you wish in statistical mechanics sort of states are probability measures and observables are functions from some configuration space into, say, the real line.",
            "But what we're interested in instead of probability measure we're interested in conditional measures.",
            "And if you like conditional statistics.",
            "Right, so integrating that against functions.",
            "And the first part of my of my talk we just standard.",
            "I'll go quickly is to describe how this conditional distribution evolves in time OK. Anne.",
            "So there are several problems, so this is the so called filtering problem.",
            "So you want to estimate the state at time T given observations.",
            "There are other problems there, for example the prediction problem where would be interested in estimating XA at time T + S. Given observations after time T, for example, or the path estimation problem where you're interested in estimating the whole path.",
            "Boss estimation.",
            "And we would be interested in characterizing this this conditional distribution.",
            "So, by the way, the pause estimation.",
            "I mean that's.",
            "So if this were, say, a finite state Markov process, and why zero or some functions of X and then the part estimation problem is there is the decoding problem OK?",
            "The distinction being though in the decoding in information theory context you can influence what you want to estimate.",
            "Right, the source if you like, by access by doing coding.",
            "So I'm control tourist in some sense.",
            "So you could think of the code or somehow as a controller and the decoder is an estimator in some sense, OK, but they're on a finite time.",
            "They interact in complicated ways.",
            "And so this I made this comment about gives a variational principle.",
            "So in order to simplify matters, you have to look at the the problem of decoding over sale.",
            "You would have to allow infinite delays essentially.",
            "And then there's a kind of a simplification, but that that procedure of.",
            "Coding for finite block length and going to Infinity.",
            "Block length is exactly like passing to a thermodynamic limit and ruin other work.",
            "We make that precise.",
            "OK, so.",
            "That's a. I could stop at this moment and.",
            "Say OK, so the other thing is that this.",
            "We know that from the single mechanics, say on a finite.",
            "On a finite state space, the.",
            "Right, the description of a system in thermodynamic equilibrium, perhaps with the heat bot, is given by the minimization of free energy.",
            "And the free energy is the average energy minus the entropy, OK, suitably with appropriate signs.",
            "And the variational principle says that it gives measure is characterized by minimization of this free energy.",
            "Now passing to the limit is is there more and and what I'm suggesting is that the the idea of free energy minimization, I'll I'll argue that the Bayesian inference is corresponds to a free energy minimization, and the free energy minimization idea is somehow more general than the base formulas.",
            "That's the other kind of a message.",
            "OK. OK, so that's the first part is and the second part is the variational viewpoint information theoretic interpretation and then connections to stochastic control.",
            "What I'm going to show is that this free energy minimization problem, if it's related to estimation of a stage which is a Markov process, then this free energy minimization has an interpretation of the stochastic control.",
            "And.",
            "I believe this I mean in other work we've shown how?",
            "Sort of, the ideas of of.",
            "Leverage etc an galavotti etc.",
            "On equilibrium States and non equilibrium state has sort of counterparts in the filtering conflict, but I don't think I have time to talk about that.",
            "OK."
        ],
        [
            "Now here is a beautiful coat.",
            "So Shannon wrote this paper on an unregistered rate distortion theory.",
            "And he at the end of this paper he he's trying to say that the source coding and channel coding somehow are duals of each other.",
            "So there's a curious and provocative duality between the properties of a source with the distortion meter and those of a channel.",
            "This duality is enhanced if we consider channel, in which there is a cost associated with the different input letters, and it is desired to find the capacity subject to the constraint that expected costs not exceed a certain quantity.",
            "Thus input letter I might have cost AI and we wish to find the capacity with the side condition right PR is the probability.",
            "I've of the alphabet AI is less than or equal to, say some.",
            "May we appear there of using input letter right?",
            "This problem Mount mathematically to maximizing and mutual information and the variation of the PR with a linear inequality is constrained.",
            "The solution of this problem leads to a capacity cost function CA for the channel.",
            "It can be shown readily that this function is concave downward.",
            "Solving this problem corresponds in a sense to finding a source that is just right for the channel and the desired cost cost.",
            "In a somewhat Dewar, a valuating the Redis or."
        ],
        [
            "And function Rd for a source amounts mathematically to minimize the mutual information and the variation of the QIOD.",
            "So those are the the channel probabilities, again with a linear inequality is constraint.",
            "The solution leads to a function Rd which is convex.",
            "And solving this problem corresponds to finding a channel that is just right to the source and allowed distortion level.",
            "This duality can be pursued further and is related to a duality between past and future.",
            "And the notions of control and knowledge.",
            "Thus we may have knowledge of the past, but cannot control it.",
            "We may control the future, but I have no knowledge of it.",
            "As far as I know, Shannon does not elaborate any further on this duality.",
            "Between control and lighting estimation in some sense.",
            "But it's quite provocative, an perhaps one interpretation of this is this duality between estimation and stochastic control, so it's not quite the same, because in an information theory context you can do coding.",
            "So you have to look at the code or and decoders together in some sense.",
            "OK."
        ],
        [
            "So our basic model is the observation equation.",
            "So think of these is some signal and this is the observation YT and there's some noise WT.",
            "So there's some standard hypothesis.",
            "And.",
            "So what what I'm doing is very quickly developing how you get to these conditional probability equations.",
            "OK, an important role is paid by the so-called innovations process in a way."
        ],
        [
            "Missions process is the observation of time T minus all the information that's contained in the observation about the signal.",
            "So ZZS hat is the conditional expectation of ZS given the observations.",
            "In contained in in Y.",
            "And one can show that Newty is actually Brownian motion.",
            "So what is what it's saying is that the filter extracts all the information it can.",
            "About the signal from the observation, then leaves you sort of Brownian motion or it's derivative white noise which supposedly contains no information.",
            "This thing is a little bit subtle because one can show that the.",
            "That the new G in some sense contains the same old same information as Whitey in the sense that the Sigma field generated by new T&YT is the same.",
            "OK.",
            "So there are references throughout and I think the slides will be available, so you can look at look at this.",
            "OK."
        ],
        [
            "OK, so."
        ],
        [
            "Al especially."
        ],
        [
            "Eyes now.",
            "And the specialization is the signal is some nonlinear function of some Markov diffusion process given by a stochastic differential equation.",
            "OK. And say the filtering problem is to compute the conditional expectation of some function of XT.",
            "Right given observations up to time Y OK.",
            "So this is a filtering problem, so you're estimating X at time T given observations.",
            "And what we are interested in is to obtain a recursive equation for.",
            "This measure right by TFI.",
            "Parties are measure right?",
            "So this notation is is really the duality between measures and function.",
            "OK, so.",
            "Now right there."
        ],
        [
            "There are several descriptions of this Markov process.",
            "One is a stochastic differential equation, but we can also describe it in terms of.",
            "Right, so L is the generator, so early some differential operator which is sort of the generator of the Markov process and we can describe the the right.",
            "There are two equations, the backward equation which tells you the evolution of this measure and the forward equation right which is the evolution of the density, so yeah.",
            "And so I mean, that's all this stuff is."
        ],
        [
            "If so, I'll skip a lot of this.",
            "And so this is what is important that this is the evolution of this measure.",
            "The conditional remember what party is.",
            "Divide by tier fires the conditional expectation of fire XD given the observation and this and we can write a an evolution equation for that.",
            "In terms of the generator of the Markov process L. OK, so this is."
        ],
        [
            "So.",
            "And what you want to see is this equation now so.",
            "Yeah, which is the equations there.",
            "Here.",
            "Screen so we can tell which 1.3 which equation.",
            "This equation, but you were pointing at the screen, so if you could point in the number of the equation pointing at this, sorry."
        ],
        [
            "This equation 10.",
            ".10 is the answer 10.",
            "Yes, so this this describes the evolution of by T. There's this unknown thing.",
            "It S and we we identify what that alligator as is, and that will.",
            "That is what reduces."
        ],
        [
            "So."
        ],
        [
            "So that's now this is a full description of the evolution of the conditional distribution in terms of the generator of the Markov process, which is a hell.",
            "And this is the if you like the part, the information that comes from the observations.",
            "18 is up one HH is the observation map.",
            "Right, so why T is?",
            "Some signal which is H of XT plus some noise basically.",
            "Little problem following which equation you are pointing at because oh I see.",
            "Oh OK, yeah yeah there's a yeah.",
            "I should project project down here or something.",
            "OK, so that's what this equation is OK now in.",
            "So let me make another comment in another work that I've done with Nigel.",
            "We've shown.",
            "The following so if you like, so there's information containing the observation than the initial state.",
            "Right, which is getting accumulated that you get more and more observation now that information gets transferred, say to the filter.",
            "Right?",
            "Overtime.",
            "So you could ask the question whether this this transfer of information is is that dissipative or conservative.",
            "I you know.",
            "Does the filter.",
            "Right the filter.",
            "Namely, the estimate of the state at time T given the observation, does it require all the information that is being transferred, or does it require only that information that is needed to understand the state at time T and the?",
            "And his future, perhaps?",
            "OK, and one can show that.",
            "Indeed, this the filter stores that information that's needed to.",
            "If you like, understand the present and the future, and discards historical information at an optimal rate given by the Fisher information.",
            "So I mean those of you who are familiar with control of this, so you can write down a sort of a dissipation inequality in terms of information storage, information supply, and information dissipation.",
            "Anne.",
            "Yes.",
            "And this part of the equation.",
            "Remember new is the innovations process.",
            "That's the new information.",
            "And this is kind of a nonlinear part coming from the observation.",
            "It turns out that this has to do with the supply of information, things that from the outside.",
            "I mean what I'm getting at is not done completely.",
            "Know how to make this precise is we want to understand how the state evolution and the observations how they interact with each other.",
            "Right?",
            "Sensory or observed information.",
            "So H yeah, so YT is.",
            "Whitey is HXSDS plus noise.",
            "I think.",
            "I called it WT.",
            "And H is this nonlinear observation right?",
            "This term is H. So in.",
            "So so we can just as you define entropy production in in, I don't know.",
            "And honestly, mystical mechanics.",
            "You can define kind of interactive entropy production.",
            "Which I guess I won't.",
            "Talk about.",
            "But I can give you their references in the end to some of this work.",
            "OK, so let."
        ],
        [
            "Me."
        ],
        [
            "Yes, so there is another equation which is."
        ],
        [
            "So Phi is any function of the state, is that yeah, any functions as well.",
            "I mean square interval function.",
            "This equation it looks like low probability.",
            "But it looks more like some log of the conditional probability, not which from the form that you wrote this type of know this is conditional probability conditional probability.",
            "Well, I'm in conditional probability is a special case of conditional expectation so.",
            "Call Matt.",
            "Looks like you factorise.",
            "I mean there is a factorization involved.",
            "I mean are posteriori.",
            "But it's not allowed.",
            "At this point.",
            "There's not a lot of probability.",
            "This was just the identity function, that's the Kushner equation.",
            "Yes, this is the Kushner request.",
            "Sorry, I have.",
            "I've gone further.",
            "I so here is an example where you can.",
            "Everything is explicit for finite state Markov chains, so that's what.",
            "This part is.",
            "This question I think.",
            "He took the first turn.",
            "In what would have been the exponential approach, the zero degree term cancels out.",
            "The first term remains, and that's what shows up on the planet.",
            "OK so for finance.",
            "So finite state Markov chains everything can be."
        ],
        [
            "Worked out."
        ],
        [
            "Yes.",
            "So.",
            "There is another equation, the so called unnormalized conditional.",
            "OK the.",
            "Yeah.",
            "So the Kushner equation, the equation for the conditional distribution is a nonlinear stochastic partial differential equation.",
            "Turns out if you if you write the conditional distribution in terms of an unnormalized, supposing it has a density and you write it as an unnormalized conditional density.",
            "Then that satisfies a linear equation an one can write an explicit solution as a Feynman cuts formula.",
            "Most of this material is in this paper which I wrote."
        ],
        [
            "Long time ago."
        ],
        [
            "OK, so."
        ],
        [
            "Now I'm going to talk about this."
        ],
        [
            "Variational formulation of Bayesian interpretation.",
            "And So what I'm saying is that.",
            "Is there a duster here should use this.",
            "Sorry.",
            "Right, if you have some sort of here in a finite setting an, suppose you have some Gibbs measure in terms of given in terms of some energy function.",
            "Right normalized Z.",
            "So and you have some say probability, space, everything is finite.",
            "And the same you is some other measure on Omega.",
            "So you can define the average energy year meal that is.",
            "Write the sum of Omega.",
            "View of Omega.",
            "So H is the energy function.",
            "Age of Omega?",
            "And you define the entropy.",
            "The - yeah.",
            "Then we know that the new minimizes.",
            "The free energy.",
            "Namely F of meal, which is the average energy minus the entropy.",
            "And then the proof of that is actually very simple if you look at fmu plus the log of Z, the partition function Z.",
            "You can show that that's non negative and is 0 if and only if.",
            "Mu is equal to new and so revenue, the so called Helmholtz free energy is just minus the log of Z.",
            "So what I'm about to do is if you like have a variational principle of this kind for Bayesian inference.",
            "OK so I have to tell you what the energy function is.",
            "And I have to tell you what the analogue of the entropy is.",
            "Where you're trying to estimate sum X some random variable from Y?",
            "And we can do this in a very general setting, so X could be random variable taking values in some separable metric space or something like that.",
            "OK, So what I want to show is.",
            "That the conditional probability P of XY.",
            "Is obtained by minimizing a certain free energy.",
            "OK. And that hasn't.",
            "So this variational principle has an information theoretic interpretation.",
            "So that's what I'm doing now.",
            "Yeah.",
            "And.",
            "OK, so so there's a little bit of a technicality.",
            "But what you?",
            "OK, so you know."
        ],
        [
            "To do that, I have to tell you what the what.",
            "The analog of the Hamiltonian H is an H is just minus log of QXY will Q of XY is just the likelihood.",
            "Right field Y given X normalized.",
            "OK.",
            "So that's the Hamiltonian thing, and then you can write down the base formula in the form of a Gibbs distribution.",
            "OK, this is in some sense rewriting, but but interesting.",
            "OK. And so, technically, that's a regular conditional probability distribution for X given Y.",
            "And that's"
        ],
        [
            "Is the definition of a regular conditional probability distribution?"
        ],
        [
            "OK, so now I have to tell you what the analogue of the entropy is.",
            "So the analogue of an entropy is going to be a relative entropy.",
            "So if I do not buy P of extra set of probability measures on the estimand, right?",
            "This is the space of random variables which you're trying to estimate.",
            "NHX is sort of measurable functions on the same on this SpaceX.",
            "And so.",
            "Right, so we have.",
            "Given X&Y, we have, we are given the joint distribution P of XY&P of X. Anfield, why are the marginals?",
            "OK.",
            "So PX still there is some other measure on the space of probability distribution POX.",
            "An H so this is the.",
            "The If you like the relative entropy of PX still with respect to PX.",
            "So the interpretation of this.",
            "This is the information gain.",
            "Alfie of X~ if you believe.",
            "That the if the specified distribution was P of X but actually it's PX~ So you gain some information.",
            "That's what the relative entropy is, and that by definition is.",
            "Well.",
            "I mean, this is essentially the density right of one with the other?",
            "An higher H~ I mean this is like a logarithm of a moment generating function.",
            "OK. And this is just notation.",
            "So H~ is.",
            "Right, it's some function you know that's rectify that I talked about before and this is just integration with respect to TPX Builder."
        ],
        [
            "OK, so this is the interpretation.",
            "It is well known that the relative entropy can be interpreted.",
            "Information gain of the probability measure PX~ over.",
            "PXI guess this is still done.",
            "This is hat.",
            "OK, so we are no longer in a finite setting, so instead of entropy, a better object to work with his relative entropy.",
            "That's always well defined, as long as they're absolutely continuous.",
            "If you like the entropy is is the relative entropy respect to the counting measure.",
            "OK, so so this object minus log of this.",
            "Derivative is a generalization of the Shannon information for X. OK, it's a measure of the relative degree of surprising the outcome.",
            "X = X Four, 2 distribution P Excellent PX ilda.",
            "OK, so this is the average reduction in the degree of surprised in this outcome arising from the acceptance of VX.",
            "Still does the distribution for X rather than practical.",
            "This is interpretation."
        ],
        [
            "Anne.",
            "Exponential minus age still does the likelihood function for X right associated with some unspecified observation, so this is if you like.",
            "This is the residual degree of surprise contained in the observation given the prior distribution PX.",
            "OK."
        ],
        [
            "And the theorem says that.",
            "It's exactly like so, so this is the free energy now where the entropy is is replaced by the relative entropy, and this is the integration.",
            "Right age is the energy function, so this is like the average energy.",
            "Average with respect to PX~ and we minimize that with respect to PX~ and that this is exactly minus log of Z.",
            "Right logarithm of the moment generating function.",
            "Now this this is a convex programming problem on the space of measures.",
            "It is a legend ventral dual that characterizes the distance of the conditional distribution from PX.",
            "Right, this is the amount of if you like information gain in the conditional distribution relative to PX and that has a variation interpretation.",
            "So these are conjugate certain regards, Fenchel duality of conjugate convex functions.",
            "OK."
        ],
        [
            "So there's an interpretation of that which.",
            "Let me see."
        ],
        [
            "Skip that."
        ],
        [
            "I will let you look at that."
        ],
        [
            "By the way, this is this generalizes the idea of maximum entropy and so on, and it's a much more.",
            "Sort of robust.",
            "Version of those objects.",
            "OK, so just one comment is that."
        ],
        [
            "It turns out that this, for example, this dual problem can be used to study things like.",
            "Sort of.",
            "Robustness to perturbations.",
            "For example, if you have the wrong statistics in the observation, what is the effect of that and that could be studied by studying the dual problem OK?",
            "OK, so this is very general.",
            "X is some random variable taking values in some space, and while some related random variable taking values in some other space and we have a characterization of the conditional distribution of execution, right?",
            "So what we want to now specialize and we specialize that X is now a Markov diffusion process and we explicitly.",
            "Describe what the observations are.",
            "OK, so I mean the way I think think about it is that.",
            "If you like, I mean, a lot of parts of physics has to do with the Fokker Planck equation in some way, one form or another.",
            "But we're more interested in sort of understanding the interaction between observations and states.",
            "And in sort of traditional distribution about that has to tell us."
        ],
        [
            "OK."
        ],
        [
            "So now I'm specializing."
        ],
        [
            "So I have now a description of the state evolution of the state.",
            "Unfortunately this is changing rotation, but I mean the idea of fixing a notation.",
            "Is this so that you can change it?",
            "So I think I had G there before, but that doesn't matter.",
            "This is the the noise spot, and this is the dynamical part.",
            "If you like and I have an observation, I think I had H before, it's now G. OK. Now."
        ],
        [
            "What what do you expect is that?",
            "Because you have a Markov process now.",
            "Right you you would hope that this free energy minimization problem.",
            "Somehow should be amenable to dynamic programming, stochastic dynamic programming, OK, and that's what what is made precise."
        ],
        [
            "OK, so.",
            "So there is one idea that is used here, which comes from measuring tirion so castec control is that.",
            "So this is the observation now.",
            "So I think I had dinner.",
            "OK.",
            "So I look at the part space.",
            "Let's say Y0T.",
            "And there's a measure on that part space, let's call it PYK.",
            "Down from the Cameron Martin theorem and Gusano theorem, what we want to do is right and we are interested in computing, say the conditional expectation of P of XD given Y from zero to T. So the idea is, is and so these are sort of the trajectories of Y, right?",
            "And this trajectory described by this sort of stochastic differential equation.",
            "What we would like to do is we would like to do.",
            "We would like to look at these trajectory's under a different measure.",
            "And we would like to that measure to be such that YT looks like Brownian motion.",
            "Furthermore, the Whitey and so if I call it the OSYT&OS should be independent of each other and the distribution of the Earth shouldn't change.",
            "Ann from the Cameron Martin ideas and gson of ideas.",
            "Such a measure exists.",
            "Because YT now becomes independent of ZS.",
            "This condition.",
            "This conditional expectation becomes an expectation.",
            "So that's this change of measure argument that is used here, which.",
            "OK. And this change of measure is actually given in terms of.",
            "Function like this?",
            "This is the sort of the Cameron Martin.",
            "Function."
        ],
        [
            "OK, so.",
            "So the The upshot of using this technology is that the free energy minimization problem when we when we specify.",
            "The evolution when we specify the thing that I'm trying to estimate, namely the X's as a Markov diffusion process, turns out to become a stochastic control problem and.",
            "So, and the part estimation problem.",
            "Turns out that you can solve the pause estimation problem.",
            "And as a backward forward or forward backward filter.",
            "So you start at the end.",
            "And you run a likelihood filter.",
            "In order to estimate.",
            "The initial stage X0.",
            "Now in the process, you lose some information as I've said that the filter, so that's like a filtering problem, and I've argued that the filtering problem the filter stores that information that's needed in order to understand the present and the future.",
            "And this is historical information.",
            "Right?",
            "At some optimal rate turns out to be given by the Fisher information, so there's a filtering problem, so we've lost some information and we are interested in estimating the path.",
            "X03.",
            "And what we have lost is obtained by solving a stochastic control problem on the forward direction.",
            "Yeah, you might wanna emphasize Kozol.",
            "Feel free as opposed to yes, yes.",
            "What Roger is commenting is that the path estimation problem is in some sense non causal in some sense, right?",
            "And that's it's about estimation problem that is relevant to information theory because you're allowed to kind of go back and sort of.",
            "Right, you do it in blocks as it's batch processing if you like, Yep.",
            "And this stochastic control problem is I will write this down.",
            "So.",
            "And the same as saying that is this path.",
            "Estimate or this is not a marginal no.",
            "This is the path whole part, so I'm doing that because in this case you have a fixed thing.",
            "Otherwise I mean I have to do it the free energy at each time T and so yeah.",
            "The filtering problem is contained in this path estimation problem if you like.",
            "So this process timation is like smoothing is like having the common figure running back and forth right now, but I'm yeah, but but I mean the smoothing problem typically is.",
            "So here is 0 T here is.",
            "X.",
            "Right, the filtering problem you estimate at time S the smoothing problem.",
            "You would estimate at some time.",
            "Say tower less than us.",
            "Anne.",
            "You can run forward backward filters, but here I'm estimating the whole path so it's.",
            "It is smoothing if you wish.",
            "But this is not been given by Feynman cuts.",
            "The stochastic control part will be.",
            "The filter will be given by Feynman cuts formula.",
            "Causal filtering problem.",
            "What the causal filtering problem will be given by account.",
            "So let's let's think of everything in terms of systems.",
            "Let's say that we do smoothing.",
            "Now the equivalent.",
            "Tell me the rotation for a.",
            "For the dual control.",
            "Of doing smooth so there, I mean that's what I'm giving you, one that when you run the backward filter you lose some information right?",
            "Because the filter stores only that information that's needed to understand the present and the future.",
            "Which if you want to estimate the part you have to recover.",
            "The information that you have lost and that is obtained by an appropriate stochastic control problem.",
            "And the forward direction.",
            "OK.",
            "But try this on precise.",
            "The duality between a.",
            "Free endpoint problem.",
            "Then what you have to add by way of cost to get them fixed.",
            "Endpoint version is a kind of duality between filtering and smoothing, yeah?",
            "So let me write down the stochastic the stochastic control problem.",
            "I was just asking basically the controls we get about.",
            "We have unstable systems right?",
            "And we want to control an unstable system.",
            "So what is really the equivalent of smoking in them?",
            "So do I start in in controls from stable solution and I go to unstable solution about what does it really mean substantial?",
            "Controlling the system.",
            "What I mean?",
            "Maybe you could, maybe we could have a discussion at the end of the talk, yeah?",
            "OK, at the moment what I want to say is that this backward filter, right which estimates the initial state?",
            "By the way, if the initial state is fixed, then you don't need the backward filter, who just needs a stochastic control problem in the forward direction.",
            "So when you solve the optimal stochastic control problem right, you will get a measure on the space of parts.",
            "And that measure is the condition measure.",
            "OK.",
            "I mean, there's a broad conceptual suggestion that what matters is the measure on the part space.",
            "In all of this.",
            "And that's what I'm emphasizing here.",
            "Sort of the past face build on the Martin Gale view and so.",
            "Will also recognizing the estimation problem rather than control problem.",
            "Yeah so.",
            "OK, another way to say that there are certainly certain stochastic you could apply the duality in the other direction.",
            "There are certain stochastic control problems, namely, which has a minimum energy term and some nonlinear function of the state that has a dual problem, which is like a filtering problem which can then be solved using particle filtering and so on.",
            "So it goes both ways, so the filtering problem corresponds to a certain stochastic control problem and certain classes of stochastic control problems.",
            "Is related to filtering problems.",
            "OK, so this is the stochastic."
        ],
        [
            "Troll problem.",
            "So notice that the there is a control term now.",
            "So with this, with a suitable scaling A. OK. And the."
        ],
        [
            "Alright, the."
        ],
        [
            "The stochastic control problem, if you assume that the.",
            "Yeah, look at equation 41 if you assume for a moment that.",
            "The observations are differentiable.",
            "Bye.",
            "Then the stochastic control problem has two paths.",
            "One is a minimum energy term, if you like.",
            "And this is like a tracking term.",
            "And those of you are familiar with things like, I think Bryson, Frazer, and.",
            "So there's this variational interpretation of the Kalman filter.",
            "So if you like, this is trying to.",
            "Track the observation as best as it can with a constraint on the energy.",
            "OK. And that's but this is what I'm showing you now is a quite a big generalization about is known in the linear in the linear case.",
            "So there's some technicalities here, and the technicalities lead that the essentially the filtering problem has a corresponding stochastic control problem, which is like this kind of an optimal tracking problem.",
            "With with a stochastic differential equation, which is sort of modulated by this control term.",
            "And the, by the way, there's the dual problem.",
            "So this is what I."
        ],
        [
            "I've written here.",
            "This involves an energy term for the control and at least squares term for the observation part fit.",
            "These corresponded to two terms in base formula, representing the degrees of match with the prior distribution and the observation part.",
            "If you like this minimum energy term really is the relative entropy term with respect to the prior looked at in an appropriate measure.",
            "Using this change of measure argument.",
            "Weather in the UK."
        ],
        [
            "Solve this stochastic control problem, which is turned out to be the gradient of the value function."
        ],
        [
            "There is a dual problem which also has right.",
            "Remember I showed this Fenchel duality, that isn't."
        ],
        [
            "Optimal control interpretation.",
            "This is your log row appearing here.",
            "This is related to in some ways to work that I did with Wendell Fleming of using the Hough transformation, which relates Hamilton Jacobi equations and parabolic equations."
        ],
        [
            "OK, so I think."
        ],
        [
            "My time is up."
        ],
        [
            "With me I think I."
        ],
        [
            "Conclusion sex."
        ],
        [
            "Yeah."
        ],
        [
            "OK, so there's this two part paper which I can make available which shows how you can prove the noisy channel coding theorem using these ideas of free energy minimization.",
            "I think the infinite time behavior of the filter is is I would say, still open, maybe Ramone unhandled would have something to say about that.",
            "All business of control filtering which I haven't talked talked about.",
            "And there's lots of numerical work going on, in particular of the work of shoring.",
            "On implicit sampling for particle filtering.",
            "Anne.",
            "That's it.",
            "I think as I remember in the abstract you mentioned that is offensive transform as well, yeah?",
            "Yeah, so these are.",
            "So these two problems."
        ],
        [
            "Namely, so this is minimization of the relative entropy.",
            "So this is the of entropy plus average energy, and this has a dual problem.",
            "Remember what I is I is this minus log of this moment generating function.",
            "Minus.",
            "Right the edge till there is some other likelihood function.",
            "OK and this is integrated against the conditional distribution.",
            "These two are Fenchel, dual for each other.",
            "Jean central I mean.",
            "If you like, this is a special case of the sort of the at least one side of it of the dance cover and variational principle that appears in large deviations, which is a much more general thing.",
            "We don't need that.",
            "Doing I age like where does it come from machine learning.",
            "People use it all over the place and.",
            "Is this the first time house it will show or?",
            "So what's this?",
            "Yeah, yeah.",
            "Can I put it or or doesn't have a look somewhere else?",
            "Question about history of the No.",
            "I mean I think.",
            "If a if you wish, I mean the proof of this theorem is very easy, and to me the proof of the theorem is not the important part.",
            "Is this is the theorem itself.",
            "And the point is that if you're going, if you think of the noisy channel coding theorem.",
            "Can be posed as a problem estimation problem, but over an infinite time and you cannot write a base formula for that.",
            "So you have to proceed via this free energy minimization take limits, just has to prove the Gibbs variational principle.",
            "You have to.",
            "You know you you have to look at.",
            "Super specific, free energies, etc.",
            "And then take an appropriate limit and the limiting argument is.",
            "So what I'm suggesting is the idea of.",
            "I think the idea of free energy minimization is in some sense more general than the base formula.",
            "Claiming that this goes back to gives us.",
            "Very original stuff gives this is built on this.",
            "Sorry I missed that.",
            "Prove this this duality.",
            "We offer filtering, but yeah well I mean what I just described is of course well known that the finite case.",
            "The yeah.",
            "Yup, I think was in gives book from published in 1901.",
            "Basically before that, maybe.",
            "But you know these, this talk of relating information theory to statistical mechanics has been talked about for long times, right?",
            "Shannon himself said that once we understand information theory, you know deeply.",
            "It'll be like 10 million amix or something.",
            "But as far as I know, nobody has shown that the noisy channel coding theorem is like.",
            "It's like Bayesian estimation on an infinite horizon.",
            "And that is in a very precise sense, like the Gibbs variational principle.",
            "And we we use the full Arsenal of, you know the Russian ideas.",
            "But on one of your latest slides, you had where you linked it to to the final cuts formula you have this control formulation.",
            "We had a function a multiplying the control yeah, and then you had a noise term.",
            "Yeah, in the formulation is not familiar with this.",
            "There has to be a relation between these two terms.",
            "I didn't see that.",
            "Funny.",
            "Channel some of the last yeah."
        ],
        [
            "So the other way."
        ],
        [
            "Make this remark here.",
            "Sorry I lost.",
            "Right?",
            "So I had the equation for.",
            "Buy GFI.",
            "Right, that's just second right, which is the Kushner equation.",
            "Now if you write by Tier 5 as he wrote YIFY.",
            "Integral roti then roti satisfies a linear stochastic differential equation, so it is.",
            "So I should foster the density and then write it in a normal that satisfies this equation.",
            "Age is the observation.",
            "Roti gyt right, and this equation can you can solve using Panthers formula which is expectation over the measure.",
            "UX measure.",
            "Of.",
            "Say no zero XD.",
            "Exponential internal 0TH D YS minus 1/2.",
            "Explain this.",
            "And that's the final customer.",
            "Well, so if you start with respect to.",
            "Major fix times will be why this is well.",
            "No, I meant.",
            "In the formulation that I wrote down, there is a.",
            "There is the better control and the noise acting the same subspace.",
            "And there is there multiply both business by AFP.",
            "Yeah yeah.",
            "Which is Sigma Sigma transpose?",
            "I was just asking whether this was more general, but it was basically the same.",
            "Well, I don't think you can write down a formal funding cost formula for the control problem.",
            "I mean, you can write down for the stochastic differential equation, you can write down a final cuts formula."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for inviting me to speak here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "So I've been interested in the last few years on on.",
                    "label": 0
                },
                {
                    "sent": "So they're investigating.",
                    "label": 0
                },
                {
                    "sent": "The relationships between filtering theory or estimation problems.",
                    "label": 0
                },
                {
                    "sent": "In more general terms, information theory answer typical mechanics and.",
                    "label": 0
                },
                {
                    "sent": "So if so this, so the broad idea was this interesting book called.",
                    "label": 0
                },
                {
                    "sent": "Gives measures.",
                    "label": 0
                },
                {
                    "sent": "And phase transitions.",
                    "label": 0
                },
                {
                    "sent": "By georgy.",
                    "label": 0
                },
                {
                    "sent": "It works better.",
                    "label": 0
                },
                {
                    "sent": "Thanks anyway yeah.",
                    "label": 0
                },
                {
                    "sent": "And I believe it's Chapter 16.",
                    "label": 0
                },
                {
                    "sent": "Which is devoted to the Gibbs variational principle.",
                    "label": 0
                },
                {
                    "sent": "And so this chapter essentially gives you a characterization of translation invariant Gibbs measures in terms of minimizing a certain free energy.",
                    "label": 0
                },
                {
                    "sent": "And behind it is this this.",
                    "label": 0
                },
                {
                    "sent": "I would say the devolution view where instead of if you think of the color of extension theorem where you look at marginal densities, Ann and consistency as you look at the measures over a larger time interval and projected down to a lower smaller time interval.",
                    "label": 0
                },
                {
                    "sent": "They should coincide.",
                    "label": 0
                },
                {
                    "sent": "The same argument is carried over for conditional distributions rather than marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "And in this case there there maybe there isn't a single extension.",
                    "label": 0
                },
                {
                    "sent": "If you like their whole convex compact set of Gibbs measures OK, and what I'm going to talk about is, is what I will not talk about is the recent work that I've done jointly with Nigel Newton on on giving a proof of the noisy channel coding theorem as a problem in statistical mechanics as a problem?",
                    "label": 0
                },
                {
                    "sent": "Gives variational principle.",
                    "label": 0
                },
                {
                    "sent": "I could talk about it informally later on what I will talk about is to see Bayesian inference as as a problem of free energy minimization, and that also is related to this duality between estimation and control.",
                    "label": 0
                },
                {
                    "sent": "Password.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first part is now standard material in nonlinear filtering.",
                    "label": 0
                },
                {
                    "sent": "And so the I'll quickly go over that.",
                    "label": 0
                },
                {
                    "sent": "So the idea of a filter if you like.",
                    "label": 0
                },
                {
                    "sent": "Or an estimator?",
                    "label": 0
                },
                {
                    "sent": "That you have some observations, say Whitey.",
                    "label": 0
                },
                {
                    "sent": "The grades that are equal to 0 and perhaps some initial state X0, which is random an.",
                    "label": 0
                },
                {
                    "sent": "So the idea of a filter is so observations are coming in to the filter an.",
                    "label": 0
                },
                {
                    "sent": "And what the filter produces is.",
                    "label": 0
                },
                {
                    "sent": "So there's a notion of a state XD.",
                    "label": 0
                },
                {
                    "sent": "And what the filter produces is the conditional distribution of XT given.",
                    "label": 0
                },
                {
                    "sent": "Observation say from Delta T. So the filter is really a map from the space of observations and perhaps initial state into the space of conditional distribution, so those are measured on XT an.",
                    "label": 0
                },
                {
                    "sent": "If XD were say a Markov diffusion process, then the several descriptions of that markup diffusion process as a stochastic differential equation perhaps, but also in terms of if there's a density than the focal Planck equation which describes the evolution of XD.",
                    "label": 0
                },
                {
                    "sent": "But when we have.",
                    "label": 0
                },
                {
                    "sent": "Observations we're interested in conditional distribution, so we have to describe how the conditional distribution evolves in time.",
                    "label": 0
                },
                {
                    "sent": "So if you wish in statistical mechanics sort of states are probability measures and observables are functions from some configuration space into, say, the real line.",
                    "label": 0
                },
                {
                    "sent": "But what we're interested in instead of probability measure we're interested in conditional measures.",
                    "label": 0
                },
                {
                    "sent": "And if you like conditional statistics.",
                    "label": 0
                },
                {
                    "sent": "Right, so integrating that against functions.",
                    "label": 0
                },
                {
                    "sent": "And the first part of my of my talk we just standard.",
                    "label": 0
                },
                {
                    "sent": "I'll go quickly is to describe how this conditional distribution evolves in time OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "So there are several problems, so this is the so called filtering problem.",
                    "label": 0
                },
                {
                    "sent": "So you want to estimate the state at time T given observations.",
                    "label": 0
                },
                {
                    "sent": "There are other problems there, for example the prediction problem where would be interested in estimating XA at time T + S. Given observations after time T, for example, or the path estimation problem where you're interested in estimating the whole path.",
                    "label": 0
                },
                {
                    "sent": "Boss estimation.",
                    "label": 0
                },
                {
                    "sent": "And we would be interested in characterizing this this conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So, by the way, the pause estimation.",
                    "label": 0
                },
                {
                    "sent": "I mean that's.",
                    "label": 0
                },
                {
                    "sent": "So if this were, say, a finite state Markov process, and why zero or some functions of X and then the part estimation problem is there is the decoding problem OK?",
                    "label": 0
                },
                {
                    "sent": "The distinction being though in the decoding in information theory context you can influence what you want to estimate.",
                    "label": 0
                },
                {
                    "sent": "Right, the source if you like, by access by doing coding.",
                    "label": 0
                },
                {
                    "sent": "So I'm control tourist in some sense.",
                    "label": 0
                },
                {
                    "sent": "So you could think of the code or somehow as a controller and the decoder is an estimator in some sense, OK, but they're on a finite time.",
                    "label": 0
                },
                {
                    "sent": "They interact in complicated ways.",
                    "label": 0
                },
                {
                    "sent": "And so this I made this comment about gives a variational principle.",
                    "label": 0
                },
                {
                    "sent": "So in order to simplify matters, you have to look at the the problem of decoding over sale.",
                    "label": 0
                },
                {
                    "sent": "You would have to allow infinite delays essentially.",
                    "label": 0
                },
                {
                    "sent": "And then there's a kind of a simplification, but that that procedure of.",
                    "label": 0
                },
                {
                    "sent": "Coding for finite block length and going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Block length is exactly like passing to a thermodynamic limit and ruin other work.",
                    "label": 0
                },
                {
                    "sent": "We make that precise.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That's a. I could stop at this moment and.",
                    "label": 0
                },
                {
                    "sent": "Say OK, so the other thing is that this.",
                    "label": 0
                },
                {
                    "sent": "We know that from the single mechanics, say on a finite.",
                    "label": 0
                },
                {
                    "sent": "On a finite state space, the.",
                    "label": 0
                },
                {
                    "sent": "Right, the description of a system in thermodynamic equilibrium, perhaps with the heat bot, is given by the minimization of free energy.",
                    "label": 0
                },
                {
                    "sent": "And the free energy is the average energy minus the entropy, OK, suitably with appropriate signs.",
                    "label": 0
                },
                {
                    "sent": "And the variational principle says that it gives measure is characterized by minimization of this free energy.",
                    "label": 0
                },
                {
                    "sent": "Now passing to the limit is is there more and and what I'm suggesting is that the the idea of free energy minimization, I'll I'll argue that the Bayesian inference is corresponds to a free energy minimization, and the free energy minimization idea is somehow more general than the base formulas.",
                    "label": 0
                },
                {
                    "sent": "That's the other kind of a message.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so that's the first part is and the second part is the variational viewpoint information theoretic interpretation and then connections to stochastic control.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to show is that this free energy minimization problem, if it's related to estimation of a stage which is a Markov process, then this free energy minimization has an interpretation of the stochastic control.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I believe this I mean in other work we've shown how?",
                    "label": 1
                },
                {
                    "sent": "Sort of, the ideas of of.",
                    "label": 0
                },
                {
                    "sent": "Leverage etc an galavotti etc.",
                    "label": 0
                },
                {
                    "sent": "On equilibrium States and non equilibrium state has sort of counterparts in the filtering conflict, but I don't think I have time to talk about that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here is a beautiful coat.",
                    "label": 0
                },
                {
                    "sent": "So Shannon wrote this paper on an unregistered rate distortion theory.",
                    "label": 0
                },
                {
                    "sent": "And he at the end of this paper he he's trying to say that the source coding and channel coding somehow are duals of each other.",
                    "label": 0
                },
                {
                    "sent": "So there's a curious and provocative duality between the properties of a source with the distortion meter and those of a channel.",
                    "label": 1
                },
                {
                    "sent": "This duality is enhanced if we consider channel, in which there is a cost associated with the different input letters, and it is desired to find the capacity subject to the constraint that expected costs not exceed a certain quantity.",
                    "label": 1
                },
                {
                    "sent": "Thus input letter I might have cost AI and we wish to find the capacity with the side condition right PR is the probability.",
                    "label": 0
                },
                {
                    "sent": "I've of the alphabet AI is less than or equal to, say some.",
                    "label": 1
                },
                {
                    "sent": "May we appear there of using input letter right?",
                    "label": 1
                },
                {
                    "sent": "This problem Mount mathematically to maximizing and mutual information and the variation of the PR with a linear inequality is constrained.",
                    "label": 0
                },
                {
                    "sent": "The solution of this problem leads to a capacity cost function CA for the channel.",
                    "label": 0
                },
                {
                    "sent": "It can be shown readily that this function is concave downward.",
                    "label": 1
                },
                {
                    "sent": "Solving this problem corresponds in a sense to finding a source that is just right for the channel and the desired cost cost.",
                    "label": 0
                },
                {
                    "sent": "In a somewhat Dewar, a valuating the Redis or.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And function Rd for a source amounts mathematically to minimize the mutual information and the variation of the QIOD.",
                    "label": 1
                },
                {
                    "sent": "So those are the the channel probabilities, again with a linear inequality is constraint.",
                    "label": 1
                },
                {
                    "sent": "The solution leads to a function Rd which is convex.",
                    "label": 1
                },
                {
                    "sent": "And solving this problem corresponds to finding a channel that is just right to the source and allowed distortion level.",
                    "label": 1
                },
                {
                    "sent": "This duality can be pursued further and is related to a duality between past and future.",
                    "label": 1
                },
                {
                    "sent": "And the notions of control and knowledge.",
                    "label": 0
                },
                {
                    "sent": "Thus we may have knowledge of the past, but cannot control it.",
                    "label": 0
                },
                {
                    "sent": "We may control the future, but I have no knowledge of it.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, Shannon does not elaborate any further on this duality.",
                    "label": 0
                },
                {
                    "sent": "Between control and lighting estimation in some sense.",
                    "label": 0
                },
                {
                    "sent": "But it's quite provocative, an perhaps one interpretation of this is this duality between estimation and stochastic control, so it's not quite the same, because in an information theory context you can do coding.",
                    "label": 0
                },
                {
                    "sent": "So you have to look at the code or and decoders together in some sense.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our basic model is the observation equation.",
                    "label": 1
                },
                {
                    "sent": "So think of these is some signal and this is the observation YT and there's some noise WT.",
                    "label": 0
                },
                {
                    "sent": "So there's some standard hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So what what I'm doing is very quickly developing how you get to these conditional probability equations.",
                    "label": 0
                },
                {
                    "sent": "OK, an important role is paid by the so-called innovations process in a way.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Missions process is the observation of time T minus all the information that's contained in the observation about the signal.",
                    "label": 0
                },
                {
                    "sent": "So ZZS hat is the conditional expectation of ZS given the observations.",
                    "label": 0
                },
                {
                    "sent": "In contained in in Y.",
                    "label": 0
                },
                {
                    "sent": "And one can show that Newty is actually Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So what is what it's saying is that the filter extracts all the information it can.",
                    "label": 0
                },
                {
                    "sent": "About the signal from the observation, then leaves you sort of Brownian motion or it's derivative white noise which supposedly contains no information.",
                    "label": 1
                },
                {
                    "sent": "This thing is a little bit subtle because one can show that the.",
                    "label": 0
                },
                {
                    "sent": "That the new G in some sense contains the same old same information as Whitey in the sense that the Sigma field generated by new T&YT is the same.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there are references throughout and I think the slides will be available, so you can look at look at this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Al especially.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eyes now.",
                    "label": 0
                },
                {
                    "sent": "And the specialization is the signal is some nonlinear function of some Markov diffusion process given by a stochastic differential equation.",
                    "label": 1
                },
                {
                    "sent": "OK. And say the filtering problem is to compute the conditional expectation of some function of XT.",
                    "label": 0
                },
                {
                    "sent": "Right given observations up to time Y OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a filtering problem, so you're estimating X at time T given observations.",
                    "label": 0
                },
                {
                    "sent": "And what we are interested in is to obtain a recursive equation for.",
                    "label": 1
                },
                {
                    "sent": "This measure right by TFI.",
                    "label": 0
                },
                {
                    "sent": "Parties are measure right?",
                    "label": 0
                },
                {
                    "sent": "So this notation is is really the duality between measures and function.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now right there.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are several descriptions of this Markov process.",
                    "label": 0
                },
                {
                    "sent": "One is a stochastic differential equation, but we can also describe it in terms of.",
                    "label": 1
                },
                {
                    "sent": "Right, so L is the generator, so early some differential operator which is sort of the generator of the Markov process and we can describe the the right.",
                    "label": 1
                },
                {
                    "sent": "There are two equations, the backward equation which tells you the evolution of this measure and the forward equation right which is the evolution of the density, so yeah.",
                    "label": 0
                },
                {
                    "sent": "And so I mean, that's all this stuff is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If so, I'll skip a lot of this.",
                    "label": 0
                },
                {
                    "sent": "And so this is what is important that this is the evolution of this measure.",
                    "label": 0
                },
                {
                    "sent": "The conditional remember what party is.",
                    "label": 0
                },
                {
                    "sent": "Divide by tier fires the conditional expectation of fire XD given the observation and this and we can write a an evolution equation for that.",
                    "label": 0
                },
                {
                    "sent": "In terms of the generator of the Markov process L. OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And what you want to see is this equation now so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is the equations there.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Screen so we can tell which 1.3 which equation.",
                    "label": 0
                },
                {
                    "sent": "This equation, but you were pointing at the screen, so if you could point in the number of the equation pointing at this, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This equation 10.",
                    "label": 0
                },
                {
                    "sent": ".10 is the answer 10.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this this describes the evolution of by T. There's this unknown thing.",
                    "label": 0
                },
                {
                    "sent": "It S and we we identify what that alligator as is, and that will.",
                    "label": 0
                },
                {
                    "sent": "That is what reduces.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's now this is a full description of the evolution of the conditional distribution in terms of the generator of the Markov process, which is a hell.",
                    "label": 1
                },
                {
                    "sent": "And this is the if you like the part, the information that comes from the observations.",
                    "label": 0
                },
                {
                    "sent": "18 is up one HH is the observation map.",
                    "label": 0
                },
                {
                    "sent": "Right, so why T is?",
                    "label": 0
                },
                {
                    "sent": "Some signal which is H of XT plus some noise basically.",
                    "label": 0
                },
                {
                    "sent": "Little problem following which equation you are pointing at because oh I see.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, yeah yeah there's a yeah.",
                    "label": 0
                },
                {
                    "sent": "I should project project down here or something.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what this equation is OK now in.",
                    "label": 0
                },
                {
                    "sent": "So let me make another comment in another work that I've done with Nigel.",
                    "label": 0
                },
                {
                    "sent": "We've shown.",
                    "label": 0
                },
                {
                    "sent": "The following so if you like, so there's information containing the observation than the initial state.",
                    "label": 0
                },
                {
                    "sent": "Right, which is getting accumulated that you get more and more observation now that information gets transferred, say to the filter.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 1
                },
                {
                    "sent": "So you could ask the question whether this this transfer of information is is that dissipative or conservative.",
                    "label": 0
                },
                {
                    "sent": "I you know.",
                    "label": 0
                },
                {
                    "sent": "Does the filter.",
                    "label": 0
                },
                {
                    "sent": "Right the filter.",
                    "label": 0
                },
                {
                    "sent": "Namely, the estimate of the state at time T given the observation, does it require all the information that is being transferred, or does it require only that information that is needed to understand the state at time T and the?",
                    "label": 0
                },
                {
                    "sent": "And his future, perhaps?",
                    "label": 0
                },
                {
                    "sent": "OK, and one can show that.",
                    "label": 1
                },
                {
                    "sent": "Indeed, this the filter stores that information that's needed to.",
                    "label": 0
                },
                {
                    "sent": "If you like, understand the present and the future, and discards historical information at an optimal rate given by the Fisher information.",
                    "label": 0
                },
                {
                    "sent": "So I mean those of you who are familiar with control of this, so you can write down a sort of a dissipation inequality in terms of information storage, information supply, and information dissipation.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And this part of the equation.",
                    "label": 0
                },
                {
                    "sent": "Remember new is the innovations process.",
                    "label": 0
                },
                {
                    "sent": "That's the new information.",
                    "label": 1
                },
                {
                    "sent": "And this is kind of a nonlinear part coming from the observation.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this has to do with the supply of information, things that from the outside.",
                    "label": 0
                },
                {
                    "sent": "I mean what I'm getting at is not done completely.",
                    "label": 0
                },
                {
                    "sent": "Know how to make this precise is we want to understand how the state evolution and the observations how they interact with each other.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Sensory or observed information.",
                    "label": 0
                },
                {
                    "sent": "So H yeah, so YT is.",
                    "label": 0
                },
                {
                    "sent": "Whitey is HXSDS plus noise.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I called it WT.",
                    "label": 0
                },
                {
                    "sent": "And H is this nonlinear observation right?",
                    "label": 0
                },
                {
                    "sent": "This term is H. So in.",
                    "label": 0
                },
                {
                    "sent": "So so we can just as you define entropy production in in, I don't know.",
                    "label": 0
                },
                {
                    "sent": "And honestly, mystical mechanics.",
                    "label": 0
                },
                {
                    "sent": "You can define kind of interactive entropy production.",
                    "label": 0
                },
                {
                    "sent": "Which I guess I won't.",
                    "label": 0
                },
                {
                    "sent": "Talk about.",
                    "label": 0
                },
                {
                    "sent": "But I can give you their references in the end to some of this work.",
                    "label": 0
                },
                {
                    "sent": "OK, so let.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Me.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, so there is another equation which is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Phi is any function of the state, is that yeah, any functions as well.",
                    "label": 0
                },
                {
                    "sent": "I mean square interval function.",
                    "label": 0
                },
                {
                    "sent": "This equation it looks like low probability.",
                    "label": 0
                },
                {
                    "sent": "But it looks more like some log of the conditional probability, not which from the form that you wrote this type of know this is conditional probability conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm in conditional probability is a special case of conditional expectation so.",
                    "label": 0
                },
                {
                    "sent": "Call Matt.",
                    "label": 0
                },
                {
                    "sent": "Looks like you factorise.",
                    "label": 0
                },
                {
                    "sent": "I mean there is a factorization involved.",
                    "label": 0
                },
                {
                    "sent": "I mean are posteriori.",
                    "label": 0
                },
                {
                    "sent": "But it's not allowed.",
                    "label": 0
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "There's not a lot of probability.",
                    "label": 0
                },
                {
                    "sent": "This was just the identity function, that's the Kushner equation.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is the Kushner request.",
                    "label": 1
                },
                {
                    "sent": "Sorry, I have.",
                    "label": 0
                },
                {
                    "sent": "I've gone further.",
                    "label": 0
                },
                {
                    "sent": "I so here is an example where you can.",
                    "label": 0
                },
                {
                    "sent": "Everything is explicit for finite state Markov chains, so that's what.",
                    "label": 0
                },
                {
                    "sent": "This part is.",
                    "label": 0
                },
                {
                    "sent": "This question I think.",
                    "label": 0
                },
                {
                    "sent": "He took the first turn.",
                    "label": 1
                },
                {
                    "sent": "In what would have been the exponential approach, the zero degree term cancels out.",
                    "label": 0
                },
                {
                    "sent": "The first term remains, and that's what shows up on the planet.",
                    "label": 0
                },
                {
                    "sent": "OK so for finance.",
                    "label": 0
                },
                {
                    "sent": "So finite state Markov chains everything can be.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worked out.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There is another equation, the so called unnormalized conditional.",
                    "label": 0
                },
                {
                    "sent": "OK the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the Kushner equation, the equation for the conditional distribution is a nonlinear stochastic partial differential equation.",
                    "label": 1
                },
                {
                    "sent": "Turns out if you if you write the conditional distribution in terms of an unnormalized, supposing it has a density and you write it as an unnormalized conditional density.",
                    "label": 1
                },
                {
                    "sent": "Then that satisfies a linear equation an one can write an explicit solution as a Feynman cuts formula.",
                    "label": 0
                },
                {
                    "sent": "Most of this material is in this paper which I wrote.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Long time ago.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to talk about this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variational formulation of Bayesian interpretation.",
                    "label": 0
                },
                {
                    "sent": "And So what I'm saying is that.",
                    "label": 0
                },
                {
                    "sent": "Is there a duster here should use this.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Right, if you have some sort of here in a finite setting an, suppose you have some Gibbs measure in terms of given in terms of some energy function.",
                    "label": 0
                },
                {
                    "sent": "Right normalized Z.",
                    "label": 0
                },
                {
                    "sent": "So and you have some say probability, space, everything is finite.",
                    "label": 0
                },
                {
                    "sent": "And the same you is some other measure on Omega.",
                    "label": 0
                },
                {
                    "sent": "So you can define the average energy year meal that is.",
                    "label": 0
                },
                {
                    "sent": "Write the sum of Omega.",
                    "label": 0
                },
                {
                    "sent": "View of Omega.",
                    "label": 0
                },
                {
                    "sent": "So H is the energy function.",
                    "label": 0
                },
                {
                    "sent": "Age of Omega?",
                    "label": 0
                },
                {
                    "sent": "And you define the entropy.",
                    "label": 0
                },
                {
                    "sent": "The - yeah.",
                    "label": 0
                },
                {
                    "sent": "Then we know that the new minimizes.",
                    "label": 0
                },
                {
                    "sent": "The free energy.",
                    "label": 0
                },
                {
                    "sent": "Namely F of meal, which is the average energy minus the entropy.",
                    "label": 0
                },
                {
                    "sent": "And then the proof of that is actually very simple if you look at fmu plus the log of Z, the partition function Z.",
                    "label": 0
                },
                {
                    "sent": "You can show that that's non negative and is 0 if and only if.",
                    "label": 0
                },
                {
                    "sent": "Mu is equal to new and so revenue, the so called Helmholtz free energy is just minus the log of Z.",
                    "label": 0
                },
                {
                    "sent": "So what I'm about to do is if you like have a variational principle of this kind for Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "OK so I have to tell you what the energy function is.",
                    "label": 0
                },
                {
                    "sent": "And I have to tell you what the analogue of the entropy is.",
                    "label": 0
                },
                {
                    "sent": "Where you're trying to estimate sum X some random variable from Y?",
                    "label": 0
                },
                {
                    "sent": "And we can do this in a very general setting, so X could be random variable taking values in some separable metric space or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want to show is.",
                    "label": 0
                },
                {
                    "sent": "That the conditional probability P of XY.",
                    "label": 0
                },
                {
                    "sent": "Is obtained by minimizing a certain free energy.",
                    "label": 0
                },
                {
                    "sent": "OK. And that hasn't.",
                    "label": 0
                },
                {
                    "sent": "So this variational principle has an information theoretic interpretation.",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm doing now.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so so there's a little bit of a technicality.",
                    "label": 0
                },
                {
                    "sent": "But what you?",
                    "label": 0
                },
                {
                    "sent": "OK, so you know.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do that, I have to tell you what the what.",
                    "label": 0
                },
                {
                    "sent": "The analog of the Hamiltonian H is an H is just minus log of QXY will Q of XY is just the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Right field Y given X normalized.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the Hamiltonian thing, and then you can write down the base formula in the form of a Gibbs distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, this is in some sense rewriting, but but interesting.",
                    "label": 0
                },
                {
                    "sent": "OK. And so, technically, that's a regular conditional probability distribution for X given Y.",
                    "label": 0
                },
                {
                    "sent": "And that's",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the definition of a regular conditional probability distribution?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I have to tell you what the analogue of the entropy is.",
                    "label": 0
                },
                {
                    "sent": "So the analogue of an entropy is going to be a relative entropy.",
                    "label": 0
                },
                {
                    "sent": "So if I do not buy P of extra set of probability measures on the estimand, right?",
                    "label": 0
                },
                {
                    "sent": "This is the space of random variables which you're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "NHX is sort of measurable functions on the same on this SpaceX.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Right, so we have.",
                    "label": 0
                },
                {
                    "sent": "Given X&Y, we have, we are given the joint distribution P of XY&P of X. Anfield, why are the marginals?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So PX still there is some other measure on the space of probability distribution POX.",
                    "label": 1
                },
                {
                    "sent": "An H so this is the.",
                    "label": 0
                },
                {
                    "sent": "The If you like the relative entropy of PX still with respect to PX.",
                    "label": 0
                },
                {
                    "sent": "So the interpretation of this.",
                    "label": 0
                },
                {
                    "sent": "This is the information gain.",
                    "label": 0
                },
                {
                    "sent": "Alfie of X~ if you believe.",
                    "label": 0
                },
                {
                    "sent": "That the if the specified distribution was P of X but actually it's PX~ So you gain some information.",
                    "label": 0
                },
                {
                    "sent": "That's what the relative entropy is, and that by definition is.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is essentially the density right of one with the other?",
                    "label": 0
                },
                {
                    "sent": "An higher H~ I mean this is like a logarithm of a moment generating function.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is just notation.",
                    "label": 0
                },
                {
                    "sent": "So H~ is.",
                    "label": 0
                },
                {
                    "sent": "Right, it's some function you know that's rectify that I talked about before and this is just integration with respect to TPX Builder.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the interpretation.",
                    "label": 0
                },
                {
                    "sent": "It is well known that the relative entropy can be interpreted.",
                    "label": 0
                },
                {
                    "sent": "Information gain of the probability measure PX~ over.",
                    "label": 0
                },
                {
                    "sent": "PXI guess this is still done.",
                    "label": 0
                },
                {
                    "sent": "This is hat.",
                    "label": 0
                },
                {
                    "sent": "OK, so we are no longer in a finite setting, so instead of entropy, a better object to work with his relative entropy.",
                    "label": 0
                },
                {
                    "sent": "That's always well defined, as long as they're absolutely continuous.",
                    "label": 0
                },
                {
                    "sent": "If you like the entropy is is the relative entropy respect to the counting measure.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this object minus log of this.",
                    "label": 0
                },
                {
                    "sent": "Derivative is a generalization of the Shannon information for X. OK, it's a measure of the relative degree of surprising the outcome.",
                    "label": 0
                },
                {
                    "sent": "X = X Four, 2 distribution P Excellent PX ilda.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the average reduction in the degree of surprised in this outcome arising from the acceptance of VX.",
                    "label": 0
                },
                {
                    "sent": "Still does the distribution for X rather than practical.",
                    "label": 0
                },
                {
                    "sent": "This is interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Exponential minus age still does the likelihood function for X right associated with some unspecified observation, so this is if you like.",
                    "label": 1
                },
                {
                    "sent": "This is the residual degree of surprise contained in the observation given the prior distribution PX.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the theorem says that.",
                    "label": 0
                },
                {
                    "sent": "It's exactly like so, so this is the free energy now where the entropy is is replaced by the relative entropy, and this is the integration.",
                    "label": 0
                },
                {
                    "sent": "Right age is the energy function, so this is like the average energy.",
                    "label": 0
                },
                {
                    "sent": "Average with respect to PX~ and we minimize that with respect to PX~ and that this is exactly minus log of Z.",
                    "label": 0
                },
                {
                    "sent": "Right logarithm of the moment generating function.",
                    "label": 0
                },
                {
                    "sent": "Now this this is a convex programming problem on the space of measures.",
                    "label": 0
                },
                {
                    "sent": "It is a legend ventral dual that characterizes the distance of the conditional distribution from PX.",
                    "label": 0
                },
                {
                    "sent": "Right, this is the amount of if you like information gain in the conditional distribution relative to PX and that has a variation interpretation.",
                    "label": 0
                },
                {
                    "sent": "So these are conjugate certain regards, Fenchel duality of conjugate convex functions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's an interpretation of that which.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will let you look at that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the way, this is this generalizes the idea of maximum entropy and so on, and it's a much more.",
                    "label": 0
                },
                {
                    "sent": "Sort of robust.",
                    "label": 0
                },
                {
                    "sent": "Version of those objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so just one comment is that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that this, for example, this dual problem can be used to study things like.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Robustness to perturbations.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have the wrong statistics in the observation, what is the effect of that and that could be studied by studying the dual problem OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very general.",
                    "label": 0
                },
                {
                    "sent": "X is some random variable taking values in some space, and while some related random variable taking values in some other space and we have a characterization of the conditional distribution of execution, right?",
                    "label": 0
                },
                {
                    "sent": "So what we want to now specialize and we specialize that X is now a Markov diffusion process and we explicitly.",
                    "label": 0
                },
                {
                    "sent": "Describe what the observations are.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean the way I think think about it is that.",
                    "label": 0
                },
                {
                    "sent": "If you like, I mean, a lot of parts of physics has to do with the Fokker Planck equation in some way, one form or another.",
                    "label": 0
                },
                {
                    "sent": "But we're more interested in sort of understanding the interaction between observations and states.",
                    "label": 0
                },
                {
                    "sent": "And in sort of traditional distribution about that has to tell us.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm specializing.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have now a description of the state evolution of the state.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately this is changing rotation, but I mean the idea of fixing a notation.",
                    "label": 0
                },
                {
                    "sent": "Is this so that you can change it?",
                    "label": 0
                },
                {
                    "sent": "So I think I had G there before, but that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "This is the the noise spot, and this is the dynamical part.",
                    "label": 0
                },
                {
                    "sent": "If you like and I have an observation, I think I had H before, it's now G. OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What what do you expect is that?",
                    "label": 0
                },
                {
                    "sent": "Because you have a Markov process now.",
                    "label": 0
                },
                {
                    "sent": "Right you you would hope that this free energy minimization problem.",
                    "label": 0
                },
                {
                    "sent": "Somehow should be amenable to dynamic programming, stochastic dynamic programming, OK, and that's what what is made precise.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So there is one idea that is used here, which comes from measuring tirion so castec control is that.",
                    "label": 0
                },
                {
                    "sent": "So this is the observation now.",
                    "label": 0
                },
                {
                    "sent": "So I think I had dinner.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I look at the part space.",
                    "label": 0
                },
                {
                    "sent": "Let's say Y0T.",
                    "label": 0
                },
                {
                    "sent": "And there's a measure on that part space, let's call it PYK.",
                    "label": 0
                },
                {
                    "sent": "Down from the Cameron Martin theorem and Gusano theorem, what we want to do is right and we are interested in computing, say the conditional expectation of P of XD given Y from zero to T. So the idea is, is and so these are sort of the trajectories of Y, right?",
                    "label": 0
                },
                {
                    "sent": "And this trajectory described by this sort of stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "What we would like to do is we would like to do.",
                    "label": 0
                },
                {
                    "sent": "We would like to look at these trajectory's under a different measure.",
                    "label": 0
                },
                {
                    "sent": "And we would like to that measure to be such that YT looks like Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the Whitey and so if I call it the OSYT&OS should be independent of each other and the distribution of the Earth shouldn't change.",
                    "label": 0
                },
                {
                    "sent": "Ann from the Cameron Martin ideas and gson of ideas.",
                    "label": 0
                },
                {
                    "sent": "Such a measure exists.",
                    "label": 0
                },
                {
                    "sent": "Because YT now becomes independent of ZS.",
                    "label": 0
                },
                {
                    "sent": "This condition.",
                    "label": 0
                },
                {
                    "sent": "This conditional expectation becomes an expectation.",
                    "label": 0
                },
                {
                    "sent": "So that's this change of measure argument that is used here, which.",
                    "label": 0
                },
                {
                    "sent": "OK. And this change of measure is actually given in terms of.",
                    "label": 0
                },
                {
                    "sent": "Function like this?",
                    "label": 0
                },
                {
                    "sent": "This is the sort of the Cameron Martin.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the The upshot of using this technology is that the free energy minimization problem when we when we specify.",
                    "label": 0
                },
                {
                    "sent": "The evolution when we specify the thing that I'm trying to estimate, namely the X's as a Markov diffusion process, turns out to become a stochastic control problem and.",
                    "label": 0
                },
                {
                    "sent": "So, and the part estimation problem.",
                    "label": 0
                },
                {
                    "sent": "Turns out that you can solve the pause estimation problem.",
                    "label": 0
                },
                {
                    "sent": "And as a backward forward or forward backward filter.",
                    "label": 0
                },
                {
                    "sent": "So you start at the end.",
                    "label": 0
                },
                {
                    "sent": "And you run a likelihood filter.",
                    "label": 0
                },
                {
                    "sent": "In order to estimate.",
                    "label": 0
                },
                {
                    "sent": "The initial stage X0.",
                    "label": 0
                },
                {
                    "sent": "Now in the process, you lose some information as I've said that the filter, so that's like a filtering problem, and I've argued that the filtering problem the filter stores that information that's needed in order to understand the present and the future.",
                    "label": 0
                },
                {
                    "sent": "And this is historical information.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "At some optimal rate turns out to be given by the Fisher information, so there's a filtering problem, so we've lost some information and we are interested in estimating the path.",
                    "label": 0
                },
                {
                    "sent": "X03.",
                    "label": 0
                },
                {
                    "sent": "And what we have lost is obtained by solving a stochastic control problem on the forward direction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you might wanna emphasize Kozol.",
                    "label": 0
                },
                {
                    "sent": "Feel free as opposed to yes, yes.",
                    "label": 0
                },
                {
                    "sent": "What Roger is commenting is that the path estimation problem is in some sense non causal in some sense, right?",
                    "label": 0
                },
                {
                    "sent": "And that's it's about estimation problem that is relevant to information theory because you're allowed to kind of go back and sort of.",
                    "label": 0
                },
                {
                    "sent": "Right, you do it in blocks as it's batch processing if you like, Yep.",
                    "label": 0
                },
                {
                    "sent": "And this stochastic control problem is I will write this down.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the same as saying that is this path.",
                    "label": 0
                },
                {
                    "sent": "Estimate or this is not a marginal no.",
                    "label": 0
                },
                {
                    "sent": "This is the path whole part, so I'm doing that because in this case you have a fixed thing.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I mean I have to do it the free energy at each time T and so yeah.",
                    "label": 0
                },
                {
                    "sent": "The filtering problem is contained in this path estimation problem if you like.",
                    "label": 0
                },
                {
                    "sent": "So this process timation is like smoothing is like having the common figure running back and forth right now, but I'm yeah, but but I mean the smoothing problem typically is.",
                    "label": 0
                },
                {
                    "sent": "So here is 0 T here is.",
                    "label": 0
                },
                {
                    "sent": "X.",
                    "label": 0
                },
                {
                    "sent": "Right, the filtering problem you estimate at time S the smoothing problem.",
                    "label": 0
                },
                {
                    "sent": "You would estimate at some time.",
                    "label": 0
                },
                {
                    "sent": "Say tower less than us.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You can run forward backward filters, but here I'm estimating the whole path so it's.",
                    "label": 0
                },
                {
                    "sent": "It is smoothing if you wish.",
                    "label": 0
                },
                {
                    "sent": "But this is not been given by Feynman cuts.",
                    "label": 0
                },
                {
                    "sent": "The stochastic control part will be.",
                    "label": 0
                },
                {
                    "sent": "The filter will be given by Feynman cuts formula.",
                    "label": 0
                },
                {
                    "sent": "Causal filtering problem.",
                    "label": 0
                },
                {
                    "sent": "What the causal filtering problem will be given by account.",
                    "label": 0
                },
                {
                    "sent": "So let's let's think of everything in terms of systems.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we do smoothing.",
                    "label": 0
                },
                {
                    "sent": "Now the equivalent.",
                    "label": 0
                },
                {
                    "sent": "Tell me the rotation for a.",
                    "label": 0
                },
                {
                    "sent": "For the dual control.",
                    "label": 0
                },
                {
                    "sent": "Of doing smooth so there, I mean that's what I'm giving you, one that when you run the backward filter you lose some information right?",
                    "label": 0
                },
                {
                    "sent": "Because the filter stores only that information that's needed to understand the present and the future.",
                    "label": 0
                },
                {
                    "sent": "Which if you want to estimate the part you have to recover.",
                    "label": 0
                },
                {
                    "sent": "The information that you have lost and that is obtained by an appropriate stochastic control problem.",
                    "label": 0
                },
                {
                    "sent": "And the forward direction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But try this on precise.",
                    "label": 0
                },
                {
                    "sent": "The duality between a.",
                    "label": 0
                },
                {
                    "sent": "Free endpoint problem.",
                    "label": 0
                },
                {
                    "sent": "Then what you have to add by way of cost to get them fixed.",
                    "label": 0
                },
                {
                    "sent": "Endpoint version is a kind of duality between filtering and smoothing, yeah?",
                    "label": 0
                },
                {
                    "sent": "So let me write down the stochastic the stochastic control problem.",
                    "label": 0
                },
                {
                    "sent": "I was just asking basically the controls we get about.",
                    "label": 0
                },
                {
                    "sent": "We have unstable systems right?",
                    "label": 0
                },
                {
                    "sent": "And we want to control an unstable system.",
                    "label": 0
                },
                {
                    "sent": "So what is really the equivalent of smoking in them?",
                    "label": 0
                },
                {
                    "sent": "So do I start in in controls from stable solution and I go to unstable solution about what does it really mean substantial?",
                    "label": 0
                },
                {
                    "sent": "Controlling the system.",
                    "label": 0
                },
                {
                    "sent": "What I mean?",
                    "label": 0
                },
                {
                    "sent": "Maybe you could, maybe we could have a discussion at the end of the talk, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, at the moment what I want to say is that this backward filter, right which estimates the initial state?",
                    "label": 0
                },
                {
                    "sent": "By the way, if the initial state is fixed, then you don't need the backward filter, who just needs a stochastic control problem in the forward direction.",
                    "label": 0
                },
                {
                    "sent": "So when you solve the optimal stochastic control problem right, you will get a measure on the space of parts.",
                    "label": 0
                },
                {
                    "sent": "And that measure is the condition measure.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a broad conceptual suggestion that what matters is the measure on the part space.",
                    "label": 0
                },
                {
                    "sent": "In all of this.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm emphasizing here.",
                    "label": 0
                },
                {
                    "sent": "Sort of the past face build on the Martin Gale view and so.",
                    "label": 0
                },
                {
                    "sent": "Will also recognizing the estimation problem rather than control problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "OK, another way to say that there are certainly certain stochastic you could apply the duality in the other direction.",
                    "label": 0
                },
                {
                    "sent": "There are certain stochastic control problems, namely, which has a minimum energy term and some nonlinear function of the state that has a dual problem, which is like a filtering problem which can then be solved using particle filtering and so on.",
                    "label": 0
                },
                {
                    "sent": "So it goes both ways, so the filtering problem corresponds to a certain stochastic control problem and certain classes of stochastic control problems.",
                    "label": 0
                },
                {
                    "sent": "Is related to filtering problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Troll problem.",
                    "label": 0
                },
                {
                    "sent": "So notice that the there is a control term now.",
                    "label": 0
                },
                {
                    "sent": "So with this, with a suitable scaling A. OK. And the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The stochastic control problem, if you assume that the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, look at equation 41 if you assume for a moment that.",
                    "label": 0
                },
                {
                    "sent": "The observations are differentiable.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "Then the stochastic control problem has two paths.",
                    "label": 0
                },
                {
                    "sent": "One is a minimum energy term, if you like.",
                    "label": 0
                },
                {
                    "sent": "And this is like a tracking term.",
                    "label": 0
                },
                {
                    "sent": "And those of you are familiar with things like, I think Bryson, Frazer, and.",
                    "label": 0
                },
                {
                    "sent": "So there's this variational interpretation of the Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "So if you like, this is trying to.",
                    "label": 0
                },
                {
                    "sent": "Track the observation as best as it can with a constraint on the energy.",
                    "label": 0
                },
                {
                    "sent": "OK. And that's but this is what I'm showing you now is a quite a big generalization about is known in the linear in the linear case.",
                    "label": 0
                },
                {
                    "sent": "So there's some technicalities here, and the technicalities lead that the essentially the filtering problem has a corresponding stochastic control problem, which is like this kind of an optimal tracking problem.",
                    "label": 0
                },
                {
                    "sent": "With with a stochastic differential equation, which is sort of modulated by this control term.",
                    "label": 0
                },
                {
                    "sent": "And the, by the way, there's the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So this is what I.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've written here.",
                    "label": 0
                },
                {
                    "sent": "This involves an energy term for the control and at least squares term for the observation part fit.",
                    "label": 0
                },
                {
                    "sent": "These corresponded to two terms in base formula, representing the degrees of match with the prior distribution and the observation part.",
                    "label": 0
                },
                {
                    "sent": "If you like this minimum energy term really is the relative entropy term with respect to the prior looked at in an appropriate measure.",
                    "label": 0
                },
                {
                    "sent": "Using this change of measure argument.",
                    "label": 0
                },
                {
                    "sent": "Weather in the UK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve this stochastic control problem, which is turned out to be the gradient of the value function.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a dual problem which also has right.",
                    "label": 0
                },
                {
                    "sent": "Remember I showed this Fenchel duality, that isn't.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimal control interpretation.",
                    "label": 0
                },
                {
                    "sent": "This is your log row appearing here.",
                    "label": 0
                },
                {
                    "sent": "This is related to in some ways to work that I did with Wendell Fleming of using the Hough transformation, which relates Hamilton Jacobi equations and parabolic equations.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My time is up.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With me I think I.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusion sex.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's this two part paper which I can make available which shows how you can prove the noisy channel coding theorem using these ideas of free energy minimization.",
                    "label": 0
                },
                {
                    "sent": "I think the infinite time behavior of the filter is is I would say, still open, maybe Ramone unhandled would have something to say about that.",
                    "label": 0
                },
                {
                    "sent": "All business of control filtering which I haven't talked talked about.",
                    "label": 0
                },
                {
                    "sent": "And there's lots of numerical work going on, in particular of the work of shoring.",
                    "label": 0
                },
                {
                    "sent": "On implicit sampling for particle filtering.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "I think as I remember in the abstract you mentioned that is offensive transform as well, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these are.",
                    "label": 0
                },
                {
                    "sent": "So these two problems.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Namely, so this is minimization of the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "So this is the of entropy plus average energy, and this has a dual problem.",
                    "label": 0
                },
                {
                    "sent": "Remember what I is I is this minus log of this moment generating function.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "Right the edge till there is some other likelihood function.",
                    "label": 0
                },
                {
                    "sent": "OK and this is integrated against the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "These two are Fenchel, dual for each other.",
                    "label": 0
                },
                {
                    "sent": "Jean central I mean.",
                    "label": 0
                },
                {
                    "sent": "If you like, this is a special case of the sort of the at least one side of it of the dance cover and variational principle that appears in large deviations, which is a much more general thing.",
                    "label": 0
                },
                {
                    "sent": "We don't need that.",
                    "label": 0
                },
                {
                    "sent": "Doing I age like where does it come from machine learning.",
                    "label": 0
                },
                {
                    "sent": "People use it all over the place and.",
                    "label": 0
                },
                {
                    "sent": "Is this the first time house it will show or?",
                    "label": 0
                },
                {
                    "sent": "So what's this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Can I put it or or doesn't have a look somewhere else?",
                    "label": 0
                },
                {
                    "sent": "Question about history of the No.",
                    "label": 0
                },
                {
                    "sent": "I mean I think.",
                    "label": 0
                },
                {
                    "sent": "If a if you wish, I mean the proof of this theorem is very easy, and to me the proof of the theorem is not the important part.",
                    "label": 0
                },
                {
                    "sent": "Is this is the theorem itself.",
                    "label": 0
                },
                {
                    "sent": "And the point is that if you're going, if you think of the noisy channel coding theorem.",
                    "label": 0
                },
                {
                    "sent": "Can be posed as a problem estimation problem, but over an infinite time and you cannot write a base formula for that.",
                    "label": 0
                },
                {
                    "sent": "So you have to proceed via this free energy minimization take limits, just has to prove the Gibbs variational principle.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You know you you have to look at.",
                    "label": 0
                },
                {
                    "sent": "Super specific, free energies, etc.",
                    "label": 0
                },
                {
                    "sent": "And then take an appropriate limit and the limiting argument is.",
                    "label": 0
                },
                {
                    "sent": "So what I'm suggesting is the idea of.",
                    "label": 0
                },
                {
                    "sent": "I think the idea of free energy minimization is in some sense more general than the base formula.",
                    "label": 0
                },
                {
                    "sent": "Claiming that this goes back to gives us.",
                    "label": 0
                },
                {
                    "sent": "Very original stuff gives this is built on this.",
                    "label": 0
                },
                {
                    "sent": "Sorry I missed that.",
                    "label": 0
                },
                {
                    "sent": "Prove this this duality.",
                    "label": 0
                },
                {
                    "sent": "We offer filtering, but yeah well I mean what I just described is of course well known that the finite case.",
                    "label": 0
                },
                {
                    "sent": "The yeah.",
                    "label": 0
                },
                {
                    "sent": "Yup, I think was in gives book from published in 1901.",
                    "label": 0
                },
                {
                    "sent": "Basically before that, maybe.",
                    "label": 0
                },
                {
                    "sent": "But you know these, this talk of relating information theory to statistical mechanics has been talked about for long times, right?",
                    "label": 0
                },
                {
                    "sent": "Shannon himself said that once we understand information theory, you know deeply.",
                    "label": 0
                },
                {
                    "sent": "It'll be like 10 million amix or something.",
                    "label": 0
                },
                {
                    "sent": "But as far as I know, nobody has shown that the noisy channel coding theorem is like.",
                    "label": 0
                },
                {
                    "sent": "It's like Bayesian estimation on an infinite horizon.",
                    "label": 0
                },
                {
                    "sent": "And that is in a very precise sense, like the Gibbs variational principle.",
                    "label": 0
                },
                {
                    "sent": "And we we use the full Arsenal of, you know the Russian ideas.",
                    "label": 0
                },
                {
                    "sent": "But on one of your latest slides, you had where you linked it to to the final cuts formula you have this control formulation.",
                    "label": 0
                },
                {
                    "sent": "We had a function a multiplying the control yeah, and then you had a noise term.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the formulation is not familiar with this.",
                    "label": 0
                },
                {
                    "sent": "There has to be a relation between these two terms.",
                    "label": 0
                },
                {
                    "sent": "I didn't see that.",
                    "label": 0
                },
                {
                    "sent": "Funny.",
                    "label": 0
                },
                {
                    "sent": "Channel some of the last yeah.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other way.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make this remark here.",
                    "label": 0
                },
                {
                    "sent": "Sorry I lost.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I had the equation for.",
                    "label": 0
                },
                {
                    "sent": "Buy GFI.",
                    "label": 0
                },
                {
                    "sent": "Right, that's just second right, which is the Kushner equation.",
                    "label": 1
                },
                {
                    "sent": "Now if you write by Tier 5 as he wrote YIFY.",
                    "label": 0
                },
                {
                    "sent": "Integral roti then roti satisfies a linear stochastic differential equation, so it is.",
                    "label": 0
                },
                {
                    "sent": "So I should foster the density and then write it in a normal that satisfies this equation.",
                    "label": 0
                },
                {
                    "sent": "Age is the observation.",
                    "label": 0
                },
                {
                    "sent": "Roti gyt right, and this equation can you can solve using Panthers formula which is expectation over the measure.",
                    "label": 0
                },
                {
                    "sent": "UX measure.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Say no zero XD.",
                    "label": 0
                },
                {
                    "sent": "Exponential internal 0TH D YS minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "Explain this.",
                    "label": 0
                },
                {
                    "sent": "And that's the final customer.",
                    "label": 0
                },
                {
                    "sent": "Well, so if you start with respect to.",
                    "label": 0
                },
                {
                    "sent": "Major fix times will be why this is well.",
                    "label": 0
                },
                {
                    "sent": "No, I meant.",
                    "label": 1
                },
                {
                    "sent": "In the formulation that I wrote down, there is a.",
                    "label": 0
                },
                {
                    "sent": "There is the better control and the noise acting the same subspace.",
                    "label": 0
                },
                {
                    "sent": "And there is there multiply both business by AFP.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Which is Sigma Sigma transpose?",
                    "label": 0
                },
                {
                    "sent": "I was just asking whether this was more general, but it was basically the same.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't think you can write down a formal funding cost formula for the control problem.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can write down for the stochastic differential equation, you can write down a final cuts formula.",
                    "label": 0
                }
            ]
        }
    }
}