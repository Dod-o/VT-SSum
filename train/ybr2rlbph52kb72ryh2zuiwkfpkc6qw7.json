{
    "id": "ybr2rlbph52kb72ryh2zuiwkfpkc6qw7",
    "title": "Dynamical Pose Filtering for Mixtures of Gaussian Processes",
    "info": {
        "author": [
            "Martin Fergie, University of Manchester"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_fergie_pose_filtering/",
    "segmentation": [
        [
            "My name is Martin Fergie from the University of Manchester and I'm looking into doing discriminative pose estimation an.",
            "So discriminative pose estimation is a different approach, where you extract a single image feature."
        ],
        [
            "From your image of your subject and you learn a regression function to map to the pose.",
            "So the illustration I've got to build on the left hand side is trying to illustrate a bag of words feature with sift descriptors, so I'm sure you're familiar with back of words.",
            "We extract a codebook and so he reach what we're showing is the dominant orientation scale of each feature, and these are automatically extracted from the image and we then histogram against this to get a descriptor for our image, we learn a regression function directly to map the pose.",
            "Now, the advantage of these techniques is they're very fast, so our technique will predict 1000 or so frames in a matter of seconds.",
            "But they require an offline training corpus in order to build the models.",
            "The challenges is."
        ],
        [
            "So you've got large dimensional datasets, so your input features your image features can be about 300 dimensions, and for a 3D skeleton you've got about 50 dimensions of pose output, so that's all the three dimensions for each joint axis.",
            "And so this makes learning regression functions very difficult.",
            "And we've also got large datasets, so that means that you've got to have regression models that scale well.",
            "Multimodal predictions so you can have ambiguities caused by UN observed depth and background distractions.",
            "These things need to be modeled by your regression function.",
            "As well, large amounts of ambiguity due to the background.",
            "So the two contributions in this paper is the first one is a mixture of Gaussian processes model, where we are following a kind of mixtures."
        ],
        [
            "Experts idea, but we're going to look at how we can use Gaussian processes and will explain why that's valuable.",
            "And the second part is how we can combine these discriminative models with a dynamics model.",
            "OK, so it may."
        ],
        [
            "List of experts model is being commonly used for human pose estimation and the general idea is you model your regression function with a combination of linear predictors.",
            "So we've drawn this is.",
            "We've got functional mapping, so each expert FI of X gives a prediction over the pose, which is a function of the input and is weighted by prior, and this prior was set using a logistic regression model, and so it just gives a probability for an image feature that gives a probability this probability distribution over which image feature that expert belongs to.",
            "And so, in general you model this is a mixture of Gaussian distributions.",
            "So your predictive distribution is a mixture of normal distributions, where the mean and covariance are given as a function of your input, and then your gating.",
            "As I said, is logistic regression model.",
            "So typically you'd use linear experts with these, which has the advantage of being fast and you train them by."
        ],
        [
            "Taking away to combination of input points, but we want to look into using Gaussian processes in the advantage of this is that firstly allows us to have each expert map a nonlinear function.",
            "And it allows us to get an accurate predictive uncertainty over our pose.",
            "So the two plots that I've shown above, the top plot is showing Gaussian process regression, so this is.",
            "Anne takes your input data and your training data and a test input and it gives you a normal distribution over the pose.",
            "So the black crosses are the training points, the blue line, the blue line, and the Patch behind it shows the mean and variance of prediction.",
            "And as you can see as your test data moves away from your training points, your predictor variance increases as you would hope because this is saying your expert doesn't know so much about this particular example that's looking at, so it gives more uncertainty in its prediction.",
            "However, there are advantages disadvantages to Gaussian processes.",
            "Firstly, they can only model a union model unimodal function on their own.",
            "So the lower plot shows essentially multimodal data where we've got two distinct modes in the output.",
            "So your input feature can map onto two different poses, and this is a case of ambiguity in your data.",
            "And so a single Gaussian process will average over these modes and give you an erroneous prediction, and so they also have cubic training complexity.",
            "And this means you can't use them in normal mixture of experts model, because it would be infeasible to train.",
            "So we break our experts up into smaller chunks in order to make our end small and train lots of small Gaussian processes.",
            "OK, so to apply them into mixtures of."
        ],
        [
            "Sports model.",
            "We need to introduce a partitioning over our data set so we introduce an indicator variable which we've denoted as Z, which basically says for a given value zed N. So for a particular training example, we assign it to one of our experts, so we give it an integer I in case so it's either one to K depending on what experts it belongs to.",
            "So I've illustrated on the left hand side.",
            "We've got the same data set we saw earlier.",
            "Um, we've randomly initialized these.",
            "Expert points and what we wish to do is learn these indicator variables to get something like on the right hand side where the different parts of our data set have been modeled by different experts, particularly the two.",
            "The section where it's multimodal, we've got a separate experts modeling each mode as well as the top right hand corner has much less signal noise than the top left hand corner.",
            "There's less uncertainty in that region of the data set, so we want to model that uncertainty with one coherent expert.",
            "So I just.",
            "So so so to do this we use a Gibbs sampling technique."
        ],
        [
            "So essentially what we wish to do is build a probability distribution for each training point.",
            "We can remove it from the model and we want to calculate probability of which of those experts that model belongs to, and so this is formed as a combination of.",
            "So we take the likelihood of our removed training point given each expert, and then we multiply that by the weighting given by the logistic regression model.",
            "And so this is similar in principle to extra expectation maximization in M you have.",
            "An like a set of soft assignments, whereas here we have to use a hard assignment and Gibbs sampling in order to be tractable with the Gaussian process.",
            "So I'll just demonstrate this learning procedure.",
            "OK, So what we have here is on the left hand side.",
            "We've got the plot we've seen already where we've got randomly initialized are indicator variables in the middle.",
            "I'm visualizing the predictive distribution of the model so each colored line and Patch behind it shows the mean and variance of the predictive distribution, and on the right hand side we show how the priors vary with the test input.",
            "So this is saying what's the probability of each expert at each test input.",
            "Swiss Re show the.",
            "Algorithm learning.",
            "What we see is start to see who starts a clustering effect on the left hand side.",
            "So you can see the blue points in the top right of still started clustering to one expert, and this causes the predicted distribution to come in and accurately model.",
            "The distribution of our data.",
            "And then eventually the prize will settle down the algorithm's tables to give a good fit of the data and the red dots.",
            "You can see a test samples drawn from the model, so we just sampled from.",
            "The posterior distribution of the model.",
            "We can see that it fits the training data well.",
            "OK."
        ],
        [
            "So the next thing we're going to look at the dynamics model.",
            "So the problem with these systems is, so we've got this Gaussian mixture model, which gives us a prediction for each frame independently.",
            "What we want to do is be able to obtain a smooth path through this Gaussian mixture distribution for all our frames.",
            "So the typical naive approach for predicting things is too, and this is what's commonly done in the mixtures of experts.",
            "Literature is to take a expectation over our distribution.",
            "So we basically take away to combination of all the different components.",
            "However, this averages out multimodality and it has no temporal dependency, so the tracking that you receive is often very jittery and jumpy.",
            "So what we wish to do is incorporated dynamics constraint.",
            "So here what we're doing is we're taking a combined prediction where opposes condition not only on the image."
        ],
        [
            "Teacher puts on the previous poses and we do this by essentially doing a kind of a joint distribution between our appearance model and our dynamics model.",
            "Now this is a challenging area in discriminative pose estimation because the dynamics is very dependent on your previous post.",
            "If your previous pose estimate is wrong, then your current pose estimate will be predicted wrong.",
            "And so we need to build a model which builds in some robustness towards this and also mixture of Gaussian district.",
            "So mixture of Gaussian distributions posterior distribution can't be applied in a linear dynamical setting.",
            "You can't use standard Max.",
            "Some algorithms with this distribution because it doesn't form a closed form on its product and so the distribution grows exponentially.",
            "So what we do is we again we use the indicator variable that we."
        ],
        [
            "Discussed in earlier on but in a different setting, we're now going to say throughout our tracking sequence, we maintain K predictions.",
            "One.",
            "Each prediction is essentially saying So what would be our estimate if appearance expert I made the prediction at this frame and so this essentially breaking our mixture of Gaussian distributions into separate Gaussian predictions on each frame and allowing us to do these kind of dynamic predictions using.",
            "A graphical model maximum algorithm so wish there is infer the sequence of experts.",
            "So the dynamics model we use is."
        ],
        [
            "Very simple dynamics model, basically built on the idea that human motion, if it's sufficiently well sample, can be well modeled with a second order linear process.",
            "So the two plots we've got above.",
            "So on the left hand side, we're showing a sensually a regression mapping in the form of a dynamics model.",
            "So we're taking our previous frames trying to predict forwards to our current frame, and what you can see is a large amounts of ambiguity.",
            "So for any previous pose, there can be a whole range of.",
            "Pose in the next frame.",
            "That makes a lot of intuitive sense because people move at different velocities.",
            "And you need to take that into account.",
            "But when we introduce the 2nd order term so 2 frames ago, we find that the vast majority of this.",
            "This ambiguity is resolved, and so for the sake of this paper, we're using a relatively simple dynamics model, which is just a linear regression function, but standard Bayesian linear regression based on a second order term.",
            "OK, so now we're going to look at how we get our predictions for each frame so.",
            "This equation will take you through step."
        ],
        [
            "By step, so the first step is we're going to take the expert prediction for a single expert I.",
            "And then we're going to combine this against.",
            "Die"
        ],
        [
            "Amix predictions which we are going to integrate out."
        ],
        [
            "Previous predictions, so our current frame expert I were going to take that Gaussian prediction and former Gaussian prior against our weighted combinations of all the previous positions that we're in.",
            "This is how we reduce the sensitivity to erroneous previous poses."
        ],
        [
            "And finally, we need to evaluate a node marginal and this is a standard maximum formulation algorithm, so I'm explaining where we get the terms for this from an, so the node manager is basing saying what's the probability of disappearance expert at frame I frame end.",
            "Sorry, making the prediction so this is formed of an appearance gating, so that's the logistic regression model from are mixtures of experts.",
            "Oppose density model.",
            "Now what this does is it ensures some structure between the joints.",
            "So our dynamics model modeled each joint independently, and we found that this was able to give much better generalization on test sets on a mixture of datasets.",
            "But by including its kernel density model, which is what we use, this brings the structure back in and forms that ensures that the poses a valid pose with respect to the training data.",
            "And we also wait that by await a combination of our previous marginal distributions.",
            "And so then we just use standard maxims.",
            "We do a forward pass, evaluate the X."
        ],
        [
            "Predictions as well as their marginals when we get to the end of the sequence.",
            "We picked the most likely wanna backtrack in a verb style to get our predictive sequence."
        ],
        [
            "So.",
            "1st, I'll show and this is a result from the sign language data set we've seen already today.",
            "And the tracking is reasonable.",
            "It has some errors in it, but it does give a smooth signal, but we still find that when our experts are wrong, our dynamics isn't doing enough of a job to correct it.",
            "But we don't do any background subtraction or anything special for our datasets here.",
            "It's just literally working with the data itself.",
            "And so we've got another.",
            "Anne.",
            "The day over.",
            "So this is a ballet data set now and this is the 3D datasets we're tracking 3D joint positions and we haven't had to change the model at all to adapt from the 2D to 3D.",
            "Just point out the green is our estimate and there are some mistakes.",
            "But they're relatively fast and complicated sequences.",
            "So fine."
        ],
        [
            "So just comparing to some.",
            "Other competitive models, so these are other people doing discriminative pose estimation.",
            "And.",
            "Overall, do very competitively in one case.",
            "Shared kernel information embedding outperforms us, but the mixture of Gaussian processes model does do a very good job of modeling these regression problems.",
            "And so here's a."
        ],
        [
            "Just rayshon showing we're taking the risk joint of ballet dancer relative to their hips, and we're demonstrating the position relative to the hips is overtime for the entire sequence, and so in the black is our ground truth data.",
            "The Red line is kind of naive prediction from my parents model and the green line is our dynamics model, which is smoothing.",
            "An naive prediction, the experts, but it still does closely follow the expert, so some an area for improvement is to give more confidence the dynamics so that it can steer things away when the appearance experts go wrong and then the blue line shows a linear dynamical system which struggled to learn on this data set and so the performance of that was quite poor.",
            "Unfortunately, we're not actually seeing the kind of improvements in the tracking errors with this dynamic framework.",
            "Yes, there's still quite a bit of work to be done.",
            "And that's it."
        ],
        [
            "Any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Martin Fergie from the University of Manchester and I'm looking into doing discriminative pose estimation an.",
                    "label": 0
                },
                {
                    "sent": "So discriminative pose estimation is a different approach, where you extract a single image feature.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From your image of your subject and you learn a regression function to map to the pose.",
                    "label": 0
                },
                {
                    "sent": "So the illustration I've got to build on the left hand side is trying to illustrate a bag of words feature with sift descriptors, so I'm sure you're familiar with back of words.",
                    "label": 0
                },
                {
                    "sent": "We extract a codebook and so he reach what we're showing is the dominant orientation scale of each feature, and these are automatically extracted from the image and we then histogram against this to get a descriptor for our image, we learn a regression function directly to map the pose.",
                    "label": 0
                },
                {
                    "sent": "Now, the advantage of these techniques is they're very fast, so our technique will predict 1000 or so frames in a matter of seconds.",
                    "label": 0
                },
                {
                    "sent": "But they require an offline training corpus in order to build the models.",
                    "label": 0
                },
                {
                    "sent": "The challenges is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you've got large dimensional datasets, so your input features your image features can be about 300 dimensions, and for a 3D skeleton you've got about 50 dimensions of pose output, so that's all the three dimensions for each joint axis.",
                    "label": 0
                },
                {
                    "sent": "And so this makes learning regression functions very difficult.",
                    "label": 0
                },
                {
                    "sent": "And we've also got large datasets, so that means that you've got to have regression models that scale well.",
                    "label": 0
                },
                {
                    "sent": "Multimodal predictions so you can have ambiguities caused by UN observed depth and background distractions.",
                    "label": 0
                },
                {
                    "sent": "These things need to be modeled by your regression function.",
                    "label": 0
                },
                {
                    "sent": "As well, large amounts of ambiguity due to the background.",
                    "label": 1
                },
                {
                    "sent": "So the two contributions in this paper is the first one is a mixture of Gaussian processes model, where we are following a kind of mixtures.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experts idea, but we're going to look at how we can use Gaussian processes and will explain why that's valuable.",
                    "label": 0
                },
                {
                    "sent": "And the second part is how we can combine these discriminative models with a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "OK, so it may.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List of experts model is being commonly used for human pose estimation and the general idea is you model your regression function with a combination of linear predictors.",
                    "label": 1
                },
                {
                    "sent": "So we've drawn this is.",
                    "label": 0
                },
                {
                    "sent": "We've got functional mapping, so each expert FI of X gives a prediction over the pose, which is a function of the input and is weighted by prior, and this prior was set using a logistic regression model, and so it just gives a probability for an image feature that gives a probability this probability distribution over which image feature that expert belongs to.",
                    "label": 1
                },
                {
                    "sent": "And so, in general you model this is a mixture of Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "So your predictive distribution is a mixture of normal distributions, where the mean and covariance are given as a function of your input, and then your gating.",
                    "label": 0
                },
                {
                    "sent": "As I said, is logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "So typically you'd use linear experts with these, which has the advantage of being fast and you train them by.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Taking away to combination of input points, but we want to look into using Gaussian processes in the advantage of this is that firstly allows us to have each expert map a nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "And it allows us to get an accurate predictive uncertainty over our pose.",
                    "label": 1
                },
                {
                    "sent": "So the two plots that I've shown above, the top plot is showing Gaussian process regression, so this is.",
                    "label": 0
                },
                {
                    "sent": "Anne takes your input data and your training data and a test input and it gives you a normal distribution over the pose.",
                    "label": 0
                },
                {
                    "sent": "So the black crosses are the training points, the blue line, the blue line, and the Patch behind it shows the mean and variance of prediction.",
                    "label": 0
                },
                {
                    "sent": "And as you can see as your test data moves away from your training points, your predictor variance increases as you would hope because this is saying your expert doesn't know so much about this particular example that's looking at, so it gives more uncertainty in its prediction.",
                    "label": 1
                },
                {
                    "sent": "However, there are advantages disadvantages to Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Firstly, they can only model a union model unimodal function on their own.",
                    "label": 0
                },
                {
                    "sent": "So the lower plot shows essentially multimodal data where we've got two distinct modes in the output.",
                    "label": 0
                },
                {
                    "sent": "So your input feature can map onto two different poses, and this is a case of ambiguity in your data.",
                    "label": 0
                },
                {
                    "sent": "And so a single Gaussian process will average over these modes and give you an erroneous prediction, and so they also have cubic training complexity.",
                    "label": 0
                },
                {
                    "sent": "And this means you can't use them in normal mixture of experts model, because it would be infeasible to train.",
                    "label": 0
                },
                {
                    "sent": "So we break our experts up into smaller chunks in order to make our end small and train lots of small Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "OK, so to apply them into mixtures of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sports model.",
                    "label": 0
                },
                {
                    "sent": "We need to introduce a partitioning over our data set so we introduce an indicator variable which we've denoted as Z, which basically says for a given value zed N. So for a particular training example, we assign it to one of our experts, so we give it an integer I in case so it's either one to K depending on what experts it belongs to.",
                    "label": 1
                },
                {
                    "sent": "So I've illustrated on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "We've got the same data set we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "Um, we've randomly initialized these.",
                    "label": 0
                },
                {
                    "sent": "Expert points and what we wish to do is learn these indicator variables to get something like on the right hand side where the different parts of our data set have been modeled by different experts, particularly the two.",
                    "label": 0
                },
                {
                    "sent": "The section where it's multimodal, we've got a separate experts modeling each mode as well as the top right hand corner has much less signal noise than the top left hand corner.",
                    "label": 0
                },
                {
                    "sent": "There's less uncertainty in that region of the data set, so we want to model that uncertainty with one coherent expert.",
                    "label": 1
                },
                {
                    "sent": "So I just.",
                    "label": 0
                },
                {
                    "sent": "So so so to do this we use a Gibbs sampling technique.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially what we wish to do is build a probability distribution for each training point.",
                    "label": 0
                },
                {
                    "sent": "We can remove it from the model and we want to calculate probability of which of those experts that model belongs to, and so this is formed as a combination of.",
                    "label": 0
                },
                {
                    "sent": "So we take the likelihood of our removed training point given each expert, and then we multiply that by the weighting given by the logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "And so this is similar in principle to extra expectation maximization in M you have.",
                    "label": 0
                },
                {
                    "sent": "An like a set of soft assignments, whereas here we have to use a hard assignment and Gibbs sampling in order to be tractable with the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So I'll just demonstrate this learning procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we have here is on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "We've got the plot we've seen already where we've got randomly initialized are indicator variables in the middle.",
                    "label": 0
                },
                {
                    "sent": "I'm visualizing the predictive distribution of the model so each colored line and Patch behind it shows the mean and variance of the predictive distribution, and on the right hand side we show how the priors vary with the test input.",
                    "label": 0
                },
                {
                    "sent": "So this is saying what's the probability of each expert at each test input.",
                    "label": 0
                },
                {
                    "sent": "Swiss Re show the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm learning.",
                    "label": 0
                },
                {
                    "sent": "What we see is start to see who starts a clustering effect on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "So you can see the blue points in the top right of still started clustering to one expert, and this causes the predicted distribution to come in and accurately model.",
                    "label": 0
                },
                {
                    "sent": "The distribution of our data.",
                    "label": 0
                },
                {
                    "sent": "And then eventually the prize will settle down the algorithm's tables to give a good fit of the data and the red dots.",
                    "label": 0
                },
                {
                    "sent": "You can see a test samples drawn from the model, so we just sampled from.",
                    "label": 0
                },
                {
                    "sent": "The posterior distribution of the model.",
                    "label": 0
                },
                {
                    "sent": "We can see that it fits the training data well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next thing we're going to look at the dynamics model.",
                    "label": 0
                },
                {
                    "sent": "So the problem with these systems is, so we've got this Gaussian mixture model, which gives us a prediction for each frame independently.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is be able to obtain a smooth path through this Gaussian mixture distribution for all our frames.",
                    "label": 1
                },
                {
                    "sent": "So the typical naive approach for predicting things is too, and this is what's commonly done in the mixtures of experts.",
                    "label": 0
                },
                {
                    "sent": "Literature is to take a expectation over our distribution.",
                    "label": 0
                },
                {
                    "sent": "So we basically take away to combination of all the different components.",
                    "label": 0
                },
                {
                    "sent": "However, this averages out multimodality and it has no temporal dependency, so the tracking that you receive is often very jittery and jumpy.",
                    "label": 1
                },
                {
                    "sent": "So what we wish to do is incorporated dynamics constraint.",
                    "label": 0
                },
                {
                    "sent": "So here what we're doing is we're taking a combined prediction where opposes condition not only on the image.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Teacher puts on the previous poses and we do this by essentially doing a kind of a joint distribution between our appearance model and our dynamics model.",
                    "label": 0
                },
                {
                    "sent": "Now this is a challenging area in discriminative pose estimation because the dynamics is very dependent on your previous post.",
                    "label": 0
                },
                {
                    "sent": "If your previous pose estimate is wrong, then your current pose estimate will be predicted wrong.",
                    "label": 0
                },
                {
                    "sent": "And so we need to build a model which builds in some robustness towards this and also mixture of Gaussian district.",
                    "label": 0
                },
                {
                    "sent": "So mixture of Gaussian distributions posterior distribution can't be applied in a linear dynamical setting.",
                    "label": 1
                },
                {
                    "sent": "You can't use standard Max.",
                    "label": 0
                },
                {
                    "sent": "Some algorithms with this distribution because it doesn't form a closed form on its product and so the distribution grows exponentially.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we again we use the indicator variable that we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discussed in earlier on but in a different setting, we're now going to say throughout our tracking sequence, we maintain K predictions.",
                    "label": 0
                },
                {
                    "sent": "One.",
                    "label": 0
                },
                {
                    "sent": "Each prediction is essentially saying So what would be our estimate if appearance expert I made the prediction at this frame and so this essentially breaking our mixture of Gaussian distributions into separate Gaussian predictions on each frame and allowing us to do these kind of dynamic predictions using.",
                    "label": 1
                },
                {
                    "sent": "A graphical model maximum algorithm so wish there is infer the sequence of experts.",
                    "label": 0
                },
                {
                    "sent": "So the dynamics model we use is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple dynamics model, basically built on the idea that human motion, if it's sufficiently well sample, can be well modeled with a second order linear process.",
                    "label": 0
                },
                {
                    "sent": "So the two plots we've got above.",
                    "label": 0
                },
                {
                    "sent": "So on the left hand side, we're showing a sensually a regression mapping in the form of a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "So we're taking our previous frames trying to predict forwards to our current frame, and what you can see is a large amounts of ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So for any previous pose, there can be a whole range of.",
                    "label": 0
                },
                {
                    "sent": "Pose in the next frame.",
                    "label": 0
                },
                {
                    "sent": "That makes a lot of intuitive sense because people move at different velocities.",
                    "label": 0
                },
                {
                    "sent": "And you need to take that into account.",
                    "label": 0
                },
                {
                    "sent": "But when we introduce the 2nd order term so 2 frames ago, we find that the vast majority of this.",
                    "label": 0
                },
                {
                    "sent": "This ambiguity is resolved, and so for the sake of this paper, we're using a relatively simple dynamics model, which is just a linear regression function, but standard Bayesian linear regression based on a second order term.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to look at how we get our predictions for each frame so.",
                    "label": 0
                },
                {
                    "sent": "This equation will take you through step.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By step, so the first step is we're going to take the expert prediction for a single expert I.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to combine this against.",
                    "label": 0
                },
                {
                    "sent": "Die",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amix predictions which we are going to integrate out.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous predictions, so our current frame expert I were going to take that Gaussian prediction and former Gaussian prior against our weighted combinations of all the previous positions that we're in.",
                    "label": 0
                },
                {
                    "sent": "This is how we reduce the sensitivity to erroneous previous poses.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we need to evaluate a node marginal and this is a standard maximum formulation algorithm, so I'm explaining where we get the terms for this from an, so the node manager is basing saying what's the probability of disappearance expert at frame I frame end.",
                    "label": 0
                },
                {
                    "sent": "Sorry, making the prediction so this is formed of an appearance gating, so that's the logistic regression model from are mixtures of experts.",
                    "label": 0
                },
                {
                    "sent": "Oppose density model.",
                    "label": 0
                },
                {
                    "sent": "Now what this does is it ensures some structure between the joints.",
                    "label": 0
                },
                {
                    "sent": "So our dynamics model modeled each joint independently, and we found that this was able to give much better generalization on test sets on a mixture of datasets.",
                    "label": 0
                },
                {
                    "sent": "But by including its kernel density model, which is what we use, this brings the structure back in and forms that ensures that the poses a valid pose with respect to the training data.",
                    "label": 0
                },
                {
                    "sent": "And we also wait that by await a combination of our previous marginal distributions.",
                    "label": 0
                },
                {
                    "sent": "And so then we just use standard maxims.",
                    "label": 0
                },
                {
                    "sent": "We do a forward pass, evaluate the X.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predictions as well as their marginals when we get to the end of the sequence.",
                    "label": 0
                },
                {
                    "sent": "We picked the most likely wanna backtrack in a verb style to get our predictive sequence.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll show and this is a result from the sign language data set we've seen already today.",
                    "label": 0
                },
                {
                    "sent": "And the tracking is reasonable.",
                    "label": 0
                },
                {
                    "sent": "It has some errors in it, but it does give a smooth signal, but we still find that when our experts are wrong, our dynamics isn't doing enough of a job to correct it.",
                    "label": 0
                },
                {
                    "sent": "But we don't do any background subtraction or anything special for our datasets here.",
                    "label": 0
                },
                {
                    "sent": "It's just literally working with the data itself.",
                    "label": 0
                },
                {
                    "sent": "And so we've got another.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The day over.",
                    "label": 0
                },
                {
                    "sent": "So this is a ballet data set now and this is the 3D datasets we're tracking 3D joint positions and we haven't had to change the model at all to adapt from the 2D to 3D.",
                    "label": 0
                },
                {
                    "sent": "Just point out the green is our estimate and there are some mistakes.",
                    "label": 0
                },
                {
                    "sent": "But they're relatively fast and complicated sequences.",
                    "label": 0
                },
                {
                    "sent": "So fine.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just comparing to some.",
                    "label": 0
                },
                {
                    "sent": "Other competitive models, so these are other people doing discriminative pose estimation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Overall, do very competitively in one case.",
                    "label": 0
                },
                {
                    "sent": "Shared kernel information embedding outperforms us, but the mixture of Gaussian processes model does do a very good job of modeling these regression problems.",
                    "label": 0
                },
                {
                    "sent": "And so here's a.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just rayshon showing we're taking the risk joint of ballet dancer relative to their hips, and we're demonstrating the position relative to the hips is overtime for the entire sequence, and so in the black is our ground truth data.",
                    "label": 0
                },
                {
                    "sent": "The Red line is kind of naive prediction from my parents model and the green line is our dynamics model, which is smoothing.",
                    "label": 0
                },
                {
                    "sent": "An naive prediction, the experts, but it still does closely follow the expert, so some an area for improvement is to give more confidence the dynamics so that it can steer things away when the appearance experts go wrong and then the blue line shows a linear dynamical system which struggled to learn on this data set and so the performance of that was quite poor.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we're not actually seeing the kind of improvements in the tracking errors with this dynamic framework.",
                    "label": 0
                },
                {
                    "sent": "Yes, there's still quite a bit of work to be done.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        }
    }
}