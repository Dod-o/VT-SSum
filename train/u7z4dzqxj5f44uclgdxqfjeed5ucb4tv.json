{
    "id": "u7z4dzqxj5f44uclgdxqfjeed5ucb4tv",
    "title": "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity",
    "info": {
        "author": [
            "Rasmus J. Kyng, Department of Computer Science, Yale University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_kyng_sample_complexity/",
    "segmentation": [
        [
            "Hi, I am Rasmus I'm giving this talk for Santos from Paula and Yunxiao.",
            "They unfortunately couldn't make it.",
            "I think they're out catching bad guys.",
            "Sorry, let's start with the title.",
            "It's Max versus min tensor decomposition and independent component analysis.",
            "With nearly linear sample complexity.",
            "So."
        ],
        [
            "This is the problem.",
            "Let me just talk you through that.",
            "It says we have IID samples of the Form X, which is a vector it's equal to AS.",
            "Where S is a vector of N fully independent components and A is an N by N matrix, and we can assume without loss of generality that S has mean zero and it has standard deviation one and that is unitary.",
            "And then we need a non Gaussian assumption to make sure that we can actually solve this problem.",
            "So we assume that the 4th moment is bounded away from 3.",
            "And now the state of the art.",
            "For solving this problem requires us to have order N to the five samples.",
            "So what we'll be doing today is to get a much better sample complexity.",
            "Now we can look at this matrix, which is the Hessian of the log of the Fourier transform of X.",
            "This turns out to be a good.",
            "Thing to look at if we want to figure out what a is.",
            "So that's the problem.",
            "Estimate a."
        ],
        [
            "Yeah.",
            "And a so this Hessian is equal to a times diagonal matrix which depends on the point that we're evaluating the Hessian.",
            "At times a transpose.",
            "And you can see the diagonal entries they have.",
            "These said I squared where set is equal to 8 * U and so if we choose you to be an independent normally distributed point then set is also an independent normally distributed point.",
            "So it's a matrix with random eigenvalues, and the eigenvalues are degree two polynomials in the set eyes.",
            "And so it suffices to find a.",
            "We can just compute an eigen decomposition of this matrix."
        ],
        [
            "Now the traditional approach to this is to just do one eigen decomposition and then get a from this.",
            "But that requires that the eigenvalues are well separated.",
            "And in particular, we see that the the accuracy that we get depends on the minimum gap of the said eyes.",
            "And so.",
            "It.",
            "The smallest set I is going to be order 1 / N and so the minimum gap is 1 / N ^2 and this means that you need to estimate the Hessian to order 1 / N ^2 accuracy and this is what drives the very high sample complexity."
        ],
        [
            "Now.",
            "What Santosh and Young do instead is they say, let's not do just one eigen decomposition.",
            "Let's do more, and so we'll use.",
            "First one eigen decomposition and find the maximum gap between two eigenvalues.",
            "And then we'll recurse on two subspaces that correspond to either side of that maximum gap.",
            "And then the maximum gap turns out to be quite well behaved, much better than the minimum gap, so we don't need as many samples to get a good.",
            "Estimate of this.",
            "And then we recurse on either side.",
            "And it works out that the this should not be order N samples.",
            "It should be.",
            "Oh~ there's a log factor, I guess.",
            "It turns out that we need much fewer samples because the maximum cap is much more well behaved.",
            "And that's it.",
            "You'll need to look at the paper for the error analysis.",
            "There's very cool result, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I am Rasmus I'm giving this talk for Santos from Paula and Yunxiao.",
                    "label": 0
                },
                {
                    "sent": "They unfortunately couldn't make it.",
                    "label": 0
                },
                {
                    "sent": "I think they're out catching bad guys.",
                    "label": 0
                },
                {
                    "sent": "Sorry, let's start with the title.",
                    "label": 0
                },
                {
                    "sent": "It's Max versus min tensor decomposition and independent component analysis.",
                    "label": 1
                },
                {
                    "sent": "With nearly linear sample complexity.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the problem.",
                    "label": 0
                },
                {
                    "sent": "Let me just talk you through that.",
                    "label": 0
                },
                {
                    "sent": "It says we have IID samples of the Form X, which is a vector it's equal to AS.",
                    "label": 0
                },
                {
                    "sent": "Where S is a vector of N fully independent components and A is an N by N matrix, and we can assume without loss of generality that S has mean zero and it has standard deviation one and that is unitary.",
                    "label": 0
                },
                {
                    "sent": "And then we need a non Gaussian assumption to make sure that we can actually solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the 4th moment is bounded away from 3.",
                    "label": 0
                },
                {
                    "sent": "And now the state of the art.",
                    "label": 0
                },
                {
                    "sent": "For solving this problem requires us to have order N to the five samples.",
                    "label": 0
                },
                {
                    "sent": "So what we'll be doing today is to get a much better sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Now we can look at this matrix, which is the Hessian of the log of the Fourier transform of X.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be a good.",
                    "label": 0
                },
                {
                    "sent": "Thing to look at if we want to figure out what a is.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem.",
                    "label": 0
                },
                {
                    "sent": "Estimate a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And a so this Hessian is equal to a times diagonal matrix which depends on the point that we're evaluating the Hessian.",
                    "label": 0
                },
                {
                    "sent": "At times a transpose.",
                    "label": 0
                },
                {
                    "sent": "And you can see the diagonal entries they have.",
                    "label": 0
                },
                {
                    "sent": "These said I squared where set is equal to 8 * U and so if we choose you to be an independent normally distributed point then set is also an independent normally distributed point.",
                    "label": 0
                },
                {
                    "sent": "So it's a matrix with random eigenvalues, and the eigenvalues are degree two polynomials in the set eyes.",
                    "label": 0
                },
                {
                    "sent": "And so it suffices to find a.",
                    "label": 0
                },
                {
                    "sent": "We can just compute an eigen decomposition of this matrix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the traditional approach to this is to just do one eigen decomposition and then get a from this.",
                    "label": 0
                },
                {
                    "sent": "But that requires that the eigenvalues are well separated.",
                    "label": 0
                },
                {
                    "sent": "And in particular, we see that the the accuracy that we get depends on the minimum gap of the said eyes.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "The smallest set I is going to be order 1 / N and so the minimum gap is 1 / N ^2 and this means that you need to estimate the Hessian to order 1 / N ^2 accuracy and this is what drives the very high sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What Santosh and Young do instead is they say, let's not do just one eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "Let's do more, and so we'll use.",
                    "label": 0
                },
                {
                    "sent": "First one eigen decomposition and find the maximum gap between two eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And then we'll recurse on two subspaces that correspond to either side of that maximum gap.",
                    "label": 0
                },
                {
                    "sent": "And then the maximum gap turns out to be quite well behaved, much better than the minimum gap, so we don't need as many samples to get a good.",
                    "label": 0
                },
                {
                    "sent": "Estimate of this.",
                    "label": 0
                },
                {
                    "sent": "And then we recurse on either side.",
                    "label": 0
                },
                {
                    "sent": "And it works out that the this should not be order N samples.",
                    "label": 0
                },
                {
                    "sent": "It should be.",
                    "label": 0
                },
                {
                    "sent": "Oh~ there's a log factor, I guess.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we need much fewer samples because the maximum cap is much more well behaved.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "You'll need to look at the paper for the error analysis.",
                    "label": 0
                },
                {
                    "sent": "There's very cool result, thank you.",
                    "label": 0
                }
            ]
        }
    }
}