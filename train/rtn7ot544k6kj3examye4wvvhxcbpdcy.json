{
    "id": "rtn7ot544k6kj3examye4wvvhxcbpdcy",
    "title": "Multiple kernel learning for multiple sources",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning"
        ]
    },
    "url": "http://videolectures.net/lms08_bach_mklms/",
    "segmentation": [
        [
            "Next, invited talk is from Spectrum Account normal video.",
            "This talk is not focused on learning more.",
            "So first I would like to thank the organizers for inviting me to give it talk about multiple sources, so I don't exactly know what multiple sources means, so I gather it will be multiple kernel instead.",
            "So to me multiple connection is just a way of doing multiple sources in a supervised way.",
            "Maybe kind of 1 so."
        ],
        [
            "So this will be the outline of my outlook, my talk, I would just produce an example from computer vision trying to show that this is a different thing to do to consider multiple sources for the vision problem.",
            "They represent how you can do it using multiple kernel learning an we show the different formulations for that problem.",
            "And finally, we will consider quickly some theoretical analysis, and I believe that the tools that I've been using for that could be very useful for other settings of learning with multiple sources in particular in particular.",
            "Using convince operators is a very simple way to analyze what you do when you have multiple views."
        ],
        [
            "So I guess this is not new that you have more and more digital media.",
            "You have videos, you have a lot of pictures both in the industry.",
            "Also for your own on your own personal PC, have gigabytes of pictures.",
            "If you have kids every kid is 1 gigabyte.",
            "So I have three kids.",
            "It makes finger bites.",
            "And when I was trying to show you that although stocks also, you have many tasks to be to be solved for computer vision and they are all associated with the machine learning problem.",
            "So if you can really find a good way of presenting images then you can apply by applying different, applying different machine learning techniques.",
            "We could have solved."
        ],
        [
            "Different tasks.",
            "So the 1st two times the first task is a one of image retrieval.",
            "So you see the output of the Great London, so you can see you get pictures of London.",
            "You get junior, the tube, you get some weird stuff over there.",
            "So here when I need those queries Google is only looking at surrounding surrounding text, so they don't look at the email right now.",
            "But at the time and here the goal you really need to look at the image to be able to output better to get a better output, or maybe classify the outputs.",
            "As pictures or Maps and even worse."
        ],
        [
            "If you take Paris, similarly, you get the pictures you get the map.",
            "You also get Paris Hilton.",
            "In a sense, this is a true query.",
            "I didn't invent it and so you really need to be able to look at the image to separate Paris or Paris Hilton.",
            "So this is a two simple example."
        ],
        [
            "And they weren't going back to kids.",
            "OK, so I have many pictures and usually they organized by by year and at the end it's very tough to look at them.",
            "OK, because you have so many of them, so you would like to be able to do some clustering, maybe clustering by kids.",
            "I will answer all the pictures of the kid one or the ones of key two.",
            "You might do want to visualize.",
            "We might want to do.",
            "Detection of a new kid maybe?",
            "Is it gonna do it so?",
            "So anyway, you have lots of tasks and all of them can be framed as a machine learning problem.",
            "Classification, clustering, outlier detection.",
            "Delete all those tasks of that complex.",
            "OK, so and so massive so here we don't deal with an image.",
            "We deal with.",
            "One image is one data point, so it means that she is very very massive.",
            "Is very you have a lot of heterogeneity.",
            "Yeah, different media, sometimes even inside an image you have a lot of different lots of different sources."
        ],
        [
            "OK then typically you have different cues for for for images, so you can look at shape.",
            "So I say you are extracted from some object and you want to capture as its shape is very important for recognition.",
            "Color is also important.",
            "Q also have texture which is a different queue.",
            "You have image can be segmented into different crew.",
            "You have interest points the way they are located inside the image.",
            "So at the end we have a lot of different cues and what?",
            "I claim that when you want to design A kernel, of course I will consider economy service in this talk if you want to design the kernel, it's a lot easier if you focus on a single single source at a time.",
            "Designing a kernel which is good straight for both shape, color, texture and segment interest points is kind of difficult, but if you want to focus only on interest point, people have been doing that."
        ],
        [
            "Play down, pull their, consider interest points.",
            "So here we have too many nice line, so this is just a corner corner detector and people have used like Roman and RLS.",
            "If we can both have used the pyramid match kernel was essentially divide the image into pieces and take an Instagram of the number histogram of.",
            "Of 6 features as interest point, so this is easy to do and the good thing is that this is really for interest points.",
            "Then if you want to kernel."
        ],
        [
            "For.",
            "For texture, so you can, but people do you really say, take a bunch of features like Graduate or Gabor wavelets, and then you get get histogram.",
            "Oh, that's great.",
            "So here you can see that it's easy to design for another."
        ],
        [
            "Designing a kernel for a simple one Q and finally I will only work with later.",
            "Actually is to look at segmented images with the assumption that usually segmentation does not work.",
            "You will never get.",
            "In two in the in two segments, one object in the middle segment and the background is the other segments always get things which are cut into pieces.",
            "So if you look if you consider that you over segment the image that you make the assumption you make the assumption that you have multi objects will be cut into pieces, then no segment will spend more.",
            "The object of interest in the background.",
            "So here.",
            "You go from Los Square Grid, so the images we pixel to a small graph and we'll see how it goes for dozens or hundreds of region.",
            "So how does it works very simply?",
            "Thank you image."
        ],
        [
            "Compute the gradient.",
            "Do a watershed transform from the team.",
            "For me, called me in, and then you can reduce the number.",
            "The number of segments and we know what we have done is under Colonel on those on those types of objects and they will represent these objects."
        ],
        [
            "Is biograf.",
            "But this is the image.",
            "This is a segmentation of the image.",
            "So we assign to each have one vertex per region.",
            "We connect, especially neighboring region and then we put a label on top of the region, which is just the histogram of pixels in that region.",
            "So this will capture essentially transitions in color, so it's another way of looking at the specific Q.",
            "For example, here we do not consider texture or interest points.",
            "OK, so it's it's a kernel on the single view if you wish.",
            "But now we have design.",
            "We have a graph so we can use a graph kernel technology to be able to get a graph on top of that.",
            "And before I go on."
        ],
        [
            "I think I would like to mention the similarity with bearing semantics, so buying semantics actually the way I understand it as a lot of common problems where you have.",
            "For a given task, you have a lot of different sources of information and an.",
            "Essentially you are bound to use similar techniques on top of it, even for single view the queues are very similar.",
            "You have queues based on 3D structure.",
            "For example, there's a statement vision and learning is quite amusing that there are a lot of exchanges between the two.",
            "The two fields.",
            "So before I go, no.",
            "So you have multiple sources in computer vision."
        ],
        [
            "Anne.",
            "The way I will combine them, it would be to consider so one kernel power source and may be considered community summing kernels as a way of combining combining sources, so this may be the most important part of the talk.",
            "Is that when you some kernels you actually concatenating features OK, so this is.",
            "And not very deep, but it seems a little be remember as soon as we added kernels is like taking the features for each kernel and concatenating them, and this is.",
            "So when we sum over the M kernels, so we have one kernel power source.",
            "Then it's like having a big picture vector.",
            "When we look at an 8 or the fear of the future."
        ],
        [
            "So no one other somewhat sometimes point which which is a bit ambiguous is that you have two views of this of this problem.",
            "The first one is to consider that you have a single input.",
            "SpaceX and you have multiple kernels over that same input space.",
            "This is usually available for images.",
            "This is what I've done.",
            "You take an image, you have one kernel for texture, one kernel, four segments for example.",
            "But you can also, so this is the first view.",
            "The second view is you may.",
            "You may assume that you have M views which are different and for which you have a kernel on each view.",
            "OK, so here there's only One X and the other is 1X1 expert source, so those are totally equivalent.",
            "You can go back and forth from one to the other, but often people misled.",
            "By that, so either you have one one input space and a lot of lot of kernels, or you have multiple.",
            "You have multiple pairs of inputs, video space and of course, since I'm a bit messy in this talk about user two formulas formalism."
        ],
        [
            "Also, what is multiple kernel learning, so we will consider.",
            "Learning a weighted sum of kernels.",
            "So as I told you a summing kernels is equivalent to.",
            "Community features so now we call fidget with 15 map for Colonel Colonel J essentially where if you concatenate features you also your weight vector or your function.",
            "We also we will also be a concatenation of F1 to FM.",
            "So now and you predict as a sum of each of those model products so it ends up doing what you take an X, map it to all the official spaces dot product with your functions and you put it by the system.",
            "So this is actually exactly.",
            "What I mean by concatenating features, not the many question is how you regularize on the on the set of how we penalize F1 to FM."
        ],
        [
            "Who used a CRM feature map?",
            "That's the minimize with respect to the sun.",
            "So first thing is that if I use the sum of the square norm of AJ is exactly the norm.",
            "The square norm of recognition for it means that is essentially doing the regular kernel machine.",
            "With that with that feature vector.",
            "And since the kernel is son of kernels for that representation at the end.",
            "Sum of squared norm is equivalent to summing kernels.",
            "OK, so this is a good thing to do.",
            "Often we just catenate features."
        ],
        [
            "So now if I change.",
            "The way we do arise if I just remove the square so it looks like a minimal minimum modification that makes a big difference and see the same difference as going from L2 to L1.",
            "So I assume the sun I serve the norms of each.",
            "Weight vector OK, so here, as I guess you all know that when you penalized by the L1 norm, which is the sum of the absolute values, you get lots of zeros.",
            "Here if you penalize the son of the North of AJ, you should get a lot of FJ which which are put to zero.",
            "OK altogether, we don't make any assumptions on what's happening inside FJ.",
            "We want object would be equal to 0 exactly or a or if it is not zero.",
            "However, however you want OK.",
            "So here.",
            "And how many questions when you use that?",
            "So I could at the block L1 norm.",
            "So first I will show you it is equivalent to a previous way of learning the kernel.",
            "Let's put in some algorithms and then present how you can analyze the way you induce positive with all those L1.",
            "Technique was mostly justifiable heuristics.",
            "You know that you should get some zeros, so you hope that you will be able to do both regularization and model selection essentially.",
            "Try to say to study if this is true or not."
        ],
        [
            "So how does it relate to the general kernel learning framework started by laundry, yet no clear title.",
            "So here you assume that this is a regular supervised learning machine where you have a son of a data fitting term over there and squared norm regularization.",
            "OK, so usually thing you can.",
            "Find the convex dual that problem and you get the Max with respect to Alpha of that thing.",
            "So for those familiar with the SVM, SVM, return in another way.",
            "What's important here is that at the end, if you take the optimal value after we have optimized for the best F of the best Alpha, you get maximum over Alpha of a linear function of K. So as a maximum of linear functions this is a convex function of K, and this is essentially what great liquid and colleagues have done is that since you have a convex function of K, just minimize it or a convex set and they have proved that this leads to a theoretical learning bounds, which were, I think, improved by Scrabble.",
            "David later, so this is the usual, so this is not restricted to kernels which are sums of.",
            "This is this is not restricted to kernels, just some of the finite number of kernels.",
            "OK, so this is more general and this has either publications to multitask learning."
        ],
        [
            "So how's equivalent?",
            "So this is the kernel in formulation as a girl has got liquid.",
            "When you maximize optimal, you minimize the optimal value of the SVM, and this is our formulation.",
            "When we use the same the same loss, but we penalize, but there's some of the norms, so putting a square here is it won't change the properties of the estimator, but one Lambda without the square there is a number with the square OK, but this makes these two problems formally equivalent.",
            "If you solve that one, you exactly have a solution of that one, and vice versa.",
            "So here this is a.",
            "This is an equivalence of multiple kernel learning by multiple block L1 norms and multiple kerning.",
            "Multiple kernel learning by minimizing with respect to the kernel weights the full the full SVM cost function.",
            "So here at the end, here the eight as they might, it's not clear where they come from, but they come from LaGrange multipliers, so you can look at the papers to see how you can get them from at the end.",
            "That's what's nice, is that.",
            "In a single optimization problem, you can learn both the weights of your kernel and the classifier itself with those weights.",
            "So it's a double.",
            "We have two things for the price.",
            "For the price of 1."
        ],
        [
            "But uh, algorithms quickly, so this was first cast as ASDP or QCQP.",
            "So quality programming with quadratic constraints.",
            "So for that you can use 2 boxes, But this takes awhile because here you have to remember that we have to pile up kernel matrices.",
            "OK, so if you have 100 kernels on the 1000 data points, which is very small.",
            "Already some problems too thick to fit that into memory, so all those techniques, which is which are based on 2 boxes won't really work OK, and I think one important point is that.",
            "Following about two in Bousquet is not useful to really optimize to machine precision.",
            "You can only optimize to over very low precision, and if you want to do that first order methods that only look at the gradient will be a largely enough, whereas generator boxes.",
            "These are usually 2nd order methods which are well adapted to get you exactly the solution at a high cost.",
            "Here you get you want to get.",
            "So much close to the solution at the Locust.",
            "So all those methods are essentially first of the methods to speed up the process.",
            "So our first wife was really using his.",
            "Anyone known formulations to rederive nieuwsma algorithms?",
            "But this needs this needed to record the SVM and this is not an easy thing to do if you want to be scalable in North.",
            "And other people have no design algorithms to go even to optimize the function of care directly, either by a cutting plane so bad people dissent and also additional work at this conference at the main conference.",
            "So here I think right now you can go up to.",
            "I don't know.",
            "We say 50,000 that are points and let's say 100 kernels.",
            "I think maybe maybe doable with those techniques.",
            "OK, so essentially.",
            "It has a.",
            "It ends up doing like a somewhat a lot of SVM training, so if you can.",
            "If your problem is more enough so that the SVM can still be run OK, then you have to be.",
            "You can run that, let's say 100 times and you will get the estimate of the MK of the problem."
        ],
        [
            "So now they are other important slides.",
            "Should you some kernel, so should you optimize the weights so this is a.",
            "This is very important in practice, So what I've told you is that if when you realize by the sum of the square norms, so you assume that you have one kernel or source, and then you have one FJ Purnell.",
            "So if you sum the squared norms, it's equivalent to assuming the kernels.",
            "So obviously when you want to sum the kernels, the complexity is simple, just one SVM problem.",
            "Here, if you regularize by the sun, the non squared norms, then you have to use one of the algorithms which are shown the above the previous slide.",
            "And of course it's lower.",
            "So there's a cost from going to.",
            "A fixed sum of kernels.",
            "Going from a fixed sum of kernels to a weighted sum of kernels and with a weighted sum with many many zeros.",
            "OK, so for me to take a message is really that since we have defined the multiple kernel learning as using L1 norm and that album GNOME especially suited for sparse problem.",
            "So here if you expect your problem to have only a small number of kernels at the end.",
            "So if you expect sparsity that the L1 norm.",
            "Should really help.",
            "On the contrary, if you if we expect all your kernels to be to be useful for your problem, then summing them is by far the best.",
            "The best thing to do.",
            "OK so for non sparse problem, anyone know might not beat L2 norms.",
            "And even worse it doesn't.",
            "But it can be.",
            "It can be performed worse and at the end is less efficient in terms of computation.",
            "So you get everything button.",
            "So really it's a matter of matter of expected Spa City.",
            "If you expect Spa City at 1:00 or MK is a. I think the thing to do, but if you don't submit the kernel is really is really as a really simple."
        ],
        [
            "So now let's take 2 examples on computer vision, where we have applied multiple kernel learning.",
            "And the first one is simply on this database of coral 14 or 14 we have.",
            "1400 natural images with 14 classes here.",
            "The goal was to essentially learn the hyperparameters of kernels."
        ],
        [
            "We have design with the data, shall we?",
            "Which would essentially graph kernels on segmented images.",
            "So here we compare.",
            "So this is histogram, which is the simplest approach without problem and those are three different types of graph kernels, so you can see the best performance for each of the types of kernel.",
            "And you combine them by multiple kernel learning, so you do get an improvement, which is significant.",
            "So here two things important here is that the type.",
            "Often OK, when you when you when you do.",
            "When you fuse multiple sources, sometimes one source is really better than the other ones.",
            "So in many cases when you want to learn the spice sparse.",
            "As possible combination you get 110 and all the rest 0.",
            "So to me it is still a successful application in the sense that you just had one kernel which was a lot better and the the algorithm just selected.",
            "It is very often the case.",
            "I'll just get just take the best one.",
            "In many cases that are Fusion is not clear.",
            "Working in the sense that you expect to get better performance by combining stuff, but sometimes it doesn't.",
            "So he does, but not by far, by a big margin.",
            "OK so second thing is that here we haven't tried yet, something the kernels OKC we should have done it and it is not done yet.",
            "An here since we had like 200 kernels I expect that you would do really a bad thing.",
            "OK. Also another point is that when you have many kernels.",
            "So yeah, when you have 10 kernels, swimming or taking, finding the weights is not solution.",
            "And then when you have like 200 or 300 then it starts to be important to get this past combination."
        ],
        [
            "But a better example is by other people than me, which is on the running similar techniques on the contact .1 database.",
            "So this is this was like 2 years ago the standard benchmark in computer vision for object recognition.",
            "Or you have 101 images like that and the goal of course is to do multi class."
        ],
        [
            "Specification.",
            "So we have many, in array.",
            "So what they simply did is to consider different kernels.",
            "So here I don't exactly know what they do, but I think this one is in shape.",
            "But it is said this one is simply an integral of gradients which I presented.",
            "Here and this one are based on color, but I don't have, but I forgot what it meant and for all those kernels.",
            "OK, so it tries several things.",
            "First they try first one nearest neighbor with that kernel, so I think this is very nice that people Julia design kernels to run an SVM.",
            "But once you have a kernel we have a metric.",
            "So if you have a metric you can do nearest neighbor and often it does not perverse you would expect it performs very bad, but here it's not as good as running the SVM based, not.",
            "Not really bad, so I think it's quite interesting to see that.",
            "Sometimes simple methods are actually a competitive, so they try them one versus one and one versus rest SVM.",
            "So this is 411 source of information, so one view if you wish.",
            "Well, they have lied.",
            "Our code and Badgers running this ad 1L1 block where L1 norm regularization they can.",
            "Go former at most 60% to more than 75% just by compiling kernels.",
            "So here someone this morning told me that when there's some kernels it doesn't doesn't work as well.",
            "Maybe 3% less is OK, so summing works decently, but not just not as good as a learning the weights.",
            "OK, they even improved it.",
            "And I kind of forgot what tricks are played, but they managed to even improve the performance over the regular regular MCL blockade."
        ],
        [
            "So let's no.",
            "Let's not try to analyze."
        ],
        [
            "The other thing.",
            "So here we have just returned the same problem for the square loss for from now on they will consider the square loss, which is simpler to analyze.",
            "So here I want to predict why I OK given the XIJ.",
            "So here assume that the hive X is decomposing 2M, different exponent, two XJ.",
            "XJ is simply the.",
            "X Ji is simply is the ice observation of the Jays viable?",
            "OK, so as I told you, when you have multiple kernel it's like having using a sum for prediction.",
            "So this is my prediction for the for the data, for the observation and here are regularised by the square norm.",
            "But there is a sub square of those are some of the norms.",
            "So here is exactly what people call in statistics generalized that is modeled by the son of functions of each of each viable.",
            "OK, so this is nice work in similar settings by radical metal for the sparse additive models, so it is a very important thing, and I think this is true for all learnings, all methods which are doing nonparametric.",
            "Estimation for multiple views, or even a single view for the algorithm, we use the Alpha.",
            "OK, so if F is a bit Hilbert space, let's say server space F as very high dimensions.",
            "So the only way you can estimate F is by using the presenter theorem or essentially.",
            "Finally, the convex do all and learning the famous alphas for the SVM.",
            "For example for learning for estimation you need Alpha.",
            "For analysis, Alpha is really really really nasty to use for analysis.",
            "In fact, if you can do, it is better to be able to use Alpha.",
            "What else you can still stay in the input space?",
            "You have tools to stay the input space and to do as if you were in a in a finite dimension so they see exactly what they're going to present.",
            "That algorithms go for Alpha and finally sees it.",
            "It's a lot easier to never, is never use Alpha, just use covariance operator."
        ],
        [
            "So what are those?",
            "So let's say you have a single random variable X, so Sigma XX will be a linear operator from the.",
            "From the from, the Hilbert space is a Hilbert space and CPD find through products by F&G.",
            "So if you take any functions in your Hilbert space F times Sigma X X * G is just the expectation of the product FG.",
            "So here are considered non centered covariance operators.",
            "But you could also consider centered one.",
            "When you remove.",
            "This is exactly the covariance between FX and GX and remove the product of the expectations for here.",
            "You don't really.",
            "You don't really need it, and you can simply so consider this operator, so here.",
            "You have to consider a bit of functional functional analysis so that is for you.",
            "If you don't know anything about Hilbert spaces, Sigma think of it as being just covariance matrix.",
            "Fix.",
            "So it's symmetric, so auto auto agent in the functional analysis space non negative like appearance like a covariance matrix.",
            "And Hilbert Schmidt for those we know what it means and really well.",
            "But it's nice is that this operator can be really estimated in a very simple way.",
            "Why how?",
            "But take FG?",
            "And so instead of taking the exact expectation with respect to X, just take the empirical expectation.",
            "So you replace the expectation here, but by an average as a cool thing.",
            "Is that Sigma Sigma hat you estimate converges to see my XX in the Hilbert space norm.",
            "So essentially you can use the same trick.",
            "It's like infinite dimension.",
            "You can estimate your confidence metrics really in the simple way.",
            "OK, so really it's it's very natural way of doing that."
        ],
        [
            "Very simple dinner.",
            "So if you have more than one view and that he sees the watch interest, people in this workshop, so you have multiple sources, so you want to consider the link between two sources and in the finite dimensional setting you will consider the cost covariance matrix.",
            "And here we just consider the cross covariance operator.",
            "Would you consider a function of viable XI in the function of variable XJ and you just define it like like an expectation of the product, which can be estimated also with the empirical averages.",
            "And now you can define a big John covariance operator to analyze your problem.",
            "Because we find correlations, but this is not so so important."
        ],
        [
            "So why I think it is a simple tool for analyzing like relationship between sources that formally used a input space.",
            "Everything is written in terms F covariance matrices.",
            "And you never need to go to the alphas, so you have to.",
            "You have to be a bit more careful, of course, because you have to prove some.",
            "Emergencies for the functional.",
            "For the functional case.",
            "But it provides a lot of it's really a good tool for that, and you can do a lot more than just studying supervised settings.",
            "You can do converse easier, or you can consider studying the behavior of nonparametric test statistics work I've been doing with the they'd air clean.",
            "And really, if you really want to.",
            "To be able to analyze all multiple multiple view problems, really, you should get those tools going on.",
            "And if you want to hire a good recently graduating PhD student and they actually whatever there is an expert in those settings.",
            "So I welcome everybody to make a PhD postdoc offer to him.",
            "I promise I would do it so."
        ],
        [
            "So how does it work in our setting?",
            "OK, so.",
            "Here we assume, so we assume that we the data were generated by a linear model, so we assume that we actually have a generated general generalized additive model is Y is a sum of functions of individual variables, and I have some some notes and double it.",
            "So on top of technical assumptions here.",
            "You need one assumption, which is a one of capacity of the correlation operator.",
            "So it might seems a bit abstract, but it has some nice meaning in our setting.",
            "As a nice man."
        ],
        [
            "That you get a capacity.",
            "Essentially, if this quantity is finite, so this quantity is just P. XI XJ, simply the marginal distribution of excise, J marginal density, and pure XI is exactly the marginal density over XI as he sees is very similar to the mutual information between XI and XJ.",
            "So what this assumption showed is that.",
            "OK for my phone, for my analysts to hold unit's dependence, because variables, this dependence is not too strong.",
            "OK, so for me this is quite a problem when you learn from multiple sources.",
            "Do you want sources to be similar or be very very dissimilar?",
            "So here at least in my set, under my seat assumptions, I need the sources to be.",
            "Enough dissimilar because if there are two similar then we get high dependence and this becomes infinite and I cannot apply my my theorem, so of course it may be a shortcoming of my analysis, but still seriously it is interesting that by just looking at the appropriate conditions for for your covariance operator you get back.",
            "Condition, which is somewhat interesting that you should have a finite mutual information between the XI and XJ."
        ],
        [
            "So what is the Nokia analysis?",
            "So here you might want to look at two things.",
            "So when you look at sparse problem, you both.",
            "You both want to be consistent.",
            "You want to recover to recover your functions that the true function that have generated your data but also seems to work with path methods you really want to estimate the good ones.",
            "OK, so I have people have been promising everybody that L1 norm can do both estimation and model selection.",
            "And for the last few years people have tried to study if this were the case for the lasso, which is just using blocks of size 1.",
            "And this is just an extension for the group lasso.",
            "Essentially, if this condition is satisfied, then you get you get consistent model estimation.",
            "You get a good sparsity pattern at the end, and this is not satisfied.",
            "Then you don't get the sparsity.",
            "The good sparsity pattern.",
            "So to pass this it's.",
            "It's Even so to two parties.",
            "It's a bit complicated if you're not used to it, but the most important point is this one.",
            "OK, focus on that one.",
            "That one is just a correlation between XI and XJ.",
            "So what?",
            "This says that if you have local relations then you're consistent, formal selection, and if you have high correlations, you're not consistent for model selection.",
            "So to me this was a very.",
            "Disappointing result that L1 does not actually always work.",
            "Sometimes it does, sometimes it does not.",
            "So this has led to a lot of work trying to fix that either through a two step procedure or through.",
            "Pre sampling, so this is really.",
            "To me, very disappointing that the lasso does not always work, so."
        ],
        [
            "So to conclude.",
            "I think so.",
            "I presented the multiple kernel learning for supervised learning.",
            "OK, so I guess this is 1 versions, 1 version of multiple sources, so I've tried to convince you that this is important for computer vision, where naturally you can express.",
            "Different types of information for your images and also I do believe that in Bertha Matic, this is important as well, but Bill Noble had a very sad talk for me this morning saying that in Belgium, attics in other cases it's right, it's better to serve the kernels and selecting the kernels by convex optimization.",
            "So I still believe that this is because it's too good.",
            "It is kernel that is designing so good that he knows in advance that they should enter the combination so he doesn't expect positive.",
            "So if you don't expect sparsity, and two is better to do, an L2 corresponds to assuming the kernels.",
            "So this is my interpretation.",
            "So so important or two ways of equivalent formulation for the multiple Quillen kernel learning framework?",
            "Yeah, the first one where you optimize first with respect to the weight vector.",
            "Then we expect the kernel, which is the usual one, but to me the one for which you can really get the most.",
            "Bang for the book is really the L1 interpretation.",
            "As soon as you go through the air.",
            "One interpretation you are one way to compare it easily to submit the kernels like a one or two and people have been doing a lot of those L1 and two comparisons in recent years.",
            "But I think there's a two important problems.",
            "The first one is that I think it's linked to.",
            "Recent advances in the theory of the of the lasso solasso is simply when all the kernels are rank one.",
            "OK, so the groups of size 1 and people have shown here that so if P is the number of variables OK, you can have P can be almost exponentially in NN being the number of data points.",
            "So it means that you can have a lot a lot of irrelevant variables.",
            "OK, and if you sell one, you can still be.",
            "You can still estimate at a good estimation of your problem.",
            "Here I'll say is 10,000.",
            "Which is not not so uncommon in machine learning.",
            "They say the Theoretischen says that he can be almost as big as the exponential 10,000.",
            "OK, great.",
            "So how do you write an algorithm on that many features?",
            "So it's kind of difficult and even worse, how can you even imagine that many features?",
            "So to me, the only way to even think about that many features if when those features come in a recursive way, OK, you cannot think you need a program to be able to describe those features that cannot be enumerated one by one, so I think.",
            "Is all setting let's P is very large.",
            "There's no way you can.",
            "You can do the practice without structure and I work at the main conference room to see.",
            "How you can really use that structure to be able to run lasso type of things when P is very large and by the piece very large, there's not so many ways of.",
            "Of doing this right now and we're using structure is quite important and I encourage you to look at that if you're interested in the P being very, very large.",
            "Finally, I think this is really bothering me is when the design kernels OK for further problem should consider to try to look at a lot of kernels which are all very similar.",
            "Or should we try to design kernels which are as different as possible?",
            "So if they are quite similar, you have some sharing information, and if they are not, you have no sharing.",
            "And to me this is really really hard for the people that try to design sources, but you do control design.",
            "You're designing your own source of information and it is really tough and I have no, I don't know any clear answer to that problem.",
            "If you do try to make things different or similar, my analysis shows that.",
            "If you want them to be true, that should be different, but this is only a theorem.",
            "And finally, I think this is also very important and often completely overlooked.",
            "Sometimes people say I use multiple sources, culture, mean whatever.",
            "But is it is it?",
            "Is it actually worth it?",
            "And under what conditions exactly does it work better?",
            "So I've been trying to do that.",
            "It's not.",
            "It's not so clear yet and.",
            "It's not so clear for the supervised case, but I guess it's also not simply for the unsupervised case that people have been talking about this morning.",
            "So thank you for your attention.",
            "Any questions?",
            "Yep, Yep.",
            "With only respect to the group.",
            "In general, does it breakdown so it's you mean the one the one time constraints, so it breaks down?",
            "I think you mean for all losses.",
            "For all sizes yet we it will breakdown at one point if you have.",
            "If you have high correlations it you won't get the good, but that's not so bad in the sense it's not because you have the wrong pattern that you do wrong estimation, so we have three types of results here.",
            "You have the regular model consistency, which is the strongest if you're the correct model, you get everything right.",
            "We have the regular consistency, so you might have the wrong pattern we use today.",
            "Estimate things correctly, and what often people are interesting interested.",
            "About is just efficiencies.",
            "The terms that you want to predict.",
            "Well, you don't really care if you go close to the true F, as long as you put it correctly, so this is what this is really.",
            "What people do in learning.",
            "But here in this setting people have gone back to try to consider regular consistency, an model consistency.",
            "So in terms of efficiency, still works well in terms of consistency, doesn't work anymore.",
            "Sure.",
            "Also, a large number of people.",
            "Crosses sometimes.",
            "Very complicated.",
            "So you should never."
        ],
        [
            "CQB short shorts or here for the QCQP is very hard to go beyond, like more than 500 data points and 10 kernels OK, but for all of those you can go up to a few 100 kernels an as as many data points that you algorithm can handle so I don't know exactly that noise.",
            "50 or.",
            "500 heavenwood yeah.",
            "Sure.",
            "But using what you P?",
            "Yeah, I totally agree with that.",
            "Then you should use like all the other words that people have been having been doing to really.",
            "Be able to reuse existing code here.",
            "The goal is your bit lazy mother be lazy or efficient, so efficient use of people do and if you can read those methods, allows to reuse existing SVM code.",
            "So if you like SVM, libusb, name.",
            "If you have simple SVM using simple SVM, Ann, those methods we just iterate and generate kernels and have the SVM liberal simple SVM solve for the kernel.",
            "But then you can really go a lot higher than 500 points, but true if you if you do that, you're bound to fail.",
            "They have them.",
            "I mean.",
            "Which was posted, yeah?",
            "Have you thought of but actually?",
            "But something really playing problems and any problems that may be causing cancer.",
            "Different properties of short, so very good.",
            "Very good as he market.",
            "Basically the most simple set covers possibly yeah.",
            "Capture information about the data is structured data.",
            "I think they're similar to a combination of colors.",
            "You you mean like pyramid match, kernel or?",
            "This I asked for the second question should have been compared to end of kernels, but we actually so send me if you send me your.",
            "Is it implemented?",
            "Or not, if it is implemented, we can add it to our comparison, but otherwise maybe complicated than the for the first question.",
            "What was it already?",
            "Uh.",
            "Oh yeah, I think that's very important.",
            "In fact, many people criticize that work because they will say what you really want is to learn the parameters of the Gaussian kernel.",
            "OK, I agree with them, in a sense, what would we like it to be able to put a different width on each viable build?",
            "A big gas in kernel?",
            "Maybe being able to learn those widths by like the Gaussian process guys do?",
            "But here you, by allowing only sums you keep comes convexity.",
            "And I do believe that this convexity helps you so much that it's OK to not doing that.",
            "Not doing what you would think originally.",
            "And yes, we are able to know consider products of kernels.",
            "This is exactly what I've been doing for this year at NIPS, which is 2.",
            "Consider."
        ],
        [
            "Modeling all the sons of kernels.",
            "But all the products."
        ],
        [
            "Give me give me."
        ],
        [
            "Two kernels and consider all the possible products of the kernels.",
            "OK, so you have to secure those and what I've been doing trying to exactly learn sparse combination of those two to the Q kernels.",
            "Here you get other products.",
            "This is the way of doing it, but we did being able to optimize parameters an.",
            "Trying to optimize the function which is not linear in the parameters is really.",
            "I think it's very difficult.",
            "Especially in high dimensions.",
            "So here the goal is to for this nonlinear variable selection problem to go to queue up to a few thousand.",
            "OK, so here, connectivity is important when you have a few thousand 2000 variables.",
            "Connectivity is really a decent thing to have.",
            "Training multiple kernels like this you end up with the difference or factors for each part.",
            "No.",
            "At the end you get 118 are OK and know the solution.",
            "You are fast correspond exactly to the solution of the regular SVM.",
            "With that combination of kernels.",
            "So you get one set of support vectors.",
            "One A1 enter."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next, invited talk is from Spectrum Account normal video.",
                    "label": 0
                },
                {
                    "sent": "This talk is not focused on learning more.",
                    "label": 0
                },
                {
                    "sent": "So first I would like to thank the organizers for inviting me to give it talk about multiple sources, so I don't exactly know what multiple sources means, so I gather it will be multiple kernel instead.",
                    "label": 1
                },
                {
                    "sent": "So to me multiple connection is just a way of doing multiple sources in a supervised way.",
                    "label": 0
                },
                {
                    "sent": "Maybe kind of 1 so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this will be the outline of my outlook, my talk, I would just produce an example from computer vision trying to show that this is a different thing to do to consider multiple sources for the vision problem.",
                    "label": 0
                },
                {
                    "sent": "They represent how you can do it using multiple kernel learning an we show the different formulations for that problem.",
                    "label": 1
                },
                {
                    "sent": "And finally, we will consider quickly some theoretical analysis, and I believe that the tools that I've been using for that could be very useful for other settings of learning with multiple sources in particular in particular.",
                    "label": 1
                },
                {
                    "sent": "Using convince operators is a very simple way to analyze what you do when you have multiple views.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess this is not new that you have more and more digital media.",
                    "label": 0
                },
                {
                    "sent": "You have videos, you have a lot of pictures both in the industry.",
                    "label": 0
                },
                {
                    "sent": "Also for your own on your own personal PC, have gigabytes of pictures.",
                    "label": 0
                },
                {
                    "sent": "If you have kids every kid is 1 gigabyte.",
                    "label": 0
                },
                {
                    "sent": "So I have three kids.",
                    "label": 0
                },
                {
                    "sent": "It makes finger bites.",
                    "label": 0
                },
                {
                    "sent": "And when I was trying to show you that although stocks also, you have many tasks to be to be solved for computer vision and they are all associated with the machine learning problem.",
                    "label": 1
                },
                {
                    "sent": "So if you can really find a good way of presenting images then you can apply by applying different, applying different machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "We could have solved.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different tasks.",
                    "label": 0
                },
                {
                    "sent": "So the 1st two times the first task is a one of image retrieval.",
                    "label": 1
                },
                {
                    "sent": "So you see the output of the Great London, so you can see you get pictures of London.",
                    "label": 0
                },
                {
                    "sent": "You get junior, the tube, you get some weird stuff over there.",
                    "label": 0
                },
                {
                    "sent": "So here when I need those queries Google is only looking at surrounding surrounding text, so they don't look at the email right now.",
                    "label": 0
                },
                {
                    "sent": "But at the time and here the goal you really need to look at the image to be able to output better to get a better output, or maybe classify the outputs.",
                    "label": 0
                },
                {
                    "sent": "As pictures or Maps and even worse.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you take Paris, similarly, you get the pictures you get the map.",
                    "label": 0
                },
                {
                    "sent": "You also get Paris Hilton.",
                    "label": 0
                },
                {
                    "sent": "In a sense, this is a true query.",
                    "label": 0
                },
                {
                    "sent": "I didn't invent it and so you really need to be able to look at the image to separate Paris or Paris Hilton.",
                    "label": 0
                },
                {
                    "sent": "So this is a two simple example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they weren't going back to kids.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have many pictures and usually they organized by by year and at the end it's very tough to look at them.",
                    "label": 0
                },
                {
                    "sent": "OK, because you have so many of them, so you would like to be able to do some clustering, maybe clustering by kids.",
                    "label": 0
                },
                {
                    "sent": "I will answer all the pictures of the kid one or the ones of key two.",
                    "label": 0
                },
                {
                    "sent": "You might do want to visualize.",
                    "label": 0
                },
                {
                    "sent": "We might want to do.",
                    "label": 0
                },
                {
                    "sent": "Detection of a new kid maybe?",
                    "label": 0
                },
                {
                    "sent": "Is it gonna do it so?",
                    "label": 0
                },
                {
                    "sent": "So anyway, you have lots of tasks and all of them can be framed as a machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "Classification, clustering, outlier detection.",
                    "label": 0
                },
                {
                    "sent": "Delete all those tasks of that complex.",
                    "label": 0
                },
                {
                    "sent": "OK, so and so massive so here we don't deal with an image.",
                    "label": 0
                },
                {
                    "sent": "We deal with.",
                    "label": 0
                },
                {
                    "sent": "One image is one data point, so it means that she is very very massive.",
                    "label": 0
                },
                {
                    "sent": "Is very you have a lot of heterogeneity.",
                    "label": 0
                },
                {
                    "sent": "Yeah, different media, sometimes even inside an image you have a lot of different lots of different sources.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK then typically you have different cues for for for images, so you can look at shape.",
                    "label": 0
                },
                {
                    "sent": "So I say you are extracted from some object and you want to capture as its shape is very important for recognition.",
                    "label": 0
                },
                {
                    "sent": "Color is also important.",
                    "label": 0
                },
                {
                    "sent": "Q also have texture which is a different queue.",
                    "label": 0
                },
                {
                    "sent": "You have image can be segmented into different crew.",
                    "label": 0
                },
                {
                    "sent": "You have interest points the way they are located inside the image.",
                    "label": 0
                },
                {
                    "sent": "So at the end we have a lot of different cues and what?",
                    "label": 1
                },
                {
                    "sent": "I claim that when you want to design A kernel, of course I will consider economy service in this talk if you want to design the kernel, it's a lot easier if you focus on a single single source at a time.",
                    "label": 0
                },
                {
                    "sent": "Designing a kernel which is good straight for both shape, color, texture and segment interest points is kind of difficult, but if you want to focus only on interest point, people have been doing that.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Play down, pull their, consider interest points.",
                    "label": 1
                },
                {
                    "sent": "So here we have too many nice line, so this is just a corner corner detector and people have used like Roman and RLS.",
                    "label": 0
                },
                {
                    "sent": "If we can both have used the pyramid match kernel was essentially divide the image into pieces and take an Instagram of the number histogram of.",
                    "label": 0
                },
                {
                    "sent": "Of 6 features as interest point, so this is easy to do and the good thing is that this is really for interest points.",
                    "label": 0
                },
                {
                    "sent": "Then if you want to kernel.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For texture, so you can, but people do you really say, take a bunch of features like Graduate or Gabor wavelets, and then you get get histogram.",
                    "label": 1
                },
                {
                    "sent": "Oh, that's great.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that it's easy to design for another.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Designing a kernel for a simple one Q and finally I will only work with later.",
                    "label": 0
                },
                {
                    "sent": "Actually is to look at segmented images with the assumption that usually segmentation does not work.",
                    "label": 0
                },
                {
                    "sent": "You will never get.",
                    "label": 0
                },
                {
                    "sent": "In two in the in two segments, one object in the middle segment and the background is the other segments always get things which are cut into pieces.",
                    "label": 0
                },
                {
                    "sent": "So if you look if you consider that you over segment the image that you make the assumption you make the assumption that you have multi objects will be cut into pieces, then no segment will spend more.",
                    "label": 0
                },
                {
                    "sent": "The object of interest in the background.",
                    "label": 1
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "You go from Los Square Grid, so the images we pixel to a small graph and we'll see how it goes for dozens or hundreds of region.",
                    "label": 1
                },
                {
                    "sent": "So how does it works very simply?",
                    "label": 0
                },
                {
                    "sent": "Thank you image.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compute the gradient.",
                    "label": 0
                },
                {
                    "sent": "Do a watershed transform from the team.",
                    "label": 1
                },
                {
                    "sent": "For me, called me in, and then you can reduce the number.",
                    "label": 0
                },
                {
                    "sent": "The number of segments and we know what we have done is under Colonel on those on those types of objects and they will represent these objects.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is biograf.",
                    "label": 0
                },
                {
                    "sent": "But this is the image.",
                    "label": 0
                },
                {
                    "sent": "This is a segmentation of the image.",
                    "label": 0
                },
                {
                    "sent": "So we assign to each have one vertex per region.",
                    "label": 0
                },
                {
                    "sent": "We connect, especially neighboring region and then we put a label on top of the region, which is just the histogram of pixels in that region.",
                    "label": 0
                },
                {
                    "sent": "So this will capture essentially transitions in color, so it's another way of looking at the specific Q.",
                    "label": 0
                },
                {
                    "sent": "For example, here we do not consider texture or interest points.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's it's a kernel on the single view if you wish.",
                    "label": 0
                },
                {
                    "sent": "But now we have design.",
                    "label": 0
                },
                {
                    "sent": "We have a graph so we can use a graph kernel technology to be able to get a graph on top of that.",
                    "label": 0
                },
                {
                    "sent": "And before I go on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I would like to mention the similarity with bearing semantics, so buying semantics actually the way I understand it as a lot of common problems where you have.",
                    "label": 0
                },
                {
                    "sent": "For a given task, you have a lot of different sources of information and an.",
                    "label": 0
                },
                {
                    "sent": "Essentially you are bound to use similar techniques on top of it, even for single view the queues are very similar.",
                    "label": 0
                },
                {
                    "sent": "You have queues based on 3D structure.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a statement vision and learning is quite amusing that there are a lot of exchanges between the two.",
                    "label": 0
                },
                {
                    "sent": "The two fields.",
                    "label": 0
                },
                {
                    "sent": "So before I go, no.",
                    "label": 0
                },
                {
                    "sent": "So you have multiple sources in computer vision.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The way I will combine them, it would be to consider so one kernel power source and may be considered community summing kernels as a way of combining combining sources, so this may be the most important part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Is that when you some kernels you actually concatenating features OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "And not very deep, but it seems a little be remember as soon as we added kernels is like taking the features for each kernel and concatenating them, and this is.",
                    "label": 0
                },
                {
                    "sent": "So when we sum over the M kernels, so we have one kernel power source.",
                    "label": 0
                },
                {
                    "sent": "Then it's like having a big picture vector.",
                    "label": 0
                },
                {
                    "sent": "When we look at an 8 or the fear of the future.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So no one other somewhat sometimes point which which is a bit ambiguous is that you have two views of this of this problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is to consider that you have a single input.",
                    "label": 1
                },
                {
                    "sent": "SpaceX and you have multiple kernels over that same input space.",
                    "label": 1
                },
                {
                    "sent": "This is usually available for images.",
                    "label": 0
                },
                {
                    "sent": "This is what I've done.",
                    "label": 0
                },
                {
                    "sent": "You take an image, you have one kernel for texture, one kernel, four segments for example.",
                    "label": 0
                },
                {
                    "sent": "But you can also, so this is the first view.",
                    "label": 1
                },
                {
                    "sent": "The second view is you may.",
                    "label": 0
                },
                {
                    "sent": "You may assume that you have M views which are different and for which you have a kernel on each view.",
                    "label": 0
                },
                {
                    "sent": "OK, so here there's only One X and the other is 1X1 expert source, so those are totally equivalent.",
                    "label": 0
                },
                {
                    "sent": "You can go back and forth from one to the other, but often people misled.",
                    "label": 0
                },
                {
                    "sent": "By that, so either you have one one input space and a lot of lot of kernels, or you have multiple.",
                    "label": 1
                },
                {
                    "sent": "You have multiple pairs of inputs, video space and of course, since I'm a bit messy in this talk about user two formulas formalism.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, what is multiple kernel learning, so we will consider.",
                    "label": 1
                },
                {
                    "sent": "Learning a weighted sum of kernels.",
                    "label": 0
                },
                {
                    "sent": "So as I told you a summing kernels is equivalent to.",
                    "label": 1
                },
                {
                    "sent": "Community features so now we call fidget with 15 map for Colonel Colonel J essentially where if you concatenate features you also your weight vector or your function.",
                    "label": 0
                },
                {
                    "sent": "We also we will also be a concatenation of F1 to FM.",
                    "label": 0
                },
                {
                    "sent": "So now and you predict as a sum of each of those model products so it ends up doing what you take an X, map it to all the official spaces dot product with your functions and you put it by the system.",
                    "label": 0
                },
                {
                    "sent": "So this is actually exactly.",
                    "label": 0
                },
                {
                    "sent": "What I mean by concatenating features, not the many question is how you regularize on the on the set of how we penalize F1 to FM.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who used a CRM feature map?",
                    "label": 0
                },
                {
                    "sent": "That's the minimize with respect to the sun.",
                    "label": 1
                },
                {
                    "sent": "So first thing is that if I use the sum of the square norm of AJ is exactly the norm.",
                    "label": 0
                },
                {
                    "sent": "The square norm of recognition for it means that is essentially doing the regular kernel machine.",
                    "label": 0
                },
                {
                    "sent": "With that with that feature vector.",
                    "label": 0
                },
                {
                    "sent": "And since the kernel is son of kernels for that representation at the end.",
                    "label": 0
                },
                {
                    "sent": "Sum of squared norm is equivalent to summing kernels.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a good thing to do.",
                    "label": 0
                },
                {
                    "sent": "Often we just catenate features.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if I change.",
                    "label": 0
                },
                {
                    "sent": "The way we do arise if I just remove the square so it looks like a minimal minimum modification that makes a big difference and see the same difference as going from L2 to L1.",
                    "label": 0
                },
                {
                    "sent": "So I assume the sun I serve the norms of each.",
                    "label": 0
                },
                {
                    "sent": "Weight vector OK, so here, as I guess you all know that when you penalized by the L1 norm, which is the sum of the absolute values, you get lots of zeros.",
                    "label": 0
                },
                {
                    "sent": "Here if you penalize the son of the North of AJ, you should get a lot of FJ which which are put to zero.",
                    "label": 0
                },
                {
                    "sent": "OK altogether, we don't make any assumptions on what's happening inside FJ.",
                    "label": 0
                },
                {
                    "sent": "We want object would be equal to 0 exactly or a or if it is not zero.",
                    "label": 0
                },
                {
                    "sent": "However, however you want OK.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "And how many questions when you use that?",
                    "label": 1
                },
                {
                    "sent": "So I could at the block L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So first I will show you it is equivalent to a previous way of learning the kernel.",
                    "label": 1
                },
                {
                    "sent": "Let's put in some algorithms and then present how you can analyze the way you induce positive with all those L1.",
                    "label": 0
                },
                {
                    "sent": "Technique was mostly justifiable heuristics.",
                    "label": 0
                },
                {
                    "sent": "You know that you should get some zeros, so you hope that you will be able to do both regularization and model selection essentially.",
                    "label": 0
                },
                {
                    "sent": "Try to say to study if this is true or not.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it relate to the general kernel learning framework started by laundry, yet no clear title.",
                    "label": 1
                },
                {
                    "sent": "So here you assume that this is a regular supervised learning machine where you have a son of a data fitting term over there and squared norm regularization.",
                    "label": 0
                },
                {
                    "sent": "OK, so usually thing you can.",
                    "label": 0
                },
                {
                    "sent": "Find the convex dual that problem and you get the Max with respect to Alpha of that thing.",
                    "label": 0
                },
                {
                    "sent": "So for those familiar with the SVM, SVM, return in another way.",
                    "label": 0
                },
                {
                    "sent": "What's important here is that at the end, if you take the optimal value after we have optimized for the best F of the best Alpha, you get maximum over Alpha of a linear function of K. So as a maximum of linear functions this is a convex function of K, and this is essentially what great liquid and colleagues have done is that since you have a convex function of K, just minimize it or a convex set and they have proved that this leads to a theoretical learning bounds, which were, I think, improved by Scrabble.",
                    "label": 1
                },
                {
                    "sent": "David later, so this is the usual, so this is not restricted to kernels which are sums of.",
                    "label": 0
                },
                {
                    "sent": "This is this is not restricted to kernels, just some of the finite number of kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is more general and this has either publications to multitask learning.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how's equivalent?",
                    "label": 0
                },
                {
                    "sent": "So this is the kernel in formulation as a girl has got liquid.",
                    "label": 0
                },
                {
                    "sent": "When you maximize optimal, you minimize the optimal value of the SVM, and this is our formulation.",
                    "label": 1
                },
                {
                    "sent": "When we use the same the same loss, but we penalize, but there's some of the norms, so putting a square here is it won't change the properties of the estimator, but one Lambda without the square there is a number with the square OK, but this makes these two problems formally equivalent.",
                    "label": 0
                },
                {
                    "sent": "If you solve that one, you exactly have a solution of that one, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So here this is a.",
                    "label": 1
                },
                {
                    "sent": "This is an equivalence of multiple kernel learning by multiple block L1 norms and multiple kerning.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning by minimizing with respect to the kernel weights the full the full SVM cost function.",
                    "label": 1
                },
                {
                    "sent": "So here at the end, here the eight as they might, it's not clear where they come from, but they come from LaGrange multipliers, so you can look at the papers to see how you can get them from at the end.",
                    "label": 1
                },
                {
                    "sent": "That's what's nice, is that.",
                    "label": 0
                },
                {
                    "sent": "In a single optimization problem, you can learn both the weights of your kernel and the classifier itself with those weights.",
                    "label": 0
                },
                {
                    "sent": "So it's a double.",
                    "label": 0
                },
                {
                    "sent": "We have two things for the price.",
                    "label": 0
                },
                {
                    "sent": "For the price of 1.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But uh, algorithms quickly, so this was first cast as ASDP or QCQP.",
                    "label": 0
                },
                {
                    "sent": "So quality programming with quadratic constraints.",
                    "label": 0
                },
                {
                    "sent": "So for that you can use 2 boxes, But this takes awhile because here you have to remember that we have to pile up kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have 100 kernels on the 1000 data points, which is very small.",
                    "label": 0
                },
                {
                    "sent": "Already some problems too thick to fit that into memory, so all those techniques, which is which are based on 2 boxes won't really work OK, and I think one important point is that.",
                    "label": 0
                },
                {
                    "sent": "Following about two in Bousquet is not useful to really optimize to machine precision.",
                    "label": 0
                },
                {
                    "sent": "You can only optimize to over very low precision, and if you want to do that first order methods that only look at the gradient will be a largely enough, whereas generator boxes.",
                    "label": 0
                },
                {
                    "sent": "These are usually 2nd order methods which are well adapted to get you exactly the solution at a high cost.",
                    "label": 0
                },
                {
                    "sent": "Here you get you want to get.",
                    "label": 0
                },
                {
                    "sent": "So much close to the solution at the Locust.",
                    "label": 0
                },
                {
                    "sent": "So all those methods are essentially first of the methods to speed up the process.",
                    "label": 0
                },
                {
                    "sent": "So our first wife was really using his.",
                    "label": 0
                },
                {
                    "sent": "Anyone known formulations to rederive nieuwsma algorithms?",
                    "label": 0
                },
                {
                    "sent": "But this needs this needed to record the SVM and this is not an easy thing to do if you want to be scalable in North.",
                    "label": 0
                },
                {
                    "sent": "And other people have no design algorithms to go even to optimize the function of care directly, either by a cutting plane so bad people dissent and also additional work at this conference at the main conference.",
                    "label": 0
                },
                {
                    "sent": "So here I think right now you can go up to.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "We say 50,000 that are points and let's say 100 kernels.",
                    "label": 0
                },
                {
                    "sent": "I think maybe maybe doable with those techniques.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially.",
                    "label": 0
                },
                {
                    "sent": "It has a.",
                    "label": 0
                },
                {
                    "sent": "It ends up doing like a somewhat a lot of SVM training, so if you can.",
                    "label": 0
                },
                {
                    "sent": "If your problem is more enough so that the SVM can still be run OK, then you have to be.",
                    "label": 0
                },
                {
                    "sent": "You can run that, let's say 100 times and you will get the estimate of the MK of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now they are other important slides.",
                    "label": 0
                },
                {
                    "sent": "Should you some kernel, so should you optimize the weights so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is very important in practice, So what I've told you is that if when you realize by the sum of the square norms, so you assume that you have one kernel or source, and then you have one FJ Purnell.",
                    "label": 0
                },
                {
                    "sent": "So if you sum the squared norms, it's equivalent to assuming the kernels.",
                    "label": 1
                },
                {
                    "sent": "So obviously when you want to sum the kernels, the complexity is simple, just one SVM problem.",
                    "label": 0
                },
                {
                    "sent": "Here, if you regularize by the sun, the non squared norms, then you have to use one of the algorithms which are shown the above the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And of course it's lower.",
                    "label": 0
                },
                {
                    "sent": "So there's a cost from going to.",
                    "label": 0
                },
                {
                    "sent": "A fixed sum of kernels.",
                    "label": 0
                },
                {
                    "sent": "Going from a fixed sum of kernels to a weighted sum of kernels and with a weighted sum with many many zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, so for me to take a message is really that since we have defined the multiple kernel learning as using L1 norm and that album GNOME especially suited for sparse problem.",
                    "label": 0
                },
                {
                    "sent": "So here if you expect your problem to have only a small number of kernels at the end.",
                    "label": 0
                },
                {
                    "sent": "So if you expect sparsity that the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Should really help.",
                    "label": 1
                },
                {
                    "sent": "On the contrary, if you if we expect all your kernels to be to be useful for your problem, then summing them is by far the best.",
                    "label": 0
                },
                {
                    "sent": "The best thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK so for non sparse problem, anyone know might not beat L2 norms.",
                    "label": 0
                },
                {
                    "sent": "And even worse it doesn't.",
                    "label": 0
                },
                {
                    "sent": "But it can be.",
                    "label": 1
                },
                {
                    "sent": "It can be performed worse and at the end is less efficient in terms of computation.",
                    "label": 0
                },
                {
                    "sent": "So you get everything button.",
                    "label": 0
                },
                {
                    "sent": "So really it's a matter of matter of expected Spa City.",
                    "label": 0
                },
                {
                    "sent": "If you expect Spa City at 1:00 or MK is a. I think the thing to do, but if you don't submit the kernel is really is really as a really simple.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's take 2 examples on computer vision, where we have applied multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "And the first one is simply on this database of coral 14 or 14 we have.",
                    "label": 0
                },
                {
                    "sent": "1400 natural images with 14 classes here.",
                    "label": 1
                },
                {
                    "sent": "The goal was to essentially learn the hyperparameters of kernels.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have design with the data, shall we?",
                    "label": 0
                },
                {
                    "sent": "Which would essentially graph kernels on segmented images.",
                    "label": 0
                },
                {
                    "sent": "So here we compare.",
                    "label": 0
                },
                {
                    "sent": "So this is histogram, which is the simplest approach without problem and those are three different types of graph kernels, so you can see the best performance for each of the types of kernel.",
                    "label": 0
                },
                {
                    "sent": "And you combine them by multiple kernel learning, so you do get an improvement, which is significant.",
                    "label": 0
                },
                {
                    "sent": "So here two things important here is that the type.",
                    "label": 0
                },
                {
                    "sent": "Often OK, when you when you when you do.",
                    "label": 0
                },
                {
                    "sent": "When you fuse multiple sources, sometimes one source is really better than the other ones.",
                    "label": 0
                },
                {
                    "sent": "So in many cases when you want to learn the spice sparse.",
                    "label": 0
                },
                {
                    "sent": "As possible combination you get 110 and all the rest 0.",
                    "label": 0
                },
                {
                    "sent": "So to me it is still a successful application in the sense that you just had one kernel which was a lot better and the the algorithm just selected.",
                    "label": 0
                },
                {
                    "sent": "It is very often the case.",
                    "label": 0
                },
                {
                    "sent": "I'll just get just take the best one.",
                    "label": 0
                },
                {
                    "sent": "In many cases that are Fusion is not clear.",
                    "label": 0
                },
                {
                    "sent": "Working in the sense that you expect to get better performance by combining stuff, but sometimes it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So he does, but not by far, by a big margin.",
                    "label": 0
                },
                {
                    "sent": "OK so second thing is that here we haven't tried yet, something the kernels OKC we should have done it and it is not done yet.",
                    "label": 0
                },
                {
                    "sent": "An here since we had like 200 kernels I expect that you would do really a bad thing.",
                    "label": 0
                },
                {
                    "sent": "OK. Also another point is that when you have many kernels.",
                    "label": 0
                },
                {
                    "sent": "So yeah, when you have 10 kernels, swimming or taking, finding the weights is not solution.",
                    "label": 0
                },
                {
                    "sent": "And then when you have like 200 or 300 then it starts to be important to get this past combination.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But a better example is by other people than me, which is on the running similar techniques on the contact .1 database.",
                    "label": 0
                },
                {
                    "sent": "So this is this was like 2 years ago the standard benchmark in computer vision for object recognition.",
                    "label": 0
                },
                {
                    "sent": "Or you have 101 images like that and the goal of course is to do multi class.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Specification.",
                    "label": 0
                },
                {
                    "sent": "So we have many, in array.",
                    "label": 0
                },
                {
                    "sent": "So what they simply did is to consider different kernels.",
                    "label": 0
                },
                {
                    "sent": "So here I don't exactly know what they do, but I think this one is in shape.",
                    "label": 0
                },
                {
                    "sent": "But it is said this one is simply an integral of gradients which I presented.",
                    "label": 0
                },
                {
                    "sent": "Here and this one are based on color, but I don't have, but I forgot what it meant and for all those kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so it tries several things.",
                    "label": 0
                },
                {
                    "sent": "First they try first one nearest neighbor with that kernel, so I think this is very nice that people Julia design kernels to run an SVM.",
                    "label": 0
                },
                {
                    "sent": "But once you have a kernel we have a metric.",
                    "label": 0
                },
                {
                    "sent": "So if you have a metric you can do nearest neighbor and often it does not perverse you would expect it performs very bad, but here it's not as good as running the SVM based, not.",
                    "label": 0
                },
                {
                    "sent": "Not really bad, so I think it's quite interesting to see that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes simple methods are actually a competitive, so they try them one versus one and one versus rest SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is 411 source of information, so one view if you wish.",
                    "label": 0
                },
                {
                    "sent": "Well, they have lied.",
                    "label": 0
                },
                {
                    "sent": "Our code and Badgers running this ad 1L1 block where L1 norm regularization they can.",
                    "label": 0
                },
                {
                    "sent": "Go former at most 60% to more than 75% just by compiling kernels.",
                    "label": 0
                },
                {
                    "sent": "So here someone this morning told me that when there's some kernels it doesn't doesn't work as well.",
                    "label": 0
                },
                {
                    "sent": "Maybe 3% less is OK, so summing works decently, but not just not as good as a learning the weights.",
                    "label": 0
                },
                {
                    "sent": "OK, they even improved it.",
                    "label": 0
                },
                {
                    "sent": "And I kind of forgot what tricks are played, but they managed to even improve the performance over the regular regular MCL blockade.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's no.",
                    "label": 0
                },
                {
                    "sent": "Let's not try to analyze.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing.",
                    "label": 0
                },
                {
                    "sent": "So here we have just returned the same problem for the square loss for from now on they will consider the square loss, which is simpler to analyze.",
                    "label": 0
                },
                {
                    "sent": "So here I want to predict why I OK given the XIJ.",
                    "label": 0
                },
                {
                    "sent": "So here assume that the hive X is decomposing 2M, different exponent, two XJ.",
                    "label": 0
                },
                {
                    "sent": "XJ is simply the.",
                    "label": 0
                },
                {
                    "sent": "X Ji is simply is the ice observation of the Jays viable?",
                    "label": 0
                },
                {
                    "sent": "OK, so as I told you, when you have multiple kernel it's like having using a sum for prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is my prediction for the for the data, for the observation and here are regularised by the square norm.",
                    "label": 0
                },
                {
                    "sent": "But there is a sub square of those are some of the norms.",
                    "label": 0
                },
                {
                    "sent": "So here is exactly what people call in statistics generalized that is modeled by the son of functions of each of each viable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is nice work in similar settings by radical metal for the sparse additive models, so it is a very important thing, and I think this is true for all learnings, all methods which are doing nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Estimation for multiple views, or even a single view for the algorithm, we use the Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so if F is a bit Hilbert space, let's say server space F as very high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So the only way you can estimate F is by using the presenter theorem or essentially.",
                    "label": 0
                },
                {
                    "sent": "Finally, the convex do all and learning the famous alphas for the SVM.",
                    "label": 0
                },
                {
                    "sent": "For example for learning for estimation you need Alpha.",
                    "label": 0
                },
                {
                    "sent": "For analysis, Alpha is really really really nasty to use for analysis.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you can do, it is better to be able to use Alpha.",
                    "label": 0
                },
                {
                    "sent": "What else you can still stay in the input space?",
                    "label": 1
                },
                {
                    "sent": "You have tools to stay the input space and to do as if you were in a in a finite dimension so they see exactly what they're going to present.",
                    "label": 0
                },
                {
                    "sent": "That algorithms go for Alpha and finally sees it.",
                    "label": 1
                },
                {
                    "sent": "It's a lot easier to never, is never use Alpha, just use covariance operator.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are those?",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a single random variable X, so Sigma XX will be a linear operator from the.",
                    "label": 1
                },
                {
                    "sent": "From the from, the Hilbert space is a Hilbert space and CPD find through products by F&G.",
                    "label": 0
                },
                {
                    "sent": "So if you take any functions in your Hilbert space F times Sigma X X * G is just the expectation of the product FG.",
                    "label": 1
                },
                {
                    "sent": "So here are considered non centered covariance operators.",
                    "label": 0
                },
                {
                    "sent": "But you could also consider centered one.",
                    "label": 0
                },
                {
                    "sent": "When you remove.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the covariance between FX and GX and remove the product of the expectations for here.",
                    "label": 0
                },
                {
                    "sent": "You don't really.",
                    "label": 0
                },
                {
                    "sent": "You don't really need it, and you can simply so consider this operator, so here.",
                    "label": 0
                },
                {
                    "sent": "You have to consider a bit of functional functional analysis so that is for you.",
                    "label": 0
                },
                {
                    "sent": "If you don't know anything about Hilbert spaces, Sigma think of it as being just covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Fix.",
                    "label": 0
                },
                {
                    "sent": "So it's symmetric, so auto auto agent in the functional analysis space non negative like appearance like a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And Hilbert Schmidt for those we know what it means and really well.",
                    "label": 0
                },
                {
                    "sent": "But it's nice is that this operator can be really estimated in a very simple way.",
                    "label": 0
                },
                {
                    "sent": "Why how?",
                    "label": 0
                },
                {
                    "sent": "But take FG?",
                    "label": 0
                },
                {
                    "sent": "And so instead of taking the exact expectation with respect to X, just take the empirical expectation.",
                    "label": 0
                },
                {
                    "sent": "So you replace the expectation here, but by an average as a cool thing.",
                    "label": 0
                },
                {
                    "sent": "Is that Sigma Sigma hat you estimate converges to see my XX in the Hilbert space norm.",
                    "label": 0
                },
                {
                    "sent": "So essentially you can use the same trick.",
                    "label": 0
                },
                {
                    "sent": "It's like infinite dimension.",
                    "label": 0
                },
                {
                    "sent": "You can estimate your confidence metrics really in the simple way.",
                    "label": 0
                },
                {
                    "sent": "OK, so really it's it's very natural way of doing that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very simple dinner.",
                    "label": 0
                },
                {
                    "sent": "So if you have more than one view and that he sees the watch interest, people in this workshop, so you have multiple sources, so you want to consider the link between two sources and in the finite dimensional setting you will consider the cost covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And here we just consider the cross covariance operator.",
                    "label": 1
                },
                {
                    "sent": "Would you consider a function of viable XI in the function of variable XJ and you just define it like like an expectation of the product, which can be estimated also with the empirical averages.",
                    "label": 0
                },
                {
                    "sent": "And now you can define a big John covariance operator to analyze your problem.",
                    "label": 1
                },
                {
                    "sent": "Because we find correlations, but this is not so so important.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why I think it is a simple tool for analyzing like relationship between sources that formally used a input space.",
                    "label": 1
                },
                {
                    "sent": "Everything is written in terms F covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "And you never need to go to the alphas, so you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to be a bit more careful, of course, because you have to prove some.",
                    "label": 0
                },
                {
                    "sent": "Emergencies for the functional.",
                    "label": 0
                },
                {
                    "sent": "For the functional case.",
                    "label": 0
                },
                {
                    "sent": "But it provides a lot of it's really a good tool for that, and you can do a lot more than just studying supervised settings.",
                    "label": 0
                },
                {
                    "sent": "You can do converse easier, or you can consider studying the behavior of nonparametric test statistics work I've been doing with the they'd air clean.",
                    "label": 0
                },
                {
                    "sent": "And really, if you really want to.",
                    "label": 0
                },
                {
                    "sent": "To be able to analyze all multiple multiple view problems, really, you should get those tools going on.",
                    "label": 0
                },
                {
                    "sent": "And if you want to hire a good recently graduating PhD student and they actually whatever there is an expert in those settings.",
                    "label": 0
                },
                {
                    "sent": "So I welcome everybody to make a PhD postdoc offer to him.",
                    "label": 0
                },
                {
                    "sent": "I promise I would do it so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it work in our setting?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here we assume, so we assume that we the data were generated by a linear model, so we assume that we actually have a generated general generalized additive model is Y is a sum of functions of individual variables, and I have some some notes and double it.",
                    "label": 1
                },
                {
                    "sent": "So on top of technical assumptions here.",
                    "label": 0
                },
                {
                    "sent": "You need one assumption, which is a one of capacity of the correlation operator.",
                    "label": 0
                },
                {
                    "sent": "So it might seems a bit abstract, but it has some nice meaning in our setting.",
                    "label": 0
                },
                {
                    "sent": "As a nice man.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you get a capacity.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if this quantity is finite, so this quantity is just P. XI XJ, simply the marginal distribution of excise, J marginal density, and pure XI is exactly the marginal density over XI as he sees is very similar to the mutual information between XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "So what this assumption showed is that.",
                    "label": 0
                },
                {
                    "sent": "OK for my phone, for my analysts to hold unit's dependence, because variables, this dependence is not too strong.",
                    "label": 1
                },
                {
                    "sent": "OK, so for me this is quite a problem when you learn from multiple sources.",
                    "label": 0
                },
                {
                    "sent": "Do you want sources to be similar or be very very dissimilar?",
                    "label": 0
                },
                {
                    "sent": "So here at least in my set, under my seat assumptions, I need the sources to be.",
                    "label": 0
                },
                {
                    "sent": "Enough dissimilar because if there are two similar then we get high dependence and this becomes infinite and I cannot apply my my theorem, so of course it may be a shortcoming of my analysis, but still seriously it is interesting that by just looking at the appropriate conditions for for your covariance operator you get back.",
                    "label": 0
                },
                {
                    "sent": "Condition, which is somewhat interesting that you should have a finite mutual information between the XI and XJ.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the Nokia analysis?",
                    "label": 0
                },
                {
                    "sent": "So here you might want to look at two things.",
                    "label": 0
                },
                {
                    "sent": "So when you look at sparse problem, you both.",
                    "label": 0
                },
                {
                    "sent": "You both want to be consistent.",
                    "label": 0
                },
                {
                    "sent": "You want to recover to recover your functions that the true function that have generated your data but also seems to work with path methods you really want to estimate the good ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have people have been promising everybody that L1 norm can do both estimation and model selection.",
                    "label": 0
                },
                {
                    "sent": "And for the last few years people have tried to study if this were the case for the lasso, which is just using blocks of size 1.",
                    "label": 1
                },
                {
                    "sent": "And this is just an extension for the group lasso.",
                    "label": 1
                },
                {
                    "sent": "Essentially, if this condition is satisfied, then you get you get consistent model estimation.",
                    "label": 0
                },
                {
                    "sent": "You get a good sparsity pattern at the end, and this is not satisfied.",
                    "label": 0
                },
                {
                    "sent": "Then you don't get the sparsity.",
                    "label": 0
                },
                {
                    "sent": "The good sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So to pass this it's.",
                    "label": 0
                },
                {
                    "sent": "It's Even so to two parties.",
                    "label": 0
                },
                {
                    "sent": "It's a bit complicated if you're not used to it, but the most important point is this one.",
                    "label": 0
                },
                {
                    "sent": "OK, focus on that one.",
                    "label": 0
                },
                {
                    "sent": "That one is just a correlation between XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "This says that if you have local relations then you're consistent, formal selection, and if you have high correlations, you're not consistent for model selection.",
                    "label": 0
                },
                {
                    "sent": "So to me this was a very.",
                    "label": 0
                },
                {
                    "sent": "Disappointing result that L1 does not actually always work.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it does, sometimes it does not.",
                    "label": 0
                },
                {
                    "sent": "So this has led to a lot of work trying to fix that either through a two step procedure or through.",
                    "label": 0
                },
                {
                    "sent": "Pre sampling, so this is really.",
                    "label": 1
                },
                {
                    "sent": "To me, very disappointing that the lasso does not always work, so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "I presented the multiple kernel learning for supervised learning.",
                    "label": 1
                },
                {
                    "sent": "OK, so I guess this is 1 versions, 1 version of multiple sources, so I've tried to convince you that this is important for computer vision, where naturally you can express.",
                    "label": 0
                },
                {
                    "sent": "Different types of information for your images and also I do believe that in Bertha Matic, this is important as well, but Bill Noble had a very sad talk for me this morning saying that in Belgium, attics in other cases it's right, it's better to serve the kernels and selecting the kernels by convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So I still believe that this is because it's too good.",
                    "label": 0
                },
                {
                    "sent": "It is kernel that is designing so good that he knows in advance that they should enter the combination so he doesn't expect positive.",
                    "label": 0
                },
                {
                    "sent": "So if you don't expect sparsity, and two is better to do, an L2 corresponds to assuming the kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is my interpretation.",
                    "label": 0
                },
                {
                    "sent": "So so important or two ways of equivalent formulation for the multiple Quillen kernel learning framework?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the first one where you optimize first with respect to the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Then we expect the kernel, which is the usual one, but to me the one for which you can really get the most.",
                    "label": 0
                },
                {
                    "sent": "Bang for the book is really the L1 interpretation.",
                    "label": 0
                },
                {
                    "sent": "As soon as you go through the air.",
                    "label": 0
                },
                {
                    "sent": "One interpretation you are one way to compare it easily to submit the kernels like a one or two and people have been doing a lot of those L1 and two comparisons in recent years.",
                    "label": 0
                },
                {
                    "sent": "But I think there's a two important problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is that I think it's linked to.",
                    "label": 0
                },
                {
                    "sent": "Recent advances in the theory of the of the lasso solasso is simply when all the kernels are rank one.",
                    "label": 0
                },
                {
                    "sent": "OK, so the groups of size 1 and people have shown here that so if P is the number of variables OK, you can have P can be almost exponentially in NN being the number of data points.",
                    "label": 0
                },
                {
                    "sent": "So it means that you can have a lot a lot of irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you sell one, you can still be.",
                    "label": 0
                },
                {
                    "sent": "You can still estimate at a good estimation of your problem.",
                    "label": 0
                },
                {
                    "sent": "Here I'll say is 10,000.",
                    "label": 0
                },
                {
                    "sent": "Which is not not so uncommon in machine learning.",
                    "label": 0
                },
                {
                    "sent": "They say the Theoretischen says that he can be almost as big as the exponential 10,000.",
                    "label": 0
                },
                {
                    "sent": "OK, great.",
                    "label": 0
                },
                {
                    "sent": "So how do you write an algorithm on that many features?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of difficult and even worse, how can you even imagine that many features?",
                    "label": 0
                },
                {
                    "sent": "So to me, the only way to even think about that many features if when those features come in a recursive way, OK, you cannot think you need a program to be able to describe those features that cannot be enumerated one by one, so I think.",
                    "label": 0
                },
                {
                    "sent": "Is all setting let's P is very large.",
                    "label": 0
                },
                {
                    "sent": "There's no way you can.",
                    "label": 0
                },
                {
                    "sent": "You can do the practice without structure and I work at the main conference room to see.",
                    "label": 0
                },
                {
                    "sent": "How you can really use that structure to be able to run lasso type of things when P is very large and by the piece very large, there's not so many ways of.",
                    "label": 0
                },
                {
                    "sent": "Of doing this right now and we're using structure is quite important and I encourage you to look at that if you're interested in the P being very, very large.",
                    "label": 0
                },
                {
                    "sent": "Finally, I think this is really bothering me is when the design kernels OK for further problem should consider to try to look at a lot of kernels which are all very similar.",
                    "label": 0
                },
                {
                    "sent": "Or should we try to design kernels which are as different as possible?",
                    "label": 0
                },
                {
                    "sent": "So if they are quite similar, you have some sharing information, and if they are not, you have no sharing.",
                    "label": 0
                },
                {
                    "sent": "And to me this is really really hard for the people that try to design sources, but you do control design.",
                    "label": 0
                },
                {
                    "sent": "You're designing your own source of information and it is really tough and I have no, I don't know any clear answer to that problem.",
                    "label": 0
                },
                {
                    "sent": "If you do try to make things different or similar, my analysis shows that.",
                    "label": 0
                },
                {
                    "sent": "If you want them to be true, that should be different, but this is only a theorem.",
                    "label": 0
                },
                {
                    "sent": "And finally, I think this is also very important and often completely overlooked.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people say I use multiple sources, culture, mean whatever.",
                    "label": 0
                },
                {
                    "sent": "But is it is it?",
                    "label": 0
                },
                {
                    "sent": "Is it actually worth it?",
                    "label": 0
                },
                {
                    "sent": "And under what conditions exactly does it work better?",
                    "label": 0
                },
                {
                    "sent": "So I've been trying to do that.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear yet and.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear for the supervised case, but I guess it's also not simply for the unsupervised case that people have been talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yep, Yep.",
                    "label": 1
                },
                {
                    "sent": "With only respect to the group.",
                    "label": 0
                },
                {
                    "sent": "In general, does it breakdown so it's you mean the one the one time constraints, so it breaks down?",
                    "label": 0
                },
                {
                    "sent": "I think you mean for all losses.",
                    "label": 0
                },
                {
                    "sent": "For all sizes yet we it will breakdown at one point if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have high correlations it you won't get the good, but that's not so bad in the sense it's not because you have the wrong pattern that you do wrong estimation, so we have three types of results here.",
                    "label": 0
                },
                {
                    "sent": "You have the regular model consistency, which is the strongest if you're the correct model, you get everything right.",
                    "label": 0
                },
                {
                    "sent": "We have the regular consistency, so you might have the wrong pattern we use today.",
                    "label": 0
                },
                {
                    "sent": "Estimate things correctly, and what often people are interesting interested.",
                    "label": 0
                },
                {
                    "sent": "About is just efficiencies.",
                    "label": 0
                },
                {
                    "sent": "The terms that you want to predict.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't really care if you go close to the true F, as long as you put it correctly, so this is what this is really.",
                    "label": 0
                },
                {
                    "sent": "What people do in learning.",
                    "label": 0
                },
                {
                    "sent": "But here in this setting people have gone back to try to consider regular consistency, an model consistency.",
                    "label": 0
                },
                {
                    "sent": "So in terms of efficiency, still works well in terms of consistency, doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Also, a large number of people.",
                    "label": 0
                },
                {
                    "sent": "Crosses sometimes.",
                    "label": 0
                },
                {
                    "sent": "Very complicated.",
                    "label": 0
                },
                {
                    "sent": "So you should never.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CQB short shorts or here for the QCQP is very hard to go beyond, like more than 500 data points and 10 kernels OK, but for all of those you can go up to a few 100 kernels an as as many data points that you algorithm can handle so I don't know exactly that noise.",
                    "label": 0
                },
                {
                    "sent": "50 or.",
                    "label": 0
                },
                {
                    "sent": "500 heavenwood yeah.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "But using what you P?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I totally agree with that.",
                    "label": 0
                },
                {
                    "sent": "Then you should use like all the other words that people have been having been doing to really.",
                    "label": 0
                },
                {
                    "sent": "Be able to reuse existing code here.",
                    "label": 0
                },
                {
                    "sent": "The goal is your bit lazy mother be lazy or efficient, so efficient use of people do and if you can read those methods, allows to reuse existing SVM code.",
                    "label": 0
                },
                {
                    "sent": "So if you like SVM, libusb, name.",
                    "label": 0
                },
                {
                    "sent": "If you have simple SVM using simple SVM, Ann, those methods we just iterate and generate kernels and have the SVM liberal simple SVM solve for the kernel.",
                    "label": 0
                },
                {
                    "sent": "But then you can really go a lot higher than 500 points, but true if you if you do that, you're bound to fail.",
                    "label": 0
                },
                {
                    "sent": "They have them.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Which was posted, yeah?",
                    "label": 0
                },
                {
                    "sent": "Have you thought of but actually?",
                    "label": 0
                },
                {
                    "sent": "But something really playing problems and any problems that may be causing cancer.",
                    "label": 0
                },
                {
                    "sent": "Different properties of short, so very good.",
                    "label": 0
                },
                {
                    "sent": "Very good as he market.",
                    "label": 0
                },
                {
                    "sent": "Basically the most simple set covers possibly yeah.",
                    "label": 0
                },
                {
                    "sent": "Capture information about the data is structured data.",
                    "label": 0
                },
                {
                    "sent": "I think they're similar to a combination of colors.",
                    "label": 0
                },
                {
                    "sent": "You you mean like pyramid match, kernel or?",
                    "label": 0
                },
                {
                    "sent": "This I asked for the second question should have been compared to end of kernels, but we actually so send me if you send me your.",
                    "label": 0
                },
                {
                    "sent": "Is it implemented?",
                    "label": 0
                },
                {
                    "sent": "Or not, if it is implemented, we can add it to our comparison, but otherwise maybe complicated than the for the first question.",
                    "label": 0
                },
                {
                    "sent": "What was it already?",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I think that's very important.",
                    "label": 0
                },
                {
                    "sent": "In fact, many people criticize that work because they will say what you really want is to learn the parameters of the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, I agree with them, in a sense, what would we like it to be able to put a different width on each viable build?",
                    "label": 0
                },
                {
                    "sent": "A big gas in kernel?",
                    "label": 0
                },
                {
                    "sent": "Maybe being able to learn those widths by like the Gaussian process guys do?",
                    "label": 0
                },
                {
                    "sent": "But here you, by allowing only sums you keep comes convexity.",
                    "label": 0
                },
                {
                    "sent": "And I do believe that this convexity helps you so much that it's OK to not doing that.",
                    "label": 0
                },
                {
                    "sent": "Not doing what you would think originally.",
                    "label": 0
                },
                {
                    "sent": "And yes, we are able to know consider products of kernels.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what I've been doing for this year at NIPS, which is 2.",
                    "label": 0
                },
                {
                    "sent": "Consider.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modeling all the sons of kernels.",
                    "label": 0
                },
                {
                    "sent": "But all the products.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give me give me.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two kernels and consider all the possible products of the kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have to secure those and what I've been doing trying to exactly learn sparse combination of those two to the Q kernels.",
                    "label": 0
                },
                {
                    "sent": "Here you get other products.",
                    "label": 0
                },
                {
                    "sent": "This is the way of doing it, but we did being able to optimize parameters an.",
                    "label": 0
                },
                {
                    "sent": "Trying to optimize the function which is not linear in the parameters is really.",
                    "label": 0
                },
                {
                    "sent": "I think it's very difficult.",
                    "label": 0
                },
                {
                    "sent": "Especially in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So here the goal is to for this nonlinear variable selection problem to go to queue up to a few thousand.",
                    "label": 0
                },
                {
                    "sent": "OK, so here, connectivity is important when you have a few thousand 2000 variables.",
                    "label": 0
                },
                {
                    "sent": "Connectivity is really a decent thing to have.",
                    "label": 0
                },
                {
                    "sent": "Training multiple kernels like this you end up with the difference or factors for each part.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "At the end you get 118 are OK and know the solution.",
                    "label": 0
                },
                {
                    "sent": "You are fast correspond exactly to the solution of the regular SVM.",
                    "label": 0
                },
                {
                    "sent": "With that combination of kernels.",
                    "label": 0
                },
                {
                    "sent": "So you get one set of support vectors.",
                    "label": 0
                },
                {
                    "sent": "One A1 enter.",
                    "label": 0
                }
            ]
        }
    }
}