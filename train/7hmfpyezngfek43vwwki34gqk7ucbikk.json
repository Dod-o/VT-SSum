{
    "id": "7hmfpyezngfek43vwwki34gqk7ucbikk",
    "title": "Randomized partition trees for exact nearest neighbor search",
    "info": {
        "author": [
            "Sanjoy Dasgupta, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_dasgupta_trees/",
    "segmentation": [
        [
            "I'll be talking about fast nearest neighbor search OK answer."
        ],
        [
            "So, so here's the situation.",
            "You have a set of N data points OK, and subsequently you're going to be asked all sorts of queries, and for each query Q what you need to do is to return the closest of those endpoints.",
            "OK, now the brute force way to do this is that when you get a query, you look through all the end points and you find the one that's closest and you return that one.",
            "But that takes time order N, which is sort of disastrous in very large datasets.",
            "So we'd like to be able to do this faster.",
            "And so when you get this initial set of end points, you're allowed to take a little bit of time to build up a data structure that will hopefully help you later on.",
            "OK, So what are the requirements?",
            "So the query time should be better than brute force.",
            "It should be a little O of M and the data structure in the key and the sort of very large data regime should basically be linear in maybe a little bit more.",
            "Maybe something like N log in, or enter the 1.1, but it shouldn't be too much more than.",
            "The size of the data set.",
            "OK, So what can one do for a problem like this so?"
        ],
        [
            "There's this conjecture that's fairly common, though I think it's I think I don't think it's been proved, and the conjecture is that really this problem is subject to a curse of dimensionality and the way it's sometimes been stated is that either the query time has to be exponential in the dimension or the size of the data structure has to be exponential in the dimension just to give you an example of something that would be considered a bad case for nearest neighbor.",
            "When you're in D dimensional space, it's possible to pick roughly 2 to the D points that are about the same distance from each other.",
            "OK, in fact, it's not hard to do that if you pick points uniformly on the surface of the unit sphere, you can pick two to the epsilon squared D points and they'll all be a distance one, plus or minus epsilon Times Square root 2 from each other.",
            "So imagine that you now get a query point also from the same distribution.",
            "So its nearest neighbor is going to be a distance 1 minus epsilon and the entire data set is within distance one plus epsilon, and so this seems like the sort of case that would be hard, and this is the sort of case that has motivated this conjecture and also this just sort of general feeling about a cursive dimension for this problem.",
            "OK, so how can one get around?"
        ],
        [
            "Cases like this, how can cases like this be defeated?",
            "Well, in this particular example, in this particular bad case, it's difficult to find the nearest neighbor.",
            "It looks like you're going to have to look through all endpoints, but it's really easy to find the point that's just a little bit further away.",
            "Just returning any point.",
            "OK so."
        ],
        [
            "So that's sort of 1 very popular way of getting around.",
            "This curse is just to say well, instead of doing nearest neighbor, let's do approximate nearest neighbor and so ASI approximate nearest neighbor is any point that's at most C times the distance to the nearest neighbor and if one moves to this definition then actually there is abuse."
        ],
        [
            "Beautiful method called locality sensitive hashing do do Pyotr Indic and colleagues and this is what they were able to achieve.",
            "So let's let's say that for instance you want to approximate nearest neighbors.",
            "In that case you can get a query time event to the quarter.",
            "So much better than order EM.",
            "And the size of the data structure is N to the 1.25, so it's not linear, but you know it's not all that much more OK. And this is.",
            "This is an immensely popular scheme.",
            "OK, now one little caveat."
        ],
        [
            "That's you know this sort of good to point out is that this particular parameter C can be a little bit hard to interpret.",
            "So for some datasets A2 approximate nearest neighbor can be excellent.",
            "It might be the case that the two approximate points are just the nearest neighbor and a few more points, whereas for other datasets, like the bad example I gave the two approximate nearest neighbor can be the entire data set.",
            "So this parameter C can be a little bit hard to interpret.",
            "It means very different things for different datasets.",
            "OK now anyway, this is one way to sort of get around this curse of dimension, but returning to the sphere of exact nearest neighbor search."
        ],
        [
            "The other way, the other sort of common way to get around this curse of dimension is to say well instead of exponential in the dimension, can we do exponential in the intrinsic dimension for different notions of intrinsic dimension?",
            "And when it comes to exact nearest neighbor search, the notion of intrinsic dimension that is typically been used is the notion of a doubling measure.",
            "OK, this is the exact nearest neighbor and what this says is that.",
            "If you look at any ball and you double the radius of the ball, then the probability mass of the ball grows only by a factor of two to the DO, where D0 is this intrinsic dimension and this is the way we behave.",
            "We expect volume to behave in D0 dimensions and therefore and therefore we can somehow consider this value DO as the intrinsic dimension OK, and so there are there are.",
            "Many, many nearest neighbor methods that they are able to achieve exponential query time in this smaller dimension of the."
        ],
        [
            "I believe the most popular is probably the cover tree method.",
            "Due to bagels, IMA Kakade, and Langford.",
            "This is a beautiful method.",
            "It works not just in Euclidean space, but in any metric space the size of the data structure is just ordering OK the query time.",
            "It can be shown that the query time for one of these doubling measures is 2 to the small.",
            "This little D 0 times log.",
            "OK, so it's exponential in the intrinsic dimension.",
            "So OK, so this is so this is.",
            "This is another popular data structure.",
            "I think it's it's used quite a lot.",
            "I think there's a lot of open problems.",
            "However, about this and."
        ],
        [
            "Part of the reason for that is that this notion of doubling measures actually a very brittle notion, and I expect that it's something this phenomenon of low doubling measure is probably not something that holds very broadly.",
            "OK, so this is an algorithm that's clearly, you know, that seems to do well in practice.",
            "It's a good algorithm, but what we know about it is that in the worst case it's you know, just like brute force.",
            "And then there's this.",
            "There's this sort of we have this other extreme where it seems to be really good.",
            "But reality lies somewhere in the middle and we don't really, you know, it's an open problem to really characterize how it does in that regime.",
            "OK and OK.",
            "So this is another data structure now.",
            "Now finally, there is a third class of data structures that are also very popular and that basically have not been analyzed all that careful."
        ],
        [
            "And these are data structures based on the KD tree, and these are kind of what I want to focus on.",
            "OK, so KD tree is basically a hierarchical partition of space.",
            "You put a box around all the points, then you choose a direction and you split it.",
            "The median on that direction, and then you recurse and in this way you divide up space into hyper rectangular cells.",
            "Now the usual way in which this is used for nearest neighbor is that when you get a query point, you just move down the tree to the leaf.",
            "And you returned the nearest neighbor within that leave.",
            "Now that might not actually be its nearest neighbor, but you can just return that OK, and so there's a chance that this will fail.",
            "OK, this will not return the right answer, but you know it's very quick.",
            "It's very simple and it's very quick now.",
            "This failure probability this chance of returning the wrong answer is widely sort of recognized as being particularly severe in high dimension.",
            "It just depends on the number of points you know, the size of the leaves, so you know as long as you keep the leaves relatively small then you're only searching amongst those points.",
            "So in high dimensions people have come up with various fixes or various versions of this tree that seemed to improve performance quite dramatic."
        ],
        [
            "And so let me just describe some of these to you.",
            "OK, the first variant is to not is to start by randomizing the basis.",
            "So you take your data set and you just sort of spin it randomly and you work with this random basis.",
            "So you can think of picking random directions instead of coordinate directions.",
            "The second one that seems to help a lot is to use overlapping cells, so when you split a cell instead of splitting it into two disjoint parts, have a little overlap region in the middle, OK?",
            "And so we look at that more closely but basic."
        ],
        [
            "What I'll be talking about is analyzing the failure probability of these kinds of data structures for exact nearest neighbor search.",
            "OK, so I'll talk about three data structures in particular.",
            "So there are two tricks.",
            "One is random directions, the other one is over."
        ],
        [
            "Lapping cells, and we'll just try to formalize these and then analyze how these data structures behave OK.",
            "So the first trick is just random direction.",
            "So here's the data structure.",
            "Will look at instead of picking coordinate directions, pick a random direction from the unit sphere.",
            "Will also throw in an extra trick, which is that instead of splitting at the median, just choose a value beta uniformly from 2:45 quarters or any range, including a half, and split it that fractile.",
            "So don't necessarily split it 1/2.",
            "Maybe we'll put it quarter or third, something like that.",
            "OK, so this is the first data structure.",
            "This uses only one of the tricks.",
            "This does not use overlapping cells.",
            "Now let's look at."
        ],
        [
            "Overlapping cells OK, so these are typically called spill trees.",
            "When you use overlapping cells, OK, So what we'll do is, again, we'll pick a random direction.",
            "But now what we'll do is we'll have this fraction Alpha the top half minus Alpha fraction of the points go to one side, the bottom half minus fraction of the points go to the other side, and the 2A fraction of the points in the middle go both ways.",
            "OK, that's how you build a tree, and so when you build the tree, there are going to be some data points that end up in multiple leaves.",
            "Now when you get a query, you want to be able to answer the query quickly, so the query gets routed only to 1 leaf.",
            "By looking at the median.",
            "OK, so if it's below the median you go one side.",
            "If it's above the median, you go to the other side.",
            "OK, so the query time remains the depth of the tree plus the size of the League.",
            "OK, so this is called the spill tree, and so this is one of the data structures we look at."
        ],
        [
            "And now this data structure is not linear size.",
            "OK, Big cause some points end up in multiple leaves and so the size of the data structure depends on this fraction Alpha.",
            "Suppose you have 10% of the data that goes both ways.",
            "In each leaf, 10% of the data goes both left and right.",
            "In that case, the size of the data structure will for instance be an to the one .16.",
            "OK, it will be more than linear, which is not good, but you know it's not crazy either.",
            "OK.",
            "So."
        ],
        [
            "So, so we talked about this.",
            "This is a KD tree, just uses median splits for both routing data and routing queries.",
            "And RP tree uses the perturbed split for routing data and routing queries.",
            "A spill tree builds this data structure using overlapping splits, but when it's query time, it uses the median split.",
            "You can also think of another variant.",
            "Suppose you want a data structure that's linear sites.",
            "You can just build the data structure so that each data point goes to a single leaf, but when it's query time, then you let the queries go to multiple leaves.",
            "OK, so I mean so.",
            "So we have these three new data structures now, and so the question is when it comes to exact nearest neighbor search, what is the?",
            "You know they're all simple.",
            "Then they all seem relatively fast, but we know that they might not return the right answer.",
            "So what is the failure probability?",
            "Under what conditions can we analyze the failure probability of these schemes?",
            "And it turns out that it's actually extremely easy to analyze the failure probability and you don't need any conditions on the data whatsoever."
        ],
        [
            "OK, and so let me tell you what the result turns out to be.",
            "So pick any data set you like and any query you like.",
            "OK, no conditions.",
            "And now let's just order the points by distance from the query.",
            "OK, the closest, the second closest, and so on.",
            "So we fixed.",
            "We fixed a data set.",
            "We fixed a query but there is a lot of randomness in the actual data structure, so we can talk about the failure probability over the randomness in the data structure, and it turns out that that failure probability is just a very simple function of this potential function.",
            "OK, which is the following.",
            "It says.",
            "Look at the distance to the nearest neighbour divided by the distance to the second nearest.",
            "Plus the distance to the nearest neighbour divided by the distance to the third nearest plus the distance of the nearest neighbor divided by the distance of the fourth nearest average.",
            "All of these guys.",
            "OK, so this is going to give you a value between zero and one.",
            "The worst case is when all the points are roughly the same distance from the query, and in that case this is going to be 1.",
            "And the failure probability is going to be high.",
            "The best case is when most of the points are when most of the points are much, much further away than the nearest neighbor, in which case this is going to be close to 0 and the failure probability will be close to 0.",
            "I forgot.",
            "So I haven't stated the results.",
            "It's going to be a function of this that includes Alpha.",
            "Basically it's going to be one over Alpha in front.",
            "OK."
        ],
        [
            "So it's going to turn out that for the spill trees, the failure probability is proportional to this function.",
            "Where is the skill trees is the one that uses both tricks.",
            "The random direction and the overlapping cell.",
            "For the RP tree, which uses only one trick, it doesn't use the overlapping sell.",
            "The failure probability is a little bit higher.",
            "It's this times its feet times log one over Phi OK, and so this just holds for any for any data set.",
            "And now we can go back and look at standard assumptions like doubling, measure and so on and see what see what this function evaluates to under various conditions.",
            "So before we get into that, I still have to state the theorem properly and I also want to briefly indicate why this particular function comes up."
        ],
        [
            "And so the relevant thing over here is to look at the random projection of three points.",
            "OK, so suppose you had three points QX&Y.",
            "So let's think of Q is the query point, X is being some point that's close to it?",
            "And why is being a point that's just a little bit further away.",
            "OK, so this is the query point.",
            "Now you pick a random direction and you project all of them onto that direction.",
            "What is the probability that Y the further away point will actually fall between X&Q on that random projection?",
            "Now this is really easy to characterize exactly if you just construct the random unit vector as from Gaussians.",
            "Then all the quantities are basically chi squared and Koshy variables.",
            "We know everything's distribution and you can give this probability exactly and you can show that in fact it's just at most 1/2 of the distance to X divided by the distance to Y OK. And in fact, this is this is almost tight, you know, as long as the points are cold."
        ],
        [
            "Neo.",
            "And so now you can answer.",
            "That's the result from before, and I cannot OK now we have N data points and we have a query.",
            "Suppose you pick a random direction and you project all endpoints to that direction.",
            "How many points are going to fall between the query and its nearest neighbor?",
            "How many points are going to come in the way?",
            "And so you just take this and you just use linearity of expectation and you get exactly half of that potential function OK.",
            "So this means that in any cell of this tree.",
            "So the bad case with these trees is just that you know in each cell you're picking a random direction you're projecting onto that direction.",
            "If not many points fall between a point and its nearest neighbor, then because of this buffer zone, because of this, spills on the point in its queue, the query and its nearest neighbor are going to stay together.",
            "The only thing that can lead to failure is if lots of points fall between Q and its nearest neighbor, and so here we have the expected number of points that fall between Q and its nearest neighbor.",
            "It is not a function of points.",
            "Alpha is just the width of that.",
            "Joe metric know it's a fraction.",
            "Yeah it's a fraction, yeah.",
            "Yes yeah, that's key, yeah."
        ],
        [
            "OK, and so now."
        ],
        [
            "And so now what is the probability that any given split separates a query from its nearest neighbor?",
            "It's just this potential function divided by Alpha, the expected number of points, the expected fraction of points between the query in its nearest neighbor.",
            "Is this potential function over 2?",
            "The probability that the fraction of points that fall between a query and its nearest neighbor is greater than Alpha is by Markov's inequality just that divided by 2A?",
            "So this is the chance of getting messed up in.",
            "Any given cell of the tree OK, and now you just have to sum over cells at all possible levels of the tree, and so you get the."
        ],
        [
            "Final statement over here, where at the top cell you look at all the data points.",
            "Then you look at just 3/4 of them and then or just 1/2 of them and then 1/4 of them and then an 8 of them and so on.",
            "And so this is the expression over here."
        ],
        [
            "And it is."
        ],
        [
            "Without that you get."
        ],
        [
            "Essentially, you get very similar results for the other kind of spill tree for the RP tree, you just get free, log one over Phi, and it's very easy to extend to K nearest neighbors.",
            "So."
        ],
        [
            "Now let's go back to doubling measures.",
            "We have this potential function.",
            "What does this look like for a doubling measure, since that's the standard assumption, OK?",
            "So."
        ],
        [
            "So you have a doubling measure.",
            "You've got this query, let's say the distance to its nearest neighbor is Delta.",
            "So if you look at a ball of radius Delta around, the query contains just one point.",
            "When you go to 2D, you know you're going to have at most 2 to the zero points.",
            "When you go to 4D, you're going to have at most 4 to the zero points, so this potential function behaves very well.",
            "There are just a few points you know within radius 2D, there's just a few more points within radius for Delta an in this way you can show that this potential."
        ],
        [
            "Function in the case of a doubling measure is really very small, and so you recover this result that the query time is exponential in the intrinsic dimension.",
            "Now we can let's look at a bad case.",
            "So now that we have this, we can also talk about bad cases Anna."
        ],
        [
            "Bad case is a simple topic model.",
            "OK, so this is a model in which you have T topics.",
            "And each of them has independent coordinates.",
            "OK, OK, so I have to finish up, but this is basically."
        ],
        [
            "This is a mixture model in."
        ],
        [
            "Twitch.",
            "In order to double the number of points in a ball instead of doubling the radius of the ball, you just grow the radius by one and so this is a very bad case for nearest neighbor search.",
            "And in this case the query time is only a little bit better than brute force.",
            "It only improves by two to the square root L, where L is the expected length of our document OK."
        ],
        [
            "So."
        ],
        [
            "Let me finish up."
        ],
        [
            "Now.",
            "So for me, the nice thing about this is that it.",
            "Provides a potential function that captures the difficulty of nearest neighbor search.",
            "Now this potential function applies only to this specific class of methods and I think it would be interesting to see if one can prove something about other methods in terms of this function to sort of give a broader understanding of what is difficult.",
            "Cases for nearest neighbor search the other open problem is that in practice what works better than choosing random directions is to pick a direction by principle component analysis and there's some work on that and it would be nice too.",
            "Characterize that in theory, OK.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be talking about fast nearest neighbor search OK answer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so here's the situation.",
                    "label": 0
                },
                {
                    "sent": "You have a set of N data points OK, and subsequently you're going to be asked all sorts of queries, and for each query Q what you need to do is to return the closest of those endpoints.",
                    "label": 0
                },
                {
                    "sent": "OK, now the brute force way to do this is that when you get a query, you look through all the end points and you find the one that's closest and you return that one.",
                    "label": 0
                },
                {
                    "sent": "But that takes time order N, which is sort of disastrous in very large datasets.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to be able to do this faster.",
                    "label": 0
                },
                {
                    "sent": "And so when you get this initial set of end points, you're allowed to take a little bit of time to build up a data structure that will hopefully help you later on.",
                    "label": 1
                },
                {
                    "sent": "OK, So what are the requirements?",
                    "label": 0
                },
                {
                    "sent": "So the query time should be better than brute force.",
                    "label": 1
                },
                {
                    "sent": "It should be a little O of M and the data structure in the key and the sort of very large data regime should basically be linear in maybe a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Maybe something like N log in, or enter the 1.1, but it shouldn't be too much more than.",
                    "label": 1
                },
                {
                    "sent": "The size of the data set.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can one do for a problem like this so?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's this conjecture that's fairly common, though I think it's I think I don't think it's been proved, and the conjecture is that really this problem is subject to a curse of dimensionality and the way it's sometimes been stated is that either the query time has to be exponential in the dimension or the size of the data structure has to be exponential in the dimension just to give you an example of something that would be considered a bad case for nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "When you're in D dimensional space, it's possible to pick roughly 2 to the D points that are about the same distance from each other.",
                    "label": 1
                },
                {
                    "sent": "OK, in fact, it's not hard to do that if you pick points uniformly on the surface of the unit sphere, you can pick two to the epsilon squared D points and they'll all be a distance one, plus or minus epsilon Times Square root 2 from each other.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you now get a query point also from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So its nearest neighbor is going to be a distance 1 minus epsilon and the entire data set is within distance one plus epsilon, and so this seems like the sort of case that would be hard, and this is the sort of case that has motivated this conjecture and also this just sort of general feeling about a cursive dimension for this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so how can one get around?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cases like this, how can cases like this be defeated?",
                    "label": 1
                },
                {
                    "sent": "Well, in this particular example, in this particular bad case, it's difficult to find the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "It looks like you're going to have to look through all endpoints, but it's really easy to find the point that's just a little bit further away.",
                    "label": 0
                },
                {
                    "sent": "Just returning any point.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of 1 very popular way of getting around.",
                    "label": 0
                },
                {
                    "sent": "This curse is just to say well, instead of doing nearest neighbor, let's do approximate nearest neighbor and so ASI approximate nearest neighbor is any point that's at most C times the distance to the nearest neighbor and if one moves to this definition then actually there is abuse.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Beautiful method called locality sensitive hashing do do Pyotr Indic and colleagues and this is what they were able to achieve.",
                    "label": 0
                },
                {
                    "sent": "So let's let's say that for instance you want to approximate nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "In that case you can get a query time event to the quarter.",
                    "label": 1
                },
                {
                    "sent": "So much better than order EM.",
                    "label": 0
                },
                {
                    "sent": "And the size of the data structure is N to the 1.25, so it's not linear, but you know it's not all that much more OK. And this is.",
                    "label": 0
                },
                {
                    "sent": "This is an immensely popular scheme.",
                    "label": 0
                },
                {
                    "sent": "OK, now one little caveat.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's you know this sort of good to point out is that this particular parameter C can be a little bit hard to interpret.",
                    "label": 0
                },
                {
                    "sent": "So for some datasets A2 approximate nearest neighbor can be excellent.",
                    "label": 0
                },
                {
                    "sent": "It might be the case that the two approximate points are just the nearest neighbor and a few more points, whereas for other datasets, like the bad example I gave the two approximate nearest neighbor can be the entire data set.",
                    "label": 1
                },
                {
                    "sent": "So this parameter C can be a little bit hard to interpret.",
                    "label": 1
                },
                {
                    "sent": "It means very different things for different datasets.",
                    "label": 0
                },
                {
                    "sent": "OK now anyway, this is one way to sort of get around this curse of dimension, but returning to the sphere of exact nearest neighbor search.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other way, the other sort of common way to get around this curse of dimension is to say well instead of exponential in the dimension, can we do exponential in the intrinsic dimension for different notions of intrinsic dimension?",
                    "label": 1
                },
                {
                    "sent": "And when it comes to exact nearest neighbor search, the notion of intrinsic dimension that is typically been used is the notion of a doubling measure.",
                    "label": 1
                },
                {
                    "sent": "OK, this is the exact nearest neighbor and what this says is that.",
                    "label": 0
                },
                {
                    "sent": "If you look at any ball and you double the radius of the ball, then the probability mass of the ball grows only by a factor of two to the DO, where D0 is this intrinsic dimension and this is the way we behave.",
                    "label": 0
                },
                {
                    "sent": "We expect volume to behave in D0 dimensions and therefore and therefore we can somehow consider this value DO as the intrinsic dimension OK, and so there are there are.",
                    "label": 0
                },
                {
                    "sent": "Many, many nearest neighbor methods that they are able to achieve exponential query time in this smaller dimension of the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I believe the most popular is probably the cover tree method.",
                    "label": 1
                },
                {
                    "sent": "Due to bagels, IMA Kakade, and Langford.",
                    "label": 0
                },
                {
                    "sent": "This is a beautiful method.",
                    "label": 1
                },
                {
                    "sent": "It works not just in Euclidean space, but in any metric space the size of the data structure is just ordering OK the query time.",
                    "label": 1
                },
                {
                    "sent": "It can be shown that the query time for one of these doubling measures is 2 to the small.",
                    "label": 0
                },
                {
                    "sent": "This little D 0 times log.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's exponential in the intrinsic dimension.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is so this is.",
                    "label": 0
                },
                {
                    "sent": "This is another popular data structure.",
                    "label": 0
                },
                {
                    "sent": "I think it's it's used quite a lot.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot of open problems.",
                    "label": 0
                },
                {
                    "sent": "However, about this and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Part of the reason for that is that this notion of doubling measures actually a very brittle notion, and I expect that it's something this phenomenon of low doubling measure is probably not something that holds very broadly.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is an algorithm that's clearly, you know, that seems to do well in practice.",
                    "label": 1
                },
                {
                    "sent": "It's a good algorithm, but what we know about it is that in the worst case it's you know, just like brute force.",
                    "label": 0
                },
                {
                    "sent": "And then there's this.",
                    "label": 0
                },
                {
                    "sent": "There's this sort of we have this other extreme where it seems to be really good.",
                    "label": 0
                },
                {
                    "sent": "But reality lies somewhere in the middle and we don't really, you know, it's an open problem to really characterize how it does in that regime.",
                    "label": 0
                },
                {
                    "sent": "OK and OK.",
                    "label": 1
                },
                {
                    "sent": "So this is another data structure now.",
                    "label": 0
                },
                {
                    "sent": "Now finally, there is a third class of data structures that are also very popular and that basically have not been analyzed all that careful.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are data structures based on the KD tree, and these are kind of what I want to focus on.",
                    "label": 0
                },
                {
                    "sent": "OK, so KD tree is basically a hierarchical partition of space.",
                    "label": 1
                },
                {
                    "sent": "You put a box around all the points, then you choose a direction and you split it.",
                    "label": 0
                },
                {
                    "sent": "The median on that direction, and then you recurse and in this way you divide up space into hyper rectangular cells.",
                    "label": 0
                },
                {
                    "sent": "Now the usual way in which this is used for nearest neighbor is that when you get a query point, you just move down the tree to the leaf.",
                    "label": 0
                },
                {
                    "sent": "And you returned the nearest neighbor within that leave.",
                    "label": 0
                },
                {
                    "sent": "Now that might not actually be its nearest neighbor, but you can just return that OK, and so there's a chance that this will fail.",
                    "label": 0
                },
                {
                    "sent": "OK, this will not return the right answer, but you know it's very quick.",
                    "label": 0
                },
                {
                    "sent": "It's very simple and it's very quick now.",
                    "label": 0
                },
                {
                    "sent": "This failure probability this chance of returning the wrong answer is widely sort of recognized as being particularly severe in high dimension.",
                    "label": 0
                },
                {
                    "sent": "It just depends on the number of points you know, the size of the leaves, so you know as long as you keep the leaves relatively small then you're only searching amongst those points.",
                    "label": 0
                },
                {
                    "sent": "So in high dimensions people have come up with various fixes or various versions of this tree that seemed to improve performance quite dramatic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let me just describe some of these to you.",
                    "label": 0
                },
                {
                    "sent": "OK, the first variant is to not is to start by randomizing the basis.",
                    "label": 0
                },
                {
                    "sent": "So you take your data set and you just sort of spin it randomly and you work with this random basis.",
                    "label": 0
                },
                {
                    "sent": "So you can think of picking random directions instead of coordinate directions.",
                    "label": 0
                },
                {
                    "sent": "The second one that seems to help a lot is to use overlapping cells, so when you split a cell instead of splitting it into two disjoint parts, have a little overlap region in the middle, OK?",
                    "label": 0
                },
                {
                    "sent": "And so we look at that more closely but basic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'll be talking about is analyzing the failure probability of these kinds of data structures for exact nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'll talk about three data structures in particular.",
                    "label": 0
                },
                {
                    "sent": "So there are two tricks.",
                    "label": 0
                },
                {
                    "sent": "One is random directions, the other one is over.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lapping cells, and we'll just try to formalize these and then analyze how these data structures behave OK.",
                    "label": 0
                },
                {
                    "sent": "So the first trick is just random direction.",
                    "label": 0
                },
                {
                    "sent": "So here's the data structure.",
                    "label": 0
                },
                {
                    "sent": "Will look at instead of picking coordinate directions, pick a random direction from the unit sphere.",
                    "label": 1
                },
                {
                    "sent": "Will also throw in an extra trick, which is that instead of splitting at the median, just choose a value beta uniformly from 2:45 quarters or any range, including a half, and split it that fractile.",
                    "label": 0
                },
                {
                    "sent": "So don't necessarily split it 1/2.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll put it quarter or third, something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the first data structure.",
                    "label": 1
                },
                {
                    "sent": "This uses only one of the tricks.",
                    "label": 0
                },
                {
                    "sent": "This does not use overlapping cells.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overlapping cells OK, so these are typically called spill trees.",
                    "label": 1
                },
                {
                    "sent": "When you use overlapping cells, OK, So what we'll do is, again, we'll pick a random direction.",
                    "label": 0
                },
                {
                    "sent": "But now what we'll do is we'll have this fraction Alpha the top half minus Alpha fraction of the points go to one side, the bottom half minus fraction of the points go to the other side, and the 2A fraction of the points in the middle go both ways.",
                    "label": 0
                },
                {
                    "sent": "OK, that's how you build a tree, and so when you build the tree, there are going to be some data points that end up in multiple leaves.",
                    "label": 0
                },
                {
                    "sent": "Now when you get a query, you want to be able to answer the query quickly, so the query gets routed only to 1 leaf.",
                    "label": 0
                },
                {
                    "sent": "By looking at the median.",
                    "label": 0
                },
                {
                    "sent": "OK, so if it's below the median you go one side.",
                    "label": 0
                },
                {
                    "sent": "If it's above the median, you go to the other side.",
                    "label": 0
                },
                {
                    "sent": "OK, so the query time remains the depth of the tree plus the size of the League.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is called the spill tree, and so this is one of the data structures we look at.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now this data structure is not linear size.",
                    "label": 0
                },
                {
                    "sent": "OK, Big cause some points end up in multiple leaves and so the size of the data structure depends on this fraction Alpha.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have 10% of the data that goes both ways.",
                    "label": 0
                },
                {
                    "sent": "In each leaf, 10% of the data goes both left and right.",
                    "label": 0
                },
                {
                    "sent": "In that case, the size of the data structure will for instance be an to the one .16.",
                    "label": 0
                },
                {
                    "sent": "OK, it will be more than linear, which is not good, but you know it's not crazy either.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we talked about this.",
                    "label": 0
                },
                {
                    "sent": "This is a KD tree, just uses median splits for both routing data and routing queries.",
                    "label": 0
                },
                {
                    "sent": "And RP tree uses the perturbed split for routing data and routing queries.",
                    "label": 1
                },
                {
                    "sent": "A spill tree builds this data structure using overlapping splits, but when it's query time, it uses the median split.",
                    "label": 0
                },
                {
                    "sent": "You can also think of another variant.",
                    "label": 0
                },
                {
                    "sent": "Suppose you want a data structure that's linear sites.",
                    "label": 0
                },
                {
                    "sent": "You can just build the data structure so that each data point goes to a single leaf, but when it's query time, then you let the queries go to multiple leaves.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean so.",
                    "label": 0
                },
                {
                    "sent": "So we have these three new data structures now, and so the question is when it comes to exact nearest neighbor search, what is the?",
                    "label": 0
                },
                {
                    "sent": "You know they're all simple.",
                    "label": 0
                },
                {
                    "sent": "Then they all seem relatively fast, but we know that they might not return the right answer.",
                    "label": 0
                },
                {
                    "sent": "So what is the failure probability?",
                    "label": 0
                },
                {
                    "sent": "Under what conditions can we analyze the failure probability of these schemes?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's actually extremely easy to analyze the failure probability and you don't need any conditions on the data whatsoever.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so let me tell you what the result turns out to be.",
                    "label": 0
                },
                {
                    "sent": "So pick any data set you like and any query you like.",
                    "label": 1
                },
                {
                    "sent": "OK, no conditions.",
                    "label": 1
                },
                {
                    "sent": "And now let's just order the points by distance from the query.",
                    "label": 0
                },
                {
                    "sent": "OK, the closest, the second closest, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we fixed.",
                    "label": 0
                },
                {
                    "sent": "We fixed a data set.",
                    "label": 0
                },
                {
                    "sent": "We fixed a query but there is a lot of randomness in the actual data structure, so we can talk about the failure probability over the randomness in the data structure, and it turns out that that failure probability is just a very simple function of this potential function.",
                    "label": 0
                },
                {
                    "sent": "OK, which is the following.",
                    "label": 0
                },
                {
                    "sent": "It says.",
                    "label": 0
                },
                {
                    "sent": "Look at the distance to the nearest neighbour divided by the distance to the second nearest.",
                    "label": 0
                },
                {
                    "sent": "Plus the distance to the nearest neighbour divided by the distance to the third nearest plus the distance of the nearest neighbor divided by the distance of the fourth nearest average.",
                    "label": 0
                },
                {
                    "sent": "All of these guys.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is going to give you a value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "The worst case is when all the points are roughly the same distance from the query, and in that case this is going to be 1.",
                    "label": 1
                },
                {
                    "sent": "And the failure probability is going to be high.",
                    "label": 0
                },
                {
                    "sent": "The best case is when most of the points are when most of the points are much, much further away than the nearest neighbor, in which case this is going to be close to 0 and the failure probability will be close to 0.",
                    "label": 0
                },
                {
                    "sent": "I forgot.",
                    "label": 0
                },
                {
                    "sent": "So I haven't stated the results.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a function of this that includes Alpha.",
                    "label": 0
                },
                {
                    "sent": "Basically it's going to be one over Alpha in front.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's going to turn out that for the spill trees, the failure probability is proportional to this function.",
                    "label": 1
                },
                {
                    "sent": "Where is the skill trees is the one that uses both tricks.",
                    "label": 0
                },
                {
                    "sent": "The random direction and the overlapping cell.",
                    "label": 1
                },
                {
                    "sent": "For the RP tree, which uses only one trick, it doesn't use the overlapping sell.",
                    "label": 1
                },
                {
                    "sent": "The failure probability is a little bit higher.",
                    "label": 1
                },
                {
                    "sent": "It's this times its feet times log one over Phi OK, and so this just holds for any for any data set.",
                    "label": 0
                },
                {
                    "sent": "And now we can go back and look at standard assumptions like doubling, measure and so on and see what see what this function evaluates to under various conditions.",
                    "label": 0
                },
                {
                    "sent": "So before we get into that, I still have to state the theorem properly and I also want to briefly indicate why this particular function comes up.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the relevant thing over here is to look at the random projection of three points.",
                    "label": 1
                },
                {
                    "sent": "OK, so suppose you had three points QX&Y.",
                    "label": 0
                },
                {
                    "sent": "So let's think of Q is the query point, X is being some point that's close to it?",
                    "label": 0
                },
                {
                    "sent": "And why is being a point that's just a little bit further away.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the query point.",
                    "label": 0
                },
                {
                    "sent": "Now you pick a random direction and you project all of them onto that direction.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that Y the further away point will actually fall between X&Q on that random projection?",
                    "label": 0
                },
                {
                    "sent": "Now this is really easy to characterize exactly if you just construct the random unit vector as from Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Then all the quantities are basically chi squared and Koshy variables.",
                    "label": 0
                },
                {
                    "sent": "We know everything's distribution and you can give this probability exactly and you can show that in fact it's just at most 1/2 of the distance to X divided by the distance to Y OK. And in fact, this is this is almost tight, you know, as long as the points are cold.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Neo.",
                    "label": 0
                },
                {
                    "sent": "And so now you can answer.",
                    "label": 0
                },
                {
                    "sent": "That's the result from before, and I cannot OK now we have N data points and we have a query.",
                    "label": 0
                },
                {
                    "sent": "Suppose you pick a random direction and you project all endpoints to that direction.",
                    "label": 0
                },
                {
                    "sent": "How many points are going to fall between the query and its nearest neighbor?",
                    "label": 0
                },
                {
                    "sent": "How many points are going to come in the way?",
                    "label": 0
                },
                {
                    "sent": "And so you just take this and you just use linearity of expectation and you get exactly half of that potential function OK.",
                    "label": 0
                },
                {
                    "sent": "So this means that in any cell of this tree.",
                    "label": 0
                },
                {
                    "sent": "So the bad case with these trees is just that you know in each cell you're picking a random direction you're projecting onto that direction.",
                    "label": 0
                },
                {
                    "sent": "If not many points fall between a point and its nearest neighbor, then because of this buffer zone, because of this, spills on the point in its queue, the query and its nearest neighbor are going to stay together.",
                    "label": 0
                },
                {
                    "sent": "The only thing that can lead to failure is if lots of points fall between Q and its nearest neighbor, and so here we have the expected number of points that fall between Q and its nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "It is not a function of points.",
                    "label": 0
                },
                {
                    "sent": "Alpha is just the width of that.",
                    "label": 0
                },
                {
                    "sent": "Joe metric know it's a fraction.",
                    "label": 0
                },
                {
                    "sent": "Yeah it's a fraction, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah, that's key, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now what is the probability that any given split separates a query from its nearest neighbor?",
                    "label": 0
                },
                {
                    "sent": "It's just this potential function divided by Alpha, the expected number of points, the expected fraction of points between the query in its nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Is this potential function over 2?",
                    "label": 0
                },
                {
                    "sent": "The probability that the fraction of points that fall between a query and its nearest neighbor is greater than Alpha is by Markov's inequality just that divided by 2A?",
                    "label": 1
                },
                {
                    "sent": "So this is the chance of getting messed up in.",
                    "label": 0
                },
                {
                    "sent": "Any given cell of the tree OK, and now you just have to sum over cells at all possible levels of the tree, and so you get the.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Final statement over here, where at the top cell you look at all the data points.",
                    "label": 0
                },
                {
                    "sent": "Then you look at just 3/4 of them and then or just 1/2 of them and then 1/4 of them and then an 8 of them and so on.",
                    "label": 0
                },
                {
                    "sent": "And so this is the expression over here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Without that you get.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, you get very similar results for the other kind of spill tree for the RP tree, you just get free, log one over Phi, and it's very easy to extend to K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's go back to doubling measures.",
                    "label": 0
                },
                {
                    "sent": "We have this potential function.",
                    "label": 0
                },
                {
                    "sent": "What does this look like for a doubling measure, since that's the standard assumption, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have a doubling measure.",
                    "label": 1
                },
                {
                    "sent": "You've got this query, let's say the distance to its nearest neighbor is Delta.",
                    "label": 0
                },
                {
                    "sent": "So if you look at a ball of radius Delta around, the query contains just one point.",
                    "label": 0
                },
                {
                    "sent": "When you go to 2D, you know you're going to have at most 2 to the zero points.",
                    "label": 0
                },
                {
                    "sent": "When you go to 4D, you're going to have at most 4 to the zero points, so this potential function behaves very well.",
                    "label": 0
                },
                {
                    "sent": "There are just a few points you know within radius 2D, there's just a few more points within radius for Delta an in this way you can show that this potential.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function in the case of a doubling measure is really very small, and so you recover this result that the query time is exponential in the intrinsic dimension.",
                    "label": 1
                },
                {
                    "sent": "Now we can let's look at a bad case.",
                    "label": 0
                },
                {
                    "sent": "So now that we have this, we can also talk about bad cases Anna.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bad case is a simple topic model.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a model in which you have T topics.",
                    "label": 0
                },
                {
                    "sent": "And each of them has independent coordinates.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so I have to finish up, but this is basically.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a mixture model in.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Twitch.",
                    "label": 0
                },
                {
                    "sent": "In order to double the number of points in a ball instead of doubling the radius of the ball, you just grow the radius by one and so this is a very bad case for nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "And in this case the query time is only a little bit better than brute force.",
                    "label": 0
                },
                {
                    "sent": "It only improves by two to the square root L, where L is the expected length of our document OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me finish up.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So for me, the nice thing about this is that it.",
                    "label": 0
                },
                {
                    "sent": "Provides a potential function that captures the difficulty of nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "Now this potential function applies only to this specific class of methods and I think it would be interesting to see if one can prove something about other methods in terms of this function to sort of give a broader understanding of what is difficult.",
                    "label": 0
                },
                {
                    "sent": "Cases for nearest neighbor search the other open problem is that in practice what works better than choosing random directions is to pick a direction by principle component analysis and there's some work on that and it would be nice too.",
                    "label": 0
                },
                {
                    "sent": "Characterize that in theory, OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}