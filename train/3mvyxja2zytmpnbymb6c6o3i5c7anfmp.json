{
    "id": "3mvyxja2zytmpnbymb6c6o3i5c7anfmp",
    "title": "Beyond Backpropagation: Uncertainty Propagation",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_lawrence_beyond_backpropagation/",
    "segmentation": [
        [
            "What I thought I'd like to do today is to start talking about."
        ],
        [
            "The man that inspired me incredibly throughout my career and I'll make reference in throughout the tour.",
            "This is David Mackay for those of you who don't know, he died on the 14th of April from stomach cancer.",
            "So David was only 49.",
            "He was five years old with me.",
            "He's left a young family behind.",
            "He's one of the most inspirational people you'll ever meet.",
            "This is a frame.",
            "From him in a Cambridge ideas video where he's talking about the idea of using the light bulb as a unit of measure, so each apparently and in UK use 140 light bulbs worth of energy everyday.",
            "And describing the importance of making big changes to avoid climate change.",
            "That was a career turn he took.",
            "She awhile ago now instead of over maybe seven or eight years ago and he ended up being a writing a book, becoming Chief Scientific Advisor for the UK Government.",
            "Before that he'd a revolution revolutionized.",
            "Certainly from my perspective 2 fields, one of which is our own field machine learning, and the other riches information theory.",
            "I'll mention the ways in which he's inspired me.",
            "We had a symposium before he died or a month before he died, where many people from across the community in areas I didn't even really know who had influence came and talked about how they inspired him.",
            "I'm sure there's other people in the audience who feel similarly.",
            "Anyway, that's David, who very, very sadly died last month."
        ],
        [
            "OK, so.",
            "I thought I'd start by talking a little bit about why I got into the field.",
            "Which was because of these models here.",
            "So this is a neural network, and before I worked in machine learning, I actually spent time.",
            "It's upside down.",
            "I like to think differently.",
            "There's a reason why it's upside down.",
            "I'll come back to it later.",
            "I always draw them this way so.",
            "I used to work on oil rigs before I became a PhD student and I used to spend my time on oil rigs implementing neural networks with my friend Alex Rogers, who's now professor at University of Oxford who also worked on aerobics with me.",
            "So apparently it's a good place to spend time thinking about academic subjects, so this is a neural network, I just use it to introduce a bit of notation here, so we've got.",
            "I'm interested in it as a function approximators, so I've written it as a sort of function here with a weighted sum of basis functions which could be of any type, whatever the flavor of the month is.",
            "And the activations to those themselves are weighted sums of the inputs.",
            "I'm sure we all know that."
        ],
        [
            "So typically we might minimize an error function an.",
            "I'm mainly going to be talking about regression type problems, so squared error.",
            "But you can do what I'm talking about.",
            "For classification, it's a little bit more complicated, but squared error is the sort of typical thing we might use.",
            "I'm missing a negative sign here, but with a negative sign here.",
            "This is also the negative log over log.",
            "Likely the squared at the error function is the negative log likelihood of the Gaussian likelihood, and indeed that's why the distribution is named after Gauss because he justified its use in least squares through a Gaussian noise.",
            "Oddly, Laplace Odori invented the distribution before him, but it seems that you know, like class also proved the central limit theorem and various other things, but it seems you have to do a lot of work on something and still nothing against something named after you so.",
            "The sort of radical thing that David worked on.",
            "So that should be in Alpha V. There is the idea that instead of just fitting these functions directly by minimizing the error that we should introduce prior distributions over the weights.",
            "These prior distributions.",
            "So we sort of what they do is they force this this system instead of being just one set of functions to become a class of functions.",
            "So you can do this with your neural networks to see what sort of thing they say about data you can sample from your input weights and then you can put through the activations and then sample from your output weights.",
            "Take a look at what sort of functions you get."
        ],
        [
            "That idea is also represented in weight decay, so weight decay was something that was sort of known about, so you've got these two weight decay terms for the input and the output layers.",
            "So this is so, this should be the log joint distribution, not the conditional distribution.",
            "And what we want to do in Bayesian inference is rather than optimizing across those parameters, we want to sort of integrate them out.",
            "We want to get rid of them."
        ],
        [
            "Now.",
            "What's going on in the field very recently, over the last four or five years?",
            "I mean, this is actually 2007 this graph, so I don't know if you can quite read, but if you see the top here, it says 2007.",
            "This is 1986, which is rather conveniently.",
            "I guess when the backdrop papers written.",
            "So about sorry about these sort of times.",
            "Convents were developed and then by this sort of.",
            "Support vector machines were dominating nips.",
            "What happened was in this.",
            "Here, of course we just did not have enough data to get confidence working.",
            "I know there's a lot of stories about conspiracies and winters and stuff like that.",
            "But it was quite simple.",
            "There were papers that showed you could get equal or better performance with support vector machines on large datasets like EM lists.",
            "Then you could with convolutional neural networks and those models were easier to understand from a modeling perspective, not from an algorithmic perspective.",
            "From a modeling perspective.",
            "So the community switched.",
            "Now the shocking thing is when that came back the other way with the image net results from 2012, the Community switch back.",
            "Now to me that this seems like a community being sensible about what methods it's interested in, but you can put your narrative.",
            "So what have you like and the reason why, of course, is because of this massive explosion of digital data, and we're really seeing that these these methods are tending to dominate in areas where we have an enormous amount of data like computer vision, speech language.",
            "That's really, really thrilling.",
            "'cause we're doing some amazing things that we couldn't do before.",
            "So what?"
        ],
        [
            "Gaussian process well in a Gaussian process we take a totally different approach to modeling, or at least it appears like it's totally different approach to modeling.",
            "So these slides I was watching.",
            "Um, I guess this morning lecture by David.",
            "From let's see I've lost Chrome.",
            "Let me just get that up.",
            "So.",
            "Sorry, I had to shut down and restart and forgot to find these.",
            "So I was watching this Gaussian process.",
            "I mean, I do a lot of educational Gaussian processes.",
            "Including summer schools, and this is a video lecture, so we got video lectures today, so this is a meter video lecture.",
            "This is one of the most extraordinary explanations you'll ever see of Gaussian process is when I watch this video back, I see David saying things that I say today that I don't even remember originated in David's idea of how you present them.",
            "This is actually I sort of have it at this frame here, because this is me.",
            "This is my head in the front of the auditorium there, and he's just chatting to me.",
            "Just says to me, Neil, he's teaching his exercises in the middle of an hour long talk to make sure people understand how Gaussians work and he's just saying.",
            "Oh nails done the 5th exercise, so the rest of you must have finished.",
            "Making a joke at my expense that this is an extraordinary tutorial and Gaussian processes, and I I recommend anyone watch it.",
            "It's got 40,000 views over the last.",
            "It's 10 years old, next month actually, and it's got 40,000 views over the last 10 years.",
            "I mean why it doesn't have 4 million views?",
            "I don't know.",
            "It is an example of how to teach.",
            "It's an example of how to teach Gaussian processes."
        ],
        [
            "So let me just pause in there so we don't get it coming through.",
            "OK, so this is actually a slide that's kind of inspired a bit by that tutorial, but it's a sort of shortcut.",
            "So what is a Gaussian process?",
            "Well, this what I show you here is actually a Gaussian distribution, and I visualize the covariance matrix here for 25 dimensional Gaussian distribution.",
            "OK, so this is a 25 by 25 distribution.",
            "Unfortunately, Python seems to smooth out the pixels.",
            "You should see this more pixelated than it is, and then I've taken 1 sample 1 sample from this Gaussian distribution and I'm showing you this sample.",
            "On the left hand side.",
            "Now because of the structure of this covariance matrix, because nearby points to each other are strongly correlated, we see that the samples are very strongly correlated if their neighboring in this index.",
            "Now the idea of a Gaussian process this is a distribution and a single sample from a distribution.",
            "But in a process.",
            "We actually create this covariance matrix for covariance function, so for any two given inputs, I put time in here, but these could be spatial inputs.",
            "We can compute the covariance matrix.",
            "So we compute this covariance for any given finite set of points, and we can sample over functions in exactly the same way, and it's an incredibly powerful approach to dealing with nonlinear functions, because in some sense, yet the functions are nonlinear, but the system itself is based on Gaussians, so you can do all sorts of interesting operations like you can jointly model derivatives and the function itself.",
            "You can model integrals in the function itself, so very powerful model."
        ],
        [
            "So one way I like to think of that and this is really a diagram for Bayesian inference, not just Gaussian process is is that you have a prior over functions that you start with.",
            "So just like we had a prior over the weights in the neural network case, now we've got a prior over functions.",
            "We make some observations of some data points, two data points.",
            "Here we combine it with the likelihood.",
            "The likelihood says how much noise there is on the functions and we basically throw away all functions that don't fit.",
            "So the way I generated this plot was just like in the previous plot.",
            "I sampled like 100 or 1000, I can't remember.",
            "Gaussian samples and then I've thrown away those samples that don't go through these two points and this is what we call the posterior distribution.",
            "So this is our posterior representation of what we believe about this function.",
            "Now, given our initial belief about what the functions look like and the data and the thing that David introduced into the machine learning community will certainly to me.",
            "I mean, I think others were talking about it.",
            "But did the most to his 1992 thesis.",
            "Is just for neural computation papers stapled together so you can either get the issue of neural computation or you can get his thesis and the contents basically the same.",
            "He did his PhD thesis and I some minimal amount of time.",
            "It's all about how you apply these methods in neural networks using something called the Laplace approximation.",
            "I was reading it again this morning.",
            "It's incredible insight for that.",
            "I mean, David was one of the people who revolutionized the community.",
            "I think I mean in terms of his ideas about how we should address these models, how we should control for complexity in this era of low data, it was really vital in changing the thinking.",
            "Certainly was a massive influence on me.",
            "He gave a tutorial on Gaussian process is at my first NIPS in 1997.",
            "Which I think many people remember 'cause we didn't have video in those days where we."
        ],
        [
            "Video but.",
            "Not on the web.",
            "We were down in that part of the plot.",
            "I showed earlier.",
            "OK, now the really interesting thing about this is that Radford Neal had shown, and This is why Gaussian processes came about Radford.",
            "Neal, in his thesis, which I think was inspired partially by David.",
            "He was doing some sampling.",
            "Is that actually if we take that old neural network we had before and it doesn't matter how we sample from V, as long as we do finite variance samples?",
            "What we get because of the central Limit Theorem is this output here, which is a Gaussian weighted sum of sum.",
            "So it doesn't matter how we sample from here.",
            "If it's finite, it doesn't matter who we sample there for other reasons, but doesn't matter.",
            "So we can do non Gaussian samples here."
        ],
        [
            "As we take the hidden layer to Infinity as we increase the number of neurons in a neural network, then the neural net converges towards a Gaussian process.",
            "There's some wonderful plots in Radford Neal's thesis where he does this sort of just by playing with very large neural networks and shows how these things start to look like Gaussian processes.",
            "Now, this immediately shows you one of the weaknesses of these models because the way you do it is you create the you take the hidden layer to Infinity.",
            "But of course you have to scale down the weights on the output.",
            "You have to scale down the variance.",
            "So no single activation function will dominate anymore, so you can't get this thing anymore where you get sort of a sudden step in your function and then goes off somewhere else, because that step must be equally likely to be present everywhere, and that's the sort of Gaussian assumption that these nonlinearities are sort of.",
            "They can't say 101 single one can dominate."
        ],
        [
            "I mean, if you want to do that sort of thing, if you want to have nonparametrics for the other thing you have to look at Indian buffet process."
        ],
        [
            "Things like that.",
            "Now, in practice for low data, these models are extremely important, so this is an example of this is a Javier Gonzalez who's sitting over there who's a postdoc in Sheffield with me, who's been looking at Bayesian optimization.",
            "So alongside the 2012 image net paper, there's the sort of other paper that founded a company, founded a Twitter machine learning empire that is about how you can optimize.",
            "Neural Nets using Gaussian processes as a proxy as a surrogate for optimization function, so this idea of Bayesian optimization.",
            "So in this case your data is you sort of move around.",
            "You try optimizing the neural net with a certain set of parameters, and then you look at the validation error for that, and then you approximate that function, which is a function from a set of parameters to validation error with the Gaussian process.",
            "Then you can move around.",
            "So once you've got this, so this is what's going on in this top block here, but having a sort of visualizing it finding a minima.",
            "At the start, though, there's very few points and things are uncertain as it moves forward.",
            "I think it's missing a few frames at the beginning as it moves forward, it becomes more confident about where that memory is, as shown by the sort of activation function at the bottom.",
            "Now it's really, really important in this domain that you sustain uncertainty over the functions.",
            "I like to sit there sometimes, say you know.",
            "Why is it that when their training deep neural networks they use Gaussian process is to train them?",
            "'cause if deep neural networks were so great they should just use deep neural networks?",
            "Well, there's two possible answers.",
            "It's either because young and you're sure have decided that they're worried about the singularity, and they know if y'all net trains another deep neural network then that's kind of going to lead to this cycle of ever increasing intelligence, and they're just using Gaussian process is to slow up the whole process.",
            "Or I'll leave the rest your imagination.",
            "So this is a sort of auto ML challenge where what Javier is down is if he's run Gaussian process is random forests on on one of these sort of training challenges and Gaussian process is typically do very, very well.",
            "What we're also showing here is a deep Gaussian process which is one of the things I'll talk about today that's in the cyan alongside the yellow.",
            "So that's a generalization of Galaxy."
        ],
        [
            "Process is and we see this sort of across different areas.",
            "Random forests sometimes work well, but normally they are very much outperformed by Gaussian processes.",
            "OK.",
            "So how can we use these things in practice well?",
            "So one of the things I've been working on in Sheffield is this idea of open data science and trying to realize that you know the use of data analysis.",
            "I think you know developing world countries have far more to benefit from them.",
            "We have in the developed world because you can ingrain data analysis in their systems as you build them, which is way better than trying to go back retrospectively and modify your entire health system as anyone who's worked in health data has worked out.",
            "So one of the things we've been doing is."
        ],
        [
            "Doing Gaussian process fits for malaria incidents in in Uganda and this is a project with UN global pulse and.",
            "John Queenan Martin McGuinness begin.",
            "See who's in global pulse.",
            "Based in Uganda what you've got here is actually there's a blue line, and as a yellow line so there's two fits.",
            "One of the things you can do with the Gaussian process, you have separate length scales, so there's actually two of these covariances stuck together here, and what you're seeing is the model moving forward.",
            "Once handling like short-term variation, which is just to do with blips of things going up and the other process is handling slow variation.",
            "That's the sort of understandability we want, because in this map here what we've got.",
            "Is we're taking the idea so we can actually also find the gradient of the process, and we're trying to visualize where in Uganda in different states there's a crisis at any one moment with malaria.",
            "So.",
            "Rather than showing too much that these are, this is Ricardo is my student.",
            "He's now UCSF continuing to work on malaria.",
            "Is John Quinn, who's at global Pulse and Martin, who's a global pulse.",
            "So what they've actually got now is this is on stream for the UN, so this is an exact.",
            "This isn't actually what the UN look at, but they have now a map that they can pull in data from across the states as it's reported to try and understand where diseases and where they need to dedicate resource is now.",
            "Importantly, there's a lot of this is actually just visualization, so this is a really interesting thing.",
            "When you work with real data, you find that most of your modeling is far less important than the way that the data is.",
            "Visualized, but the thing that some Ricardo was able to do was to deal with missing reports in states using the Gaussian process models.",
            "And it has a very significant effect on what the map looks like.",
            "So where the crisis areas are, so you get a lot of missing reports in terms of health centers that didn't submit data for whatever reason that can be correlated.",
            "And Ricardo used Gaussian processes for dealing with that.",
            "You're just seeing a Fast forward overtime this.",
            "This map is live and it's being used.",
            "These sort of ideas were used to deal with the typhoid outbreak in Kampala in.",
            "March not this year.",
            "Last year, John couldn't attend a conference with us because he's dealing with at one point because there's an unfolding crisis in Ethiopia in terms of with drought and you can imagine.",
            "OK, it's all very well as having loads of resources to deal with that.",
            "But if you don't know where people are dying, it's kind of pointless and data can solve that right data.",
            "We've got everyone kind of pretty much everyone in."
        ],
        [
            "And this is a sort of UN result.",
            "There's more people have access to mobile phones than they have access to toilets.",
            "OK, so that's sort of worrying in terms of people's access for toilets.",
            "But it also means that you can have an incredible effect by doing things on mobile phones, and increasingly smartphones.",
            "You can pick up in Kenya or Uganda, a very nice smartphone for about $60.00.",
            "Android, not iPhone."
        ],
        [
            "OK.",
            "So that's one sort of thing that we can do with these sort of models, and actually we're not the first to do that.",
            "They're very widely used in spatial Epidemiology.",
            "And we've collaborated with people like, well, talk to people like Peter Diggle and Peter gathering about."
        ],
        [
            "You know ways of taking advantage of these models?",
            "This is another little example that I like, so this is a genome transcription situation where you've got an M RNA that operates a transcription factor, and then it's got a target M RNA and within the model what we've got is something we can't observe, which is this protein that's between the two, but we can infer it if we create these differential equations.",
            "It turns out we can build a joint Gaussian process over these three functions, and we can infer the indirect thing in the middle, and we can do that because the properties of Gaussians under linear operators they stay Gaussian.",
            "That's a nice example too.",
            "OK."
        ],
        [
            "So this is a quote I remember David saying at the NIPS tutorial in 1997 to me he gave were at that time.",
            "I was new to the field, I guess I've been in the field a year or 6, seven months or something now just over a year and.",
            "We were very interested in the sort of questions people care about today, like how how big your neural network should be and and how you should train it and all these sort of things and the Gaussian process really seemed to solve that.",
            "If you're in the area of small data, it really did solve that like we couldn't run it on EM list at the time, but emanus was like the largest, you know, Jan had spent ages collecting that data set a lot of money I'm guessing.",
            "AT&T, or you know?",
            "Yeah, so it was endless.",
            "Yeah next year did.",
            "Yeah, so a lot of the data sets we were dealing with were much smaller and Gaussian process.",
            "It seemed like the obvious solution, but David even 1997 made this quote and it's in his tutorial on Gaussian processes.",
            "He said have we thrown out the baby with the bathwater which was just to meet?",
            "This totally freaked me out because as far as I had seen, he just solved every problem that I knew offers a problem in machine learning or haven't solved it.",
            "Be given a tutorial showing how to solve it.",
            "And then he said, but there's a problem with this and the problem he was saying is actually in the end of the day.",
            "These are just smoothing machines.",
            "Because of their single layer nature, they just smoothing things right.",
            "You all you're doing in a very sophisticated way.",
            "You're smoothing things like away and like it's amazing what you can do is moving.",
            "That's true, but the structural things that we're interested in with neural networks that we want to get out.",
            "If we really want to understand abstractions and so forth."
        ],
        [
            "And that's of course why we come to deep learning.",
            "So this is a slide ripped off from a deep face paper.",
            "From Facebook and or I like to call it the Calista Flockhart detector.",
            "So this is just trying to illustrate that actually what you've got in this system.",
            "We can write down your arms linear threshold units and whatever else, but it's just a serial function.",
            "The series of functions, so the actual output of the vector valued functions, vector valued functions, but functions nevertheless.",
            "And the clever bit, and I remember watching and talk about this in 1997.",
            "Also, the first time I saw David Speaker machine learning Generalization Summer School, which I think was a sort of big moment in the community.",
            "The importance of.",
            "I remember in what the time Jan was working on the check writing system, I think you're going to back propagate through the entire check writing system.",
            "This is for AT&T, so Jan was always ahead of the game and knowing this thing was possible, but the basic idea with deep learning is to have things you can differentiate that you can differentiate through and then you compose them together, compose simple functions together to form complex functions.",
            "I mean, that's the key, and for a mathematician the thing at the bottom is kind of meaningless.",
            "Just write G of X, you save a lot of ink, but the class of functions is changing.",
            "That's very important.",
            "OK, do I want to say much about this slide, you know?",
            "Probably not.",
            "The moment maybe I'll come back to that.",
            "'cause well, I thought I was going to see next."
        ],
        [
            "Maybe I'll have to.",
            "OK.",
            "I'm missing a slide now.",
            "See the whole talk.",
            "OK, right?",
            "I'll say something about that slide.",
            "Apparently, apparently the earlier Neal has put me in a difficult situation.",
            "So.",
            "Now what we're trying to do, typically in Bayesian inference, is we're trying to deal with the parametric model in this way, so W Now represents our user RV's, and we have a joint probability distribution.",
            "And then we convert it to a marginal distribution.",
            "So what we're really interested in in Bayesian inference is the model that predicts what wise do we expect to get given some set of input X is.",
            "That's what the Gaussian process defines.",
            "It said it's a GPU with some corrupted noise on it."
        ],
        [
            "Now, in practice, how do we deal with this?",
            "One way is to deal with it using variational inference, because those integrals are very often not tractable, and I like this decomposition.",
            "So variational inference says that instead of working with the log likelihood directly, this marginal likelihood that we're interested in, we typically lower bound it.",
            "Now lower bounding this likelihood.",
            "Has something like this form So what we do is we find an approximation to distributions over the parameters, so we would really like to know the true posterior, But that's often difficult to find.",
            "So instead we seek an approximation QW, but that approximation so this lower bound turns out to be equal to the log likelihood minus a KL divergent.",
            "So that's an information theoretic measure between this approximating distribution and the true posterior we're interested in, and this is a really beautiful thing because this decomp."
        ],
        [
            "Addition can then be written in this way, so this is the lower bound on the log.",
            "Likelihood is given by an expected value appear.",
            "Why, given W, X?",
            "Expectation under Q of WR current belief about what the best distribution over parameters is.",
            "Minus this KL divergences between Q of W&P of W. Now what this equation is saying is that this lower bound that we're looking at, which is tight if we allow full flexible forms for Q&W.",
            "Is given by the expected log likelihood, so that is just the expected error function basically or the negative expected error function.",
            "Mine is something that says that cube W should be close to our prior.",
            "So basically there's a fight in these two terms between something that is saying fit your data.",
            "The data term is the blue term and the orange term is fit your prior and this always emerges from this variational approximation and Dustin's going to probably give a few more details on that.",
            "Well, I like about that is this is now an optimization problem, so variational methods term integrations into optimizations for us and then instead of working with the integrals, which are typically very intractable.",
            "We tend to work with the.",
            "Distributions over W if Q of W was a Delta function, then this would just be the standard thing.",
            "People fit in neural Nets, but this term would actually then sort of be Infinity or something because it's gotten entropy in it, which we entropy of W and that would be ugly.",
            "OK.",
            "So.",
            "That's the sort of setup we're looking at in Bayesian inference."
        ],
        [
            "Now another remark on David.",
            "There's a Waterhouse Mackay paper that is probably the most staggering paper at this.",
            "So he read Geoff Hinton had this work on doing this in neural networks.",
            "Can't remember the name of the paper, but it was a it was Hinton Vancamp wasn't it?",
            "I CML, I think, but he had this whole bits back argument about why this is a good thing to do, and David was very close to Jess lab and it's always been a great admirer of Jeff and he wrote this paper about doing variational inference in a freeform way where you make factorizing assumptions for Q&W.",
            "And it's an absolute classic.",
            "It's not cited half enough Waterhouse, and Mackay is one of them.",
            "Is the 1st paper to say.",
            "How I mean, we were then dominated by implementing that model for a number of years in the Community.",
            "So yeah, we've got this sort of thing that looks like the expected error function.",
            "OK, so the Gaussian process is a little bit different from that because in a Gaussian process you have this difficulty that your prior is directly over the function, which is an infinite dimensional thing.",
            "So sustaining AQ distribution over an infinite dimensional thing is a little bit hard.",
            "So we defined a covariance function which relates.",
            "Our data to this covariance in here and then we define a likelihood that so that's our input data and then our target data is typically given by just the Gaussian noise or we have to do something more complicated for doing classification.",
            "But if it's a Gaussian noise, everything's kind of analytic.",
            "Now that graphical model looks rather different to the previous one, so the previous one."
        ],
        [
            "Here.",
            "Has this sort of parameters separate from the data, but marginalizing out?",
            "In effect, you can see it as marginalizing out the parameters.",
            "You get this effect here, where the prior itself is dependent on the inputs and this means that approximations in these models are other hub.",
            "So, David Bly is done and others have many collaborators.",
            "Hoffman and others, have done a lot of work and how you scale up inference in those classical models with something called stochastic variational inference.",
            "Very important technique.",
            "We're seeing work by Maxwell in Dark, Ingmar, Shakira, Mohammed Jenny, Lorenzo today.",
            "All these sort of things are using those approximations and stochastic approximations in them, and they're all very exciting.",
            "Of course, I hope they failed and Gaussian processes went out the day, but there's lots of people interested in these sort of probabilistic ways of pushing uncertainty."
        ],
        [
            "The model.",
            "So.",
            "The trick that Wayne trick that we've done is to actually take this model and augment it so this is an odd sort of trick that we have to do in order to work at large scale.",
            "You meant the model.",
            "With some observations you never made.",
            "Then you do a bit of manipulation in the model.",
            "So now I've written rewritten this model.",
            "I've taken this these things.",
            "We never made.",
            "The reason that this is true, because we can just trivially integrate out across peer review, and we get the original model.",
            "But in this model here we now augment the model we introduce another prior.",
            "And.",
            "Integrate over F. So this gives us a new model that looks awfully like the parametric model, and this trick is the key to scaling up Gaussian processes, because the thing I failed to mention is that the challenge with Gaussian processes computational complexity is cubic in the number of data you're looking at, so that was still OK when we had 1000 data points.",
            "In fact, today you can do M list exactly 60,000 data, but you know you cannot do a million or billion because you've got cubic complexity and actually what hits you first is the quadratic storage complexity.",
            "So these techniques here.",
            "This sort of I don't have too much time to go through in detail, but there's papers by my car list.",
            "It's yes, those papers.",
            "I mean, this is a time at which David had sort of moved on to do energy, so there's no real work by David in this area.",
            "But papers by Ed Snelson zooming Armani various people in, sort of how you apply these techniques."
        ],
        [
            "So the interesting thing about this relating to the previous talk that this you looks like a parameter, But there's this weird thing.",
            "So in the previous talk we will talk about how we should change capacity of models according to our data set size.",
            "Absolutely not.",
            "We should never do that.",
            "No?",
            "I mean you could do that.",
            "In practice you do it, but that nonparametric Bayesian answer to that is null.",
            "Start with a super super complex model.",
            "Deal with the uncertainty correctly.",
            "And as you obtain more data, your uncertainty will gracefully collapse in the right way.",
            "Unfortunately, that means storing all the data, but this sort of.",
            "Approximation here has this interesting parametric form.",
            "Looks like a parametric model, but the weird thing is because it's giving us a lower bound in the end we can increase the dimensionality of you at runtime and that increases the tightness of our lower bound.",
            "So we've got this tradeoff between when we want to sort of move these dimensionality of you around, which is a sort of very special thing which we have nowhere near exploited enough yet in trying to scale up these models."
        ],
        [
            "So the challenge for Gaussian process is if you want to do them in a deep way.",
            "Is this?",
            "The composition of functions appears is a chain of integrals.",
            "When you write it down probabilistically.",
            "So this is a Gaussian process over the first layer, F1.",
            "Sorry I'm missing an F1 in here.",
            "Shouldn't we are missing a pair of F2 given F1?",
            "But then you basically change through the process that in each process governs the process below it.",
            "So these are functions.",
            "This is equivalent to this.",
            "Now that leads to a bunch of very serious nasty integrals that are difficult to do in practice.",
            "But if we can do that paired integral there, it turns out that everything else sort of follows.",
            "So we can chain it.",
            "It just recurses through.",
            "So we can apply bounds recursively to this system, and the innovation we had in this paper with my car listed C as we just had a Jamel our version of the paper come out last month with Andreas and we all know who's also here is how to do that for a one layer model.",
            "Once you've got that you can put many layers together.",
            "Why is this difficult?",
            "Well because if you put a Gaussian distribution through a nonlinearity then you get something that's very non Gaussian and intractable.",
            "So in general this distribution."
        ],
        [
            "Alright, it looks really cool and it can do lots of things, but it's also you can't normalize it, so you can't write down its likelihood exactly, but these variational techniques I've talked about.",
            "We can use them to lower bound the likelihood.",
            "So the objective here and people are suggesting other techniques.",
            "So these models are catching on.",
            "Some people are doing expectation propagation sampling, mixing in sampling.",
            "There was a paper by Christos who's talking after me where they're doing some sampling approaches with parametric approximations inside yesterday.",
            "So talk to him about that.",
            "That seems to be working quite well."
        ],
        [
            "But this in the end gives you something very interesting.",
            "It gives you a new approach to forming stochastic process is so stochastic processes are very interesting classes of models 'cause we can sustain beliefs and probabilities over models, but they are often limited.",
            "You can only write down certain forms, but what we're sort of saying here now is that these deep Gaussian processes are giving us the ability to do stochastic process composition.",
            "So compose one process to another to form new process.",
            "Of course, that thing is intractable, but we can start saying interesting things about it.",
            "Properties, so there's a paper on the pathologies of deep learning, which I think isn't really about pathologies.",
            "But it was a good way of getting it published where they analyze deep Gaussian processes.",
            "As so trying to understand the way deep learning works, because if you understand each of these neural networks is an approximation to a Gaussian process, you can really start applying some theory onto how deep learning actually works in practice."
        ],
        [
            "So here's the sort of thing that we get if we fit these sort of models to data.",
            "So here's a multilayer perception being fitted to only 20 three data points.",
            "200 iterations of fit.",
            "The fit looks quite good.",
            "This is something called the motorcycle data.",
            "This."
        ],
        [
            "Is when you actually converge it.",
            "Not great.",
            "This."
        ],
        [
            "Is a Gaussian process fit?",
            "Now what you see on this Gaussian process fit is it has to get short length scale to drop down.",
            "Now yes, you might say there's some overfitting going on here, but bear in mind the function the model itself has these big error bars.",
            "It's saying the function goes up and down all over the place potentially."
        ],
        [
            "This is a deep Gaussian process, so the deep Gaussian process can warp the inputs, but it doesn't overfit.",
            "It doesn't overfit, it just sort of a is actually doing.",
            "You know, people would almost say that's Le."
        ],
        [
            "Overfit than that."
        ],
        [
            "Would disagree with that, but that's the sort of semantic thing, and then this is a three layer."
        ],
        [
            "Gaussian process, so we've put three Gaussian processes together to do the fit here, so it's a sort of fully Bayesian thing where you get all these nice things, and these error bars and so."
        ],
        [
            "And yeah, indeed the two DEG.",
            "The DGP with two layers happens to slightly outperform the three layer.",
            "I'm sure that's in the noise.",
            "The three layer looks pretty good as well."
        ],
        [
            "And they perform kind of well on."
        ],
        [
            "But the thing that really interested me is how to do unsupervised learning.",
            "This is how we started in this area.",
            "So classical latent variable models.",
            "Sort of of this form where you have some sort of latent space, and then you have a sort of an observed space.",
            "In my analogy for this is like a puppet so you don't observe the puppeteer's hand, but you observe the motion of the figure underneath and you want to know what the puppeteer did.",
            "Given the high dimensional data.",
            "Now.",
            "I'm a bit short of time, so I'm just going to run through a little bit quickly.",
            "Some data is up on the screen on screen.",
            "Very good.",
            "So this is some motion capture data, so it's only.",
            "It's just, I think, 45 frames or something, not even that of 102 dimensional data of a figure moving, so it's a 3D point cloud.",
            "I love motion capture data because it's not a rare example of a data set that humans can visualize the high dimensions and they don't worry about it.",
            "Now David did some great work that I just wanted to include quickly.",
            "Where he actually did neural networks latent variable models.",
            "So this is an example of that fit.",
            "It doesn't work great unfortunately, because in this low data it it won't work that well.",
            "So this is a latent space.",
            "This is the puppeteer space, and now I'm moving the figure around and you can see some of the motion is there, right?",
            "So this is using a neural network to map from the latent space to the data space, and it's using important sampling.",
            "He called it density Nets.",
            "It was a really cool model.",
            "It's way ahead of its time.",
            "Actually, it was those density Nets that inspired me to do this with Gaussian processes.",
            "So this is what happens when you do it with Gaussian processes.",
            "So let me turn the guy around so that you can see what these different parts of the model do.",
            "So you can make the guy sort of lean up and down.",
            "This works about 10 years old now and then you can get the running motion out.",
            "But what we couldn't do in this initial model is learn the dimensionality of this latent space.",
            "So what we did after that and that was the work I mentioned where we integrate over the inputs is we actually learn the dimensionality and this is a work of Andreas Damianou who I think to to implement this example where it actually decides that there's actually 4 latent dimensions controlling this figure.",
            "The reason there's four I don't have too much time to Dick around, but there's one minute left apparently.",
            "If I look at the third dimension here, so one of the dimensions is just to do with his starting position.",
            "Right, so he starts there.",
            "This dimension is one of his sort of leg movement.",
            "And then the other dimensions in dimension two, dimension 4 rather beautifully completely deals with his angle of run.",
            "So let me turn him around for that.",
            "So that's the angle in which he runs, and it pulls all these things out on really really low data.",
            "So the challenge for these sort of things is scaling them up.",
            "So.",
            "We know, um, so I've just been talking about that.",
            "So the challenge?"
        ],
        [
            "Is scaling them up 'cause we'd love to be able to do these sort of abstraction of different concepts so my."
        ],
        [
            "So this is an example where we sort of do a layered model on motion capture.",
            "This is an example where we."
        ],
        [
            "Your layered model on the handwritten digits, so we can learn the structure of these models but them."
        ],
        [
            "My name?",
            "So we've actually started a company because scaling up these things you don't get academic papers for all the work you put in, as many people will be aware, I'm sure.",
            "So we formed the company which is scaling these models up."
        ],
        [
            "And the really nice early results is what happens.",
            "One of the problems you get when you scale up is massive sort of amounts of numerical noise.",
            "So we totally redeveloped how we do these approximations and what this is showing here is with the new approach from inferential that we get rid of this numerical noise in the optimization, do an enormous amount better."
        ],
        [
            "So the end aim is to do this sort of thing to do deep health as I call it, had been calling it for a number of years where we try and assimilate all the different aspects of a human into one model."
        ],
        [
            "You can find out a lot more about Gaussian process by either attending our Gaussian process summer school or all the videos are online.",
            "The slides and everything else.",
            "So please have a look at that.",
            "There's two posters, I TLR one on recurrent Gaussian process is so we've done recurrent neural networks with Gaussian process is an one of variational autoencoder, deep Gaussian processes, and then we have Python software.",
            "And of course there's Davids Gaussian Process Basics talk, so I just want to end with you on a final thought.",
            "For David, I would not be doing any of the things I'm doing today without David and I never coauthored a paper with him.",
            "I just used to attend his group meetings in Cambridge.",
            "'cause he was very welcoming to the person.",
            "I mean, he was kind of penetrating.",
            "If you thought you were wrong about something, but a lot of academics are like that and he was just a wonderful inspiration and I think it's a terrible tragedy that he leaves behind.",
            "I think Tony's oldest is 4 and his youngest is 1 and those children will never benefit from direct interaction with such a famous man as adults.",
            "That's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I thought I'd like to do today is to start talking about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The man that inspired me incredibly throughout my career and I'll make reference in throughout the tour.",
                    "label": 0
                },
                {
                    "sent": "This is David Mackay for those of you who don't know, he died on the 14th of April from stomach cancer.",
                    "label": 0
                },
                {
                    "sent": "So David was only 49.",
                    "label": 0
                },
                {
                    "sent": "He was five years old with me.",
                    "label": 0
                },
                {
                    "sent": "He's left a young family behind.",
                    "label": 0
                },
                {
                    "sent": "He's one of the most inspirational people you'll ever meet.",
                    "label": 0
                },
                {
                    "sent": "This is a frame.",
                    "label": 0
                },
                {
                    "sent": "From him in a Cambridge ideas video where he's talking about the idea of using the light bulb as a unit of measure, so each apparently and in UK use 140 light bulbs worth of energy everyday.",
                    "label": 0
                },
                {
                    "sent": "And describing the importance of making big changes to avoid climate change.",
                    "label": 0
                },
                {
                    "sent": "That was a career turn he took.",
                    "label": 0
                },
                {
                    "sent": "She awhile ago now instead of over maybe seven or eight years ago and he ended up being a writing a book, becoming Chief Scientific Advisor for the UK Government.",
                    "label": 0
                },
                {
                    "sent": "Before that he'd a revolution revolutionized.",
                    "label": 0
                },
                {
                    "sent": "Certainly from my perspective 2 fields, one of which is our own field machine learning, and the other riches information theory.",
                    "label": 0
                },
                {
                    "sent": "I'll mention the ways in which he's inspired me.",
                    "label": 0
                },
                {
                    "sent": "We had a symposium before he died or a month before he died, where many people from across the community in areas I didn't even really know who had influence came and talked about how they inspired him.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there's other people in the audience who feel similarly.",
                    "label": 0
                },
                {
                    "sent": "Anyway, that's David, who very, very sadly died last month.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I thought I'd start by talking a little bit about why I got into the field.",
                    "label": 0
                },
                {
                    "sent": "Which was because of these models here.",
                    "label": 0
                },
                {
                    "sent": "So this is a neural network, and before I worked in machine learning, I actually spent time.",
                    "label": 0
                },
                {
                    "sent": "It's upside down.",
                    "label": 0
                },
                {
                    "sent": "I like to think differently.",
                    "label": 0
                },
                {
                    "sent": "There's a reason why it's upside down.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to it later.",
                    "label": 0
                },
                {
                    "sent": "I always draw them this way so.",
                    "label": 0
                },
                {
                    "sent": "I used to work on oil rigs before I became a PhD student and I used to spend my time on oil rigs implementing neural networks with my friend Alex Rogers, who's now professor at University of Oxford who also worked on aerobics with me.",
                    "label": 0
                },
                {
                    "sent": "So apparently it's a good place to spend time thinking about academic subjects, so this is a neural network, I just use it to introduce a bit of notation here, so we've got.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in it as a function approximators, so I've written it as a sort of function here with a weighted sum of basis functions which could be of any type, whatever the flavor of the month is.",
                    "label": 0
                },
                {
                    "sent": "And the activations to those themselves are weighted sums of the inputs.",
                    "label": 0
                },
                {
                    "sent": "I'm sure we all know that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typically we might minimize an error function an.",
                    "label": 0
                },
                {
                    "sent": "I'm mainly going to be talking about regression type problems, so squared error.",
                    "label": 0
                },
                {
                    "sent": "But you can do what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "For classification, it's a little bit more complicated, but squared error is the sort of typical thing we might use.",
                    "label": 0
                },
                {
                    "sent": "I'm missing a negative sign here, but with a negative sign here.",
                    "label": 0
                },
                {
                    "sent": "This is also the negative log over log.",
                    "label": 0
                },
                {
                    "sent": "Likely the squared at the error function is the negative log likelihood of the Gaussian likelihood, and indeed that's why the distribution is named after Gauss because he justified its use in least squares through a Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Oddly, Laplace Odori invented the distribution before him, but it seems that you know, like class also proved the central limit theorem and various other things, but it seems you have to do a lot of work on something and still nothing against something named after you so.",
                    "label": 0
                },
                {
                    "sent": "The sort of radical thing that David worked on.",
                    "label": 0
                },
                {
                    "sent": "So that should be in Alpha V. There is the idea that instead of just fitting these functions directly by minimizing the error that we should introduce prior distributions over the weights.",
                    "label": 0
                },
                {
                    "sent": "These prior distributions.",
                    "label": 0
                },
                {
                    "sent": "So we sort of what they do is they force this this system instead of being just one set of functions to become a class of functions.",
                    "label": 0
                },
                {
                    "sent": "So you can do this with your neural networks to see what sort of thing they say about data you can sample from your input weights and then you can put through the activations and then sample from your output weights.",
                    "label": 0
                },
                {
                    "sent": "Take a look at what sort of functions you get.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That idea is also represented in weight decay, so weight decay was something that was sort of known about, so you've got these two weight decay terms for the input and the output layers.",
                    "label": 0
                },
                {
                    "sent": "So this is so, this should be the log joint distribution, not the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do in Bayesian inference is rather than optimizing across those parameters, we want to sort of integrate them out.",
                    "label": 0
                },
                {
                    "sent": "We want to get rid of them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What's going on in the field very recently, over the last four or five years?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is actually 2007 this graph, so I don't know if you can quite read, but if you see the top here, it says 2007.",
                    "label": 0
                },
                {
                    "sent": "This is 1986, which is rather conveniently.",
                    "label": 0
                },
                {
                    "sent": "I guess when the backdrop papers written.",
                    "label": 0
                },
                {
                    "sent": "So about sorry about these sort of times.",
                    "label": 0
                },
                {
                    "sent": "Convents were developed and then by this sort of.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines were dominating nips.",
                    "label": 0
                },
                {
                    "sent": "What happened was in this.",
                    "label": 0
                },
                {
                    "sent": "Here, of course we just did not have enough data to get confidence working.",
                    "label": 0
                },
                {
                    "sent": "I know there's a lot of stories about conspiracies and winters and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "But it was quite simple.",
                    "label": 0
                },
                {
                    "sent": "There were papers that showed you could get equal or better performance with support vector machines on large datasets like EM lists.",
                    "label": 0
                },
                {
                    "sent": "Then you could with convolutional neural networks and those models were easier to understand from a modeling perspective, not from an algorithmic perspective.",
                    "label": 0
                },
                {
                    "sent": "From a modeling perspective.",
                    "label": 0
                },
                {
                    "sent": "So the community switched.",
                    "label": 0
                },
                {
                    "sent": "Now the shocking thing is when that came back the other way with the image net results from 2012, the Community switch back.",
                    "label": 0
                },
                {
                    "sent": "Now to me that this seems like a community being sensible about what methods it's interested in, but you can put your narrative.",
                    "label": 0
                },
                {
                    "sent": "So what have you like and the reason why, of course, is because of this massive explosion of digital data, and we're really seeing that these these methods are tending to dominate in areas where we have an enormous amount of data like computer vision, speech language.",
                    "label": 0
                },
                {
                    "sent": "That's really, really thrilling.",
                    "label": 0
                },
                {
                    "sent": "'cause we're doing some amazing things that we couldn't do before.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussian process well in a Gaussian process we take a totally different approach to modeling, or at least it appears like it's totally different approach to modeling.",
                    "label": 0
                },
                {
                    "sent": "So these slides I was watching.",
                    "label": 0
                },
                {
                    "sent": "Um, I guess this morning lecture by David.",
                    "label": 0
                },
                {
                    "sent": "From let's see I've lost Chrome.",
                    "label": 0
                },
                {
                    "sent": "Let me just get that up.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I had to shut down and restart and forgot to find these.",
                    "label": 0
                },
                {
                    "sent": "So I was watching this Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "I mean, I do a lot of educational Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Including summer schools, and this is a video lecture, so we got video lectures today, so this is a meter video lecture.",
                    "label": 0
                },
                {
                    "sent": "This is one of the most extraordinary explanations you'll ever see of Gaussian process is when I watch this video back, I see David saying things that I say today that I don't even remember originated in David's idea of how you present them.",
                    "label": 0
                },
                {
                    "sent": "This is actually I sort of have it at this frame here, because this is me.",
                    "label": 0
                },
                {
                    "sent": "This is my head in the front of the auditorium there, and he's just chatting to me.",
                    "label": 0
                },
                {
                    "sent": "Just says to me, Neil, he's teaching his exercises in the middle of an hour long talk to make sure people understand how Gaussians work and he's just saying.",
                    "label": 0
                },
                {
                    "sent": "Oh nails done the 5th exercise, so the rest of you must have finished.",
                    "label": 0
                },
                {
                    "sent": "Making a joke at my expense that this is an extraordinary tutorial and Gaussian processes, and I I recommend anyone watch it.",
                    "label": 0
                },
                {
                    "sent": "It's got 40,000 views over the last.",
                    "label": 0
                },
                {
                    "sent": "It's 10 years old, next month actually, and it's got 40,000 views over the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "I mean why it doesn't have 4 million views?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It is an example of how to teach.",
                    "label": 0
                },
                {
                    "sent": "It's an example of how to teach Gaussian processes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me just pause in there so we don't get it coming through.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is actually a slide that's kind of inspired a bit by that tutorial, but it's a sort of shortcut.",
                    "label": 0
                },
                {
                    "sent": "So what is a Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "Well, this what I show you here is actually a Gaussian distribution, and I visualize the covariance matrix here for 25 dimensional Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a 25 by 25 distribution.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, Python seems to smooth out the pixels.",
                    "label": 0
                },
                {
                    "sent": "You should see this more pixelated than it is, and then I've taken 1 sample 1 sample from this Gaussian distribution and I'm showing you this sample.",
                    "label": 0
                },
                {
                    "sent": "On the left hand side.",
                    "label": 0
                },
                {
                    "sent": "Now because of the structure of this covariance matrix, because nearby points to each other are strongly correlated, we see that the samples are very strongly correlated if their neighboring in this index.",
                    "label": 0
                },
                {
                    "sent": "Now the idea of a Gaussian process this is a distribution and a single sample from a distribution.",
                    "label": 0
                },
                {
                    "sent": "But in a process.",
                    "label": 0
                },
                {
                    "sent": "We actually create this covariance matrix for covariance function, so for any two given inputs, I put time in here, but these could be spatial inputs.",
                    "label": 0
                },
                {
                    "sent": "We can compute the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So we compute this covariance for any given finite set of points, and we can sample over functions in exactly the same way, and it's an incredibly powerful approach to dealing with nonlinear functions, because in some sense, yet the functions are nonlinear, but the system itself is based on Gaussians, so you can do all sorts of interesting operations like you can jointly model derivatives and the function itself.",
                    "label": 0
                },
                {
                    "sent": "You can model integrals in the function itself, so very powerful model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way I like to think of that and this is really a diagram for Bayesian inference, not just Gaussian process is is that you have a prior over functions that you start with.",
                    "label": 0
                },
                {
                    "sent": "So just like we had a prior over the weights in the neural network case, now we've got a prior over functions.",
                    "label": 0
                },
                {
                    "sent": "We make some observations of some data points, two data points.",
                    "label": 0
                },
                {
                    "sent": "Here we combine it with the likelihood.",
                    "label": 0
                },
                {
                    "sent": "The likelihood says how much noise there is on the functions and we basically throw away all functions that don't fit.",
                    "label": 0
                },
                {
                    "sent": "So the way I generated this plot was just like in the previous plot.",
                    "label": 0
                },
                {
                    "sent": "I sampled like 100 or 1000, I can't remember.",
                    "label": 0
                },
                {
                    "sent": "Gaussian samples and then I've thrown away those samples that don't go through these two points and this is what we call the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is our posterior representation of what we believe about this function.",
                    "label": 0
                },
                {
                    "sent": "Now, given our initial belief about what the functions look like and the data and the thing that David introduced into the machine learning community will certainly to me.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think others were talking about it.",
                    "label": 0
                },
                {
                    "sent": "But did the most to his 1992 thesis.",
                    "label": 0
                },
                {
                    "sent": "Is just for neural computation papers stapled together so you can either get the issue of neural computation or you can get his thesis and the contents basically the same.",
                    "label": 0
                },
                {
                    "sent": "He did his PhD thesis and I some minimal amount of time.",
                    "label": 0
                },
                {
                    "sent": "It's all about how you apply these methods in neural networks using something called the Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "I was reading it again this morning.",
                    "label": 0
                },
                {
                    "sent": "It's incredible insight for that.",
                    "label": 0
                },
                {
                    "sent": "I mean, David was one of the people who revolutionized the community.",
                    "label": 0
                },
                {
                    "sent": "I think I mean in terms of his ideas about how we should address these models, how we should control for complexity in this era of low data, it was really vital in changing the thinking.",
                    "label": 0
                },
                {
                    "sent": "Certainly was a massive influence on me.",
                    "label": 0
                },
                {
                    "sent": "He gave a tutorial on Gaussian process is at my first NIPS in 1997.",
                    "label": 0
                },
                {
                    "sent": "Which I think many people remember 'cause we didn't have video in those days where we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video but.",
                    "label": 0
                },
                {
                    "sent": "Not on the web.",
                    "label": 0
                },
                {
                    "sent": "We were down in that part of the plot.",
                    "label": 0
                },
                {
                    "sent": "I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, now the really interesting thing about this is that Radford Neal had shown, and This is why Gaussian processes came about Radford.",
                    "label": 0
                },
                {
                    "sent": "Neal, in his thesis, which I think was inspired partially by David.",
                    "label": 0
                },
                {
                    "sent": "He was doing some sampling.",
                    "label": 0
                },
                {
                    "sent": "Is that actually if we take that old neural network we had before and it doesn't matter how we sample from V, as long as we do finite variance samples?",
                    "label": 0
                },
                {
                    "sent": "What we get because of the central Limit Theorem is this output here, which is a Gaussian weighted sum of sum.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't matter how we sample from here.",
                    "label": 0
                },
                {
                    "sent": "If it's finite, it doesn't matter who we sample there for other reasons, but doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So we can do non Gaussian samples here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we take the hidden layer to Infinity as we increase the number of neurons in a neural network, then the neural net converges towards a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "There's some wonderful plots in Radford Neal's thesis where he does this sort of just by playing with very large neural networks and shows how these things start to look like Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Now, this immediately shows you one of the weaknesses of these models because the way you do it is you create the you take the hidden layer to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But of course you have to scale down the weights on the output.",
                    "label": 0
                },
                {
                    "sent": "You have to scale down the variance.",
                    "label": 0
                },
                {
                    "sent": "So no single activation function will dominate anymore, so you can't get this thing anymore where you get sort of a sudden step in your function and then goes off somewhere else, because that step must be equally likely to be present everywhere, and that's the sort of Gaussian assumption that these nonlinearities are sort of.",
                    "label": 0
                },
                {
                    "sent": "They can't say 101 single one can dominate.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, if you want to do that sort of thing, if you want to have nonparametrics for the other thing you have to look at Indian buffet process.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice for low data, these models are extremely important, so this is an example of this is a Javier Gonzalez who's sitting over there who's a postdoc in Sheffield with me, who's been looking at Bayesian optimization.",
                    "label": 0
                },
                {
                    "sent": "So alongside the 2012 image net paper, there's the sort of other paper that founded a company, founded a Twitter machine learning empire that is about how you can optimize.",
                    "label": 0
                },
                {
                    "sent": "Neural Nets using Gaussian processes as a proxy as a surrogate for optimization function, so this idea of Bayesian optimization.",
                    "label": 0
                },
                {
                    "sent": "So in this case your data is you sort of move around.",
                    "label": 0
                },
                {
                    "sent": "You try optimizing the neural net with a certain set of parameters, and then you look at the validation error for that, and then you approximate that function, which is a function from a set of parameters to validation error with the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Then you can move around.",
                    "label": 0
                },
                {
                    "sent": "So once you've got this, so this is what's going on in this top block here, but having a sort of visualizing it finding a minima.",
                    "label": 0
                },
                {
                    "sent": "At the start, though, there's very few points and things are uncertain as it moves forward.",
                    "label": 0
                },
                {
                    "sent": "I think it's missing a few frames at the beginning as it moves forward, it becomes more confident about where that memory is, as shown by the sort of activation function at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Now it's really, really important in this domain that you sustain uncertainty over the functions.",
                    "label": 0
                },
                {
                    "sent": "I like to sit there sometimes, say you know.",
                    "label": 0
                },
                {
                    "sent": "Why is it that when their training deep neural networks they use Gaussian process is to train them?",
                    "label": 0
                },
                {
                    "sent": "'cause if deep neural networks were so great they should just use deep neural networks?",
                    "label": 0
                },
                {
                    "sent": "Well, there's two possible answers.",
                    "label": 0
                },
                {
                    "sent": "It's either because young and you're sure have decided that they're worried about the singularity, and they know if y'all net trains another deep neural network then that's kind of going to lead to this cycle of ever increasing intelligence, and they're just using Gaussian process is to slow up the whole process.",
                    "label": 0
                },
                {
                    "sent": "Or I'll leave the rest your imagination.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of auto ML challenge where what Javier is down is if he's run Gaussian process is random forests on on one of these sort of training challenges and Gaussian process is typically do very, very well.",
                    "label": 0
                },
                {
                    "sent": "What we're also showing here is a deep Gaussian process which is one of the things I'll talk about today that's in the cyan alongside the yellow.",
                    "label": 0
                },
                {
                    "sent": "So that's a generalization of Galaxy.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process is and we see this sort of across different areas.",
                    "label": 0
                },
                {
                    "sent": "Random forests sometimes work well, but normally they are very much outperformed by Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how can we use these things in practice well?",
                    "label": 0
                },
                {
                    "sent": "So one of the things I've been working on in Sheffield is this idea of open data science and trying to realize that you know the use of data analysis.",
                    "label": 1
                },
                {
                    "sent": "I think you know developing world countries have far more to benefit from them.",
                    "label": 0
                },
                {
                    "sent": "We have in the developed world because you can ingrain data analysis in their systems as you build them, which is way better than trying to go back retrospectively and modify your entire health system as anyone who's worked in health data has worked out.",
                    "label": 0
                },
                {
                    "sent": "So one of the things we've been doing is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing Gaussian process fits for malaria incidents in in Uganda and this is a project with UN global pulse and.",
                    "label": 0
                },
                {
                    "sent": "John Queenan Martin McGuinness begin.",
                    "label": 0
                },
                {
                    "sent": "See who's in global pulse.",
                    "label": 0
                },
                {
                    "sent": "Based in Uganda what you've got here is actually there's a blue line, and as a yellow line so there's two fits.",
                    "label": 0
                },
                {
                    "sent": "One of the things you can do with the Gaussian process, you have separate length scales, so there's actually two of these covariances stuck together here, and what you're seeing is the model moving forward.",
                    "label": 0
                },
                {
                    "sent": "Once handling like short-term variation, which is just to do with blips of things going up and the other process is handling slow variation.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of understandability we want, because in this map here what we've got.",
                    "label": 0
                },
                {
                    "sent": "Is we're taking the idea so we can actually also find the gradient of the process, and we're trying to visualize where in Uganda in different states there's a crisis at any one moment with malaria.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Rather than showing too much that these are, this is Ricardo is my student.",
                    "label": 0
                },
                {
                    "sent": "He's now UCSF continuing to work on malaria.",
                    "label": 0
                },
                {
                    "sent": "Is John Quinn, who's at global Pulse and Martin, who's a global pulse.",
                    "label": 0
                },
                {
                    "sent": "So what they've actually got now is this is on stream for the UN, so this is an exact.",
                    "label": 0
                },
                {
                    "sent": "This isn't actually what the UN look at, but they have now a map that they can pull in data from across the states as it's reported to try and understand where diseases and where they need to dedicate resource is now.",
                    "label": 0
                },
                {
                    "sent": "Importantly, there's a lot of this is actually just visualization, so this is a really interesting thing.",
                    "label": 0
                },
                {
                    "sent": "When you work with real data, you find that most of your modeling is far less important than the way that the data is.",
                    "label": 0
                },
                {
                    "sent": "Visualized, but the thing that some Ricardo was able to do was to deal with missing reports in states using the Gaussian process models.",
                    "label": 0
                },
                {
                    "sent": "And it has a very significant effect on what the map looks like.",
                    "label": 0
                },
                {
                    "sent": "So where the crisis areas are, so you get a lot of missing reports in terms of health centers that didn't submit data for whatever reason that can be correlated.",
                    "label": 0
                },
                {
                    "sent": "And Ricardo used Gaussian processes for dealing with that.",
                    "label": 0
                },
                {
                    "sent": "You're just seeing a Fast forward overtime this.",
                    "label": 0
                },
                {
                    "sent": "This map is live and it's being used.",
                    "label": 0
                },
                {
                    "sent": "These sort of ideas were used to deal with the typhoid outbreak in Kampala in.",
                    "label": 0
                },
                {
                    "sent": "March not this year.",
                    "label": 0
                },
                {
                    "sent": "Last year, John couldn't attend a conference with us because he's dealing with at one point because there's an unfolding crisis in Ethiopia in terms of with drought and you can imagine.",
                    "label": 0
                },
                {
                    "sent": "OK, it's all very well as having loads of resources to deal with that.",
                    "label": 0
                },
                {
                    "sent": "But if you don't know where people are dying, it's kind of pointless and data can solve that right data.",
                    "label": 0
                },
                {
                    "sent": "We've got everyone kind of pretty much everyone in.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a sort of UN result.",
                    "label": 0
                },
                {
                    "sent": "There's more people have access to mobile phones than they have access to toilets.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort of worrying in terms of people's access for toilets.",
                    "label": 0
                },
                {
                    "sent": "But it also means that you can have an incredible effect by doing things on mobile phones, and increasingly smartphones.",
                    "label": 0
                },
                {
                    "sent": "You can pick up in Kenya or Uganda, a very nice smartphone for about $60.00.",
                    "label": 0
                },
                {
                    "sent": "Android, not iPhone.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's one sort of thing that we can do with these sort of models, and actually we're not the first to do that.",
                    "label": 0
                },
                {
                    "sent": "They're very widely used in spatial Epidemiology.",
                    "label": 0
                },
                {
                    "sent": "And we've collaborated with people like, well, talk to people like Peter Diggle and Peter gathering about.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know ways of taking advantage of these models?",
                    "label": 0
                },
                {
                    "sent": "This is another little example that I like, so this is a genome transcription situation where you've got an M RNA that operates a transcription factor, and then it's got a target M RNA and within the model what we've got is something we can't observe, which is this protein that's between the two, but we can infer it if we create these differential equations.",
                    "label": 0
                },
                {
                    "sent": "It turns out we can build a joint Gaussian process over these three functions, and we can infer the indirect thing in the middle, and we can do that because the properties of Gaussians under linear operators they stay Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's a nice example too.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a quote I remember David saying at the NIPS tutorial in 1997 to me he gave were at that time.",
                    "label": 0
                },
                {
                    "sent": "I was new to the field, I guess I've been in the field a year or 6, seven months or something now just over a year and.",
                    "label": 0
                },
                {
                    "sent": "We were very interested in the sort of questions people care about today, like how how big your neural network should be and and how you should train it and all these sort of things and the Gaussian process really seemed to solve that.",
                    "label": 0
                },
                {
                    "sent": "If you're in the area of small data, it really did solve that like we couldn't run it on EM list at the time, but emanus was like the largest, you know, Jan had spent ages collecting that data set a lot of money I'm guessing.",
                    "label": 0
                },
                {
                    "sent": "AT&T, or you know?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it was endless.",
                    "label": 0
                },
                {
                    "sent": "Yeah next year did.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so a lot of the data sets we were dealing with were much smaller and Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "It seemed like the obvious solution, but David even 1997 made this quote and it's in his tutorial on Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "He said have we thrown out the baby with the bathwater which was just to meet?",
                    "label": 1
                },
                {
                    "sent": "This totally freaked me out because as far as I had seen, he just solved every problem that I knew offers a problem in machine learning or haven't solved it.",
                    "label": 0
                },
                {
                    "sent": "Be given a tutorial showing how to solve it.",
                    "label": 0
                },
                {
                    "sent": "And then he said, but there's a problem with this and the problem he was saying is actually in the end of the day.",
                    "label": 0
                },
                {
                    "sent": "These are just smoothing machines.",
                    "label": 0
                },
                {
                    "sent": "Because of their single layer nature, they just smoothing things right.",
                    "label": 0
                },
                {
                    "sent": "You all you're doing in a very sophisticated way.",
                    "label": 0
                },
                {
                    "sent": "You're smoothing things like away and like it's amazing what you can do is moving.",
                    "label": 0
                },
                {
                    "sent": "That's true, but the structural things that we're interested in with neural networks that we want to get out.",
                    "label": 0
                },
                {
                    "sent": "If we really want to understand abstractions and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's of course why we come to deep learning.",
                    "label": 0
                },
                {
                    "sent": "So this is a slide ripped off from a deep face paper.",
                    "label": 0
                },
                {
                    "sent": "From Facebook and or I like to call it the Calista Flockhart detector.",
                    "label": 0
                },
                {
                    "sent": "So this is just trying to illustrate that actually what you've got in this system.",
                    "label": 0
                },
                {
                    "sent": "We can write down your arms linear threshold units and whatever else, but it's just a serial function.",
                    "label": 0
                },
                {
                    "sent": "The series of functions, so the actual output of the vector valued functions, vector valued functions, but functions nevertheless.",
                    "label": 0
                },
                {
                    "sent": "And the clever bit, and I remember watching and talk about this in 1997.",
                    "label": 0
                },
                {
                    "sent": "Also, the first time I saw David Speaker machine learning Generalization Summer School, which I think was a sort of big moment in the community.",
                    "label": 0
                },
                {
                    "sent": "The importance of.",
                    "label": 0
                },
                {
                    "sent": "I remember in what the time Jan was working on the check writing system, I think you're going to back propagate through the entire check writing system.",
                    "label": 0
                },
                {
                    "sent": "This is for AT&T, so Jan was always ahead of the game and knowing this thing was possible, but the basic idea with deep learning is to have things you can differentiate that you can differentiate through and then you compose them together, compose simple functions together to form complex functions.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the key, and for a mathematician the thing at the bottom is kind of meaningless.",
                    "label": 0
                },
                {
                    "sent": "Just write G of X, you save a lot of ink, but the class of functions is changing.",
                    "label": 0
                },
                {
                    "sent": "That's very important.",
                    "label": 0
                },
                {
                    "sent": "OK, do I want to say much about this slide, you know?",
                    "label": 0
                },
                {
                    "sent": "Probably not.",
                    "label": 0
                },
                {
                    "sent": "The moment maybe I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "'cause well, I thought I was going to see next.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe I'll have to.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm missing a slide now.",
                    "label": 0
                },
                {
                    "sent": "See the whole talk.",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "I'll say something about that slide.",
                    "label": 0
                },
                {
                    "sent": "Apparently, apparently the earlier Neal has put me in a difficult situation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now what we're trying to do, typically in Bayesian inference, is we're trying to deal with the parametric model in this way, so W Now represents our user RV's, and we have a joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And then we convert it to a marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "So what we're really interested in in Bayesian inference is the model that predicts what wise do we expect to get given some set of input X is.",
                    "label": 0
                },
                {
                    "sent": "That's what the Gaussian process defines.",
                    "label": 0
                },
                {
                    "sent": "It said it's a GPU with some corrupted noise on it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in practice, how do we deal with this?",
                    "label": 0
                },
                {
                    "sent": "One way is to deal with it using variational inference, because those integrals are very often not tractable, and I like this decomposition.",
                    "label": 0
                },
                {
                    "sent": "So variational inference says that instead of working with the log likelihood directly, this marginal likelihood that we're interested in, we typically lower bound it.",
                    "label": 0
                },
                {
                    "sent": "Now lower bounding this likelihood.",
                    "label": 0
                },
                {
                    "sent": "Has something like this form So what we do is we find an approximation to distributions over the parameters, so we would really like to know the true posterior, But that's often difficult to find.",
                    "label": 0
                },
                {
                    "sent": "So instead we seek an approximation QW, but that approximation so this lower bound turns out to be equal to the log likelihood minus a KL divergent.",
                    "label": 0
                },
                {
                    "sent": "So that's an information theoretic measure between this approximating distribution and the true posterior we're interested in, and this is a really beautiful thing because this decomp.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addition can then be written in this way, so this is the lower bound on the log.",
                    "label": 0
                },
                {
                    "sent": "Likelihood is given by an expected value appear.",
                    "label": 0
                },
                {
                    "sent": "Why, given W, X?",
                    "label": 0
                },
                {
                    "sent": "Expectation under Q of WR current belief about what the best distribution over parameters is.",
                    "label": 0
                },
                {
                    "sent": "Minus this KL divergences between Q of W&P of W. Now what this equation is saying is that this lower bound that we're looking at, which is tight if we allow full flexible forms for Q&W.",
                    "label": 0
                },
                {
                    "sent": "Is given by the expected log likelihood, so that is just the expected error function basically or the negative expected error function.",
                    "label": 1
                },
                {
                    "sent": "Mine is something that says that cube W should be close to our prior.",
                    "label": 0
                },
                {
                    "sent": "So basically there's a fight in these two terms between something that is saying fit your data.",
                    "label": 0
                },
                {
                    "sent": "The data term is the blue term and the orange term is fit your prior and this always emerges from this variational approximation and Dustin's going to probably give a few more details on that.",
                    "label": 0
                },
                {
                    "sent": "Well, I like about that is this is now an optimization problem, so variational methods term integrations into optimizations for us and then instead of working with the integrals, which are typically very intractable.",
                    "label": 0
                },
                {
                    "sent": "We tend to work with the.",
                    "label": 0
                },
                {
                    "sent": "Distributions over W if Q of W was a Delta function, then this would just be the standard thing.",
                    "label": 0
                },
                {
                    "sent": "People fit in neural Nets, but this term would actually then sort of be Infinity or something because it's gotten entropy in it, which we entropy of W and that would be ugly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of setup we're looking at in Bayesian inference.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another remark on David.",
                    "label": 0
                },
                {
                    "sent": "There's a Waterhouse Mackay paper that is probably the most staggering paper at this.",
                    "label": 0
                },
                {
                    "sent": "So he read Geoff Hinton had this work on doing this in neural networks.",
                    "label": 0
                },
                {
                    "sent": "Can't remember the name of the paper, but it was a it was Hinton Vancamp wasn't it?",
                    "label": 0
                },
                {
                    "sent": "I CML, I think, but he had this whole bits back argument about why this is a good thing to do, and David was very close to Jess lab and it's always been a great admirer of Jeff and he wrote this paper about doing variational inference in a freeform way where you make factorizing assumptions for Q&W.",
                    "label": 0
                },
                {
                    "sent": "And it's an absolute classic.",
                    "label": 0
                },
                {
                    "sent": "It's not cited half enough Waterhouse, and Mackay is one of them.",
                    "label": 0
                },
                {
                    "sent": "Is the 1st paper to say.",
                    "label": 0
                },
                {
                    "sent": "How I mean, we were then dominated by implementing that model for a number of years in the Community.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we've got this sort of thing that looks like the expected error function.",
                    "label": 0
                },
                {
                    "sent": "OK, so the Gaussian process is a little bit different from that because in a Gaussian process you have this difficulty that your prior is directly over the function, which is an infinite dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "So sustaining AQ distribution over an infinite dimensional thing is a little bit hard.",
                    "label": 0
                },
                {
                    "sent": "So we defined a covariance function which relates.",
                    "label": 0
                },
                {
                    "sent": "Our data to this covariance in here and then we define a likelihood that so that's our input data and then our target data is typically given by just the Gaussian noise or we have to do something more complicated for doing classification.",
                    "label": 0
                },
                {
                    "sent": "But if it's a Gaussian noise, everything's kind of analytic.",
                    "label": 0
                },
                {
                    "sent": "Now that graphical model looks rather different to the previous one, so the previous one.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Has this sort of parameters separate from the data, but marginalizing out?",
                    "label": 0
                },
                {
                    "sent": "In effect, you can see it as marginalizing out the parameters.",
                    "label": 0
                },
                {
                    "sent": "You get this effect here, where the prior itself is dependent on the inputs and this means that approximations in these models are other hub.",
                    "label": 0
                },
                {
                    "sent": "So, David Bly is done and others have many collaborators.",
                    "label": 0
                },
                {
                    "sent": "Hoffman and others, have done a lot of work and how you scale up inference in those classical models with something called stochastic variational inference.",
                    "label": 0
                },
                {
                    "sent": "Very important technique.",
                    "label": 0
                },
                {
                    "sent": "We're seeing work by Maxwell in Dark, Ingmar, Shakira, Mohammed Jenny, Lorenzo today.",
                    "label": 0
                },
                {
                    "sent": "All these sort of things are using those approximations and stochastic approximations in them, and they're all very exciting.",
                    "label": 0
                },
                {
                    "sent": "Of course, I hope they failed and Gaussian processes went out the day, but there's lots of people interested in these sort of probabilistic ways of pushing uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The trick that Wayne trick that we've done is to actually take this model and augment it so this is an odd sort of trick that we have to do in order to work at large scale.",
                    "label": 0
                },
                {
                    "sent": "You meant the model.",
                    "label": 0
                },
                {
                    "sent": "With some observations you never made.",
                    "label": 0
                },
                {
                    "sent": "Then you do a bit of manipulation in the model.",
                    "label": 0
                },
                {
                    "sent": "So now I've written rewritten this model.",
                    "label": 0
                },
                {
                    "sent": "I've taken this these things.",
                    "label": 0
                },
                {
                    "sent": "We never made.",
                    "label": 0
                },
                {
                    "sent": "The reason that this is true, because we can just trivially integrate out across peer review, and we get the original model.",
                    "label": 0
                },
                {
                    "sent": "But in this model here we now augment the model we introduce another prior.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Integrate over F. So this gives us a new model that looks awfully like the parametric model, and this trick is the key to scaling up Gaussian processes, because the thing I failed to mention is that the challenge with Gaussian processes computational complexity is cubic in the number of data you're looking at, so that was still OK when we had 1000 data points.",
                    "label": 0
                },
                {
                    "sent": "In fact, today you can do M list exactly 60,000 data, but you know you cannot do a million or billion because you've got cubic complexity and actually what hits you first is the quadratic storage complexity.",
                    "label": 0
                },
                {
                    "sent": "So these techniques here.",
                    "label": 0
                },
                {
                    "sent": "This sort of I don't have too much time to go through in detail, but there's papers by my car list.",
                    "label": 0
                },
                {
                    "sent": "It's yes, those papers.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a time at which David had sort of moved on to do energy, so there's no real work by David in this area.",
                    "label": 0
                },
                {
                    "sent": "But papers by Ed Snelson zooming Armani various people in, sort of how you apply these techniques.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the interesting thing about this relating to the previous talk that this you looks like a parameter, But there's this weird thing.",
                    "label": 1
                },
                {
                    "sent": "So in the previous talk we will talk about how we should change capacity of models according to our data set size.",
                    "label": 0
                },
                {
                    "sent": "Absolutely not.",
                    "label": 0
                },
                {
                    "sent": "We should never do that.",
                    "label": 0
                },
                {
                    "sent": "No?",
                    "label": 0
                },
                {
                    "sent": "I mean you could do that.",
                    "label": 0
                },
                {
                    "sent": "In practice you do it, but that nonparametric Bayesian answer to that is null.",
                    "label": 0
                },
                {
                    "sent": "Start with a super super complex model.",
                    "label": 0
                },
                {
                    "sent": "Deal with the uncertainty correctly.",
                    "label": 0
                },
                {
                    "sent": "And as you obtain more data, your uncertainty will gracefully collapse in the right way.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that means storing all the data, but this sort of.",
                    "label": 0
                },
                {
                    "sent": "Approximation here has this interesting parametric form.",
                    "label": 0
                },
                {
                    "sent": "Looks like a parametric model, but the weird thing is because it's giving us a lower bound in the end we can increase the dimensionality of you at runtime and that increases the tightness of our lower bound.",
                    "label": 0
                },
                {
                    "sent": "So we've got this tradeoff between when we want to sort of move these dimensionality of you around, which is a sort of very special thing which we have nowhere near exploited enough yet in trying to scale up these models.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the challenge for Gaussian process is if you want to do them in a deep way.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "The composition of functions appears is a chain of integrals.",
                    "label": 0
                },
                {
                    "sent": "When you write it down probabilistically.",
                    "label": 0
                },
                {
                    "sent": "So this is a Gaussian process over the first layer, F1.",
                    "label": 0
                },
                {
                    "sent": "Sorry I'm missing an F1 in here.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't we are missing a pair of F2 given F1?",
                    "label": 0
                },
                {
                    "sent": "But then you basically change through the process that in each process governs the process below it.",
                    "label": 0
                },
                {
                    "sent": "So these are functions.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to this.",
                    "label": 0
                },
                {
                    "sent": "Now that leads to a bunch of very serious nasty integrals that are difficult to do in practice.",
                    "label": 0
                },
                {
                    "sent": "But if we can do that paired integral there, it turns out that everything else sort of follows.",
                    "label": 0
                },
                {
                    "sent": "So we can chain it.",
                    "label": 0
                },
                {
                    "sent": "It just recurses through.",
                    "label": 0
                },
                {
                    "sent": "So we can apply bounds recursively to this system, and the innovation we had in this paper with my car listed C as we just had a Jamel our version of the paper come out last month with Andreas and we all know who's also here is how to do that for a one layer model.",
                    "label": 0
                },
                {
                    "sent": "Once you've got that you can put many layers together.",
                    "label": 0
                },
                {
                    "sent": "Why is this difficult?",
                    "label": 0
                },
                {
                    "sent": "Well because if you put a Gaussian distribution through a nonlinearity then you get something that's very non Gaussian and intractable.",
                    "label": 0
                },
                {
                    "sent": "So in general this distribution.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, it looks really cool and it can do lots of things, but it's also you can't normalize it, so you can't write down its likelihood exactly, but these variational techniques I've talked about.",
                    "label": 0
                },
                {
                    "sent": "We can use them to lower bound the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the objective here and people are suggesting other techniques.",
                    "label": 0
                },
                {
                    "sent": "So these models are catching on.",
                    "label": 0
                },
                {
                    "sent": "Some people are doing expectation propagation sampling, mixing in sampling.",
                    "label": 0
                },
                {
                    "sent": "There was a paper by Christos who's talking after me where they're doing some sampling approaches with parametric approximations inside yesterday.",
                    "label": 0
                },
                {
                    "sent": "So talk to him about that.",
                    "label": 0
                },
                {
                    "sent": "That seems to be working quite well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this in the end gives you something very interesting.",
                    "label": 0
                },
                {
                    "sent": "It gives you a new approach to forming stochastic process is so stochastic processes are very interesting classes of models 'cause we can sustain beliefs and probabilities over models, but they are often limited.",
                    "label": 1
                },
                {
                    "sent": "You can only write down certain forms, but what we're sort of saying here now is that these deep Gaussian processes are giving us the ability to do stochastic process composition.",
                    "label": 0
                },
                {
                    "sent": "So compose one process to another to form new process.",
                    "label": 0
                },
                {
                    "sent": "Of course, that thing is intractable, but we can start saying interesting things about it.",
                    "label": 0
                },
                {
                    "sent": "Properties, so there's a paper on the pathologies of deep learning, which I think isn't really about pathologies.",
                    "label": 0
                },
                {
                    "sent": "But it was a good way of getting it published where they analyze deep Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "As so trying to understand the way deep learning works, because if you understand each of these neural networks is an approximation to a Gaussian process, you can really start applying some theory onto how deep learning actually works in practice.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the sort of thing that we get if we fit these sort of models to data.",
                    "label": 0
                },
                {
                    "sent": "So here's a multilayer perception being fitted to only 20 three data points.",
                    "label": 0
                },
                {
                    "sent": "200 iterations of fit.",
                    "label": 0
                },
                {
                    "sent": "The fit looks quite good.",
                    "label": 0
                },
                {
                    "sent": "This is something called the motorcycle data.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is when you actually converge it.",
                    "label": 0
                },
                {
                    "sent": "Not great.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a Gaussian process fit?",
                    "label": 0
                },
                {
                    "sent": "Now what you see on this Gaussian process fit is it has to get short length scale to drop down.",
                    "label": 0
                },
                {
                    "sent": "Now yes, you might say there's some overfitting going on here, but bear in mind the function the model itself has these big error bars.",
                    "label": 0
                },
                {
                    "sent": "It's saying the function goes up and down all over the place potentially.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a deep Gaussian process, so the deep Gaussian process can warp the inputs, but it doesn't overfit.",
                    "label": 0
                },
                {
                    "sent": "It doesn't overfit, it just sort of a is actually doing.",
                    "label": 0
                },
                {
                    "sent": "You know, people would almost say that's Le.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overfit than that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would disagree with that, but that's the sort of semantic thing, and then this is a three layer.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussian process, so we've put three Gaussian processes together to do the fit here, so it's a sort of fully Bayesian thing where you get all these nice things, and these error bars and so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, indeed the two DEG.",
                    "label": 0
                },
                {
                    "sent": "The DGP with two layers happens to slightly outperform the three layer.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that's in the noise.",
                    "label": 0
                },
                {
                    "sent": "The three layer looks pretty good as well.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they perform kind of well on.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the thing that really interested me is how to do unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "This is how we started in this area.",
                    "label": 0
                },
                {
                    "sent": "So classical latent variable models.",
                    "label": 0
                },
                {
                    "sent": "Sort of of this form where you have some sort of latent space, and then you have a sort of an observed space.",
                    "label": 0
                },
                {
                    "sent": "In my analogy for this is like a puppet so you don't observe the puppeteer's hand, but you observe the motion of the figure underneath and you want to know what the puppeteer did.",
                    "label": 0
                },
                {
                    "sent": "Given the high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I'm a bit short of time, so I'm just going to run through a little bit quickly.",
                    "label": 0
                },
                {
                    "sent": "Some data is up on the screen on screen.",
                    "label": 0
                },
                {
                    "sent": "Very good.",
                    "label": 0
                },
                {
                    "sent": "So this is some motion capture data, so it's only.",
                    "label": 0
                },
                {
                    "sent": "It's just, I think, 45 frames or something, not even that of 102 dimensional data of a figure moving, so it's a 3D point cloud.",
                    "label": 0
                },
                {
                    "sent": "I love motion capture data because it's not a rare example of a data set that humans can visualize the high dimensions and they don't worry about it.",
                    "label": 0
                },
                {
                    "sent": "Now David did some great work that I just wanted to include quickly.",
                    "label": 0
                },
                {
                    "sent": "Where he actually did neural networks latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of that fit.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work great unfortunately, because in this low data it it won't work that well.",
                    "label": 0
                },
                {
                    "sent": "So this is a latent space.",
                    "label": 0
                },
                {
                    "sent": "This is the puppeteer space, and now I'm moving the figure around and you can see some of the motion is there, right?",
                    "label": 0
                },
                {
                    "sent": "So this is using a neural network to map from the latent space to the data space, and it's using important sampling.",
                    "label": 0
                },
                {
                    "sent": "He called it density Nets.",
                    "label": 0
                },
                {
                    "sent": "It was a really cool model.",
                    "label": 0
                },
                {
                    "sent": "It's way ahead of its time.",
                    "label": 0
                },
                {
                    "sent": "Actually, it was those density Nets that inspired me to do this with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens when you do it with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So let me turn the guy around so that you can see what these different parts of the model do.",
                    "label": 0
                },
                {
                    "sent": "So you can make the guy sort of lean up and down.",
                    "label": 0
                },
                {
                    "sent": "This works about 10 years old now and then you can get the running motion out.",
                    "label": 0
                },
                {
                    "sent": "But what we couldn't do in this initial model is learn the dimensionality of this latent space.",
                    "label": 0
                },
                {
                    "sent": "So what we did after that and that was the work I mentioned where we integrate over the inputs is we actually learn the dimensionality and this is a work of Andreas Damianou who I think to to implement this example where it actually decides that there's actually 4 latent dimensions controlling this figure.",
                    "label": 0
                },
                {
                    "sent": "The reason there's four I don't have too much time to Dick around, but there's one minute left apparently.",
                    "label": 0
                },
                {
                    "sent": "If I look at the third dimension here, so one of the dimensions is just to do with his starting position.",
                    "label": 0
                },
                {
                    "sent": "Right, so he starts there.",
                    "label": 0
                },
                {
                    "sent": "This dimension is one of his sort of leg movement.",
                    "label": 0
                },
                {
                    "sent": "And then the other dimensions in dimension two, dimension 4 rather beautifully completely deals with his angle of run.",
                    "label": 0
                },
                {
                    "sent": "So let me turn him around for that.",
                    "label": 0
                },
                {
                    "sent": "So that's the angle in which he runs, and it pulls all these things out on really really low data.",
                    "label": 0
                },
                {
                    "sent": "So the challenge for these sort of things is scaling them up.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know, um, so I've just been talking about that.",
                    "label": 0
                },
                {
                    "sent": "So the challenge?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is scaling them up 'cause we'd love to be able to do these sort of abstraction of different concepts so my.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example where we sort of do a layered model on motion capture.",
                    "label": 0
                },
                {
                    "sent": "This is an example where we.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your layered model on the handwritten digits, so we can learn the structure of these models but them.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name?",
                    "label": 0
                },
                {
                    "sent": "So we've actually started a company because scaling up these things you don't get academic papers for all the work you put in, as many people will be aware, I'm sure.",
                    "label": 0
                },
                {
                    "sent": "So we formed the company which is scaling these models up.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the really nice early results is what happens.",
                    "label": 0
                },
                {
                    "sent": "One of the problems you get when you scale up is massive sort of amounts of numerical noise.",
                    "label": 0
                },
                {
                    "sent": "So we totally redeveloped how we do these approximations and what this is showing here is with the new approach from inferential that we get rid of this numerical noise in the optimization, do an enormous amount better.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the end aim is to do this sort of thing to do deep health as I call it, had been calling it for a number of years where we try and assimilate all the different aspects of a human into one model.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can find out a lot more about Gaussian process by either attending our Gaussian process summer school or all the videos are online.",
                    "label": 0
                },
                {
                    "sent": "The slides and everything else.",
                    "label": 0
                },
                {
                    "sent": "So please have a look at that.",
                    "label": 0
                },
                {
                    "sent": "There's two posters, I TLR one on recurrent Gaussian process is so we've done recurrent neural networks with Gaussian process is an one of variational autoencoder, deep Gaussian processes, and then we have Python software.",
                    "label": 0
                },
                {
                    "sent": "And of course there's Davids Gaussian Process Basics talk, so I just want to end with you on a final thought.",
                    "label": 0
                },
                {
                    "sent": "For David, I would not be doing any of the things I'm doing today without David and I never coauthored a paper with him.",
                    "label": 0
                },
                {
                    "sent": "I just used to attend his group meetings in Cambridge.",
                    "label": 0
                },
                {
                    "sent": "'cause he was very welcoming to the person.",
                    "label": 0
                },
                {
                    "sent": "I mean, he was kind of penetrating.",
                    "label": 0
                },
                {
                    "sent": "If you thought you were wrong about something, but a lot of academics are like that and he was just a wonderful inspiration and I think it's a terrible tragedy that he leaves behind.",
                    "label": 0
                },
                {
                    "sent": "I think Tony's oldest is 4 and his youngest is 1 and those children will never benefit from direct interaction with such a famous man as adults.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}