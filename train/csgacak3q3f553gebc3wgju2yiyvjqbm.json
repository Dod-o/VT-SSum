{
    "id": "csgacak3q3f553gebc3wgju2yiyvjqbm",
    "title": "Direct Policy Ranking with Robot Data Streams",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Riad Akrour, INRIA Saclay - \u00cele-de-France"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Robotics"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_akrour_robot/",
    "segmentation": [
        [
            "So hi everyone, thank you for the few of you that came early in the morning, so I'm cool and I will present you.",
            "The worker represents you will be about preference learning and we also have some element anywhere forcement learning.",
            "So it was primarily motivated for application in small boutiques, but keep in mind that all that we will see can be applied to a wider."
        ],
        [
            "Random application.",
            "As soon as your goal is to learn a policy, so here is a general definition of policy.",
            "It's mapping from state space to election space as the policy will give you which action to do in each state.",
            "And the output of our algorithm will be a week expert.",
            "So what we mean by week expert?",
            "It's an expert that is not able to solve the problem, so it cannot just provide provide the solution of the problem and you try to mimic this solution.",
            "It can also not define a reward function, local it and say OK in this state.",
            "It is good to do this action or I'll ban.",
            "But if you give him two behaviors, he is able to tell it to say I prefer the first one over the second one.",
            "So as we don't have a reward function, we cannot use the reference machine learning, and as we don't have optimal policies to learn from, we cannot use an inversion for enforcement, learning and learning by imitation."
        ],
        [
            "So the motivation of this work was application to Swarm Robotics.",
            "And our requirement requirement was that this method has to learn directly over the robot so we don't have to use any ground truth like the position of the robot or anything that the robot it cannot be aware of from external sensor.",
            "But you just have to use the internal sensor of the robot.",
            "The reason of that is to avoid reality gap when you use simulators and informative."
        ],
        [
            "Teachers so OK.",
            "This was our context, so if we wanted to use.",
            "Methods from the literature.",
            "We could think of reinforcement learning, so we might want.",
            "In this case we would have to define a real function that says how good it is to do a specific action in a specific state.",
            "And the optimal policy then to learn from this function with one maximizing the expectation of the sum of this country well.",
            "So in some cases it is easy to define this word function and it was in the games where you can just win or lose this obvious divine involved.",
            "But in robotics application, in our case where you have continuous state action space, it may be more difficult to camp with."
        ],
        [
            "The handcrafted we want and to illustrate this, let's see standard where forcement learning task where the robot have to navigate through the maze.",
            "But instead of knowing his position and having just four action going up, down, left and right, he had just infrared sensors and he had to give real motor command like rotation and translation speed.",
            "And so your state action space will look more like that, and to definer out over this is a bit more difficult, and you also have the problem of partial observability as we cannot really know where you are in the green state.",
            "And in the goal state so."
        ],
        [
            "It is more difficult when you don't have a formal features.",
            "So.",
            "So what we could also use when we don't have well function is to learn it and to use apprenticeship learning.",
            "So in this case you have optimal policy and you try to uncover the policy that generated them.",
            "So there is many ways to do that, but it ever requires this optimal policies and."
        ],
        [
            "Swarm robotics.",
            "It is also difficult to show optimal policy to swarm of robots, As for instance, in this case we have 100 of robots and how you do to show what you want to do this one."
        ],
        [
            "So what our proposal preference based policy learning is a simple three step iteration process so.",
            "We ask first an expert to express some preferences over demonstrated policies.",
            "And from this demonstration we will learn a policy return estimate.",
            "Will see what it is in the next slide, and from this policy returning stemate it will help us in the self training phase.",
            "We will optimize this policy return estimate with an exploration term and come with a new policy that will be shown to the expert and we will iterate this process until."
        ],
        [
            "Express satisfaction so after some background we see into more details our algorithm so we will show what is this policy returns teammate.",
            "We see that we need also to enforce some exploration to come with a group policy and we see what will be the."
        ],
        [
            "You see that we will show in each iteration.",
            "So policy return estimate is a simple scoring function to guide the search of a policy.",
            "It is a linear function with the Learn parameter Omega or W and.",
            "Mu, which describes the trajectory.",
            "And it is learning from the constraint of the experts, such as if experts prefer and policy mu I to polishing UJ.",
            "You have to give a higher rank.",
            "2 new I Atlanta New Jay and to learn this we use standard learning to rank approaches such as defined in joaquins 1005."
        ],
        [
            "So the problem is not really how to learn this function, but on which space to learn it.",
            "So the goal is to find the parameter of the policy.",
            "And if you try to learn this model directly on the parameters of the policy.",
            "The problem is that this space is missing.",
            "So for instance, if you have your controller is under a network.",
            "And you can see that a small change in the parameters can lead to.",
            "The great change in behavior and also you have some invariant property of neural network.",
            "If you divide by any constant it will still remain the same policies, so it is unlikely that this function is linear.",
            "And this is also consistent in presence of noise as one parameter can lead to many different trajectories.",
            "So instead of learning this policy return estimate in the parametric space, we use."
        ],
        [
            "Representation.",
            "That is built over the sensory motor flow.",
            "So from a trajectory you have a flow of sensor and motor commands.",
            "And then you will contest that using a clustering algorithm and the verbal representation will be the histogram of visited sensorimotor state means the number of time global spend in each of the clusters.",
            "And if you look at the calculus and you name the reward as being the.",
            "The result of that states D and action T as being declared.",
            "The component of the vector of the cluster that contains state state at time T. An action authenti your policy returns teammates will look very close to Q value of at find it or isn't."
        ],
        [
            "OK, so now that we have this policy return estimate and we can tell how likely will be the expert to like the trajectory you have to notice that this policy return estimate is only defined on the sensorimotor state that were previously shown that contained in the previous.",
            "Policies, so if you have a sensory motor that is new, you have no information overheads, so you need a way to enforce some exploration.",
            "And to do that, we have an archive of already demonstrated policies.",
            "And we calculate an exploration metric, which is the minimal distance between our trajectory and all the trajectories of the archives.",
            "And this will tell us how novel is a policy compared to the."
        ],
        [
            "The rest.",
            "So in the next steps in the self training phase two come with a new policy will maximize this summer.",
            "Some of extra exploitation Turman exploration term?",
            "And Alpha here make the balance between the two.",
            "So to maximize that, you cannot use graduates method as you don't have the control over.",
            "You only have the control over the parameter of the policy.",
            "You can modify the parameter of the policy, but you don't have direct access to the behavior, so we use unstead black box optimization algorithm.",
            "And we update the Alpha that makes the balance between the exploitation and exploration term in an online fashion.",
            "As if the new policy that you proposed.",
            "Makes an improvement if it is not dominated by any other policy in the archive you increase the exploration.",
            "Two and hope to go to converge faster, and if it is dominated by a previous policy, you decrease exploration."
        ],
        [
            "Trade more on exploitation.",
            "So here is the algorithm.",
            "This is the summary of all resource, so you start with a random policy and then you optimize this as this sum of exploitation exploration term.",
            "That gives you a new policy.",
            "You will ask the experts to rank it with.",
            "Policy of the iPad, and from that you will learn your the new policy return estimate.",
            "And then you will update Alpha.",
            "In the way we explained before."
        ],
        [
            "OK, so."
        ],
        [
            "We will pass on to some experiments.",
            "So the experiments we made one on one and two robots each of 1 H of each one of them have 8 proximity sensor that tell him how far is the closest object, if any.",
            "And the robot have two motor commands without the rotation and translation speed.",
            "The robots are controlled by a one Hydra 1 hidden layer layer feedforward neural network which have 121 para meters.",
            "And to make the experiment, we used the publicly available simulator Oval, but keep in mind that we do not make use of any of the ground truth available in this simulator, but we just use the internal sensor of the robot.",
            "They only use of the ground.",
            "Truth is for emulating the expert preferences.",
            "Oh, and all the results that will be presented are operated over 41 runs.",
            "So we compiled against a variance variant of our algorithm where we don't where we learn the policy return estimate on the parametric space.",
            "Instead of we have behavior representation.",
            "What is the comparative against the expert only setting where you basically plug in the evolutionary algorithm and the function that you optimizes the expert preferences?",
            "And finally to novelty search, which may tell something to people in evolutionary robotics, which basically just enforce exploration."
        ],
        [
            "So in the first experiment.",
            "The goal is of course to find away the shortest way to the goal state.",
            "And you can see that our method in purple performs much better than the expert on this setting.",
            "And that the when you learn the policy return estimate over the parameter of the neural network, you basically see no improvements over the expert only setting.",
            "And novelty search.",
            "Brings no improvement at all due to a very large search space.",
            "And we can say that we need approximately 4 in average 40.",
            "To show to demonstrate faulty policy to the expert.",
            "Until we reach the Green Zone and all the other methods do not even reach degrees on after 100.",
            "Demonstrated policy."
        ],
        [
            "For the second experiment, which is quite more difficult.",
            "Where the goal is to explore the maze, but at the constraint that the robot must be close to each other.",
            "So to emulate the express reference here, we count the number of tears that the Robot Explorer, while being under minimal distance and the difficulty, arises from the fact that with just with the sensors, a robot cannot distinguish a wall or any other object to another robot.",
            "So I basically cannot very know if he's close to a robot or not.",
            "And the conclusion are the same at the difference that our method performs even better."
        ],
        [
            "So to recap our methods so.",
            "What we need is or what we actually can cope with.",
            "The weak expert that is just in a format outside there.",
            "We it works in partially observable sitting.",
            "And it is affordable with respect and human effort.",
            "And in the bad side.",
            "The self training phase is time consuming and in the context of robotics it is energy consuming.",
            "It means that you have to change the battery in the which at the end will cost human efforts.",
            "So there may be some way for."
        ],
        [
            "Improvements here.",
            "As for future work, we have three major leads which are.",
            "To see that the expert may only prefer just a part of the trajectory or assume behavior Anstead of the water victory.",
            "So maybe we can cut the learning on the multiple instance framework.",
            "Also we can add the hierarchy to the clustering algorithm and decided to go and link it to the exploitation exploration team the dilemma and decided to go further in deeper in the sensory motor states where we want fine grained details and we want to duplicate better policy for exploitation or we want wider less details for wider exploration.",
            "As for the last month, and as we stated before, we can also try to improve the self training phase and maybe CW as a reference underwear forcement learning reward and try to combine the black box optimization with some policy improvement."
        ],
        [
            "OK."
        ],
        [
            "And this is a thank you for attention and are pleased to answer question.",
            "So.",
            "I think it's a.",
            "Gun show so.",
            "Basically, how sensitive is the algorithm to the length of the demonstrations of the key to the expert?",
            "As to the excellent work that you use in the clustering."
        ],
        [
            "So how is something we did not made explicit?",
            "Experiment on the size of the of the epsilon so."
        ],
        [
            "I have no clear answer to that, but yeah, of course we don't have to use a two big.",
            "In other words, everything will look the same, and if you use it too small epsilon, you will have too much features.",
            "The space will be too big.",
            "And for the second question, it was for how long to the demo station?",
            "Yeah, well it will be.",
            "It has to be long enough so the robot can solve the task physically and apart from that.",
            "There are no difference between a rare form of learning task or any other things.",
            "You have to give enough time to for the robot to solve the task.",
            "That's it.",
            "My first question is, when you make this map here to the histogram whether you don't lose important information because it seems to me that you lose information about the temporal order and the second question regarding the effort or difficulty for the expert.",
            "You say it's affordable.",
            "Of course I believe, but still I can't imagine that if an expert has to compare whole sequence whole trajectory's, especially if some of them have been in the past.",
            "And yes to remember them and so on.",
            "If I said it quite difficult to say, OK, I like this one more than that one.",
            "So I have for the last of sample affirmation.",
            "Of course, it occurs as it occurs in a reference point.",
            "Learning what you could do in the future work would be maybe to use kernel in the SBM.",
            "For instance polynomial kernel, where.",
            "Your reward in this map, the space your reward will be defined on projects of of state so you don't see only one state, but you see a sequence of state.",
            "We could do that.",
            "And for, uh, yeah.",
            "The difficulty of comparing what we think we would do in practice is not really compared to buy to a policy, but more give grades to a policy.",
            "Even if it is approximate.",
            "And from this set of grades you can then use the order in the preferences."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hi everyone, thank you for the few of you that came early in the morning, so I'm cool and I will present you.",
                    "label": 0
                },
                {
                    "sent": "The worker represents you will be about preference learning and we also have some element anywhere forcement learning.",
                    "label": 0
                },
                {
                    "sent": "So it was primarily motivated for application in small boutiques, but keep in mind that all that we will see can be applied to a wider.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Random application.",
                    "label": 0
                },
                {
                    "sent": "As soon as your goal is to learn a policy, so here is a general definition of policy.",
                    "label": 0
                },
                {
                    "sent": "It's mapping from state space to election space as the policy will give you which action to do in each state.",
                    "label": 0
                },
                {
                    "sent": "And the output of our algorithm will be a week expert.",
                    "label": 0
                },
                {
                    "sent": "So what we mean by week expert?",
                    "label": 0
                },
                {
                    "sent": "It's an expert that is not able to solve the problem, so it cannot just provide provide the solution of the problem and you try to mimic this solution.",
                    "label": 1
                },
                {
                    "sent": "It can also not define a reward function, local it and say OK in this state.",
                    "label": 1
                },
                {
                    "sent": "It is good to do this action or I'll ban.",
                    "label": 0
                },
                {
                    "sent": "But if you give him two behaviors, he is able to tell it to say I prefer the first one over the second one.",
                    "label": 1
                },
                {
                    "sent": "So as we don't have a reward function, we cannot use the reference machine learning, and as we don't have optimal policies to learn from, we cannot use an inversion for enforcement, learning and learning by imitation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the motivation of this work was application to Swarm Robotics.",
                    "label": 1
                },
                {
                    "sent": "And our requirement requirement was that this method has to learn directly over the robot so we don't have to use any ground truth like the position of the robot or anything that the robot it cannot be aware of from external sensor.",
                    "label": 0
                },
                {
                    "sent": "But you just have to use the internal sensor of the robot.",
                    "label": 0
                },
                {
                    "sent": "The reason of that is to avoid reality gap when you use simulators and informative.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Teachers so OK.",
                    "label": 0
                },
                {
                    "sent": "This was our context, so if we wanted to use.",
                    "label": 0
                },
                {
                    "sent": "Methods from the literature.",
                    "label": 0
                },
                {
                    "sent": "We could think of reinforcement learning, so we might want.",
                    "label": 1
                },
                {
                    "sent": "In this case we would have to define a real function that says how good it is to do a specific action in a specific state.",
                    "label": 0
                },
                {
                    "sent": "And the optimal policy then to learn from this function with one maximizing the expectation of the sum of this country well.",
                    "label": 0
                },
                {
                    "sent": "So in some cases it is easy to define this word function and it was in the games where you can just win or lose this obvious divine involved.",
                    "label": 1
                },
                {
                    "sent": "But in robotics application, in our case where you have continuous state action space, it may be more difficult to camp with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The handcrafted we want and to illustrate this, let's see standard where forcement learning task where the robot have to navigate through the maze.",
                    "label": 0
                },
                {
                    "sent": "But instead of knowing his position and having just four action going up, down, left and right, he had just infrared sensors and he had to give real motor command like rotation and translation speed.",
                    "label": 0
                },
                {
                    "sent": "And so your state action space will look more like that, and to definer out over this is a bit more difficult, and you also have the problem of partial observability as we cannot really know where you are in the green state.",
                    "label": 0
                },
                {
                    "sent": "And in the goal state so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is more difficult when you don't have a formal features.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we could also use when we don't have well function is to learn it and to use apprenticeship learning.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have optimal policy and you try to uncover the policy that generated them.",
                    "label": 0
                },
                {
                    "sent": "So there is many ways to do that, but it ever requires this optimal policies and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Swarm robotics.",
                    "label": 0
                },
                {
                    "sent": "It is also difficult to show optimal policy to swarm of robots, As for instance, in this case we have 100 of robots and how you do to show what you want to do this one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what our proposal preference based policy learning is a simple three step iteration process so.",
                    "label": 0
                },
                {
                    "sent": "We ask first an expert to express some preferences over demonstrated policies.",
                    "label": 1
                },
                {
                    "sent": "And from this demonstration we will learn a policy return estimate.",
                    "label": 0
                },
                {
                    "sent": "Will see what it is in the next slide, and from this policy returning stemate it will help us in the self training phase.",
                    "label": 0
                },
                {
                    "sent": "We will optimize this policy return estimate with an exploration term and come with a new policy that will be shown to the expert and we will iterate this process until.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Express satisfaction so after some background we see into more details our algorithm so we will show what is this policy returns teammate.",
                    "label": 0
                },
                {
                    "sent": "We see that we need also to enforce some exploration to come with a group policy and we see what will be the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see that we will show in each iteration.",
                    "label": 0
                },
                {
                    "sent": "So policy return estimate is a simple scoring function to guide the search of a policy.",
                    "label": 1
                },
                {
                    "sent": "It is a linear function with the Learn parameter Omega or W and.",
                    "label": 0
                },
                {
                    "sent": "Mu, which describes the trajectory.",
                    "label": 0
                },
                {
                    "sent": "And it is learning from the constraint of the experts, such as if experts prefer and policy mu I to polishing UJ.",
                    "label": 0
                },
                {
                    "sent": "You have to give a higher rank.",
                    "label": 1
                },
                {
                    "sent": "2 new I Atlanta New Jay and to learn this we use standard learning to rank approaches such as defined in joaquins 1005.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is not really how to learn this function, but on which space to learn it.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to find the parameter of the policy.",
                    "label": 0
                },
                {
                    "sent": "And if you try to learn this model directly on the parameters of the policy.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this space is missing.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have your controller is under a network.",
                    "label": 0
                },
                {
                    "sent": "And you can see that a small change in the parameters can lead to.",
                    "label": 0
                },
                {
                    "sent": "The great change in behavior and also you have some invariant property of neural network.",
                    "label": 0
                },
                {
                    "sent": "If you divide by any constant it will still remain the same policies, so it is unlikely that this function is linear.",
                    "label": 0
                },
                {
                    "sent": "And this is also consistent in presence of noise as one parameter can lead to many different trajectories.",
                    "label": 1
                },
                {
                    "sent": "So instead of learning this policy return estimate in the parametric space, we use.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "That is built over the sensory motor flow.",
                    "label": 0
                },
                {
                    "sent": "So from a trajectory you have a flow of sensor and motor commands.",
                    "label": 0
                },
                {
                    "sent": "And then you will contest that using a clustering algorithm and the verbal representation will be the histogram of visited sensorimotor state means the number of time global spend in each of the clusters.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the calculus and you name the reward as being the.",
                    "label": 0
                },
                {
                    "sent": "The result of that states D and action T as being declared.",
                    "label": 0
                },
                {
                    "sent": "The component of the vector of the cluster that contains state state at time T. An action authenti your policy returns teammates will look very close to Q value of at find it or isn't.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that we have this policy return estimate and we can tell how likely will be the expert to like the trajectory you have to notice that this policy return estimate is only defined on the sensorimotor state that were previously shown that contained in the previous.",
                    "label": 0
                },
                {
                    "sent": "Policies, so if you have a sensory motor that is new, you have no information overheads, so you need a way to enforce some exploration.",
                    "label": 0
                },
                {
                    "sent": "And to do that, we have an archive of already demonstrated policies.",
                    "label": 1
                },
                {
                    "sent": "And we calculate an exploration metric, which is the minimal distance between our trajectory and all the trajectories of the archives.",
                    "label": 0
                },
                {
                    "sent": "And this will tell us how novel is a policy compared to the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rest.",
                    "label": 0
                },
                {
                    "sent": "So in the next steps in the self training phase two come with a new policy will maximize this summer.",
                    "label": 0
                },
                {
                    "sent": "Some of extra exploitation Turman exploration term?",
                    "label": 0
                },
                {
                    "sent": "And Alpha here make the balance between the two.",
                    "label": 1
                },
                {
                    "sent": "So to maximize that, you cannot use graduates method as you don't have the control over.",
                    "label": 0
                },
                {
                    "sent": "You only have the control over the parameter of the policy.",
                    "label": 0
                },
                {
                    "sent": "You can modify the parameter of the policy, but you don't have direct access to the behavior, so we use unstead black box optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we update the Alpha that makes the balance between the exploitation and exploration term in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "As if the new policy that you proposed.",
                    "label": 0
                },
                {
                    "sent": "Makes an improvement if it is not dominated by any other policy in the archive you increase the exploration.",
                    "label": 0
                },
                {
                    "sent": "Two and hope to go to converge faster, and if it is dominated by a previous policy, you decrease exploration.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trade more on exploitation.",
                    "label": 0
                },
                {
                    "sent": "So here is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the summary of all resource, so you start with a random policy and then you optimize this as this sum of exploitation exploration term.",
                    "label": 0
                },
                {
                    "sent": "That gives you a new policy.",
                    "label": 0
                },
                {
                    "sent": "You will ask the experts to rank it with.",
                    "label": 0
                },
                {
                    "sent": "Policy of the iPad, and from that you will learn your the new policy return estimate.",
                    "label": 0
                },
                {
                    "sent": "And then you will update Alpha.",
                    "label": 0
                },
                {
                    "sent": "In the way we explained before.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will pass on to some experiments.",
                    "label": 0
                },
                {
                    "sent": "So the experiments we made one on one and two robots each of 1 H of each one of them have 8 proximity sensor that tell him how far is the closest object, if any.",
                    "label": 1
                },
                {
                    "sent": "And the robot have two motor commands without the rotation and translation speed.",
                    "label": 0
                },
                {
                    "sent": "The robots are controlled by a one Hydra 1 hidden layer layer feedforward neural network which have 121 para meters.",
                    "label": 0
                },
                {
                    "sent": "And to make the experiment, we used the publicly available simulator Oval, but keep in mind that we do not make use of any of the ground truth available in this simulator, but we just use the internal sensor of the robot.",
                    "label": 0
                },
                {
                    "sent": "They only use of the ground.",
                    "label": 1
                },
                {
                    "sent": "Truth is for emulating the expert preferences.",
                    "label": 0
                },
                {
                    "sent": "Oh, and all the results that will be presented are operated over 41 runs.",
                    "label": 1
                },
                {
                    "sent": "So we compiled against a variance variant of our algorithm where we don't where we learn the policy return estimate on the parametric space.",
                    "label": 0
                },
                {
                    "sent": "Instead of we have behavior representation.",
                    "label": 1
                },
                {
                    "sent": "What is the comparative against the expert only setting where you basically plug in the evolutionary algorithm and the function that you optimizes the expert preferences?",
                    "label": 0
                },
                {
                    "sent": "And finally to novelty search, which may tell something to people in evolutionary robotics, which basically just enforce exploration.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the first experiment.",
                    "label": 0
                },
                {
                    "sent": "The goal is of course to find away the shortest way to the goal state.",
                    "label": 1
                },
                {
                    "sent": "And you can see that our method in purple performs much better than the expert on this setting.",
                    "label": 0
                },
                {
                    "sent": "And that the when you learn the policy return estimate over the parameter of the neural network, you basically see no improvements over the expert only setting.",
                    "label": 0
                },
                {
                    "sent": "And novelty search.",
                    "label": 0
                },
                {
                    "sent": "Brings no improvement at all due to a very large search space.",
                    "label": 1
                },
                {
                    "sent": "And we can say that we need approximately 4 in average 40.",
                    "label": 1
                },
                {
                    "sent": "To show to demonstrate faulty policy to the expert.",
                    "label": 0
                },
                {
                    "sent": "Until we reach the Green Zone and all the other methods do not even reach degrees on after 100.",
                    "label": 0
                },
                {
                    "sent": "Demonstrated policy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the second experiment, which is quite more difficult.",
                    "label": 1
                },
                {
                    "sent": "Where the goal is to explore the maze, but at the constraint that the robot must be close to each other.",
                    "label": 0
                },
                {
                    "sent": "So to emulate the express reference here, we count the number of tears that the Robot Explorer, while being under minimal distance and the difficulty, arises from the fact that with just with the sensors, a robot cannot distinguish a wall or any other object to another robot.",
                    "label": 0
                },
                {
                    "sent": "So I basically cannot very know if he's close to a robot or not.",
                    "label": 0
                },
                {
                    "sent": "And the conclusion are the same at the difference that our method performs even better.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to recap our methods so.",
                    "label": 0
                },
                {
                    "sent": "What we need is or what we actually can cope with.",
                    "label": 0
                },
                {
                    "sent": "The weak expert that is just in a format outside there.",
                    "label": 0
                },
                {
                    "sent": "We it works in partially observable sitting.",
                    "label": 1
                },
                {
                    "sent": "And it is affordable with respect and human effort.",
                    "label": 1
                },
                {
                    "sent": "And in the bad side.",
                    "label": 0
                },
                {
                    "sent": "The self training phase is time consuming and in the context of robotics it is energy consuming.",
                    "label": 0
                },
                {
                    "sent": "It means that you have to change the battery in the which at the end will cost human efforts.",
                    "label": 0
                },
                {
                    "sent": "So there may be some way for.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Improvements here.",
                    "label": 0
                },
                {
                    "sent": "As for future work, we have three major leads which are.",
                    "label": 1
                },
                {
                    "sent": "To see that the expert may only prefer just a part of the trajectory or assume behavior Anstead of the water victory.",
                    "label": 1
                },
                {
                    "sent": "So maybe we can cut the learning on the multiple instance framework.",
                    "label": 0
                },
                {
                    "sent": "Also we can add the hierarchy to the clustering algorithm and decided to go and link it to the exploitation exploration team the dilemma and decided to go further in deeper in the sensory motor states where we want fine grained details and we want to duplicate better policy for exploitation or we want wider less details for wider exploration.",
                    "label": 1
                },
                {
                    "sent": "As for the last month, and as we stated before, we can also try to improve the self training phase and maybe CW as a reference underwear forcement learning reward and try to combine the black box optimization with some policy improvement.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is a thank you for attention and are pleased to answer question.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think it's a.",
                    "label": 0
                },
                {
                    "sent": "Gun show so.",
                    "label": 0
                },
                {
                    "sent": "Basically, how sensitive is the algorithm to the length of the demonstrations of the key to the expert?",
                    "label": 0
                },
                {
                    "sent": "As to the excellent work that you use in the clustering.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how is something we did not made explicit?",
                    "label": 0
                },
                {
                    "sent": "Experiment on the size of the of the epsilon so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have no clear answer to that, but yeah, of course we don't have to use a two big.",
                    "label": 0
                },
                {
                    "sent": "In other words, everything will look the same, and if you use it too small epsilon, you will have too much features.",
                    "label": 0
                },
                {
                    "sent": "The space will be too big.",
                    "label": 0
                },
                {
                    "sent": "And for the second question, it was for how long to the demo station?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well it will be.",
                    "label": 0
                },
                {
                    "sent": "It has to be long enough so the robot can solve the task physically and apart from that.",
                    "label": 0
                },
                {
                    "sent": "There are no difference between a rare form of learning task or any other things.",
                    "label": 0
                },
                {
                    "sent": "You have to give enough time to for the robot to solve the task.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "My first question is, when you make this map here to the histogram whether you don't lose important information because it seems to me that you lose information about the temporal order and the second question regarding the effort or difficulty for the expert.",
                    "label": 0
                },
                {
                    "sent": "You say it's affordable.",
                    "label": 0
                },
                {
                    "sent": "Of course I believe, but still I can't imagine that if an expert has to compare whole sequence whole trajectory's, especially if some of them have been in the past.",
                    "label": 0
                },
                {
                    "sent": "And yes to remember them and so on.",
                    "label": 0
                },
                {
                    "sent": "If I said it quite difficult to say, OK, I like this one more than that one.",
                    "label": 0
                },
                {
                    "sent": "So I have for the last of sample affirmation.",
                    "label": 0
                },
                {
                    "sent": "Of course, it occurs as it occurs in a reference point.",
                    "label": 0
                },
                {
                    "sent": "Learning what you could do in the future work would be maybe to use kernel in the SBM.",
                    "label": 0
                },
                {
                    "sent": "For instance polynomial kernel, where.",
                    "label": 0
                },
                {
                    "sent": "Your reward in this map, the space your reward will be defined on projects of of state so you don't see only one state, but you see a sequence of state.",
                    "label": 0
                },
                {
                    "sent": "We could do that.",
                    "label": 0
                },
                {
                    "sent": "And for, uh, yeah.",
                    "label": 0
                },
                {
                    "sent": "The difficulty of comparing what we think we would do in practice is not really compared to buy to a policy, but more give grades to a policy.",
                    "label": 0
                },
                {
                    "sent": "Even if it is approximate.",
                    "label": 0
                },
                {
                    "sent": "And from this set of grades you can then use the order in the preferences.",
                    "label": 0
                }
            ]
        }
    }
}