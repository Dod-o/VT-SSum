{
    "id": "f3l4jxk4ed57ggblatsjodtusxakmecg",
    "title": "PAC-Bayesian Learning and Domain Adaptation",
    "info": {
        "author": [
            "Pascal Germain, GRAAL, D\u00e9partement d'informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_germain_adaptation/",
    "segmentation": [
        [
            "So like the title suggests, I would present pack Bayesian learning approach to domain adaptation."
        ],
        [
            "First of all, I will do a quick overview of the domain adaptation and present classical demeanor that session bound after all, I will present a new domain adaptation bound well suited for the pack Bayesian learning.",
            "An after that I will present to our approach and the algorithm we made from it, and some experimental experimental results."
        ],
        [
            "So first of all, the domain adaptation is done when the learning distribution is different from the distribution.",
            "So for example, imagine that we want to do level picture pictures to know if there is a person or not on it, but our training set is concentrate by pictures coming from the web and our testing are.",
            "It pictures extracted from videos so we can presume that the two distributions, the one generating the image from the web and the one is generating the image from the videos, are quite the same but slightly different.",
            "So the big question is how to learn from the source domain.",
            "Whoever classifier or target domain."
        ],
        [
            "So a little bit formally, let's recall the classical supervised classification problem, so we will consider here the binary classification task, where X is an input space and Y is a liberal set.",
            "So the liberal here is minus one or plus one.",
            "We will denote PS the source demand an.",
            "I will offer news DS for the marginal distribution FPS.",
            "Over X and we have access to source label source sample so that I set for our algorithm to learn.",
            "So our objective is to learn the classifiers a classifier with low source risk that we will denote RPSH."
        ],
        [
            "In the nation we target the men that we PT.",
            "This is a distribution also over X * Y and I will send it out that the marginal distribution over X. Anne.",
            "D is a unlabeled target.",
            "Simple so objective here is to find the classifier H with root target too.",
            "So if we look at the procedure, we have a label distribution and non label distribution that generate two data sets.",
            "Our learning algorithm will use those data set to learn my model and finally we will learn we will use this classifier to classify.",
            "Open on this distribution."
        ],
        [
            "First coming adaptation bound is the one from Bendavid suggest in 2010 so.",
            "It says that the risk on the target dissolution of classifier H is lower than the risk on the source distribution.",
            "Plus Jason measure that I will talk about later an parameter Lambda, which is the lower sum of error between the source and the target distribution.",
            "So usually we will consider that.",
            "It's quite small, otherwise there is no learning in the figure that they shouldn't is impossible.",
            "So.",
            "To be my just bound would minimize those two terms where the first one is a near the classical expected.",
            "They were on the souls of men and just over second one is the.",
            "What will give it all the H the existence between between the source and the target domain?",
            "Tentatively, the this this this measure measure how much the classifier from there?",
            "Committee of classifier.",
            "Change this language between the source and the target domain.",
            "So in this picture we have the green dot that represents the tourist domain and range that represents the target domain.",
            "So here DH that would be big because we can separate the source and the target domain, but here it would be.",
            "It would be low because there's no possible way to distinguish between the two domains.",
            "So this bound only bound the error of a classifier H to apply Bayesian learning technique."
        ],
        [
            "We will use an averaging over all classifiers, so more specifically, we will consider a weight distribution rollover H that we will suggest the averaging of our classifiers in the pack Bayesian Ward.",
            "We call this the risk of the Gibbs classifier.",
            "And this gives a bound that looks similar to the bound the previous round, except that here we have.",
            "We can see it as a majority vote, so we have the risk the average risk of other classifiers that is upper bounded by the.",
            "This is the risk on the target distribution and this is lower than the average risk on the source distribution.",
            "Plus again, distance measure and a term landrew landrew is also quite similar to the one of the previous bound, but at this time it depends on on the road distribution, but we will say consider that if we are situation proper to domain adaptation this term is low, so.",
            "We will."
        ],
        [
            "Want to minimize what I call here?",
            "The big quantity, which is only the sum of the two first term we want to.",
            "We want to minimize this, but in the real world we have only access to a source and a target.",
            "Saint Paul so we want to.",
            "We will need to use."
        ],
        [
            "The Back Bay story 2.",
            "Two to minimize.",
            "To minimize this?",
            "OK, I will go quickly over a slide.",
            "Just I have run out of time, but just to say that in 2009 we developed an algorithm in the classical supervised supervised learning case where we minimize the eastbound specialized to linear classifiers and it turns out to be a tradeoff between two quantity.",
            "The the risk and that turns out to be the cigarette loss that raise the signal, either loss an regularizer that it's similar to the one of SVM."
        ],
        [
            "And if I go back to my domain adaptation."
        ],
        [
            "We develop a similar bound.",
            "Bound this big quantity here and enter it turns out to be a tradeoff between three quantity.",
            "The risk on the source domain, which is the red last year, the sigmoid loss.",
            "The disagreement between the target and source distribution that measures somehow.",
            "The the the the, the adaptation capability, and the commonweal riser.",
            "OK."
        ],
        [
            "I single quickly just to present you, we minimize the bound by the by gradient descent and we test our algorithm on toy problem, which is called the intertwining room.",
            "So here on the graphic the the red and green dots are the source domain, the source that I said it's always the same, but the target.",
            "Thus we want less if I is the.",
            "Some tools, but what they said from for certain angle so it so here we can see the decisionone there and we can see that it adapts somehow.",
            "We can see that.",
            "And see the fact that there's a kind of domain adaptation here, because the one day we is more interested, it's to classify the targets that I set an.",
            "You want to see the graphic.",
            "I welcome you to my post or that it will have at the end of the.",
            "So that's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So like the title suggests, I would present pack Bayesian learning approach to domain adaptation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, I will do a quick overview of the domain adaptation and present classical demeanor that session bound after all, I will present a new domain adaptation bound well suited for the pack Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "An after that I will present to our approach and the algorithm we made from it, and some experimental experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, the domain adaptation is done when the learning distribution is different from the distribution.",
                    "label": 1
                },
                {
                    "sent": "So for example, imagine that we want to do level picture pictures to know if there is a person or not on it, but our training set is concentrate by pictures coming from the web and our testing are.",
                    "label": 0
                },
                {
                    "sent": "It pictures extracted from videos so we can presume that the two distributions, the one generating the image from the web and the one is generating the image from the videos, are quite the same but slightly different.",
                    "label": 1
                },
                {
                    "sent": "So the big question is how to learn from the source domain.",
                    "label": 0
                },
                {
                    "sent": "Whoever classifier or target domain.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a little bit formally, let's recall the classical supervised classification problem, so we will consider here the binary classification task, where X is an input space and Y is a liberal set.",
                    "label": 1
                },
                {
                    "sent": "So the liberal here is minus one or plus one.",
                    "label": 0
                },
                {
                    "sent": "We will denote PS the source demand an.",
                    "label": 0
                },
                {
                    "sent": "I will offer news DS for the marginal distribution FPS.",
                    "label": 1
                },
                {
                    "sent": "Over X and we have access to source label source sample so that I set for our algorithm to learn.",
                    "label": 1
                },
                {
                    "sent": "So our objective is to learn the classifiers a classifier with low source risk that we will denote RPSH.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the nation we target the men that we PT.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution also over X * Y and I will send it out that the marginal distribution over X. Anne.",
                    "label": 1
                },
                {
                    "sent": "D is a unlabeled target.",
                    "label": 0
                },
                {
                    "sent": "Simple so objective here is to find the classifier H with root target too.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the procedure, we have a label distribution and non label distribution that generate two data sets.",
                    "label": 0
                },
                {
                    "sent": "Our learning algorithm will use those data set to learn my model and finally we will learn we will use this classifier to classify.",
                    "label": 0
                },
                {
                    "sent": "Open on this distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First coming adaptation bound is the one from Bendavid suggest in 2010 so.",
                    "label": 1
                },
                {
                    "sent": "It says that the risk on the target dissolution of classifier H is lower than the risk on the source distribution.",
                    "label": 1
                },
                {
                    "sent": "Plus Jason measure that I will talk about later an parameter Lambda, which is the lower sum of error between the source and the target distribution.",
                    "label": 0
                },
                {
                    "sent": "So usually we will consider that.",
                    "label": 0
                },
                {
                    "sent": "It's quite small, otherwise there is no learning in the figure that they shouldn't is impossible.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To be my just bound would minimize those two terms where the first one is a near the classical expected.",
                    "label": 0
                },
                {
                    "sent": "They were on the souls of men and just over second one is the.",
                    "label": 0
                },
                {
                    "sent": "What will give it all the H the existence between between the source and the target domain?",
                    "label": 0
                },
                {
                    "sent": "Tentatively, the this this this measure measure how much the classifier from there?",
                    "label": 0
                },
                {
                    "sent": "Committee of classifier.",
                    "label": 0
                },
                {
                    "sent": "Change this language between the source and the target domain.",
                    "label": 0
                },
                {
                    "sent": "So in this picture we have the green dot that represents the tourist domain and range that represents the target domain.",
                    "label": 0
                },
                {
                    "sent": "So here DH that would be big because we can separate the source and the target domain, but here it would be.",
                    "label": 0
                },
                {
                    "sent": "It would be low because there's no possible way to distinguish between the two domains.",
                    "label": 0
                },
                {
                    "sent": "So this bound only bound the error of a classifier H to apply Bayesian learning technique.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will use an averaging over all classifiers, so more specifically, we will consider a weight distribution rollover H that we will suggest the averaging of our classifiers in the pack Bayesian Ward.",
                    "label": 0
                },
                {
                    "sent": "We call this the risk of the Gibbs classifier.",
                    "label": 0
                },
                {
                    "sent": "And this gives a bound that looks similar to the bound the previous round, except that here we have.",
                    "label": 0
                },
                {
                    "sent": "We can see it as a majority vote, so we have the risk the average risk of other classifiers that is upper bounded by the.",
                    "label": 0
                },
                {
                    "sent": "This is the risk on the target distribution and this is lower than the average risk on the source distribution.",
                    "label": 0
                },
                {
                    "sent": "Plus again, distance measure and a term landrew landrew is also quite similar to the one of the previous bound, but at this time it depends on on the road distribution, but we will say consider that if we are situation proper to domain adaptation this term is low, so.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to minimize what I call here?",
                    "label": 0
                },
                {
                    "sent": "The big quantity, which is only the sum of the two first term we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize this, but in the real world we have only access to a source and a target.",
                    "label": 0
                },
                {
                    "sent": "Saint Paul so we want to.",
                    "label": 0
                },
                {
                    "sent": "We will need to use.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Back Bay story 2.",
                    "label": 0
                },
                {
                    "sent": "Two to minimize.",
                    "label": 0
                },
                {
                    "sent": "To minimize this?",
                    "label": 0
                },
                {
                    "sent": "OK, I will go quickly over a slide.",
                    "label": 0
                },
                {
                    "sent": "Just I have run out of time, but just to say that in 2009 we developed an algorithm in the classical supervised supervised learning case where we minimize the eastbound specialized to linear classifiers and it turns out to be a tradeoff between two quantity.",
                    "label": 0
                },
                {
                    "sent": "The the risk and that turns out to be the cigarette loss that raise the signal, either loss an regularizer that it's similar to the one of SVM.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if I go back to my domain adaptation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We develop a similar bound.",
                    "label": 0
                },
                {
                    "sent": "Bound this big quantity here and enter it turns out to be a tradeoff between three quantity.",
                    "label": 0
                },
                {
                    "sent": "The risk on the source domain, which is the red last year, the sigmoid loss.",
                    "label": 0
                },
                {
                    "sent": "The disagreement between the target and source distribution that measures somehow.",
                    "label": 0
                },
                {
                    "sent": "The the the the, the adaptation capability, and the commonweal riser.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I single quickly just to present you, we minimize the bound by the by gradient descent and we test our algorithm on toy problem, which is called the intertwining room.",
                    "label": 1
                },
                {
                    "sent": "So here on the graphic the the red and green dots are the source domain, the source that I said it's always the same, but the target.",
                    "label": 0
                },
                {
                    "sent": "Thus we want less if I is the.",
                    "label": 0
                },
                {
                    "sent": "Some tools, but what they said from for certain angle so it so here we can see the decisionone there and we can see that it adapts somehow.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "And see the fact that there's a kind of domain adaptation here, because the one day we is more interested, it's to classify the targets that I set an.",
                    "label": 0
                },
                {
                    "sent": "You want to see the graphic.",
                    "label": 1
                },
                {
                    "sent": "I welcome you to my post or that it will have at the end of the.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}