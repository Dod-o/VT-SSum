{
    "id": "xfpf74ynb7i2tl35dmsoxg5ku63jjv34",
    "title": "Alternating Direction Method of Multipliers",
    "info": {
        "author": [
            "Stephen P. Boyd, Department of Electrical Engineering, Stanford University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_boyd_multipliers/",
    "segmentation": [
        [
            "So I'm going to talk about, well, alternating direction method of multipliers and just to give you a rough idea of what what.",
            "What I'm going to, you know the type, what the talk is going to be.",
            "I just going to kind of like tutorial and it's going to be sort of background.",
            "I know there are people in this room who know actually way more than what I'm going to talk about in this talk, so I apologize to them.",
            "Some of them actually even came willingly.",
            "I don't know why, but there they are here, but not for the others, is just supposed to be an introduction to.",
            "To do one method for for doing distributed optimization and will get that and I'll just show the simplest step.",
            "All of this is taken from something that's much longer, but still kind of tutorial in nature, and it's one of these foundations and trends booklets.",
            "It's everything is available online, I mean including source code.",
            "For every example you'll see in things like that, OK?",
            "Oh, and this is joint work with a with a bunch of coauthors, and in fact I should say this is mostly work from the 70s.",
            "In fact, it could be argued to have come in from.",
            "You can trace it to the 50s.",
            "So so this is."
        ],
        [
            "OK, so what's the goal?",
            "The goal is.",
            "I mean, you really want to do something like arbitrary scale optimization, so and you know the most.",
            "I mean, the most obvious example, certainly for everyone here is to do machine learning and statistics with huge datasets.",
            "I mean that's that's obvious, but actually there are many other applications, so not all optimization problems are machine learning ones.",
            "An example of another one would be dynamic optimization on a large scale network, just to take one example.",
            "Yes, you can take electricity dispatch.",
            "I mean just take one.",
            "So in California is maybe like 3100 generators.",
            "They schedule in 15 minute intervals for 24 hours.",
            "So for each generator you have to work out 100 numbers.",
            "That's still that's not bad.",
            "It's I don't know what we're up to.",
            "That's like 30,000 variable it still.",
            "And it's not a small problem, but it's not huge.",
            "However, all of that has to happen over a network with about 5000 nodes and maybe 15,000 edges, and there are variables for the power flow on every edge that that network is capacit ated.",
            "And guess what?",
            "Now you have a really big problem.",
            "I mean bigger than in fact anyone can handle.",
            "By the way the lights to come on in California.",
            "So you might ask how is that solved?",
            "And it solved in an ad hoc way.",
            "They alternate between going back and forth between sort of network optimization and then.",
            "Scheduling generators and they go back and forth a couple of times and usually it works and the lights come on the next day in California.",
            "So mostly I should say OK, OK, now the approach is going to be a decentralized optimization and so the idea is that you know you have different devices, processors, agents, workers, threads, whatever you want to call it.",
            "Different entities, computing entities they'll coordinate to solve a large problem, bypassing relatively small messages, and in fact.",
            "That's actually an important point here that this is this is not going to be something like implementing stochastic gradient descent on data center or something like that, where lots of data is flowing all over the place.",
            "I mean, these things are optimized for that, so that's fine.",
            "This is this.",
            "In contrast, is the idea is going to be relatively small messages and a fair amount of heavy lifting is going to be done at each worker or node, and then relatively small messages are going to flow.",
            "That's going to be the idea.",
            "And obviously this is going to the math doesn't matter, but it operates at many levels, right?",
            "So this could be.",
            "This could be something like.",
            "This could just be how to take.",
            "You could have multiple threads and just take advantage of different cores.",
            "This could be something operating a data center could be Jeep, target GPU's and all that sort of stuff and I won't even talk about that but there."
        ],
        [
            "Lots of applications.",
            "OK, so here's the outline.",
            "So I'll go way back in history and actually the first 2 sections are history.",
            "Well, maybe history, and I'll say a little bit.",
            "This is just the set up for the method we're going to talk about, so it should be.",
            "Maybe review for most."
        ],
        [
            "So OK, so we'll start with dual decomposition.",
            "And that starts.",
            "This way you want to solve this convex optimization.",
            "Minimize a convex function subject to some equality constraints, and so your formula grangeon.",
            "So which basically says you're going to allow the equality constraint to be violated and you'll have a residual, and you have a dual vector or a price vector, and you can interpret this as something like a payment either to or from.",
            "It's either like a charge or a subsidy, depending on the sign for what happened.",
            "What happens if you violate the constraints?",
            "If you minimize in this in this framework over X another let you allow yourself to violate the constraints, but you either pay or paid for it.",
            "Through this dual variable Y you get the dual function and.",
            "You then I mean this is a dual function that's always concave, and if you maximize the dual function and if all goes well, the maximizer is in fact a solution of this now that there are lot there actually details there.",
            "And they're not just little baby technical details, because in fact for some of the problems you might like to solve, like linear programs, this scheme fails utterly right?",
            "So these are not just sort of little mathematical details.",
            "There are things that make this not work, but you know, with enough assumptions you know one being like F is is, you know, has a minimum.",
            "Curvature or something like that.",
            "This method works OK."
        ],
        [
            "So how would you?",
            "How do you?",
            "How do you maximize the dual function?",
            "Well, you would use a gradient method, or in the case when it's not differentiable.",
            "This subgradient method and so you would calculate the gradient and then make a step in that direction.",
            "These are a positive step lengths.",
            "Well the gradient of that dual function is nothing but the residual, so the algorithm then looks like this.",
            "It's called dual accent and this is maybe maybe from the 60s, but probably people knew it in Moscow in the 50s or something like that.",
            "And so I mean, that's where it all.",
            "That's role of this comes from.",
            "And so the dual asset method looks like this and this should remember because you're going to be seeing this for the rest of this hour.",
            "OK, so it looks like this.",
            "You take this Lagrangian, you fix the price vector or dual vector, then you minimize over X. Yeah, you minimize over X, you calculate the residual.",
            "But if the residual zero you quit because your optimal you stop.",
            "If it's not zero, you update the price or dual vector.",
            "In fact, by some positive number times the residual.",
            "And that's it.",
            "So that's it.",
            "And it's got other names in economics.",
            "You'd call this a like a tattoo on mom procedure or something like that, or a price adjustment procedure.",
            "And the idea is the ax equals B constraint.",
            "There would be something like a market clearing and then this is a price adjustment mechanism, right?",
            "And that so it's got other names and long traditions in lots of fields, OK?",
            "Are you first of all you might ask, you know why, right?",
            "Why?",
            "Why should you solve the dual instead of the primal or something like?"
        ],
        [
            "Set an and the reason has to do with decomposition.",
            "So suppose the function you're minimizing.",
            "The objective is separable, so it's a sum of functions of blocks of the original vector, right?",
            "So it separates out like this.",
            "These could be all the way down to the components, but these could be vectors as well, so this is separable.",
            "Well, in that case the Lagrangian is separable, right?",
            "And because you're adding to it now find function.",
            "But all affine functions are separable down to the component, so this is separable.",
            "And what that says is that this man."
        ],
        [
            "I'm isation here over the Lagrangian over X actually splits and can be done in parallel because it's because you're minimizing something that is a sum of functions of independent variables."
        ],
        [
            "You just do it in parallel and that leads to dual decomposition and that looks like this, and again, I know that many people know this, But this is just sort of to give you historical background and review the way that works is this each of these things minimizes the Lagrangian.",
            "You have a price vector, each minimizes their Lagrangian term separately.",
            "You collect the contribution to the equality constraint and you evaluate the residual.",
            "If it's zero, you quit.",
            "Otherwise you update the price vector and you can see that what this requires is something like a gathering a scatter, right?",
            "So you have to gather these components to evaluate the residual and you scatter the price vector right?",
            "So that now you have now you're doing distributed optimization."
        ],
        [
            "OK. All right, next topic is, and that's maybe from around 1960 something like that.",
            "Next topic is method of multiply."
        ],
        [
            "And what is method of multipliers?",
            "Well, it's sort of.",
            "It's a method that cemented robust applied dual asset, right?",
            "So to actually make it so that it would work on things like linear programming and stuff like that, which this absolutely will not.",
            "And the idea here is goes back, at least in the 60s.",
            "But in fact it can be argued to go earlier to operator splitting methods.",
            "But in any case this to the normal.",
            "This is the usual Lagrangian term and you can think of this is sort of like a.",
            "Your free market term this says go ahead and violate the constraints, but you pay for it at these prices.",
            "Pay for it or you're subsidized for it.",
            "Depends on which end of that you're on.",
            "And then you add another term which is always non negative and it's just a quadratic penalty, right?",
            "And this this this one is.",
            "That's always sort of a cost.",
            "It is never a subsidy right?",
            "So this is this is simply a cost for not for the market not clearing.",
            "For example in economics right?",
            "So it's an augmented Lagrangian and the method of multipliers looks like this.",
            "You minimize the augmented Lagrangian and then you do a dual update and it's exactly the same as.",
            "It's identical to dual dual ascent except for one thing the the gain in the gradient update has a very specific choice of step length, it's row.",
            "It's the parameter here, right?"
        ],
        [
            "And there's a reason for that, and it's easy to understand that the original problem, I mean when it's differentiable.",
            "Original problem, the optimality conditions are primal optimal feasibility, so you have to have X = B and then you have to have dual feasibility, which is that the gradient plus a transpose Y star is zero and if XK plus one minimizes this thing, well you workout this.",
            "Again, this is a differentiable case, but it's the same in non differentiable case.",
            "You workout what what it means to minimize that?",
            "Yeah, well it just means that the gradient of the augmented Lagrangian is zero when you workout.",
            "What that is and you recognize this term here and you realize hey.",
            "If I were to up if this were to be white K plus one, then it says the following.",
            "It says that at the end of method multiplies each step you have dual feasibility, always right?",
            "So so in fact what it says is that with that choice of, with that choice of update dual update you get dual feasibility.",
            "Sort of that happens for free then what your means is as the algorithm progresses, what you're hoping is that the is that the residuals the primal residuals will go to 0.",
            "OK, so that's the that's the idea."
        ],
        [
            "OK. Now it's good news and bad news.",
            "So the good news is that adding the quadratic augmenting term when I should add, you know, you can add other types of augmenting terms, and that's become very popular in the last couple of years.",
            "We had Bregman divergences or things like that.",
            "I'll just keep I'll stick with quadratic because it's simple and if you understand that you can surely understand what happens if you had like an entropy or something like that there.",
            "So OK, the good news is adding that adding that regularization makes this go makes dual.",
            "Assend go from a very fragile algorithm that has to kind of be coaxed to work, and in most cases you would think of doesn't work at all to something absolutely completely robust.",
            "It just works.",
            "Under all circumstances.",
            "Really basically converges always.",
            "So if the problem including ridiculous things where F can take on the value plus Infinity, that can be nondifferentiable.",
            "In fact, one of the will see a lot of the interesting cases F is an indicator function, meaning it's zero on some convex set, and then plus Infinity off it.",
            "That in depth indicate that set could even be thin.",
            "It could, it could be flat, it could have non empty interior, right?",
            "So these are not nice functions from the analyst point of view.",
            "And it just works perfectly here.",
            "So that's the good news.",
            "It's now rock solid.",
            "It's robust.",
            "The bad news is it destroys the splitting of the X update.",
            "So you can't do decomposition anymore, right?",
            "So?"
        ],
        [
            "OK, and that is the historical back."
        ],
        [
            "Ground that brings us to a DMM.",
            "And the idea is that something like a method that has good rub good, good robustness of the method of multipliers.",
            "That's and it's supposed to support decomposition right?",
            "And so this is the way you should think of it.",
            "And this was proposed in well, 1976.",
            "Although it turned out later in the mid 80s, it was discovered that you could re derive this algorithm from material from the 50s, right?",
            "Maybe they did note in the 50s probably I don't know.",
            "They probably didn't explicitly know it, but anyway, so.",
            "But the main point is this is this.",
            "This is not new right?",
            "So?",
            "But by the way, neither is stochastic gradient descent or any any."
        ],
        [
            "Any of these other things so OK?",
            "Alright, so the form is this.",
            "I should say this is the Western form, not the Russian form.",
            "There's a Russian form there, slightly different.",
            "It turns out there kind of the same.",
            "So in the in the Western form you split the variable which we had previously called X into two groups.",
            "I'm going to call it X&Z.",
            "Night we ask that the objective should be separable across X&Z and we have a completely general equality constraint linking X&Z, so that's a completely general constraint.",
            "That's it?",
            "OK, FNG or convex?",
            "You form an augmented Lagrangian and Adm is extremely simple.",
            "It looks like all the other.",
            "The other things we've been looking at for a while.",
            "You have a dual vector Y and it says this it actually it's one Gauss Seidel sweep in the method of multipliers.",
            "So it says.",
            "Optimize over X.",
            "Then Z, then you do a dual variable update and notice it's identical to method of multipliers, right?",
            "It's got the row there.",
            "Everything is the same.",
            "OK, so that's it.",
            "Now you can already see some nice things going to happen.",
            "The first thing is you never ever optimize over X&Z at the same time, right?",
            "So basically this says if you have a method for handling minimization over X and a method for handling minimization over Z, this will combine them into something that can minimize problem.",
            "We can solve problems that involve minimization over both.",
            "Right, and I mean, usually this would be special methods?",
            "Well, we'll see specific examples."
        ],
        [
            "OK. Yeah, and I already said this.",
            "I mean, if you were to minimize over X is jointly, you get method of multipliers.",
            "One thing that should be clear is you're welcome to minimize over X&Z as many times as you feel like before you update the dual variable, right?",
            "And if you do it alot you basically have you have method of multipliers OK?"
        ],
        [
            "And it'll be examples will make the ability to split obvious, so here you can say something similar to why the update, and you can actually understand how this works here.",
            "Well, we split our primal variable into two variables, X&Z and.",
            "So there are there's primal feasibility.",
            "Equality constraints have to hold and then you have dual feasibility.",
            "These are really stacked, so for their two equations there's X dual feasibility and why dual feasibility?",
            "Now when when you minimize the second step, you minimize Z over this thing and you workout what it is and then you recognize all this and you say hey this plus ro times this thing.",
            "The residual is in fact that's our updated dual variable.",
            "What you see is this.",
            "It says the following.",
            "It says that in a DMM there's three optimality conditions, primal and two dual and it says that the the second dual is free that when you just simply do your dual update.",
            "You get your second.",
            "You get your second dual feasibility condition for free, and so we're simply waiting for this primal residual, and this dual residual to converge to 0.",
            "So that's that's what it comes down to, OK?"
        ],
        [
            "Now one thing I should I should point out.",
            "It's kind of obvious, but it simplifies things.",
            "It's this if you have a.",
            "If you have a linear function into that, yet quadratic even combine these into one.",
            "You just complete the square or whatever you want to call it and you combine this into a single quadratic and then plus a constant which I've dropped because it doesn't matter.",
            "You're just minimizing right?",
            "So you'll see in this case you is this scaled dual variable.",
            "It's one over Rd times YK.",
            "And so sometimes you get shorter formulas with this.",
            "Sometimes it's maybe clearer with this because you see explicitly.",
            "I mean, this has a nicer story, right?",
            "That's the vector of prices in some way.",
            "You could explain anybody what it means, right?",
            "You know when it converges, these are the optimal prices.",
            "This is zero, and then you point to this term and you say what's that?",
            "That's a cost for violating.",
            "This is harder to more compact formulas, so we'll see both."
        ],
        [
            "So here's the convergence theory.",
            "You make the absolute minimum assumptions you could possibly make would basically is.",
            "This functions are convex, closed proper and the other thing is that the problem has to have a solution.",
            "Otherwise it's silly to be talking about an algorithm for solving it.",
            "OK, so literally that's it.",
            "There are no weaker assumptions you could possibly make none, and the theorem is this again from from the 70s, but you can go to the 50s, you know you take your choice as to when this was known.",
            "The theorem is it works so, and this is very carefully worded here.",
            "So this says.",
            "Here's here's what you can say.",
            "By the way, there have been no assumptions whatsoever on A&B, for example here, none.",
            "So what is asserted is this.",
            "The iterates the primal residual goes to zero, and the objective value goes to the optimal value.",
            "OK, so that's true always.",
            "Here's what's not said.",
            "There are many things not said here because they are false, but here they are.",
            "And when if you look this up another reference, other stuff, you'll find it's a lot more complicated than this, and the reason is you know your hypothesis will go on for half a page, and the reason is this.",
            "They'll say things like this.",
            "Here's what's not said here.",
            "XC converges, that's false, does not converge.",
            "It did not converge.",
            "ZC converges.",
            "There's a stronger statement.",
            "XK converges to the optimal set.",
            "That's false too.",
            "KZK convergence, the optimal set that's false too.",
            "OK, so so what's being said here?",
            "Or here's another one XC and ZC converge to X star Onsie start Witcher optimal.",
            "That's also false.",
            "OK, so very carefully saying minimum, but if you actually go back and think if you really sit back and think by the way, a lot of those other statements become true.",
            "But you have to start making lots of assumptions and the whole point of this is it works no matter what, so I don't aesthetically I don't get it, I don't.",
            "What's that that you can actually say?",
            "Why does converge to why star?",
            "So I didn't write it down here, but that's that's actually something you can say an is true.",
            "Actually, right?",
            "So so you can say things that aren't true too.",
            "Right, and I certainly don't mind doing that, but.",
            "But I don't do it when I'm being recorded though.",
            "So well I try not too.",
            "I should say so they can have plausible deniability.",
            "OK, so actually if you think my feeling.",
            "But this is actually very simple, the more you think about it, the more you realize that in fact all those other statements are utterly irrelevant.",
            "I had I've had screaming fights with people who said things like like you don't care that X Kenzie Kai don't converge to some optimal values, and I said, no, I don't.",
            "In fact, if you think carefully about it in any algorithm, if you implement anything you don't care about anything but these two things, because these these are stopping.",
            "Criterion will be based on this.",
            "And it's utterly irrelevant that weather XC converges to a Nexstar or ZC convergence too noisy star or anything like that.",
            "So I mean just go backwards from do to utilitarian approach and you say it's a numerical method.",
            "What would you consider to be an approximate solution?",
            "And I think no one would say anything other than that except.",
            "Quality constraints and you wanted to identify particular about did I get active set or you want to get the active set right?",
            "If you care bout active set then you you can actually stop at some point and then switch to some active set method or something.",
            "I mean people do that."
        ],
        [
            "Alright, so I should say I call it.",
            "I mean I'm calling an ATM.",
            "In fact it turns out it is related to an exactly the same as many other methods.",
            "So let me explain some of that.",
            "First of all, I absolutely identical to something called Douglas Rachford Douglas Rachford splitting operator splitting.",
            "It's identical.",
            "I mean if you choose the right operator to split and split it the right way, it's not related, it's identical.",
            "So another way you'll hear this is Douglas Rachford der splitting, so it's not that it's the similar.",
            "It's the same.",
            "It's related to things like Douglas Peaceman Rachford splitting, and this stuff goes into the 50s.",
            "Let's see all of these.",
            "It's also it is exactly the same as the proximal point algorithm, which is very general, but applied to the right operator.",
            "Then it's identical.",
            "I mean you course you have to figure out what the right operator is, But anyway.",
            "And a lot of this wasn't known until maybe the mid 80s.",
            "In fact I mean so there were papers on all of these kinds of things, all different, and it wasn't known that they were kind of all the same.",
            "It's identical, or this is a special case of it is Dykes resulting projections method reinvented again and 85.",
            "It's the same as progressive hedging.",
            "It's related to proximal methods and then sort of the modern vein substitutes for the quadratic Bregman divergences.",
            "You start getting other things like this.",
            "So I think that's the story on this, but by the way, you might ask why?",
            "You know how is it that different well known algorithms could actually be the same and not notice?",
            "Because actually it's complicated enough.",
            "I mean it doesn't look that."
        ],
        [
            "Complicated, but it's complicated enough.",
            "Here you go something like that that if you redo the order and change the variables and instead of using you use U + X or something like that.",
            "I mean then it can actually take quite a while to take algorithm A.",
            "An algorithm B and actually certify that they are in fact the same right?",
            "It's just it's enough complexity here that it's hard to do.",
            "You're going to.",
            "Minimizing completely operating precisely right?",
            "That's the other thing, yeah, yeah, exactly so."
        ],
        [
            "Right, first of all, you have to do the baseline.",
            "The first translation.",
            "OK, so now I'm going to talk about just just a common patterns that you see, and so we're working.",
            "We're working towards actually doing something.",
            "We have done anything yet, but we're working towards it.",
            "OK, so, but it's just a couple of obvious, very simple observations.",
            "I should mention, oh, I should mention one more thing before we move on 'cause I've had other discussions with people about it.",
            "The proof of convergence requires high school algebra, absolutely nothing else, so there's nothing complicated in you know to subgradient is inequality.",
            "Cauchy Schwarz?",
            "I mean that's it right so?",
            "It's very simple, right?",
            "And if you see anything more complicated, you should be deeply suspicious.",
            "So and you will see things much more complicated than that.",
            "OK, the second thing I'll say is you might ask, well, what's the order?",
            "Well, how fast does it converge?",
            "And the answer is, it converges with the worst possible rate for these methods, one over epsilon squared steps to get an epsilon approximation, right?",
            "So now of course this order optimal right?",
            "Because we're making.",
            "Merely that we're not assuming a Lipschitz constant on things, right?",
            "These things, they're not even differentiable, right, so?",
            "Seems to me.",
            "I mean this is part of the trade off and this so this is not going to win any contests.",
            "Any complexity, algorithm, complexity contests, that's for sure, right?",
            "But that's kind of not the spirit of all this.",
            "The spirit of all this is, you know that it should.",
            "It should be like something like conjugate gradients, right?",
            "Even run in numerics where you get horrible round off error affects.",
            "It should be something like.",
            "You know what you're hoping is that you get something that's relative.",
            "That's actually useful in an application in some modest number of steps, a couple 100 or something like that.",
            "I mean, that's kind of what you're hoping.",
            "Right, so so this is not about.",
            "Complexity, worst case complexities OK. OK, so let's look at some common patterns here.",
            "The idea is this.",
            "Oh, and let's think about abstractly, what what this is.",
            "And actually I like to think of it not as an optimization algorithm, but is a sort of a meta algorithm.",
            "Cousin an optimization algorithm.",
            "You end up working with primitives like evaluating a gradient or a subgradient, or something like that.",
            "You know you do some linear algebra and stuff like that, and all of a sudden you have an algorithm, right?",
            "So these are these are very low level operators here.",
            "The operators actually are higher level, they require this, they require you to minimize.",
            "A function plus a quadratic, a quadratically augmented function, right?",
            "So it's a higher level concept now.",
            "I mean, at least in some cases, this will reduce to doing a gradient calculation or something like that, or something like a gradient calculation.",
            "But in general this could be something much more complicated, right?",
            "Well, this could.",
            "This could actually require some heavy lifting actually solving real convex optimization problem, OK?",
            "So we're going to look now at at so actually, by the way, what this says is you only have to implement one method for F, right?",
            "I mean, it's just you have to be able to carry out this.",
            "This quadratically augmented minimization.",
            "But let's look at some special cases and things where the splits 'cause you put them together."
        ],
        [
            "For these and all of a sudden this looks interesting.",
            "Well, the first one this is like kind of obvious if F is block separable and a transposase block separable.",
            "By the way, we'll see that in a lot of applications A is like I or minus.",
            "I will see when we see applications.",
            "Then, well, of course, minimizing if you have to minimize."
        ],
        [
            "This, you know, take a equals I and you have to minimize this and F splits.",
            "Then of course the whole thing splits, so you can immediately target.",
            "You can run that section in parallel and that's how we're going to distributed optimization.",
            "OK, so I mean, this is kind of just like."
        ],
        [
            "Completely obvious, OK?",
            "Another case occurs when a equals I.",
            "This occurs so frequently it has a name and a long history, so this is called the proximal operator associated with F and it.",
            "Basically it minimum.",
            "You know it minimizes F + a quadratic cost of deviating from V. An you can, by the way, you can almost sort of see certain things here.",
            "If you make Ro super high, very large, then you can kind of NF is quadratic.",
            "You'll see that this returns something which is a gradient.",
            "It's a gradient step.",
            "It's exactly a gradient step.",
            "If row is small, I mean you get other other things and I'll explain some of these, but these you can these you can workout depending on what F is.",
            "I mean when an example, let's look at some special cases.",
            "If F is an indicator function, so F is zero off, some on some convex set an plus Infinity outside and this really simple.",
            "This just says minimize this norm.",
            "the Rose totally irrelevant.",
            "This is projection.",
            "So in fact the idea is that approximal operator is a generalization of a projection operator and reduces the projection operator when F is an indicator function.",
            "If F is any separable function, the whole thing becomes completely trivial because you minimize it's actually absolutely.",
            "You know, certainly don't need to sum of absolute value.",
            "Any separable function is completely trivial because you're doing scalar proxamol calculations.",
            "How long does it take to minimize the scalar function convex function?",
            "The answer is 0.",
            "I mean 'cause you put it's all done in little registers and things like that, so it's all zero.",
            "Basically it costs you more to move the data in and out OK.",
            "So.",
            "But then you know you have pretty formulas for it, not that that matters, But for example the L1 norm.",
            "The proximal operator is this soft threshold thing right?",
            "Which looks like, let's see for you it goes up like this is a little flat chunk and then it goes back up again OK?",
            "And there's lots and lots of others."
        ],
        [
            "OK, quadratic objective.",
            "Well, if you minimize a quadratic objective, well then of course you augmented quadratic objective with quadratic is still quadratic.",
            "Minimize a quadratic you that's you can solve this by just by linear algebra, I mean you just solve a set of linear equations right?",
            "So you get something like this and it is a lot you can say about how to do this.",
            "Well, for one thing, depending on sparsity patterns and things like that, you may want to arrange for this this thing.",
            "You might want to use something like a matrix inversion lemma or something like that, and the rough idea is if in the dense case you want to be absolutely certain that the cost of computing this is the big dimension times a small squared right.",
            "I mean the big dimension squared times a small one being wrong, so and the big big dimension cube being very wrong.",
            "Implementation.",
            "If you're using a direct method, you also see something interesting that's this.",
            "When you, when you if you had to do this repeatedly, what changes in each iteration is V and what it says is I'll cash a factorization.",
            "If you cash the factorization of this in the dense case, you get a effusa direct method right?",
            "Then the first time you solve it, you pay for a factorization Anna back solve.",
            "Thereafter you only pay back solve and you get a discount on the on the method.",
            "On on on computing the least squares problem, which is on the order of this, which is the small dimension, right?",
            "So we're course we intend to do this for large problems, so the small dimension is a pretty good discount.",
            "So by the way, this also means that this business of counting iterations is not going to be too irrelevant, because what it says is that after the first iteration of a DMM, when you're doing, if you're doing direct method an factorization cashing it says after you do the first iteration, thereafter iterations cost one 500 or 1000 of the first one.",
            "And you see, in a regime like that, you know, really, who cares whether you take 200 or or for that matter, 10,000 steps?",
            "I mean, it's just not relevant, OK?",
            "OK, now you're using an iterative method here, like LSQR or some CG type thing.",
            "Then there's other tricks you don't cache anymore, but what you do is you do warm start and you start.",
            "The minimum is 8 LSQR minimization of the quadratic from for example the previous point.",
            "And as you converge it means you start taking you start doing better and better OK.",
            "So there are lots of tricks I'm not going to go into them, but these are these are these are serious there obvious ones and they confer."
        ],
        [
            "A huge advantage.",
            "OK, so I should say something else."
        ],
        [
            "That will see in a statistical in a data fitting context, this a quadratic update is going to correspond to just Ridge regression, right?",
            "So this says that in those cases you do a Ridge regression and after you do the first Ridge regression there after you get a huge discount for the substantial discount for further."
        ],
        [
            "Iterations right?",
            "OK, now if is smooth by using whatever you like.",
            "Could use something like Newton Quasi Newton if you use Newton if you can if you can.",
            "If you can use a direct method, that's a excellent choice.",
            "Again, you would.",
            "Well you don't catch the factorization, but you what you do.",
            "Cash is the permutation for the sparse matrix factorization that's cashed across everything.",
            "A very good choice would be something like limited memory FDS that will scale to extremely large problems and I should add something here.",
            "Even things that wouldn't work well with LB FGS will now work very well because you've augment you've augmented it with a quadratic, and for methods that depend on smoothness and things like this, adding that Rover two quadratic term is just like it's perfect right?",
            "You just it's just what they want to make them work extremely well.",
            "I'm including things like LBF's.",
            "OK."
        ],
        [
            "Alright now."
        ],
        [
            "Going to look at that, we're just going to assemble all the parts, an kind of have everything you would just put 'em together and you'll see what's weird is, although nothing I've said so far has been.",
            "I mean, it's all very basic, so now we just start assembling, so will start just with convex general convex optimization.",
            "I mean just generic problem, minimize F / a convex set will reformulate rate formula for Adm this way and I mean if someone comes up to you and says I'm going to write this problem this way.",
            "I mean this is just like completely stupid.",
            "It doesn't look like in fact if you think about if you know about how linear programming solvers work, the first thing they do is they do a pre solve and a pre solve scans the problem and looks for looks for constraints this stupid.",
            "Write this constraint says like X1 equals one and so that's exactly what a pre solving an LP solver does.",
            "It looks it looks for constraints that are idiotically stupid.",
            "It makes a note that Z1 is X1.",
            "It replaces it here and it has a little symbol table.",
            "Later it says oh, by the way, at the end.",
            "Please write for Z11X1 is everything I'm saying here.",
            "So I mean this doesn't look promising right?",
            "And then I've written it this way.",
            "G is the following.",
            "It's zero if you're in C and plus Infinity outside.",
            "I mean so this does not look.",
            "I don't know, it just looks too simple too.",
            "That any good can come of it, right?",
            "OK, but the algorithm is very simple, so here you do.",
            "This is approx.",
            "Step on F. The PROC step with G is simple.",
            "G is a indicator function, so the process is projection on to see and so you can see you now have a method that will minimize F / C, But it's different for something like projected gradient, although for Ro extremely high it is projected gradient, it's exactly the same and F smooth.",
            "This does not require F smooth.",
            "This works for.",
            "F can itself can contain constraints.",
            "It could be something horrible, but it doesn't make any difference.",
            "Works just works.",
            "And so you can see what we have is a method for minimizing F / X and C, and the only method you have to implement is approx method for F, that's this.",
            "And you have to be able to project onto C, right?",
            "And you can also see here.",
            "I think it's not that hard to see that if Rd gets really large, this is projected gradient method, right?",
            "Because this is a gradient step.",
            "And this is then you make a projection and so on, OK?",
            "Alright."
        ],
        [
            "So let's let's look at another one.",
            "Let's look at Lasso.",
            "I mean, why not ten 10,000 algorithms for solving this?",
            "And this probably another one developed last week or something like that with a new acronym two so.",
            "OK, so let's try it.",
            "So here you have a least squares term, which is a standard regression term and then you have an L1 sparsifying regularizer.",
            "And you know, the spirit is very simple.",
            "I mean the spirit is quadratic.",
            "Yeah, we have a very good way to solve those.",
            "I guess you know Gauss wrote a book in Latin from around 1800 about least squares.",
            "So yes, we know how to do that, but we have a lot.",
            "I mean, actually we have a lot behind stuff like this, right?",
            "There's a lot of.",
            "Lot of lot of integrated knowledge over the years about how to solve problems like that with this one in there it's harder, but in fact we know how to handle approx of that.",
            "So it's all going to work.",
            "So you write it this way and again this is you can see the splitting is like embarrassing that what you're really doing is you're taking a single variable and you're replicating it, calling it X&Z and adding a constraint that they're in consensus that they agree with each other.",
            "It sounds dumb, but that's it, and you end up you just write this down and you get this thing you do.",
            "You do.",
            "You can if you want, you can do factorization caching here.",
            "There's a soft thresholding and then there's a dual update.",
            "OK, so did you just turn the."
        ],
        [
            "Rank just to see that it kind of works.",
            "Here's an example, just with a dense A.",
            "It's like 1500 by 5000.",
            "And so if you do this, this is all, by the way something like an 8 line implementation in Matlab or something.",
            "So you shouldn't believe.",
            "I mean you shouldn't trust any of the numbers I mean, but real implementation.",
            "It would be similar just with smaller numbers, right?",
            "So the factorization oh by the way, which is a Ridge regression.",
            "So that's 1.3 seconds and then after that you get a discount.",
            "It should have been more but 30 milliseconds and you can see by the way, when your first iteration is a second and subsequently this should have been something like 10 milliseconds, right?",
            "You can see that now getting all bent out of shape over how many steps it takes.",
            "Is completely idiotic.",
            "I mean, if you're doing factorization caching right, so let's solve takes.",
            "I don't know a couple of seconds, by the way.",
            "You can then do the full regularization path because you can share if you change Lambda, it doesn't change in any way.",
            "The fact you just keep the same factorization right?",
            "So just a quick example that's not bad.",
            "This is like a six line Matlab script.",
            "You're actually close to competitive here.",
            "Sorry you're in the you're in the range, right?",
            "So you write something real quick and it's it's right there, untuned, untouched, I mean.",
            "Nothing was tuned or anything like that, so.",
            "OK, so this is just to show that knowing about these methods is good, it just means you can actually put bits and pieces together and you can get something very quickly that."
        ],
        [
            "Is in the ballpark.",
            "I'll do one more.",
            "I'll go over this quickly and then we'll get to distributed stuff.",
            "Was there a question somewhere I know OK, so for sparse inverse covariance selection, here you have samples from zero mean Gaussian with Sigma, inverse sparse and you want to estimate you want to estimate Sigma, right?",
            "So you write down will estimate Sigma inverse which is called X here and this would be the negative.",
            "These two these two terms are the negative log likelihood you know up to some constant or something.",
            "Actually, native content multiplicative constant.",
            "That's the negative log likelihood.",
            "And of course if you just Max maximize this, you get X equals Sigma.",
            "Sorry X equals Sigma inverse right so, but we add a sparsifying regularizer on X, which is our estimate.",
            "Sigma inverse right?",
            "And then we turn the crank on Lambda to trade off sparsity of the inverse versus versus fit here.",
            "OK. And I mean this is something that's been, so I mean, these are just some of the things people have looked at it actually others have too, including people here."
        ],
        [
            "Well, I was just talking to someone beforehand.",
            "OK, So what do you do?",
            "Again, it's silly.",
            "You just do the X = y thing.",
            "By the way, will see some splittings that don't involve this embarrassing splitting here.",
            "And you get something that looks like this.",
            "Here's your this thing.",
            "We know how to minimize that, but we have to quadratically augmented this.",
            "We know how to deal with, because that's just going to be a soft threshold, right?",
            "So on the X step, you minimize this thing, and otherwise you do a sparse.",
            "I mean, sorry, a soft threshold and you do the standard update, and so you look at this for awhile and you could solve this by.",
            "You know LB, FGS or something like that, but you look at it long enough and it should start dawning on you.",
            "We can solve that.",
            "You know, semi analytically, actually analytically, right?",
            "Certainly you can solve that analytically, right?",
            "And the key would be to say look without loss of generality.",
            "We can do this for XY.",
            "This is unitarily invariant, so you can assume that X is is a diagonal.",
            "Then it splits and it turns out this is Unitarily invariant to its Frobenius norm squared.",
            "So you can solve this analytically."
        ],
        [
            "Anne.",
            "It's just this.",
            "You compute an eigen decomposition.",
            "And then you solve a quadratic involving the eigenvalues, and then reassemble OK, so it says that the cost is an eigen decomposition.",
            "So again you know you write a date line."
        ],
        [
            "Script and you know it's not totally unoptimized.",
            "An unoptimized you get something that's you know within a factor of 10 or something like that of the state of the art type thing.",
            "For something like this right?",
            "Which again for maximum coding time.",
            "I mean like I don't know 5 minutes or something like that, maybe 45 minutes of pencil and paper to do the work out what the update is.",
            "This is not bad, right?"
        ],
        [
            "OK.",
            "I will do now anyway so that that finishes up that section and the point there is just by assembling these bits and pieces, you can actually do stuff with this, I mean quite quite quickly you can get something up and running."
        ],
        [
            "OK, now we're going to look at distributed optimization, and to do that we're just going to look at two Canonical problems, one, but in much more detail than the other, which is consensus, and so it's sort of like the Canonical, like it's the first, it's the Hello world of distributed optimization, and it's got a beautiful interpretation.",
            "It's it's very simple, but honestly, once you understand this, you understand you understand other distributed optimization problems, so OK. Or at least this is a stepping stone to it.",
            "You want to minimize a sum of functions of a variable, and these functions if you want to think of something.",
            "Think of FI as the loss function for the ice block of training data, so that's what it is, right?",
            "It's something like that.",
            "And so you could make some big deal and you could say, oh, this is like collaborative filtering because you see I have all these different data sources and they're going to combine and minimize the total loss and come up with an X that's better than any of them by themselves might have come up with or something and make a big story about.",
            "So that's that's the problem.",
            "OK, so we simply write it this way.",
            "Again, it's quite stupid.",
            "What you do is you allow each one to have its own private opinion.",
            "So instead of a global variable X, they all have their own local opinion XI, and you add a constraint that says you can have your own local opinion, but.",
            "But it has to be in consent.",
            "They all have to be agree with this global variable Z.",
            "Again, it doesn't these things look so obvious and silly that it doesn't look like any any good is going to come from this right?",
            "OK, that's the idea."
        ],
        [
            "Right, so you just turn the crank you you form an augmented Lagrangian, you minimize it.",
            "Turns out this first one obviously splits entirely, so the only thing you have to do is approx is approx operator on each FI.",
            "Then you do.",
            "Oh, how do you?",
            "How do you minimize this thing that you over Z?",
            "Well, that's trivial.",
            "This is totally irrelevant.",
            "That's a bunch of linear functions.",
            "That's a bunch of quadratic fact, not only just quadratic functions, it's the norm squared of X -- Y.",
            "How do you minimize that?",
            "It's like an average.",
            "I mean, it's just analytical.",
            "It's just this thing.",
            "This can even be simplified more 'cause it turns out that some of the dual variables is 0.",
            "Um?"
        ],
        [
            "And so it turns out you end up with a very, very simple algorithm.",
            "It's just beautiful, and it looks like this.",
            "It's this, it says each each.",
            "Each worker, whatever subunit or agent separately minimizes their function plus the linear term.",
            "That's irrelevant, of course, is just for aesthetics, right?",
            "Plus a linear term plus this quadratic and the quadratic penalizes you from moving from the last average.",
            "And I mean, if you sent a message to this thing and say OK times up, please give me what tell me what X is.",
            "You would average all of the opinion, the local opinions and that would be it.",
            "And why does averaging work well because?",
            "Everything is convex and so Jensen's inequality says you average and things get better.",
            "OK, so that's what that's the idea.",
            "So you get this beautiful quadratic penalty for averaging, and you can sort of understand all the bits and pieces of this right?",
            "So in fact this would be dual decomposition right there, just with the penalty.",
            "And then this says you add the quadratic regularization you meet towards the mean and it makes the algorithm go from quite non robust, fragile to utterly and completely robust.",
            "And you can see it's like a scatter gather.",
            "Type thing you know what?",
            "It's not even it's not a gather.",
            "Even that is is not correct.",
            "In fact, the wrong way of course, to average a million vectors of size a million.",
            "The exactly wrong method would be to gather them all in one place and then average them.",
            "So, but I mean, but you know what I mean by this so."
        ],
        [
            "OK. Now the statistical interpretation is absolutely beautiful.",
            "It says this if you want it.",
            "Basically what it says is, let's do distributed maximum likelihood distribution.",
            "Maximum likelihood says I have a bunch of data blocks each each one, and then I have a.",
            "Those are in case that I have and the only thing I have to do.",
            "There's only one method I have to implement for each box, each box of data at the only thing you have to do is approx."
        ],
        [
            "Operator right, you have to be able to minimize.",
            "I have to be able to pass in this thing in this thing or for that matter I combine these into one thing I pass in one thing and then I had.",
            "I say minimize your loss function plus a quadratic, but minimizing a loss."
        ],
        [
            "Implicit quadratic has a very simple interpretation, is just map estimate X maximum posteriori probability estimation with a Gaussian prior.",
            "So the only thing that you have to implement on each of these data boxes or anything like that.",
            "Is A is a map method, so that's it does it doesn't matter what the measurements are, how many there are utterly irrelevant.",
            "The only needs a map method you send to the box aprire with the Gaussian an it sends back what it what the map estimate would be under that prior.",
            "And that's the only thing you have to do, right?",
            "So anyway, it's quite.",
            "I think it's kind of cool.",
            "Let's not not obvious."
        ],
        [
            "OK, so now we'll do some examples.",
            "I'll start with a little baby one.",
            "Just so you can see visually what it is, and I I'm sure here I don't have to go into all of this, so I'll make it very, very fast.",
            "You know you will do just do classification so you know I have a bunch of Boolean outcomes and feature vectors and I form a margin.",
            "My variables are WMV.",
            "I'll make a loss.",
            "You can make anything you like, you know any any of the obvious options, and then you want to minimize an average an average loss plus a regularization term that could be L2 squared, L1, whatever you like, exponential, anything like that anyway.",
            "Then the ideas you split data and will use."
        ],
        [
            "Adm consensus to solve will look at a really super small example, just one that you don't.",
            "In fact, you don't even need.",
            "You shouldn't be doing SPM on, But anyway, well, well just look at their two features.",
            "An 400 examples, but just so I can plot it right and will split the 400 examples into twenty groups, but will do it in a sick way.",
            "Each group of 20 has only seen either all positive or all negative examples.",
            "Right, so it's the most skewed, so each box of 20 examples has seen all spam or all, not right?",
            "So that's how it works.",
            "So they're going to have to collaborate too.",
            "I mean, probably the right way to do this, actually, is to hash the data, in which case each of them by the one by themselves actually gets a pretty good estimate.",
            "And the only thing they do by collaborating is get statistical power right?",
            "So this is not that.",
            "This is just for the show how this works."
        ],
        [
            "OK, so let's see how this works.",
            "Well, these are all the private local estimates, and of course they're all terrible because they've all been given false.",
            "They've all been given terrible ideas of what the data looks like, 'cause they've only seen all positive role negative things.",
            "By the way, the reason they're not all way off in the middle of nowhere is because the quadratic regularization term.",
            "OK, actually, embarrassingly after one step, which consists of averaging all of those, you don't get something is not so bad, right?",
            "So if you then?"
        ],
        [
            "Run a DMM like 5 steps.",
            "You get to something like this and you."
        ],
        [
            "And kind of you get the picture right, so that's what happens after 40 steps, right?",
            "But the interesting thing here is just this.",
            "To visualize.",
            "Actually the data flow in the communication.",
            "Here, each box has seen like you're a box you've seen only positive examples.",
            "You have absolutely no idea who else has an opinion about about this problem.",
            "Who else you're collaborating with to form a class, but you know nothing.",
            "The only thing you have is a protocol that implements map, and that's it.",
            "And what happens is they come back and they say.",
            "Well, the weight vector prior looks like this.",
            "What would you do then?",
            "And you'd say, well, it's very strange.",
            "It's highly inconsistent with my data frankly, but OK, whatever and it returns W and then actually when these things come to convergence, it means they've solved the global problem.",
            "Each box has absolutely no idea how many others there are what they look like.",
            "Anything I mean.",
            "So I don't want to make too big a deal about it, but it's worth thinking about what actually happens here."
        ],
        [
            "OK, let's look at another one.",
            "This is distributed lasso example and this is just made.",
            "This is just for fun, just to show that you can do this.",
            "So this is probably bigger than anyone would ever solve, so it's dense problem with four 100,000 four 100,000 examples in 8000 regressors or something like that.",
            "An that's about 30 gigabytes of data.",
            "I mean, it's people like we're routinely solve problems with this amount of data for sparse problems.",
            "This one is not sparse, so it's dense.",
            "So anyway, I probably somebody has solved problems like this, But anyway, alright, so just to do something real simple you take this, you split it, you just split the data into 80 subsystems.",
            "That's ten 8 core machines, right?",
            "Something really simple.",
            "Not totally unoptimized.",
            "Extremely short, right?",
            "Just the shortest thing you could possibly have and then here, just to give you a rough idea of what this does.",
            "It does something like this.",
            "Factorization, by the way, is a.",
            "That's a parallel Ridge regression.",
            "I mean, statistically, that's what you're doing.",
            "You're doing a parallel Ridge regression and that takes about 5 minutes.",
            "Subsequent iterations take about a second, so again, it's totally irrelevant how many steps you take.",
            "I mean not totally irrelevant, but it's certainly it's not as interesting as arguing about whether something takes 20 steps or 100 total.",
            "Now, that's kind of irrelevant, and the total lasso solve is takes like 5 minutes.",
            "By the way, this makes a very nice story, right?",
            "So there was a story.",
            "You could say about interior Point methods that would go like this.",
            "The story would say someone would say what are you doing?",
            "You say when solving a convex optimization problem and you would say oh great indeed, profile the code and find out that in fact what they're really doing is 20 for 20 times they were solving a least squares system.",
            "Everybody you know, and that's what it is.",
            "That's when the interior point method does.",
            "It solves the KKT system like 20 * 30 times whatever something like that.",
            "So you're solving least squares, right?",
            "And so someone could say, well, why would you do this convex optimization?",
            "And you say, well, it's much more expressive and the cost over least squares is some modest number, like 30 or something.",
            "Most 30 right?",
            "So I mean sometimes a factor of 30 is actually is a deal breaker, right?",
            "But you know anyway, not this relevant to any of you because interior point methods are not relevant.",
            "Probably to any of you.",
            "But anyway, but there are people for whom it's relevant, right?",
            "Those who chose wisely to go into fields without big data, but it's too late for all of you now.",
            "Maybe some of the younger people are not in any way whatever.",
            "OK, so the idea there is that, but actually you get the same story here, but it's even more shocking, right?",
            "'cause here, the story is something like this.",
            "How long does it take you to just do like least squares regression regression on a data set like that?",
            "And the answer is if you know what you're doing.",
            "I mean, it's a crappy implementation, but it's 5 minutes.",
            "Got the answer and you go.",
            "Oh, how long did it take you to do Lasso which is solving a nontrivial convex problem and you go 5 1/2 minutes so you get the same story that the cost, but they did.",
            "Marginal cost is not much higher for solving convex optimization problem.",
            "As for solving at least squares problem right?",
            "So OK."
        ],
        [
            "We look at this one more problem and then I'll quit.",
            "And it is in fact the dual of the consensus problem.",
            "So that's this problem.",
            "It's you minimize some FI of XI.",
            "Now here the XI are not just replicates of each other like they were in consensus right here.",
            "They're actually truly different, but they have to sum to zero.",
            "And by the way, if you want to know how you know it's a dual, well, consensus is the same objective here.",
            "And it says ex.",
            "The stacked X is in the range of IIII.",
            "Times E or sorry, just the range of I I write notes in consensus.",
            "Here it says X is in the null space of the the transpose, which is I I I don't know if any of that made any sense, but.",
            "I think a few people few people got it, maybe a few others were nodding politely.",
            "That's fine, OK, alright?",
            "And it's got this.",
            "This like consensus has beautiful interpretation, right?",
            "So here you think of XYZ as characterizing a set of amounts of goods?",
            "And then this is an exchange.",
            "This says that they clear.",
            "It says that people only exchange things.",
            "The total amount of commodity three that everyone either contributes or takes from the exchange.",
            "It all balances.",
            "That's this, right?",
            "And then you know you get a price and things like that.",
            "OK, so here."
        ],
        [
            "I won't you can work out what it is exactly, but you get something very similar an it's really quite simple and beautiful, so it looks like this again that there's a common price you implement approx method and here you have again a term.",
            "The term is interesting.",
            "It's not penalized towards the mean, it's this actually here you want the average to be 0.",
            "This would be your contribution if you.",
            "Personally we're going to make up all.",
            "Of the amount by which the exchange fails to clear, right?",
            "That's what this would be.",
            "And so this says you should.",
            "You should add it.",
            "You should move towards that amount, right?",
            "So again, it makes just perfect sense, right?",
            "And you get you get regularize, this is regularised Teton Mall right at home on is this price process where you look at how much is consumed you compare it to how much you have and then you increase or decrease prices."
        ],
        [
            "Which is dual decomposition.",
            "OK, so I think I'll."
        ],
        [
            "I'll skip this and just mention that this drops directly into.",
            "Don't things like dynamic energy management, right?",
            "So here if you're exchanging power in a bunch of periods, these are just these are completely separate commodities.",
            "They are completely independent, and in fact this just works perfectly for that, and I think I won't.",
            "I think maybe what I'll do is skip ahead so that if people have questions where I'm not going to run out of Question Time, so I think I'll I'll even skip this."
        ],
        [
            "Whoops, sorry.",
            "And just go right to the conclusions.",
            "OK, so good, pretty simple conclusions.",
            "Hmm, it's identical to Douglas Rachford splitting it from the 70s.",
            "I could make a pretty good argument that the seeds of the idea extend to the mid 50s.",
            "I mean, probably those who did it then wouldn't recognize it, but that's too bad.",
            "They would after I argued with him for a while anyway.",
            "So you get.",
            "I mean, it's nice you get single processor algorithms that can be competitive with state of the art, so you can implement things very quickly.",
            "But the main interesting thing is it allows you to coordinate a lot of processors, each solving a substantial problem to solve a very large problem.",
            "And I think what's very different about sort of this approach to the methods I have seen people implement.",
            "For large scale optimization and machine learning they do things like take stochastic gradient.",
            "I mean that's perfectly OK and then map that onto some modern framework.",
            "This is actually quite different.",
            "This says if you want to unregularized logistic regression, no problem.",
            "Have a bunch of have a bunch of workers doing it, but they're not just doing stochastic gradient calculations and they're actually doing full up.",
            "Each one is doing full up logistic Reg, logistic minimization, maybe with sorry.",
            "With a quadratic regularization term right?",
            "So it's actually quite different way to do this.",
            "Unfortunately, it Maps very poorly onto onto modern frameworks only because.",
            "No one asked modern frameworks to do things like this before, but maybe that'll maybe that'll get fixed.",
            "Maybe maybe it won't, but hopefully it will be.",
            "And then maybe these will be viable methods.",
            "OK, so I'll I'll quit here.",
            "Oh, I'm so glad you asked that, so I'll repeat the question, how do you choose the answer?",
            "Oh, you want the honest answer?",
            "We don't know.",
            "So my.",
            "The question was the honest answer.",
            "Oh the semi honest answer.",
            "Well.",
            "Let's see I can I can make it up.",
            "I can make one up on the fly so I could say this, but notice that it's not an answer I could say.",
            "That there's values of row in a wide range.",
            "We get reasonable performance and if you like, did you like that?",
            "And I'd also say the following if you do any kind of caching or something like that, then it's less relevant here.",
            "Whether you do 20 or 200 steps, then in an algorithm where each step costs about the same amount.",
            "But I do want to give Full disclosure that that was not an answer, so so it's.",
            "I mean, we've we've looked at, it obviously has to do with sort of the curvature of F&G and things like that.",
            "But in the case when F&G are non differentiable, which is the whole point here, right?",
            "'cause if F&G have a curvature that you know about, you should probably be using like accelerated type methods.",
            "You know you should nest arises that a verb.",
            "I think it is.",
            "Anyway around Stanford, people refer to that algorithm and not this right?",
            "So?",
            "I mean for things with Lipschitz, I mean for things that are which have known curvatures, you could answer that question.",
            "But here in the general case.",
            "We don't really know.",
            "Any other questions I can given an answer to, yeah.",
            "That's that's actually what I think.",
            "How do you we scale?",
            "We scale data appropriately and then take row equals one.",
            "Oh, I didn't know.",
            "That's the kind of thing I don't like to say on on the record.",
            "I've heard there are people who do that.",
            "Yeah, sure, OK, so actually that was even known in the 80s.",
            "The question was, what about in exact minimization?",
            "And so?",
            "In fact, even in the in the 80s when the sub problems were solved by an iterative method, one of the obvious things is, you know when you're starting this out, you don't.",
            "There's no reason to minimize these augmented lagrangians exactly, so even even then it was worked out and their simple conditions would be things like.",
            "But they all the simple condition is this.",
            "If the sum of the suboptimality's in solving the subproblems.",
            "My night then it converges, right?",
            "But the rough idea is the the precision which you solve.",
            "The subproblems should go down as you as you as you run, right?",
            "So but OK, now that I've already like you, know how we do it ready ready for this?",
            "Can you OK?",
            "Can you handle it right so we do 15 steps of like LBF's just.",
            "15 and call it a day works fine.",
            "But yes, maybe you can easily prove that you converge to within that some Epson.",
            "So if you do not let epsilon disappear right, you can easily prove that you converge to within epsilon.",
            "Epsilon does good you know why epsilon goes to 0, because when you get down to where the solution 15 steps of LBF's is more than enough to do the trick, remember this regularization, right?",
            "So with regularization of like a logistic function, trust me.",
            "15 steps.",
            "When you get into, the solution is way more than enough.",
            "So theoretically, yes, you.",
            "You could you could do that, I presume.",
            "Yes.",
            "Right?",
            "Just one.",
            "Right, actually sorry for the map ones.",
            "There's a unique one, right?",
            "Because you've added this quadratic regularization right?",
            "So it's unique.",
            "Right, yeah, right.",
            "So with the quote.",
            "Sorry with an A in there then it's not unique 'cause you could have anything in sort of the null space of a or something like that and then it doesn't matter what it what it returns.",
            "So yeah, Oh yeah, so I never I mentioned that they never never got back to it.",
            "So the question is how do you split into three and things like that?",
            "And you can work things like that out and I should mention that the Russian style of all this was to split into K sets right off the bat, but it turns out you know once you study it more, you realize that we split into two, but the key was that in one of them the function was separable.",
            "So what looked like a single minimization was actually split was splitting right?",
            "So I think there are two styles.",
            "I think they're kind of the same.",
            "Or at least we can do what we want.",
            "I can it, like the algorithm itself.",
            "The master algorithm splits only into two things, but each of the steps might split into parallel things.",
            "So did that.",
            "That's yeah, that's possible, so I won't deny it.",
            "But what's amazing is how far you can get with two separate ability.",
            "Yeah.",
            "Reconstruction.",
            "So how do you warm start it?",
            "Don't you just?",
            "It's no problem.",
            "I mean you start it from, you have to warm.",
            "Start with primal and dual variables.",
            "Obviously you can't just reinitialize the dual variable zero or you spend all your time rebuilding up to dual variable.",
            "But if you initialize the primal variable and the dual variable you'll do very well.",
            "So the other thing is of course for things like that, for reweighting, if those weights.",
            "If you do a splitting where the weights appear in the in the park but not in the linear algebra part, then you can do factorization caching.",
            "I know you can't because your problems are too big.",
            "I know I know.",
            "OK, OK that's what you're saying.",
            "And yeah, but you know what?",
            "There's ways to wiggle things around so they don't even when they appear to appear in the.",
            "In the quarter, but it doesn't matter.",
            "You do image processing.",
            "Those problems are too big anyway, so you can't use a direct method, but OK. OK, yes.",
            "That's an excellent question.",
            "Not really right in dual decomposition, of course.",
            "When you minimize that you gotta you gotta deal feasible.",
            "You get a lower lower bound and actually you don't in a DMM so you can get things that are surrogates for it.",
            "So there are some excellent surrogates for it, right?",
            "But like an interior point methods, right?",
            "What's actually looking at an interior point method generally is you're looking at something that would be the dual residual if the if the other residuals were zero, which they're not or something.",
            "In the end, they're all small, and then everything is cool, but and you get something like that here, it has the same flavor.",
            "Yeah.",
            "For DC problem.",
            "Actually yes, so for non convex problems.",
            "In fact there's an interesting thing we did here where you can take a DMM and run it for non convex split F&G.",
            "And here's an example.",
            "Let's take Jeep which was a Lambda times in one norm OK and you say well that's a sparsifying regularizer.",
            "But let's say let's do the following.",
            "Let's let G be the indicator function of the set of vectors with no more than K nonzeros.",
            "Right, so it's the indicator function of.",
            "It's a horrible non convex set, but we can compute Euclidean projection onto it exactly.",
            "Right 'cause how do you project onto you?",
            "You find the largest K elements and user out all the others.",
            "I mean it's simple, right?",
            "So then you can run a DMM.",
            "OK and then and then you you get some bonus points for solving each subproblem you're solving globally, including the nonconvex one.",
            "Then you might ask what can you say about the whole thing?",
            "And as far as I know, the answer is absolutely nothing.",
            "However, it does seem to work quite well.",
            "I mean, that's kind of weird, like if you do the if you take Lambda and then very landed to get the sparsity fit tradeoff and then do the same thing with this algorithm, about which you know nothing.",
            "You get pretty much the same tradeoff, so.",
            "It can do with it whatever you want.",
            "It's just I'm just telling you so.",
            "Would be great to actually be able to say something about that.",
            "Magical values of know you might be able to see you might be able to say something you know.",
            "My suspicion, though, is that what you would say would ultimately be it be fine, you know it'll be OK.",
            "It's going to be a little boring.",
            "It's going to be something like this.",
            "Yeah, whole bunch of conditions and you'll say in that case a DMM converted some local minimum.",
            "You know, frankly, big deal.",
            "I mean converted to local minimum.",
            "I mean, so I mean, actually algorithms always converge because you just put in you always right?",
            "For it are equals one to Max enter.",
            "So I don't really.",
            "I mean, I don't really understand this fetish about knowing what does this mean.",
            "It means, like you'd say, no, there's a huge advance.",
            "You don't understand.",
            "This means it means that if we were to run this album forever, which we would never do, then it would eventually converge to something you would that be the solution?",
            "You say no.",
            "Sci-fi anyway.",
            "To be able to say after you stop after 100 steps right did you reach?",
            "Right, that is hard to say actually.",
            "No, no.",
            "Is it exactly?",
            "It's probably even a local minimum quality stationary point, right?",
            "So OK so.",
            "Yes.",
            "Oh, that's simple.",
            "Actually that was answered by someone else's question about the uniqueness.",
            "Just take a matrix, a right which has a nullspace an I'll arrange so that any you know any element in the null space is a salute.",
            "You know you have a big big optimal set.",
            "And then when you return an Arg min, maybe you have a choice.",
            "You have a whole subspace of things you can choose, so you simply return different ones each time.",
            "Just for fun, right?",
            "And what will happen is of course X won't converge.",
            "However, I can tell you what will happen.",
            "The primal residual will go to zero and the objective value will go to the optimal value, but your ex is jumping all around.",
            "But if you think carefully about what you need in practice.",
            "So what if X is not converging?",
            "It's totally irrelevant and it means nothing in practice.",
            "That means you probably have a silly optimization problem is what it really means, right?",
            "I mean not you personally I'm just saying, you know.",
            "You know?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about, well, alternating direction method of multipliers and just to give you a rough idea of what what.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to, you know the type, what the talk is going to be.",
                    "label": 0
                },
                {
                    "sent": "I just going to kind of like tutorial and it's going to be sort of background.",
                    "label": 0
                },
                {
                    "sent": "I know there are people in this room who know actually way more than what I'm going to talk about in this talk, so I apologize to them.",
                    "label": 0
                },
                {
                    "sent": "Some of them actually even came willingly.",
                    "label": 0
                },
                {
                    "sent": "I don't know why, but there they are here, but not for the others, is just supposed to be an introduction to.",
                    "label": 1
                },
                {
                    "sent": "To do one method for for doing distributed optimization and will get that and I'll just show the simplest step.",
                    "label": 0
                },
                {
                    "sent": "All of this is taken from something that's much longer, but still kind of tutorial in nature, and it's one of these foundations and trends booklets.",
                    "label": 0
                },
                {
                    "sent": "It's everything is available online, I mean including source code.",
                    "label": 0
                },
                {
                    "sent": "For every example you'll see in things like that, OK?",
                    "label": 0
                },
                {
                    "sent": "Oh, and this is joint work with a with a bunch of coauthors, and in fact I should say this is mostly work from the 70s.",
                    "label": 0
                },
                {
                    "sent": "In fact, it could be argued to have come in from.",
                    "label": 0
                },
                {
                    "sent": "You can trace it to the 50s.",
                    "label": 0
                },
                {
                    "sent": "So so this is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's the goal?",
                    "label": 0
                },
                {
                    "sent": "The goal is.",
                    "label": 0
                },
                {
                    "sent": "I mean, you really want to do something like arbitrary scale optimization, so and you know the most.",
                    "label": 0
                },
                {
                    "sent": "I mean, the most obvious example, certainly for everyone here is to do machine learning and statistics with huge datasets.",
                    "label": 0
                },
                {
                    "sent": "I mean that's that's obvious, but actually there are many other applications, so not all optimization problems are machine learning ones.",
                    "label": 0
                },
                {
                    "sent": "An example of another one would be dynamic optimization on a large scale network, just to take one example.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can take electricity dispatch.",
                    "label": 0
                },
                {
                    "sent": "I mean just take one.",
                    "label": 0
                },
                {
                    "sent": "So in California is maybe like 3100 generators.",
                    "label": 0
                },
                {
                    "sent": "They schedule in 15 minute intervals for 24 hours.",
                    "label": 0
                },
                {
                    "sent": "So for each generator you have to work out 100 numbers.",
                    "label": 0
                },
                {
                    "sent": "That's still that's not bad.",
                    "label": 0
                },
                {
                    "sent": "It's I don't know what we're up to.",
                    "label": 0
                },
                {
                    "sent": "That's like 30,000 variable it still.",
                    "label": 0
                },
                {
                    "sent": "And it's not a small problem, but it's not huge.",
                    "label": 0
                },
                {
                    "sent": "However, all of that has to happen over a network with about 5000 nodes and maybe 15,000 edges, and there are variables for the power flow on every edge that that network is capacit ated.",
                    "label": 0
                },
                {
                    "sent": "And guess what?",
                    "label": 0
                },
                {
                    "sent": "Now you have a really big problem.",
                    "label": 0
                },
                {
                    "sent": "I mean bigger than in fact anyone can handle.",
                    "label": 0
                },
                {
                    "sent": "By the way the lights to come on in California.",
                    "label": 0
                },
                {
                    "sent": "So you might ask how is that solved?",
                    "label": 0
                },
                {
                    "sent": "And it solved in an ad hoc way.",
                    "label": 0
                },
                {
                    "sent": "They alternate between going back and forth between sort of network optimization and then.",
                    "label": 0
                },
                {
                    "sent": "Scheduling generators and they go back and forth a couple of times and usually it works and the lights come on the next day in California.",
                    "label": 0
                },
                {
                    "sent": "So mostly I should say OK, OK, now the approach is going to be a decentralized optimization and so the idea is that you know you have different devices, processors, agents, workers, threads, whatever you want to call it.",
                    "label": 0
                },
                {
                    "sent": "Different entities, computing entities they'll coordinate to solve a large problem, bypassing relatively small messages, and in fact.",
                    "label": 1
                },
                {
                    "sent": "That's actually an important point here that this is this is not going to be something like implementing stochastic gradient descent on data center or something like that, where lots of data is flowing all over the place.",
                    "label": 0
                },
                {
                    "sent": "I mean, these things are optimized for that, so that's fine.",
                    "label": 0
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "In contrast, is the idea is going to be relatively small messages and a fair amount of heavy lifting is going to be done at each worker or node, and then relatively small messages are going to flow.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the idea.",
                    "label": 0
                },
                {
                    "sent": "And obviously this is going to the math doesn't matter, but it operates at many levels, right?",
                    "label": 0
                },
                {
                    "sent": "So this could be.",
                    "label": 0
                },
                {
                    "sent": "This could be something like.",
                    "label": 0
                },
                {
                    "sent": "This could just be how to take.",
                    "label": 0
                },
                {
                    "sent": "You could have multiple threads and just take advantage of different cores.",
                    "label": 0
                },
                {
                    "sent": "This could be something operating a data center could be Jeep, target GPU's and all that sort of stuff and I won't even talk about that but there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lots of applications.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the outline.",
                    "label": 0
                },
                {
                    "sent": "So I'll go way back in history and actually the first 2 sections are history.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe history, and I'll say a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is just the set up for the method we're going to talk about, so it should be.",
                    "label": 0
                },
                {
                    "sent": "Maybe review for most.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so we'll start with dual decomposition.",
                    "label": 1
                },
                {
                    "sent": "And that starts.",
                    "label": 0
                },
                {
                    "sent": "This way you want to solve this convex optimization.",
                    "label": 1
                },
                {
                    "sent": "Minimize a convex function subject to some equality constraints, and so your formula grangeon.",
                    "label": 0
                },
                {
                    "sent": "So which basically says you're going to allow the equality constraint to be violated and you'll have a residual, and you have a dual vector or a price vector, and you can interpret this as something like a payment either to or from.",
                    "label": 0
                },
                {
                    "sent": "It's either like a charge or a subsidy, depending on the sign for what happened.",
                    "label": 0
                },
                {
                    "sent": "What happens if you violate the constraints?",
                    "label": 0
                },
                {
                    "sent": "If you minimize in this in this framework over X another let you allow yourself to violate the constraints, but you either pay or paid for it.",
                    "label": 1
                },
                {
                    "sent": "Through this dual variable Y you get the dual function and.",
                    "label": 0
                },
                {
                    "sent": "You then I mean this is a dual function that's always concave, and if you maximize the dual function and if all goes well, the maximizer is in fact a solution of this now that there are lot there actually details there.",
                    "label": 0
                },
                {
                    "sent": "And they're not just little baby technical details, because in fact for some of the problems you might like to solve, like linear programs, this scheme fails utterly right?",
                    "label": 0
                },
                {
                    "sent": "So these are not just sort of little mathematical details.",
                    "label": 0
                },
                {
                    "sent": "There are things that make this not work, but you know, with enough assumptions you know one being like F is is, you know, has a minimum.",
                    "label": 0
                },
                {
                    "sent": "Curvature or something like that.",
                    "label": 0
                },
                {
                    "sent": "This method works OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how would you?",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 0
                },
                {
                    "sent": "How do you maximize the dual function?",
                    "label": 0
                },
                {
                    "sent": "Well, you would use a gradient method, or in the case when it's not differentiable.",
                    "label": 1
                },
                {
                    "sent": "This subgradient method and so you would calculate the gradient and then make a step in that direction.",
                    "label": 0
                },
                {
                    "sent": "These are a positive step lengths.",
                    "label": 0
                },
                {
                    "sent": "Well the gradient of that dual function is nothing but the residual, so the algorithm then looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's called dual accent and this is maybe maybe from the 60s, but probably people knew it in Moscow in the 50s or something like that.",
                    "label": 0
                },
                {
                    "sent": "And so I mean, that's where it all.",
                    "label": 0
                },
                {
                    "sent": "That's role of this comes from.",
                    "label": 0
                },
                {
                    "sent": "And so the dual asset method looks like this and this should remember because you're going to be seeing this for the rest of this hour.",
                    "label": 0
                },
                {
                    "sent": "OK, so it looks like this.",
                    "label": 0
                },
                {
                    "sent": "You take this Lagrangian, you fix the price vector or dual vector, then you minimize over X. Yeah, you minimize over X, you calculate the residual.",
                    "label": 0
                },
                {
                    "sent": "But if the residual zero you quit because your optimal you stop.",
                    "label": 0
                },
                {
                    "sent": "If it's not zero, you update the price or dual vector.",
                    "label": 0
                },
                {
                    "sent": "In fact, by some positive number times the residual.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "And it's got other names in economics.",
                    "label": 0
                },
                {
                    "sent": "You'd call this a like a tattoo on mom procedure or something like that, or a price adjustment procedure.",
                    "label": 0
                },
                {
                    "sent": "And the idea is the ax equals B constraint.",
                    "label": 0
                },
                {
                    "sent": "There would be something like a market clearing and then this is a price adjustment mechanism, right?",
                    "label": 1
                },
                {
                    "sent": "And that so it's got other names and long traditions in lots of fields, OK?",
                    "label": 0
                },
                {
                    "sent": "Are you first of all you might ask, you know why, right?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why should you solve the dual instead of the primal or something like?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set an and the reason has to do with decomposition.",
                    "label": 0
                },
                {
                    "sent": "So suppose the function you're minimizing.",
                    "label": 0
                },
                {
                    "sent": "The objective is separable, so it's a sum of functions of blocks of the original vector, right?",
                    "label": 0
                },
                {
                    "sent": "So it separates out like this.",
                    "label": 0
                },
                {
                    "sent": "These could be all the way down to the components, but these could be vectors as well, so this is separable.",
                    "label": 0
                },
                {
                    "sent": "Well, in that case the Lagrangian is separable, right?",
                    "label": 1
                },
                {
                    "sent": "And because you're adding to it now find function.",
                    "label": 1
                },
                {
                    "sent": "But all affine functions are separable down to the component, so this is separable.",
                    "label": 0
                },
                {
                    "sent": "And what that says is that this man.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm isation here over the Lagrangian over X actually splits and can be done in parallel because it's because you're minimizing something that is a sum of functions of independent variables.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just do it in parallel and that leads to dual decomposition and that looks like this, and again, I know that many people know this, But this is just sort of to give you historical background and review the way that works is this each of these things minimizes the Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "You have a price vector, each minimizes their Lagrangian term separately.",
                    "label": 0
                },
                {
                    "sent": "You collect the contribution to the equality constraint and you evaluate the residual.",
                    "label": 0
                },
                {
                    "sent": "If it's zero, you quit.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you update the price vector and you can see that what this requires is something like a gathering a scatter, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to gather these components to evaluate the residual and you scatter the price vector right?",
                    "label": 0
                },
                {
                    "sent": "So that now you have now you're doing distributed optimization.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. All right, next topic is, and that's maybe from around 1960 something like that.",
                    "label": 0
                },
                {
                    "sent": "Next topic is method of multiply.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what is method of multipliers?",
                    "label": 1
                },
                {
                    "sent": "Well, it's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's a method that cemented robust applied dual asset, right?",
                    "label": 0
                },
                {
                    "sent": "So to actually make it so that it would work on things like linear programming and stuff like that, which this absolutely will not.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is goes back, at least in the 60s.",
                    "label": 0
                },
                {
                    "sent": "But in fact it can be argued to go earlier to operator splitting methods.",
                    "label": 0
                },
                {
                    "sent": "But in any case this to the normal.",
                    "label": 0
                },
                {
                    "sent": "This is the usual Lagrangian term and you can think of this is sort of like a.",
                    "label": 0
                },
                {
                    "sent": "Your free market term this says go ahead and violate the constraints, but you pay for it at these prices.",
                    "label": 0
                },
                {
                    "sent": "Pay for it or you're subsidized for it.",
                    "label": 0
                },
                {
                    "sent": "Depends on which end of that you're on.",
                    "label": 0
                },
                {
                    "sent": "And then you add another term which is always non negative and it's just a quadratic penalty, right?",
                    "label": 0
                },
                {
                    "sent": "And this this this one is.",
                    "label": 0
                },
                {
                    "sent": "That's always sort of a cost.",
                    "label": 0
                },
                {
                    "sent": "It is never a subsidy right?",
                    "label": 0
                },
                {
                    "sent": "So this is this is simply a cost for not for the market not clearing.",
                    "label": 0
                },
                {
                    "sent": "For example in economics right?",
                    "label": 1
                },
                {
                    "sent": "So it's an augmented Lagrangian and the method of multipliers looks like this.",
                    "label": 1
                },
                {
                    "sent": "You minimize the augmented Lagrangian and then you do a dual update and it's exactly the same as.",
                    "label": 0
                },
                {
                    "sent": "It's identical to dual dual ascent except for one thing the the gain in the gradient update has a very specific choice of step length, it's row.",
                    "label": 0
                },
                {
                    "sent": "It's the parameter here, right?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a reason for that, and it's easy to understand that the original problem, I mean when it's differentiable.",
                    "label": 0
                },
                {
                    "sent": "Original problem, the optimality conditions are primal optimal feasibility, so you have to have X = B and then you have to have dual feasibility, which is that the gradient plus a transpose Y star is zero and if XK plus one minimizes this thing, well you workout this.",
                    "label": 0
                },
                {
                    "sent": "Again, this is a differentiable case, but it's the same in non differentiable case.",
                    "label": 0
                },
                {
                    "sent": "You workout what what it means to minimize that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well it just means that the gradient of the augmented Lagrangian is zero when you workout.",
                    "label": 0
                },
                {
                    "sent": "What that is and you recognize this term here and you realize hey.",
                    "label": 0
                },
                {
                    "sent": "If I were to up if this were to be white K plus one, then it says the following.",
                    "label": 0
                },
                {
                    "sent": "It says that at the end of method multiplies each step you have dual feasibility, always right?",
                    "label": 0
                },
                {
                    "sent": "So so in fact what it says is that with that choice of, with that choice of update dual update you get dual feasibility.",
                    "label": 0
                },
                {
                    "sent": "Sort of that happens for free then what your means is as the algorithm progresses, what you're hoping is that the is that the residuals the primal residuals will go to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now it's good news and bad news.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that adding the quadratic augmenting term when I should add, you know, you can add other types of augmenting terms, and that's become very popular in the last couple of years.",
                    "label": 0
                },
                {
                    "sent": "We had Bregman divergences or things like that.",
                    "label": 0
                },
                {
                    "sent": "I'll just keep I'll stick with quadratic because it's simple and if you understand that you can surely understand what happens if you had like an entropy or something like that there.",
                    "label": 0
                },
                {
                    "sent": "So OK, the good news is adding that adding that regularization makes this go makes dual.",
                    "label": 0
                },
                {
                    "sent": "Assend go from a very fragile algorithm that has to kind of be coaxed to work, and in most cases you would think of doesn't work at all to something absolutely completely robust.",
                    "label": 0
                },
                {
                    "sent": "It just works.",
                    "label": 0
                },
                {
                    "sent": "Under all circumstances.",
                    "label": 0
                },
                {
                    "sent": "Really basically converges always.",
                    "label": 0
                },
                {
                    "sent": "So if the problem including ridiculous things where F can take on the value plus Infinity, that can be nondifferentiable.",
                    "label": 1
                },
                {
                    "sent": "In fact, one of the will see a lot of the interesting cases F is an indicator function, meaning it's zero on some convex set, and then plus Infinity off it.",
                    "label": 0
                },
                {
                    "sent": "That in depth indicate that set could even be thin.",
                    "label": 0
                },
                {
                    "sent": "It could, it could be flat, it could have non empty interior, right?",
                    "label": 0
                },
                {
                    "sent": "So these are not nice functions from the analyst point of view.",
                    "label": 0
                },
                {
                    "sent": "And it just works perfectly here.",
                    "label": 1
                },
                {
                    "sent": "So that's the good news.",
                    "label": 0
                },
                {
                    "sent": "It's now rock solid.",
                    "label": 0
                },
                {
                    "sent": "It's robust.",
                    "label": 1
                },
                {
                    "sent": "The bad news is it destroys the splitting of the X update.",
                    "label": 0
                },
                {
                    "sent": "So you can't do decomposition anymore, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and that is the historical back.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ground that brings us to a DMM.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that something like a method that has good rub good, good robustness of the method of multipliers.",
                    "label": 1
                },
                {
                    "sent": "That's and it's supposed to support decomposition right?",
                    "label": 0
                },
                {
                    "sent": "And so this is the way you should think of it.",
                    "label": 0
                },
                {
                    "sent": "And this was proposed in well, 1976.",
                    "label": 0
                },
                {
                    "sent": "Although it turned out later in the mid 80s, it was discovered that you could re derive this algorithm from material from the 50s, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe they did note in the 50s probably I don't know.",
                    "label": 0
                },
                {
                    "sent": "They probably didn't explicitly know it, but anyway, so.",
                    "label": 0
                },
                {
                    "sent": "But the main point is this is this.",
                    "label": 0
                },
                {
                    "sent": "This is not new right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "But by the way, neither is stochastic gradient descent or any any.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any of these other things so OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so the form is this.",
                    "label": 0
                },
                {
                    "sent": "I should say this is the Western form, not the Russian form.",
                    "label": 0
                },
                {
                    "sent": "There's a Russian form there, slightly different.",
                    "label": 0
                },
                {
                    "sent": "It turns out there kind of the same.",
                    "label": 0
                },
                {
                    "sent": "So in the in the Western form you split the variable which we had previously called X into two groups.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call it X&Z.",
                    "label": 0
                },
                {
                    "sent": "Night we ask that the objective should be separable across X&Z and we have a completely general equality constraint linking X&Z, so that's a completely general constraint.",
                    "label": 0
                },
                {
                    "sent": "That's it?",
                    "label": 0
                },
                {
                    "sent": "OK, FNG or convex?",
                    "label": 0
                },
                {
                    "sent": "You form an augmented Lagrangian and Adm is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "It looks like all the other.",
                    "label": 0
                },
                {
                    "sent": "The other things we've been looking at for a while.",
                    "label": 0
                },
                {
                    "sent": "You have a dual vector Y and it says this it actually it's one Gauss Seidel sweep in the method of multipliers.",
                    "label": 1
                },
                {
                    "sent": "So it says.",
                    "label": 0
                },
                {
                    "sent": "Optimize over X.",
                    "label": 0
                },
                {
                    "sent": "Then Z, then you do a dual variable update and notice it's identical to method of multipliers, right?",
                    "label": 1
                },
                {
                    "sent": "It's got the row there.",
                    "label": 0
                },
                {
                    "sent": "Everything is the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Now you can already see some nice things going to happen.",
                    "label": 0
                },
                {
                    "sent": "The first thing is you never ever optimize over X&Z at the same time, right?",
                    "label": 0
                },
                {
                    "sent": "So basically this says if you have a method for handling minimization over X and a method for handling minimization over Z, this will combine them into something that can minimize problem.",
                    "label": 0
                },
                {
                    "sent": "We can solve problems that involve minimization over both.",
                    "label": 0
                },
                {
                    "sent": "Right, and I mean, usually this would be special methods?",
                    "label": 0
                },
                {
                    "sent": "Well, we'll see specific examples.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, and I already said this.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you were to minimize over X is jointly, you get method of multipliers.",
                    "label": 1
                },
                {
                    "sent": "One thing that should be clear is you're welcome to minimize over X&Z as many times as you feel like before you update the dual variable, right?",
                    "label": 1
                },
                {
                    "sent": "And if you do it alot you basically have you have method of multipliers OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it'll be examples will make the ability to split obvious, so here you can say something similar to why the update, and you can actually understand how this works here.",
                    "label": 0
                },
                {
                    "sent": "Well, we split our primal variable into two variables, X&Z and.",
                    "label": 0
                },
                {
                    "sent": "So there are there's primal feasibility.",
                    "label": 1
                },
                {
                    "sent": "Equality constraints have to hold and then you have dual feasibility.",
                    "label": 0
                },
                {
                    "sent": "These are really stacked, so for their two equations there's X dual feasibility and why dual feasibility?",
                    "label": 1
                },
                {
                    "sent": "Now when when you minimize the second step, you minimize Z over this thing and you workout what it is and then you recognize all this and you say hey this plus ro times this thing.",
                    "label": 0
                },
                {
                    "sent": "The residual is in fact that's our updated dual variable.",
                    "label": 1
                },
                {
                    "sent": "What you see is this.",
                    "label": 0
                },
                {
                    "sent": "It says the following.",
                    "label": 0
                },
                {
                    "sent": "It says that in a DMM there's three optimality conditions, primal and two dual and it says that the the second dual is free that when you just simply do your dual update.",
                    "label": 0
                },
                {
                    "sent": "You get your second.",
                    "label": 0
                },
                {
                    "sent": "You get your second dual feasibility condition for free, and so we're simply waiting for this primal residual, and this dual residual to converge to 0.",
                    "label": 1
                },
                {
                    "sent": "So that's that's what it comes down to, OK?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now one thing I should I should point out.",
                    "label": 0
                },
                {
                    "sent": "It's kind of obvious, but it simplifies things.",
                    "label": 0
                },
                {
                    "sent": "It's this if you have a.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear function into that, yet quadratic even combine these into one.",
                    "label": 0
                },
                {
                    "sent": "You just complete the square or whatever you want to call it and you combine this into a single quadratic and then plus a constant which I've dropped because it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You're just minimizing right?",
                    "label": 0
                },
                {
                    "sent": "So you'll see in this case you is this scaled dual variable.",
                    "label": 0
                },
                {
                    "sent": "It's one over Rd times YK.",
                    "label": 0
                },
                {
                    "sent": "And so sometimes you get shorter formulas with this.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's maybe clearer with this because you see explicitly.",
                    "label": 0
                },
                {
                    "sent": "I mean, this has a nicer story, right?",
                    "label": 0
                },
                {
                    "sent": "That's the vector of prices in some way.",
                    "label": 0
                },
                {
                    "sent": "You could explain anybody what it means, right?",
                    "label": 0
                },
                {
                    "sent": "You know when it converges, these are the optimal prices.",
                    "label": 0
                },
                {
                    "sent": "This is zero, and then you point to this term and you say what's that?",
                    "label": 0
                },
                {
                    "sent": "That's a cost for violating.",
                    "label": 0
                },
                {
                    "sent": "This is harder to more compact formulas, so we'll see both.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the convergence theory.",
                    "label": 0
                },
                {
                    "sent": "You make the absolute minimum assumptions you could possibly make would basically is.",
                    "label": 0
                },
                {
                    "sent": "This functions are convex, closed proper and the other thing is that the problem has to have a solution.",
                    "label": 1
                },
                {
                    "sent": "Otherwise it's silly to be talking about an algorithm for solving it.",
                    "label": 0
                },
                {
                    "sent": "OK, so literally that's it.",
                    "label": 0
                },
                {
                    "sent": "There are no weaker assumptions you could possibly make none, and the theorem is this again from from the 70s, but you can go to the 50s, you know you take your choice as to when this was known.",
                    "label": 0
                },
                {
                    "sent": "The theorem is it works so, and this is very carefully worded here.",
                    "label": 0
                },
                {
                    "sent": "So this says.",
                    "label": 0
                },
                {
                    "sent": "Here's here's what you can say.",
                    "label": 0
                },
                {
                    "sent": "By the way, there have been no assumptions whatsoever on A&B, for example here, none.",
                    "label": 0
                },
                {
                    "sent": "So what is asserted is this.",
                    "label": 1
                },
                {
                    "sent": "The iterates the primal residual goes to zero, and the objective value goes to the optimal value.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's true always.",
                    "label": 0
                },
                {
                    "sent": "Here's what's not said.",
                    "label": 0
                },
                {
                    "sent": "There are many things not said here because they are false, but here they are.",
                    "label": 0
                },
                {
                    "sent": "And when if you look this up another reference, other stuff, you'll find it's a lot more complicated than this, and the reason is you know your hypothesis will go on for half a page, and the reason is this.",
                    "label": 0
                },
                {
                    "sent": "They'll say things like this.",
                    "label": 0
                },
                {
                    "sent": "Here's what's not said here.",
                    "label": 0
                },
                {
                    "sent": "XC converges, that's false, does not converge.",
                    "label": 0
                },
                {
                    "sent": "It did not converge.",
                    "label": 0
                },
                {
                    "sent": "ZC converges.",
                    "label": 0
                },
                {
                    "sent": "There's a stronger statement.",
                    "label": 0
                },
                {
                    "sent": "XK converges to the optimal set.",
                    "label": 0
                },
                {
                    "sent": "That's false too.",
                    "label": 0
                },
                {
                    "sent": "KZK convergence, the optimal set that's false too.",
                    "label": 0
                },
                {
                    "sent": "OK, so so what's being said here?",
                    "label": 0
                },
                {
                    "sent": "Or here's another one XC and ZC converge to X star Onsie start Witcher optimal.",
                    "label": 0
                },
                {
                    "sent": "That's also false.",
                    "label": 0
                },
                {
                    "sent": "OK, so very carefully saying minimum, but if you actually go back and think if you really sit back and think by the way, a lot of those other statements become true.",
                    "label": 0
                },
                {
                    "sent": "But you have to start making lots of assumptions and the whole point of this is it works no matter what, so I don't aesthetically I don't get it, I don't.",
                    "label": 0
                },
                {
                    "sent": "What's that that you can actually say?",
                    "label": 0
                },
                {
                    "sent": "Why does converge to why star?",
                    "label": 0
                },
                {
                    "sent": "So I didn't write it down here, but that's that's actually something you can say an is true.",
                    "label": 0
                },
                {
                    "sent": "Actually, right?",
                    "label": 0
                },
                {
                    "sent": "So so you can say things that aren't true too.",
                    "label": 0
                },
                {
                    "sent": "Right, and I certainly don't mind doing that, but.",
                    "label": 0
                },
                {
                    "sent": "But I don't do it when I'm being recorded though.",
                    "label": 0
                },
                {
                    "sent": "So well I try not too.",
                    "label": 0
                },
                {
                    "sent": "I should say so they can have plausible deniability.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually if you think my feeling.",
                    "label": 0
                },
                {
                    "sent": "But this is actually very simple, the more you think about it, the more you realize that in fact all those other statements are utterly irrelevant.",
                    "label": 0
                },
                {
                    "sent": "I had I've had screaming fights with people who said things like like you don't care that X Kenzie Kai don't converge to some optimal values, and I said, no, I don't.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you think carefully about it in any algorithm, if you implement anything you don't care about anything but these two things, because these these are stopping.",
                    "label": 0
                },
                {
                    "sent": "Criterion will be based on this.",
                    "label": 0
                },
                {
                    "sent": "And it's utterly irrelevant that weather XC converges to a Nexstar or ZC convergence too noisy star or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So I mean just go backwards from do to utilitarian approach and you say it's a numerical method.",
                    "label": 0
                },
                {
                    "sent": "What would you consider to be an approximate solution?",
                    "label": 0
                },
                {
                    "sent": "And I think no one would say anything other than that except.",
                    "label": 0
                },
                {
                    "sent": "Quality constraints and you wanted to identify particular about did I get active set or you want to get the active set right?",
                    "label": 0
                },
                {
                    "sent": "If you care bout active set then you you can actually stop at some point and then switch to some active set method or something.",
                    "label": 0
                },
                {
                    "sent": "I mean people do that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I should say I call it.",
                    "label": 0
                },
                {
                    "sent": "I mean I'm calling an ATM.",
                    "label": 0
                },
                {
                    "sent": "In fact it turns out it is related to an exactly the same as many other methods.",
                    "label": 0
                },
                {
                    "sent": "So let me explain some of that.",
                    "label": 0
                },
                {
                    "sent": "First of all, I absolutely identical to something called Douglas Rachford Douglas Rachford splitting operator splitting.",
                    "label": 0
                },
                {
                    "sent": "It's identical.",
                    "label": 0
                },
                {
                    "sent": "I mean if you choose the right operator to split and split it the right way, it's not related, it's identical.",
                    "label": 0
                },
                {
                    "sent": "So another way you'll hear this is Douglas Rachford der splitting, so it's not that it's the similar.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                },
                {
                    "sent": "It's related to things like Douglas Peaceman Rachford splitting, and this stuff goes into the 50s.",
                    "label": 1
                },
                {
                    "sent": "Let's see all of these.",
                    "label": 0
                },
                {
                    "sent": "It's also it is exactly the same as the proximal point algorithm, which is very general, but applied to the right operator.",
                    "label": 1
                },
                {
                    "sent": "Then it's identical.",
                    "label": 0
                },
                {
                    "sent": "I mean you course you have to figure out what the right operator is, But anyway.",
                    "label": 0
                },
                {
                    "sent": "And a lot of this wasn't known until maybe the mid 80s.",
                    "label": 0
                },
                {
                    "sent": "In fact I mean so there were papers on all of these kinds of things, all different, and it wasn't known that they were kind of all the same.",
                    "label": 0
                },
                {
                    "sent": "It's identical, or this is a special case of it is Dykes resulting projections method reinvented again and 85.",
                    "label": 0
                },
                {
                    "sent": "It's the same as progressive hedging.",
                    "label": 0
                },
                {
                    "sent": "It's related to proximal methods and then sort of the modern vein substitutes for the quadratic Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "You start getting other things like this.",
                    "label": 0
                },
                {
                    "sent": "So I think that's the story on this, but by the way, you might ask why?",
                    "label": 0
                },
                {
                    "sent": "You know how is it that different well known algorithms could actually be the same and not notice?",
                    "label": 0
                },
                {
                    "sent": "Because actually it's complicated enough.",
                    "label": 0
                },
                {
                    "sent": "I mean it doesn't look that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated, but it's complicated enough.",
                    "label": 0
                },
                {
                    "sent": "Here you go something like that that if you redo the order and change the variables and instead of using you use U + X or something like that.",
                    "label": 0
                },
                {
                    "sent": "I mean then it can actually take quite a while to take algorithm A.",
                    "label": 0
                },
                {
                    "sent": "An algorithm B and actually certify that they are in fact the same right?",
                    "label": 0
                },
                {
                    "sent": "It's just it's enough complexity here that it's hard to do.",
                    "label": 0
                },
                {
                    "sent": "You're going to.",
                    "label": 0
                },
                {
                    "sent": "Minimizing completely operating precisely right?",
                    "label": 0
                },
                {
                    "sent": "That's the other thing, yeah, yeah, exactly so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, first of all, you have to do the baseline.",
                    "label": 0
                },
                {
                    "sent": "The first translation.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to talk about just just a common patterns that you see, and so we're working.",
                    "label": 1
                },
                {
                    "sent": "We're working towards actually doing something.",
                    "label": 0
                },
                {
                    "sent": "We have done anything yet, but we're working towards it.",
                    "label": 0
                },
                {
                    "sent": "OK, so, but it's just a couple of obvious, very simple observations.",
                    "label": 0
                },
                {
                    "sent": "I should mention, oh, I should mention one more thing before we move on 'cause I've had other discussions with people about it.",
                    "label": 0
                },
                {
                    "sent": "The proof of convergence requires high school algebra, absolutely nothing else, so there's nothing complicated in you know to subgradient is inequality.",
                    "label": 0
                },
                {
                    "sent": "Cauchy Schwarz?",
                    "label": 0
                },
                {
                    "sent": "I mean that's it right so?",
                    "label": 0
                },
                {
                    "sent": "It's very simple, right?",
                    "label": 0
                },
                {
                    "sent": "And if you see anything more complicated, you should be deeply suspicious.",
                    "label": 0
                },
                {
                    "sent": "So and you will see things much more complicated than that.",
                    "label": 0
                },
                {
                    "sent": "OK, the second thing I'll say is you might ask, well, what's the order?",
                    "label": 0
                },
                {
                    "sent": "Well, how fast does it converge?",
                    "label": 0
                },
                {
                    "sent": "And the answer is, it converges with the worst possible rate for these methods, one over epsilon squared steps to get an epsilon approximation, right?",
                    "label": 0
                },
                {
                    "sent": "So now of course this order optimal right?",
                    "label": 0
                },
                {
                    "sent": "Because we're making.",
                    "label": 0
                },
                {
                    "sent": "Merely that we're not assuming a Lipschitz constant on things, right?",
                    "label": 0
                },
                {
                    "sent": "These things, they're not even differentiable, right, so?",
                    "label": 0
                },
                {
                    "sent": "Seems to me.",
                    "label": 0
                },
                {
                    "sent": "I mean this is part of the trade off and this so this is not going to win any contests.",
                    "label": 0
                },
                {
                    "sent": "Any complexity, algorithm, complexity contests, that's for sure, right?",
                    "label": 0
                },
                {
                    "sent": "But that's kind of not the spirit of all this.",
                    "label": 0
                },
                {
                    "sent": "The spirit of all this is, you know that it should.",
                    "label": 0
                },
                {
                    "sent": "It should be like something like conjugate gradients, right?",
                    "label": 0
                },
                {
                    "sent": "Even run in numerics where you get horrible round off error affects.",
                    "label": 0
                },
                {
                    "sent": "It should be something like.",
                    "label": 0
                },
                {
                    "sent": "You know what you're hoping is that you get something that's relative.",
                    "label": 0
                },
                {
                    "sent": "That's actually useful in an application in some modest number of steps, a couple 100 or something like that.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's kind of what you're hoping.",
                    "label": 0
                },
                {
                    "sent": "Right, so so this is not about.",
                    "label": 0
                },
                {
                    "sent": "Complexity, worst case complexities OK. OK, so let's look at some common patterns here.",
                    "label": 1
                },
                {
                    "sent": "The idea is this.",
                    "label": 0
                },
                {
                    "sent": "Oh, and let's think about abstractly, what what this is.",
                    "label": 0
                },
                {
                    "sent": "And actually I like to think of it not as an optimization algorithm, but is a sort of a meta algorithm.",
                    "label": 0
                },
                {
                    "sent": "Cousin an optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "You end up working with primitives like evaluating a gradient or a subgradient, or something like that.",
                    "label": 0
                },
                {
                    "sent": "You know you do some linear algebra and stuff like that, and all of a sudden you have an algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So these are these are very low level operators here.",
                    "label": 0
                },
                {
                    "sent": "The operators actually are higher level, they require this, they require you to minimize.",
                    "label": 0
                },
                {
                    "sent": "A function plus a quadratic, a quadratically augmented function, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a higher level concept now.",
                    "label": 0
                },
                {
                    "sent": "I mean, at least in some cases, this will reduce to doing a gradient calculation or something like that, or something like a gradient calculation.",
                    "label": 0
                },
                {
                    "sent": "But in general this could be something much more complicated, right?",
                    "label": 0
                },
                {
                    "sent": "Well, this could.",
                    "label": 0
                },
                {
                    "sent": "This could actually require some heavy lifting actually solving real convex optimization problem, OK?",
                    "label": 0
                },
                {
                    "sent": "So we're going to look now at at so actually, by the way, what this says is you only have to implement one method for F, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just you have to be able to carry out this.",
                    "label": 0
                },
                {
                    "sent": "This quadratically augmented minimization.",
                    "label": 1
                },
                {
                    "sent": "But let's look at some special cases and things where the splits 'cause you put them together.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For these and all of a sudden this looks interesting.",
                    "label": 0
                },
                {
                    "sent": "Well, the first one this is like kind of obvious if F is block separable and a transposase block separable.",
                    "label": 1
                },
                {
                    "sent": "By the way, we'll see that in a lot of applications A is like I or minus.",
                    "label": 0
                },
                {
                    "sent": "I will see when we see applications.",
                    "label": 0
                },
                {
                    "sent": "Then, well, of course, minimizing if you have to minimize.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, you know, take a equals I and you have to minimize this and F splits.",
                    "label": 0
                },
                {
                    "sent": "Then of course the whole thing splits, so you can immediately target.",
                    "label": 0
                },
                {
                    "sent": "You can run that section in parallel and that's how we're going to distributed optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean, this is kind of just like.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Completely obvious, OK?",
                    "label": 0
                },
                {
                    "sent": "Another case occurs when a equals I.",
                    "label": 1
                },
                {
                    "sent": "This occurs so frequently it has a name and a long history, so this is called the proximal operator associated with F and it.",
                    "label": 0
                },
                {
                    "sent": "Basically it minimum.",
                    "label": 0
                },
                {
                    "sent": "You know it minimizes F + a quadratic cost of deviating from V. An you can, by the way, you can almost sort of see certain things here.",
                    "label": 0
                },
                {
                    "sent": "If you make Ro super high, very large, then you can kind of NF is quadratic.",
                    "label": 0
                },
                {
                    "sent": "You'll see that this returns something which is a gradient.",
                    "label": 0
                },
                {
                    "sent": "It's a gradient step.",
                    "label": 0
                },
                {
                    "sent": "It's exactly a gradient step.",
                    "label": 0
                },
                {
                    "sent": "If row is small, I mean you get other other things and I'll explain some of these, but these you can these you can workout depending on what F is.",
                    "label": 0
                },
                {
                    "sent": "I mean when an example, let's look at some special cases.",
                    "label": 1
                },
                {
                    "sent": "If F is an indicator function, so F is zero off, some on some convex set an plus Infinity outside and this really simple.",
                    "label": 0
                },
                {
                    "sent": "This just says minimize this norm.",
                    "label": 0
                },
                {
                    "sent": "the Rose totally irrelevant.",
                    "label": 0
                },
                {
                    "sent": "This is projection.",
                    "label": 0
                },
                {
                    "sent": "So in fact the idea is that approximal operator is a generalization of a projection operator and reduces the projection operator when F is an indicator function.",
                    "label": 0
                },
                {
                    "sent": "If F is any separable function, the whole thing becomes completely trivial because you minimize it's actually absolutely.",
                    "label": 0
                },
                {
                    "sent": "You know, certainly don't need to sum of absolute value.",
                    "label": 0
                },
                {
                    "sent": "Any separable function is completely trivial because you're doing scalar proxamol calculations.",
                    "label": 0
                },
                {
                    "sent": "How long does it take to minimize the scalar function convex function?",
                    "label": 0
                },
                {
                    "sent": "The answer is 0.",
                    "label": 0
                },
                {
                    "sent": "I mean 'cause you put it's all done in little registers and things like that, so it's all zero.",
                    "label": 0
                },
                {
                    "sent": "Basically it costs you more to move the data in and out OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "But then you know you have pretty formulas for it, not that that matters, But for example the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "The proximal operator is this soft threshold thing right?",
                    "label": 0
                },
                {
                    "sent": "Which looks like, let's see for you it goes up like this is a little flat chunk and then it goes back up again OK?",
                    "label": 0
                },
                {
                    "sent": "And there's lots and lots of others.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, quadratic objective.",
                    "label": 0
                },
                {
                    "sent": "Well, if you minimize a quadratic objective, well then of course you augmented quadratic objective with quadratic is still quadratic.",
                    "label": 1
                },
                {
                    "sent": "Minimize a quadratic you that's you can solve this by just by linear algebra, I mean you just solve a set of linear equations right?",
                    "label": 0
                },
                {
                    "sent": "So you get something like this and it is a lot you can say about how to do this.",
                    "label": 0
                },
                {
                    "sent": "Well, for one thing, depending on sparsity patterns and things like that, you may want to arrange for this this thing.",
                    "label": 0
                },
                {
                    "sent": "You might want to use something like a matrix inversion lemma or something like that, and the rough idea is if in the dense case you want to be absolutely certain that the cost of computing this is the big dimension times a small squared right.",
                    "label": 1
                },
                {
                    "sent": "I mean the big dimension squared times a small one being wrong, so and the big big dimension cube being very wrong.",
                    "label": 0
                },
                {
                    "sent": "Implementation.",
                    "label": 0
                },
                {
                    "sent": "If you're using a direct method, you also see something interesting that's this.",
                    "label": 0
                },
                {
                    "sent": "When you, when you if you had to do this repeatedly, what changes in each iteration is V and what it says is I'll cash a factorization.",
                    "label": 0
                },
                {
                    "sent": "If you cash the factorization of this in the dense case, you get a effusa direct method right?",
                    "label": 1
                },
                {
                    "sent": "Then the first time you solve it, you pay for a factorization Anna back solve.",
                    "label": 0
                },
                {
                    "sent": "Thereafter you only pay back solve and you get a discount on the on the method.",
                    "label": 0
                },
                {
                    "sent": "On on on computing the least squares problem, which is on the order of this, which is the small dimension, right?",
                    "label": 0
                },
                {
                    "sent": "So we're course we intend to do this for large problems, so the small dimension is a pretty good discount.",
                    "label": 0
                },
                {
                    "sent": "So by the way, this also means that this business of counting iterations is not going to be too irrelevant, because what it says is that after the first iteration of a DMM, when you're doing, if you're doing direct method an factorization cashing it says after you do the first iteration, thereafter iterations cost one 500 or 1000 of the first one.",
                    "label": 1
                },
                {
                    "sent": "And you see, in a regime like that, you know, really, who cares whether you take 200 or or for that matter, 10,000 steps?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just not relevant, OK?",
                    "label": 1
                },
                {
                    "sent": "OK, now you're using an iterative method here, like LSQR or some CG type thing.",
                    "label": 0
                },
                {
                    "sent": "Then there's other tricks you don't cache anymore, but what you do is you do warm start and you start.",
                    "label": 0
                },
                {
                    "sent": "The minimum is 8 LSQR minimization of the quadratic from for example the previous point.",
                    "label": 0
                },
                {
                    "sent": "And as you converge it means you start taking you start doing better and better OK.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of tricks I'm not going to go into them, but these are these are these are serious there obvious ones and they confer.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A huge advantage.",
                    "label": 0
                },
                {
                    "sent": "OK, so I should say something else.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That will see in a statistical in a data fitting context, this a quadratic update is going to correspond to just Ridge regression, right?",
                    "label": 0
                },
                {
                    "sent": "So this says that in those cases you do a Ridge regression and after you do the first Ridge regression there after you get a huge discount for the substantial discount for further.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Iterations right?",
                    "label": 0
                },
                {
                    "sent": "OK, now if is smooth by using whatever you like.",
                    "label": 0
                },
                {
                    "sent": "Could use something like Newton Quasi Newton if you use Newton if you can if you can.",
                    "label": 0
                },
                {
                    "sent": "If you can use a direct method, that's a excellent choice.",
                    "label": 1
                },
                {
                    "sent": "Again, you would.",
                    "label": 0
                },
                {
                    "sent": "Well you don't catch the factorization, but you what you do.",
                    "label": 0
                },
                {
                    "sent": "Cash is the permutation for the sparse matrix factorization that's cashed across everything.",
                    "label": 0
                },
                {
                    "sent": "A very good choice would be something like limited memory FDS that will scale to extremely large problems and I should add something here.",
                    "label": 1
                },
                {
                    "sent": "Even things that wouldn't work well with LB FGS will now work very well because you've augment you've augmented it with a quadratic, and for methods that depend on smoothness and things like this, adding that Rover two quadratic term is just like it's perfect right?",
                    "label": 0
                },
                {
                    "sent": "You just it's just what they want to make them work extremely well.",
                    "label": 0
                },
                {
                    "sent": "I'm including things like LBF's.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to look at that, we're just going to assemble all the parts, an kind of have everything you would just put 'em together and you'll see what's weird is, although nothing I've said so far has been.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's all very basic, so now we just start assembling, so will start just with convex general convex optimization.",
                    "label": 1
                },
                {
                    "sent": "I mean just generic problem, minimize F / a convex set will reformulate rate formula for Adm this way and I mean if someone comes up to you and says I'm going to write this problem this way.",
                    "label": 1
                },
                {
                    "sent": "I mean this is just like completely stupid.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look like in fact if you think about if you know about how linear programming solvers work, the first thing they do is they do a pre solve and a pre solve scans the problem and looks for looks for constraints this stupid.",
                    "label": 0
                },
                {
                    "sent": "Write this constraint says like X1 equals one and so that's exactly what a pre solving an LP solver does.",
                    "label": 0
                },
                {
                    "sent": "It looks it looks for constraints that are idiotically stupid.",
                    "label": 0
                },
                {
                    "sent": "It makes a note that Z1 is X1.",
                    "label": 0
                },
                {
                    "sent": "It replaces it here and it has a little symbol table.",
                    "label": 0
                },
                {
                    "sent": "Later it says oh, by the way, at the end.",
                    "label": 0
                },
                {
                    "sent": "Please write for Z11X1 is everything I'm saying here.",
                    "label": 0
                },
                {
                    "sent": "So I mean this doesn't look promising right?",
                    "label": 0
                },
                {
                    "sent": "And then I've written it this way.",
                    "label": 0
                },
                {
                    "sent": "G is the following.",
                    "label": 0
                },
                {
                    "sent": "It's zero if you're in C and plus Infinity outside.",
                    "label": 0
                },
                {
                    "sent": "I mean so this does not look.",
                    "label": 0
                },
                {
                    "sent": "I don't know, it just looks too simple too.",
                    "label": 0
                },
                {
                    "sent": "That any good can come of it, right?",
                    "label": 0
                },
                {
                    "sent": "OK, but the algorithm is very simple, so here you do.",
                    "label": 0
                },
                {
                    "sent": "This is approx.",
                    "label": 0
                },
                {
                    "sent": "Step on F. The PROC step with G is simple.",
                    "label": 0
                },
                {
                    "sent": "G is a indicator function, so the process is projection on to see and so you can see you now have a method that will minimize F / C, But it's different for something like projected gradient, although for Ro extremely high it is projected gradient, it's exactly the same and F smooth.",
                    "label": 0
                },
                {
                    "sent": "This does not require F smooth.",
                    "label": 0
                },
                {
                    "sent": "This works for.",
                    "label": 0
                },
                {
                    "sent": "F can itself can contain constraints.",
                    "label": 0
                },
                {
                    "sent": "It could be something horrible, but it doesn't make any difference.",
                    "label": 0
                },
                {
                    "sent": "Works just works.",
                    "label": 0
                },
                {
                    "sent": "And so you can see what we have is a method for minimizing F / X and C, and the only method you have to implement is approx method for F, that's this.",
                    "label": 1
                },
                {
                    "sent": "And you have to be able to project onto C, right?",
                    "label": 0
                },
                {
                    "sent": "And you can also see here.",
                    "label": 0
                },
                {
                    "sent": "I think it's not that hard to see that if Rd gets really large, this is projected gradient method, right?",
                    "label": 0
                },
                {
                    "sent": "Because this is a gradient step.",
                    "label": 0
                },
                {
                    "sent": "And this is then you make a projection and so on, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let's look at another one.",
                    "label": 0
                },
                {
                    "sent": "Let's look at Lasso.",
                    "label": 0
                },
                {
                    "sent": "I mean, why not ten 10,000 algorithms for solving this?",
                    "label": 0
                },
                {
                    "sent": "And this probably another one developed last week or something like that with a new acronym two so.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try it.",
                    "label": 0
                },
                {
                    "sent": "So here you have a least squares term, which is a standard regression term and then you have an L1 sparsifying regularizer.",
                    "label": 0
                },
                {
                    "sent": "And you know, the spirit is very simple.",
                    "label": 0
                },
                {
                    "sent": "I mean the spirit is quadratic.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have a very good way to solve those.",
                    "label": 0
                },
                {
                    "sent": "I guess you know Gauss wrote a book in Latin from around 1800 about least squares.",
                    "label": 0
                },
                {
                    "sent": "So yes, we know how to do that, but we have a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, actually we have a lot behind stuff like this, right?",
                    "label": 0
                },
                {
                    "sent": "There's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Lot of lot of integrated knowledge over the years about how to solve problems like that with this one in there it's harder, but in fact we know how to handle approx of that.",
                    "label": 0
                },
                {
                    "sent": "So it's all going to work.",
                    "label": 0
                },
                {
                    "sent": "So you write it this way and again this is you can see the splitting is like embarrassing that what you're really doing is you're taking a single variable and you're replicating it, calling it X&Z and adding a constraint that they're in consensus that they agree with each other.",
                    "label": 0
                },
                {
                    "sent": "It sounds dumb, but that's it, and you end up you just write this down and you get this thing you do.",
                    "label": 0
                },
                {
                    "sent": "You do.",
                    "label": 0
                },
                {
                    "sent": "You can if you want, you can do factorization caching here.",
                    "label": 0
                },
                {
                    "sent": "There's a soft thresholding and then there's a dual update.",
                    "label": 0
                },
                {
                    "sent": "OK, so did you just turn the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rank just to see that it kind of works.",
                    "label": 0
                },
                {
                    "sent": "Here's an example, just with a dense A.",
                    "label": 1
                },
                {
                    "sent": "It's like 1500 by 5000.",
                    "label": 0
                },
                {
                    "sent": "And so if you do this, this is all, by the way something like an 8 line implementation in Matlab or something.",
                    "label": 0
                },
                {
                    "sent": "So you shouldn't believe.",
                    "label": 0
                },
                {
                    "sent": "I mean you shouldn't trust any of the numbers I mean, but real implementation.",
                    "label": 0
                },
                {
                    "sent": "It would be similar just with smaller numbers, right?",
                    "label": 0
                },
                {
                    "sent": "So the factorization oh by the way, which is a Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So that's 1.3 seconds and then after that you get a discount.",
                    "label": 0
                },
                {
                    "sent": "It should have been more but 30 milliseconds and you can see by the way, when your first iteration is a second and subsequently this should have been something like 10 milliseconds, right?",
                    "label": 0
                },
                {
                    "sent": "You can see that now getting all bent out of shape over how many steps it takes.",
                    "label": 0
                },
                {
                    "sent": "Is completely idiotic.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're doing factorization caching right, so let's solve takes.",
                    "label": 0
                },
                {
                    "sent": "I don't know a couple of seconds, by the way.",
                    "label": 0
                },
                {
                    "sent": "You can then do the full regularization path because you can share if you change Lambda, it doesn't change in any way.",
                    "label": 1
                },
                {
                    "sent": "The fact you just keep the same factorization right?",
                    "label": 0
                },
                {
                    "sent": "So just a quick example that's not bad.",
                    "label": 0
                },
                {
                    "sent": "This is like a six line Matlab script.",
                    "label": 0
                },
                {
                    "sent": "You're actually close to competitive here.",
                    "label": 0
                },
                {
                    "sent": "Sorry you're in the you're in the range, right?",
                    "label": 0
                },
                {
                    "sent": "So you write something real quick and it's it's right there, untuned, untouched, I mean.",
                    "label": 0
                },
                {
                    "sent": "Nothing was tuned or anything like that, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just to show that knowing about these methods is good, it just means you can actually put bits and pieces together and you can get something very quickly that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is in the ballpark.",
                    "label": 0
                },
                {
                    "sent": "I'll do one more.",
                    "label": 0
                },
                {
                    "sent": "I'll go over this quickly and then we'll get to distributed stuff.",
                    "label": 0
                },
                {
                    "sent": "Was there a question somewhere I know OK, so for sparse inverse covariance selection, here you have samples from zero mean Gaussian with Sigma, inverse sparse and you want to estimate you want to estimate Sigma, right?",
                    "label": 1
                },
                {
                    "sent": "So you write down will estimate Sigma inverse which is called X here and this would be the negative.",
                    "label": 0
                },
                {
                    "sent": "These two these two terms are the negative log likelihood you know up to some constant or something.",
                    "label": 0
                },
                {
                    "sent": "Actually, native content multiplicative constant.",
                    "label": 0
                },
                {
                    "sent": "That's the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And of course if you just Max maximize this, you get X equals Sigma.",
                    "label": 0
                },
                {
                    "sent": "Sorry X equals Sigma inverse right so, but we add a sparsifying regularizer on X, which is our estimate.",
                    "label": 0
                },
                {
                    "sent": "Sigma inverse right?",
                    "label": 0
                },
                {
                    "sent": "And then we turn the crank on Lambda to trade off sparsity of the inverse versus versus fit here.",
                    "label": 0
                },
                {
                    "sent": "OK. And I mean this is something that's been, so I mean, these are just some of the things people have looked at it actually others have too, including people here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I was just talking to someone beforehand.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you do?",
                    "label": 0
                },
                {
                    "sent": "Again, it's silly.",
                    "label": 0
                },
                {
                    "sent": "You just do the X = y thing.",
                    "label": 0
                },
                {
                    "sent": "By the way, will see some splittings that don't involve this embarrassing splitting here.",
                    "label": 0
                },
                {
                    "sent": "And you get something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Here's your this thing.",
                    "label": 0
                },
                {
                    "sent": "We know how to minimize that, but we have to quadratically augmented this.",
                    "label": 0
                },
                {
                    "sent": "We know how to deal with, because that's just going to be a soft threshold, right?",
                    "label": 0
                },
                {
                    "sent": "So on the X step, you minimize this thing, and otherwise you do a sparse.",
                    "label": 0
                },
                {
                    "sent": "I mean, sorry, a soft threshold and you do the standard update, and so you look at this for awhile and you could solve this by.",
                    "label": 0
                },
                {
                    "sent": "You know LB, FGS or something like that, but you look at it long enough and it should start dawning on you.",
                    "label": 0
                },
                {
                    "sent": "We can solve that.",
                    "label": 0
                },
                {
                    "sent": "You know, semi analytically, actually analytically, right?",
                    "label": 0
                },
                {
                    "sent": "Certainly you can solve that analytically, right?",
                    "label": 0
                },
                {
                    "sent": "And the key would be to say look without loss of generality.",
                    "label": 0
                },
                {
                    "sent": "We can do this for XY.",
                    "label": 0
                },
                {
                    "sent": "This is unitarily invariant, so you can assume that X is is a diagonal.",
                    "label": 0
                },
                {
                    "sent": "Then it splits and it turns out this is Unitarily invariant to its Frobenius norm squared.",
                    "label": 0
                },
                {
                    "sent": "So you can solve this analytically.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It's just this.",
                    "label": 0
                },
                {
                    "sent": "You compute an eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "And then you solve a quadratic involving the eigenvalues, and then reassemble OK, so it says that the cost is an eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "So again you know you write a date line.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Script and you know it's not totally unoptimized.",
                    "label": 0
                },
                {
                    "sent": "An unoptimized you get something that's you know within a factor of 10 or something like that of the state of the art type thing.",
                    "label": 0
                },
                {
                    "sent": "For something like this right?",
                    "label": 0
                },
                {
                    "sent": "Which again for maximum coding time.",
                    "label": 0
                },
                {
                    "sent": "I mean like I don't know 5 minutes or something like that, maybe 45 minutes of pencil and paper to do the work out what the update is.",
                    "label": 0
                },
                {
                    "sent": "This is not bad, right?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I will do now anyway so that that finishes up that section and the point there is just by assembling these bits and pieces, you can actually do stuff with this, I mean quite quite quickly you can get something up and running.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we're going to look at distributed optimization, and to do that we're just going to look at two Canonical problems, one, but in much more detail than the other, which is consensus, and so it's sort of like the Canonical, like it's the first, it's the Hello world of distributed optimization, and it's got a beautiful interpretation.",
                    "label": 0
                },
                {
                    "sent": "It's it's very simple, but honestly, once you understand this, you understand you understand other distributed optimization problems, so OK. Or at least this is a stepping stone to it.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize a sum of functions of a variable, and these functions if you want to think of something.",
                    "label": 0
                },
                {
                    "sent": "Think of FI as the loss function for the ice block of training data, so that's what it is, right?",
                    "label": 1
                },
                {
                    "sent": "It's something like that.",
                    "label": 0
                },
                {
                    "sent": "And so you could make some big deal and you could say, oh, this is like collaborative filtering because you see I have all these different data sources and they're going to combine and minimize the total loss and come up with an X that's better than any of them by themselves might have come up with or something and make a big story about.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so we simply write it this way.",
                    "label": 0
                },
                {
                    "sent": "Again, it's quite stupid.",
                    "label": 0
                },
                {
                    "sent": "What you do is you allow each one to have its own private opinion.",
                    "label": 0
                },
                {
                    "sent": "So instead of a global variable X, they all have their own local opinion XI, and you add a constraint that says you can have your own local opinion, but.",
                    "label": 0
                },
                {
                    "sent": "But it has to be in consent.",
                    "label": 1
                },
                {
                    "sent": "They all have to be agree with this global variable Z.",
                    "label": 0
                },
                {
                    "sent": "Again, it doesn't these things look so obvious and silly that it doesn't look like any any good is going to come from this right?",
                    "label": 0
                },
                {
                    "sent": "OK, that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so you just turn the crank you you form an augmented Lagrangian, you minimize it.",
                    "label": 0
                },
                {
                    "sent": "Turns out this first one obviously splits entirely, so the only thing you have to do is approx is approx operator on each FI.",
                    "label": 0
                },
                {
                    "sent": "Then you do.",
                    "label": 0
                },
                {
                    "sent": "Oh, how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you minimize this thing that you over Z?",
                    "label": 0
                },
                {
                    "sent": "Well, that's trivial.",
                    "label": 0
                },
                {
                    "sent": "This is totally irrelevant.",
                    "label": 0
                },
                {
                    "sent": "That's a bunch of linear functions.",
                    "label": 0
                },
                {
                    "sent": "That's a bunch of quadratic fact, not only just quadratic functions, it's the norm squared of X -- Y.",
                    "label": 0
                },
                {
                    "sent": "How do you minimize that?",
                    "label": 0
                },
                {
                    "sent": "It's like an average.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just analytical.",
                    "label": 0
                },
                {
                    "sent": "It's just this thing.",
                    "label": 0
                },
                {
                    "sent": "This can even be simplified more 'cause it turns out that some of the dual variables is 0.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it turns out you end up with a very, very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's just beautiful, and it looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's this, it says each each.",
                    "label": 0
                },
                {
                    "sent": "Each worker, whatever subunit or agent separately minimizes their function plus the linear term.",
                    "label": 0
                },
                {
                    "sent": "That's irrelevant, of course, is just for aesthetics, right?",
                    "label": 0
                },
                {
                    "sent": "Plus a linear term plus this quadratic and the quadratic penalizes you from moving from the last average.",
                    "label": 0
                },
                {
                    "sent": "And I mean, if you sent a message to this thing and say OK times up, please give me what tell me what X is.",
                    "label": 0
                },
                {
                    "sent": "You would average all of the opinion, the local opinions and that would be it.",
                    "label": 0
                },
                {
                    "sent": "And why does averaging work well because?",
                    "label": 0
                },
                {
                    "sent": "Everything is convex and so Jensen's inequality says you average and things get better.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So you get this beautiful quadratic penalty for averaging, and you can sort of understand all the bits and pieces of this right?",
                    "label": 0
                },
                {
                    "sent": "So in fact this would be dual decomposition right there, just with the penalty.",
                    "label": 0
                },
                {
                    "sent": "And then this says you add the quadratic regularization you meet towards the mean and it makes the algorithm go from quite non robust, fragile to utterly and completely robust.",
                    "label": 0
                },
                {
                    "sent": "And you can see it's like a scatter gather.",
                    "label": 0
                },
                {
                    "sent": "Type thing you know what?",
                    "label": 0
                },
                {
                    "sent": "It's not even it's not a gather.",
                    "label": 0
                },
                {
                    "sent": "Even that is is not correct.",
                    "label": 0
                },
                {
                    "sent": "In fact, the wrong way of course, to average a million vectors of size a million.",
                    "label": 0
                },
                {
                    "sent": "The exactly wrong method would be to gather them all in one place and then average them.",
                    "label": 0
                },
                {
                    "sent": "So, but I mean, but you know what I mean by this so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now the statistical interpretation is absolutely beautiful.",
                    "label": 0
                },
                {
                    "sent": "It says this if you want it.",
                    "label": 0
                },
                {
                    "sent": "Basically what it says is, let's do distributed maximum likelihood distribution.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood says I have a bunch of data blocks each each one, and then I have a.",
                    "label": 0
                },
                {
                    "sent": "Those are in case that I have and the only thing I have to do.",
                    "label": 0
                },
                {
                    "sent": "There's only one method I have to implement for each box, each box of data at the only thing you have to do is approx.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operator right, you have to be able to minimize.",
                    "label": 0
                },
                {
                    "sent": "I have to be able to pass in this thing in this thing or for that matter I combine these into one thing I pass in one thing and then I had.",
                    "label": 0
                },
                {
                    "sent": "I say minimize your loss function plus a quadratic, but minimizing a loss.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Implicit quadratic has a very simple interpretation, is just map estimate X maximum posteriori probability estimation with a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that you have to implement on each of these data boxes or anything like that.",
                    "label": 0
                },
                {
                    "sent": "Is A is a map method, so that's it does it doesn't matter what the measurements are, how many there are utterly irrelevant.",
                    "label": 0
                },
                {
                    "sent": "The only needs a map method you send to the box aprire with the Gaussian an it sends back what it what the map estimate would be under that prior.",
                    "label": 0
                },
                {
                    "sent": "And that's the only thing you have to do, right?",
                    "label": 0
                },
                {
                    "sent": "So anyway, it's quite.",
                    "label": 0
                },
                {
                    "sent": "I think it's kind of cool.",
                    "label": 0
                },
                {
                    "sent": "Let's not not obvious.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we'll do some examples.",
                    "label": 0
                },
                {
                    "sent": "I'll start with a little baby one.",
                    "label": 0
                },
                {
                    "sent": "Just so you can see visually what it is, and I I'm sure here I don't have to go into all of this, so I'll make it very, very fast.",
                    "label": 0
                },
                {
                    "sent": "You know you will do just do classification so you know I have a bunch of Boolean outcomes and feature vectors and I form a margin.",
                    "label": 0
                },
                {
                    "sent": "My variables are WMV.",
                    "label": 0
                },
                {
                    "sent": "I'll make a loss.",
                    "label": 0
                },
                {
                    "sent": "You can make anything you like, you know any any of the obvious options, and then you want to minimize an average an average loss plus a regularization term that could be L2 squared, L1, whatever you like, exponential, anything like that anyway.",
                    "label": 1
                },
                {
                    "sent": "Then the ideas you split data and will use.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adm consensus to solve will look at a really super small example, just one that you don't.",
                    "label": 0
                },
                {
                    "sent": "In fact, you don't even need.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't be doing SPM on, But anyway, well, well just look at their two features.",
                    "label": 0
                },
                {
                    "sent": "An 400 examples, but just so I can plot it right and will split the 400 examples into twenty groups, but will do it in a sick way.",
                    "label": 0
                },
                {
                    "sent": "Each group of 20 has only seen either all positive or all negative examples.",
                    "label": 1
                },
                {
                    "sent": "Right, so it's the most skewed, so each box of 20 examples has seen all spam or all, not right?",
                    "label": 0
                },
                {
                    "sent": "So that's how it works.",
                    "label": 0
                },
                {
                    "sent": "So they're going to have to collaborate too.",
                    "label": 0
                },
                {
                    "sent": "I mean, probably the right way to do this, actually, is to hash the data, in which case each of them by the one by themselves actually gets a pretty good estimate.",
                    "label": 0
                },
                {
                    "sent": "And the only thing they do by collaborating is get statistical power right?",
                    "label": 0
                },
                {
                    "sent": "So this is not that.",
                    "label": 0
                },
                {
                    "sent": "This is just for the show how this works.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how this works.",
                    "label": 0
                },
                {
                    "sent": "Well, these are all the private local estimates, and of course they're all terrible because they've all been given false.",
                    "label": 0
                },
                {
                    "sent": "They've all been given terrible ideas of what the data looks like, 'cause they've only seen all positive role negative things.",
                    "label": 0
                },
                {
                    "sent": "By the way, the reason they're not all way off in the middle of nowhere is because the quadratic regularization term.",
                    "label": 0
                },
                {
                    "sent": "OK, actually, embarrassingly after one step, which consists of averaging all of those, you don't get something is not so bad, right?",
                    "label": 0
                },
                {
                    "sent": "So if you then?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run a DMM like 5 steps.",
                    "label": 0
                },
                {
                    "sent": "You get to something like this and you.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And kind of you get the picture right, so that's what happens after 40 steps, right?",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing here is just this.",
                    "label": 0
                },
                {
                    "sent": "To visualize.",
                    "label": 0
                },
                {
                    "sent": "Actually the data flow in the communication.",
                    "label": 0
                },
                {
                    "sent": "Here, each box has seen like you're a box you've seen only positive examples.",
                    "label": 0
                },
                {
                    "sent": "You have absolutely no idea who else has an opinion about about this problem.",
                    "label": 0
                },
                {
                    "sent": "Who else you're collaborating with to form a class, but you know nothing.",
                    "label": 0
                },
                {
                    "sent": "The only thing you have is a protocol that implements map, and that's it.",
                    "label": 0
                },
                {
                    "sent": "And what happens is they come back and they say.",
                    "label": 0
                },
                {
                    "sent": "Well, the weight vector prior looks like this.",
                    "label": 0
                },
                {
                    "sent": "What would you do then?",
                    "label": 0
                },
                {
                    "sent": "And you'd say, well, it's very strange.",
                    "label": 0
                },
                {
                    "sent": "It's highly inconsistent with my data frankly, but OK, whatever and it returns W and then actually when these things come to convergence, it means they've solved the global problem.",
                    "label": 0
                },
                {
                    "sent": "Each box has absolutely no idea how many others there are what they look like.",
                    "label": 0
                },
                {
                    "sent": "Anything I mean.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to make too big a deal about it, but it's worth thinking about what actually happens here.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's look at another one.",
                    "label": 0
                },
                {
                    "sent": "This is distributed lasso example and this is just made.",
                    "label": 1
                },
                {
                    "sent": "This is just for fun, just to show that you can do this.",
                    "label": 0
                },
                {
                    "sent": "So this is probably bigger than anyone would ever solve, so it's dense problem with four 100,000 four 100,000 examples in 8000 regressors or something like that.",
                    "label": 1
                },
                {
                    "sent": "An that's about 30 gigabytes of data.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's people like we're routinely solve problems with this amount of data for sparse problems.",
                    "label": 1
                },
                {
                    "sent": "This one is not sparse, so it's dense.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I probably somebody has solved problems like this, But anyway, alright, so just to do something real simple you take this, you split it, you just split the data into 80 subsystems.",
                    "label": 0
                },
                {
                    "sent": "That's ten 8 core machines, right?",
                    "label": 0
                },
                {
                    "sent": "Something really simple.",
                    "label": 0
                },
                {
                    "sent": "Not totally unoptimized.",
                    "label": 0
                },
                {
                    "sent": "Extremely short, right?",
                    "label": 0
                },
                {
                    "sent": "Just the shortest thing you could possibly have and then here, just to give you a rough idea of what this does.",
                    "label": 0
                },
                {
                    "sent": "It does something like this.",
                    "label": 0
                },
                {
                    "sent": "Factorization, by the way, is a.",
                    "label": 0
                },
                {
                    "sent": "That's a parallel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "I mean, statistically, that's what you're doing.",
                    "label": 0
                },
                {
                    "sent": "You're doing a parallel Ridge regression and that takes about 5 minutes.",
                    "label": 1
                },
                {
                    "sent": "Subsequent iterations take about a second, so again, it's totally irrelevant how many steps you take.",
                    "label": 0
                },
                {
                    "sent": "I mean not totally irrelevant, but it's certainly it's not as interesting as arguing about whether something takes 20 steps or 100 total.",
                    "label": 0
                },
                {
                    "sent": "Now, that's kind of irrelevant, and the total lasso solve is takes like 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "By the way, this makes a very nice story, right?",
                    "label": 0
                },
                {
                    "sent": "So there was a story.",
                    "label": 0
                },
                {
                    "sent": "You could say about interior Point methods that would go like this.",
                    "label": 0
                },
                {
                    "sent": "The story would say someone would say what are you doing?",
                    "label": 0
                },
                {
                    "sent": "You say when solving a convex optimization problem and you would say oh great indeed, profile the code and find out that in fact what they're really doing is 20 for 20 times they were solving a least squares system.",
                    "label": 0
                },
                {
                    "sent": "Everybody you know, and that's what it is.",
                    "label": 0
                },
                {
                    "sent": "That's when the interior point method does.",
                    "label": 0
                },
                {
                    "sent": "It solves the KKT system like 20 * 30 times whatever something like that.",
                    "label": 0
                },
                {
                    "sent": "So you're solving least squares, right?",
                    "label": 0
                },
                {
                    "sent": "And so someone could say, well, why would you do this convex optimization?",
                    "label": 0
                },
                {
                    "sent": "And you say, well, it's much more expressive and the cost over least squares is some modest number, like 30 or something.",
                    "label": 0
                },
                {
                    "sent": "Most 30 right?",
                    "label": 0
                },
                {
                    "sent": "So I mean sometimes a factor of 30 is actually is a deal breaker, right?",
                    "label": 0
                },
                {
                    "sent": "But you know anyway, not this relevant to any of you because interior point methods are not relevant.",
                    "label": 0
                },
                {
                    "sent": "Probably to any of you.",
                    "label": 0
                },
                {
                    "sent": "But anyway, but there are people for whom it's relevant, right?",
                    "label": 0
                },
                {
                    "sent": "Those who chose wisely to go into fields without big data, but it's too late for all of you now.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of the younger people are not in any way whatever.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea there is that, but actually you get the same story here, but it's even more shocking, right?",
                    "label": 0
                },
                {
                    "sent": "'cause here, the story is something like this.",
                    "label": 0
                },
                {
                    "sent": "How long does it take you to just do like least squares regression regression on a data set like that?",
                    "label": 0
                },
                {
                    "sent": "And the answer is if you know what you're doing.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a crappy implementation, but it's 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "Got the answer and you go.",
                    "label": 0
                },
                {
                    "sent": "Oh, how long did it take you to do Lasso which is solving a nontrivial convex problem and you go 5 1/2 minutes so you get the same story that the cost, but they did.",
                    "label": 0
                },
                {
                    "sent": "Marginal cost is not much higher for solving convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "As for solving at least squares problem right?",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We look at this one more problem and then I'll quit.",
                    "label": 0
                },
                {
                    "sent": "And it is in fact the dual of the consensus problem.",
                    "label": 1
                },
                {
                    "sent": "So that's this problem.",
                    "label": 0
                },
                {
                    "sent": "It's you minimize some FI of XI.",
                    "label": 0
                },
                {
                    "sent": "Now here the XI are not just replicates of each other like they were in consensus right here.",
                    "label": 0
                },
                {
                    "sent": "They're actually truly different, but they have to sum to zero.",
                    "label": 0
                },
                {
                    "sent": "And by the way, if you want to know how you know it's a dual, well, consensus is the same objective here.",
                    "label": 0
                },
                {
                    "sent": "And it says ex.",
                    "label": 0
                },
                {
                    "sent": "The stacked X is in the range of IIII.",
                    "label": 0
                },
                {
                    "sent": "Times E or sorry, just the range of I I write notes in consensus.",
                    "label": 0
                },
                {
                    "sent": "Here it says X is in the null space of the the transpose, which is I I I don't know if any of that made any sense, but.",
                    "label": 0
                },
                {
                    "sent": "I think a few people few people got it, maybe a few others were nodding politely.",
                    "label": 0
                },
                {
                    "sent": "That's fine, OK, alright?",
                    "label": 0
                },
                {
                    "sent": "And it's got this.",
                    "label": 1
                },
                {
                    "sent": "This like consensus has beautiful interpretation, right?",
                    "label": 1
                },
                {
                    "sent": "So here you think of XYZ as characterizing a set of amounts of goods?",
                    "label": 0
                },
                {
                    "sent": "And then this is an exchange.",
                    "label": 0
                },
                {
                    "sent": "This says that they clear.",
                    "label": 0
                },
                {
                    "sent": "It says that people only exchange things.",
                    "label": 0
                },
                {
                    "sent": "The total amount of commodity three that everyone either contributes or takes from the exchange.",
                    "label": 0
                },
                {
                    "sent": "It all balances.",
                    "label": 0
                },
                {
                    "sent": "That's this, right?",
                    "label": 0
                },
                {
                    "sent": "And then you know you get a price and things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so here.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't you can work out what it is exactly, but you get something very similar an it's really quite simple and beautiful, so it looks like this again that there's a common price you implement approx method and here you have again a term.",
                    "label": 0
                },
                {
                    "sent": "The term is interesting.",
                    "label": 0
                },
                {
                    "sent": "It's not penalized towards the mean, it's this actually here you want the average to be 0.",
                    "label": 0
                },
                {
                    "sent": "This would be your contribution if you.",
                    "label": 0
                },
                {
                    "sent": "Personally we're going to make up all.",
                    "label": 0
                },
                {
                    "sent": "Of the amount by which the exchange fails to clear, right?",
                    "label": 0
                },
                {
                    "sent": "That's what this would be.",
                    "label": 0
                },
                {
                    "sent": "And so this says you should.",
                    "label": 0
                },
                {
                    "sent": "You should add it.",
                    "label": 0
                },
                {
                    "sent": "You should move towards that amount, right?",
                    "label": 0
                },
                {
                    "sent": "So again, it makes just perfect sense, right?",
                    "label": 0
                },
                {
                    "sent": "And you get you get regularize, this is regularised Teton Mall right at home on is this price process where you look at how much is consumed you compare it to how much you have and then you increase or decrease prices.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is dual decomposition.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'll.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll skip this and just mention that this drops directly into.",
                    "label": 0
                },
                {
                    "sent": "Don't things like dynamic energy management, right?",
                    "label": 1
                },
                {
                    "sent": "So here if you're exchanging power in a bunch of periods, these are just these are completely separate commodities.",
                    "label": 0
                },
                {
                    "sent": "They are completely independent, and in fact this just works perfectly for that, and I think I won't.",
                    "label": 0
                },
                {
                    "sent": "I think maybe what I'll do is skip ahead so that if people have questions where I'm not going to run out of Question Time, so I think I'll I'll even skip this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whoops, sorry.",
                    "label": 0
                },
                {
                    "sent": "And just go right to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "OK, so good, pretty simple conclusions.",
                    "label": 0
                },
                {
                    "sent": "Hmm, it's identical to Douglas Rachford splitting it from the 70s.",
                    "label": 0
                },
                {
                    "sent": "I could make a pretty good argument that the seeds of the idea extend to the mid 50s.",
                    "label": 0
                },
                {
                    "sent": "I mean, probably those who did it then wouldn't recognize it, but that's too bad.",
                    "label": 0
                },
                {
                    "sent": "They would after I argued with him for a while anyway.",
                    "label": 0
                },
                {
                    "sent": "So you get.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's nice you get single processor algorithms that can be competitive with state of the art, so you can implement things very quickly.",
                    "label": 1
                },
                {
                    "sent": "But the main interesting thing is it allows you to coordinate a lot of processors, each solving a substantial problem to solve a very large problem.",
                    "label": 1
                },
                {
                    "sent": "And I think what's very different about sort of this approach to the methods I have seen people implement.",
                    "label": 0
                },
                {
                    "sent": "For large scale optimization and machine learning they do things like take stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "I mean that's perfectly OK and then map that onto some modern framework.",
                    "label": 0
                },
                {
                    "sent": "This is actually quite different.",
                    "label": 0
                },
                {
                    "sent": "This says if you want to unregularized logistic regression, no problem.",
                    "label": 0
                },
                {
                    "sent": "Have a bunch of have a bunch of workers doing it, but they're not just doing stochastic gradient calculations and they're actually doing full up.",
                    "label": 0
                },
                {
                    "sent": "Each one is doing full up logistic Reg, logistic minimization, maybe with sorry.",
                    "label": 0
                },
                {
                    "sent": "With a quadratic regularization term right?",
                    "label": 0
                },
                {
                    "sent": "So it's actually quite different way to do this.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it Maps very poorly onto onto modern frameworks only because.",
                    "label": 0
                },
                {
                    "sent": "No one asked modern frameworks to do things like this before, but maybe that'll maybe that'll get fixed.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe it won't, but hopefully it will be.",
                    "label": 0
                },
                {
                    "sent": "And then maybe these will be viable methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll I'll quit here.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm so glad you asked that, so I'll repeat the question, how do you choose the answer?",
                    "label": 0
                },
                {
                    "sent": "Oh, you want the honest answer?",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "So my.",
                    "label": 0
                },
                {
                    "sent": "The question was the honest answer.",
                    "label": 0
                },
                {
                    "sent": "Oh the semi honest answer.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Let's see I can I can make it up.",
                    "label": 0
                },
                {
                    "sent": "I can make one up on the fly so I could say this, but notice that it's not an answer I could say.",
                    "label": 0
                },
                {
                    "sent": "That there's values of row in a wide range.",
                    "label": 0
                },
                {
                    "sent": "We get reasonable performance and if you like, did you like that?",
                    "label": 0
                },
                {
                    "sent": "And I'd also say the following if you do any kind of caching or something like that, then it's less relevant here.",
                    "label": 0
                },
                {
                    "sent": "Whether you do 20 or 200 steps, then in an algorithm where each step costs about the same amount.",
                    "label": 0
                },
                {
                    "sent": "But I do want to give Full disclosure that that was not an answer, so so it's.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've we've looked at, it obviously has to do with sort of the curvature of F&G and things like that.",
                    "label": 0
                },
                {
                    "sent": "But in the case when F&G are non differentiable, which is the whole point here, right?",
                    "label": 0
                },
                {
                    "sent": "'cause if F&G have a curvature that you know about, you should probably be using like accelerated type methods.",
                    "label": 0
                },
                {
                    "sent": "You know you should nest arises that a verb.",
                    "label": 0
                },
                {
                    "sent": "I think it is.",
                    "label": 0
                },
                {
                    "sent": "Anyway around Stanford, people refer to that algorithm and not this right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "I mean for things with Lipschitz, I mean for things that are which have known curvatures, you could answer that question.",
                    "label": 0
                },
                {
                    "sent": "But here in the general case.",
                    "label": 0
                },
                {
                    "sent": "We don't really know.",
                    "label": 0
                },
                {
                    "sent": "Any other questions I can given an answer to, yeah.",
                    "label": 0
                },
                {
                    "sent": "That's that's actually what I think.",
                    "label": 0
                },
                {
                    "sent": "How do you we scale?",
                    "label": 0
                },
                {
                    "sent": "We scale data appropriately and then take row equals one.",
                    "label": 0
                },
                {
                    "sent": "Oh, I didn't know.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of thing I don't like to say on on the record.",
                    "label": 0
                },
                {
                    "sent": "I've heard there are people who do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure, OK, so actually that was even known in the 80s.",
                    "label": 0
                },
                {
                    "sent": "The question was, what about in exact minimization?",
                    "label": 0
                },
                {
                    "sent": "And so?",
                    "label": 0
                },
                {
                    "sent": "In fact, even in the in the 80s when the sub problems were solved by an iterative method, one of the obvious things is, you know when you're starting this out, you don't.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to minimize these augmented lagrangians exactly, so even even then it was worked out and their simple conditions would be things like.",
                    "label": 0
                },
                {
                    "sent": "But they all the simple condition is this.",
                    "label": 0
                },
                {
                    "sent": "If the sum of the suboptimality's in solving the subproblems.",
                    "label": 0
                },
                {
                    "sent": "My night then it converges, right?",
                    "label": 0
                },
                {
                    "sent": "But the rough idea is the the precision which you solve.",
                    "label": 0
                },
                {
                    "sent": "The subproblems should go down as you as you as you run, right?",
                    "label": 0
                },
                {
                    "sent": "So but OK, now that I've already like you, know how we do it ready ready for this?",
                    "label": 0
                },
                {
                    "sent": "Can you OK?",
                    "label": 0
                },
                {
                    "sent": "Can you handle it right so we do 15 steps of like LBF's just.",
                    "label": 0
                },
                {
                    "sent": "15 and call it a day works fine.",
                    "label": 0
                },
                {
                    "sent": "But yes, maybe you can easily prove that you converge to within that some Epson.",
                    "label": 0
                },
                {
                    "sent": "So if you do not let epsilon disappear right, you can easily prove that you converge to within epsilon.",
                    "label": 0
                },
                {
                    "sent": "Epsilon does good you know why epsilon goes to 0, because when you get down to where the solution 15 steps of LBF's is more than enough to do the trick, remember this regularization, right?",
                    "label": 0
                },
                {
                    "sent": "So with regularization of like a logistic function, trust me.",
                    "label": 0
                },
                {
                    "sent": "15 steps.",
                    "label": 0
                },
                {
                    "sent": "When you get into, the solution is way more than enough.",
                    "label": 0
                },
                {
                    "sent": "So theoretically, yes, you.",
                    "label": 0
                },
                {
                    "sent": "You could you could do that, I presume.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Just one.",
                    "label": 0
                },
                {
                    "sent": "Right, actually sorry for the map ones.",
                    "label": 0
                },
                {
                    "sent": "There's a unique one, right?",
                    "label": 0
                },
                {
                    "sent": "Because you've added this quadratic regularization right?",
                    "label": 0
                },
                {
                    "sent": "So it's unique.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, right.",
                    "label": 0
                },
                {
                    "sent": "So with the quote.",
                    "label": 0
                },
                {
                    "sent": "Sorry with an A in there then it's not unique 'cause you could have anything in sort of the null space of a or something like that and then it doesn't matter what it what it returns.",
                    "label": 0
                },
                {
                    "sent": "So yeah, Oh yeah, so I never I mentioned that they never never got back to it.",
                    "label": 0
                },
                {
                    "sent": "So the question is how do you split into three and things like that?",
                    "label": 0
                },
                {
                    "sent": "And you can work things like that out and I should mention that the Russian style of all this was to split into K sets right off the bat, but it turns out you know once you study it more, you realize that we split into two, but the key was that in one of them the function was separable.",
                    "label": 0
                },
                {
                    "sent": "So what looked like a single minimization was actually split was splitting right?",
                    "label": 0
                },
                {
                    "sent": "So I think there are two styles.",
                    "label": 0
                },
                {
                    "sent": "I think they're kind of the same.",
                    "label": 0
                },
                {
                    "sent": "Or at least we can do what we want.",
                    "label": 0
                },
                {
                    "sent": "I can it, like the algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "The master algorithm splits only into two things, but each of the steps might split into parallel things.",
                    "label": 0
                },
                {
                    "sent": "So did that.",
                    "label": 0
                },
                {
                    "sent": "That's yeah, that's possible, so I won't deny it.",
                    "label": 0
                },
                {
                    "sent": "But what's amazing is how far you can get with two separate ability.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So how do you warm start it?",
                    "label": 0
                },
                {
                    "sent": "Don't you just?",
                    "label": 0
                },
                {
                    "sent": "It's no problem.",
                    "label": 0
                },
                {
                    "sent": "I mean you start it from, you have to warm.",
                    "label": 0
                },
                {
                    "sent": "Start with primal and dual variables.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can't just reinitialize the dual variable zero or you spend all your time rebuilding up to dual variable.",
                    "label": 0
                },
                {
                    "sent": "But if you initialize the primal variable and the dual variable you'll do very well.",
                    "label": 0
                },
                {
                    "sent": "So the other thing is of course for things like that, for reweighting, if those weights.",
                    "label": 0
                },
                {
                    "sent": "If you do a splitting where the weights appear in the in the park but not in the linear algebra part, then you can do factorization caching.",
                    "label": 0
                },
                {
                    "sent": "I know you can't because your problems are too big.",
                    "label": 0
                },
                {
                    "sent": "I know I know.",
                    "label": 0
                },
                {
                    "sent": "OK, OK that's what you're saying.",
                    "label": 0
                },
                {
                    "sent": "And yeah, but you know what?",
                    "label": 0
                },
                {
                    "sent": "There's ways to wiggle things around so they don't even when they appear to appear in the.",
                    "label": 0
                },
                {
                    "sent": "In the quarter, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You do image processing.",
                    "label": 0
                },
                {
                    "sent": "Those problems are too big anyway, so you can't use a direct method, but OK. OK, yes.",
                    "label": 0
                },
                {
                    "sent": "That's an excellent question.",
                    "label": 0
                },
                {
                    "sent": "Not really right in dual decomposition, of course.",
                    "label": 0
                },
                {
                    "sent": "When you minimize that you gotta you gotta deal feasible.",
                    "label": 0
                },
                {
                    "sent": "You get a lower lower bound and actually you don't in a DMM so you can get things that are surrogates for it.",
                    "label": 0
                },
                {
                    "sent": "So there are some excellent surrogates for it, right?",
                    "label": 0
                },
                {
                    "sent": "But like an interior point methods, right?",
                    "label": 0
                },
                {
                    "sent": "What's actually looking at an interior point method generally is you're looking at something that would be the dual residual if the if the other residuals were zero, which they're not or something.",
                    "label": 0
                },
                {
                    "sent": "In the end, they're all small, and then everything is cool, but and you get something like that here, it has the same flavor.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For DC problem.",
                    "label": 0
                },
                {
                    "sent": "Actually yes, so for non convex problems.",
                    "label": 0
                },
                {
                    "sent": "In fact there's an interesting thing we did here where you can take a DMM and run it for non convex split F&G.",
                    "label": 0
                },
                {
                    "sent": "And here's an example.",
                    "label": 0
                },
                {
                    "sent": "Let's take Jeep which was a Lambda times in one norm OK and you say well that's a sparsifying regularizer.",
                    "label": 0
                },
                {
                    "sent": "But let's say let's do the following.",
                    "label": 0
                },
                {
                    "sent": "Let's let G be the indicator function of the set of vectors with no more than K nonzeros.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's the indicator function of.",
                    "label": 0
                },
                {
                    "sent": "It's a horrible non convex set, but we can compute Euclidean projection onto it exactly.",
                    "label": 0
                },
                {
                    "sent": "Right 'cause how do you project onto you?",
                    "label": 0
                },
                {
                    "sent": "You find the largest K elements and user out all the others.",
                    "label": 0
                },
                {
                    "sent": "I mean it's simple, right?",
                    "label": 0
                },
                {
                    "sent": "So then you can run a DMM.",
                    "label": 0
                },
                {
                    "sent": "OK and then and then you you get some bonus points for solving each subproblem you're solving globally, including the nonconvex one.",
                    "label": 0
                },
                {
                    "sent": "Then you might ask what can you say about the whole thing?",
                    "label": 0
                },
                {
                    "sent": "And as far as I know, the answer is absolutely nothing.",
                    "label": 0
                },
                {
                    "sent": "However, it does seem to work quite well.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's kind of weird, like if you do the if you take Lambda and then very landed to get the sparsity fit tradeoff and then do the same thing with this algorithm, about which you know nothing.",
                    "label": 0
                },
                {
                    "sent": "You get pretty much the same tradeoff, so.",
                    "label": 0
                },
                {
                    "sent": "It can do with it whatever you want.",
                    "label": 0
                },
                {
                    "sent": "It's just I'm just telling you so.",
                    "label": 0
                },
                {
                    "sent": "Would be great to actually be able to say something about that.",
                    "label": 0
                },
                {
                    "sent": "Magical values of know you might be able to see you might be able to say something you know.",
                    "label": 0
                },
                {
                    "sent": "My suspicion, though, is that what you would say would ultimately be it be fine, you know it'll be OK.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a little boring.",
                    "label": 0
                },
                {
                    "sent": "It's going to be something like this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, whole bunch of conditions and you'll say in that case a DMM converted some local minimum.",
                    "label": 0
                },
                {
                    "sent": "You know, frankly, big deal.",
                    "label": 0
                },
                {
                    "sent": "I mean converted to local minimum.",
                    "label": 0
                },
                {
                    "sent": "I mean, so I mean, actually algorithms always converge because you just put in you always right?",
                    "label": 0
                },
                {
                    "sent": "For it are equals one to Max enter.",
                    "label": 0
                },
                {
                    "sent": "So I don't really.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't really understand this fetish about knowing what does this mean.",
                    "label": 0
                },
                {
                    "sent": "It means, like you'd say, no, there's a huge advance.",
                    "label": 0
                },
                {
                    "sent": "You don't understand.",
                    "label": 0
                },
                {
                    "sent": "This means it means that if we were to run this album forever, which we would never do, then it would eventually converge to something you would that be the solution?",
                    "label": 0
                },
                {
                    "sent": "You say no.",
                    "label": 0
                },
                {
                    "sent": "Sci-fi anyway.",
                    "label": 0
                },
                {
                    "sent": "To be able to say after you stop after 100 steps right did you reach?",
                    "label": 0
                },
                {
                    "sent": "Right, that is hard to say actually.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "Is it exactly?",
                    "label": 0
                },
                {
                    "sent": "It's probably even a local minimum quality stationary point, right?",
                    "label": 0
                },
                {
                    "sent": "So OK so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's simple.",
                    "label": 0
                },
                {
                    "sent": "Actually that was answered by someone else's question about the uniqueness.",
                    "label": 0
                },
                {
                    "sent": "Just take a matrix, a right which has a nullspace an I'll arrange so that any you know any element in the null space is a salute.",
                    "label": 0
                },
                {
                    "sent": "You know you have a big big optimal set.",
                    "label": 0
                },
                {
                    "sent": "And then when you return an Arg min, maybe you have a choice.",
                    "label": 0
                },
                {
                    "sent": "You have a whole subspace of things you can choose, so you simply return different ones each time.",
                    "label": 0
                },
                {
                    "sent": "Just for fun, right?",
                    "label": 0
                },
                {
                    "sent": "And what will happen is of course X won't converge.",
                    "label": 0
                },
                {
                    "sent": "However, I can tell you what will happen.",
                    "label": 0
                },
                {
                    "sent": "The primal residual will go to zero and the objective value will go to the optimal value, but your ex is jumping all around.",
                    "label": 0
                },
                {
                    "sent": "But if you think carefully about what you need in practice.",
                    "label": 0
                },
                {
                    "sent": "So what if X is not converging?",
                    "label": 0
                },
                {
                    "sent": "It's totally irrelevant and it means nothing in practice.",
                    "label": 0
                },
                {
                    "sent": "That means you probably have a silly optimization problem is what it really means, right?",
                    "label": 0
                },
                {
                    "sent": "I mean not you personally I'm just saying, you know.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                }
            ]
        }
    }
}