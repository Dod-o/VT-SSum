{
    "id": "3mr7q3uoresir7f3ny5mnhwuy6tdu3au",
    "title": "Robust Multilingual Statistical Morphological Generation Models",
    "info": {
        "author": [
            "Ond\u0159ej Du\u0161ek, Institute of Formal and Applied Linguistics (\u00daFAL), Charles University Prague"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_dusek_models/",
    "segmentation": [
        [
            "Good afternoon, so I would like to talk to you for awhile about morphological generation.",
            "Oh thanks.",
            "So."
        ],
        [
            "What's it all about generating morphology is the last step in the whole NLG pipeline, and it usually doesn't get very much attention.",
            "But nevertheless, it's quite necessary so.",
            "Our system is located really at the end of the whole pipeline.",
            "An so it means that somebody needs to 1st generate syntax from semantics and morphology or like morphological.",
            "And Marks and then our task is to.",
            "Take the base forms and morphogen make backs out of it.",
            "And we do this for six languages.",
            "So."
        ],
        [
            "So.",
            "And the morphological generation is doesn't get much attention.",
            "Maybe becausw for English.",
            "It's not very much needed.",
            "Like hard coded solutions, very simple ones often work well enough, but when you have languages with a lot of flexion then the situations gets more complicated.",
            "So you need inflection even for the most simple things.",
            "These examples are from Facebook and Doodle.",
            "And.",
            "You can see that in the first example there's one word which means user which was inserted there only to avoid inflecting the name of the user and the world itself is in the dative case and is masculine, but it can stand along with a feminine name in nominative, so this is really not very fluent.",
            "UN doodle check.",
            "There's a vocative case, so when you need to address somebody you need to use his name.",
            "In vocative case, but Doodle just ignores it and uses the name in Nominative case, so this is also not very fluent, and these are very simple applications.",
            "We just use filling in some templates so."
        ],
        [
            "That our task is given given some words, which is usually a base word, form a lemma or stem and the morphological properties of these words, which means both stack in some languages case like in German, dative case, gender, it's neuter or some other properties like.",
            "This is for Spanish, so in verbs you have gender, number, person.",
            "Mood and dance and our task is to generate the correct word form that.",
            "Corresponds to the lemma and the morphological properties, so this task is more or less inverse to part of speech tagging or morphological annotation."
        ],
        [
            "So the possible solutions to this are a dictionary which works quite well, but once you get a word which is not in the dictionary, you cannot do anything about it and there are not many large coverage dictionaries that are openly available.",
            "Other possibility is to write some rules, hardcode them, but this also works well and it works mostly well enough for English.",
            "But for some languages with a lot of flexion they are very hard to maintain.",
            "And our approach which we chose was to obtain those rules automatically due to machine learning from from a tree bank or from a morphologically annotated corpus.",
            "There are many, many corpora available, and the only previous work which which we know that did this was worked out by Benbow net.",
            "It's part of the deep generation system where they apply.",
            "Machine learning to do generation."
        ],
        [
            "And how we do this?",
            "How we learn those rules or how we make those rules work in a machine learning setting is that we use thing we called edit scripts.",
            "It's A kind of diff between the base form and the inflected form.",
            "It's based on the Levenshtein distance and it just tell us how to modify the base form to get the target form so you can see that at the end you delete one letter.",
            "And at those three letters there are more possibilities, like when you when you have changes at the beginning, then then you may know that you need to add some characters to the beginning of the world besides changing the ending.",
            "Some languages change even.",
            "On the inside and here we count the number of letters from the end, and there we perform the change.",
            "We change here 1 one letter and some forms are completely irregular, so we just replace the whole world.",
            "So these kind of rules is what we need to learn from from the corpus.",
            "So this setting is very similar to the work of Benbo net, but."
        ],
        [
            "So we introduce some new features to make it really work well and to make it generalize on previously unseen inputs.",
            "So there's one observation that words that end on.",
            "Same letters usually inflect in the same way, so the blur of Sky and fly.",
            "Changes to IES or the past tense of bind and find its bound and found so.",
            "This works much better in other languages than English, but well, these examples are mainly to be understandable to everybody, so we can see that the suffixes are good features to generalize to unseen inputs, and even though it doesn't work always that way, machine learning should be able to deal with it.",
            "Another thing we know that capitalization like.",
            "Capital letters do not influence morphology in anyway, so we treat all words case insensitive, so that's the main differences from the work of bamboo net and.",
            "Yeah."
        ],
        [
            "That this is how our whole system works.",
            "We have just just the base form of a word, some morphological properties.",
            "Then we add the suffixes and sometimes even combinations of morphological properties.",
            "And then we apply logistic regression, which is the machine learning algorithm that we chose and it works quite well for this setting and we predict those rules.",
            "So now we predicted that we should add an N to the end.",
            "And oh, there's an error.",
            "There should be some more letters, OK?",
            "And.",
            "Now we will.",
            "We should change change changes character and at the end we take this rule we take the base form and.",
            "Implies those changes and generates the target form, so you can see these characters went missing from the slide, but they should be there.",
            "So, so that's how it how it all works.",
            "And."
        ],
        [
            "We evaluated our system on six different languages from the kernel 2009 data sets.",
            "These are languages with varying morphology, richness, and different text sets, and these are the results so.",
            "We can see that we stay above 90% in all the cases.",
            "Some of the languages got really very high scores, so in English it's about 99% correctness.",
            "Check also Japanese as well Catalan and Spanish.",
            "Worse, and German performs worst, and that's about 96.5% and in the red bars you can see the performance on previously unseen forms, so you can see that that four forms unseen in the training data.",
            "The system really generalize well and that should should mean that the suffix features.",
            "Anne.",
            "The things we did with the classifier would work quite well.",
            "So.",
            "But yeah, there are also some errors which are due to over generalization.",
            "So for example in English where you have verb torpedo torpedo someone and you need to have a past tense, our system will generate therapy done because the verb has the same ending as the work do.",
            "And the lower score in German, we investigated the data and.",
            "We found out that Chairman has a very syntax sensitive morphology, so if you use the definite article or indefinite article that influences the way that the noun or an adjective is inflected, even though it's, I don't know five or six words away.",
            "So, and morphological backset doesn't provide any indication about this, so this should be solved with some syntax based features."
        ],
        [
            "And we also compared.",
            "Our system with a baseline, which is just memorizing all the forms we've seen in the training data and providing those forms and the base form, as when we don't know what we can do.",
            "So that's a simple dictionary.",
            "Approach a dictionary.",
            "Learn from from the same training datasets.",
            "Our system for English Dictionary performs quite well really.",
            "Soon we can see that just on 1% of the training data we got the highest error reduction and it's it's goes, it converges.",
            "It goes down, so even even on the unknown forms to dictionary or just using the base form and leaving in an inflected is well enough in more than 80% cases.",
            "So.",
            "For English it's it's quite OK to use a dictionary, but still our system can improve on it, but four."
        ],
        [
            "Like we see a completely different picture because check is really a very morphologically rich language and we can see that.",
            "Our system continues to improve and the dictionary stays well below it an.",
            "We achieved 92% error reduction on the full training data and this number has been increasing all the way until until the full training set, so it may be even more with more training data and you can see that the dictionary for unknown forms performs really badly so.",
            "Because most of the unknown forms require some inflection and the dictionary when you don't have them in the dictionary, you cannot do anything about it.",
            "So that."
        ],
        [
            "To conclude.",
            "We have some general observations, so if this really possible to learn inflection from corpus using machine learning.",
            "The features that you suffixes of the words are really useful to inflect unseen words to generalize to unseen inputs.",
            "And.",
            "What we've seen in German if you don't have enough morphological features or some context or syntax based features, then then the performance is lower, so having those features helps.",
            "And our system is able to consistently improve on a dictionary length from the same amount of data.",
            "But this is much more visible in morphologically rich languages such as check.",
            "And our system can also be combined with the dictionary as a back off, so you can use a dictionary and when you don't find the word in the dictionary then you can use our system to provide the correct inflected form so that."
        ],
        [
            "All from me.",
            "Thank you for that attention.",
            "So thanks for the talk.",
            "Very interesting, but I've got a question to one of the first sentences of the presentation that capitalization doesn't include a influence inflection.",
            "Yes, because I don't know.",
            "Check, but in Polish it does a lot because there are many surnames that are the same as common nouns, and they inflect differently.",
            "Being a surname, then they would be in common none.",
            "So is it different in check are?",
            "Am I wrong?",
            "Yeah, it's it's different.",
            "Then check in and check that there's no difference in having non capitalized or not so OK.",
            "It's also it's always inflected in the same way.",
            "So OK, thank you.",
            "No.",
            "Yeah, but that's that's a masculine gender.",
            "That's the question of gender.",
            "The gender is different and you know the gender.",
            "Yeah, and you can have the same gender an and and same case and different things.",
            "OK, I didn't know that so maybe for Polish we would need to include some feature that would tell whether the word is capitalized or not.",
            "OK, thank you.",
            "I was just wondering how did you get the diff scripts like the machine learning seemed to be basically this multiclass logistic regression in order to pick a script, but like I'm very interested in unsupervised learning up morphology and for us.",
            "Usually the problem really is identifying the patterns we should use.",
            "So how did you derive those like?",
            "Well we have those training data where you have the lemma and the target word form an.",
            "You can use Levenshtein distance too.",
            "Two or an algorithm based on Levenshtein distance, which is completely rule based and which can tell you which can basically produce the character based diff between the lemma and then the words and then."
        ],
        [
            "We just encoded like in.",
            "In this in this really funny way so so.",
            "This is what we get from the algorithm and and this is our right.",
            "I mean, so you haven't looked at generalizing like.",
            "I mean, of course you know in German, for example, the um laotong could.",
            "If you have word like shocked, the vowel would be the third thing in place, so it would still be Schechter.",
            "And so I mean OK.",
            "So yeah, then then, then you will have a different letter here.",
            "Exactly this different number here and then it will be a different rule.",
            "OK, but we basically just use minimum edit distance and then you take the minimum edit path.",
            "And and I mean, have you thought about generalizing?",
            "You know it like, I mean, you could imagine if you had.",
            "I don't know 25 patterns like this in German, and you know, have some way of automatic.",
            "I mean that would be very cool.",
            "I have no idea how to do that well, but I think it will work quite well for German.",
            "Or maybe it would.",
            "It would fix some of the errors and we make in German, but it wouldn't be usable for other languages so well, I mean, you would hope that that OK and so.",
            "I mean actually had a second question that you answered that kind of like so if you had reduplication, for example, which doesn't occur in any of the languages, but you wouldn't really model that right?",
            "I mean, you're really learning from the training data, you're learning word specific edits, and they actually, I mean, they seem to help quite a lot.",
            "It's really impressive how far you can go, but there's no generalization involved over the templates.",
            "Yeah, OK, well, for example, for English I think we have like 200 rules from the training data, but for German it's about 1700.",
            "So it's much more.",
            "Yeah, yeah, exactly.",
            "Anymore questions.",
            "I had one sort of following up on this.",
            "What do you think could be the next step in the over generalization?",
            "'cause I find that to be a, I mean a big problem.",
            "The over generalization over generalizations that are done, for example well overdue, then overdone, but torpedo to be done.",
            "This example you mean how how to go about solving that?",
            "Or I think that."
        ],
        [
            "There's a bit of a problem that we.",
            "We use a logistic regression which is a linear classifier, so we use some concatenation of the morphological forms, but but only of them only of them, among among them.",
            "So maybe if we concatenate the.",
            "The lemma with the with the morphological forms that could maybe provide hints too.",
            "Do the class classifier, but I'm really not sure about about torpedo and or.",
            "Because the word do is really very frequent, so I didn't even try fixing it.",
            "Thank you, speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, so I would like to talk to you for awhile about morphological generation.",
                    "label": 0
                },
                {
                    "sent": "Oh thanks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's it all about generating morphology is the last step in the whole NLG pipeline, and it usually doesn't get very much attention.",
                    "label": 1
                },
                {
                    "sent": "But nevertheless, it's quite necessary so.",
                    "label": 0
                },
                {
                    "sent": "Our system is located really at the end of the whole pipeline.",
                    "label": 0
                },
                {
                    "sent": "An so it means that somebody needs to 1st generate syntax from semantics and morphology or like morphological.",
                    "label": 0
                },
                {
                    "sent": "And Marks and then our task is to.",
                    "label": 0
                },
                {
                    "sent": "Take the base forms and morphogen make backs out of it.",
                    "label": 0
                },
                {
                    "sent": "And we do this for six languages.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the morphological generation is doesn't get much attention.",
                    "label": 0
                },
                {
                    "sent": "Maybe becausw for English.",
                    "label": 0
                },
                {
                    "sent": "It's not very much needed.",
                    "label": 0
                },
                {
                    "sent": "Like hard coded solutions, very simple ones often work well enough, but when you have languages with a lot of flexion then the situations gets more complicated.",
                    "label": 1
                },
                {
                    "sent": "So you need inflection even for the most simple things.",
                    "label": 0
                },
                {
                    "sent": "These examples are from Facebook and Doodle.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can see that in the first example there's one word which means user which was inserted there only to avoid inflecting the name of the user and the world itself is in the dative case and is masculine, but it can stand along with a feminine name in nominative, so this is really not very fluent.",
                    "label": 0
                },
                {
                    "sent": "UN doodle check.",
                    "label": 0
                },
                {
                    "sent": "There's a vocative case, so when you need to address somebody you need to use his name.",
                    "label": 0
                },
                {
                    "sent": "In vocative case, but Doodle just ignores it and uses the name in Nominative case, so this is also not very fluent, and these are very simple applications.",
                    "label": 0
                },
                {
                    "sent": "We just use filling in some templates so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That our task is given given some words, which is usually a base word, form a lemma or stem and the morphological properties of these words, which means both stack in some languages case like in German, dative case, gender, it's neuter or some other properties like.",
                    "label": 1
                },
                {
                    "sent": "This is for Spanish, so in verbs you have gender, number, person.",
                    "label": 0
                },
                {
                    "sent": "Mood and dance and our task is to generate the correct word form that.",
                    "label": 1
                },
                {
                    "sent": "Corresponds to the lemma and the morphological properties, so this task is more or less inverse to part of speech tagging or morphological annotation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the possible solutions to this are a dictionary which works quite well, but once you get a word which is not in the dictionary, you cannot do anything about it and there are not many large coverage dictionaries that are openly available.",
                    "label": 1
                },
                {
                    "sent": "Other possibility is to write some rules, hardcode them, but this also works well and it works mostly well enough for English.",
                    "label": 1
                },
                {
                    "sent": "But for some languages with a lot of flexion they are very hard to maintain.",
                    "label": 0
                },
                {
                    "sent": "And our approach which we chose was to obtain those rules automatically due to machine learning from from a tree bank or from a morphologically annotated corpus.",
                    "label": 0
                },
                {
                    "sent": "There are many, many corpora available, and the only previous work which which we know that did this was worked out by Benbow net.",
                    "label": 1
                },
                {
                    "sent": "It's part of the deep generation system where they apply.",
                    "label": 0
                },
                {
                    "sent": "Machine learning to do generation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how we do this?",
                    "label": 0
                },
                {
                    "sent": "How we learn those rules or how we make those rules work in a machine learning setting is that we use thing we called edit scripts.",
                    "label": 0
                },
                {
                    "sent": "It's A kind of diff between the base form and the inflected form.",
                    "label": 1
                },
                {
                    "sent": "It's based on the Levenshtein distance and it just tell us how to modify the base form to get the target form so you can see that at the end you delete one letter.",
                    "label": 1
                },
                {
                    "sent": "And at those three letters there are more possibilities, like when you when you have changes at the beginning, then then you may know that you need to add some characters to the beginning of the world besides changing the ending.",
                    "label": 0
                },
                {
                    "sent": "Some languages change even.",
                    "label": 1
                },
                {
                    "sent": "On the inside and here we count the number of letters from the end, and there we perform the change.",
                    "label": 1
                },
                {
                    "sent": "We change here 1 one letter and some forms are completely irregular, so we just replace the whole world.",
                    "label": 0
                },
                {
                    "sent": "So these kind of rules is what we need to learn from from the corpus.",
                    "label": 0
                },
                {
                    "sent": "So this setting is very similar to the work of Benbo net, but.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we introduce some new features to make it really work well and to make it generalize on previously unseen inputs.",
                    "label": 0
                },
                {
                    "sent": "So there's one observation that words that end on.",
                    "label": 0
                },
                {
                    "sent": "Same letters usually inflect in the same way, so the blur of Sky and fly.",
                    "label": 0
                },
                {
                    "sent": "Changes to IES or the past tense of bind and find its bound and found so.",
                    "label": 0
                },
                {
                    "sent": "This works much better in other languages than English, but well, these examples are mainly to be understandable to everybody, so we can see that the suffixes are good features to generalize to unseen inputs, and even though it doesn't work always that way, machine learning should be able to deal with it.",
                    "label": 1
                },
                {
                    "sent": "Another thing we know that capitalization like.",
                    "label": 0
                },
                {
                    "sent": "Capital letters do not influence morphology in anyway, so we treat all words case insensitive, so that's the main differences from the work of bamboo net and.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this is how our whole system works.",
                    "label": 0
                },
                {
                    "sent": "We have just just the base form of a word, some morphological properties.",
                    "label": 0
                },
                {
                    "sent": "Then we add the suffixes and sometimes even combinations of morphological properties.",
                    "label": 0
                },
                {
                    "sent": "And then we apply logistic regression, which is the machine learning algorithm that we chose and it works quite well for this setting and we predict those rules.",
                    "label": 0
                },
                {
                    "sent": "So now we predicted that we should add an N to the end.",
                    "label": 0
                },
                {
                    "sent": "And oh, there's an error.",
                    "label": 0
                },
                {
                    "sent": "There should be some more letters, OK?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now we will.",
                    "label": 0
                },
                {
                    "sent": "We should change change changes character and at the end we take this rule we take the base form and.",
                    "label": 0
                },
                {
                    "sent": "Implies those changes and generates the target form, so you can see these characters went missing from the slide, but they should be there.",
                    "label": 0
                },
                {
                    "sent": "So, so that's how it how it all works.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated our system on six different languages from the kernel 2009 data sets.",
                    "label": 1
                },
                {
                    "sent": "These are languages with varying morphology, richness, and different text sets, and these are the results so.",
                    "label": 1
                },
                {
                    "sent": "We can see that we stay above 90% in all the cases.",
                    "label": 0
                },
                {
                    "sent": "Some of the languages got really very high scores, so in English it's about 99% correctness.",
                    "label": 0
                },
                {
                    "sent": "Check also Japanese as well Catalan and Spanish.",
                    "label": 1
                },
                {
                    "sent": "Worse, and German performs worst, and that's about 96.5% and in the red bars you can see the performance on previously unseen forms, so you can see that that four forms unseen in the training data.",
                    "label": 0
                },
                {
                    "sent": "The system really generalize well and that should should mean that the suffix features.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The things we did with the classifier would work quite well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But yeah, there are also some errors which are due to over generalization.",
                    "label": 0
                },
                {
                    "sent": "So for example in English where you have verb torpedo torpedo someone and you need to have a past tense, our system will generate therapy done because the verb has the same ending as the work do.",
                    "label": 0
                },
                {
                    "sent": "And the lower score in German, we investigated the data and.",
                    "label": 0
                },
                {
                    "sent": "We found out that Chairman has a very syntax sensitive morphology, so if you use the definite article or indefinite article that influences the way that the noun or an adjective is inflected, even though it's, I don't know five or six words away.",
                    "label": 0
                },
                {
                    "sent": "So, and morphological backset doesn't provide any indication about this, so this should be solved with some syntax based features.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also compared.",
                    "label": 0
                },
                {
                    "sent": "Our system with a baseline, which is just memorizing all the forms we've seen in the training data and providing those forms and the base form, as when we don't know what we can do.",
                    "label": 0
                },
                {
                    "sent": "So that's a simple dictionary.",
                    "label": 0
                },
                {
                    "sent": "Approach a dictionary.",
                    "label": 0
                },
                {
                    "sent": "Learn from from the same training datasets.",
                    "label": 1
                },
                {
                    "sent": "Our system for English Dictionary performs quite well really.",
                    "label": 1
                },
                {
                    "sent": "Soon we can see that just on 1% of the training data we got the highest error reduction and it's it's goes, it converges.",
                    "label": 0
                },
                {
                    "sent": "It goes down, so even even on the unknown forms to dictionary or just using the base form and leaving in an inflected is well enough in more than 80% cases.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For English it's it's quite OK to use a dictionary, but still our system can improve on it, but four.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like we see a completely different picture because check is really a very morphologically rich language and we can see that.",
                    "label": 0
                },
                {
                    "sent": "Our system continues to improve and the dictionary stays well below it an.",
                    "label": 1
                },
                {
                    "sent": "We achieved 92% error reduction on the full training data and this number has been increasing all the way until until the full training set, so it may be even more with more training data and you can see that the dictionary for unknown forms performs really badly so.",
                    "label": 1
                },
                {
                    "sent": "Because most of the unknown forms require some inflection and the dictionary when you don't have them in the dictionary, you cannot do anything about it.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We have some general observations, so if this really possible to learn inflection from corpus using machine learning.",
                    "label": 0
                },
                {
                    "sent": "The features that you suffixes of the words are really useful to inflect unseen words to generalize to unseen inputs.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we've seen in German if you don't have enough morphological features or some context or syntax based features, then then the performance is lower, so having those features helps.",
                    "label": 1
                },
                {
                    "sent": "And our system is able to consistently improve on a dictionary length from the same amount of data.",
                    "label": 1
                },
                {
                    "sent": "But this is much more visible in morphologically rich languages such as check.",
                    "label": 0
                },
                {
                    "sent": "And our system can also be combined with the dictionary as a back off, so you can use a dictionary and when you don't find the word in the dictionary then you can use our system to provide the correct inflected form so that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All from me.",
                    "label": 0
                },
                {
                    "sent": "Thank you for that attention.",
                    "label": 1
                },
                {
                    "sent": "So thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "Very interesting, but I've got a question to one of the first sentences of the presentation that capitalization doesn't include a influence inflection.",
                    "label": 0
                },
                {
                    "sent": "Yes, because I don't know.",
                    "label": 0
                },
                {
                    "sent": "Check, but in Polish it does a lot because there are many surnames that are the same as common nouns, and they inflect differently.",
                    "label": 0
                },
                {
                    "sent": "Being a surname, then they would be in common none.",
                    "label": 0
                },
                {
                    "sent": "So is it different in check are?",
                    "label": 0
                },
                {
                    "sent": "Am I wrong?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's different.",
                    "label": 0
                },
                {
                    "sent": "Then check in and check that there's no difference in having non capitalized or not so OK.",
                    "label": 0
                },
                {
                    "sent": "It's also it's always inflected in the same way.",
                    "label": 0
                },
                {
                    "sent": "So OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that's that's a masculine gender.",
                    "label": 0
                },
                {
                    "sent": "That's the question of gender.",
                    "label": 0
                },
                {
                    "sent": "The gender is different and you know the gender.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and you can have the same gender an and and same case and different things.",
                    "label": 0
                },
                {
                    "sent": "OK, I didn't know that so maybe for Polish we would need to include some feature that would tell whether the word is capitalized or not.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "I was just wondering how did you get the diff scripts like the machine learning seemed to be basically this multiclass logistic regression in order to pick a script, but like I'm very interested in unsupervised learning up morphology and for us.",
                    "label": 0
                },
                {
                    "sent": "Usually the problem really is identifying the patterns we should use.",
                    "label": 0
                },
                {
                    "sent": "So how did you derive those like?",
                    "label": 0
                },
                {
                    "sent": "Well we have those training data where you have the lemma and the target word form an.",
                    "label": 0
                },
                {
                    "sent": "You can use Levenshtein distance too.",
                    "label": 0
                },
                {
                    "sent": "Two or an algorithm based on Levenshtein distance, which is completely rule based and which can tell you which can basically produce the character based diff between the lemma and then the words and then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just encoded like in.",
                    "label": 0
                },
                {
                    "sent": "In this in this really funny way so so.",
                    "label": 0
                },
                {
                    "sent": "This is what we get from the algorithm and and this is our right.",
                    "label": 0
                },
                {
                    "sent": "I mean, so you haven't looked at generalizing like.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course you know in German, for example, the um laotong could.",
                    "label": 0
                },
                {
                    "sent": "If you have word like shocked, the vowel would be the third thing in place, so it would still be Schechter.",
                    "label": 0
                },
                {
                    "sent": "And so I mean OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah, then then, then you will have a different letter here.",
                    "label": 0
                },
                {
                    "sent": "Exactly this different number here and then it will be a different rule.",
                    "label": 0
                },
                {
                    "sent": "OK, but we basically just use minimum edit distance and then you take the minimum edit path.",
                    "label": 0
                },
                {
                    "sent": "And and I mean, have you thought about generalizing?",
                    "label": 0
                },
                {
                    "sent": "You know it like, I mean, you could imagine if you had.",
                    "label": 0
                },
                {
                    "sent": "I don't know 25 patterns like this in German, and you know, have some way of automatic.",
                    "label": 0
                },
                {
                    "sent": "I mean that would be very cool.",
                    "label": 0
                },
                {
                    "sent": "I have no idea how to do that well, but I think it will work quite well for German.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it would.",
                    "label": 0
                },
                {
                    "sent": "It would fix some of the errors and we make in German, but it wouldn't be usable for other languages so well, I mean, you would hope that that OK and so.",
                    "label": 0
                },
                {
                    "sent": "I mean actually had a second question that you answered that kind of like so if you had reduplication, for example, which doesn't occur in any of the languages, but you wouldn't really model that right?",
                    "label": 0
                },
                {
                    "sent": "I mean, you're really learning from the training data, you're learning word specific edits, and they actually, I mean, they seem to help quite a lot.",
                    "label": 0
                },
                {
                    "sent": "It's really impressive how far you can go, but there's no generalization involved over the templates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, well, for example, for English I think we have like 200 rules from the training data, but for German it's about 1700.",
                    "label": 0
                },
                {
                    "sent": "So it's much more.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "I had one sort of following up on this.",
                    "label": 0
                },
                {
                    "sent": "What do you think could be the next step in the over generalization?",
                    "label": 0
                },
                {
                    "sent": "'cause I find that to be a, I mean a big problem.",
                    "label": 0
                },
                {
                    "sent": "The over generalization over generalizations that are done, for example well overdue, then overdone, but torpedo to be done.",
                    "label": 0
                },
                {
                    "sent": "This example you mean how how to go about solving that?",
                    "label": 0
                },
                {
                    "sent": "Or I think that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a bit of a problem that we.",
                    "label": 0
                },
                {
                    "sent": "We use a logistic regression which is a linear classifier, so we use some concatenation of the morphological forms, but but only of them only of them, among among them.",
                    "label": 0
                },
                {
                    "sent": "So maybe if we concatenate the.",
                    "label": 0
                },
                {
                    "sent": "The lemma with the with the morphological forms that could maybe provide hints too.",
                    "label": 0
                },
                {
                    "sent": "Do the class classifier, but I'm really not sure about about torpedo and or.",
                    "label": 0
                },
                {
                    "sent": "Because the word do is really very frequent, so I didn't even try fixing it.",
                    "label": 0
                },
                {
                    "sent": "Thank you, speak again.",
                    "label": 0
                }
            ]
        }
    }
}