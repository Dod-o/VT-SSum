{
    "id": "27yljgef6cgbtvdx4nyyapu3cfa7yjlk",
    "title": "Training Structural SVMs when Exact Inference is Intractable",
    "info": {
        "author": [
            "Thomas Finley, Cornell University"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_finley_tssvm/",
    "segmentation": [
        [
            "Sounds like a good idea.",
            "OK, so our next talk is on training structural SVM's when exact inference is intractable.",
            "It's by Thomas Finley and thirsting Jochims, and the talk will be given by Thomas.",
            "OK, hello, everyone's Pedro said I'm Thomas and I'm here to tell you about training structure, last chance with inexact."
        ],
        [
            "So let's first review again for those that weren't here for the previous."
        ],
        [
            "Chocolate structured learning is instruction learning.",
            "We're trying to learn functions that map inputs to complex structured."
        ],
        [
            "That's which could include, say sequence labeling for parts."
        ],
        [
            "Is tagging natural language processing for part?"
        ],
        [
            "Stris forms of collective classification can be viewed as a structured learning task."
        ],
        [
            "Image segmentation work in an image you want to find interesting regions of the image under some definition of into."
        ],
        [
            "Clustering, where given a set of items you want to produce partitioning of that."
        ],
        [
            "And even binary classification could be viewed as a trivial version of structured learning problems so."
        ],
        [
            "All these different problems, despite their seemingly different forms."
        ],
        [
            "Typically have something in common."
        ],
        [
            "Specifically, they are all typically phrased as trying to find the output Y, which maximizes some joint discriminant function F between inputs and outputs, and much of that."
        ],
        [
            "Time this takes the.",
            "Takes the form of some inner product, not necessarily a linear inner product.",
            "That inner product between your learned model W and some combined feature function.",
            "Sigh, sigh."
        ],
        [
            "And when learning a model, you're given input output example pairs an you want find some model W which is good."
        ],
        [
            "According to your method and methods for learning, these include conditional random fields, Max margin, Markov networks structure, LCMS, and Perceptrons.",
            "They'll have roughly this formulation, although they don't necessarily phrase it in exactly those terms.",
            "But in the end, so that incoming, but they're different precisely how they choose their model.",
            "Given the input sample.",
            "So in all these methods, when we actually have practical argmax is this argmax distractible typically?",
            "The these methods are very well understood.",
            "They have these theoretical properties which with regard to correctness and say maximum likelihood and so forth depending on the method but."
        ],
        [
            "At point, once you have untractable argmax, is these things sort of?"
        ],
        [
            "Down like for example in image segmentation you the natural formulation for that would be a good structure model, in which case you have to use approximations.",
            "Then your parameter learning subsequently."
        ],
        [
            "Becomes approximate as well.",
            "Clustering is another example of a task.",
            "So where you have.",
            "Finally, the exact maximizing output would be in."
        ],
        [
            "Tractable and even some collective classification tasks.",
            "If the dependency structure you're attempting to exploit as loopy, suddenly that becomes a difficult problem."
        ],
        [
            "And correspondingly, not only is prediction difficult, but parameter inference.",
            "The parameter training step becomes difficult."
        ],
        [
            "As well, so let's focus on structural SVM."
        ],
        [
            "Answer For US fans work through this basic constraints here, which looks kind of complicated but is."
        ],
        [
            "Actually kind of simple, but saying is for every training."
        ],
        [
            "Sample and farming possible wrong output."
        ],
        [
            "About the swimming function for the correct output grade."
        ],
        [
            "Than that for the any incorrect output."
        ],
        [
            "And we want them to differ by the last between the correct and incorrect output that's being the margin loss talked about in the previous talk.",
            "Anne."
        ],
        [
            "And we want the end.",
            "We also have a slack which serves as a bound on empirical risk in the inseparable case."
        ],
        [
            "So."
        ],
        [
            "And the structural SVM.",
            "This is potentially this quadratic program here will have where we have slack as an upper bound on the."
        ],
        [
            "Miracle risk and because we have so many constraints whether it be say exponential or factorial or even infinite and some problems, we can't really solve this prob."
        ],
        [
            "Directly, So what a structural SVM employs is some sort of cutting plane algorithm.",
            "Well."
        ],
        [
            "We use we start with a completely unconcerned problem.",
            "For example, in this quadratic program.",
            "Suppose that we want to minimize this function.",
            "Wear purple is minimum subject to the circle constraint.",
            "We want solutions within this.",
            "Now if we want only linear constraints, we can't model this directly because we need an infinite number of these tangential constraints, so."
        ],
        [
            "Cutting plane would say.",
            "Start with an unconcerned problem.",
            "Introduce to constrain true optimize."
        ],
        [
            "Find the next most violate constraint re optimize.",
            "Find the next."
        ],
        [
            "Define constraint may optimize until such time as the most violated constraint is not violated by more than some time."
        ],
        [
            "Once, and that's exactly what it is."
        ],
        [
            "Spam structural learner does it starts with notice."
        ],
        [
            "Science repeatedly passes through the example."
        ],
        [
            "Fancy output associated with the most fun."
        ],
        [
            "Related constraint and if that constraint is violated by more than a tolerance epsilon, it introduces the constraint and re opt."
        ],
        [
            "Sizes and it stops at the point when no constraints are introduced in a pack."
        ],
        [
            "Us now this has several important theoretical properties, one being polynomial time termination.",
            "It terminates out after a finite number of polynomial number of iterations.",
            "It's correct insofar as at the point when we stop, we know that the most violated constraint is not violated by more than epsilon.",
            "So no constraint in the full QP is violated by more than epsilon, and because we are respecting our constraints, we maintain the empirical risk found again within this tolerance."
        ],
        [
            "But what happens when we're using a proxy?"
        ],
        [
            "Nations so.",
            "If we're trying to find this most file, a constraint, that is the the output that currently requires the greatest slack for this example."
        ],
        [
            "For using exact inference, you know we can find the most valid constraint, but supposed."
        ],
        [
            "Reusing and under generating approximation?",
            "Well, the trouble with not finding the most violated constraint is that at some point when we terminate, we might terminate."
        ],
        [
            "Too early, for example, to go back to our coming planning sample."
        ],
        [
            "You know, we might find a constraint."
        ],
        [
            "OK, we find a constraint."
        ],
        [
            "So another constraint.",
            "But now it's."
        ],
        [
            "Close at this point, our approximation tells us that this is the most violated constraint.",
            "The structural SVM learner will say oh great, I'm done, and it will terminate when of course, there's another obvious constraint which is violated, but it doesn't know about this.",
            "It hasn't found this constraint, so it thinks it's done, but it's really found a point which is infeasible in the original."
        ],
        [
            "Quadratic program so.",
            "We call these under generating approximations forms of local search that search the constraint space suboptimally, and at first things don't look too promising because under general approximations I just said general rule can be arbitrarily bad.",
            "Um, but so we.",
            "In order to."
        ],
        [
            "Make any real theoretical statements.",
            "We restrict our attention to what we call or approximations in our paper.",
            "So what are approximation is is it doesn't necessarily find the most optimal concern, but I will find an output which has a different function value at least ro times the optimal just remote function."
        ],
        [
            "So in the paper we have three theorems.",
            "I'm not going to go into."
        ],
        [
            "Too much technical detail, but they relate the three major quantities that you sort of encounter in structure."
        ],
        [
            "Lesbian learning, for example, in each iteration this slack."
        ],
        [
            "Acquired at the end of iteration the your objective."
        ],
        [
            "Option value and also your empirical risk bound."
        ],
        [
            "Right, So what these theorems?",
            "These theorems are all common insofar as they all.",
            "They are related.",
            "They all say that the true value for these quantities, that is, the value that would have been found had you done exact inference.",
            "Is greater than the value which you found with the approximation, but you can sort of bound how far off you are through some interval, which is dependent upon your."
        ],
        [
            "So approximation, and at the point hazro goes to one these intervals shrink to zero sensibly.",
            "Oak."
        ],
        [
            "So those are under generating approximations, but what can we say about a different class of approximation as well?",
            "Another form of approximation which is often used in this sort of setting is to not search this constraint space."
        ],
        [
            "The local search, but to rather work through some form of relaxation.",
            "Well, we do not.",
            "We are able to find the optimal.",
            "We are now able to find the most violated constraint, but only by virtue of expanding our search space to admit, say, fractional solutions.",
            "In the case of, say, relaxed LP or something like this.",
            "So we're able to actually solve the problem, except that we have these additional constraints with these fractional or non continuous."
        ],
        [
            "Solutions so.",
            "But in this case we sort of have more desirable theoretical results, because these maintain maintain correctness and the empirical risk bound.",
            "But because we have these additional constraints, the original parameterisation W and the slack might may no longer be feasible in the in this relaxed problem, but we are still searching over the space of constraints.",
            "Plus these additional constraints.",
            "So we are respecting all the constraints in the original copy, and then some.",
            "So it's not clear."
        ],
        [
            "Really, which one of these is better?",
            "Um?",
            "So we also have an empirical analysis where we tried various types of approximate."
        ],
        [
            "Oceans.",
            "Now we used.",
            "As an application binary pairwise Markov."
        ],
        [
            "Fields smart referendum field with binary node values in order to make it intractable because we sort of launch intractability to evaluate this, we made it completely connected."
        ],
        [
            "Now our target application is multi label classification.",
            "What this is is given an input X we try to find for that input a set of relevant labels.",
            "For example in new story clusterings you might have say.",
            "This story is about oil.",
            "This story is about the army.",
            "The story is about whatever.",
            "So.",
            "And what we do by modeling this as a Markov random field is we have the nodes correspond to the labels and if it has one, that label is on an.",
            "If it's zero, that label is does not apply to this example, the node potentials would would indicate an inputs, Tendency, tab label and the edge potentials.",
            "We move them as sort of two labels, tendency to Co occur parameter which is learned through the training data.",
            "Saying OK, it's pretty likely that these would will Co occur, or if it's negative.",
            "It's unlikely that these will ever Co occur or something like this, and within our model how we specifically modeled it as them in our model, we basically have a separate hyperplane which we learn for every single node potential, and we have a single value for each pair of labels.",
            "So it's very simple formulation and our loss simply is the.",
            "The percentage of different labels so 100% less would be all of our labels are wrong and.",
            "Thank you, thank you, thank you Apple.",
            "How do I get OK?",
            "I'm sorry about that.",
            "OK, so and lots of zero would be perfect labeling."
        ],
        [
            "I'm back up in 10 days, so alright, so what's important to see here is that we have the prediction problem, which is just regular map inference, but also our constraint inference problem is also an example of map inference, just with modified node potentials so and so we can use the same inference techniques.",
            "The same map inference techniques both for prediction an for constraint inference."
        ],
        [
            "So we have six datasets which we used Sing East Media million.",
            "Reuters well known real datasets.",
            "And we also have two synthetic datasets, one of which doesn't really require edge potentials to learn the target function in one which does.",
            "We also restricted waters and made him out to the 10 most frequent labels for reasons which will become."
        ],
        [
            "There in a moment and for our inference methods for under generating approximations, we use some greedy assignment approach and we also use loopy belief propagation.",
            "Annizah third under general approximation we have some sort of in sambol method of the two, and it returns the best one with the highest discriminate function at the highest likelihood of those."
        ],
        [
            "Two and four over generating approximations.",
            "We had two, one based on linear programming, relaxation of map inference, another on cuts, and both are really equivalent.",
            "But as we found out, but cuts is much faster than our general linear programming solver."
        ],
        [
            "We also have the third algorithm class which we use for comparison in order to give our experiment some meaning which is exact exhaustive inference, which is why we constrained our label space to just the 10 most frequent labels for orders and media Mail, which by themselves have 100 labels.",
            "So by doing this we can sort of keys out which what is the cause of some of the effects that we see on our analysis, so to say."
        ],
        [
            "But with the major results, let's let's first take a look at loopy belief propagation.",
            "Now here we see the losses on the six datasets with the various constraint inference.",
            "An predictive inference methods and what we see."
        ],
        [
            "Is that most people, like propagation, doesn't really perform that well.",
            "In fact, it performs astonishingly poorly and rather poorly.",
            "On five out of the six datasets and."
        ],
        [
            "What we can say is that it's bad as a training method, so the food predict with exact but trained with loopy belief propagation.",
            "We gets these really, really."
        ],
        [
            "Bad models and it's also bad as a predict."
        ],
        [
            "And method so these cases when we are training with an exact model, even when it's just uses a productive method.",
            "It also tends to stumble a lot."
        ],
        [
            "Now, while we believe this is happening is because loopy belief propagation has this tendency to work, but at the point when it stops working, it really stops working.",
            "So if we run a bunch of random experiments and count how many superior labelings there were on a random Markov random field, we see that although loopy belief propagation gets the right answer more often than greedy, it also has a tendency to have this really horrible performance on some of the.",
            "Exactly some of the random examples which greedy doesn't really do it.",
            "It performs greedy, perform sort of consistently less consistently, it's consistently."
        ],
        [
            "Mediocre.",
            "So let's now focus on our relaxation are over generating approaches.",
            "So in this case we're having results for media Mail which I just selected as representative."
        ],
        [
            "So you're not.",
            "That on these methods, which are is the model which was trained with the linear programming relaxation and tested with the various 5 inference methods.",
            "We see that.",
            "Linear programming training.",
            "The relaxed training results in the model, which isn't bad.",
            "In fact, on this data set, it's the best and is has very consistent.",
            "Performance was say, an exact training even, which is what we claimed we wanted to solve.",
            "We have these sort of very variance and combine and Brady are also pretty high variance with people.",
            "Propagation is consistent but only by virtue of being consistently bad."
        ],
        [
            "So.",
            "You also see though.",
            "Yeah, that's right, sorry.",
            "Now we have to say though, that when we were using relaxed prediction.",
            "Relax, production has a tendency to have very poor performance as a predictive method.",
            "So what's happening here is that the presence of fractional constraints when we are training using linear programming relaxations has this tendency to sort of smooth the search space so that it's actually easy to find the correct answer.",
            "And if we're not actually accounting for the weaknesses of linear programming inference, we're web tendency to have prediction, which is, say, fractional.",
            "Say it'll say the labels half on and half.",
            "Off in the relaxation."
        ],
        [
            "We can also deal with known approximation, so we had this.",
            "These theories regarding row approximations, but empirically, how well do these really perform too?",
            "Answer This phone built sort of artificial row approximate inference methods based on exact, which was then deliberately dumbed down to return are approximate solution.",
            "So we trained these models with this artificial low approximate.",
            "Artificial approximate inference technique and tested using exact inference, so this is.",
            "So this is generally getting worse and worse approximation.",
            "It's sort of encouraging insofar as it's not throwing catastrophic Lee, but on the other hand, we do see a fair amount of variation on some of these datasets, so it's not like it's consistently doing worse or consistently doing better."
        ],
        [
            "OK, so to summarize.",
            "We repeated structural SPMS and sort of explained the consequences of using an exact inference and theoretically and empirically evaluated both under generating.",
            "Approximate inference techniques that is runs based on some form of local search and over generating approximation techniques.",
            "Those involving some sort of relaxation.",
            "And using multi label classification with completely connected binary Markov random fields as a target application, we saw about over general methods sort of distinguish themselves about theoretically and in learning more robust MoD."
        ],
        [
            "And closed, I have some software which is available for you that I used in this work which you can download if you want and.",
            "Alright, that's are there any questions?"
        ],
        [
            "Well, can you repeat the question, yeah?",
            "So let me know when you have a room together.",
            "Open communication.",
            "So OK, so in my analysis the do you mean the empirical analysis?",
            "So you're asking me in my theoretical analysis of my always using a row approximation.",
            "Yeah, well the.",
            "Well, yeah, in the microphone and fields we had a row approximation.",
            "You're asking me if we used are approximation in the empirical results I believe.",
            "OK, so the answer to that is yes, and so far as we had that known approximation climb down now, loopy belief propagation for example, is not about approximate method.",
            "It can be arbitrarily bad and you can sort of construct scenarios where it performs extremely badly.",
            "So sometimes we are and sometimes we aren't does that.",
            "Well, greedy I.",
            "Let's see I think greedy is.",
            "I don't know about greedy, but the theoretical results are actually applicable to not just Markov random fields.",
            "The theoretical results are more general, so we just use this to evaluate OK, supposing that we are doing.",
            "Approximate inference.",
            "Not with.",
            "What's the weather like in three o'clock?",
            "That if we were using approximate inference algorithm at word, we chose roll approximation algorithms as something which applies.",
            "Well, let's say that people are population.",
            "Let's see relaxations.",
            "If you actually discretize them in one way, it becomes a .5 bro approximations.",
            "But else, I don't think so.",
            "So a quick.",
            "The quicker comment in the question, I think the reason will be repeated so bad in your experiments is that in some sense they are the worst possible setting for BP where you have a lot of short loops, so PvP as well when you have long loops so you know in these experiments, that's not no surprise that it was so terrible.",
            "So my question was, there's something that I missed, which is how do you determine the row in general?",
            "Well, the role would be so the question is how do you determine the role of your inference technique now in general?",
            "If you're actually making one of these structured predictors, and you are using a row approximation algorithm, you would know it because you're you're building the model and your learning algorithm.",
            "So you say, oh OK, I'm using this sort of greedy solution to do set cover, which has a 1 / 1 over natural E approximation.",
            "So you would actually know that a priority.",
            "So you need to use a method for which you know that which is not all methods.",
            "That's true, and that goes back to that other question.",
            "Which is that in not every approximation is a row approximation.",
            "We chose them as sort of a representative set which was common and relatively easy to analyze.",
            "I just like to have sex occasionally.",
            "Groupies relaxation.",
            "Suppose you get a fractional solutions that how do you fix the concentrated sizes?",
            "So the question is when we have these relaxed predictors and we do come up with the fractional most violent concern, how we how do we incorporate it OK?",
            "Well, when we do dates relax predictors.",
            "We simply expand our space of possible constraints to include these these relaxed constraints, because else we'd have to discretize them.",
            "And if you're discretizing them then we go back to using an under generating approximation.",
            "So we expand our constraint space to include the fractional fractional solutions.",
            "Include Contacts, combinations of constraints has allowed him straight in the Corporation.",
            "If you were actually learning with the last constraints, that would be what you would do.",
            "More questions.",
            "Hello, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sounds like a good idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so our next talk is on training structural SVM's when exact inference is intractable.",
                    "label": 1
                },
                {
                    "sent": "It's by Thomas Finley and thirsting Jochims, and the talk will be given by Thomas.",
                    "label": 0
                },
                {
                    "sent": "OK, hello, everyone's Pedro said I'm Thomas and I'm here to tell you about training structure, last chance with inexact.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first review again for those that weren't here for the previous.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chocolate structured learning is instruction learning.",
                    "label": 0
                },
                {
                    "sent": "We're trying to learn functions that map inputs to complex structured.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's which could include, say sequence labeling for parts.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is tagging natural language processing for part?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stris forms of collective classification can be viewed as a structured learning task.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image segmentation work in an image you want to find interesting regions of the image under some definition of into.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering, where given a set of items you want to produce partitioning of that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even binary classification could be viewed as a trivial version of structured learning problems so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these different problems, despite their seemingly different forms.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typically have something in common.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Specifically, they are all typically phrased as trying to find the output Y, which maximizes some joint discriminant function F between inputs and outputs, and much of that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time this takes the.",
                    "label": 0
                },
                {
                    "sent": "Takes the form of some inner product, not necessarily a linear inner product.",
                    "label": 0
                },
                {
                    "sent": "That inner product between your learned model W and some combined feature function.",
                    "label": 1
                },
                {
                    "sent": "Sigh, sigh.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when learning a model, you're given input output example pairs an you want find some model W which is good.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to your method and methods for learning, these include conditional random fields, Max margin, Markov networks structure, LCMS, and Perceptrons.",
                    "label": 0
                },
                {
                    "sent": "They'll have roughly this formulation, although they don't necessarily phrase it in exactly those terms.",
                    "label": 0
                },
                {
                    "sent": "But in the end, so that incoming, but they're different precisely how they choose their model.",
                    "label": 0
                },
                {
                    "sent": "Given the input sample.",
                    "label": 0
                },
                {
                    "sent": "So in all these methods, when we actually have practical argmax is this argmax distractible typically?",
                    "label": 0
                },
                {
                    "sent": "The these methods are very well understood.",
                    "label": 0
                },
                {
                    "sent": "They have these theoretical properties which with regard to correctness and say maximum likelihood and so forth depending on the method but.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At point, once you have untractable argmax, is these things sort of?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down like for example in image segmentation you the natural formulation for that would be a good structure model, in which case you have to use approximations.",
                    "label": 0
                },
                {
                    "sent": "Then your parameter learning subsequently.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Becomes approximate as well.",
                    "label": 0
                },
                {
                    "sent": "Clustering is another example of a task.",
                    "label": 0
                },
                {
                    "sent": "So where you have.",
                    "label": 0
                },
                {
                    "sent": "Finally, the exact maximizing output would be in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tractable and even some collective classification tasks.",
                    "label": 0
                },
                {
                    "sent": "If the dependency structure you're attempting to exploit as loopy, suddenly that becomes a difficult problem.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And correspondingly, not only is prediction difficult, but parameter inference.",
                    "label": 0
                },
                {
                    "sent": "The parameter training step becomes difficult.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, so let's focus on structural SVM.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer For US fans work through this basic constraints here, which looks kind of complicated but is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually kind of simple, but saying is for every training.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample and farming possible wrong output.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the swimming function for the correct output grade.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Than that for the any incorrect output.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we want them to differ by the last between the correct and incorrect output that's being the margin loss talked about in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we want the end.",
                    "label": 0
                },
                {
                    "sent": "We also have a slack which serves as a bound on empirical risk in the inseparable case.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the structural SVM.",
                    "label": 0
                },
                {
                    "sent": "This is potentially this quadratic program here will have where we have slack as an upper bound on the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Miracle risk and because we have so many constraints whether it be say exponential or factorial or even infinite and some problems, we can't really solve this prob.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly, So what a structural SVM employs is some sort of cutting plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use we start with a completely unconcerned problem.",
                    "label": 0
                },
                {
                    "sent": "For example, in this quadratic program.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we want to minimize this function.",
                    "label": 0
                },
                {
                    "sent": "Wear purple is minimum subject to the circle constraint.",
                    "label": 0
                },
                {
                    "sent": "We want solutions within this.",
                    "label": 0
                },
                {
                    "sent": "Now if we want only linear constraints, we can't model this directly because we need an infinite number of these tangential constraints, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cutting plane would say.",
                    "label": 0
                },
                {
                    "sent": "Start with an unconcerned problem.",
                    "label": 0
                },
                {
                    "sent": "Introduce to constrain true optimize.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the next most violate constraint re optimize.",
                    "label": 0
                },
                {
                    "sent": "Find the next.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define constraint may optimize until such time as the most violated constraint is not violated by more than some time.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once, and that's exactly what it is.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spam structural learner does it starts with notice.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Science repeatedly passes through the example.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fancy output associated with the most fun.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Related constraint and if that constraint is violated by more than a tolerance epsilon, it introduces the constraint and re opt.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sizes and it stops at the point when no constraints are introduced in a pack.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Us now this has several important theoretical properties, one being polynomial time termination.",
                    "label": 1
                },
                {
                    "sent": "It terminates out after a finite number of polynomial number of iterations.",
                    "label": 1
                },
                {
                    "sent": "It's correct insofar as at the point when we stop, we know that the most violated constraint is not violated by more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So no constraint in the full QP is violated by more than epsilon, and because we are respecting our constraints, we maintain the empirical risk found again within this tolerance.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what happens when we're using a proxy?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations so.",
                    "label": 0
                },
                {
                    "sent": "If we're trying to find this most file, a constraint, that is the the output that currently requires the greatest slack for this example.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For using exact inference, you know we can find the most valid constraint, but supposed.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reusing and under generating approximation?",
                    "label": 0
                },
                {
                    "sent": "Well, the trouble with not finding the most violated constraint is that at some point when we terminate, we might terminate.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Too early, for example, to go back to our coming planning sample.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, we might find a constraint.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we find a constraint.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another constraint.",
                    "label": 0
                },
                {
                    "sent": "But now it's.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Close at this point, our approximation tells us that this is the most violated constraint.",
                    "label": 1
                },
                {
                    "sent": "The structural SVM learner will say oh great, I'm done, and it will terminate when of course, there's another obvious constraint which is violated, but it doesn't know about this.",
                    "label": 0
                },
                {
                    "sent": "It hasn't found this constraint, so it thinks it's done, but it's really found a point which is infeasible in the original.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quadratic program so.",
                    "label": 0
                },
                {
                    "sent": "We call these under generating approximations forms of local search that search the constraint space suboptimally, and at first things don't look too promising because under general approximations I just said general rule can be arbitrarily bad.",
                    "label": 0
                },
                {
                    "sent": "Um, but so we.",
                    "label": 0
                },
                {
                    "sent": "In order to.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make any real theoretical statements.",
                    "label": 0
                },
                {
                    "sent": "We restrict our attention to what we call or approximations in our paper.",
                    "label": 0
                },
                {
                    "sent": "So what are approximation is is it doesn't necessarily find the most optimal concern, but I will find an output which has a different function value at least ro times the optimal just remote function.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the paper we have three theorems.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Too much technical detail, but they relate the three major quantities that you sort of encounter in structure.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lesbian learning, for example, in each iteration this slack.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acquired at the end of iteration the your objective.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Option value and also your empirical risk bound.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what these theorems?",
                    "label": 0
                },
                {
                    "sent": "These theorems are all common insofar as they all.",
                    "label": 0
                },
                {
                    "sent": "They are related.",
                    "label": 0
                },
                {
                    "sent": "They all say that the true value for these quantities, that is, the value that would have been found had you done exact inference.",
                    "label": 1
                },
                {
                    "sent": "Is greater than the value which you found with the approximation, but you can sort of bound how far off you are through some interval, which is dependent upon your.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So approximation, and at the point hazro goes to one these intervals shrink to zero sensibly.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those are under generating approximations, but what can we say about a different class of approximation as well?",
                    "label": 0
                },
                {
                    "sent": "Another form of approximation which is often used in this sort of setting is to not search this constraint space.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The local search, but to rather work through some form of relaxation.",
                    "label": 1
                },
                {
                    "sent": "Well, we do not.",
                    "label": 0
                },
                {
                    "sent": "We are able to find the optimal.",
                    "label": 0
                },
                {
                    "sent": "We are now able to find the most violated constraint, but only by virtue of expanding our search space to admit, say, fractional solutions.",
                    "label": 0
                },
                {
                    "sent": "In the case of, say, relaxed LP or something like this.",
                    "label": 0
                },
                {
                    "sent": "So we're able to actually solve the problem, except that we have these additional constraints with these fractional or non continuous.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions so.",
                    "label": 0
                },
                {
                    "sent": "But in this case we sort of have more desirable theoretical results, because these maintain maintain correctness and the empirical risk bound.",
                    "label": 0
                },
                {
                    "sent": "But because we have these additional constraints, the original parameterisation W and the slack might may no longer be feasible in the in this relaxed problem, but we are still searching over the space of constraints.",
                    "label": 0
                },
                {
                    "sent": "Plus these additional constraints.",
                    "label": 0
                },
                {
                    "sent": "So we are respecting all the constraints in the original copy, and then some.",
                    "label": 0
                },
                {
                    "sent": "So it's not clear.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, which one of these is better?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we also have an empirical analysis where we tried various types of approximate.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans.",
                    "label": 0
                },
                {
                    "sent": "Now we used.",
                    "label": 0
                },
                {
                    "sent": "As an application binary pairwise Markov.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fields smart referendum field with binary node values in order to make it intractable because we sort of launch intractability to evaluate this, we made it completely connected.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now our target application is multi label classification.",
                    "label": 0
                },
                {
                    "sent": "What this is is given an input X we try to find for that input a set of relevant labels.",
                    "label": 0
                },
                {
                    "sent": "For example in new story clusterings you might have say.",
                    "label": 0
                },
                {
                    "sent": "This story is about oil.",
                    "label": 0
                },
                {
                    "sent": "This story is about the army.",
                    "label": 0
                },
                {
                    "sent": "The story is about whatever.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And what we do by modeling this as a Markov random field is we have the nodes correspond to the labels and if it has one, that label is on an.",
                    "label": 1
                },
                {
                    "sent": "If it's zero, that label is does not apply to this example, the node potentials would would indicate an inputs, Tendency, tab label and the edge potentials.",
                    "label": 0
                },
                {
                    "sent": "We move them as sort of two labels, tendency to Co occur parameter which is learned through the training data.",
                    "label": 0
                },
                {
                    "sent": "Saying OK, it's pretty likely that these would will Co occur, or if it's negative.",
                    "label": 0
                },
                {
                    "sent": "It's unlikely that these will ever Co occur or something like this, and within our model how we specifically modeled it as them in our model, we basically have a separate hyperplane which we learn for every single node potential, and we have a single value for each pair of labels.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple formulation and our loss simply is the.",
                    "label": 0
                },
                {
                    "sent": "The percentage of different labels so 100% less would be all of our labels are wrong and.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you, thank you Apple.",
                    "label": 0
                },
                {
                    "sent": "How do I get OK?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so and lots of zero would be perfect labeling.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm back up in 10 days, so alright, so what's important to see here is that we have the prediction problem, which is just regular map inference, but also our constraint inference problem is also an example of map inference, just with modified node potentials so and so we can use the same inference techniques.",
                    "label": 0
                },
                {
                    "sent": "The same map inference techniques both for prediction an for constraint inference.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have six datasets which we used Sing East Media million.",
                    "label": 0
                },
                {
                    "sent": "Reuters well known real datasets.",
                    "label": 0
                },
                {
                    "sent": "And we also have two synthetic datasets, one of which doesn't really require edge potentials to learn the target function in one which does.",
                    "label": 0
                },
                {
                    "sent": "We also restricted waters and made him out to the 10 most frequent labels for reasons which will become.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There in a moment and for our inference methods for under generating approximations, we use some greedy assignment approach and we also use loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Annizah third under general approximation we have some sort of in sambol method of the two, and it returns the best one with the highest discriminate function at the highest likelihood of those.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two and four over generating approximations.",
                    "label": 0
                },
                {
                    "sent": "We had two, one based on linear programming, relaxation of map inference, another on cuts, and both are really equivalent.",
                    "label": 0
                },
                {
                    "sent": "But as we found out, but cuts is much faster than our general linear programming solver.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have the third algorithm class which we use for comparison in order to give our experiment some meaning which is exact exhaustive inference, which is why we constrained our label space to just the 10 most frequent labels for orders and media Mail, which by themselves have 100 labels.",
                    "label": 0
                },
                {
                    "sent": "So by doing this we can sort of keys out which what is the cause of some of the effects that we see on our analysis, so to say.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But with the major results, let's let's first take a look at loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Now here we see the losses on the six datasets with the various constraint inference.",
                    "label": 0
                },
                {
                    "sent": "An predictive inference methods and what we see.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that most people, like propagation, doesn't really perform that well.",
                    "label": 0
                },
                {
                    "sent": "In fact, it performs astonishingly poorly and rather poorly.",
                    "label": 0
                },
                {
                    "sent": "On five out of the six datasets and.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we can say is that it's bad as a training method, so the food predict with exact but trained with loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "We gets these really, really.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bad models and it's also bad as a predict.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And method so these cases when we are training with an exact model, even when it's just uses a productive method.",
                    "label": 0
                },
                {
                    "sent": "It also tends to stumble a lot.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, while we believe this is happening is because loopy belief propagation has this tendency to work, but at the point when it stops working, it really stops working.",
                    "label": 0
                },
                {
                    "sent": "So if we run a bunch of random experiments and count how many superior labelings there were on a random Markov random field, we see that although loopy belief propagation gets the right answer more often than greedy, it also has a tendency to have this really horrible performance on some of the.",
                    "label": 0
                },
                {
                    "sent": "Exactly some of the random examples which greedy doesn't really do it.",
                    "label": 0
                },
                {
                    "sent": "It performs greedy, perform sort of consistently less consistently, it's consistently.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mediocre.",
                    "label": 0
                },
                {
                    "sent": "So let's now focus on our relaxation are over generating approaches.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're having results for media Mail which I just selected as representative.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you're not.",
                    "label": 0
                },
                {
                    "sent": "That on these methods, which are is the model which was trained with the linear programming relaxation and tested with the various 5 inference methods.",
                    "label": 0
                },
                {
                    "sent": "We see that.",
                    "label": 0
                },
                {
                    "sent": "Linear programming training.",
                    "label": 0
                },
                {
                    "sent": "The relaxed training results in the model, which isn't bad.",
                    "label": 0
                },
                {
                    "sent": "In fact, on this data set, it's the best and is has very consistent.",
                    "label": 0
                },
                {
                    "sent": "Performance was say, an exact training even, which is what we claimed we wanted to solve.",
                    "label": 0
                },
                {
                    "sent": "We have these sort of very variance and combine and Brady are also pretty high variance with people.",
                    "label": 0
                },
                {
                    "sent": "Propagation is consistent but only by virtue of being consistently bad.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You also see though.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, sorry.",
                    "label": 0
                },
                {
                    "sent": "Now we have to say though, that when we were using relaxed prediction.",
                    "label": 0
                },
                {
                    "sent": "Relax, production has a tendency to have very poor performance as a predictive method.",
                    "label": 1
                },
                {
                    "sent": "So what's happening here is that the presence of fractional constraints when we are training using linear programming relaxations has this tendency to sort of smooth the search space so that it's actually easy to find the correct answer.",
                    "label": 0
                },
                {
                    "sent": "And if we're not actually accounting for the weaknesses of linear programming inference, we're web tendency to have prediction, which is, say, fractional.",
                    "label": 0
                },
                {
                    "sent": "Say it'll say the labels half on and half.",
                    "label": 0
                },
                {
                    "sent": "Off in the relaxation.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also deal with known approximation, so we had this.",
                    "label": 0
                },
                {
                    "sent": "These theories regarding row approximations, but empirically, how well do these really perform too?",
                    "label": 0
                },
                {
                    "sent": "Answer This phone built sort of artificial row approximate inference methods based on exact, which was then deliberately dumbed down to return are approximate solution.",
                    "label": 0
                },
                {
                    "sent": "So we trained these models with this artificial low approximate.",
                    "label": 0
                },
                {
                    "sent": "Artificial approximate inference technique and tested using exact inference, so this is.",
                    "label": 0
                },
                {
                    "sent": "So this is generally getting worse and worse approximation.",
                    "label": 0
                },
                {
                    "sent": "It's sort of encouraging insofar as it's not throwing catastrophic Lee, but on the other hand, we do see a fair amount of variation on some of these datasets, so it's not like it's consistently doing worse or consistently doing better.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize.",
                    "label": 0
                },
                {
                    "sent": "We repeated structural SPMS and sort of explained the consequences of using an exact inference and theoretically and empirically evaluated both under generating.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference techniques that is runs based on some form of local search and over generating approximation techniques.",
                    "label": 0
                },
                {
                    "sent": "Those involving some sort of relaxation.",
                    "label": 0
                },
                {
                    "sent": "And using multi label classification with completely connected binary Markov random fields as a target application, we saw about over general methods sort of distinguish themselves about theoretically and in learning more robust MoD.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And closed, I have some software which is available for you that I used in this work which you can download if you want and.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's are there any questions?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, can you repeat the question, yeah?",
                    "label": 0
                },
                {
                    "sent": "So let me know when you have a room together.",
                    "label": 0
                },
                {
                    "sent": "Open communication.",
                    "label": 0
                },
                {
                    "sent": "So OK, so in my analysis the do you mean the empirical analysis?",
                    "label": 0
                },
                {
                    "sent": "So you're asking me in my theoretical analysis of my always using a row approximation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well the.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, in the microphone and fields we had a row approximation.",
                    "label": 0
                },
                {
                    "sent": "You're asking me if we used are approximation in the empirical results I believe.",
                    "label": 0
                },
                {
                    "sent": "OK, so the answer to that is yes, and so far as we had that known approximation climb down now, loopy belief propagation for example, is not about approximate method.",
                    "label": 0
                },
                {
                    "sent": "It can be arbitrarily bad and you can sort of construct scenarios where it performs extremely badly.",
                    "label": 0
                },
                {
                    "sent": "So sometimes we are and sometimes we aren't does that.",
                    "label": 0
                },
                {
                    "sent": "Well, greedy I.",
                    "label": 0
                },
                {
                    "sent": "Let's see I think greedy is.",
                    "label": 0
                },
                {
                    "sent": "I don't know about greedy, but the theoretical results are actually applicable to not just Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "The theoretical results are more general, so we just use this to evaluate OK, supposing that we are doing.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference.",
                    "label": 0
                },
                {
                    "sent": "Not with.",
                    "label": 0
                },
                {
                    "sent": "What's the weather like in three o'clock?",
                    "label": 0
                },
                {
                    "sent": "That if we were using approximate inference algorithm at word, we chose roll approximation algorithms as something which applies.",
                    "label": 0
                },
                {
                    "sent": "Well, let's say that people are population.",
                    "label": 0
                },
                {
                    "sent": "Let's see relaxations.",
                    "label": 0
                },
                {
                    "sent": "If you actually discretize them in one way, it becomes a .5 bro approximations.",
                    "label": 0
                },
                {
                    "sent": "But else, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "So a quick.",
                    "label": 0
                },
                {
                    "sent": "The quicker comment in the question, I think the reason will be repeated so bad in your experiments is that in some sense they are the worst possible setting for BP where you have a lot of short loops, so PvP as well when you have long loops so you know in these experiments, that's not no surprise that it was so terrible.",
                    "label": 0
                },
                {
                    "sent": "So my question was, there's something that I missed, which is how do you determine the row in general?",
                    "label": 0
                },
                {
                    "sent": "Well, the role would be so the question is how do you determine the role of your inference technique now in general?",
                    "label": 0
                },
                {
                    "sent": "If you're actually making one of these structured predictors, and you are using a row approximation algorithm, you would know it because you're you're building the model and your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you say, oh OK, I'm using this sort of greedy solution to do set cover, which has a 1 / 1 over natural E approximation.",
                    "label": 0
                },
                {
                    "sent": "So you would actually know that a priority.",
                    "label": 0
                },
                {
                    "sent": "So you need to use a method for which you know that which is not all methods.",
                    "label": 0
                },
                {
                    "sent": "That's true, and that goes back to that other question.",
                    "label": 0
                },
                {
                    "sent": "Which is that in not every approximation is a row approximation.",
                    "label": 0
                },
                {
                    "sent": "We chose them as sort of a representative set which was common and relatively easy to analyze.",
                    "label": 0
                },
                {
                    "sent": "I just like to have sex occasionally.",
                    "label": 0
                },
                {
                    "sent": "Groupies relaxation.",
                    "label": 0
                },
                {
                    "sent": "Suppose you get a fractional solutions that how do you fix the concentrated sizes?",
                    "label": 0
                },
                {
                    "sent": "So the question is when we have these relaxed predictors and we do come up with the fractional most violent concern, how we how do we incorporate it OK?",
                    "label": 0
                },
                {
                    "sent": "Well, when we do dates relax predictors.",
                    "label": 0
                },
                {
                    "sent": "We simply expand our space of possible constraints to include these these relaxed constraints, because else we'd have to discretize them.",
                    "label": 0
                },
                {
                    "sent": "And if you're discretizing them then we go back to using an under generating approximation.",
                    "label": 0
                },
                {
                    "sent": "So we expand our constraint space to include the fractional fractional solutions.",
                    "label": 0
                },
                {
                    "sent": "Include Contacts, combinations of constraints has allowed him straight in the Corporation.",
                    "label": 0
                },
                {
                    "sent": "If you were actually learning with the last constraints, that would be what you would do.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Hello, thank you.",
                    "label": 0
                }
            ]
        }
    }
}