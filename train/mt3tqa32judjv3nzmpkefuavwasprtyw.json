{
    "id": "mt3tqa32judjv3nzmpkefuavwasprtyw",
    "title": "On Finding Low Error Clusterings",
    "info": {
        "author": [
            "Maria-Florina Balcan, College of Computing, Georgia Institute of Technology"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss09us_balcan_flec/",
    "segmentation": [
        [
            "So it's about finding lower clusterings and this is joint work with a bunch of people that you describe.",
            "So just to clarify, I'm from Microsoft Cambridge, New England, so I'm not from England and next I'll go to Georgia Tech.",
            "Alright, so I'm going to talk about finding lower clusterings and in fact an even better title for this work."
        ],
        [
            "Approximate clustering without the proclamation and the meaning of this title will become clear later in the book.",
            "And."
        ],
        [
            "Let me start by mentioning that problems of clustering data come up everywhere.",
            "Few examples are the following.",
            "Clustering news articles or web pages by topic clustering protein sequences by function or clustering images by who is in BAM.",
            "Now, many of these problems can be formally specified as follows, so we assume that we are given a set."
        ],
        [
            "South of an object, say and document, and we assume that there exists some unknown, desired correct ground truth clustering.",
            "But means each object has some unknown true label, say its topic, and the goal is to produce of clustering a good clustering or clustering of low error where the error of a given clustering C prime is just the fraction of the points that are misclassified respect.",
            "To see after indexing of the clusters.",
            "So again, the setting here is the following.",
            "We have a target clustering C, which is a partition of the whole set of points.",
            "So the target clustering is the partition C1C1C2C K, and given a new clustering C primer, new partition of the whole set of points.",
            "So given a new partition C, One prime seta prime CK prime, when the error rate of the clustering C prime with respect to the target clustering C is just the fraction of the points that you get wrong in the optimal matching between the classes in C. And glasses in C prime.",
            "And this is very natural, and in fact it is the analogue of the O, and loss is now in the context of clustering of the 01 loss in the context of supervised classification.",
            "And this is the case since here in the context of clustering we do not care about getting the names of the points right.",
            "We don't care about getting the labels of the points right, only care about getting the points themselves right.",
            "Alright, so again, our goal is to come up with a clustering of low error or with a good clustering, and in order to do so we are."
        ],
        [
            "So given a measure of similarity or dissimilarity, so are also given a pairwise measure between these operas.",
            "Measure between pairs of.",
            "We are given authorize, measure between pairs of objects.",
            "For example, in the document clustering case, the.",
            "The similarity dissimilarity measure that we are given can be something based on the number of keywords in common in the sequence.",
            "Protein clustering case, the.",
            "The similarity similarity measure given can be something based on the added distance and so on, and in fact for the rest of the talk, I will assume that we're actually given a dissimilarity measure which satisfies the triangle inequality.",
            "So we're giving a distance function.",
            "And again, our goal is to come up with a clustering of low error and clearly given this gold dissimilarity measure, the distance functions were given has to somehow be related to the target clustering as to somehow be related to the topic Ness, because otherwise there would be no hope.",
            "Yes.",
            "Oh, it's just the permutation.",
            "So this case, the set of all permutations from one from the set one 2K to the set one 2K.",
            "Yes.",
            "It's not the only way, but if this is a very natural definition of distance between two clusterings, an error in the context of clustering, and as I was saying is the analog.",
            "The O and loss in the context of supervised learning.",
            "It's just a natural measure.",
            "Of course, one could consider other natural measures as well, but it's certainly natural.",
            "And it captures in some sense we try to do to recover the underlying clustering.",
            "So capture that we do not care about getting the labels right the the label of the points right only get get about getting the clusters right?",
            "But I should be set up with a problem Now a standard approach."
        ],
        [
            "Gotcha very classic approach in the theoretical computer science community and also actually machine learning community is to solve the problem as following user data points as nodes in a weighted graph where the weights are based on the dissimilarity measure.",
            "But we are given and then what people do.",
            "They pick some objective function to optimize.",
            "Like comedian K means mean some and so on.",
            "And just remind you or if you've never seen this in the comedian clustering case, the goal is to find.",
            "A partition of the whole data set a partition C, one prime cita prime CK, prime and medians or Centers for those.",
            "We want to find median or Center for each set CI prime in order to minimize the sum over all points of the distance to their corresponding median.",
            "So this is a comedian objective function and K means.",
            "Single function, the goal is similar.",
            "We just want to minimize the sum of the square distances.",
            "And finally, in the mean some class in case the goal is to find the partition or the whole data set that minimizes some of the intracluster the similarities or distances.",
            "Right, so again, a classic."
        ],
        [
            "Approaching the theoretical contestants committee also in machine learning, is to view the data point is now in a weighted graph where the weights are based on the similarity measurement that we are given and when to pick some objective function to optimize and then to develop approximation algorithms to optimize this objective.",
            "Again, many of these objectives are in Bihar, so it's NP hard for many of these objectives to find an optimal solution.",
            "So then the best that we can hope for is an approximation.",
            "So the optimal solution.",
            "So for instance, for the K median clustering objective, the best known approximation is A3 plus epsilon approximation, and it is also known that is NP hard to find 1 + 2 over approximations of the optimal comedian solution.",
            "So there has been a lot of work here, and more generally there has been a lot a lot of work in the approximation algorithms community on designing approximation algorithms for this clustering objectives.",
            "So this is all fine.",
            "However, remember that in many other problems that I described, our real goal is to get the points right what we wanted to do is to recover the correct desired ground truth clustering and so that means that if we end up using the C approximation."
        ],
        [
            "Who is an object?",
            "If I say to comedian?",
            "In a setting where what we really want to do is to get the points right, then that means that we make an implicit assumption, but all the clusterings that are within a factor of C of the optimal solution for Phi are in fact epsilon close to our target clustering.",
            "So again, if we ever end up using a C approx."
        ],
        [
            "Motion algorithms data comedian.",
            "In order to cluster documents or really want to do is to get the the target clustering right.",
            "That means that we make an implicit assumption.",
            "But any clustering whose objective comedian objective values within a factor of C of the optimal solution for optimal Keegan solution.",
            "So any clustering who's even a factor of three of the optimal capital solution must in fact be epsilon close to our target clustering in terms of the symmetric difference distance that I described earlier.",
            "And for the rest of the talk, I'm going to call with the C Upsilon property, so I'll save the data, satisfies the C epsilon property if any clustering within a factor of the of the optimal solution for objective Phi is in fact epsilon close to the target.",
            "Clustering at enclosing terms of the semantic different systems.",
            "Right now it turns out that the problem of finding AC approximation algorithm to objectify under the sea epsilon property is as hard as it is in the general case.",
            "Now it is showing our our work.",
            "However, is that under the C Epsilon property we are able to cluster well that means are able to get a clustering that is close to our target clustering in terms of the semantic difference distance and we are able to do so without approximating the objective at all.",
            "That's why I'm saying, but basically the main point of the work and that I was saying at the beginning of my talk with a good title for this work is approximate clustering without the approximation.",
            "So we saw, but really want to solve get close to the target clustering, but without approximating the proxy objective.",
            "For instance, the comedian objective at all.",
            "So."
        ],
        [
            "And to be more concrete, for instance, for the comedian object, if we can show that for any constant C stick constantly stick together, then one, then under the sea epsilon property we can cluster.",
            "Well, we can.",
            "We can find the clustering that is order of epsilon closed, desired current of clustering and we can do so even for values of C. We're getting a C approximation to the comedian.",
            "Objective is probably NP hard, and Moreover we can even get.",
            "Exactly close epsilon close to the target.",
            "Clustering in the case when all the clusters are sufficiently large.",
            "So basically we are doing as well as if we could approximate the objective to an NP hard value, which I think is very nice.",
            "Alright, and before actually going into more details and describing proofs, let me make one more.",
            "Let me make a comment so from an."
        ],
        [
            "Optimization algorithms, perspective, unnatural, and absolutely legitimate motivation for improving the approximation factor from C1 to C2 or C. Twist equivalency.",
            "One is that maybe the data, but we have satisfies the C epsilon property for the value of C2 but not for C1.",
            "And this is actually legitimate.",
            "Motivation is absolutely legitimate, is natural and in fact we can show that for any constant C2 smaller than C-11 can construct the data set.",
            "And the target clustering, but satisfies this T2 epsilon property, but not even the C10 point 49 property.",
            "So it's absolutely natural to try to improve the approximation ratio from C1 to C2B, cause maybe the letter satisfies this condition.",
            "Possible, but not for C1, but the point is that in our work we can do even better because we are able to cluster well even for values of C. We're getting a C approximation to the object if Sado comedian is probably hot.",
            "Alright, and actually, before describing the proofs."
        ],
        [
            "Me quickly summarize them.",
            "Any result some of the main reasons that we have in this model first of all, for the comedian clustering problem, we can show that if the data satisfies the C epsilon property, then we can get order of epsilon over C -- 1 close to the target and Moreover if the data satisfies this epsilon property and if the target clusters are large, then we can even get epsilon close to the target, yes.",
            "All polynomial, so you are you asking about.",
            "We didn't optimize anytime, so it's like N ^2 say or yeah this is not a major issue for us.",
            "We just we're helping polynomial time algorithms but.",
            "Difficulty minus one.",
            "What is the dependence known?",
            "Running time is not depend, but it's not dependent just in the approximation final approximation guarantee.",
            "So as I was pulling sincere as I was saying, if he satisfies the epsilon property when we get order of epsilon versus minus one in the absolute true general case when you have even very very small target clusters.",
            "However, if the target classes are sufficiently large, we can even get epsilon close to the target clustering, so the dependency to answer your question comes in the approximation guarantee.",
            "In the case of lot of small clusters.",
            "K please ask questions.",
            "Actually let me find it.",
            "Alright, so these are results for comedian.",
            "We can get something very similar for K means and this actually our visa resource that appeared in joint work with Avrim, Blum and an open Gupta and so that was nine and."
        ],
        [
            "For the mean, some clustering objective which is actually a more delicate object.",
            "If we get the following type of results, we can show that in the data satisfies the C epsilon property and if the target classes are sufficiently large, when we can again get order of Epson oversee minus one close to the target.",
            "However, is a target clusters are.",
            "Allowed to be arbitrarily small.",
            "Then in the case where we don't have too many clusters.",
            "In the case where the number of target clusters is more of an log an over log log and then he can actually get order of epsilon oversee minus one.",
            "Close the target in the most general case, the best we know so far to do is to output a small list of clustering.",
            "Small meaning list of size log, log in with the property at the target.",
            "Clustering is close to one of the clusterings in the list.",
            "Type services are quick overview of this app.",
            "A quick summary of our results, and now I'm going to go in and talk in detail about the."
        ],
        [
            "See Epsilon property for the comedian clustering objective.",
            "Alright, so.",
            "From now on I'm going to focus on the comedian clustering objective and that also mentioned a little bit about the other objectives as well.",
            "OK, so let's assume that."
        ],
        [
            "Uh, we our data set satisfies the property, but any C approximation to the comedian.",
            "Optimal solution is in fact epsilon close to our target clustering and for simplicity just for the presentation.",
            "Let's assume that the target clustering is the optimal comedian solution.",
            "And also let's assume that all the classes are large enough, not large enough, but not too small.",
            "We have size at least two epsilon times then, and this is just avoid some technicality we don't need this restrictions.",
            "This is just for the talk now for any point X, let us denote by WX the distance from X to its corresponding center in the comedian optimal solution, which by assumption by that I make in the talk is also the target clustering and let W2 of XB the distance from Max to its second causes centering the comedian optimal solution.",
            "LAW average be the average over X of W bags.",
            "So just by definition, we have at OPT, which is a value of the optimal comedian.",
            "Solution is an times double average.",
            "Now.",
            "This is just a notation now to immediate things that we can say are the following so we can show the steps alone.",
            "Property implies to fax.",
            "So the first one is that at most epsilon endpoints can have W2 smaller than C -- 1 W average over epsilon.",
            "Why?",
            "Because otherwise we could move epsilon endpoints to wear second closest cluster, and we could do so without increasing the objective by more than C -- 1 W average over epsilon.",
            "All times epsilon, which is C -- 1 W average times then, which is similar sometimes up.",
            "So we get the clustering that is still a C approximation to the optimal comedian clustering and which is epsilon part from the target clustering and so contacting the epsilon property that we started with.",
            "So using the steps and property using the definition of the steps and property, we can prove that at most epsilon points can have W2 smaller than C -- 1 W average over epsilon.",
            "So this is the first fact and the second fact that we can prove which follows much more trivially from the definition is that at most 5 epsilon, an oversea minus one points can have W greater than C -- 1 W average over 5 epsilon, and we just focus on the definition of W average and using Markov inequality.",
            "Alright, now for the rest of the points for what we call the good points we have a big gap.",
            "So for the rest of the points we can we have it.",
            "The distance to Brown Center is at most C -- 1 W average over 5 epsilon and with their distance to their second closest center is at least C minus on W average over epsilon.",
            "So for the rest of the points where the good points W is smaller than this quantity right here, and W2 is at least 5 * 5 times with quantity.",
            "So now if we do not this quantity right here, by the criticals, if we do not get quantity by the critical, what we?"
        ],
        [
            "That is that.",
            "Most of the points, all the good points, one answered of epsilon fraction of the points will look like this.",
            "So there will be in distance D critical.",
            "So this points like say point X is within distance be critical of its own center, and then that means that by triangle inequality the distance between any two good points in the same cluster.",
            "Is it is upper bounded by two times the critical and now since you know that any good point is at least distance at least five times the critical to their second causal center, we also know that any two good points in different clusters are a distance at least four times the critical from each other, and this follows band by triangle inequality.",
            "Because remember, I assume without distance satisfies triangle inequality.",
            "Right and so."
        ],
        [
            "It means that basically now the word loose look.",
            "So now that means that if we now create a graph G where we connect any two points X&YE varying distance two times the critical of each other, then that means that the good points in the same cluster we're going to form a clique since the distance between two points in the same classes and multiple times the critical and we also have that no good points in different clusters can even have any neighboring comment, we cannot have a neighbor in common.",
            "Because the distance between two points in different clusters is at least four times the critical, so I cannot have a neighboring, in the graph G. So that means yes.",
            "The good points I'm talking about the good points, so at this point I want."
        ],
        [
            "Talk about the good points.",
            "Alright, so that's actually a perfectly right.",
            "So the the word actually looks like this.",
            "So most of the points are good points and these good points from clicks and this good points can be connected to bad points.",
            "So the big points are pictured here on the bottom, but not too many of em and bad points can connect to other about points, but they can Only Connect one of the good set because we know that good points in different clusters have no neighbor in common.",
            "Yes.",
            "Statement or.",
            "No, no, it's not terministic, and if I don't even have to use Markov inequality, I was just saying OK, so to go back, it's all deterministic."
        ],
        [
            "And this fact right here even follows Justin definition of W average.",
            "I was just using a fancy explanation mark of inequality, but it follows.",
            "Awesome definition right?",
            "And everything is the terministic."
        ],
        [
            "What's all going back here?",
            "So we have the world?",
            "Looks I'd be so most of the points.",
            "The good points from clicks now this good points are connected, can be connected to bad points.",
            "However bad points about points connect all about points.",
            "However they can only touch on with the good sets.",
            "So that means that if we."
        ],
        [
            "Furthermore, assume that the classes are large.",
            "In particular, if we assume that the classes have size at least 2 * B + 1, or B is a number of points, then what we can do is just create a graph H where it connects to two points X&YE ve share at least being able comments in the graph G and so that means that now the graph age will split like this, and so that means that we can just help with the largest components in age and that we get which is a clustering of error roughly.",
            "The fraction of which is a classic one, so the number of mistakes that we make is order of the size of the bed set.",
            "Now."
        ],
        [
            "Even target class."
        ],
        [
            "US Auto Lodge then it turns out you can still get order of epsilon versus minus one, but we need to see to be a bit more careful, and in particular what can happen now is that some of the clusters can be totally dominated by the bad points.",
            "However, it turns out that we can still.",
            "Glasser well, we can still get a clustering of low error by using a greedy approach.",
            "And in particular, the algorithm that we that works in this case is still quite sick."
        ],
        [
            "People, we just need to be a slightly more carefully analysis.",
            "Cancel the algorithm is as follows.",
            "Just as before, we create the graph G. We connected it to points X&Y.",
            "If ever been distance two times, the critical of each other, and then what we do, we pick the vertex of the highest degree in the graph G and remove with vertex and its entire neighborhood, and we've ended costs.",
            "And this way we output the clustering with K clusters.",
            "Anile the main idea is that we can now charge off all the errors to the bad points.",
            "And.",
            "In particular.",
            "Eve the vertex."
        ],
        [
            "One of the if the vertex Vijay, but we picked was a good point like this one here.",
            "When we are happy because it means that we pull in, we entirely pull, pull, pull out a good set.",
            "So this is a good case.",
            "I'll put a good cluster.",
            "On the other hand, if the vertex."
        ],
        [
            "That we picked was a bad point.",
            "We might end up pulling out only parts of only a part of a good set.",
            "So we might be missing some of the points in particular might be missing, say are times I PowerPoints on plaster CI.",
            "Another point is that we are greedy, and since the vertex we had the highest degree, that means that the size of the bed sets that we pick is larger than the size of the site that we need.",
            "And so that means that we can now use this tag to essentially charge of all the errors to the bed set, and so that means that we get the clustering whose error rate is proportional to the fraction of the bad points.",
            "So I get the clustering the number of mistakes that we make is order of the size of the bed set, which is order of epsilon times, then over to minus one.",
            "Hi so can also deal with small clusters, which is nice now."
        ],
        [
            "Going back to the large clusters case, it turns out that we can actually get even better.",
            "We can even get epsilon close to the target clustering in the case or the target classes are large, and the notion of laughing.",
            "It's actually depends on this constancy.",
            "And now the main idea here is that we actually have two kinds of bad points.",
            "We have some at most epsilon points, which I'm going to call confused points, since where this since the gap between the distance to their own sentence, the distance to their second class center is not too large.",
            "So for this point for the confused so this point, the confused points have a small gap between the distance to their own center in the distance to the second closest center.",
            "Now the rest of the bad points are not confused.",
            "They have a large gap between the distance to Vero sent and the distance to the second class center.",
            "However, for this non confused points for this bad but not confuse points, W itself is large, and now it turns out actually can recover this non confused points.",
            "And which will give us a clustering of error at most epsilon.",
            "Now the main idea here is that this non conference points have a large gap.",
            "As I was saying is not confused point have a large gap between their distance, the distance to run center distance to their second class center, in particular that gap is at least five times the critical.",
            "Now we don't really know which we only know the center, so we don't really know the centers.",
            "However, we now with any good point is.",
            "Distance of mostly critical of its own center and so.",
            "Then we expect imply but basically this non confused point will be.",
            "Will be closer to good points in wrong cluster.",
            "Went to good points in any other cluster.",
            "And Furthermore"
        ],
        [
            "And Moreover, now if the clusters in moral of the classes are large enough, then this class will actually be dominated by good points, and so that means that in the case where the class target classes are large enough that we can do is now the following given.",
            "So given the output C prime from the algorithm so far.",
            "So given the clustering see private, we have so far.",
            "What we do is just reclassify each point X into the clusters of laws median distance.",
            "And and basically, the point is that if the classes are large enough when the median will be controlled by a good point, and as I was saying, always non confused point are closer to good point in round cluster and good points in any other cluster and so that means that the non confused point will be pulled out in that direction.",
            "It's already covered them, so we get the clustering overall at most epsilon.",
            "Now actually one other technicality with."
        ],
        [
            "I want to briefly mention, without going too much into detail, so actually one thing is that so far I highly defect, which is that the definition of the graph G. But I also know the algorithms depend depends on the distance D kritikal, which in turn depends on the definition or depends on W average.",
            "Which is optimal, and so it depends on some unknown quantity.",
            "OK now, but the multiple ways around with in the case where the target classes are large, we have one solution.",
            "So the solution here is to start with the logs for W. So in particular started at the first case for double can be the smaller distance between two points.",
            "Start with a W and let's keep increasing W until the first time when we get with the.",
            "K largest components of the graph H cover a significant fraction of the space, in particular cover one month order of B point fraction of the points, and Moreover, all these components are large in size.",
            "And now the point is that if we impose it, all these components are large in size, which means that we never misclassify good point.",
            "And this can be kind of used to argue, but in this case this is a good way to solve the problem.",
            "So it requires some argument, but everything goes through, so this is just the main idea.",
            "So this is 1 solution.",
            "The large clusters case, and it actually is quite nice because we don't need to run an approximation algorithm now in the in the non lab."
        ],
        [
            "Plus this case in the in the case where we allow small clusters, what we have to do is visibly unfortunate, but we thought we do well.",
            "We have to actually run the first time an approximation algorithm for the object policy for comedian in order to get the constant factor approximation to the optimal solution.",
            "And so we can then use that constant factor approximation and then this approximation factor just goes into the approximation in the final approximation guarantee for our clustering just goes into the error term.",
            "Alright, so we can also handle not knowing W average.",
            "Oh, and so the kritikal now."
        ],
        [
            "Let me mention also one other extensions that we can see that so we also analyze this problem in an inductive case where the set of the point of the set of points as what we see is just a small sample from a much larger abstract instance space, and where our goal is to come up with a clustering for the whole space.",
            "So algorithmically what we do in this case, we first draw the sample S. We cluster the sample and then we send you points into the clusters.",
            "As I arrive online.",
            "Currently, in particular, in the case where the target classes are large enough, then what we do we run the algorithm that I was describing over the sample.",
            "So that means we construct the graph G. We can start the graph H, take the K largest components of H. So this is the casting of the sample and then when a new point arrives, we just insert it in one of these clusters based on the median distance to this cluster that are produced over the sample.",
            "So we were doing the inductive case in the last class, this case and the main idea of why we circuit works is that if you think about the key property, but we used to argue correctness for the algorithm in the large classes case, is that in each cluster we have more good points and bad points.",
            "This isn't the case or UW in the case.",
            "We didn't know that we needed that we have twice more good points and bad points in anyway.",
            "Either way, the point is that if the target classes are large enough and if the sample S is large enough when this was going to happen over the sample as well.",
            "So we then can be used to argue correctness of this algorithm in the inductive case.",
            "Hi so I can also deal with inductive case and the right.",
            "So this is we summarizes all our results for the comedian Classlink objective.",
            "Now we get similar results also for the chemistry."
        ],
        [
            "String objective, so we get all the argument pretty much goes through all the arguments.",
            "Pretty much go through the only thing that breaks is a is getting epsilon exactly epsilon.",
            "Close the target clustering.",
            "In the case of the target classes are large.",
            "Probably that can be solved actually now for the mean some clustering objective.",
            "We can still cluster well, but this requires a much more involved argument, and in fact the original solution that we have in the Sedona 2009 paper, we have implement an open Gupta was the following.",
            "We first connected them in some objective to something called balanced comedian, which is very similar to the Cayman objective, just except now we basically multiply the distance from Lexington Center of the size of the cluster in which the point is in.",
            "So this is the standard objective, but is used in.",
            "Your community at so to solve them in some what we do.",
            "First connect them in some objected to the balance came in objective and then we can kind of adapt the algorithm that we had for the comedian objective to the balance comedian objective.",
            "However, the things are slightly more complicated because in this case we don't really have a uniformly criticals.",
            "The definition of the graph G is not so obvious because in this case you could actually have clusters with lots of points, but with small distances and clusters with very few points, but with large distances.",
            "So the situation is slightly more complicated.",
            "However, it turns out that we in you can still use this approach to solve the problem for the case on C, stick together and two and the target classes are large.",
            "So we can still get some results even by this approach, and in fact actually this results are quite impressive even sells, because actually the best known approximation algorithm to them in some objective in the general case is a Poly log is a logarithmic approximation, the polylogarithmic approximation, so we can do much better.",
            "We can actually even cluster well in the case when C is like 3, for instance.",
            "Cancel this is actually the original solution in this other paper.",
            "Now we kind of completely solve the problem in the new upcoming called paper, But the arguments are significantly more involved.",
            "So don't go for the balance comedian, but the analysis is much more involved.",
            "Alright.",
            "And now to actually briefly summarize with epsilon properties, this part of the talk."
        ],
        [
            "Let me mention that actually won Kim user visual approximation algorithms approach to clustering is saying well, we cannot really measure what we want, which is closeness to the correct ground of clustering.",
            "So what we do is set up on a proxy objective function like K, median, mean sometimes and so on and the approximate that.",
            "Right, so this is a little bit like what I have in this picture right here where these guys about to jump off the building and we tell him we couldn't get a psychiatrist, but perhaps you'd like to talk about your skin.",
            "So Doctor Perry here is a terminologist.",
            "So we cannot really solve the problem.",
            "We have to solve.",
            "We set up proxy and hope that the proxy somehow, you know.",
            "Helpful well, so actually I guess I think that maybe the truth is not really like this because it's actually.",
            "If we do end up using a C approximation algorithm, Sato comedian or came into Class A documents that means that would really make an implicit assumption about how the distance is.",
            "How does this not information that we are given relates to the underlying underlying desire count of clustering?",
            "So make an assumption about how the distance is related to the closeness to the truth.",
            "And now the trick is that in our work we make this assumption explicit.",
            "And if we make this explicit, then we can get around inapproximability results by using the structure implied by this assumption that we were making implicitly anyway.",
            "So we need to approximate clustering by the approximation.",
            "And actually, right and.",
            "So this is kind of the main story around this epsilon property.",
            "Let me mention that more recently."
        ],
        [
            "I've also considered extensions of his properties, and in particular we have considered an extension of this property which captures clustering in the presence of outliers or misbehaved points.",
            "So in particular, natural assumption is that an actual belief might be weather data satisfies the epsilon property, not about the whole set of points, but maybe the data satisfied the epsilon property after a small fraction of the outliers or behaves points have been removed.",
            "OK.",
            "So this is very natural now.",
            "However things are, but in this case at least, clustering is actually necessary.",
            "So while in the C epsilon property case we had with any two clusterings, but satisfied the CF Salon property must be order of epsilon close to each other, because they are in particular epsilon close to the optimal comedian solution.",
            "That means that so in that case right in the CFL party case, any two clustering satisfying the property actually must be close to each other.",
            "In this case in the new Seattle.",
            "In the in the case of this new stepson property, it might be the case that two different sets of outliers could result in two completely different clusterings of the same data set.",
            "So in this case we had we have to output at least, this is the best we can hope for thought with at least with the property.",
            "But any clustering satisfying the property is actually closed.",
            "One of the clusterings in the list.",
            "So in this case at least clustering is necessary, and which is the case even in the evenin.",
            "In the special case where most of the points come from large clusters.",
            "And I'm actually going to give a small example to it."
        ],
        [
            "Ladies, so let's assume that we have a setup like this so heavy is large blobs, A1A2A3, and three.",
            "Maybe it'll be high point.",
            "I'm going to call them X one X2X3.",
            "Now we can set up the distances in a way so."
        ],
        [
            "But basically the optimal comedian solution for the set, a one Union 8293, is actually the clustering.",
            "See one where I want it in town, cluster it with a different cluster, atheism, different cluster and so in this case we can think of X1X2X3 as ill behaved or outliers points that we can ignore in the definition of the new CS and property.",
            "Hi so we can set the distances of at the optimal comedian solution for the instance A1 Union 290, sexually disgusting C1.",
            "And Moreover if we said that this is appropriately when we can even ensure that this instance I want tonight to unit 3 in fact satisfies the steps and property for the clustering is you want Now if we."
        ],
        [
            "Said if we if we said distances of X one is very far away from all those blobs.",
            "18283 when it ends."
        ],
        [
            "But we can have at the optimal comedian solution for the instance, a one for the set of points A1 unit to unit reunion.",
            "Next one is actually the clustering where X one is it in town cluster a second class.",
            "There is a one and two and the cluster is the cluster 3.",
            "OK and notice and Moreover we can also ensure that actually with instance satisfies the the C epsilon property with respect to this clustering C2.",
            "For Exxon is its own cluster, eh?",
            "One unit with a different class and Fe is the third cluster.",
            "And now that this clustering right here is significantly different from the clustering where we have a one in a separate class right in a separate class.",
            "And if in a separate cluster, so these classes are very far apart, so we could have a two significantly different classes of the data can satisfy the new C epsilon property, and so the best that we can hope here is to output the least smallest of clusterings with the property with any clustering that satisfies and you see epsilon condition.",
            "Is in fact close enough to one of the clusterings that we output.",
            "And."
        ],
        [
            "Alright, and actually one thing that we show in this recent talk with.",
            "Hey Carolyn and Sean, gotta Shank what tank is that actually in the case or most of the points come from large clusters we can infect efficiently.",
            "Alberta List of size at most K with the property, but any clustering satisfies and you see epsilon condition will in fact be close to one of the class rings that were output.",
            "So this is upper and actually one can even show a lower bound so one can modify the example that more examples, but I but I had before to actually show that K is actually the best that we can hope for.",
            "Alright, so this is I just mentioned this expansion and."
        ],
        [
            "But then for actually for open problems in terms of open problems, I think it will be actually interesting to analyze this epsilon property for other objectives, in particular for sparsest cut.",
            "So sparsest cut is notoriously known difficult objective from an approximation POV, the best known approximation just sort of log on the boys no lower bound is a constant factor approximation, and so it will be particular interesting to analyze this property.",
            "So try to cluster well under the steps and property for the sparsest cut objective and more generally, I think it will actually be interesting to take this point of view on other problems where.",
            "The solution is to set up a proxy objective and try to optimize that, so it'll be interesting to see if some of this.",
            "This you can actually be transported over to some of other problems with other problems.",
            "And actually before.",
            "So now to conclude, actually want to go to go."
        ],
        [
            "Um?",
            "I want to go back to the picture that I had at the beginning of the talk, so the classic problem that I considered in the circle of the following type, we assume that we have a set S of an objects and document, and we assume that there exists some desired correct ground of clustering.",
            "That means each object has amount reliable safe topic and our goal is to come up with a clustering of low error where the error rate of a given clustering was a fraction of the points that are misclassified up to the indexing of the clusters.",
            "OK, and so this is a goal to come up with the clustering of low error and we also assumed that in order to be able to do anything with our clustering algorithm, is has an input similarity or dissimilarity information.",
            "And clearly in order to even hope to do anything, even setting this similarity similarity information with pairwise information has to somehow be related to the ground of clustering has to somehow be ready to add trying to do.",
            "OK, so this is the setup that I was describing and."
        ],
        [
            "Actually, given this marginal setup and a very interesting direction and question is to try to understand what natural properties or what reasonable properties on the similarity function on The Perrys information are sufficient to allow us to cluster well and I wanted to point out.",
            "But actually the condition that I described today, the C epsilon property for objective Phi safer comedian was with epsilon properties.",
            "Just an example of how the similarity information that's inaccurate information can be related to the.",
            "This article grant of Clustering Effect might not be even the most natural one is one that we picked in order to connect to the approximation algorithms literature.",
            "So we see epsilon properties actually just one example of a property, but one can analyze about and.",
            "One can analyze many more properties, and in fact I wanted to mention."
        ],
        [
            "But in recent Doc actually have implementation.",
            "Paula, we have developed a whole general theory about what natural properties in the similarity function.",
            "And to allow us to cluster well and both algorithms.",
            "And actually, every bloom will talk about this.",
            "On Monday, so I. I definitely encourage you to attend the talk.",
            "Alright, thank you.",
            "So questions.",
            "Yes.",
            "Actually asking about that property here is in some strange because it's probably working with datasets, not about invention.",
            "Empirical data that you have supposed to property above is actually known.",
            "I was talking about the inductive case with the property is satisfied.",
            "So OK for most of the talk I talked about the transductive case where I assume that they have a fixed set of points.",
            "Only on class are bad, but I also briefly mentioned the inductive case where actually assume that the property is satisfied over the whole set of points and actually try to so even for the epsilon property.",
            "So I assume, but more generally.",
            "This work, we're alive, marginal property.",
            "What we do assume that the property that we talk about is satisfied with respect to the whole distribution and actually, technically speaking, is a challenge to show some time that the properties are satisfied about the sample.",
            "We had one concrete case where it had to use very complicated regularity type arguments to show that they think I will talk about it.",
            "This assumption relates or can be guaranteed by generative assumptions over the clusters.",
            "Well, so OK, this particular one.",
            "Probably OK, so the problem, for instance for generative ones.",
            "Probably we need for instance in the case on hold mixture model, say Gaussian Spring, the component of the audience would be very far apart.",
            "So if they are not very far apartment property not going to be satisfied.",
            "But I don't think this is a drawback because you know, even in supervised learning we have generative models and non interactive models now.",
            "And if you know that the data comes is generated by Mr. Gaussians, then you want to apply.",
            "Very specific algorithm for the 4th case you need you want to use your assumptions.",
            "Talking to someone, I don't think that.",
            "So if you have a mixture model, for instance a mixture of Gaussians, but the data the property will be satisfied, only the components are very far apart with the components are not reasonably close and the data is probably not going to be satisfied.",
            "Some other property that actually studying the work with every man, Santo should be satisfied, but even that is not the right approach to the problem that case.",
            "Don't have it.",
            "Well, I think that well state of the art team that.",
            "Exactly what that is.",
            "But I definitely think the algorithm for Mr Model would be better than using this approach.",
            "But I don't know this is not a drill bit because the state of the art algorithm tomorrow going to work only the data is the mixture of Gaussians.",
            "And not in a much more general situations where we provide actually.",
            "So in this work we should definitely encourage you to go to CVS stock, which I think we have a very nice general theory and then we saw analyzer, variety of condition on the similarity function and for any condition that similarity function we analyze what?",
            "What is the right type of clustering algorithm that you want to be using.",
            "So actually I think this is quite valuable.",
            "So in some sense, but we're going to talk about this kind of global statistical learning theory framework for the context of clustering, so it's something similar to the case we know for supervised classification.",
            "If we know that it is the data.",
            "Is there a separable we want to apply as yeah, more perceptron if it's not linearly separable, maybe maybe the target.",
            "Maybe there is a good decision tree for the data I want to use it recently based algorithm and so on and that we doing this work we kind of come up with the analog of a type of analysis in the context of clustering or unsupervised learning.",
            "So I definitely encourage you to go in The Tempest.",
            "But I have no questions.",
            "Other comments.",
            "Alright thank you man."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's about finding lower clusterings and this is joint work with a bunch of people that you describe.",
                    "label": 0
                },
                {
                    "sent": "So just to clarify, I'm from Microsoft Cambridge, New England, so I'm not from England and next I'll go to Georgia Tech.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to talk about finding lower clusterings and in fact an even better title for this work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate clustering without the proclamation and the meaning of this title will become clear later in the book.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start by mentioning that problems of clustering data come up everywhere.",
                    "label": 0
                },
                {
                    "sent": "Few examples are the following.",
                    "label": 0
                },
                {
                    "sent": "Clustering news articles or web pages by topic clustering protein sequences by function or clustering images by who is in BAM.",
                    "label": 1
                },
                {
                    "sent": "Now, many of these problems can be formally specified as follows, so we assume that we are given a set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "South of an object, say and document, and we assume that there exists some unknown, desired correct ground truth clustering.",
                    "label": 1
                },
                {
                    "sent": "But means each object has some unknown true label, say its topic, and the goal is to produce of clustering a good clustering or clustering of low error where the error of a given clustering C prime is just the fraction of the points that are misclassified respect.",
                    "label": 1
                },
                {
                    "sent": "To see after indexing of the clusters.",
                    "label": 1
                },
                {
                    "sent": "So again, the setting here is the following.",
                    "label": 0
                },
                {
                    "sent": "We have a target clustering C, which is a partition of the whole set of points.",
                    "label": 0
                },
                {
                    "sent": "So the target clustering is the partition C1C1C2C K, and given a new clustering C primer, new partition of the whole set of points.",
                    "label": 0
                },
                {
                    "sent": "So given a new partition C, One prime seta prime CK prime, when the error rate of the clustering C prime with respect to the target clustering C is just the fraction of the points that you get wrong in the optimal matching between the classes in C. And glasses in C prime.",
                    "label": 0
                },
                {
                    "sent": "And this is very natural, and in fact it is the analogue of the O, and loss is now in the context of clustering of the 01 loss in the context of supervised classification.",
                    "label": 0
                },
                {
                    "sent": "And this is the case since here in the context of clustering we do not care about getting the names of the points right.",
                    "label": 0
                },
                {
                    "sent": "We don't care about getting the labels of the points right, only care about getting the points themselves right.",
                    "label": 0
                },
                {
                    "sent": "Alright, so again, our goal is to come up with a clustering of low error or with a good clustering, and in order to do so we are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So given a measure of similarity or dissimilarity, so are also given a pairwise measure between these operas.",
                    "label": 0
                },
                {
                    "sent": "Measure between pairs of.",
                    "label": 0
                },
                {
                    "sent": "We are given authorize, measure between pairs of objects.",
                    "label": 0
                },
                {
                    "sent": "For example, in the document clustering case, the.",
                    "label": 0
                },
                {
                    "sent": "The similarity dissimilarity measure that we are given can be something based on the number of keywords in common in the sequence.",
                    "label": 1
                },
                {
                    "sent": "Protein clustering case, the.",
                    "label": 0
                },
                {
                    "sent": "The similarity similarity measure given can be something based on the added distance and so on, and in fact for the rest of the talk, I will assume that we're actually given a dissimilarity measure which satisfies the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "So we're giving a distance function.",
                    "label": 1
                },
                {
                    "sent": "And again, our goal is to come up with a clustering of low error and clearly given this gold dissimilarity measure, the distance functions were given has to somehow be related to the target clustering as to somehow be related to the topic Ness, because otherwise there would be no hope.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's just the permutation.",
                    "label": 1
                },
                {
                    "sent": "So this case, the set of all permutations from one from the set one 2K to the set one 2K.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It's not the only way, but if this is a very natural definition of distance between two clusterings, an error in the context of clustering, and as I was saying is the analog.",
                    "label": 0
                },
                {
                    "sent": "The O and loss in the context of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "It's just a natural measure.",
                    "label": 0
                },
                {
                    "sent": "Of course, one could consider other natural measures as well, but it's certainly natural.",
                    "label": 0
                },
                {
                    "sent": "And it captures in some sense we try to do to recover the underlying clustering.",
                    "label": 0
                },
                {
                    "sent": "So capture that we do not care about getting the labels right the the label of the points right only get get about getting the clusters right?",
                    "label": 0
                },
                {
                    "sent": "But I should be set up with a problem Now a standard approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gotcha very classic approach in the theoretical computer science community and also actually machine learning community is to solve the problem as following user data points as nodes in a weighted graph where the weights are based on the dissimilarity measure.",
                    "label": 1
                },
                {
                    "sent": "But we are given and then what people do.",
                    "label": 1
                },
                {
                    "sent": "They pick some objective function to optimize.",
                    "label": 0
                },
                {
                    "sent": "Like comedian K means mean some and so on.",
                    "label": 0
                },
                {
                    "sent": "And just remind you or if you've never seen this in the comedian clustering case, the goal is to find.",
                    "label": 0
                },
                {
                    "sent": "A partition of the whole data set a partition C, one prime cita prime CK, prime and medians or Centers for those.",
                    "label": 0
                },
                {
                    "sent": "We want to find median or Center for each set CI prime in order to minimize the sum over all points of the distance to their corresponding median.",
                    "label": 0
                },
                {
                    "sent": "So this is a comedian objective function and K means.",
                    "label": 1
                },
                {
                    "sent": "Single function, the goal is similar.",
                    "label": 0
                },
                {
                    "sent": "We just want to minimize the sum of the square distances.",
                    "label": 0
                },
                {
                    "sent": "And finally, in the mean some class in case the goal is to find the partition or the whole data set that minimizes some of the intracluster the similarities or distances.",
                    "label": 0
                },
                {
                    "sent": "Right, so again, a classic.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approaching the theoretical contestants committee also in machine learning, is to view the data point is now in a weighted graph where the weights are based on the similarity measurement that we are given and when to pick some objective function to optimize and then to develop approximation algorithms to optimize this objective.",
                    "label": 1
                },
                {
                    "sent": "Again, many of these objectives are in Bihar, so it's NP hard for many of these objectives to find an optimal solution.",
                    "label": 0
                },
                {
                    "sent": "So then the best that we can hope for is an approximation.",
                    "label": 0
                },
                {
                    "sent": "So the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "So for instance, for the K median clustering objective, the best known approximation is A3 plus epsilon approximation, and it is also known that is NP hard to find 1 + 2 over approximations of the optimal comedian solution.",
                    "label": 0
                },
                {
                    "sent": "So there has been a lot of work here, and more generally there has been a lot a lot of work in the approximation algorithms community on designing approximation algorithms for this clustering objectives.",
                    "label": 0
                },
                {
                    "sent": "So this is all fine.",
                    "label": 0
                },
                {
                    "sent": "However, remember that in many other problems that I described, our real goal is to get the points right what we wanted to do is to recover the correct desired ground truth clustering and so that means that if we end up using the C approximation.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who is an object?",
                    "label": 0
                },
                {
                    "sent": "If I say to comedian?",
                    "label": 0
                },
                {
                    "sent": "In a setting where what we really want to do is to get the points right, then that means that we make an implicit assumption, but all the clusterings that are within a factor of C of the optimal solution for Phi are in fact epsilon close to our target clustering.",
                    "label": 1
                },
                {
                    "sent": "So again, if we ever end up using a C approx.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motion algorithms data comedian.",
                    "label": 0
                },
                {
                    "sent": "In order to cluster documents or really want to do is to get the the target clustering right.",
                    "label": 0
                },
                {
                    "sent": "That means that we make an implicit assumption.",
                    "label": 0
                },
                {
                    "sent": "But any clustering whose objective comedian objective values within a factor of C of the optimal solution for optimal Keegan solution.",
                    "label": 0
                },
                {
                    "sent": "So any clustering who's even a factor of three of the optimal capital solution must in fact be epsilon close to our target clustering in terms of the symmetric difference distance that I described earlier.",
                    "label": 0
                },
                {
                    "sent": "And for the rest of the talk, I'm going to call with the C Upsilon property, so I'll save the data, satisfies the C epsilon property if any clustering within a factor of the of the optimal solution for objective Phi is in fact epsilon close to the target.",
                    "label": 0
                },
                {
                    "sent": "Clustering at enclosing terms of the semantic different systems.",
                    "label": 0
                },
                {
                    "sent": "Right now it turns out that the problem of finding AC approximation algorithm to objectify under the sea epsilon property is as hard as it is in the general case.",
                    "label": 1
                },
                {
                    "sent": "Now it is showing our our work.",
                    "label": 0
                },
                {
                    "sent": "However, is that under the C Epsilon property we are able to cluster well that means are able to get a clustering that is close to our target clustering in terms of the semantic difference distance and we are able to do so without approximating the objective at all.",
                    "label": 1
                },
                {
                    "sent": "That's why I'm saying, but basically the main point of the work and that I was saying at the beginning of my talk with a good title for this work is approximate clustering without the approximation.",
                    "label": 0
                },
                {
                    "sent": "So we saw, but really want to solve get close to the target clustering, but without approximating the proxy objective.",
                    "label": 0
                },
                {
                    "sent": "For instance, the comedian objective at all.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to be more concrete, for instance, for the comedian object, if we can show that for any constant C stick constantly stick together, then one, then under the sea epsilon property we can cluster.",
                    "label": 0
                },
                {
                    "sent": "Well, we can.",
                    "label": 0
                },
                {
                    "sent": "We can find the clustering that is order of epsilon closed, desired current of clustering and we can do so even for values of C. We're getting a C approximation to the comedian.",
                    "label": 0
                },
                {
                    "sent": "Objective is probably NP hard, and Moreover we can even get.",
                    "label": 0
                },
                {
                    "sent": "Exactly close epsilon close to the target.",
                    "label": 1
                },
                {
                    "sent": "Clustering in the case when all the clusters are sufficiently large.",
                    "label": 0
                },
                {
                    "sent": "So basically we are doing as well as if we could approximate the objective to an NP hard value, which I think is very nice.",
                    "label": 0
                },
                {
                    "sent": "Alright, and before actually going into more details and describing proofs, let me make one more.",
                    "label": 0
                },
                {
                    "sent": "Let me make a comment so from an.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization algorithms, perspective, unnatural, and absolutely legitimate motivation for improving the approximation factor from C1 to C2 or C. Twist equivalency.",
                    "label": 1
                },
                {
                    "sent": "One is that maybe the data, but we have satisfies the C epsilon property for the value of C2 but not for C1.",
                    "label": 0
                },
                {
                    "sent": "And this is actually legitimate.",
                    "label": 0
                },
                {
                    "sent": "Motivation is absolutely legitimate, is natural and in fact we can show that for any constant C2 smaller than C-11 can construct the data set.",
                    "label": 0
                },
                {
                    "sent": "And the target clustering, but satisfies this T2 epsilon property, but not even the C10 point 49 property.",
                    "label": 1
                },
                {
                    "sent": "So it's absolutely natural to try to improve the approximation ratio from C1 to C2B, cause maybe the letter satisfies this condition.",
                    "label": 0
                },
                {
                    "sent": "Possible, but not for C1, but the point is that in our work we can do even better because we are able to cluster well even for values of C. We're getting a C approximation to the object if Sado comedian is probably hot.",
                    "label": 0
                },
                {
                    "sent": "Alright, and actually, before describing the proofs.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Me quickly summarize them.",
                    "label": 0
                },
                {
                    "sent": "Any result some of the main reasons that we have in this model first of all, for the comedian clustering problem, we can show that if the data satisfies the C epsilon property, then we can get order of epsilon over C -- 1 close to the target and Moreover if the data satisfies this epsilon property and if the target clusters are large, then we can even get epsilon close to the target, yes.",
                    "label": 1
                },
                {
                    "sent": "All polynomial, so you are you asking about.",
                    "label": 0
                },
                {
                    "sent": "We didn't optimize anytime, so it's like N ^2 say or yeah this is not a major issue for us.",
                    "label": 0
                },
                {
                    "sent": "We just we're helping polynomial time algorithms but.",
                    "label": 0
                },
                {
                    "sent": "Difficulty minus one.",
                    "label": 0
                },
                {
                    "sent": "What is the dependence known?",
                    "label": 0
                },
                {
                    "sent": "Running time is not depend, but it's not dependent just in the approximation final approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "So as I was pulling sincere as I was saying, if he satisfies the epsilon property when we get order of epsilon versus minus one in the absolute true general case when you have even very very small target clusters.",
                    "label": 0
                },
                {
                    "sent": "However, if the target classes are sufficiently large, we can even get epsilon close to the target clustering, so the dependency to answer your question comes in the approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "In the case of lot of small clusters.",
                    "label": 0
                },
                {
                    "sent": "K please ask questions.",
                    "label": 0
                },
                {
                    "sent": "Actually let me find it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these are results for comedian.",
                    "label": 0
                },
                {
                    "sent": "We can get something very similar for K means and this actually our visa resource that appeared in joint work with Avrim, Blum and an open Gupta and so that was nine and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the mean, some clustering objective which is actually a more delicate object.",
                    "label": 0
                },
                {
                    "sent": "If we get the following type of results, we can show that in the data satisfies the C epsilon property and if the target classes are sufficiently large, when we can again get order of Epson oversee minus one close to the target.",
                    "label": 1
                },
                {
                    "sent": "However, is a target clusters are.",
                    "label": 0
                },
                {
                    "sent": "Allowed to be arbitrarily small.",
                    "label": 0
                },
                {
                    "sent": "Then in the case where we don't have too many clusters.",
                    "label": 0
                },
                {
                    "sent": "In the case where the number of target clusters is more of an log an over log log and then he can actually get order of epsilon oversee minus one.",
                    "label": 0
                },
                {
                    "sent": "Close the target in the most general case, the best we know so far to do is to output a small list of clustering.",
                    "label": 1
                },
                {
                    "sent": "Small meaning list of size log, log in with the property at the target.",
                    "label": 1
                },
                {
                    "sent": "Clustering is close to one of the clusterings in the list.",
                    "label": 0
                },
                {
                    "sent": "Type services are quick overview of this app.",
                    "label": 0
                },
                {
                    "sent": "A quick summary of our results, and now I'm going to go in and talk in detail about the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See Epsilon property for the comedian clustering objective.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "From now on I'm going to focus on the comedian clustering objective and that also mentioned a little bit about the other objectives as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's assume that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh, we our data set satisfies the property, but any C approximation to the comedian.",
                    "label": 0
                },
                {
                    "sent": "Optimal solution is in fact epsilon close to our target clustering and for simplicity just for the presentation.",
                    "label": 1
                },
                {
                    "sent": "Let's assume that the target clustering is the optimal comedian solution.",
                    "label": 0
                },
                {
                    "sent": "And also let's assume that all the classes are large enough, not large enough, but not too small.",
                    "label": 0
                },
                {
                    "sent": "We have size at least two epsilon times then, and this is just avoid some technicality we don't need this restrictions.",
                    "label": 0
                },
                {
                    "sent": "This is just for the talk now for any point X, let us denote by WX the distance from X to its corresponding center in the comedian optimal solution, which by assumption by that I make in the talk is also the target clustering and let W2 of XB the distance from Max to its second causes centering the comedian optimal solution.",
                    "label": 0
                },
                {
                    "sent": "LAW average be the average over X of W bags.",
                    "label": 0
                },
                {
                    "sent": "So just by definition, we have at OPT, which is a value of the optimal comedian.",
                    "label": 0
                },
                {
                    "sent": "Solution is an times double average.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is just a notation now to immediate things that we can say are the following so we can show the steps alone.",
                    "label": 0
                },
                {
                    "sent": "Property implies to fax.",
                    "label": 1
                },
                {
                    "sent": "So the first one is that at most epsilon endpoints can have W2 smaller than C -- 1 W average over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because otherwise we could move epsilon endpoints to wear second closest cluster, and we could do so without increasing the objective by more than C -- 1 W average over epsilon.",
                    "label": 0
                },
                {
                    "sent": "All times epsilon, which is C -- 1 W average times then, which is similar sometimes up.",
                    "label": 0
                },
                {
                    "sent": "So we get the clustering that is still a C approximation to the optimal comedian clustering and which is epsilon part from the target clustering and so contacting the epsilon property that we started with.",
                    "label": 0
                },
                {
                    "sent": "So using the steps and property using the definition of the steps and property, we can prove that at most epsilon points can have W2 smaller than C -- 1 W average over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this is the first fact and the second fact that we can prove which follows much more trivially from the definition is that at most 5 epsilon, an oversea minus one points can have W greater than C -- 1 W average over 5 epsilon, and we just focus on the definition of W average and using Markov inequality.",
                    "label": 0
                },
                {
                    "sent": "Alright, now for the rest of the points for what we call the good points we have a big gap.",
                    "label": 1
                },
                {
                    "sent": "So for the rest of the points we can we have it.",
                    "label": 0
                },
                {
                    "sent": "The distance to Brown Center is at most C -- 1 W average over 5 epsilon and with their distance to their second closest center is at least C minus on W average over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So for the rest of the points where the good points W is smaller than this quantity right here, and W2 is at least 5 * 5 times with quantity.",
                    "label": 0
                },
                {
                    "sent": "So now if we do not this quantity right here, by the criticals, if we do not get quantity by the critical, what we?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is that.",
                    "label": 0
                },
                {
                    "sent": "Most of the points, all the good points, one answered of epsilon fraction of the points will look like this.",
                    "label": 1
                },
                {
                    "sent": "So there will be in distance D critical.",
                    "label": 0
                },
                {
                    "sent": "So this points like say point X is within distance be critical of its own center, and then that means that by triangle inequality the distance between any two good points in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "Is it is upper bounded by two times the critical and now since you know that any good point is at least distance at least five times the critical to their second causal center, we also know that any two good points in different clusters are a distance at least four times the critical from each other, and this follows band by triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "Because remember, I assume without distance satisfies triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "Right and so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It means that basically now the word loose look.",
                    "label": 0
                },
                {
                    "sent": "So now that means that if we now create a graph G where we connect any two points X&YE varying distance two times the critical of each other, then that means that the good points in the same cluster we're going to form a clique since the distance between two points in the same classes and multiple times the critical and we also have that no good points in different clusters can even have any neighboring comment, we cannot have a neighbor in common.",
                    "label": 1
                },
                {
                    "sent": "Because the distance between two points in different clusters is at least four times the critical, so I cannot have a neighboring, in the graph G. So that means yes.",
                    "label": 0
                },
                {
                    "sent": "The good points I'm talking about the good points, so at this point I want.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about the good points.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's actually a perfectly right.",
                    "label": 0
                },
                {
                    "sent": "So the the word actually looks like this.",
                    "label": 1
                },
                {
                    "sent": "So most of the points are good points and these good points from clicks and this good points can be connected to bad points.",
                    "label": 0
                },
                {
                    "sent": "So the big points are pictured here on the bottom, but not too many of em and bad points can connect to other about points, but they can Only Connect one of the good set because we know that good points in different clusters have no neighbor in common.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Statement or.",
                    "label": 0
                },
                {
                    "sent": "No, no, it's not terministic, and if I don't even have to use Markov inequality, I was just saying OK, so to go back, it's all deterministic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this fact right here even follows Justin definition of W average.",
                    "label": 0
                },
                {
                    "sent": "I was just using a fancy explanation mark of inequality, but it follows.",
                    "label": 0
                },
                {
                    "sent": "Awesome definition right?",
                    "label": 0
                },
                {
                    "sent": "And everything is the terministic.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's all going back here?",
                    "label": 0
                },
                {
                    "sent": "So we have the world?",
                    "label": 1
                },
                {
                    "sent": "Looks I'd be so most of the points.",
                    "label": 0
                },
                {
                    "sent": "The good points from clicks now this good points are connected, can be connected to bad points.",
                    "label": 0
                },
                {
                    "sent": "However bad points about points connect all about points.",
                    "label": 0
                },
                {
                    "sent": "However they can only touch on with the good sets.",
                    "label": 1
                },
                {
                    "sent": "So that means that if we.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Furthermore, assume that the classes are large.",
                    "label": 0
                },
                {
                    "sent": "In particular, if we assume that the classes have size at least 2 * B + 1, or B is a number of points, then what we can do is just create a graph H where it connects to two points X&YE ve share at least being able comments in the graph G and so that means that now the graph age will split like this, and so that means that we can just help with the largest components in age and that we get which is a clustering of error roughly.",
                    "label": 1
                },
                {
                    "sent": "The fraction of which is a classic one, so the number of mistakes that we make is order of the size of the bed set.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even target class.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "US Auto Lodge then it turns out you can still get order of epsilon versus minus one, but we need to see to be a bit more careful, and in particular what can happen now is that some of the clusters can be totally dominated by the bad points.",
                    "label": 1
                },
                {
                    "sent": "However, it turns out that we can still.",
                    "label": 0
                },
                {
                    "sent": "Glasser well, we can still get a clustering of low error by using a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "And in particular, the algorithm that we that works in this case is still quite sick.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People, we just need to be a slightly more carefully analysis.",
                    "label": 1
                },
                {
                    "sent": "Cancel the algorithm is as follows.",
                    "label": 0
                },
                {
                    "sent": "Just as before, we create the graph G. We connected it to points X&Y.",
                    "label": 0
                },
                {
                    "sent": "If ever been distance two times, the critical of each other, and then what we do, we pick the vertex of the highest degree in the graph G and remove with vertex and its entire neighborhood, and we've ended costs.",
                    "label": 1
                },
                {
                    "sent": "And this way we output the clustering with K clusters.",
                    "label": 0
                },
                {
                    "sent": "Anile the main idea is that we can now charge off all the errors to the bad points.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                },
                {
                    "sent": "Eve the vertex.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the if the vertex Vijay, but we picked was a good point like this one here.",
                    "label": 0
                },
                {
                    "sent": "When we are happy because it means that we pull in, we entirely pull, pull, pull out a good set.",
                    "label": 0
                },
                {
                    "sent": "So this is a good case.",
                    "label": 0
                },
                {
                    "sent": "I'll put a good cluster.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the vertex.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we picked was a bad point.",
                    "label": 0
                },
                {
                    "sent": "We might end up pulling out only parts of only a part of a good set.",
                    "label": 0
                },
                {
                    "sent": "So we might be missing some of the points in particular might be missing, say are times I PowerPoints on plaster CI.",
                    "label": 0
                },
                {
                    "sent": "Another point is that we are greedy, and since the vertex we had the highest degree, that means that the size of the bed sets that we pick is larger than the size of the site that we need.",
                    "label": 0
                },
                {
                    "sent": "And so that means that we can now use this tag to essentially charge of all the errors to the bed set, and so that means that we get the clustering whose error rate is proportional to the fraction of the bad points.",
                    "label": 0
                },
                {
                    "sent": "So I get the clustering the number of mistakes that we make is order of the size of the bed set, which is order of epsilon times, then over to minus one.",
                    "label": 0
                },
                {
                    "sent": "Hi so can also deal with small clusters, which is nice now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going back to the large clusters case, it turns out that we can actually get even better.",
                    "label": 1
                },
                {
                    "sent": "We can even get epsilon close to the target clustering in the case or the target classes are large, and the notion of laughing.",
                    "label": 1
                },
                {
                    "sent": "It's actually depends on this constancy.",
                    "label": 1
                },
                {
                    "sent": "And now the main idea here is that we actually have two kinds of bad points.",
                    "label": 1
                },
                {
                    "sent": "We have some at most epsilon points, which I'm going to call confused points, since where this since the gap between the distance to their own sentence, the distance to their second class center is not too large.",
                    "label": 0
                },
                {
                    "sent": "So for this point for the confused so this point, the confused points have a small gap between the distance to their own center in the distance to the second closest center.",
                    "label": 0
                },
                {
                    "sent": "Now the rest of the bad points are not confused.",
                    "label": 0
                },
                {
                    "sent": "They have a large gap between the distance to Vero sent and the distance to the second class center.",
                    "label": 0
                },
                {
                    "sent": "However, for this non confused points for this bad but not confuse points, W itself is large, and now it turns out actually can recover this non confused points.",
                    "label": 0
                },
                {
                    "sent": "And which will give us a clustering of error at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "Now the main idea here is that this non conference points have a large gap.",
                    "label": 0
                },
                {
                    "sent": "As I was saying is not confused point have a large gap between their distance, the distance to run center distance to their second class center, in particular that gap is at least five times the critical.",
                    "label": 0
                },
                {
                    "sent": "Now we don't really know which we only know the center, so we don't really know the centers.",
                    "label": 0
                },
                {
                    "sent": "However, we now with any good point is.",
                    "label": 0
                },
                {
                    "sent": "Distance of mostly critical of its own center and so.",
                    "label": 0
                },
                {
                    "sent": "Then we expect imply but basically this non confused point will be.",
                    "label": 0
                },
                {
                    "sent": "Will be closer to good points in wrong cluster.",
                    "label": 0
                },
                {
                    "sent": "Went to good points in any other cluster.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Moreover, now if the clusters in moral of the classes are large enough, then this class will actually be dominated by good points, and so that means that in the case where the class target classes are large enough that we can do is now the following given.",
                    "label": 0
                },
                {
                    "sent": "So given the output C prime from the algorithm so far.",
                    "label": 1
                },
                {
                    "sent": "So given the clustering see private, we have so far.",
                    "label": 1
                },
                {
                    "sent": "What we do is just reclassify each point X into the clusters of laws median distance.",
                    "label": 1
                },
                {
                    "sent": "And and basically, the point is that if the classes are large enough when the median will be controlled by a good point, and as I was saying, always non confused point are closer to good point in round cluster and good points in any other cluster and so that means that the non confused point will be pulled out in that direction.",
                    "label": 0
                },
                {
                    "sent": "It's already covered them, so we get the clustering overall at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "Now actually one other technicality with.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to briefly mention, without going too much into detail, so actually one thing is that so far I highly defect, which is that the definition of the graph G. But I also know the algorithms depend depends on the distance D kritikal, which in turn depends on the definition or depends on W average.",
                    "label": 1
                },
                {
                    "sent": "Which is optimal, and so it depends on some unknown quantity.",
                    "label": 1
                },
                {
                    "sent": "OK now, but the multiple ways around with in the case where the target classes are large, we have one solution.",
                    "label": 0
                },
                {
                    "sent": "So the solution here is to start with the logs for W. So in particular started at the first case for double can be the smaller distance between two points.",
                    "label": 1
                },
                {
                    "sent": "Start with a W and let's keep increasing W until the first time when we get with the.",
                    "label": 0
                },
                {
                    "sent": "K largest components of the graph H cover a significant fraction of the space, in particular cover one month order of B point fraction of the points, and Moreover, all these components are large in size.",
                    "label": 0
                },
                {
                    "sent": "And now the point is that if we impose it, all these components are large in size, which means that we never misclassify good point.",
                    "label": 0
                },
                {
                    "sent": "And this can be kind of used to argue, but in this case this is a good way to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So it requires some argument, but everything goes through, so this is just the main idea.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 solution.",
                    "label": 0
                },
                {
                    "sent": "The large clusters case, and it actually is quite nice because we don't need to run an approximation algorithm now in the in the non lab.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plus this case in the in the case where we allow small clusters, what we have to do is visibly unfortunate, but we thought we do well.",
                    "label": 0
                },
                {
                    "sent": "We have to actually run the first time an approximation algorithm for the object policy for comedian in order to get the constant factor approximation to the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And so we can then use that constant factor approximation and then this approximation factor just goes into the approximation in the final approximation guarantee for our clustering just goes into the error term.",
                    "label": 1
                },
                {
                    "sent": "Alright, so we can also handle not knowing W average.",
                    "label": 0
                },
                {
                    "sent": "Oh, and so the kritikal now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me mention also one other extensions that we can see that so we also analyze this problem in an inductive case where the set of the point of the set of points as what we see is just a small sample from a much larger abstract instance space, and where our goal is to come up with a clustering for the whole space.",
                    "label": 0
                },
                {
                    "sent": "So algorithmically what we do in this case, we first draw the sample S. We cluster the sample and then we send you points into the clusters.",
                    "label": 0
                },
                {
                    "sent": "As I arrive online.",
                    "label": 0
                },
                {
                    "sent": "Currently, in particular, in the case where the target classes are large enough, then what we do we run the algorithm that I was describing over the sample.",
                    "label": 0
                },
                {
                    "sent": "So that means we construct the graph G. We can start the graph H, take the K largest components of H. So this is the casting of the sample and then when a new point arrives, we just insert it in one of these clusters based on the median distance to this cluster that are produced over the sample.",
                    "label": 0
                },
                {
                    "sent": "So we were doing the inductive case in the last class, this case and the main idea of why we circuit works is that if you think about the key property, but we used to argue correctness for the algorithm in the large classes case, is that in each cluster we have more good points and bad points.",
                    "label": 1
                },
                {
                    "sent": "This isn't the case or UW in the case.",
                    "label": 0
                },
                {
                    "sent": "We didn't know that we needed that we have twice more good points and bad points in anyway.",
                    "label": 0
                },
                {
                    "sent": "Either way, the point is that if the target classes are large enough and if the sample S is large enough when this was going to happen over the sample as well.",
                    "label": 1
                },
                {
                    "sent": "So we then can be used to argue correctness of this algorithm in the inductive case.",
                    "label": 0
                },
                {
                    "sent": "Hi so I can also deal with inductive case and the right.",
                    "label": 0
                },
                {
                    "sent": "So this is we summarizes all our results for the comedian Classlink objective.",
                    "label": 0
                },
                {
                    "sent": "Now we get similar results also for the chemistry.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "String objective, so we get all the argument pretty much goes through all the arguments.",
                    "label": 0
                },
                {
                    "sent": "Pretty much go through the only thing that breaks is a is getting epsilon exactly epsilon.",
                    "label": 0
                },
                {
                    "sent": "Close the target clustering.",
                    "label": 0
                },
                {
                    "sent": "In the case of the target classes are large.",
                    "label": 0
                },
                {
                    "sent": "Probably that can be solved actually now for the mean some clustering objective.",
                    "label": 0
                },
                {
                    "sent": "We can still cluster well, but this requires a much more involved argument, and in fact the original solution that we have in the Sedona 2009 paper, we have implement an open Gupta was the following.",
                    "label": 1
                },
                {
                    "sent": "We first connected them in some objective to something called balanced comedian, which is very similar to the Cayman objective, just except now we basically multiply the distance from Lexington Center of the size of the cluster in which the point is in.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard objective, but is used in.",
                    "label": 0
                },
                {
                    "sent": "Your community at so to solve them in some what we do.",
                    "label": 1
                },
                {
                    "sent": "First connect them in some objected to the balance came in objective and then we can kind of adapt the algorithm that we had for the comedian objective to the balance comedian objective.",
                    "label": 0
                },
                {
                    "sent": "However, the things are slightly more complicated because in this case we don't really have a uniformly criticals.",
                    "label": 0
                },
                {
                    "sent": "The definition of the graph G is not so obvious because in this case you could actually have clusters with lots of points, but with small distances and clusters with very few points, but with large distances.",
                    "label": 1
                },
                {
                    "sent": "So the situation is slightly more complicated.",
                    "label": 0
                },
                {
                    "sent": "However, it turns out that we in you can still use this approach to solve the problem for the case on C, stick together and two and the target classes are large.",
                    "label": 0
                },
                {
                    "sent": "So we can still get some results even by this approach, and in fact actually this results are quite impressive even sells, because actually the best known approximation algorithm to them in some objective in the general case is a Poly log is a logarithmic approximation, the polylogarithmic approximation, so we can do much better.",
                    "label": 0
                },
                {
                    "sent": "We can actually even cluster well in the case when C is like 3, for instance.",
                    "label": 0
                },
                {
                    "sent": "Cancel this is actually the original solution in this other paper.",
                    "label": 1
                },
                {
                    "sent": "Now we kind of completely solve the problem in the new upcoming called paper, But the arguments are significantly more involved.",
                    "label": 0
                },
                {
                    "sent": "So don't go for the balance comedian, but the analysis is much more involved.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And now to actually briefly summarize with epsilon properties, this part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me mention that actually won Kim user visual approximation algorithms approach to clustering is saying well, we cannot really measure what we want, which is closeness to the correct ground of clustering.",
                    "label": 0
                },
                {
                    "sent": "So what we do is set up on a proxy objective function like K, median, mean sometimes and so on and the approximate that.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a little bit like what I have in this picture right here where these guys about to jump off the building and we tell him we couldn't get a psychiatrist, but perhaps you'd like to talk about your skin.",
                    "label": 0
                },
                {
                    "sent": "So Doctor Perry here is a terminologist.",
                    "label": 0
                },
                {
                    "sent": "So we cannot really solve the problem.",
                    "label": 0
                },
                {
                    "sent": "We have to solve.",
                    "label": 0
                },
                {
                    "sent": "We set up proxy and hope that the proxy somehow, you know.",
                    "label": 0
                },
                {
                    "sent": "Helpful well, so actually I guess I think that maybe the truth is not really like this because it's actually.",
                    "label": 0
                },
                {
                    "sent": "If we do end up using a C approximation algorithm, Sato comedian or came into Class A documents that means that would really make an implicit assumption about how the distance is.",
                    "label": 0
                },
                {
                    "sent": "How does this not information that we are given relates to the underlying underlying desire count of clustering?",
                    "label": 0
                },
                {
                    "sent": "So make an assumption about how the distance is related to the closeness to the truth.",
                    "label": 1
                },
                {
                    "sent": "And now the trick is that in our work we make this assumption explicit.",
                    "label": 0
                },
                {
                    "sent": "And if we make this explicit, then we can get around inapproximability results by using the structure implied by this assumption that we were making implicitly anyway.",
                    "label": 1
                },
                {
                    "sent": "So we need to approximate clustering by the approximation.",
                    "label": 0
                },
                {
                    "sent": "And actually, right and.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the main story around this epsilon property.",
                    "label": 0
                },
                {
                    "sent": "Let me mention that more recently.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've also considered extensions of his properties, and in particular we have considered an extension of this property which captures clustering in the presence of outliers or misbehaved points.",
                    "label": 0
                },
                {
                    "sent": "So in particular, natural assumption is that an actual belief might be weather data satisfies the epsilon property, not about the whole set of points, but maybe the data satisfied the epsilon property after a small fraction of the outliers or behaves points have been removed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is very natural now.",
                    "label": 0
                },
                {
                    "sent": "However things are, but in this case at least, clustering is actually necessary.",
                    "label": 0
                },
                {
                    "sent": "So while in the C epsilon property case we had with any two clusterings, but satisfied the CF Salon property must be order of epsilon close to each other, because they are in particular epsilon close to the optimal comedian solution.",
                    "label": 0
                },
                {
                    "sent": "That means that so in that case right in the CFL party case, any two clustering satisfying the property actually must be close to each other.",
                    "label": 0
                },
                {
                    "sent": "In this case in the new Seattle.",
                    "label": 0
                },
                {
                    "sent": "In the in the case of this new stepson property, it might be the case that two different sets of outliers could result in two completely different clusterings of the same data set.",
                    "label": 0
                },
                {
                    "sent": "So in this case we had we have to output at least, this is the best we can hope for thought with at least with the property.",
                    "label": 0
                },
                {
                    "sent": "But any clustering satisfying the property is actually closed.",
                    "label": 0
                },
                {
                    "sent": "One of the clusterings in the list.",
                    "label": 0
                },
                {
                    "sent": "So in this case at least clustering is necessary, and which is the case even in the evenin.",
                    "label": 0
                },
                {
                    "sent": "In the special case where most of the points come from large clusters.",
                    "label": 0
                },
                {
                    "sent": "And I'm actually going to give a small example to it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ladies, so let's assume that we have a setup like this so heavy is large blobs, A1A2A3, and three.",
                    "label": 0
                },
                {
                    "sent": "Maybe it'll be high point.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call them X one X2X3.",
                    "label": 0
                },
                {
                    "sent": "Now we can set up the distances in a way so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But basically the optimal comedian solution for the set, a one Union 8293, is actually the clustering.",
                    "label": 0
                },
                {
                    "sent": "See one where I want it in town, cluster it with a different cluster, atheism, different cluster and so in this case we can think of X1X2X3 as ill behaved or outliers points that we can ignore in the definition of the new CS and property.",
                    "label": 0
                },
                {
                    "sent": "Hi so we can set the distances of at the optimal comedian solution for the instance A1 Union 290, sexually disgusting C1.",
                    "label": 0
                },
                {
                    "sent": "And Moreover if we said that this is appropriately when we can even ensure that this instance I want tonight to unit 3 in fact satisfies the steps and property for the clustering is you want Now if we.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Said if we if we said distances of X one is very far away from all those blobs.",
                    "label": 0
                },
                {
                    "sent": "18283 when it ends.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can have at the optimal comedian solution for the instance, a one for the set of points A1 unit to unit reunion.",
                    "label": 0
                },
                {
                    "sent": "Next one is actually the clustering where X one is it in town cluster a second class.",
                    "label": 0
                },
                {
                    "sent": "There is a one and two and the cluster is the cluster 3.",
                    "label": 0
                },
                {
                    "sent": "OK and notice and Moreover we can also ensure that actually with instance satisfies the the C epsilon property with respect to this clustering C2.",
                    "label": 0
                },
                {
                    "sent": "For Exxon is its own cluster, eh?",
                    "label": 0
                },
                {
                    "sent": "One unit with a different class and Fe is the third cluster.",
                    "label": 0
                },
                {
                    "sent": "And now that this clustering right here is significantly different from the clustering where we have a one in a separate class right in a separate class.",
                    "label": 0
                },
                {
                    "sent": "And if in a separate cluster, so these classes are very far apart, so we could have a two significantly different classes of the data can satisfy the new C epsilon property, and so the best that we can hope here is to output the least smallest of clusterings with the property with any clustering that satisfies and you see epsilon condition.",
                    "label": 0
                },
                {
                    "sent": "Is in fact close enough to one of the clusterings that we output.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and actually one thing that we show in this recent talk with.",
                    "label": 0
                },
                {
                    "sent": "Hey Carolyn and Sean, gotta Shank what tank is that actually in the case or most of the points come from large clusters we can infect efficiently.",
                    "label": 0
                },
                {
                    "sent": "Alberta List of size at most K with the property, but any clustering satisfies and you see epsilon condition will in fact be close to one of the class rings that were output.",
                    "label": 0
                },
                {
                    "sent": "So this is upper and actually one can even show a lower bound so one can modify the example that more examples, but I but I had before to actually show that K is actually the best that we can hope for.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is I just mentioned this expansion and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But then for actually for open problems in terms of open problems, I think it will be actually interesting to analyze this epsilon property for other objectives, in particular for sparsest cut.",
                    "label": 0
                },
                {
                    "sent": "So sparsest cut is notoriously known difficult objective from an approximation POV, the best known approximation just sort of log on the boys no lower bound is a constant factor approximation, and so it will be particular interesting to analyze this property.",
                    "label": 0
                },
                {
                    "sent": "So try to cluster well under the steps and property for the sparsest cut objective and more generally, I think it will actually be interesting to take this point of view on other problems where.",
                    "label": 1
                },
                {
                    "sent": "The solution is to set up a proxy objective and try to optimize that, so it'll be interesting to see if some of this.",
                    "label": 0
                },
                {
                    "sent": "This you can actually be transported over to some of other problems with other problems.",
                    "label": 0
                },
                {
                    "sent": "And actually before.",
                    "label": 0
                },
                {
                    "sent": "So now to conclude, actually want to go to go.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I want to go back to the picture that I had at the beginning of the talk, so the classic problem that I considered in the circle of the following type, we assume that we have a set S of an objects and document, and we assume that there exists some desired correct ground of clustering.",
                    "label": 0
                },
                {
                    "sent": "That means each object has amount reliable safe topic and our goal is to come up with a clustering of low error where the error rate of a given clustering was a fraction of the points that are misclassified up to the indexing of the clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is a goal to come up with the clustering of low error and we also assumed that in order to be able to do anything with our clustering algorithm, is has an input similarity or dissimilarity information.",
                    "label": 1
                },
                {
                    "sent": "And clearly in order to even hope to do anything, even setting this similarity similarity information with pairwise information has to somehow be related to the ground of clustering has to somehow be ready to add trying to do.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the setup that I was describing and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, given this marginal setup and a very interesting direction and question is to try to understand what natural properties or what reasonable properties on the similarity function on The Perrys information are sufficient to allow us to cluster well and I wanted to point out.",
                    "label": 1
                },
                {
                    "sent": "But actually the condition that I described today, the C epsilon property for objective Phi safer comedian was with epsilon properties.",
                    "label": 0
                },
                {
                    "sent": "Just an example of how the similarity information that's inaccurate information can be related to the.",
                    "label": 0
                },
                {
                    "sent": "This article grant of Clustering Effect might not be even the most natural one is one that we picked in order to connect to the approximation algorithms literature.",
                    "label": 1
                },
                {
                    "sent": "So we see epsilon properties actually just one example of a property, but one can analyze about and.",
                    "label": 0
                },
                {
                    "sent": "One can analyze many more properties, and in fact I wanted to mention.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in recent Doc actually have implementation.",
                    "label": 0
                },
                {
                    "sent": "Paula, we have developed a whole general theory about what natural properties in the similarity function.",
                    "label": 1
                },
                {
                    "sent": "And to allow us to cluster well and both algorithms.",
                    "label": 1
                },
                {
                    "sent": "And actually, every bloom will talk about this.",
                    "label": 0
                },
                {
                    "sent": "On Monday, so I. I definitely encourage you to attend the talk.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "So questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Actually asking about that property here is in some strange because it's probably working with datasets, not about invention.",
                    "label": 0
                },
                {
                    "sent": "Empirical data that you have supposed to property above is actually known.",
                    "label": 0
                },
                {
                    "sent": "I was talking about the inductive case with the property is satisfied.",
                    "label": 0
                },
                {
                    "sent": "So OK for most of the talk I talked about the transductive case where I assume that they have a fixed set of points.",
                    "label": 0
                },
                {
                    "sent": "Only on class are bad, but I also briefly mentioned the inductive case where actually assume that the property is satisfied over the whole set of points and actually try to so even for the epsilon property.",
                    "label": 0
                },
                {
                    "sent": "So I assume, but more generally.",
                    "label": 0
                },
                {
                    "sent": "This work, we're alive, marginal property.",
                    "label": 0
                },
                {
                    "sent": "What we do assume that the property that we talk about is satisfied with respect to the whole distribution and actually, technically speaking, is a challenge to show some time that the properties are satisfied about the sample.",
                    "label": 0
                },
                {
                    "sent": "We had one concrete case where it had to use very complicated regularity type arguments to show that they think I will talk about it.",
                    "label": 0
                },
                {
                    "sent": "This assumption relates or can be guaranteed by generative assumptions over the clusters.",
                    "label": 0
                },
                {
                    "sent": "Well, so OK, this particular one.",
                    "label": 0
                },
                {
                    "sent": "Probably OK, so the problem, for instance for generative ones.",
                    "label": 0
                },
                {
                    "sent": "Probably we need for instance in the case on hold mixture model, say Gaussian Spring, the component of the audience would be very far apart.",
                    "label": 0
                },
                {
                    "sent": "So if they are not very far apartment property not going to be satisfied.",
                    "label": 0
                },
                {
                    "sent": "But I don't think this is a drawback because you know, even in supervised learning we have generative models and non interactive models now.",
                    "label": 0
                },
                {
                    "sent": "And if you know that the data comes is generated by Mr. Gaussians, then you want to apply.",
                    "label": 0
                },
                {
                    "sent": "Very specific algorithm for the 4th case you need you want to use your assumptions.",
                    "label": 0
                },
                {
                    "sent": "Talking to someone, I don't think that.",
                    "label": 0
                },
                {
                    "sent": "So if you have a mixture model, for instance a mixture of Gaussians, but the data the property will be satisfied, only the components are very far apart with the components are not reasonably close and the data is probably not going to be satisfied.",
                    "label": 0
                },
                {
                    "sent": "Some other property that actually studying the work with every man, Santo should be satisfied, but even that is not the right approach to the problem that case.",
                    "label": 0
                },
                {
                    "sent": "Don't have it.",
                    "label": 0
                },
                {
                    "sent": "Well, I think that well state of the art team that.",
                    "label": 0
                },
                {
                    "sent": "Exactly what that is.",
                    "label": 0
                },
                {
                    "sent": "But I definitely think the algorithm for Mr Model would be better than using this approach.",
                    "label": 1
                },
                {
                    "sent": "But I don't know this is not a drill bit because the state of the art algorithm tomorrow going to work only the data is the mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And not in a much more general situations where we provide actually.",
                    "label": 0
                },
                {
                    "sent": "So in this work we should definitely encourage you to go to CVS stock, which I think we have a very nice general theory and then we saw analyzer, variety of condition on the similarity function and for any condition that similarity function we analyze what?",
                    "label": 0
                },
                {
                    "sent": "What is the right type of clustering algorithm that you want to be using.",
                    "label": 0
                },
                {
                    "sent": "So actually I think this is quite valuable.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, but we're going to talk about this kind of global statistical learning theory framework for the context of clustering, so it's something similar to the case we know for supervised classification.",
                    "label": 0
                },
                {
                    "sent": "If we know that it is the data.",
                    "label": 0
                },
                {
                    "sent": "Is there a separable we want to apply as yeah, more perceptron if it's not linearly separable, maybe maybe the target.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a good decision tree for the data I want to use it recently based algorithm and so on and that we doing this work we kind of come up with the analog of a type of analysis in the context of clustering or unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So I definitely encourage you to go in The Tempest.",
                    "label": 0
                },
                {
                    "sent": "But I have no questions.",
                    "label": 0
                },
                {
                    "sent": "Other comments.",
                    "label": 0
                },
                {
                    "sent": "Alright thank you man.",
                    "label": 0
                }
            ]
        }
    }
}