{
    "id": "4q4iz53oepadtrucu6jxiaty7hovlvut",
    "title": "Competing with the Empirical Risk Minimizer in a Single Pass",
    "info": {
        "author": [
            "Roy Frostig, Computer Science Department, Stanford University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_frostig_risk_minimizer/",
    "segmentation": [
        [
            "I am Roy from Stanford.",
            "This is joint work with Sham, Kakade, Ranga and Aaron Sidford at MSR."
        ],
        [
            "The work begins with a question of computational statistical tradeoffs at large scale, raised first by bumboo scanner influential 2008 work.",
            "And we're going to try and decrease the generalization error of a statistical learning problem.",
            "So formally, we want to find W stars.",
            "The minimizer of the expected risk, which is the amount of the amount of prediction loss that we expect to incur on some data point from some data distribution.",
            "And where when we're doing this, we're going to be given exactly N samples from a data from the data distribution D."
        ],
        [
            "So I want to stress that this is the generalization error and not necessarily.",
            "We're not necessarily trying to minimize this empirical approximation on the in sample.",
            "So there are two approaches to consider for this problem.",
            "One is to treat the problem as a stochastic, optimal approximate approximate approximation problem and run an algorithm such as stochastic gradient descent on it.",
            "Usually these algorithms are incremental.",
            "They support the streaming setting.",
            "They're simple to implement, and they on the empirical risk minimization problem.",
            "They may achieve somewhat poor numerical accuracy, and do so quickly.",
            "An alternative approach is more in this fashion of numerical analysis.",
            "Take a slightly more heavyweight algorithm like BFG S. Which with with the purpose of minimizing the empirical risk or finding or at least attaining good numerical accuracy on that problem, this might take longer as the algorithm has to hold more data in memory, make several passes over the data set, and arguably has a more tricky implementation."
        ],
        [
            "So.",
            "In order to compare these two approaches, we're going to set.",
            "AAAAA metric, which is going to be given by the competitive ratio between on bottom, the expected excess risk of our algorithm which produces W sub N. On and samples and the actual empirical risk minimizer analytically on N samples, that's on top.",
            "So other than just comparing these two, the empirical risk minimizer also or this ratio rather gives a good benchmark for any particular problem, because because it's actually often the sort of the.",
            "The statistical estimator of choice given sort of absent any computational considerations.",
            "And in many cases it might be.",
            "I mean it can respond to the to the maximum likelihood estimator.",
            "In some cases it's somehow optimal and others barring techniques such as shrinkage or James Stein estimation.",
            "So can we take this ratio and bring it to one with some algorithm?",
            "But actually that actually consider computational constraints such as memory, running time, or a desire for parallelization."
        ],
        [
            "So the main result of the of our paper is that we in fact provide a streaming algorithm which under regularity assumptions on the underlying problem, basically achieves all of the desired items so far.",
            "Computationally, it runs in a single pass with dimension, that is that scale with memory that scales with the dimension.",
            "It's trivially parallelizable.",
            "Statistically, it achieves the rate of the ERM or the best fit on every problem that satisfies these assumptions.",
            "It does so in finite sample.",
            "And finally, it's an iterative algorithm, so it depends on the initial error in some way, and its dependence is squashed in a super polynomial can be squashed super polynomially.",
            "So for more come to the poster.",
            "Thank you to my wonderful kalab"
        ],
        [
            "Raiders"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am Roy from Stanford.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Sham, Kakade, Ranga and Aaron Sidford at MSR.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The work begins with a question of computational statistical tradeoffs at large scale, raised first by bumboo scanner influential 2008 work.",
                    "label": 0
                },
                {
                    "sent": "And we're going to try and decrease the generalization error of a statistical learning problem.",
                    "label": 0
                },
                {
                    "sent": "So formally, we want to find W stars.",
                    "label": 0
                },
                {
                    "sent": "The minimizer of the expected risk, which is the amount of the amount of prediction loss that we expect to incur on some data point from some data distribution.",
                    "label": 0
                },
                {
                    "sent": "And where when we're doing this, we're going to be given exactly N samples from a data from the data distribution D.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to stress that this is the generalization error and not necessarily.",
                    "label": 0
                },
                {
                    "sent": "We're not necessarily trying to minimize this empirical approximation on the in sample.",
                    "label": 0
                },
                {
                    "sent": "So there are two approaches to consider for this problem.",
                    "label": 0
                },
                {
                    "sent": "One is to treat the problem as a stochastic, optimal approximate approximate approximation problem and run an algorithm such as stochastic gradient descent on it.",
                    "label": 0
                },
                {
                    "sent": "Usually these algorithms are incremental.",
                    "label": 0
                },
                {
                    "sent": "They support the streaming setting.",
                    "label": 0
                },
                {
                    "sent": "They're simple to implement, and they on the empirical risk minimization problem.",
                    "label": 1
                },
                {
                    "sent": "They may achieve somewhat poor numerical accuracy, and do so quickly.",
                    "label": 1
                },
                {
                    "sent": "An alternative approach is more in this fashion of numerical analysis.",
                    "label": 0
                },
                {
                    "sent": "Take a slightly more heavyweight algorithm like BFG S. Which with with the purpose of minimizing the empirical risk or finding or at least attaining good numerical accuracy on that problem, this might take longer as the algorithm has to hold more data in memory, make several passes over the data set, and arguably has a more tricky implementation.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In order to compare these two approaches, we're going to set.",
                    "label": 1
                },
                {
                    "sent": "AAAAA metric, which is going to be given by the competitive ratio between on bottom, the expected excess risk of our algorithm which produces W sub N. On and samples and the actual empirical risk minimizer analytically on N samples, that's on top.",
                    "label": 0
                },
                {
                    "sent": "So other than just comparing these two, the empirical risk minimizer also or this ratio rather gives a good benchmark for any particular problem, because because it's actually often the sort of the.",
                    "label": 1
                },
                {
                    "sent": "The statistical estimator of choice given sort of absent any computational considerations.",
                    "label": 1
                },
                {
                    "sent": "And in many cases it might be.",
                    "label": 1
                },
                {
                    "sent": "I mean it can respond to the to the maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "In some cases it's somehow optimal and others barring techniques such as shrinkage or James Stein estimation.",
                    "label": 0
                },
                {
                    "sent": "So can we take this ratio and bring it to one with some algorithm?",
                    "label": 1
                },
                {
                    "sent": "But actually that actually consider computational constraints such as memory, running time, or a desire for parallelization.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main result of the of our paper is that we in fact provide a streaming algorithm which under regularity assumptions on the underlying problem, basically achieves all of the desired items so far.",
                    "label": 0
                },
                {
                    "sent": "Computationally, it runs in a single pass with dimension, that is that scale with memory that scales with the dimension.",
                    "label": 0
                },
                {
                    "sent": "It's trivially parallelizable.",
                    "label": 0
                },
                {
                    "sent": "Statistically, it achieves the rate of the ERM or the best fit on every problem that satisfies these assumptions.",
                    "label": 1
                },
                {
                    "sent": "It does so in finite sample.",
                    "label": 0
                },
                {
                    "sent": "And finally, it's an iterative algorithm, so it depends on the initial error in some way, and its dependence is squashed in a super polynomial can be squashed super polynomially.",
                    "label": 0
                },
                {
                    "sent": "So for more come to the poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you to my wonderful kalab",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Raiders",
                    "label": 0
                }
            ]
        }
    }
}