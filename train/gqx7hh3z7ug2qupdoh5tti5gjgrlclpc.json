{
    "id": "gqx7hh3z7ug2qupdoh5tti5gjgrlclpc",
    "title": "Top-k Frequent Itemsets via Differentially Private FP-trees",
    "info": {
        "author": [
            "Jaewoo Lee, Department of Computer Science, Purdue University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_lee_fp_trees/",
    "segmentation": [
        [
            "Hello everyone, my name is Jerry and this work was done while I was a PhD student at Purdue University and this is the joint work with my advisor, Professor Clifton.",
            "OK."
        ],
        [
            "Title of my paper is Top K frequent itemset via differentially private tapestries.",
            "Because previous two speakers before me provided good background knowledge on differential privacy and introduced the concept of sensitivity, I will just skip that background materials and I will just directly get to the point.",
            "So frequent itemset mining is one of the wells started area and there has been introduced many algorithms an the task of frequent itemset mining given a set of items we want to find.",
            "Items whose support is greater than or equal to the threshold Tao here.",
            "And we want to find top K frequent item sets.",
            "OK, so in order to determine whether an item set is frequently or not, we have to access the database and ask the support open items at and compare it with the threshold.",
            "So basically this is equal to asking or threshold query to the database.",
            "OK, so here's some challenges for each item set we need to access the database an under the differential privacy.",
            "Each database access requires using some privacy budget.",
            "And the problem is, here we have exponentially many items as two tasks.",
            "That means we have to ask the database exponentially many queries, so answering exponentially many queries with differential privacy is very difficult problem.",
            "And here's the another problem.",
            "So no matter what, no matter if the item set is frequent or infrequent, we always have to pay the private spirit, but we don't want to waste our privacy budget to find an interesting patterns.",
            "So we introduce new methods which can avoid this problem so we don't pay the privacy budget for finding infrequent patterns.",
            "OK."
        ],
        [
            "OK, so we propose a new method called noisy Cut an it consists of two phases.",
            "In the first phase is we make a user of sparse vector technique to identify frequent itemsets, but at this time we identified frequent itemset without knowing the support of each item set.",
            "Given the given the frequent itemset, the second phase is.",
            "To derive their noisy supports.",
            "So in order to derive their knowledge support, we make use of FP3 data structure.",
            "So for each maximal frequent itemset we build an Apple tree.",
            "Apple Tree and we drive the.",
            "So supports of all the subset of maximal frequent items that.",
            "An as an optional step we imposed.",
            "Consistency constraint on the tapestry data structure, and it has turned out imposing the constancy not only make it consistent, but also increase the accuracy of the noisy outputs."
        ],
        [
            "OK, people are introducing our main algorithm.",
            "I will provide some background not the background but some intuition behind the our algorithm.",
            "So here for the same simplicity, I just put three items as an here, this sub one and sub to represent two neighboring database.",
            "So D2 is obtained by adding one more transaction to D1 Anne.",
            "And vice versa.",
            "D1 is also obtained by removing one transaction from D2.",
            "OK, so there are only two cases, so neighboring database is obtained by just adding one more transaction.",
            "So the difference between their support between these two neighboring database is whether either exactly one or there's no difference at all.",
            "So let's look at the this item sets whose support is far away from the threshold Tau.",
            "Here, if you look at the X item set X1 or X2, their support is far away from the threshold Tau.",
            "Even if you add or remove one more transaction, their support only increase or decrease by exactly 1, and that doesn't change the change the answer.",
            "So that means.",
            "Frequent itemset still remain frequent an the infrequent ones still remain infrequent, but the problem is here when the support of the item set is near the threshold.",
            "Adding one more transaction can switch the frequent itemset.",
            "Sorry can switch the infrequent items set to the frequent itemset and by sparser, so the main idea is that what if we decrease the threshold by 1 then?",
            "XX3 was infrequent in in database D1, but after we decrease the threshold it has become the frequent, so the frequent itemset between these two neighboring databases, Now X1 and X3.",
            "So they are the same, so that is indistinguishable.",
            "So the main idea of our method is to perturb the threshold so that the adversary has some uncertainty on the threshold.",
            "OK."
        ],
        [
            "Here is our method, so our algorithm start by calculating the noise threshold.",
            "So given the threshold Tau, we add Laplacian noise to it to get the noise threshold an for each item set in the items.",
            "That lattice, we calculate the noisy support and we compare it with the noise threshold.",
            "If it is greater than or equal to the noise threshold, we regard that item set is frequent otherwise.",
            "It's regarded as infrequent and in the end of this algorithm, what we'll get is a binary vector, and each element is either one or 01.",
            "Represent a frequent itemset Geo represent infrequent items set.",
            "OK, this can be.",
            "So as.",
            "Splitting the items at lattice into two parts, frequent and infrequent, so the name of the algorithm is noisy cut."
        ],
        [
            "OK, I will provide some privacy proof that detailed through but just sketch.",
            "We fixed the two neighboring database, this sub one and sub two and we run the algorithm over these two neighboring database.",
            "But what we have to notice is that the threshold we are using is not a constant anymore so it's now on random variable because we added Laplacian noise to it.",
            "So Thresh noise threshold.",
            "It is being used between these two neighboring databases are different.",
            "OK, so we fixed an output which is binary vector and we calculate the probability of having the same output having the same binary vector between these two neighboring database and we show that that probably ratio is bounded by it to the epsilon."
        ],
        [
            "OK, so the.",
            "I want you to recall that the threshold we are using is not a constant anymore, but a random variable and it will because we added the Laplacian noise to it is a lot plush and then the variable and it also satisfies the differential privacy.",
            "OK, so the probability of having this binary vector as an output can be decomposed to the equation shown in the slide an and there there exist 2 cases.",
            "The support over item set between two neighboring database might be the same or they are they differ by exactly 1.",
            "So in the first case the support of the items at X in database D1 is equal to that in database D2.",
            "So we can replace the support knowledge support of the items at XI to the node support of idem set X2.",
            "No, it is support of items that XI in D2 OK, and because the noise threshold satisfies differential privacy, there's they hold the inequality and the probability of having a specific noise threshold between these two neighboring database is bounded by it to the epsilon.",
            "So we can replace the probability of Tao head equal to X.",
            "With with the probability of Tau heads up to equal to X, so we can apply the same.",
            "Sent technique, I mean we can apply the same argument for the case where the support support over item says differs by 1.",
            "OK, so.",
            "OK."
        ],
        [
            "OK, yeah, the output of the our first phase is a set of frequent item sets, but we don't know the noisy support of each item set.",
            "The second phase is to calculate the noisy support of the frequent items that we have already identified.",
            "So in order to derive this knowledge support of each item set.",
            "So we build an Apple tree and we build this Apple tree for each maximal frequent itemset.",
            "So given a set of frequent itemset.",
            "It is easy to calculate the Max a set of maximal frequent itemsets, and here in this example the maximal frequent itemset is ACD.",
            "OK, so first we we insert.",
            "I mean we create a node for.",
            "All the subset of the maximal frequent itemset and we initialize the counts of each node with a lot less than addendum noise.",
            "So here the numbers in the red color represent the noise.",
            "That's not the account, and the next step is to updating these three data structure.",
            "So for each transaction in the database.",
            "And the given the maximum frequent itemset we calculate the intersection between the maximum frequency item set and each transaction.",
            "So in this example, let's look at.",
            "The first transaction.",
            "Is a CDE an we?",
            "We calculate the intersection with the maximum frequency at a CD.",
            "So what we'll get is a CD and we we match that transform transaction to the path in the Apple tree so it matches to the path a CD.",
            "But here what is different from the original Apple Tree algorithm is that in the original Apple tree, it always is always the increased count of the all node.",
            "In the matching path.",
            "But in our algorithm, we only increase the count of node.",
            "The last node in the matching path as this is to ensure that so that way we can ensure that the sensitivity of updating this Apple tree is just one, because for each transaction we only increase the count of just one node, right?",
            "So the sensitivity of updating this.",
            "Apple Tree data structure is equal to 1.",
            "So for the remaining transactions, let's see the second transaction."
        ],
        [
            "It is a CD, so intersection with maximal frequent itemset is also a CD, so we increase the count of the D node on the ACD by one.",
            "So if we repeat this process, what we'll get is.",
            "This this Apple tree data structure, so the numbers in the blue represent the it's it's true it's counts and the numbers in red represent the noise.",
            "So the count in.",
            "Count maintained in each node is just summation of its true count, an it's noise and noise.",
            "OK."
        ],
        [
            "Because we only update the count of the last node in the node unless node in Apple tree to get the correct account we have to propagate the count of the children to its parents.",
            "OK, this is the noise propagation stage, an after propagating the old accounts of the children to its parent.",
            "We will get the noisy apetri.",
            "So which satisfy differential privacy and the remaining step.",
            "Also remaining step is the same with the original Apetri algorithm.",
            "So so given the Apple tree, so we can request recursively find all the frequent itemsets and its supports as well."
        ],
        [
            "OK, so this is an optional step.",
            "To impose the consistency in the Apple tree, so in the Apple tree, we all know that the count of the child cannot be greater than that of the parents, right?",
            "But because he added Laplacian random noise to each node, this constraint may be violated, so two imposed to make this Apple tree in consistence, we can.",
            "Formulate that imposing consistent step as an optimizing optimization problem."
        ],
        [
            "So here the inequality represents the constraint and we can build metrics C which represent the orders.",
            "All the systems of linear inequalities.",
            "And we want to find the.",
            "Find the tree which is constant and that is close to the original app Tree.",
            "So if we solve this problem optimization problem, what we'll get is the consistence noisy apetri.",
            "So to evaluate the performance of the proposed method, we picked these datasets from the UCI machine Learning Repository an some from this, some sources and we compare it with the two state of the art algorithm.",
            "One is the private basis and the 2nd is smart truncation."
        ],
        [
            "And we first compare the average score of the prior to algorithms.",
            "An hour method an when the epsilon is large, both private basis and the proposed method performs well, but as we decrease the value of epsilon, the private base starts to.",
            "I mean the performance or private basis start starting starts to degrade, while our performance of our.",
            "Method remains the same.",
            "OK."
        ],
        [
            "So that represent our proposed method is reversed.",
            "And this represents the relative error.",
            "So we calculate the distance between our knowledge support and the true support an.",
            "And we compare that through the old datasets and the private basis an the proposed method shows the similar performance in terms of relative errors."
        ],
        [
            "OK, so in conclusion we have proposed we have proposed an algorithm reaching only pays the privacy budget for finding interesting patterns an.",
            "And we can answer potential potentially exponentially many threshold queries using the proposed method.",
            "And we believe this technique will be very useful for other applications.",
            "Anne, this is the end of my presentation.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Jerry and this work was done while I was a PhD student at Purdue University and this is the joint work with my advisor, Professor Clifton.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Title of my paper is Top K frequent itemset via differentially private tapestries.",
                    "label": 1
                },
                {
                    "sent": "Because previous two speakers before me provided good background knowledge on differential privacy and introduced the concept of sensitivity, I will just skip that background materials and I will just directly get to the point.",
                    "label": 0
                },
                {
                    "sent": "So frequent itemset mining is one of the wells started area and there has been introduced many algorithms an the task of frequent itemset mining given a set of items we want to find.",
                    "label": 0
                },
                {
                    "sent": "Items whose support is greater than or equal to the threshold Tao here.",
                    "label": 0
                },
                {
                    "sent": "And we want to find top K frequent item sets.",
                    "label": 1
                },
                {
                    "sent": "OK, so in order to determine whether an item set is frequently or not, we have to access the database and ask the support open items at and compare it with the threshold.",
                    "label": 0
                },
                {
                    "sent": "So basically this is equal to asking or threshold query to the database.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's some challenges for each item set we need to access the database an under the differential privacy.",
                    "label": 0
                },
                {
                    "sent": "Each database access requires using some privacy budget.",
                    "label": 0
                },
                {
                    "sent": "And the problem is, here we have exponentially many items as two tasks.",
                    "label": 0
                },
                {
                    "sent": "That means we have to ask the database exponentially many queries, so answering exponentially many queries with differential privacy is very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "And here's the another problem.",
                    "label": 0
                },
                {
                    "sent": "So no matter what, no matter if the item set is frequent or infrequent, we always have to pay the private spirit, but we don't want to waste our privacy budget to find an interesting patterns.",
                    "label": 0
                },
                {
                    "sent": "So we introduce new methods which can avoid this problem so we don't pay the privacy budget for finding infrequent patterns.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we propose a new method called noisy Cut an it consists of two phases.",
                    "label": 0
                },
                {
                    "sent": "In the first phase is we make a user of sparse vector technique to identify frequent itemsets, but at this time we identified frequent itemset without knowing the support of each item set.",
                    "label": 1
                },
                {
                    "sent": "Given the given the frequent itemset, the second phase is.",
                    "label": 0
                },
                {
                    "sent": "To derive their noisy supports.",
                    "label": 0
                },
                {
                    "sent": "So in order to derive their knowledge support, we make use of FP3 data structure.",
                    "label": 0
                },
                {
                    "sent": "So for each maximal frequent itemset we build an Apple tree.",
                    "label": 0
                },
                {
                    "sent": "Apple Tree and we drive the.",
                    "label": 0
                },
                {
                    "sent": "So supports of all the subset of maximal frequent items that.",
                    "label": 0
                },
                {
                    "sent": "An as an optional step we imposed.",
                    "label": 0
                },
                {
                    "sent": "Consistency constraint on the tapestry data structure, and it has turned out imposing the constancy not only make it consistent, but also increase the accuracy of the noisy outputs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, people are introducing our main algorithm.",
                    "label": 0
                },
                {
                    "sent": "I will provide some background not the background but some intuition behind the our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here for the same simplicity, I just put three items as an here, this sub one and sub to represent two neighboring database.",
                    "label": 0
                },
                {
                    "sent": "So D2 is obtained by adding one more transaction to D1 Anne.",
                    "label": 0
                },
                {
                    "sent": "And vice versa.",
                    "label": 0
                },
                {
                    "sent": "D1 is also obtained by removing one transaction from D2.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are only two cases, so neighboring database is obtained by just adding one more transaction.",
                    "label": 0
                },
                {
                    "sent": "So the difference between their support between these two neighboring database is whether either exactly one or there's no difference at all.",
                    "label": 1
                },
                {
                    "sent": "So let's look at the this item sets whose support is far away from the threshold Tau.",
                    "label": 1
                },
                {
                    "sent": "Here, if you look at the X item set X1 or X2, their support is far away from the threshold Tau.",
                    "label": 0
                },
                {
                    "sent": "Even if you add or remove one more transaction, their support only increase or decrease by exactly 1, and that doesn't change the change the answer.",
                    "label": 0
                },
                {
                    "sent": "So that means.",
                    "label": 0
                },
                {
                    "sent": "Frequent itemset still remain frequent an the infrequent ones still remain infrequent, but the problem is here when the support of the item set is near the threshold.",
                    "label": 0
                },
                {
                    "sent": "Adding one more transaction can switch the frequent itemset.",
                    "label": 0
                },
                {
                    "sent": "Sorry can switch the infrequent items set to the frequent itemset and by sparser, so the main idea is that what if we decrease the threshold by 1 then?",
                    "label": 0
                },
                {
                    "sent": "XX3 was infrequent in in database D1, but after we decrease the threshold it has become the frequent, so the frequent itemset between these two neighboring databases, Now X1 and X3.",
                    "label": 0
                },
                {
                    "sent": "So they are the same, so that is indistinguishable.",
                    "label": 0
                },
                {
                    "sent": "So the main idea of our method is to perturb the threshold so that the adversary has some uncertainty on the threshold.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is our method, so our algorithm start by calculating the noise threshold.",
                    "label": 0
                },
                {
                    "sent": "So given the threshold Tau, we add Laplacian noise to it to get the noise threshold an for each item set in the items.",
                    "label": 0
                },
                {
                    "sent": "That lattice, we calculate the noisy support and we compare it with the noise threshold.",
                    "label": 0
                },
                {
                    "sent": "If it is greater than or equal to the noise threshold, we regard that item set is frequent otherwise.",
                    "label": 0
                },
                {
                    "sent": "It's regarded as infrequent and in the end of this algorithm, what we'll get is a binary vector, and each element is either one or 01.",
                    "label": 0
                },
                {
                    "sent": "Represent a frequent itemset Geo represent infrequent items set.",
                    "label": 0
                },
                {
                    "sent": "OK, this can be.",
                    "label": 0
                },
                {
                    "sent": "So as.",
                    "label": 0
                },
                {
                    "sent": "Splitting the items at lattice into two parts, frequent and infrequent, so the name of the algorithm is noisy cut.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I will provide some privacy proof that detailed through but just sketch.",
                    "label": 0
                },
                {
                    "sent": "We fixed the two neighboring database, this sub one and sub two and we run the algorithm over these two neighboring database.",
                    "label": 1
                },
                {
                    "sent": "But what we have to notice is that the threshold we are using is not a constant anymore so it's now on random variable because we added Laplacian noise to it.",
                    "label": 0
                },
                {
                    "sent": "So Thresh noise threshold.",
                    "label": 0
                },
                {
                    "sent": "It is being used between these two neighboring databases are different.",
                    "label": 0
                },
                {
                    "sent": "OK, so we fixed an output which is binary vector and we calculate the probability of having the same output having the same binary vector between these two neighboring database and we show that that probably ratio is bounded by it to the epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the.",
                    "label": 0
                },
                {
                    "sent": "I want you to recall that the threshold we are using is not a constant anymore, but a random variable and it will because we added the Laplacian noise to it is a lot plush and then the variable and it also satisfies the differential privacy.",
                    "label": 0
                },
                {
                    "sent": "OK, so the probability of having this binary vector as an output can be decomposed to the equation shown in the slide an and there there exist 2 cases.",
                    "label": 0
                },
                {
                    "sent": "The support over item set between two neighboring database might be the same or they are they differ by exactly 1.",
                    "label": 0
                },
                {
                    "sent": "So in the first case the support of the items at X in database D1 is equal to that in database D2.",
                    "label": 0
                },
                {
                    "sent": "So we can replace the support knowledge support of the items at XI to the node support of idem set X2.",
                    "label": 0
                },
                {
                    "sent": "No, it is support of items that XI in D2 OK, and because the noise threshold satisfies differential privacy, there's they hold the inequality and the probability of having a specific noise threshold between these two neighboring database is bounded by it to the epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we can replace the probability of Tao head equal to X.",
                    "label": 0
                },
                {
                    "sent": "With with the probability of Tau heads up to equal to X, so we can apply the same.",
                    "label": 0
                },
                {
                    "sent": "Sent technique, I mean we can apply the same argument for the case where the support support over item says differs by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, yeah, the output of the our first phase is a set of frequent item sets, but we don't know the noisy support of each item set.",
                    "label": 0
                },
                {
                    "sent": "The second phase is to calculate the noisy support of the frequent items that we have already identified.",
                    "label": 0
                },
                {
                    "sent": "So in order to derive this knowledge support of each item set.",
                    "label": 0
                },
                {
                    "sent": "So we build an Apple tree and we build this Apple tree for each maximal frequent itemset.",
                    "label": 1
                },
                {
                    "sent": "So given a set of frequent itemset.",
                    "label": 1
                },
                {
                    "sent": "It is easy to calculate the Max a set of maximal frequent itemsets, and here in this example the maximal frequent itemset is ACD.",
                    "label": 0
                },
                {
                    "sent": "OK, so first we we insert.",
                    "label": 0
                },
                {
                    "sent": "I mean we create a node for.",
                    "label": 0
                },
                {
                    "sent": "All the subset of the maximal frequent itemset and we initialize the counts of each node with a lot less than addendum noise.",
                    "label": 0
                },
                {
                    "sent": "So here the numbers in the red color represent the noise.",
                    "label": 0
                },
                {
                    "sent": "That's not the account, and the next step is to updating these three data structure.",
                    "label": 0
                },
                {
                    "sent": "So for each transaction in the database.",
                    "label": 0
                },
                {
                    "sent": "And the given the maximum frequent itemset we calculate the intersection between the maximum frequency item set and each transaction.",
                    "label": 0
                },
                {
                    "sent": "So in this example, let's look at.",
                    "label": 0
                },
                {
                    "sent": "The first transaction.",
                    "label": 0
                },
                {
                    "sent": "Is a CDE an we?",
                    "label": 0
                },
                {
                    "sent": "We calculate the intersection with the maximum frequency at a CD.",
                    "label": 0
                },
                {
                    "sent": "So what we'll get is a CD and we we match that transform transaction to the path in the Apple tree so it matches to the path a CD.",
                    "label": 0
                },
                {
                    "sent": "But here what is different from the original Apple Tree algorithm is that in the original Apple tree, it always is always the increased count of the all node.",
                    "label": 0
                },
                {
                    "sent": "In the matching path.",
                    "label": 0
                },
                {
                    "sent": "But in our algorithm, we only increase the count of node.",
                    "label": 0
                },
                {
                    "sent": "The last node in the matching path as this is to ensure that so that way we can ensure that the sensitivity of updating this Apple tree is just one, because for each transaction we only increase the count of just one node, right?",
                    "label": 0
                },
                {
                    "sent": "So the sensitivity of updating this.",
                    "label": 0
                },
                {
                    "sent": "Apple Tree data structure is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So for the remaining transactions, let's see the second transaction.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a CD, so intersection with maximal frequent itemset is also a CD, so we increase the count of the D node on the ACD by one.",
                    "label": 0
                },
                {
                    "sent": "So if we repeat this process, what we'll get is.",
                    "label": 0
                },
                {
                    "sent": "This this Apple tree data structure, so the numbers in the blue represent the it's it's true it's counts and the numbers in red represent the noise.",
                    "label": 0
                },
                {
                    "sent": "So the count in.",
                    "label": 0
                },
                {
                    "sent": "Count maintained in each node is just summation of its true count, an it's noise and noise.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we only update the count of the last node in the node unless node in Apple tree to get the correct account we have to propagate the count of the children to its parents.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the noise propagation stage, an after propagating the old accounts of the children to its parent.",
                    "label": 1
                },
                {
                    "sent": "We will get the noisy apetri.",
                    "label": 0
                },
                {
                    "sent": "So which satisfy differential privacy and the remaining step.",
                    "label": 0
                },
                {
                    "sent": "Also remaining step is the same with the original Apetri algorithm.",
                    "label": 1
                },
                {
                    "sent": "So so given the Apple tree, so we can request recursively find all the frequent itemsets and its supports as well.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is an optional step.",
                    "label": 0
                },
                {
                    "sent": "To impose the consistency in the Apple tree, so in the Apple tree, we all know that the count of the child cannot be greater than that of the parents, right?",
                    "label": 0
                },
                {
                    "sent": "But because he added Laplacian random noise to each node, this constraint may be violated, so two imposed to make this Apple tree in consistence, we can.",
                    "label": 0
                },
                {
                    "sent": "Formulate that imposing consistent step as an optimizing optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here the inequality represents the constraint and we can build metrics C which represent the orders.",
                    "label": 0
                },
                {
                    "sent": "All the systems of linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "And we want to find the.",
                    "label": 0
                },
                {
                    "sent": "Find the tree which is constant and that is close to the original app Tree.",
                    "label": 0
                },
                {
                    "sent": "So if we solve this problem optimization problem, what we'll get is the consistence noisy apetri.",
                    "label": 0
                },
                {
                    "sent": "So to evaluate the performance of the proposed method, we picked these datasets from the UCI machine Learning Repository an some from this, some sources and we compare it with the two state of the art algorithm.",
                    "label": 0
                },
                {
                    "sent": "One is the private basis and the 2nd is smart truncation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we first compare the average score of the prior to algorithms.",
                    "label": 0
                },
                {
                    "sent": "An hour method an when the epsilon is large, both private basis and the proposed method performs well, but as we decrease the value of epsilon, the private base starts to.",
                    "label": 0
                },
                {
                    "sent": "I mean the performance or private basis start starting starts to degrade, while our performance of our.",
                    "label": 0
                },
                {
                    "sent": "Method remains the same.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that represent our proposed method is reversed.",
                    "label": 0
                },
                {
                    "sent": "And this represents the relative error.",
                    "label": 0
                },
                {
                    "sent": "So we calculate the distance between our knowledge support and the true support an.",
                    "label": 0
                },
                {
                    "sent": "And we compare that through the old datasets and the private basis an the proposed method shows the similar performance in terms of relative errors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion we have proposed we have proposed an algorithm reaching only pays the privacy budget for finding interesting patterns an.",
                    "label": 0
                },
                {
                    "sent": "And we can answer potential potentially exponentially many threshold queries using the proposed method.",
                    "label": 0
                },
                {
                    "sent": "And we believe this technique will be very useful for other applications.",
                    "label": 1
                },
                {
                    "sent": "Anne, this is the end of my presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}