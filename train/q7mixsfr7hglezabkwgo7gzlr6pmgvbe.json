{
    "id": "q7mixsfr7hglezabkwgo7gzlr6pmgvbe",
    "title": "Spotlights 2",
    "info": {
        "published": "Dec. 3, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_various_s2/",
    "segmentation": [
        [
            "OK, the first spotlight is Indian buffet processors with power law behavior.",
            "Hi, I'm not many naturally occurring phenomena.",
            "Exhibit exhibit power law behavior.",
            "So for example, if you look at the occurrences of words in a document corpus, you see that there will be a few words which occur in lots of the documents and lots of rare words which occur only in in a few documents and the frequency of occurrences follow a power law.",
            "Recently there has been a number of efforts trying to model power law behavior with manual processes in the context of language modeling and in clustering.",
            "Here will do the same for latent feature models.",
            "In particular, will be working in the framework of Indian buffet processes which are based on parametric priors introduced by Tom Griffiths.",
            "Anzovin Ghahramani for latent feature models with infinitely many features.",
            "In this work we extend the Indian buffet process to a tree parameter family and showed that the extension has power law behavior.",
            "The generative processes as given on the slides.",
            "So we showed our extension is also infinitely exchangeable, just As for the typical Indian buffet process and definitely measure is given by a novel stochastic process, which we call the stable beta process, come to our post number T20 to learn all about this, including the relationship to Pitman, yor processes and an application to modeling would occurrences in document corpus.",
            "Also, if you're interested in learning more basic nonparametrics.",
            "Come to our workshop up in Whistler on Saturday.",
            "Thank you.",
            "Next"
        ],
        [
            "Is submodularity cuts in applications?",
            "I have some other function is a set function that has similar properties to combat function and the problem of maximizing dysfunction appears in many machine learning problems, But is known to be pretty difficult to solve because it has exponentially large number of locally optimal solutions like convex maximization problem.",
            "In this paper we propose an algorithm for maximizing submodular function based on a cutting plane methods which is implemented as undulated binary integer.",
            "Doing programming procedure with increasing constraints.",
            "The proposed algorithm that we called submodularity cats algorithm has a guaranteed to converge into an exact solution in a finding the number of iterations and it also can estimate upper and lower bounds at the at any iteration as shown in the figure says the accuracy of the current social can be estimated at any point.",
            "Please come to our poster.",
            "Thank you.",
            "Next, we have statistical models of linear and nonlinear contextual interactions in early visual processing."
        ],
        [
            "Good afternoon, so the visual cortex, which is this part around here, is the largest and best visual area.",
            "Best started visual area of the primate brain.",
            "And yet it's not clear what function it subserves, so one prominent computational theory due to Stoppingly suggests that.",
            "That it computes the salience of visual images.",
            "Critical function for early vision.",
            "Since it can be used to direct visual attention.",
            "So to address the issue of how to reconcile these ideas with ideas about statistical processing and representation of sensory inputs in biological systems.",
            "And in the poster we will show that there is a wide range of neural and perceptual phenomena such as.",
            "Contrast and orientation contrast effect in neurons as well as saliency and contour integration.",
            "Perceptual effect and tilt processing.",
            "Texture processing, so such a wide range of effects, which can be in comparison under the umbrella of generative model of image statistics.",
            "To learn more about the model and simulations, please come by the poster.",
            "67 thanks.",
            "Next up, STD P enables spiking neurons to detect hidden causes of their inputs."
        ],
        [
            "Hi.",
            "The question we addressed in our work is how does spike timing dependent plasticity contributes to a high level learning algorithm?",
            "They present a single layer feedforward architecture with population coded inputs and a probabilistic software.",
            "Not take all output activation and in this circuit we let STP learn the presented input spike train in a completely unsupervised setting.",
            "The postulates that the synaptic potentiation is proportional to the exponential of the current synaptic rate, whereas the synaptic depression is independent of the current wait.",
            "This is also supported by experiments.",
            "We use rigorous mathematical analysis to show that the whole circuit implemented stochastic version of expectation maximization, the maximization step.",
            "Them step is just to wait update induced by SDP.",
            "The expectation step emerges from the forward operation of the circuit which produces a probabilistic population encoded representation of the latent variable which indeed represents the hidden costs of the current spiking input.",
            "Come to us and discuss this new view of SDP and Spike based expectation maximization.",
            "At post 69.",
            "Next up 3D object recognition with deep belief Nets."
        ],
        [
            "Hi, this is joint work with Geoff Hinton and as Jeff explained earlier today it deep belief Net is a generative model and type of generative model that has an undirected model at the top followed by a few one or more layers of directed connections below it.",
            "So in this paper we present a bunch of improvements over the original DBN model that was proposed a few years back.",
            "The first improvement is.",
            "A new top level model that learns separate sets of features for each of the object classes that we are interested in recognizing.",
            "So this is an example of 1/3 order of Boltzmann machine that I've talked about today.",
            "Another improvement is.",
            "A learning algorithm for the top level model that combines discriminative and generative gradients to train the weights.",
            "The nice thing about this algorithm is that it avoids the problem of poor Markov chain mixing that affected the classification performance of top level models in earlier DBN's.",
            "So we take this improved DBN and applied to recognizing 3D objects from stereo pair images.",
            "We use the NORB object recognition data set to measure performance and on the standard normal task we beat SVM.",
            "Zayn get close to convolutional neural Nets which have built in prior knowledge about images.",
            "So.",
            "One of the big advantages of DBN's is that you can train it semi supervised, so we consider a modified version of the North task where we allow unlabeled images to be used as extra training data and on this modified task we beat didn't beat convolutional neural Nets.",
            "So if you're interested in deep belief Nets or object recognition or both, please come by or poster.",
            "And even if you're not familiar with the details of the BMS, please come by and we will be happy to give you a little intro of deviance.",
            "Thanks."
        ],
        [
            "Next up maximum maximum likelihood trajectories for continuous time Markov chains.",
            "Hello so I work in terms concerns continuous time Markov shades.",
            "These are models of dynamical systems that have a discrete states set and they transition stochastically between between states just like a Markov chain, but they remain or dwell in their states for real valued amounts of time.",
            "There are exponentially distributed.",
            "These models are used in many domains including my favorite area which is the last stochastic molecular dynamics.",
            "They used to model ion channels using pilot index, queueing theory, lots of domains.",
            "So we studied 2 problems for these for this type of model, which happens off ages ago for discrete time models.",
            "But if you're not even addressed for continuous time Markov chains, so in the first version of the problem you're given the state of the system at two different times, and you want to infer the most likely trajectory the system took between those two times.",
            "That means the most likely sequence of state transitions, and the most likely dwell times, and in the second version of the problem.",
            "Extending that problem, then you can assume that you've received a sequence of observations at the system, which may be noisy partial observations.",
            "And again, you want to infer the most likely trajectory, so be sure that for both of these problems there are efficient dynamic programming solutions.",
            "However, the solutions are a bit weird sometimes, and if you want to know what that means, please come to the post here, thank you.",
            "Next up time varying dynamic Bashan net."
        ],
        [
            "Books.",
            "I mean, this is joint work with Mladen Colleran everything, so we're concerned about network analysis.",
            "Network analysis has greatly advanced our understanding of many complex systems, such as Jim regulatory system and also central numerous times neural system.",
            "So unfortunately most of such analysis are based on static networks, so a major challenge actually is to understand the dynamics of the networks based on the nonstationary.",
            "Time series measurements.",
            "So the goal our study is to uncover the dynamically changing and structurally awhile writing networks based on type.",
            "This time series measurements.",
            "So we propose the formulas and called dynamic tiva writing dynamic based network for this task and where we assume that the coefficients of the model are sparse and verion smoothly across time, and then the sparsity pattern of the coefficient and then also the.",
            "Timeline structure of the model is going to give you the timeline network, so we have described efficient an probably consistent procedure for estimating the coefficient of the net, the model, and we experimented with both his cell cycle data.",
            "Any data.",
            "In both cases the Timeline Network provides much more insight into the dynamic assistant then static networks.",
            "Thank you if you want to know more counter poster.",
            "T-35 Next up.",
            "Construction of nonparametric Bayesian models from Parametric Bayes equations."
        ],
        [
            "Good afternoon, so I'm interested in the construction of nonparametric Bayesian models.",
            "And if you have a look at the models that are available in the literature, like Gaussian processes or directly processes, one thing that they all have in common is that they have conjugate posteriors, so the posterior distributions of these models are analytically tractable, and in this work here I'm asking whether there is a general way to characterize the class of all nonparametric Bayesian model sets have such tractable posteriors.",
            "And one result is that the class of these models is, roughly speaking, the infinite dimensional analogue of the exponential family.",
            "Another result is that this characterization is actually constructive, so if you want to construct one particular model in this class instead of the directly process or the Gaussian process you want to construct your whatever process, then there is a fairly generic way of how you can do that and how we can characterize the properties of the model.",
            "And the poster number is T 50.",
            "Thank you.",
            "Next we have training factor graphs with reinforcement learning.",
            "For efficient map inference.",
            "So learning and info."
        ],
        [
            "It's in a large factor.",
            "Graphs is an extremely difficult problem and so solutions are often derived from MCMC.",
            "However, these solutions are problematic, as illustrated by this graph here.",
            "So if you focus on the lower curve, this represents a model trained by maximum likelihood, and because it doesn't take into account the local transitions of an MCMC method, the optimization surface is extremely bumpy.",
            "So ideally what we want is this top curve here and then we could March straight to the goal.",
            "And so the way we achieve this top curve is we map the problem of inference into a Markov decision process and then once we have this Markov decision process, we can treat parameter learning in the factor graph as reinforcement learning.",
            "In this MDP, and so we're actually able to achieve state of the art results on a real world data set using reinforcement learning with millions of parameters in an exponentially sized state space.",
            "And so our poster number is 82.",
            "I believe we're downstairs sort of in the backroom, very passionate about this work and look forward to talking to you all, thanks.",
            "Next"
        ],
        [
            "Riffle independence for ranked data.",
            "I'm John and this is joint work with my advisor Carlos.",
            "Question permutations come up in a variety of applications.",
            "For example, they come up in tracking and they come up in ranking problems, but they're typically pretty hard to represent intractable even because there's N factorial permutations.",
            "If you have an objects and so one simplifying assumption, you might think of is, well, what if I can approximate it by two subsets, being independent of each other?",
            "So, for example, we might have the fruit rankings be independent of the vegetable rankings.",
            "Turns out this assumption is problematic, and it's because of what we call mutual exclusivity, which is that two objects can't share the same rank in the permutation.",
            "The way we get around this problem is through a generalization of probabilistic independence, which we call rippled independence and hear what we're doing.",
            "Is we're first ranking the fruits, then why ranking the vegetables independently of the fruits?",
            "Then we just interleave them together to form a full ranking, as if by shuffling two piles of a deck of cards together.",
            "We showed that rippled independence is a natural notion of independence for rank data, and all the while maintaining some of the computational benefits of full independence.",
            "Come to party 25 if you want to hear more details.",
            "Next up, a smooth approximate linear program."
        ],
        [
            "So this work is about a new mathematical programming formulation for approximate DP or reinforcement learning.",
            "If you will a few years ago, Defries and Roy introduced a specific linear program for this very task.",
            "This program has been now been well studied.",
            "It has very attractive theoretical properties, but on hard DPS.",
            "For example, Tetris is typically not at the top of the heap.",
            "What we do is by careful understanding of the dynamic programming polytope.",
            "Understand that this particular formulation is perhaps not the best formulation, and in particular we propose a family family of formulations wherein a particular element of this family constitutes a new approximate dynamic program that inherits very attractive theoretical properties in terms of approximation guarantees and so forth.",
            "Moreover.",
            "On games such as Tetris, this offers a 10X improvement over the LP approach, and it compares favorably with other approaches as well.",
            "At the end of the day, this sort of a very geometric and simple algorithm, and I'd love to talk to you about it.",
            "This is at poster T 86.",
            "Next up randomized pruning efficiently calculating expectations in large dynamic."
        ],
        [
            "Programs.",
            "In LPN computational biology applications, the bottleneck of many learning algorithm is to repeatedly compute expectations over large, natural spaces.",
            "These computations often require high dynamic dynamic degree dynamic programs in machine translation, for example, polynomials of degree 6 or higher are common running times even in basic models, in order to scale up to large datasets.",
            "Heuristic bruening approximations.",
            "I've been proposed, however, these approximations induce bias with poorly understood consequences on the learn parameters, and Moreover the tradeoff between approximation quality and speed is hard to tune.",
            "So to address these problems, we have developed a randomized approximation scheme that works for large dynamic programs.",
            "The approach is both easier to analyze, an empirically outperforms standard pruning heuristics in a translation grammar induction task.",
            "The approach is relevant to a wide range of supervised and unsupervised machine learning scenarios, including applications in statistical parsing, RNA structure prediction, machine translation, an multiple sequence alignment.",
            "Thank you.",
            "Finally, we have subject independent EG based."
        ],
        [
            "PCI decoding.",
            "Hello, my name is Gemma Crosley and my coworkers and me.",
            "We would like to present a poster on our latest work on Brain Computer interfacing.",
            "And as many of you know, surely the brain computer interfacing is about reading people's thoughts.",
            "And well, however it's not so simple.",
            "So generally we need a small training set for the subject in question, and so our idea was to so having a large database, we wanted to.",
            "Basically the idea was to find a combination of sparse combination of previous subjects.",
            "To be able to predict the unseen subjects thoughts and so we were successful.",
            "And now we are able to skip a 30 minute training session and can read those thoughts immediately.",
            "So if you're intrigued, scared or just interested, please come round to a poster T 72 thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the first spotlight is Indian buffet processors with power law behavior.",
                    "label": 0
                },
                {
                    "sent": "Hi, I'm not many naturally occurring phenomena.",
                    "label": 0
                },
                {
                    "sent": "Exhibit exhibit power law behavior.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at the occurrences of words in a document corpus, you see that there will be a few words which occur in lots of the documents and lots of rare words which occur only in in a few documents and the frequency of occurrences follow a power law.",
                    "label": 0
                },
                {
                    "sent": "Recently there has been a number of efforts trying to model power law behavior with manual processes in the context of language modeling and in clustering.",
                    "label": 0
                },
                {
                    "sent": "Here will do the same for latent feature models.",
                    "label": 0
                },
                {
                    "sent": "In particular, will be working in the framework of Indian buffet processes which are based on parametric priors introduced by Tom Griffiths.",
                    "label": 0
                },
                {
                    "sent": "Anzovin Ghahramani for latent feature models with infinitely many features.",
                    "label": 0
                },
                {
                    "sent": "In this work we extend the Indian buffet process to a tree parameter family and showed that the extension has power law behavior.",
                    "label": 0
                },
                {
                    "sent": "The generative processes as given on the slides.",
                    "label": 0
                },
                {
                    "sent": "So we showed our extension is also infinitely exchangeable, just As for the typical Indian buffet process and definitely measure is given by a novel stochastic process, which we call the stable beta process, come to our post number T20 to learn all about this, including the relationship to Pitman, yor processes and an application to modeling would occurrences in document corpus.",
                    "label": 0
                },
                {
                    "sent": "Also, if you're interested in learning more basic nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Come to our workshop up in Whistler on Saturday.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is submodularity cuts in applications?",
                    "label": 1
                },
                {
                    "sent": "I have some other function is a set function that has similar properties to combat function and the problem of maximizing dysfunction appears in many machine learning problems, But is known to be pretty difficult to solve because it has exponentially large number of locally optimal solutions like convex maximization problem.",
                    "label": 1
                },
                {
                    "sent": "In this paper we propose an algorithm for maximizing submodular function based on a cutting plane methods which is implemented as undulated binary integer.",
                    "label": 0
                },
                {
                    "sent": "Doing programming procedure with increasing constraints.",
                    "label": 0
                },
                {
                    "sent": "The proposed algorithm that we called submodularity cats algorithm has a guaranteed to converge into an exact solution in a finding the number of iterations and it also can estimate upper and lower bounds at the at any iteration as shown in the figure says the accuracy of the current social can be estimated at any point.",
                    "label": 1
                },
                {
                    "sent": "Please come to our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Next, we have statistical models of linear and nonlinear contextual interactions in early visual processing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, so the visual cortex, which is this part around here, is the largest and best visual area.",
                    "label": 0
                },
                {
                    "sent": "Best started visual area of the primate brain.",
                    "label": 0
                },
                {
                    "sent": "And yet it's not clear what function it subserves, so one prominent computational theory due to Stoppingly suggests that.",
                    "label": 0
                },
                {
                    "sent": "That it computes the salience of visual images.",
                    "label": 0
                },
                {
                    "sent": "Critical function for early vision.",
                    "label": 0
                },
                {
                    "sent": "Since it can be used to direct visual attention.",
                    "label": 0
                },
                {
                    "sent": "So to address the issue of how to reconcile these ideas with ideas about statistical processing and representation of sensory inputs in biological systems.",
                    "label": 0
                },
                {
                    "sent": "And in the poster we will show that there is a wide range of neural and perceptual phenomena such as.",
                    "label": 0
                },
                {
                    "sent": "Contrast and orientation contrast effect in neurons as well as saliency and contour integration.",
                    "label": 0
                },
                {
                    "sent": "Perceptual effect and tilt processing.",
                    "label": 0
                },
                {
                    "sent": "Texture processing, so such a wide range of effects, which can be in comparison under the umbrella of generative model of image statistics.",
                    "label": 0
                },
                {
                    "sent": "To learn more about the model and simulations, please come by the poster.",
                    "label": 0
                },
                {
                    "sent": "67 thanks.",
                    "label": 0
                },
                {
                    "sent": "Next up, STD P enables spiking neurons to detect hidden causes of their inputs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "The question we addressed in our work is how does spike timing dependent plasticity contributes to a high level learning algorithm?",
                    "label": 0
                },
                {
                    "sent": "They present a single layer feedforward architecture with population coded inputs and a probabilistic software.",
                    "label": 1
                },
                {
                    "sent": "Not take all output activation and in this circuit we let STP learn the presented input spike train in a completely unsupervised setting.",
                    "label": 0
                },
                {
                    "sent": "The postulates that the synaptic potentiation is proportional to the exponential of the current synaptic rate, whereas the synaptic depression is independent of the current wait.",
                    "label": 0
                },
                {
                    "sent": "This is also supported by experiments.",
                    "label": 0
                },
                {
                    "sent": "We use rigorous mathematical analysis to show that the whole circuit implemented stochastic version of expectation maximization, the maximization step.",
                    "label": 0
                },
                {
                    "sent": "Them step is just to wait update induced by SDP.",
                    "label": 0
                },
                {
                    "sent": "The expectation step emerges from the forward operation of the circuit which produces a probabilistic population encoded representation of the latent variable which indeed represents the hidden costs of the current spiking input.",
                    "label": 1
                },
                {
                    "sent": "Come to us and discuss this new view of SDP and Spike based expectation maximization.",
                    "label": 0
                },
                {
                    "sent": "At post 69.",
                    "label": 0
                },
                {
                    "sent": "Next up 3D object recognition with deep belief Nets.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, this is joint work with Geoff Hinton and as Jeff explained earlier today it deep belief Net is a generative model and type of generative model that has an undirected model at the top followed by a few one or more layers of directed connections below it.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we present a bunch of improvements over the original DBN model that was proposed a few years back.",
                    "label": 0
                },
                {
                    "sent": "The first improvement is.",
                    "label": 0
                },
                {
                    "sent": "A new top level model that learns separate sets of features for each of the object classes that we are interested in recognizing.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of 1/3 order of Boltzmann machine that I've talked about today.",
                    "label": 0
                },
                {
                    "sent": "Another improvement is.",
                    "label": 0
                },
                {
                    "sent": "A learning algorithm for the top level model that combines discriminative and generative gradients to train the weights.",
                    "label": 1
                },
                {
                    "sent": "The nice thing about this algorithm is that it avoids the problem of poor Markov chain mixing that affected the classification performance of top level models in earlier DBN's.",
                    "label": 0
                },
                {
                    "sent": "So we take this improved DBN and applied to recognizing 3D objects from stereo pair images.",
                    "label": 0
                },
                {
                    "sent": "We use the NORB object recognition data set to measure performance and on the standard normal task we beat SVM.",
                    "label": 0
                },
                {
                    "sent": "Zayn get close to convolutional neural Nets which have built in prior knowledge about images.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One of the big advantages of DBN's is that you can train it semi supervised, so we consider a modified version of the North task where we allow unlabeled images to be used as extra training data and on this modified task we beat didn't beat convolutional neural Nets.",
                    "label": 1
                },
                {
                    "sent": "So if you're interested in deep belief Nets or object recognition or both, please come by or poster.",
                    "label": 0
                },
                {
                    "sent": "And even if you're not familiar with the details of the BMS, please come by and we will be happy to give you a little intro of deviance.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next up maximum maximum likelihood trajectories for continuous time Markov chains.",
                    "label": 1
                },
                {
                    "sent": "Hello so I work in terms concerns continuous time Markov shades.",
                    "label": 0
                },
                {
                    "sent": "These are models of dynamical systems that have a discrete states set and they transition stochastically between between states just like a Markov chain, but they remain or dwell in their states for real valued amounts of time.",
                    "label": 0
                },
                {
                    "sent": "There are exponentially distributed.",
                    "label": 0
                },
                {
                    "sent": "These models are used in many domains including my favorite area which is the last stochastic molecular dynamics.",
                    "label": 0
                },
                {
                    "sent": "They used to model ion channels using pilot index, queueing theory, lots of domains.",
                    "label": 0
                },
                {
                    "sent": "So we studied 2 problems for these for this type of model, which happens off ages ago for discrete time models.",
                    "label": 1
                },
                {
                    "sent": "But if you're not even addressed for continuous time Markov chains, so in the first version of the problem you're given the state of the system at two different times, and you want to infer the most likely trajectory the system took between those two times.",
                    "label": 0
                },
                {
                    "sent": "That means the most likely sequence of state transitions, and the most likely dwell times, and in the second version of the problem.",
                    "label": 0
                },
                {
                    "sent": "Extending that problem, then you can assume that you've received a sequence of observations at the system, which may be noisy partial observations.",
                    "label": 1
                },
                {
                    "sent": "And again, you want to infer the most likely trajectory, so be sure that for both of these problems there are efficient dynamic programming solutions.",
                    "label": 0
                },
                {
                    "sent": "However, the solutions are a bit weird sometimes, and if you want to know what that means, please come to the post here, thank you.",
                    "label": 0
                },
                {
                    "sent": "Next up time varying dynamic Bashan net.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Books.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is joint work with Mladen Colleran everything, so we're concerned about network analysis.",
                    "label": 0
                },
                {
                    "sent": "Network analysis has greatly advanced our understanding of many complex systems, such as Jim regulatory system and also central numerous times neural system.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately most of such analysis are based on static networks, so a major challenge actually is to understand the dynamics of the networks based on the nonstationary.",
                    "label": 0
                },
                {
                    "sent": "Time series measurements.",
                    "label": 0
                },
                {
                    "sent": "So the goal our study is to uncover the dynamically changing and structurally awhile writing networks based on type.",
                    "label": 0
                },
                {
                    "sent": "This time series measurements.",
                    "label": 0
                },
                {
                    "sent": "So we propose the formulas and called dynamic tiva writing dynamic based network for this task and where we assume that the coefficients of the model are sparse and verion smoothly across time, and then the sparsity pattern of the coefficient and then also the.",
                    "label": 0
                },
                {
                    "sent": "Timeline structure of the model is going to give you the timeline network, so we have described efficient an probably consistent procedure for estimating the coefficient of the net, the model, and we experimented with both his cell cycle data.",
                    "label": 0
                },
                {
                    "sent": "Any data.",
                    "label": 0
                },
                {
                    "sent": "In both cases the Timeline Network provides much more insight into the dynamic assistant then static networks.",
                    "label": 0
                },
                {
                    "sent": "Thank you if you want to know more counter poster.",
                    "label": 0
                },
                {
                    "sent": "T-35 Next up.",
                    "label": 0
                },
                {
                    "sent": "Construction of nonparametric Bayesian models from Parametric Bayes equations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, so I'm interested in the construction of nonparametric Bayesian models.",
                    "label": 1
                },
                {
                    "sent": "And if you have a look at the models that are available in the literature, like Gaussian processes or directly processes, one thing that they all have in common is that they have conjugate posteriors, so the posterior distributions of these models are analytically tractable, and in this work here I'm asking whether there is a general way to characterize the class of all nonparametric Bayesian model sets have such tractable posteriors.",
                    "label": 0
                },
                {
                    "sent": "And one result is that the class of these models is, roughly speaking, the infinite dimensional analogue of the exponential family.",
                    "label": 0
                },
                {
                    "sent": "Another result is that this characterization is actually constructive, so if you want to construct one particular model in this class instead of the directly process or the Gaussian process you want to construct your whatever process, then there is a fairly generic way of how you can do that and how we can characterize the properties of the model.",
                    "label": 0
                },
                {
                    "sent": "And the poster number is T 50.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Next we have training factor graphs with reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "For efficient map inference.",
                    "label": 0
                },
                {
                    "sent": "So learning and info.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's in a large factor.",
                    "label": 0
                },
                {
                    "sent": "Graphs is an extremely difficult problem and so solutions are often derived from MCMC.",
                    "label": 0
                },
                {
                    "sent": "However, these solutions are problematic, as illustrated by this graph here.",
                    "label": 0
                },
                {
                    "sent": "So if you focus on the lower curve, this represents a model trained by maximum likelihood, and because it doesn't take into account the local transitions of an MCMC method, the optimization surface is extremely bumpy.",
                    "label": 0
                },
                {
                    "sent": "So ideally what we want is this top curve here and then we could March straight to the goal.",
                    "label": 0
                },
                {
                    "sent": "And so the way we achieve this top curve is we map the problem of inference into a Markov decision process and then once we have this Markov decision process, we can treat parameter learning in the factor graph as reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "In this MDP, and so we're actually able to achieve state of the art results on a real world data set using reinforcement learning with millions of parameters in an exponentially sized state space.",
                    "label": 0
                },
                {
                    "sent": "And so our poster number is 82.",
                    "label": 0
                },
                {
                    "sent": "I believe we're downstairs sort of in the backroom, very passionate about this work and look forward to talking to you all, thanks.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Riffle independence for ranked data.",
                    "label": 1
                },
                {
                    "sent": "I'm John and this is joint work with my advisor Carlos.",
                    "label": 0
                },
                {
                    "sent": "Question permutations come up in a variety of applications.",
                    "label": 0
                },
                {
                    "sent": "For example, they come up in tracking and they come up in ranking problems, but they're typically pretty hard to represent intractable even because there's N factorial permutations.",
                    "label": 0
                },
                {
                    "sent": "If you have an objects and so one simplifying assumption, you might think of is, well, what if I can approximate it by two subsets, being independent of each other?",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might have the fruit rankings be independent of the vegetable rankings.",
                    "label": 0
                },
                {
                    "sent": "Turns out this assumption is problematic, and it's because of what we call mutual exclusivity, which is that two objects can't share the same rank in the permutation.",
                    "label": 0
                },
                {
                    "sent": "The way we get around this problem is through a generalization of probabilistic independence, which we call rippled independence and hear what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Is we're first ranking the fruits, then why ranking the vegetables independently of the fruits?",
                    "label": 1
                },
                {
                    "sent": "Then we just interleave them together to form a full ranking, as if by shuffling two piles of a deck of cards together.",
                    "label": 0
                },
                {
                    "sent": "We showed that rippled independence is a natural notion of independence for rank data, and all the while maintaining some of the computational benefits of full independence.",
                    "label": 0
                },
                {
                    "sent": "Come to party 25 if you want to hear more details.",
                    "label": 0
                },
                {
                    "sent": "Next up, a smooth approximate linear program.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this work is about a new mathematical programming formulation for approximate DP or reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "If you will a few years ago, Defries and Roy introduced a specific linear program for this very task.",
                    "label": 0
                },
                {
                    "sent": "This program has been now been well studied.",
                    "label": 0
                },
                {
                    "sent": "It has very attractive theoretical properties, but on hard DPS.",
                    "label": 0
                },
                {
                    "sent": "For example, Tetris is typically not at the top of the heap.",
                    "label": 0
                },
                {
                    "sent": "What we do is by careful understanding of the dynamic programming polytope.",
                    "label": 0
                },
                {
                    "sent": "Understand that this particular formulation is perhaps not the best formulation, and in particular we propose a family family of formulations wherein a particular element of this family constitutes a new approximate dynamic program that inherits very attractive theoretical properties in terms of approximation guarantees and so forth.",
                    "label": 0
                },
                {
                    "sent": "Moreover.",
                    "label": 0
                },
                {
                    "sent": "On games such as Tetris, this offers a 10X improvement over the LP approach, and it compares favorably with other approaches as well.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day, this sort of a very geometric and simple algorithm, and I'd love to talk to you about it.",
                    "label": 0
                },
                {
                    "sent": "This is at poster T 86.",
                    "label": 0
                },
                {
                    "sent": "Next up randomized pruning efficiently calculating expectations in large dynamic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Programs.",
                    "label": 0
                },
                {
                    "sent": "In LPN computational biology applications, the bottleneck of many learning algorithm is to repeatedly compute expectations over large, natural spaces.",
                    "label": 1
                },
                {
                    "sent": "These computations often require high dynamic dynamic degree dynamic programs in machine translation, for example, polynomials of degree 6 or higher are common running times even in basic models, in order to scale up to large datasets.",
                    "label": 0
                },
                {
                    "sent": "Heuristic bruening approximations.",
                    "label": 0
                },
                {
                    "sent": "I've been proposed, however, these approximations induce bias with poorly understood consequences on the learn parameters, and Moreover the tradeoff between approximation quality and speed is hard to tune.",
                    "label": 0
                },
                {
                    "sent": "So to address these problems, we have developed a randomized approximation scheme that works for large dynamic programs.",
                    "label": 1
                },
                {
                    "sent": "The approach is both easier to analyze, an empirically outperforms standard pruning heuristics in a translation grammar induction task.",
                    "label": 1
                },
                {
                    "sent": "The approach is relevant to a wide range of supervised and unsupervised machine learning scenarios, including applications in statistical parsing, RNA structure prediction, machine translation, an multiple sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Finally, we have subject independent EG based.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PCI decoding.",
                    "label": 0
                },
                {
                    "sent": "Hello, my name is Gemma Crosley and my coworkers and me.",
                    "label": 0
                },
                {
                    "sent": "We would like to present a poster on our latest work on Brain Computer interfacing.",
                    "label": 0
                },
                {
                    "sent": "And as many of you know, surely the brain computer interfacing is about reading people's thoughts.",
                    "label": 0
                },
                {
                    "sent": "And well, however it's not so simple.",
                    "label": 0
                },
                {
                    "sent": "So generally we need a small training set for the subject in question, and so our idea was to so having a large database, we wanted to.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea was to find a combination of sparse combination of previous subjects.",
                    "label": 0
                },
                {
                    "sent": "To be able to predict the unseen subjects thoughts and so we were successful.",
                    "label": 0
                },
                {
                    "sent": "And now we are able to skip a 30 minute training session and can read those thoughts immediately.",
                    "label": 0
                },
                {
                    "sent": "So if you're intrigued, scared or just interested, please come round to a poster T 72 thanks.",
                    "label": 0
                }
            ]
        }
    }
}