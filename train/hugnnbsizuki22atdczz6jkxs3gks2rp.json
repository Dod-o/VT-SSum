{
    "id": "hugnnbsizuki22atdczz6jkxs3gks2rp",
    "title": "Can matrix coherence be efficiently and accurately estimated?",
    "info": {
        "author": [
            "Ameet Talwalkar, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Network Analysis->Social Networks"
        ]
    },
    "url": "http://videolectures.net/aistats2011_talwalkar_matrix/",
    "segmentation": [
        [
            "Alright, so thank you.",
            "My name is Amit.",
            "This is joint work with Mary or Moryan.",
            "I'll be addressing the question of whether matrix coherence can be sufficient."
        ],
        [
            "Be inaccurately estimated.",
            "Alright, so a common theme of this conference has been that datasets can be large and high dimensional and that they can be represented by large matrices.",
            "Few examples include videos, images or documents on the web just to name a few.",
            "And also, as we've heard already today, low rank approximations are often appropriate to get a concise representation of these matrices, and again a few Canonical examples include principal component analysis or other dimensionality reduction techniques, as well as a bunch of algorithms for.",
            "Collaborative filtering."
        ],
        [
            "And again, the high level idea of learning approximation is that if you start with start with a large and by a matrix, that matrix can either exactly more likely approximately be represented as the product of 2 low rank as a product of low rank matrices, where here are is the low rank and it's much less than N or M. An although learning comprise missions are nice and there are lots of different methods for solving them in certain cases, when your input matrix is very large, standard methods such as SVD can be quite expensive, just the runtime of them, as well as just the storage of the full matrix.",
            "An in these cases, sampling based methods provide a nice alternative as these methods provide generated low rank approximation just by sampling by storing and operating on a subset of the matrix columns.",
            "So as I said there.",
            "Tractable often when SVD is not, and Moreover they give you some interpretability in terms of your low rank approximation and that you get an approximation based on actual entries of your matrix as input."
        ],
        [
            "So in the case of SVD, linear combinations of your entries.",
            "OK, that mind there are two.",
            "There's some key assumptions associated with this sampling based approximation.",
            "The first of course is that generating approximation in the 1st place is something you want to do, but more interesting in this work is the idea that finding a good subset of columns is in fact possible.",
            "And of course whether or not this is possible depending on how you sample columns.",
            "But in general it's not always easy, and this pathological example here we saw something similar like this earlier today.",
            "In the case where you do have a low rank matrix.",
            "The first are columns are just the columns of the end by an identity or the end dimensional identity matrix with columns or zero.",
            "And here for instance.",
            "If you sample uniformly at random, unless you're very lucky and your sample contains those first our columns, you're not going to like approximation.",
            "So how do you deal with that?",
            "Well, one line of work for these sampling based approximations involves working with nonuniform distributions over the columns, or doing some sort of adaptive sampling of the columns.",
            "And there was a.",
            "Paper stats here about a new method for adaptive sampling.",
            "For the nicer approximation in particular.",
            "Alternatively, it's noted that oftentimes in practice at least, uniform sampling works well.",
            "Of course, this is data dependent, but for many of the matrices people work with practice, they find that uniform sampling works quite well.",
            "And recently there's been work showing that the use of uniform sampling can be justified under incoherence, assumptions.",
            "And again, we heard earlier today about some negative aspects of incoherence, and it is indeed the case that incoherence isn't necessary.",
            "Isn't a necessary condition necessary for using uniform sampling?",
            "But as I said, it has at least been shown to be sufficient for using uniform sampling, so this is nice to give some justification for using uniform sampling in certain cases, but on a case by case basis, it's not very.",
            "Useful in practice because as I'll discuss a little bit more, go into more detail in a little bit in coherence coherence of the matrix is derived from the top singular vectors of the matrix an in the context of sampling based approximation.",
            "The whole point is that it's much faster than computing SVD, so if you need to be to get the coherence in the 1st place, I kind of defeats the purpose of these methods, so in this work the question is whether we can estimate."
        ],
        [
            "Coherence much more efficiently and see whether or not uniform sampling is going to work for us.",
            "OK, so for the major of the talk I'm going first.",
            "Define what coherence is and introduce the algorithm.",
            "Then I'll show some analysis in the low rank setting and finally end with a bunch of experimental."
        ],
        [
            "Thoughts?",
            "OK, So what does matrix coherence?",
            "So if you start with some matrix X and it stopped left, singular vectors are used.",
            "Are you some arm?",
            "You can then define the projection matrix PR, which is just the orthogonal projection onto the column space of you are and then the musiro coherence is a bunch of different related definitions of coherence.",
            "But the musiro coherence in particular is then defined as an over R times the largest diagonal entry of this projection matrix PR an.",
            "Intuitively what it's doing is coherence is measuring is the degree to which.",
            "The singular vectors correspond to the Canonical basis.",
            "And to give some more intuition, when for the rank one matrix where all the entries are identical as minimal coherence of 1 an.",
            "Alternatively the pathological example then I showed before where the single vectors are indeed the the Canonical basis has maximal coherence, which is an over R and as you."
        ],
        [
            "Checked the existing sampling based guarantees that showed that incoherent matrices work well.",
            "Rule out these high coherent matrices such as that action on that slide.",
            "And just to note, going into assumptions had been used in other lines of work.",
            "As we heard earlier today, for instance, in matrix completion and robust PCA, and also in this Mr. For this talk, I'm actually going to be focusing on a related notion of coherence, which I called gamma gamma coherence, and it's very closely related, as you can see to the zero coherence just without the scaling terms an it's easier for us to work with this an we're going to be trying to estimate gamma coherence and comparing it to the true gamma."
        ],
        [
            "Coherence, and in doing so, dropping the scaling terms is a reasonable thing to do.",
            "OK, so that in mind, let me explain the proposed algorithm and it's very simple.",
            "It's basically a sampling based algorithm.",
            "You have you sample uniformly at random.",
            "A small subset L of your columns of the matrix.",
            "To get this matrix X1 and then the idea is simply to compute the coherence of X1 an, use that as a proxy for the coherence of the full matrix.",
            "And this is.",
            "Computationally, and storage wise less expensive than of course because X one is a much smaller matrix FedEx.",
            "So if X is exactly low rank, it's very simple.",
            "You compute, you find the the top singular vectors of X1 and then calculate your your coherence and you're done.",
            "And similarly if for an arbitrary matrix it's not exactly low rank but approximately low rank you have."
        ],
        [
            "That you provide a rank parameter R, and that's similar to other.",
            "You know when you do.",
            "When you do SVD or you know that you have to pick the top principal components in PCA so you pick you fix that rank frame parameter and then you do the same sort of thing you fine."
        ],
        [
            "The top are singular vectors of X1 and use them to estimate the coherence of the of the matrix X. OK, so with that in mind and briefly.",
            "Show you how this algorithm works in the lowering setting, so again here it's DEF."
        ],
        [
            "The idealized and not not what you would not not not completely realistic, but it does give us some some intuition.",
            "Here we're assuming that X is exactly low rank, and in doing so we first first observation is twofold.",
            "One that are estimated.",
            "Lily is X.",
            "One is a monotonically increasing function L. So as you sample more and more columns miss assessment of coherence goes up and is upper bounded by the coherence.",
            "We want to estimate second with high probability, the coherence, the coherence of our estimator is equal to what we're trying to estimate.",
            "If you sample enough columns where enough columns is OR squared musiro of you are and it's important to note here that the second point that the number of columns you need in order to.",
            "With high probability get with high probability to estimate coherence correctly is indeed depend on the coherence itself.",
            "And that's probably not the most desirable property in an analysis of an algorithm we were hoping to get something stronger than that because I'll show you in a second or second observation shows that it's not really an issue with our analysis, it's just this is just an inherent property of trying to estimate coherence and it sort of makes sense if you're trying to make here matrix without looking at all of its entries.",
            "You can come up with adversarially constructed matrices for which you have a tough time doing that.",
            "OK."
        ],
        [
            "Before I go to bed, second observation, I just want to note that in order to obtain these results, it's fairly simple and we simply just trying to relate the projection matrices associated with com space of X1 and the comp space of X, and that's a natural thing to do, given that coherence is derived based on this projection matrices and along with that we use previous results.",
            "Previous coherence based analysis of sampling based approximations."
        ],
        [
            "OK, so the 2nd second observation is simply saying for a fixed RNM if you pick some some gamma.",
            "If you pick some gamma then you can construct a matrix X that has that gamma such that there's a gap between the estimate gamma and X1 and the true coherence gamma and that gap gap gap exists so long as your sample happens to not include the first column of the Matrix an.",
            "The important thing to take out of this is that there's a gap here between gamma Gamma hat.",
            "The matrix is constructed in such a way that Gamma Hat is a fixed RNM is a constant, and it's very small and thus this gap is growing as growing proportional to the size of gamma.",
            "So as you as your matrix is more and more coherent, the gap between your estimate and the true coherence gets bigger and bigger.",
            "And again, this is a it's a simple construction to come up this matrix and it uses properties of random orthogonal model for coherence.",
            "Desmond looked at previously OK, so finally I'm just going to show some experiments.",
            "The analysis shows that the."
        ],
        [
            "Explain how the algorithm works, but it's not completely clear, but it always does work, especially for."
        ],
        [
            "High coherence matrices.",
            "It turns out experimentally that you tend to do a lot better.",
            "So first initial results for low rank synthetic data.",
            "And here we're working with matrices that are 1000 by 1000, and they have exactly rank 50.",
            "We generated the matrices as follows.",
            "First, the singular values are generated to decay exponentially and 2nd singular vectors.",
            "We first fix one singular vector to have the desired coherence that we want, and we then use.",
            "QR decomposition to get an orthogonal basis to fill out the rest of the singular vectors.",
            "And we choose three different values of coherence to work with.",
            "So as you recall, gamma ranges from zero to 1, so we have low, medium and high here, and so that's what we see in the left figure on the right figure you see the results of trying to estimate coherence using the algorithm that I previously discussed.",
            "The X axis shows the number of columns sampled and the Y axis is showing the difference between the approximation and the exact coherence, and so first note that the.",
            "That results here match the the analysis that we saw before 1st that the estimate of coherence consistency is upper bounded by the exact coherence and 2nd that the estimates are converge quicker for matrices that have low coherence, there that are more incoherent.",
            "However, the takeaway here is that the true coherence is recovered when L is greater than our and.",
            "That's interesting, but it's things are are very easy when everything is exactly low rank.",
            "So we then looked at."
        ],
        [
            "Again, simulated data synthetic data, but with actually full rank.",
            "So we started with the same low rank synthetic data that we were working with before.",
            "But then instead of zeroing out the rest of the signal, the other end minus our singular values, we instead sent them set them to equal some constant epsilon times the smallest of the singular values.",
            "And here we set epsilon equal to be .1 or .9.",
            "So noise small is corresponding to epsilon .1 noise largest corresponding to epsilon .9.",
            "And we see here is similar similar sort of results in that when you when you get good recovery of gamma, when you sample some small multiple of the rank of the matrix.",
            "So we sent here are again to be 50 and you see on the left here if you sampled 100 or 150 columns you get a very good estimate an when the noise is large you get sample more columns and you still don't get an exact estimate.",
            "But when you sample two or three 100 comes you get.",
            "Pretty reasonable estimate of the coherence."
        ],
        [
            "OK, and finally we looked at real data.",
            "We worked with seven real datasets.",
            "There were all kernel matrices that range from one and a half 1000 to 5000.",
            "And by an RN was 1000 to 5000 and again we sample letters that they get the coherence of these datasets and again they range from.",
            "There's not really any middle here, they're either small or quite large that red one I guess can be considered middle, but it's still pretty small.",
            "But then in the right figure again we showed the estimation error again, the X axis is the number of columns sampled, the Y axis is the error of approximation, and again we get good estimates after sampling roughly 100 columns and we also see as we'd expect that high coherence matrices have they converge to get estimates lower, and they also have higher variance, and I think I forgot to mention this.",
            "But all of our experiments were averages over 10 trials of samples."
        ],
        [
            "OK, so and then just to get back to the point of all this.",
            "The idea here is that coherence is a sufficient condition for using these uniform sampling at random with these sampling based methods, and we did some experiments to verify whether or to validate this this observation and we were indeed able to find that this is the case.",
            "So the left left figure again is just the coherence of these role of these datasets and the right figure is.",
            "Showing the error of Nystrom approximation using different numbers of sample columns with error measured as follows.",
            "We have a normalized error so just for business, the reconstruction error divided by the norm of the matrix to be approximated and what you can see is that the datasets with high coherence don't work as well as you'd expect, and the nice thing is that as you see, as we showed in the previous slide, we're able to to estimate this this coherence well.",
            "And this is a similar plot for a different sort of low rank approximation sampler approximation."
        ],
        [
            "Where you just project your matrix onto the subspace spanned by the columns that you sample, and again you get the same sort of behavior that the datasets with high coherence don't perform as well.",
            "Alright, so that's all in conclusion.",
            "We presented an algorithm to estimate matrix coherence.",
            "Our theory shows that the estimate depends on coherence itself, which is not necessarily the best thing, but in practice we get accurate estimates of coherence across across various values of coherence.",
            "And Moreover, we are able to show or validate again that using coherence is a good predictor of whether."
        ],
        [
            "Sampling uniformly random is a good thing to do for learning approximation.",
            "So future work involves future work involves analyzing this algorithm in the context of full rank matrices or low rank matrices with noise, as well as possibly combining several other estimates for approximation.",
            "An somewhat slightly less related line of future work would be just to come up with new analysis of the sample Miss algorithms that are independent of coherence, and using using notions such as spikiness or something that are easier to compute exactly.",
            "So that's all, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so thank you.",
                    "label": 0
                },
                {
                    "sent": "My name is Amit.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Mary or Moryan.",
                    "label": 0
                },
                {
                    "sent": "I'll be addressing the question of whether matrix coherence can be sufficient.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be inaccurately estimated.",
                    "label": 0
                },
                {
                    "sent": "Alright, so a common theme of this conference has been that datasets can be large and high dimensional and that they can be represented by large matrices.",
                    "label": 1
                },
                {
                    "sent": "Few examples include videos, images or documents on the web just to name a few.",
                    "label": 0
                },
                {
                    "sent": "And also, as we've heard already today, low rank approximations are often appropriate to get a concise representation of these matrices, and again a few Canonical examples include principal component analysis or other dimensionality reduction techniques, as well as a bunch of algorithms for.",
                    "label": 0
                },
                {
                    "sent": "Collaborative filtering.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, the high level idea of learning approximation is that if you start with start with a large and by a matrix, that matrix can either exactly more likely approximately be represented as the product of 2 low rank as a product of low rank matrices, where here are is the low rank and it's much less than N or M. An although learning comprise missions are nice and there are lots of different methods for solving them in certain cases, when your input matrix is very large, standard methods such as SVD can be quite expensive, just the runtime of them, as well as just the storage of the full matrix.",
                    "label": 0
                },
                {
                    "sent": "An in these cases, sampling based methods provide a nice alternative as these methods provide generated low rank approximation just by sampling by storing and operating on a subset of the matrix columns.",
                    "label": 0
                },
                {
                    "sent": "So as I said there.",
                    "label": 0
                },
                {
                    "sent": "Tractable often when SVD is not, and Moreover they give you some interpretability in terms of your low rank approximation and that you get an approximation based on actual entries of your matrix as input.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the case of SVD, linear combinations of your entries.",
                    "label": 0
                },
                {
                    "sent": "OK, that mind there are two.",
                    "label": 0
                },
                {
                    "sent": "There's some key assumptions associated with this sampling based approximation.",
                    "label": 1
                },
                {
                    "sent": "The first of course is that generating approximation in the 1st place is something you want to do, but more interesting in this work is the idea that finding a good subset of columns is in fact possible.",
                    "label": 1
                },
                {
                    "sent": "And of course whether or not this is possible depending on how you sample columns.",
                    "label": 0
                },
                {
                    "sent": "But in general it's not always easy, and this pathological example here we saw something similar like this earlier today.",
                    "label": 0
                },
                {
                    "sent": "In the case where you do have a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "The first are columns are just the columns of the end by an identity or the end dimensional identity matrix with columns or zero.",
                    "label": 0
                },
                {
                    "sent": "And here for instance.",
                    "label": 0
                },
                {
                    "sent": "If you sample uniformly at random, unless you're very lucky and your sample contains those first our columns, you're not going to like approximation.",
                    "label": 0
                },
                {
                    "sent": "So how do you deal with that?",
                    "label": 0
                },
                {
                    "sent": "Well, one line of work for these sampling based approximations involves working with nonuniform distributions over the columns, or doing some sort of adaptive sampling of the columns.",
                    "label": 0
                },
                {
                    "sent": "And there was a.",
                    "label": 0
                },
                {
                    "sent": "Paper stats here about a new method for adaptive sampling.",
                    "label": 0
                },
                {
                    "sent": "For the nicer approximation in particular.",
                    "label": 1
                },
                {
                    "sent": "Alternatively, it's noted that oftentimes in practice at least, uniform sampling works well.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is data dependent, but for many of the matrices people work with practice, they find that uniform sampling works quite well.",
                    "label": 0
                },
                {
                    "sent": "And recently there's been work showing that the use of uniform sampling can be justified under incoherence, assumptions.",
                    "label": 1
                },
                {
                    "sent": "And again, we heard earlier today about some negative aspects of incoherence, and it is indeed the case that incoherence isn't necessary.",
                    "label": 0
                },
                {
                    "sent": "Isn't a necessary condition necessary for using uniform sampling?",
                    "label": 0
                },
                {
                    "sent": "But as I said, it has at least been shown to be sufficient for using uniform sampling, so this is nice to give some justification for using uniform sampling in certain cases, but on a case by case basis, it's not very.",
                    "label": 0
                },
                {
                    "sent": "Useful in practice because as I'll discuss a little bit more, go into more detail in a little bit in coherence coherence of the matrix is derived from the top singular vectors of the matrix an in the context of sampling based approximation.",
                    "label": 0
                },
                {
                    "sent": "The whole point is that it's much faster than computing SVD, so if you need to be to get the coherence in the 1st place, I kind of defeats the purpose of these methods, so in this work the question is whether we can estimate.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coherence much more efficiently and see whether or not uniform sampling is going to work for us.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the major of the talk I'm going first.",
                    "label": 0
                },
                {
                    "sent": "Define what coherence is and introduce the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then I'll show some analysis in the low rank setting and finally end with a bunch of experimental.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thoughts?",
                    "label": 0
                },
                {
                    "sent": "OK, So what does matrix coherence?",
                    "label": 1
                },
                {
                    "sent": "So if you start with some matrix X and it stopped left, singular vectors are used.",
                    "label": 1
                },
                {
                    "sent": "Are you some arm?",
                    "label": 0
                },
                {
                    "sent": "You can then define the projection matrix PR, which is just the orthogonal projection onto the column space of you are and then the musiro coherence is a bunch of different related definitions of coherence.",
                    "label": 0
                },
                {
                    "sent": "But the musiro coherence in particular is then defined as an over R times the largest diagonal entry of this projection matrix PR an.",
                    "label": 0
                },
                {
                    "sent": "Intuitively what it's doing is coherence is measuring is the degree to which.",
                    "label": 1
                },
                {
                    "sent": "The singular vectors correspond to the Canonical basis.",
                    "label": 1
                },
                {
                    "sent": "And to give some more intuition, when for the rank one matrix where all the entries are identical as minimal coherence of 1 an.",
                    "label": 0
                },
                {
                    "sent": "Alternatively the pathological example then I showed before where the single vectors are indeed the the Canonical basis has maximal coherence, which is an over R and as you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Checked the existing sampling based guarantees that showed that incoherent matrices work well.",
                    "label": 0
                },
                {
                    "sent": "Rule out these high coherent matrices such as that action on that slide.",
                    "label": 0
                },
                {
                    "sent": "And just to note, going into assumptions had been used in other lines of work.",
                    "label": 1
                },
                {
                    "sent": "As we heard earlier today, for instance, in matrix completion and robust PCA, and also in this Mr. For this talk, I'm actually going to be focusing on a related notion of coherence, which I called gamma gamma coherence, and it's very closely related, as you can see to the zero coherence just without the scaling terms an it's easier for us to work with this an we're going to be trying to estimate gamma coherence and comparing it to the true gamma.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coherence, and in doing so, dropping the scaling terms is a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so that in mind, let me explain the proposed algorithm and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It's basically a sampling based algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have you sample uniformly at random.",
                    "label": 1
                },
                {
                    "sent": "A small subset L of your columns of the matrix.",
                    "label": 0
                },
                {
                    "sent": "To get this matrix X1 and then the idea is simply to compute the coherence of X1 an, use that as a proxy for the coherence of the full matrix.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 1
                },
                {
                    "sent": "Computationally, and storage wise less expensive than of course because X one is a much smaller matrix FedEx.",
                    "label": 0
                },
                {
                    "sent": "So if X is exactly low rank, it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You compute, you find the the top singular vectors of X1 and then calculate your your coherence and you're done.",
                    "label": 0
                },
                {
                    "sent": "And similarly if for an arbitrary matrix it's not exactly low rank but approximately low rank you have.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you provide a rank parameter R, and that's similar to other.",
                    "label": 0
                },
                {
                    "sent": "You know when you do.",
                    "label": 0
                },
                {
                    "sent": "When you do SVD or you know that you have to pick the top principal components in PCA so you pick you fix that rank frame parameter and then you do the same sort of thing you fine.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The top are singular vectors of X1 and use them to estimate the coherence of the of the matrix X. OK, so with that in mind and briefly.",
                    "label": 0
                },
                {
                    "sent": "Show you how this algorithm works in the lowering setting, so again here it's DEF.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idealized and not not what you would not not not completely realistic, but it does give us some some intuition.",
                    "label": 0
                },
                {
                    "sent": "Here we're assuming that X is exactly low rank, and in doing so we first first observation is twofold.",
                    "label": 0
                },
                {
                    "sent": "One that are estimated.",
                    "label": 0
                },
                {
                    "sent": "Lily is X.",
                    "label": 0
                },
                {
                    "sent": "One is a monotonically increasing function L. So as you sample more and more columns miss assessment of coherence goes up and is upper bounded by the coherence.",
                    "label": 1
                },
                {
                    "sent": "We want to estimate second with high probability, the coherence, the coherence of our estimator is equal to what we're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "If you sample enough columns where enough columns is OR squared musiro of you are and it's important to note here that the second point that the number of columns you need in order to.",
                    "label": 1
                },
                {
                    "sent": "With high probability get with high probability to estimate coherence correctly is indeed depend on the coherence itself.",
                    "label": 0
                },
                {
                    "sent": "And that's probably not the most desirable property in an analysis of an algorithm we were hoping to get something stronger than that because I'll show you in a second or second observation shows that it's not really an issue with our analysis, it's just this is just an inherent property of trying to estimate coherence and it sort of makes sense if you're trying to make here matrix without looking at all of its entries.",
                    "label": 0
                },
                {
                    "sent": "You can come up with adversarially constructed matrices for which you have a tough time doing that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I go to bed, second observation, I just want to note that in order to obtain these results, it's fairly simple and we simply just trying to relate the projection matrices associated with com space of X1 and the comp space of X, and that's a natural thing to do, given that coherence is derived based on this projection matrices and along with that we use previous results.",
                    "label": 0
                },
                {
                    "sent": "Previous coherence based analysis of sampling based approximations.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the 2nd second observation is simply saying for a fixed RNM if you pick some some gamma.",
                    "label": 0
                },
                {
                    "sent": "If you pick some gamma then you can construct a matrix X that has that gamma such that there's a gap between the estimate gamma and X1 and the true coherence gamma and that gap gap gap exists so long as your sample happens to not include the first column of the Matrix an.",
                    "label": 0
                },
                {
                    "sent": "The important thing to take out of this is that there's a gap here between gamma Gamma hat.",
                    "label": 0
                },
                {
                    "sent": "The matrix is constructed in such a way that Gamma Hat is a fixed RNM is a constant, and it's very small and thus this gap is growing as growing proportional to the size of gamma.",
                    "label": 1
                },
                {
                    "sent": "So as you as your matrix is more and more coherent, the gap between your estimate and the true coherence gets bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "And again, this is a it's a simple construction to come up this matrix and it uses properties of random orthogonal model for coherence.",
                    "label": 1
                },
                {
                    "sent": "Desmond looked at previously OK, so finally I'm just going to show some experiments.",
                    "label": 0
                },
                {
                    "sent": "The analysis shows that the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explain how the algorithm works, but it's not completely clear, but it always does work, especially for.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "High coherence matrices.",
                    "label": 0
                },
                {
                    "sent": "It turns out experimentally that you tend to do a lot better.",
                    "label": 0
                },
                {
                    "sent": "So first initial results for low rank synthetic data.",
                    "label": 1
                },
                {
                    "sent": "And here we're working with matrices that are 1000 by 1000, and they have exactly rank 50.",
                    "label": 0
                },
                {
                    "sent": "We generated the matrices as follows.",
                    "label": 0
                },
                {
                    "sent": "First, the singular values are generated to decay exponentially and 2nd singular vectors.",
                    "label": 0
                },
                {
                    "sent": "We first fix one singular vector to have the desired coherence that we want, and we then use.",
                    "label": 0
                },
                {
                    "sent": "QR decomposition to get an orthogonal basis to fill out the rest of the singular vectors.",
                    "label": 0
                },
                {
                    "sent": "And we choose three different values of coherence to work with.",
                    "label": 0
                },
                {
                    "sent": "So as you recall, gamma ranges from zero to 1, so we have low, medium and high here, and so that's what we see in the left figure on the right figure you see the results of trying to estimate coherence using the algorithm that I previously discussed.",
                    "label": 0
                },
                {
                    "sent": "The X axis shows the number of columns sampled and the Y axis is showing the difference between the approximation and the exact coherence, and so first note that the.",
                    "label": 1
                },
                {
                    "sent": "That results here match the the analysis that we saw before 1st that the estimate of coherence consistency is upper bounded by the exact coherence and 2nd that the estimates are converge quicker for matrices that have low coherence, there that are more incoherent.",
                    "label": 0
                },
                {
                    "sent": "However, the takeaway here is that the true coherence is recovered when L is greater than our and.",
                    "label": 1
                },
                {
                    "sent": "That's interesting, but it's things are are very easy when everything is exactly low rank.",
                    "label": 0
                },
                {
                    "sent": "So we then looked at.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, simulated data synthetic data, but with actually full rank.",
                    "label": 1
                },
                {
                    "sent": "So we started with the same low rank synthetic data that we were working with before.",
                    "label": 0
                },
                {
                    "sent": "But then instead of zeroing out the rest of the signal, the other end minus our singular values, we instead sent them set them to equal some constant epsilon times the smallest of the singular values.",
                    "label": 1
                },
                {
                    "sent": "And here we set epsilon equal to be .1 or .9.",
                    "label": 0
                },
                {
                    "sent": "So noise small is corresponding to epsilon .1 noise largest corresponding to epsilon .9.",
                    "label": 0
                },
                {
                    "sent": "And we see here is similar similar sort of results in that when you when you get good recovery of gamma, when you sample some small multiple of the rank of the matrix.",
                    "label": 1
                },
                {
                    "sent": "So we sent here are again to be 50 and you see on the left here if you sampled 100 or 150 columns you get a very good estimate an when the noise is large you get sample more columns and you still don't get an exact estimate.",
                    "label": 0
                },
                {
                    "sent": "But when you sample two or three 100 comes you get.",
                    "label": 0
                },
                {
                    "sent": "Pretty reasonable estimate of the coherence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and finally we looked at real data.",
                    "label": 1
                },
                {
                    "sent": "We worked with seven real datasets.",
                    "label": 1
                },
                {
                    "sent": "There were all kernel matrices that range from one and a half 1000 to 5000.",
                    "label": 0
                },
                {
                    "sent": "And by an RN was 1000 to 5000 and again we sample letters that they get the coherence of these datasets and again they range from.",
                    "label": 0
                },
                {
                    "sent": "There's not really any middle here, they're either small or quite large that red one I guess can be considered middle, but it's still pretty small.",
                    "label": 0
                },
                {
                    "sent": "But then in the right figure again we showed the estimation error again, the X axis is the number of columns sampled, the Y axis is the error of approximation, and again we get good estimates after sampling roughly 100 columns and we also see as we'd expect that high coherence matrices have they converge to get estimates lower, and they also have higher variance, and I think I forgot to mention this.",
                    "label": 1
                },
                {
                    "sent": "But all of our experiments were averages over 10 trials of samples.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so and then just to get back to the point of all this.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that coherence is a sufficient condition for using these uniform sampling at random with these sampling based methods, and we did some experiments to verify whether or to validate this this observation and we were indeed able to find that this is the case.",
                    "label": 0
                },
                {
                    "sent": "So the left left figure again is just the coherence of these role of these datasets and the right figure is.",
                    "label": 0
                },
                {
                    "sent": "Showing the error of Nystrom approximation using different numbers of sample columns with error measured as follows.",
                    "label": 0
                },
                {
                    "sent": "We have a normalized error so just for business, the reconstruction error divided by the norm of the matrix to be approximated and what you can see is that the datasets with high coherence don't work as well as you'd expect, and the nice thing is that as you see, as we showed in the previous slide, we're able to to estimate this this coherence well.",
                    "label": 0
                },
                {
                    "sent": "And this is a similar plot for a different sort of low rank approximation sampler approximation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you just project your matrix onto the subspace spanned by the columns that you sample, and again you get the same sort of behavior that the datasets with high coherence don't perform as well.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's all in conclusion.",
                    "label": 0
                },
                {
                    "sent": "We presented an algorithm to estimate matrix coherence.",
                    "label": 0
                },
                {
                    "sent": "Our theory shows that the estimate depends on coherence itself, which is not necessarily the best thing, but in practice we get accurate estimates of coherence across across various values of coherence.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, we are able to show or validate again that using coherence is a good predictor of whether.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sampling uniformly random is a good thing to do for learning approximation.",
                    "label": 0
                },
                {
                    "sent": "So future work involves future work involves analyzing this algorithm in the context of full rank matrices or low rank matrices with noise, as well as possibly combining several other estimates for approximation.",
                    "label": 1
                },
                {
                    "sent": "An somewhat slightly less related line of future work would be just to come up with new analysis of the sample Miss algorithms that are independent of coherence, and using using notions such as spikiness or something that are easier to compute exactly.",
                    "label": 0
                },
                {
                    "sent": "So that's all, thanks.",
                    "label": 0
                }
            ]
        }
    }
}