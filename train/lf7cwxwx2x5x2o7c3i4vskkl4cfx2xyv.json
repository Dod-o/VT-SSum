{
    "id": "lf7cwxwx2x5x2o7c3i4vskkl4cfx2xyv",
    "title": "Multiword Expressions and Idiomaticity: How Much of the Sailing Has Been Plain?",
    "info": {
        "author": [
            "Aline Villavicencio, Federal University of Rio Grande do Sul"
        ],
        "published": "Oct. 8, 2019",
        "recorded": "September 2019",
        "category": [
            "Top->Computer Science",
            "Top->Humanities->Languages",
            "Top->Humanities->Literature"
        ]
    },
    "url": "http://videolectures.net/textSpeechDialogue_villavicencio_multiword_express/",
    "segmentation": [
        [
            "So thank you very much for the invitation.",
            "I'm delighted to be here, so please let me know if you can hear me at the back.",
            "And if my accent, my Brazilian accent is clear enough or otherwise feel free to stop me and ask for clarifications.",
            "So I'm going to be talking today about multi word expressions and you are going to and title is going to make sense in a short while.",
            "So multi word expressions.",
            "Moe"
        ],
        [
            "The word expressions are combinations that need need to be treated as a unit at some level of linguistic description, so they include compound nouns, verb, particle constructions, idioms.",
            "And they exist in all languages, so you have things in English, for example, like a French kiss, which just means a kiss, and it can be given by people of all nationalities.",
            "You don't need to be French to give a French kiss, and your brain doesn't need to be on the table.",
            "If you have an open mind.",
            "So these are combinations that contain more.",
            "Then what their individual words are telling us?",
            "So what what is so?"
        ],
        [
            "Interesting about them.",
            "An why should we care?",
            "First of all, they have lexical, syntactic, semantic and pragmatic and statistically just secrecies.",
            "So we use expressions for example, like ad hoc, even though we are not native speakers of Latin, and even though most of us wouldn't be able to tell what hoc means on its own.",
            "So they are also very arbitrary.",
            "An institutionalized.",
            "There is no reason why we prefer to say salt and pepper instead of pepper and salt, but it's just the convention that the linguistic community has adopted for a given language.",
            "And they are limited syntactics lexically and semantically, in terms of variability.",
            "So even though you can say that you kick the bucket with the meaning of dying, if you say the bucket was kicking, it was kicked.",
            "It loses the idiomatic meaning."
        ],
        [
            "The interesting thing about them is that it's estimated that about formal toward expressions are produced per minute of discourse, and they exist in the same order of magnitude as single words in the mental lexicon of a native speaker.",
            "And they also compose a large proportion of technical language, so it's estimated that between 30 and 40% of technical language are multi word expressions.",
            "It's also been found that we have faster processing times for multi words compared to normal towards, so we have some facilitation there in our brains."
        ],
        [
            "So what happens for language technology for ignore them if we don't pay attention to what they do, so I'm going to ask for some audience participation now to wake you up and to motivate why multi word expressions are interesting.",
            "So how many of you here know the expression jumped the shark?",
            "Thank you so lips sealed, so for the others, would you guess what it means?",
            "The funniest the guest, the better.",
            "Come on, Nicola.",
            "Sorry to put you on the spot.",
            "That's a good guess.",
            "Anybody else?",
            "OK, so I'm going to disclose here that jumped the shark means to try to boost audience numbers by means of drastic actions.",
            "So in this case it refers to the episode of Happy Days when Fonzie the main character needs to water ski over a tank containing a shark.",
            "So since then it has been used to describe when I show is going downhill and they do something that."
        ],
        [
            "So as you can see, it's really difficult to infer the meaning of an expression just by the meanings of the parts.",
            "And for machine translation, for example, this is really crucial.",
            "So if you try to use Google Translate to translate something like this headline, this shows jumped the shark the shark.",
            "Last year they're going to translate this literally to Portuguese, for example, and it doesn't make any sense.",
            "If you try to do tax simplification, just simplifying the words on their own's, you can end up with something like they moved over the fish which also doesn't make any sense.",
            "And if you do information retrieval you may end up with lots of pictures of Fonzie jumping over the shark which also doesn't contain the meaning of the expression."
        ],
        [
            "So for natural language processing and for example, the important tasks that we need to do to treat multi word expressions is to first of all decide if a sequence or a combination of words is a multiword expression or not.",
            "'cause we want to treat rocket science as something special that means more more than the words, but we don't want to do that with something like small boy.",
            "And we also want to know how syntactically flexible they are, because if we need to generate.",
            "We don't want to generate something that loses the meaning, or it's not natural to native speakers.",
            "And we also want to know how we domatic it is because some combinations like olive oil for example, are more transparent.",
            "We understand oil an we understand olive oil made of olives.",
            "But rocket science we can't do that.",
            "So we need to decide how to process a sequence of words in order to do that accurately.",
            "So for for the area we usually use clues from the text itself, so we use Co locational preferences trying to detect combinations that are recurrent.",
            "In text, because they are one of the indicators of multi word expressions.",
            "We try to see if they are also the contextual preferences of a possible multi word compared to the preferences of the individual components.",
            "So we try to see.",
            "If rocket science.",
            "It's used in a context that is similar to rocket or similar to science.",
            "And we also try to use to detect how fixed or how flexible are given.",
            "Expression is because multi word expressions usually are more fixed than normal standard combinations.",
            "And we also use clues, for example from multilingual data.",
            "So we try to see how a symmetric are given.",
            "Expression is as a indication if it's an expression or not.",
            "So if I translate for example, kicked the bucket in various languages, I'm going to find out that at least in one language, it's going to be translated as dying, and that's a good clue because they're completely unrelated to the words.",
            "OK so um."
        ],
        [
            "Today I'm going to just concentrate on one of them, which is using contextual preferences to try to identify if and if I put potential expression is an expression or not.",
            "So I'm going to give a quick overview."
        ],
        [
            "Of multi word expressions are crash course and then I'm going to talk about intimate icity detection using distribution automatic models, and I'm going to describe a multilingual evaluation that we did using some gold standards to the area and what conclusions we took from there."
        ],
        [
            "OK, so coming back again.",
            "What the importance of detecting if something is idiomatic or not for multi word expressions?",
            "The crucial part is that in natural language processing, we often adopt the compositionality principle.",
            "Which means that we try to construct the meaning of a sentence or the meaning of.",
            "At the whole, from the meaning of the parts.",
            "So if I tell you, for example, that the mouse is running from the Brown cat, you are probably going to imagine a mouse, a cat modified by Brown and a running event.",
            "And if you join them together, you're going to come up with an interpretation which looks more or less like this.",
            "But"
        ],
        [
            "The problem is and this has been successfully done by distributional semantic models.",
            "Ann Ryan, the invited Speaker of yesterday's afternoon was talking about these models.",
            "Which are based on the intuition by first.",
            "That you shall know award by the company it keeps, so you can infer the meaning of award by the combinations in which it occurs.",
            "So.",
            "We, if we assume that, for example, write, rewrite, compose and create can be used with author and book, we can also infer that they are semantically similar.",
            "And the other thing that these models do is that once you place award based on the context in this multi dimensional space, you can use the proximity of two points as an approximation for dissimilarity.",
            "So the closer two points are in this multi dimensional space, the more similar we assume that they are."
        ],
        [
            "OK, so the problem for multiword expressions is that there are some implicit relations and the meaning may not be straightforwardly derived from the meaning of the words.",
            "So even though I can say that a brick wall is away, that is well that is made out, made out of bricks.",
            "So there is a made of relation that is implicit there.",
            "I can't do the same, which is nice.",
            "So the relation there is going to be different.",
            "So cheese knife is not a knife made of cheese, but it's a knife for cutting cheese.",
            "Anne, it's even more extreme in the case of a loan shark, which is not a loan shark that is out too long, but it's a person who offers money at high interest rates.",
            "So there is this great, great scale of compositionality from really transparent and compositional expressions like Access Rd, which is a Rd for accessing a place, and then you have expressions like Grand Father Clock, which are related to one of the words in the expression.",
            "In this case it's a type of Clock.",
            "One of those big clocks, and you have expressions like Cloud 9 from to be in cloud Nine, which means to be in a state of happiness or bliss, and they are completely unrelated to either of the words."
        ],
        [
            "So how how can we use distributional semantic models too?",
            "Represent the meaning of multi word expressions.",
            "Well, people have been using them with cosine similarity, representing, for example the meaning of the compound like Ivory Tower.",
            "Instructing them from their occurrence in corpus.",
            "In comparing with the representation that you get by combining the representation of the individual components using an operation like for example vector addition.",
            "So you compare using cosine how far the two are going to be in multi dimensional space and the assumption is.",
            "That if they are far apart, let's say the vector for Ivory Tower as a compound is here and for Ivory Plus tower is here, then we're going to assume that they are that the compound is idiomatic.",
            "Does that make sense to everybody, OK?"
        ],
        [
            "So we one question that we can ask then is how well do these models capture?",
            "Multi word expressions, semantics and do they work well for idiomatic as well As for compositional expressions, and is the accuracy of the model dependent on characteristics such as the type of model that we're using and the language and corpora that we're using to train these models?"
        ],
        [
            "So what I'm going to describe today is an analysis that we did using over 700 around 700 models.",
            "And over 9000 comparisons and has been done in collaboration with Carlos Hamish.",
            "Mark, without an Sylvia Cordell."
        ],
        [
            "So.",
            "The justice every cap.",
            "So the distribution of semantic models.",
            "Of all kinds, from standard models like clean, two more neural network informed models like Elmo, Word, Two VEC and so on.",
            "They are simply models that tabulate counts in corpus in a corpus.",
            "So for example you have here target words and you have their older words.",
            "All the context words that occur with this target in corpus and you calculate how many times they occur together and then what these models usually do is that they use.",
            "Some associations course to decide which of these counts are really relevant and which are just random chance.",
            "Occurrence of frequent words and then what you do is you compare.",
            "Once you have this Association strength, you compare how similar.",
            "The profiles of this target words are from one another, and we're going to see that, for example, banana chocolate have more similar profiles than banana in paper, for example, so that's the basic of most models, and then neural network models.",
            "They do some clever is moving some clever calculations, but they're basically based on this."
        ],
        [
            "This type of calculation.",
            "And you have several packages that you can use that construct models for you, like for example this secton minimum antics which construct standard distributional semantic models, and then you have packages like word, two VEC, Globe Elmo which are the newest."
        ],
        [
            "Versions of these models.",
            "And in my group we constructed an alternative to some of these models that outperforms worth avec, engulfed in some of these word similarity tasks.",
            "It's freely available open source."
        ],
        [
            "And it's there.",
            "So we in our in our analysis we took models from different families, so there's standard distribution, automatic models, Grover to back, and this model that we constructed lacks back.",
            "And to compare the influence of the language, we use 3 languages.",
            "French, English and Portuguese, and we use the web as a corpus available for each of these three languages, and they contain around 2 billion words each, so more less in similar sizes.",
            "And to compare how much prep processing we needed to do for a good performance, we used from the complete.",
            "Non processed version of the corpus.",
            "The original version of the corpus there the original version with Stopwords removed, so just the the content words and.",
            "We also then lemmatized the content words and we lemmatized and added part of speech tag.",
            "To help us isn't great.",
            "For example, in English between a word used as a verb or used as a now.",
            "So we use this for versions to construct these different models.",
            "And we also used some different window sizes 1, four and eight because some studies use smaller windows.",
            "Some studies use larger windows and we wanted to know what would work best for you.",
            "Mathis city.",
            "And we also used vector representations of different sizes 200 and 5500 and 750.",
            "So the intuition here is that the smaller that I mentioned, the more compact the model needs to be, the more generalized it's going to be.",
            "The larger the model, the closer to the data, so the last compression the model is going to use.",
            "So we wanted to see what would be the best for automaticity."
        ],
        [
            "Anne, for gold standards for detecting.",
            "For measuring idiomatic you have a few of them available, one for German, using crowdsourced data for 244 compounds, and then we use three orders for English, so the fair amount at all has over 1000 compounds.",
            "That were judged by 4 experts and they were asked to save a compound worse, idiomatic an if it was conventional or not.",
            "So it was binary judgments.",
            "And there is another data set that was developed by Ready, which instead of asking for binary judgment of compositionality, which is a concept that believe me, I sometimes have difficulties understanding, even though I work in the area, so they asked crowdsourcing judgments using a notion of literality, so they asked people to say how you think this compound relates literally to this work, and how do you think it relates to this word.",
            "So more judgment that laypeople could do.",
            "And we adopted this protocol and we extended this for French and for Portuguese, and we added more compounds for for English to have similar."
        ],
        [
            "Datasets for the three languages.",
            "So for English we ended up with 270 compounds and 180 for French and Portuguese, and they're all of the same type of compounds.",
            "Which is compiled nouns for English.",
            "That means either 2 nouns or an adjective or noun, and for French the similar would be nouns, followed by adjectives like morcilla and for Portuguese morcela.",
            "As in the characteristic of this data set is that it's balanced for compositionality because we wanted to see if the models would work with Nest for compositional, or for idiomatic compounds.",
            "So we have 1/3 of them, which is idiomatic.",
            "1/3 that is partly compositional and 1/3 that is completely composition."
        ],
        [
            "And their judgments, just to give you an idea, the judgments that we ask people to do would be 3.",
            "Three questions for each of the compounds.",
            "So we would give the annotators three sentences where the compound appeared in each of the sentences.",
            "Had the compound with exactly the same sense.",
            "So either the dramatic or the literal.",
            "And we asked for example for the compound is a climate change truly literally our change in climate?",
            "So we rephrased we paraphrase the compound using the combination of the words.",
            "And then for each of the components we also asked."
        ],
        [
            "Is an ivory tower literally made of ivory and easily ivory tower, literally a tower, and we asked people to use this Likert scale from zero to five.",
            "So zero meant they didn't agree with that, and five meant that they fully agreed with that.",
            "So."
        ],
        [
            "Collected this judgments for using Mechanical Turkers for English and bit of Mechanical Turkers for French, but for Portuguese we didn't have enough native speakers in available in Amazon at the time, so we asked our annotators our computer science students and linguists.",
            "To annotate the data.",
            "But the good thing is that we could then compare the quality of the judgments of the known crowd source and the crowd source workers.",
            "And the judgments were very, very compatible.",
            "I'm happy to say.",
            "And the other thing that we could do, since we had the annotations in Portuguese by the same annotators West to calculate the Alpha in cap agreement.",
            "Among them so in general, our annotators for Portuguese were had an Alpha agreement of .42 for the compound .50, two for the head, an .36 for the modifier, so reasonable agreement.",
            "But to give you an idea of the difficulty of the task, we asked the same annotator, linguist linguistics, PhD, translator, and to re annotate the compounds one month later and the only agreement that he could get with himself was point 59 for the compound.",
            "So you can see how difficult this judgment is.",
            "And I'll tell you why in a second.",
            "But the good thing is that we can use this so the office call would be .49 and the Spearman correlation would be .77.",
            "So we can use this as an upper bound for what our models could be expected to reach, because if not even a human can agree with himself one month later, we can't expect models to do better than them.",
            "Right?"
        ],
        [
            "And so the here are some of the compounds that they had to judge and on the top you can see the compounds where they had the least agreement.",
            "The more variation between the annotators.",
            "And in the bottom part you can see the compounds where they really had a lot of agreement.",
            "So most of our painters agreed, for example, that a graduate student was really a student was literally a student, and was literally doing graduate studies.",
            "But they had difficulties agreeing whether a dirty word was a metaphorical.",
            "I don't know, swear word or if it was more a completely idiomatic expression, so some of them interpreted dirt as more metaphorical sense of dirt, so I'm more police in most cases than a real idiomatic case, so you can see here that they had almost two points different in terms of the.",
            "The head and 2.5 in terms of the whole compound.",
            "Or actually 2.5 with one point 7 of standard variation."
        ],
        [
            "So OK.",
            "So if I had to tell you how our models did, we were pleasantly surprised that they did very well in this judgment.",
            "So when we compared what the Spearman correlation that our models got with their human annotators, they got .82.",
            "For the original Ready data set.",
            "Which was more than the authors of the Ready data set got when they did the evaluation?",
            "And we now we're extended data set we got .73.",
            "So we think it was a bit more difficult.",
            "But still they the models got a good performance and for French and Portuguese the results were slightly lower.",
            "So for French .7 Spearman correlation and for Portuguese .6.",
            "But remember that our annotator got .77 with himself and he's a very picky linguist.",
            "So we can see here in each of these charts the the Spearman correlation on the Y axis and the family of models in the X.",
            "So in here you have the more traditional models.",
            "Here you have clothes and here you have the two versions versions of Word, two Vec C, Bowen, Skip gram.",
            "So you can see that here is the two models that perform best was were to VAC and our traditional PMI with some.",
            "I'm.",
            "Some dimensionality reduction.",
            "So the two of them got around the same, so I think this one got .72 and this one got .73 three, one correlation.",
            "And the same models were the best also for for English.",
            "For Portuguese, so here is French and here is Portuguese.",
            "For Portuguese the word of X didn't do as well, and the models that did better where the traditional link based distributional semantic models.",
            "So in this model, so you can actually.",
            "The good part is that you can read them well, you know what each of the dimensions mean.",
            "So for example, one dimension could be eat, another dimension could be like, so you can actually read what they're doing and interpret, and we use this just to test our understanding of what these models were doing.",
            "And these were the ones that fared better for the tooling."
        ],
        [
            "Which is.",
            "So when we try to see.",
            "If this was due to the level of preprocessing that we were doing in the data, what we found was that for English, as you probably notice that if you use one of these models, no amount of processing is needed, so you can use the corpus as it is with as much noise as it has.",
            "And it's going to make no difference, at least for a dramatic that action.",
            "If you actually clean the corpus.",
            "If ulimit eyes.",
            "If you do some preprocessing so the results actually got worse when we did that forward to back models.",
            "But for French and for Portuguese, because they are morphologically richer languages.",
            "If you don't do any pre processing.",
            "So in here you can see the in the light you can see the most pre processing.",
            "So the lemmatized form and here in darker color you can see the original corpus.",
            "So in French, if you don't do any processing, you actually lose considerably and it pays off doing lemmatization, an stopword removal and the same goes for Portuguese.",
            "So the two models that did best were the ones where we use the lemmatized version."
        ],
        [
            "And what about in terms of dimensions?",
            "So when we looked at the dimensions, the best models for each of the languages and most of the top performing preferred larger dimensions for representing idiomaticity.",
            "So if we compress the models too much.",
            "Then then they lost quite alot, specially for Portuguese.",
            "You can see here.",
            "So the lighter is the 250 in the middle the 500 and the 750 in the darkest blue.",
            "And it gets that there is a large difference there."
        ],
        [
            "An what about in terms of window sizes?",
            "So the the hypothesis is that the smaller windows get more syntactic relations and the larger windows get more semantic relations.",
            "But we didn't find anything that was.",
            "Was robust across the languages are across the models, so we had very variable results and we couldn't get any conclusions there.",
            "But what we can say is that for the three best models in each of the languages, the smaller window which is in the lighter color was the one that gave better results and reducing the window.",
            "Actually got worse results."
        ],
        [
            "So the other question that we could ask is if the difference in performance between the languages was due to the corpus size and so here you have the the.",
            "In this chart you have the corpus size and performance for our compounds.",
            "And what do you have?",
            "So for Portuguese, you're having green in English or having bread and French in blue.",
            "So we divided the corpus in 10 different portions and we used increment and retrain the models in Crim incrementally larger portions.",
            "So in here you have for example 2 two parts of the corpus, so 210 / 2 and you can see that the models actually stabilize.",
            "With around 1 billion warrants for each of the three languages afterwards, you can give it more data, but it doesn't make much difference, so the performance is more or less the same in our intuition.",
            "For that when we looked at the results with that.",
            "But when we had one billion words we have we had enough data for all of the compounds to be learned.",
            "And then what came Additionally didn't make much difference an it's the case for both the traditional distributional semantic models, the one that performed best and where to back, which was the one that performed best for English.",
            "So around 1 billion words was all we needed for the three languages, so the difference in the languages is actually what is causing the difference in performance there."
        ],
        [
            "OK, so to conclude.",
            "So we looked at the extent to which we could use distributional semantic models to capture idiomaticity in compound nouns.",
            "In three languages, French, English, and Portuguese, and we looked at how they would fare compared to human judgments and for multiword expressions of various levels of idiomatic city.",
            "And we looked at whether their accuracy was related to the characteristics of the models.",
            "Or to the characteristics of the language and corporate that we were using."
        ],
        [
            "To do this study, we had to construct this data set of compositionality judgments, and it's a resource that is freely available and the protocol.",
            "Is easily replicated for different languages, so if you feel like doing this for your own language, we had an athlete tutorial last year where we ask participants from different languages to give us some some indications and they had a lot of fun.",
            "So I recommend if you have students and you don't know which activity to give them, give them some compositionality judgments and then let me know how you find.",
            "And."
        ],
        [
            "We also collected for each of the compounds that we asked our judges to analyze a paraphrase or a synonym for the compound.",
            "So with them constructed for Portuguese data, set of substitution of these compounds.",
            "Sometimes a compound can be substituted by a single word, sometimes by paraphrases, but we have then and this data set can be used, for example for text simplification.",
            "And it's also freely available."
        ],
        [
            "At the moment it's only available for Portuguese, but we intend to release the Portuguese and French versions soon.",
            "So to summarize, for English, French and Portuguese, we constructed over 600 models, almost 700 models, almost 9000 evaluations using different families of distributional semantic models, and I know that most of you and probably your students are really keen on using neural network models, but the distributional semantic models the more traditional kind, also pays off.",
            "And it's easier to read and understand what's going on there.",
            "What I didn't include here was that we had as good performance for the dramatic compounds as we had for the compositional.",
            "Transparent compounds so the level of compositionality actually didn't affect how these models were performing."
        ],
        [
            "Most of these analysis were done using the multi word Expressions Toolkit, which is also freely available and was developed by Carlos, Hamish and Silvio Cordell.",
            "The Co.",
            "Authors of the paper that I just described to you.",
            "It does all your multiword expression needs, so from extracting multi words from corporate.",
            "If you just have a corpus to a notating multi words back into a corpus.",
            "If you have a Dictionary of multi word expressions and you want to find them in a corpus.",
            "It calculates Association measures for you.",
            "And a number of them.",
            "So dies Chi Square point rates, mutual information.",
            "So a number of them is implemented there.",
            "And once you have the word embeddings, it also helps you compare with the data set how your word embeddings are doing.",
            "So the evaluations that I told you about."
        ],
        [
            "So as future work, we now that we know that this representations they work really well at representing Idiomatic City.",
            "We want to see how much more precise we can make them 'cause for Portuguese.",
            "If we compare with the human performance there, there is still a large margin of improvement that we can that we can expect our models to have.",
            "So what we want to do is actually to see how we can incorporate some knowledge about the dramatic city.",
            "Maybe some guide how the model is learning to see if we can get better performance and there were some papers at ACL this year, for example which were exploring different models.",
            "Of semantic representation for more accurate performance and you have for example, even more sophisticated models which use different types of multi dimensional representation.",
            "And we also want to use this models for token idiomatic city identification.",
            "So when you see a compound in the sentence or a potential compounding the sentence, is it used in the literal sense or easy to use Indian idiomatic sense in that sentence?",
            "So for example, I say that I don't know Nikolaou kick the bucket.",
            "I probably mean to say that he just bumped on a container and not that he died.",
            "Sorry about using OK, so we want to use that.",
            "The knowledge of these models to also help us in this task.",
            "And this is the crucial one.",
            "For example, for tax simplification for words, Ness is ambiguation for machine translation, and for conversational agents, if you are working with them.",
            "So we want to use also these linguistic intuitions about how fixed are given multi word is to detect idiomaticity."
        ],
        [
            "OK, so to summarize this, this was a collaboration done with researchers from Brazil from France and it was sponsored by the Brazilian Funding Agency.",
            "So thank you very much for the invitation to talk here."
        ],
        [
            "And now just to give you a teaser here.",
            "For those of you who speak German, just read the top line and you will see some interesting cases which you probably never considered.",
            "For those of you like me, who unfortunately don't speak German, there are the translations there.",
            "So apparently in German if you have tomatoes in your eyes, it means that you are seeing things that no, no analysis thing, and if you if you only understand the train station, it means that you don't understand anything.",
            "So probably 4.",
            "Most of us it's going to be his speaking Greek or he's speaking some language which is not very familiar in your native country.",
            "So there you have it.",
            "So any questions?",
            "Respective German you have the compound.",
            "Does that make the problem more difficult because you first have to identify them in other languages.",
            "You have the space in between so.",
            "That's a great question, so we are when we are working with text and we're working with romance languages or with English.",
            "We have the the ease of having a white space.",
            "Is telling us that it's different words, but for a lot of the languages and probably half of the population of the world, we don't have this this luxury.",
            "So for example, in languages like Chinese, some Arabic language is German.",
            "One word can have various characters and you don't know where where to divide, but so some of the groups who have been working on multi word expressions.",
            "What they do is first day segment the words where they think the spaces could be.",
            "So they say OK, this potentially could be a space so they segment the words first and then they do multi word expressions extraction.",
            "But what we found that for example for Germany it's actually quite good to have the big compounding word because that's an indication over edited.",
            "2nd pound, so it's actually a plus rather than a mile.",
            "A word to be alert then that would be an indication exactly and then the question is to determine how idiomatic or how compositionally you can construct the meaning of the word.",
            "Do you have any idea how the whole game changes now with these?",
            "Dynamic embeddings, so once we go from static, remembering that expires models, how would approaches or how those probably reason just ordered approaching problems?",
            "Thank you very much for the questions, so there there are more more modern kind of.",
            "Incorporations of these models these days?",
            "And you're probably thinking OK, were tobacco.",
            "This is so old news, but for us to understand what these models were doing with you, idiomatic City, we wanted to be able to read the models.",
            "And that's why we use some very traditional models like lens for example, which is based on Point Reyes mutual information.",
            "And as I said, you can read the dimensions.",
            "So if you could understand what were the salient information that these models were doing, we hope to understand what the neural network models were doing.",
            "So one big area of research these days is trying to interpret neural network and how they incorporate information.",
            "So we were thinking about doing some kind of black box reengineering.",
            "To understand what they could be capturing.",
            "But there are models, for example like Elmo, which are contextual, contextually aware.",
            "So for each of the words or move towards that you have there, they would have a slightly different representation based on the sentence in which it occurred.",
            "So you would have for example, instead of having a type representation for the MULTIWORD expression.",
            "So a single one for kick the bucket, you would have various representations for kick the bucket depending on the different sentences in which it occurred.",
            "So our idea with this models is to see if you really need to have all that expensive machinery.",
            "Because these are really expensive models to train and you need a lot of data.",
            "And for a lot of the compounds, even though they are very common in daily language, for you to get enough instances of these models, sometimes it's difficult.",
            "So we want to see if we can use linguistically informed models that are perhaps more compact and easier to train than the contextually aware model.",
            "So we are looking at ways of doing that at the moment.",
            "Thank you.",
            "Very much involved in this FIFA.",
            "What bothers me?",
            "Should we really rely mostly just on some kind of unique approaches?",
            "Find summer presentations while not trying to do some dialogue, system agent and or trying to learn from our conversations or human being without buying, understand, compounding.",
            "Station is not locked.",
            "Some kind of knowledge base.",
            "Effective language models with platform that is rare.",
            "Simone, thank you very much and that's why I really like to be here because I'm hoping that you give me ideas for future works, so that's a great idea, and this is actually probably closer.",
            "So if you are a native speaker of English, for example, you probably remember how hard it was to learn some of these expressions and how they didn't make any sense and how you had to learn them by heart.",
            "So you had to have lists of memorized.",
            "Some of these expressions.",
            "And it's the same for us for multiple languages.",
            "And somehow we learn and children learn, so there should be something enough in the in the context.",
            "An interactions that can help you learn these expressions.",
            "So having something like a tutored learning would be really nice, so some of the things that are being researched any probably have seen this adversarial networks and you could use them, for example to try to do something like that so.",
            "Have a network generating something and see if it would be understood by another network and then.",
            "Try to see if they would reach an agreement about the meaning of the compound, or perhaps using a dialogue system and helping second language learners to reach.",
            "To help with possible avenues of errors or correct understanding of the compounds.",
            "Literally anything can keep talking Lawrence exactly.",
            "So if I can open a brief ( 1 of the fun parts of this work was also that we used.",
            "Some of these techniques to analyze children's data so the child's corpus in English, and we looked, we identified the verb particle constructions that the adult sentences contained, and that the children sentence contained, and we compared the overlapping vocabulary and the usage and what we found was that you needed more or less 5 instances, 5 occurrences of a given vertical constructions for their children in the adult data for their children to start using the.",
            "For particle construction, but they were using quite closely what the adults were doing, so they probably were doing something like this language game implicitly and they learn very efficiently from 5 instances they were using, so whatever the vertical constructions in English, they can be separated.",
            "The verb can be separated by the particle by up to 5 words.",
            "And most of our data was.",
            "There was mostly one or two words in between the verb and the particle, but whatever the adults were doing, their children were also doing.",
            "The difference that we found was that in the top ten work, 10 vertical constructions that were used by the children, they had a lot of fell down, fell down, fell down, and the adults had pick up.",
            "Pick up pick up.",
            "So probably parenting children discussions there.",
            "Is there something like not very familiar with public, but is there something like it?",
            "Long term multi word expressions versus?",
            "This incident creation that will just because something happens in the world and then you know people use it for a couple weeks or months and then it dies.",
            "Is there a different use of those or happening?",
            "Thank you so much.",
            "That's another really interesting questions, so some people working on the area have been looking at diachronic corpora, so trying to see how an expression changes sense and how they incorporate this more idiomatic meaning, because at some point some of these expressions were quite understandable.",
            "So, for example, kick the bucket.",
            "Apparently the meaning of kick the bucket has to do it, pigs being slaughtered, and some buckets.",
            "So probably people could understanding.",
            "I don't know 1600s when they were using this expression for the first time, but now, for as it's completely idiomatic, we can't reconstruct the meaning.",
            "And one of the things about multi word expressions is that they are very convenient way of expressing a situation in a very short format.",
            "So in English it's beautiful because you can just compile.",
            "You can aggregate as many nouns as you want and you have a compound and people probably understand so you have to have ways of automatically going through Corporation.",
            "Finding this new expression and trying to find the meaning for them.",
            "See if you can find at least the implicit relation that could.",
            "Be between the words like a brick wall or if something completely different, but there is a study about it and the other thing is that you can think of OK, so let's just pick any sequence of towards and believe it's a compound.",
            "If you go to chemical tags for example, you have compounds that are 30 words long.",
            "So in in domain specific texts.",
            "You have this huge huge words, so if you think that you have to construct all possible sequences from a corpus of up to 30 words long, then you are going to have a lot of work just trying to get enough statistics for each of the possible compounds.",
            "I would like to.",
            "Comment on one paper which is actually also by Carlson Sylvia.",
            "It's a position statements.",
            "Expressions.",
            "I think they Seattle this year.",
            "Says the time says without lexicons, module expression identification would never fly.",
            "So.",
            "I mean what the paper says is basically that.",
            "You can put the long efforts.",
            "Machine.",
            "In the end, you can actually make the list of wonderful expressions in the language.",
            "And.",
            "Usually this kind of a camera is on.",
            "Things like.",
            "Dictionaries.",
            "So.",
            "Question, I think if.",
            "Into making this list and.",
            "Michael Expression identification will actually become the simulation of liberal senators versus something that is in the lexicon.",
            "Thank you so much and that's a great food for thought to use a multiword expression there.",
            "So I think what I completely agree with them an I have to say that I'm a computational linguist.",
            "I'm not frustrated, I'm not a linguist unfortunately, but I'm a computational linguist, so I really believe that without some linguistic intuition with some linguistic knowledge, what you learn is something, but you don't know what it is.",
            "So most of these models are learning something and sometimes we can't interpret what they learn.",
            "So you can.",
            "You can create a model, but if you don't have any way of evaluating what the model is learning.",
            "Then you may have garbage in the end, so I haven't rented their paper, but thanks for summarizing and what I would, I would say is that.",
            "For many domains and many languages, we still need lists like that, so even for a computer science, depending on the topic that you are working on, you are going to have specific ways of describing this terms and what I think that this methods can do is to facilitate the work of a lexicographer or a terminal terminologies.",
            "Because it's actually quite quite hard work and no matter how many words you put into a list of multi word expressions, there are always going to be more, and they're always going to be created.",
            "So we need to somehow facilitates that, but without having the list of multi word expressions you can't evaluate how your models are doing so it's a chicken and egg kind of game.",
            "So I believe my view is that we can help the lexicographers and they can help us evaluate these methods, but I would never do without lexicographers an without linguistic intuition there.",
            "Specially when you look at syntax and what you can generate.",
            "It's not clear that you that you can generate like standard language, so known multiword expressions and you really need syntactic knowledge and you need specific knowledge about the variance to be able to know what you can generate from a given expression.",
            "So I think I agree with them and I I'm curious now to read what they're saying, but I think it's a provocation to just automatisering everything and believing whatever the model tells you is correct.",
            "So be sceptical.",
            "Active.",
            "Lexical type of language knowledge base.",
            "Just what I would try to imagine fixed and looking forward to the Registrar's office to make something more standardized.",
            "Moving forward.",
            "Spoilers.",
            "Compounds public events semantic software from PowerShell.",
            "Just some next description or something post.",
            "The description is right.",
            "Well, I have a machine.",
            "Apology syntax and semantics.",
            "And so the they have been involved in this cost project, which I don't know if everyone is familiar here.",
            "So it was a really, really important initiative by the RCM cost action.",
            "So I think Nikolaou here was involved in it.",
            "Yeah, exactly.",
            "And so there were lexicons created for multi word expressions or variables languages.",
            "So if you saw his poster yesterday so now now you have for three domains specific vocabulary for multi word expressions for is laughing, but it's really hard work and it was.",
            "I think there's a lot of languages were described by that.",
            "So if you are interested in multi word expressions for.",
            "Languages of Europe.",
            "The RCM cost action is a good place to look at."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much for the invitation.",
                    "label": 0
                },
                {
                    "sent": "I'm delighted to be here, so please let me know if you can hear me at the back.",
                    "label": 0
                },
                {
                    "sent": "And if my accent, my Brazilian accent is clear enough or otherwise feel free to stop me and ask for clarifications.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking today about multi word expressions and you are going to and title is going to make sense in a short while.",
                    "label": 0
                },
                {
                    "sent": "So multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "Moe",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The word expressions are combinations that need need to be treated as a unit at some level of linguistic description, so they include compound nouns, verb, particle constructions, idioms.",
                    "label": 1
                },
                {
                    "sent": "And they exist in all languages, so you have things in English, for example, like a French kiss, which just means a kiss, and it can be given by people of all nationalities.",
                    "label": 0
                },
                {
                    "sent": "You don't need to be French to give a French kiss, and your brain doesn't need to be on the table.",
                    "label": 0
                },
                {
                    "sent": "If you have an open mind.",
                    "label": 0
                },
                {
                    "sent": "So these are combinations that contain more.",
                    "label": 0
                },
                {
                    "sent": "Then what their individual words are telling us?",
                    "label": 0
                },
                {
                    "sent": "So what what is so?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting about them.",
                    "label": 0
                },
                {
                    "sent": "An why should we care?",
                    "label": 0
                },
                {
                    "sent": "First of all, they have lexical, syntactic, semantic and pragmatic and statistically just secrecies.",
                    "label": 1
                },
                {
                    "sent": "So we use expressions for example, like ad hoc, even though we are not native speakers of Latin, and even though most of us wouldn't be able to tell what hoc means on its own.",
                    "label": 0
                },
                {
                    "sent": "So they are also very arbitrary.",
                    "label": 0
                },
                {
                    "sent": "An institutionalized.",
                    "label": 0
                },
                {
                    "sent": "There is no reason why we prefer to say salt and pepper instead of pepper and salt, but it's just the convention that the linguistic community has adopted for a given language.",
                    "label": 1
                },
                {
                    "sent": "And they are limited syntactics lexically and semantically, in terms of variability.",
                    "label": 0
                },
                {
                    "sent": "So even though you can say that you kick the bucket with the meaning of dying, if you say the bucket was kicking, it was kicked.",
                    "label": 0
                },
                {
                    "sent": "It loses the idiomatic meaning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The interesting thing about them is that it's estimated that about formal toward expressions are produced per minute of discourse, and they exist in the same order of magnitude as single words in the mental lexicon of a native speaker.",
                    "label": 1
                },
                {
                    "sent": "And they also compose a large proportion of technical language, so it's estimated that between 30 and 40% of technical language are multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "It's also been found that we have faster processing times for multi words compared to normal towards, so we have some facilitation there in our brains.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happens for language technology for ignore them if we don't pay attention to what they do, so I'm going to ask for some audience participation now to wake you up and to motivate why multi word expressions are interesting.",
                    "label": 0
                },
                {
                    "sent": "So how many of you here know the expression jumped the shark?",
                    "label": 0
                },
                {
                    "sent": "Thank you so lips sealed, so for the others, would you guess what it means?",
                    "label": 0
                },
                {
                    "sent": "The funniest the guest, the better.",
                    "label": 0
                },
                {
                    "sent": "Come on, Nicola.",
                    "label": 0
                },
                {
                    "sent": "Sorry to put you on the spot.",
                    "label": 0
                },
                {
                    "sent": "That's a good guess.",
                    "label": 0
                },
                {
                    "sent": "Anybody else?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to disclose here that jumped the shark means to try to boost audience numbers by means of drastic actions.",
                    "label": 1
                },
                {
                    "sent": "So in this case it refers to the episode of Happy Days when Fonzie the main character needs to water ski over a tank containing a shark.",
                    "label": 0
                },
                {
                    "sent": "So since then it has been used to describe when I show is going downhill and they do something that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as you can see, it's really difficult to infer the meaning of an expression just by the meanings of the parts.",
                    "label": 0
                },
                {
                    "sent": "And for machine translation, for example, this is really crucial.",
                    "label": 1
                },
                {
                    "sent": "So if you try to use Google Translate to translate something like this headline, this shows jumped the shark the shark.",
                    "label": 0
                },
                {
                    "sent": "Last year they're going to translate this literally to Portuguese, for example, and it doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "If you try to do tax simplification, just simplifying the words on their own's, you can end up with something like they moved over the fish which also doesn't make any sense.",
                    "label": 1
                },
                {
                    "sent": "And if you do information retrieval you may end up with lots of pictures of Fonzie jumping over the shark which also doesn't contain the meaning of the expression.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for natural language processing and for example, the important tasks that we need to do to treat multi word expressions is to first of all decide if a sequence or a combination of words is a multiword expression or not.",
                    "label": 1
                },
                {
                    "sent": "'cause we want to treat rocket science as something special that means more more than the words, but we don't want to do that with something like small boy.",
                    "label": 1
                },
                {
                    "sent": "And we also want to know how syntactically flexible they are, because if we need to generate.",
                    "label": 1
                },
                {
                    "sent": "We don't want to generate something that loses the meaning, or it's not natural to native speakers.",
                    "label": 0
                },
                {
                    "sent": "And we also want to know how we domatic it is because some combinations like olive oil for example, are more transparent.",
                    "label": 1
                },
                {
                    "sent": "We understand oil an we understand olive oil made of olives.",
                    "label": 0
                },
                {
                    "sent": "But rocket science we can't do that.",
                    "label": 0
                },
                {
                    "sent": "So we need to decide how to process a sequence of words in order to do that accurately.",
                    "label": 0
                },
                {
                    "sent": "So for for the area we usually use clues from the text itself, so we use Co locational preferences trying to detect combinations that are recurrent.",
                    "label": 0
                },
                {
                    "sent": "In text, because they are one of the indicators of multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "We try to see if they are also the contextual preferences of a possible multi word compared to the preferences of the individual components.",
                    "label": 0
                },
                {
                    "sent": "So we try to see.",
                    "label": 0
                },
                {
                    "sent": "If rocket science.",
                    "label": 0
                },
                {
                    "sent": "It's used in a context that is similar to rocket or similar to science.",
                    "label": 0
                },
                {
                    "sent": "And we also try to use to detect how fixed or how flexible are given.",
                    "label": 0
                },
                {
                    "sent": "Expression is because multi word expressions usually are more fixed than normal standard combinations.",
                    "label": 0
                },
                {
                    "sent": "And we also use clues, for example from multilingual data.",
                    "label": 0
                },
                {
                    "sent": "So we try to see how a symmetric are given.",
                    "label": 0
                },
                {
                    "sent": "Expression is as a indication if it's an expression or not.",
                    "label": 0
                },
                {
                    "sent": "So if I translate for example, kicked the bucket in various languages, I'm going to find out that at least in one language, it's going to be translated as dying, and that's a good clue because they're completely unrelated to the words.",
                    "label": 0
                },
                {
                    "sent": "OK so um.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to just concentrate on one of them, which is using contextual preferences to try to identify if and if I put potential expression is an expression or not.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give a quick overview.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of multi word expressions are crash course and then I'm going to talk about intimate icity detection using distribution automatic models, and I'm going to describe a multilingual evaluation that we did using some gold standards to the area and what conclusions we took from there.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so coming back again.",
                    "label": 0
                },
                {
                    "sent": "What the importance of detecting if something is idiomatic or not for multi word expressions?",
                    "label": 0
                },
                {
                    "sent": "The crucial part is that in natural language processing, we often adopt the compositionality principle.",
                    "label": 0
                },
                {
                    "sent": "Which means that we try to construct the meaning of a sentence or the meaning of.",
                    "label": 0
                },
                {
                    "sent": "At the whole, from the meaning of the parts.",
                    "label": 1
                },
                {
                    "sent": "So if I tell you, for example, that the mouse is running from the Brown cat, you are probably going to imagine a mouse, a cat modified by Brown and a running event.",
                    "label": 0
                },
                {
                    "sent": "And if you join them together, you're going to come up with an interpretation which looks more or less like this.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is and this has been successfully done by distributional semantic models.",
                    "label": 1
                },
                {
                    "sent": "Ann Ryan, the invited Speaker of yesterday's afternoon was talking about these models.",
                    "label": 0
                },
                {
                    "sent": "Which are based on the intuition by first.",
                    "label": 0
                },
                {
                    "sent": "That you shall know award by the company it keeps, so you can infer the meaning of award by the combinations in which it occurs.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We, if we assume that, for example, write, rewrite, compose and create can be used with author and book, we can also infer that they are semantically similar.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that these models do is that once you place award based on the context in this multi dimensional space, you can use the proximity of two points as an approximation for dissimilarity.",
                    "label": 0
                },
                {
                    "sent": "So the closer two points are in this multi dimensional space, the more similar we assume that they are.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the problem for multiword expressions is that there are some implicit relations and the meaning may not be straightforwardly derived from the meaning of the words.",
                    "label": 0
                },
                {
                    "sent": "So even though I can say that a brick wall is away, that is well that is made out, made out of bricks.",
                    "label": 0
                },
                {
                    "sent": "So there is a made of relation that is implicit there.",
                    "label": 1
                },
                {
                    "sent": "I can't do the same, which is nice.",
                    "label": 0
                },
                {
                    "sent": "So the relation there is going to be different.",
                    "label": 0
                },
                {
                    "sent": "So cheese knife is not a knife made of cheese, but it's a knife for cutting cheese.",
                    "label": 1
                },
                {
                    "sent": "Anne, it's even more extreme in the case of a loan shark, which is not a loan shark that is out too long, but it's a person who offers money at high interest rates.",
                    "label": 0
                },
                {
                    "sent": "So there is this great, great scale of compositionality from really transparent and compositional expressions like Access Rd, which is a Rd for accessing a place, and then you have expressions like Grand Father Clock, which are related to one of the words in the expression.",
                    "label": 0
                },
                {
                    "sent": "In this case it's a type of Clock.",
                    "label": 0
                },
                {
                    "sent": "One of those big clocks, and you have expressions like Cloud 9 from to be in cloud Nine, which means to be in a state of happiness or bliss, and they are completely unrelated to either of the words.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how how can we use distributional semantic models too?",
                    "label": 0
                },
                {
                    "sent": "Represent the meaning of multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "Well, people have been using them with cosine similarity, representing, for example the meaning of the compound like Ivory Tower.",
                    "label": 1
                },
                {
                    "sent": "Instructing them from their occurrence in corpus.",
                    "label": 0
                },
                {
                    "sent": "In comparing with the representation that you get by combining the representation of the individual components using an operation like for example vector addition.",
                    "label": 0
                },
                {
                    "sent": "So you compare using cosine how far the two are going to be in multi dimensional space and the assumption is.",
                    "label": 0
                },
                {
                    "sent": "That if they are far apart, let's say the vector for Ivory Tower as a compound is here and for Ivory Plus tower is here, then we're going to assume that they are that the compound is idiomatic.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense to everybody, OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we one question that we can ask then is how well do these models capture?",
                    "label": 0
                },
                {
                    "sent": "Multi word expressions, semantics and do they work well for idiomatic as well As for compositional expressions, and is the accuracy of the model dependent on characteristics such as the type of model that we're using and the language and corpora that we're using to train these models?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to describe today is an analysis that we did using over 700 around 700 models.",
                    "label": 0
                },
                {
                    "sent": "And over 9000 comparisons and has been done in collaboration with Carlos Hamish.",
                    "label": 0
                },
                {
                    "sent": "Mark, without an Sylvia Cordell.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The justice every cap.",
                    "label": 0
                },
                {
                    "sent": "So the distribution of semantic models.",
                    "label": 1
                },
                {
                    "sent": "Of all kinds, from standard models like clean, two more neural network informed models like Elmo, Word, Two VEC and so on.",
                    "label": 0
                },
                {
                    "sent": "They are simply models that tabulate counts in corpus in a corpus.",
                    "label": 0
                },
                {
                    "sent": "So for example you have here target words and you have their older words.",
                    "label": 0
                },
                {
                    "sent": "All the context words that occur with this target in corpus and you calculate how many times they occur together and then what these models usually do is that they use.",
                    "label": 0
                },
                {
                    "sent": "Some associations course to decide which of these counts are really relevant and which are just random chance.",
                    "label": 0
                },
                {
                    "sent": "Occurrence of frequent words and then what you do is you compare.",
                    "label": 1
                },
                {
                    "sent": "Once you have this Association strength, you compare how similar.",
                    "label": 0
                },
                {
                    "sent": "The profiles of this target words are from one another, and we're going to see that, for example, banana chocolate have more similar profiles than banana in paper, for example, so that's the basic of most models, and then neural network models.",
                    "label": 0
                },
                {
                    "sent": "They do some clever is moving some clever calculations, but they're basically based on this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This type of calculation.",
                    "label": 0
                },
                {
                    "sent": "And you have several packages that you can use that construct models for you, like for example this secton minimum antics which construct standard distributional semantic models, and then you have packages like word, two VEC, Globe Elmo which are the newest.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Versions of these models.",
                    "label": 0
                },
                {
                    "sent": "And in my group we constructed an alternative to some of these models that outperforms worth avec, engulfed in some of these word similarity tasks.",
                    "label": 1
                },
                {
                    "sent": "It's freely available open source.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's there.",
                    "label": 0
                },
                {
                    "sent": "So we in our in our analysis we took models from different families, so there's standard distribution, automatic models, Grover to back, and this model that we constructed lacks back.",
                    "label": 0
                },
                {
                    "sent": "And to compare the influence of the language, we use 3 languages.",
                    "label": 0
                },
                {
                    "sent": "French, English and Portuguese, and we use the web as a corpus available for each of these three languages, and they contain around 2 billion words each, so more less in similar sizes.",
                    "label": 0
                },
                {
                    "sent": "And to compare how much prep processing we needed to do for a good performance, we used from the complete.",
                    "label": 0
                },
                {
                    "sent": "Non processed version of the corpus.",
                    "label": 0
                },
                {
                    "sent": "The original version of the corpus there the original version with Stopwords removed, so just the the content words and.",
                    "label": 0
                },
                {
                    "sent": "We also then lemmatized the content words and we lemmatized and added part of speech tag.",
                    "label": 0
                },
                {
                    "sent": "To help us isn't great.",
                    "label": 0
                },
                {
                    "sent": "For example, in English between a word used as a verb or used as a now.",
                    "label": 0
                },
                {
                    "sent": "So we use this for versions to construct these different models.",
                    "label": 0
                },
                {
                    "sent": "And we also used some different window sizes 1, four and eight because some studies use smaller windows.",
                    "label": 0
                },
                {
                    "sent": "Some studies use larger windows and we wanted to know what would work best for you.",
                    "label": 0
                },
                {
                    "sent": "Mathis city.",
                    "label": 0
                },
                {
                    "sent": "And we also used vector representations of different sizes 200 and 5500 and 750.",
                    "label": 0
                },
                {
                    "sent": "So the intuition here is that the smaller that I mentioned, the more compact the model needs to be, the more generalized it's going to be.",
                    "label": 0
                },
                {
                    "sent": "The larger the model, the closer to the data, so the last compression the model is going to use.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to see what would be the best for automaticity.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne, for gold standards for detecting.",
                    "label": 0
                },
                {
                    "sent": "For measuring idiomatic you have a few of them available, one for German, using crowdsourced data for 244 compounds, and then we use three orders for English, so the fair amount at all has over 1000 compounds.",
                    "label": 0
                },
                {
                    "sent": "That were judged by 4 experts and they were asked to save a compound worse, idiomatic an if it was conventional or not.",
                    "label": 0
                },
                {
                    "sent": "So it was binary judgments.",
                    "label": 0
                },
                {
                    "sent": "And there is another data set that was developed by Ready, which instead of asking for binary judgment of compositionality, which is a concept that believe me, I sometimes have difficulties understanding, even though I work in the area, so they asked crowdsourcing judgments using a notion of literality, so they asked people to say how you think this compound relates literally to this work, and how do you think it relates to this word.",
                    "label": 0
                },
                {
                    "sent": "So more judgment that laypeople could do.",
                    "label": 0
                },
                {
                    "sent": "And we adopted this protocol and we extended this for French and for Portuguese, and we added more compounds for for English to have similar.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Datasets for the three languages.",
                    "label": 0
                },
                {
                    "sent": "So for English we ended up with 270 compounds and 180 for French and Portuguese, and they're all of the same type of compounds.",
                    "label": 0
                },
                {
                    "sent": "Which is compiled nouns for English.",
                    "label": 0
                },
                {
                    "sent": "That means either 2 nouns or an adjective or noun, and for French the similar would be nouns, followed by adjectives like morcilla and for Portuguese morcela.",
                    "label": 0
                },
                {
                    "sent": "As in the characteristic of this data set is that it's balanced for compositionality because we wanted to see if the models would work with Nest for compositional, or for idiomatic compounds.",
                    "label": 1
                },
                {
                    "sent": "So we have 1/3 of them, which is idiomatic.",
                    "label": 1
                },
                {
                    "sent": "1/3 that is partly compositional and 1/3 that is completely composition.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And their judgments, just to give you an idea, the judgments that we ask people to do would be 3.",
                    "label": 0
                },
                {
                    "sent": "Three questions for each of the compounds.",
                    "label": 0
                },
                {
                    "sent": "So we would give the annotators three sentences where the compound appeared in each of the sentences.",
                    "label": 0
                },
                {
                    "sent": "Had the compound with exactly the same sense.",
                    "label": 0
                },
                {
                    "sent": "So either the dramatic or the literal.",
                    "label": 0
                },
                {
                    "sent": "And we asked for example for the compound is a climate change truly literally our change in climate?",
                    "label": 0
                },
                {
                    "sent": "So we rephrased we paraphrase the compound using the combination of the words.",
                    "label": 0
                },
                {
                    "sent": "And then for each of the components we also asked.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an ivory tower literally made of ivory and easily ivory tower, literally a tower, and we asked people to use this Likert scale from zero to five.",
                    "label": 0
                },
                {
                    "sent": "So zero meant they didn't agree with that, and five meant that they fully agreed with that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Collected this judgments for using Mechanical Turkers for English and bit of Mechanical Turkers for French, but for Portuguese we didn't have enough native speakers in available in Amazon at the time, so we asked our annotators our computer science students and linguists.",
                    "label": 0
                },
                {
                    "sent": "To annotate the data.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is that we could then compare the quality of the judgments of the known crowd source and the crowd source workers.",
                    "label": 0
                },
                {
                    "sent": "And the judgments were very, very compatible.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to say.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that we could do, since we had the annotations in Portuguese by the same annotators West to calculate the Alpha in cap agreement.",
                    "label": 0
                },
                {
                    "sent": "Among them so in general, our annotators for Portuguese were had an Alpha agreement of .42 for the compound .50, two for the head, an .36 for the modifier, so reasonable agreement.",
                    "label": 1
                },
                {
                    "sent": "But to give you an idea of the difficulty of the task, we asked the same annotator, linguist linguistics, PhD, translator, and to re annotate the compounds one month later and the only agreement that he could get with himself was point 59 for the compound.",
                    "label": 0
                },
                {
                    "sent": "So you can see how difficult this judgment is.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you why in a second.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is that we can use this so the office call would be .49 and the Spearman correlation would be .77.",
                    "label": 0
                },
                {
                    "sent": "So we can use this as an upper bound for what our models could be expected to reach, because if not even a human can agree with himself one month later, we can't expect models to do better than them.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the here are some of the compounds that they had to judge and on the top you can see the compounds where they had the least agreement.",
                    "label": 0
                },
                {
                    "sent": "The more variation between the annotators.",
                    "label": 0
                },
                {
                    "sent": "And in the bottom part you can see the compounds where they really had a lot of agreement.",
                    "label": 0
                },
                {
                    "sent": "So most of our painters agreed, for example, that a graduate student was really a student was literally a student, and was literally doing graduate studies.",
                    "label": 0
                },
                {
                    "sent": "But they had difficulties agreeing whether a dirty word was a metaphorical.",
                    "label": 0
                },
                {
                    "sent": "I don't know, swear word or if it was more a completely idiomatic expression, so some of them interpreted dirt as more metaphorical sense of dirt, so I'm more police in most cases than a real idiomatic case, so you can see here that they had almost two points different in terms of the.",
                    "label": 0
                },
                {
                    "sent": "The head and 2.5 in terms of the whole compound.",
                    "label": 0
                },
                {
                    "sent": "Or actually 2.5 with one point 7 of standard variation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So if I had to tell you how our models did, we were pleasantly surprised that they did very well in this judgment.",
                    "label": 0
                },
                {
                    "sent": "So when we compared what the Spearman correlation that our models got with their human annotators, they got .82.",
                    "label": 0
                },
                {
                    "sent": "For the original Ready data set.",
                    "label": 0
                },
                {
                    "sent": "Which was more than the authors of the Ready data set got when they did the evaluation?",
                    "label": 0
                },
                {
                    "sent": "And we now we're extended data set we got .73.",
                    "label": 0
                },
                {
                    "sent": "So we think it was a bit more difficult.",
                    "label": 0
                },
                {
                    "sent": "But still they the models got a good performance and for French and Portuguese the results were slightly lower.",
                    "label": 0
                },
                {
                    "sent": "So for French .7 Spearman correlation and for Portuguese .6.",
                    "label": 0
                },
                {
                    "sent": "But remember that our annotator got .77 with himself and he's a very picky linguist.",
                    "label": 0
                },
                {
                    "sent": "So we can see here in each of these charts the the Spearman correlation on the Y axis and the family of models in the X.",
                    "label": 0
                },
                {
                    "sent": "So in here you have the more traditional models.",
                    "label": 0
                },
                {
                    "sent": "Here you have clothes and here you have the two versions versions of Word, two Vec C, Bowen, Skip gram.",
                    "label": 0
                },
                {
                    "sent": "So you can see that here is the two models that perform best was were to VAC and our traditional PMI with some.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Some dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So the two of them got around the same, so I think this one got .72 and this one got .73 three, one correlation.",
                    "label": 0
                },
                {
                    "sent": "And the same models were the best also for for English.",
                    "label": 0
                },
                {
                    "sent": "For Portuguese, so here is French and here is Portuguese.",
                    "label": 0
                },
                {
                    "sent": "For Portuguese the word of X didn't do as well, and the models that did better where the traditional link based distributional semantic models.",
                    "label": 0
                },
                {
                    "sent": "So in this model, so you can actually.",
                    "label": 0
                },
                {
                    "sent": "The good part is that you can read them well, you know what each of the dimensions mean.",
                    "label": 0
                },
                {
                    "sent": "So for example, one dimension could be eat, another dimension could be like, so you can actually read what they're doing and interpret, and we use this just to test our understanding of what these models were doing.",
                    "label": 0
                },
                {
                    "sent": "And these were the ones that fared better for the tooling.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "So when we try to see.",
                    "label": 0
                },
                {
                    "sent": "If this was due to the level of preprocessing that we were doing in the data, what we found was that for English, as you probably notice that if you use one of these models, no amount of processing is needed, so you can use the corpus as it is with as much noise as it has.",
                    "label": 0
                },
                {
                    "sent": "And it's going to make no difference, at least for a dramatic that action.",
                    "label": 0
                },
                {
                    "sent": "If you actually clean the corpus.",
                    "label": 0
                },
                {
                    "sent": "If ulimit eyes.",
                    "label": 0
                },
                {
                    "sent": "If you do some preprocessing so the results actually got worse when we did that forward to back models.",
                    "label": 0
                },
                {
                    "sent": "But for French and for Portuguese, because they are morphologically richer languages.",
                    "label": 1
                },
                {
                    "sent": "If you don't do any pre processing.",
                    "label": 0
                },
                {
                    "sent": "So in here you can see the in the light you can see the most pre processing.",
                    "label": 0
                },
                {
                    "sent": "So the lemmatized form and here in darker color you can see the original corpus.",
                    "label": 0
                },
                {
                    "sent": "So in French, if you don't do any processing, you actually lose considerably and it pays off doing lemmatization, an stopword removal and the same goes for Portuguese.",
                    "label": 0
                },
                {
                    "sent": "So the two models that did best were the ones where we use the lemmatized version.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what about in terms of dimensions?",
                    "label": 1
                },
                {
                    "sent": "So when we looked at the dimensions, the best models for each of the languages and most of the top performing preferred larger dimensions for representing idiomaticity.",
                    "label": 0
                },
                {
                    "sent": "So if we compress the models too much.",
                    "label": 0
                },
                {
                    "sent": "Then then they lost quite alot, specially for Portuguese.",
                    "label": 0
                },
                {
                    "sent": "You can see here.",
                    "label": 0
                },
                {
                    "sent": "So the lighter is the 250 in the middle the 500 and the 750 in the darkest blue.",
                    "label": 0
                },
                {
                    "sent": "And it gets that there is a large difference there.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An what about in terms of window sizes?",
                    "label": 1
                },
                {
                    "sent": "So the the hypothesis is that the smaller windows get more syntactic relations and the larger windows get more semantic relations.",
                    "label": 0
                },
                {
                    "sent": "But we didn't find anything that was.",
                    "label": 0
                },
                {
                    "sent": "Was robust across the languages are across the models, so we had very variable results and we couldn't get any conclusions there.",
                    "label": 0
                },
                {
                    "sent": "But what we can say is that for the three best models in each of the languages, the smaller window which is in the lighter color was the one that gave better results and reducing the window.",
                    "label": 1
                },
                {
                    "sent": "Actually got worse results.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other question that we could ask is if the difference in performance between the languages was due to the corpus size and so here you have the the.",
                    "label": 1
                },
                {
                    "sent": "In this chart you have the corpus size and performance for our compounds.",
                    "label": 0
                },
                {
                    "sent": "And what do you have?",
                    "label": 0
                },
                {
                    "sent": "So for Portuguese, you're having green in English or having bread and French in blue.",
                    "label": 0
                },
                {
                    "sent": "So we divided the corpus in 10 different portions and we used increment and retrain the models in Crim incrementally larger portions.",
                    "label": 0
                },
                {
                    "sent": "So in here you have for example 2 two parts of the corpus, so 210 / 2 and you can see that the models actually stabilize.",
                    "label": 0
                },
                {
                    "sent": "With around 1 billion warrants for each of the three languages afterwards, you can give it more data, but it doesn't make much difference, so the performance is more or less the same in our intuition.",
                    "label": 0
                },
                {
                    "sent": "For that when we looked at the results with that.",
                    "label": 0
                },
                {
                    "sent": "But when we had one billion words we have we had enough data for all of the compounds to be learned.",
                    "label": 0
                },
                {
                    "sent": "And then what came Additionally didn't make much difference an it's the case for both the traditional distributional semantic models, the one that performed best and where to back, which was the one that performed best for English.",
                    "label": 0
                },
                {
                    "sent": "So around 1 billion words was all we needed for the three languages, so the difference in the languages is actually what is causing the difference in performance there.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the extent to which we could use distributional semantic models to capture idiomaticity in compound nouns.",
                    "label": 0
                },
                {
                    "sent": "In three languages, French, English, and Portuguese, and we looked at how they would fare compared to human judgments and for multiword expressions of various levels of idiomatic city.",
                    "label": 1
                },
                {
                    "sent": "And we looked at whether their accuracy was related to the characteristics of the models.",
                    "label": 1
                },
                {
                    "sent": "Or to the characteristics of the language and corporate that we were using.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this study, we had to construct this data set of compositionality judgments, and it's a resource that is freely available and the protocol.",
                    "label": 0
                },
                {
                    "sent": "Is easily replicated for different languages, so if you feel like doing this for your own language, we had an athlete tutorial last year where we ask participants from different languages to give us some some indications and they had a lot of fun.",
                    "label": 0
                },
                {
                    "sent": "So I recommend if you have students and you don't know which activity to give them, give them some compositionality judgments and then let me know how you find.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also collected for each of the compounds that we asked our judges to analyze a paraphrase or a synonym for the compound.",
                    "label": 0
                },
                {
                    "sent": "So with them constructed for Portuguese data, set of substitution of these compounds.",
                    "label": 1
                },
                {
                    "sent": "Sometimes a compound can be substituted by a single word, sometimes by paraphrases, but we have then and this data set can be used, for example for text simplification.",
                    "label": 0
                },
                {
                    "sent": "And it's also freely available.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the moment it's only available for Portuguese, but we intend to release the Portuguese and French versions soon.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, for English, French and Portuguese, we constructed over 600 models, almost 700 models, almost 9000 evaluations using different families of distributional semantic models, and I know that most of you and probably your students are really keen on using neural network models, but the distributional semantic models the more traditional kind, also pays off.",
                    "label": 1
                },
                {
                    "sent": "And it's easier to read and understand what's going on there.",
                    "label": 0
                },
                {
                    "sent": "What I didn't include here was that we had as good performance for the dramatic compounds as we had for the compositional.",
                    "label": 0
                },
                {
                    "sent": "Transparent compounds so the level of compositionality actually didn't affect how these models were performing.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of these analysis were done using the multi word Expressions Toolkit, which is also freely available and was developed by Carlos, Hamish and Silvio Cordell.",
                    "label": 0
                },
                {
                    "sent": "The Co.",
                    "label": 0
                },
                {
                    "sent": "Authors of the paper that I just described to you.",
                    "label": 0
                },
                {
                    "sent": "It does all your multiword expression needs, so from extracting multi words from corporate.",
                    "label": 0
                },
                {
                    "sent": "If you just have a corpus to a notating multi words back into a corpus.",
                    "label": 0
                },
                {
                    "sent": "If you have a Dictionary of multi word expressions and you want to find them in a corpus.",
                    "label": 0
                },
                {
                    "sent": "It calculates Association measures for you.",
                    "label": 0
                },
                {
                    "sent": "And a number of them.",
                    "label": 0
                },
                {
                    "sent": "So dies Chi Square point rates, mutual information.",
                    "label": 0
                },
                {
                    "sent": "So a number of them is implemented there.",
                    "label": 0
                },
                {
                    "sent": "And once you have the word embeddings, it also helps you compare with the data set how your word embeddings are doing.",
                    "label": 0
                },
                {
                    "sent": "So the evaluations that I told you about.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as future work, we now that we know that this representations they work really well at representing Idiomatic City.",
                    "label": 0
                },
                {
                    "sent": "We want to see how much more precise we can make them 'cause for Portuguese.",
                    "label": 0
                },
                {
                    "sent": "If we compare with the human performance there, there is still a large margin of improvement that we can that we can expect our models to have.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is actually to see how we can incorporate some knowledge about the dramatic city.",
                    "label": 0
                },
                {
                    "sent": "Maybe some guide how the model is learning to see if we can get better performance and there were some papers at ACL this year, for example which were exploring different models.",
                    "label": 0
                },
                {
                    "sent": "Of semantic representation for more accurate performance and you have for example, even more sophisticated models which use different types of multi dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "And we also want to use this models for token idiomatic city identification.",
                    "label": 0
                },
                {
                    "sent": "So when you see a compound in the sentence or a potential compounding the sentence, is it used in the literal sense or easy to use Indian idiomatic sense in that sentence?",
                    "label": 0
                },
                {
                    "sent": "So for example, I say that I don't know Nikolaou kick the bucket.",
                    "label": 0
                },
                {
                    "sent": "I probably mean to say that he just bumped on a container and not that he died.",
                    "label": 0
                },
                {
                    "sent": "Sorry about using OK, so we want to use that.",
                    "label": 0
                },
                {
                    "sent": "The knowledge of these models to also help us in this task.",
                    "label": 0
                },
                {
                    "sent": "And this is the crucial one.",
                    "label": 0
                },
                {
                    "sent": "For example, for tax simplification for words, Ness is ambiguation for machine translation, and for conversational agents, if you are working with them.",
                    "label": 0
                },
                {
                    "sent": "So we want to use also these linguistic intuitions about how fixed are given multi word is to detect idiomaticity.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize this, this was a collaboration done with researchers from Brazil from France and it was sponsored by the Brazilian Funding Agency.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for the invitation to talk here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now just to give you a teaser here.",
                    "label": 0
                },
                {
                    "sent": "For those of you who speak German, just read the top line and you will see some interesting cases which you probably never considered.",
                    "label": 0
                },
                {
                    "sent": "For those of you like me, who unfortunately don't speak German, there are the translations there.",
                    "label": 0
                },
                {
                    "sent": "So apparently in German if you have tomatoes in your eyes, it means that you are seeing things that no, no analysis thing, and if you if you only understand the train station, it means that you don't understand anything.",
                    "label": 0
                },
                {
                    "sent": "So probably 4.",
                    "label": 0
                },
                {
                    "sent": "Most of us it's going to be his speaking Greek or he's speaking some language which is not very familiar in your native country.",
                    "label": 0
                },
                {
                    "sent": "So there you have it.",
                    "label": 0
                },
                {
                    "sent": "So any questions?",
                    "label": 0
                },
                {
                    "sent": "Respective German you have the compound.",
                    "label": 0
                },
                {
                    "sent": "Does that make the problem more difficult because you first have to identify them in other languages.",
                    "label": 0
                },
                {
                    "sent": "You have the space in between so.",
                    "label": 0
                },
                {
                    "sent": "That's a great question, so we are when we are working with text and we're working with romance languages or with English.",
                    "label": 0
                },
                {
                    "sent": "We have the the ease of having a white space.",
                    "label": 0
                },
                {
                    "sent": "Is telling us that it's different words, but for a lot of the languages and probably half of the population of the world, we don't have this this luxury.",
                    "label": 0
                },
                {
                    "sent": "So for example, in languages like Chinese, some Arabic language is German.",
                    "label": 0
                },
                {
                    "sent": "One word can have various characters and you don't know where where to divide, but so some of the groups who have been working on multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "What they do is first day segment the words where they think the spaces could be.",
                    "label": 0
                },
                {
                    "sent": "So they say OK, this potentially could be a space so they segment the words first and then they do multi word expressions extraction.",
                    "label": 0
                },
                {
                    "sent": "But what we found that for example for Germany it's actually quite good to have the big compounding word because that's an indication over edited.",
                    "label": 0
                },
                {
                    "sent": "2nd pound, so it's actually a plus rather than a mile.",
                    "label": 0
                },
                {
                    "sent": "A word to be alert then that would be an indication exactly and then the question is to determine how idiomatic or how compositionally you can construct the meaning of the word.",
                    "label": 0
                },
                {
                    "sent": "Do you have any idea how the whole game changes now with these?",
                    "label": 0
                },
                {
                    "sent": "Dynamic embeddings, so once we go from static, remembering that expires models, how would approaches or how those probably reason just ordered approaching problems?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for the questions, so there there are more more modern kind of.",
                    "label": 0
                },
                {
                    "sent": "Incorporations of these models these days?",
                    "label": 0
                },
                {
                    "sent": "And you're probably thinking OK, were tobacco.",
                    "label": 0
                },
                {
                    "sent": "This is so old news, but for us to understand what these models were doing with you, idiomatic City, we wanted to be able to read the models.",
                    "label": 0
                },
                {
                    "sent": "And that's why we use some very traditional models like lens for example, which is based on Point Reyes mutual information.",
                    "label": 0
                },
                {
                    "sent": "And as I said, you can read the dimensions.",
                    "label": 0
                },
                {
                    "sent": "So if you could understand what were the salient information that these models were doing, we hope to understand what the neural network models were doing.",
                    "label": 0
                },
                {
                    "sent": "So one big area of research these days is trying to interpret neural network and how they incorporate information.",
                    "label": 0
                },
                {
                    "sent": "So we were thinking about doing some kind of black box reengineering.",
                    "label": 0
                },
                {
                    "sent": "To understand what they could be capturing.",
                    "label": 0
                },
                {
                    "sent": "But there are models, for example like Elmo, which are contextual, contextually aware.",
                    "label": 0
                },
                {
                    "sent": "So for each of the words or move towards that you have there, they would have a slightly different representation based on the sentence in which it occurred.",
                    "label": 0
                },
                {
                    "sent": "So you would have for example, instead of having a type representation for the MULTIWORD expression.",
                    "label": 0
                },
                {
                    "sent": "So a single one for kick the bucket, you would have various representations for kick the bucket depending on the different sentences in which it occurred.",
                    "label": 0
                },
                {
                    "sent": "So our idea with this models is to see if you really need to have all that expensive machinery.",
                    "label": 0
                },
                {
                    "sent": "Because these are really expensive models to train and you need a lot of data.",
                    "label": 0
                },
                {
                    "sent": "And for a lot of the compounds, even though they are very common in daily language, for you to get enough instances of these models, sometimes it's difficult.",
                    "label": 0
                },
                {
                    "sent": "So we want to see if we can use linguistically informed models that are perhaps more compact and easier to train than the contextually aware model.",
                    "label": 0
                },
                {
                    "sent": "So we are looking at ways of doing that at the moment.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Very much involved in this FIFA.",
                    "label": 0
                },
                {
                    "sent": "What bothers me?",
                    "label": 0
                },
                {
                    "sent": "Should we really rely mostly just on some kind of unique approaches?",
                    "label": 0
                },
                {
                    "sent": "Find summer presentations while not trying to do some dialogue, system agent and or trying to learn from our conversations or human being without buying, understand, compounding.",
                    "label": 0
                },
                {
                    "sent": "Station is not locked.",
                    "label": 0
                },
                {
                    "sent": "Some kind of knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Effective language models with platform that is rare.",
                    "label": 0
                },
                {
                    "sent": "Simone, thank you very much and that's why I really like to be here because I'm hoping that you give me ideas for future works, so that's a great idea, and this is actually probably closer.",
                    "label": 0
                },
                {
                    "sent": "So if you are a native speaker of English, for example, you probably remember how hard it was to learn some of these expressions and how they didn't make any sense and how you had to learn them by heart.",
                    "label": 0
                },
                {
                    "sent": "So you had to have lists of memorized.",
                    "label": 0
                },
                {
                    "sent": "Some of these expressions.",
                    "label": 0
                },
                {
                    "sent": "And it's the same for us for multiple languages.",
                    "label": 0
                },
                {
                    "sent": "And somehow we learn and children learn, so there should be something enough in the in the context.",
                    "label": 0
                },
                {
                    "sent": "An interactions that can help you learn these expressions.",
                    "label": 0
                },
                {
                    "sent": "So having something like a tutored learning would be really nice, so some of the things that are being researched any probably have seen this adversarial networks and you could use them, for example to try to do something like that so.",
                    "label": 0
                },
                {
                    "sent": "Have a network generating something and see if it would be understood by another network and then.",
                    "label": 0
                },
                {
                    "sent": "Try to see if they would reach an agreement about the meaning of the compound, or perhaps using a dialogue system and helping second language learners to reach.",
                    "label": 0
                },
                {
                    "sent": "To help with possible avenues of errors or correct understanding of the compounds.",
                    "label": 1
                },
                {
                    "sent": "Literally anything can keep talking Lawrence exactly.",
                    "label": 0
                },
                {
                    "sent": "So if I can open a brief ( 1 of the fun parts of this work was also that we used.",
                    "label": 0
                },
                {
                    "sent": "Some of these techniques to analyze children's data so the child's corpus in English, and we looked, we identified the verb particle constructions that the adult sentences contained, and that the children sentence contained, and we compared the overlapping vocabulary and the usage and what we found was that you needed more or less 5 instances, 5 occurrences of a given vertical constructions for their children in the adult data for their children to start using the.",
                    "label": 0
                },
                {
                    "sent": "For particle construction, but they were using quite closely what the adults were doing, so they probably were doing something like this language game implicitly and they learn very efficiently from 5 instances they were using, so whatever the vertical constructions in English, they can be separated.",
                    "label": 0
                },
                {
                    "sent": "The verb can be separated by the particle by up to 5 words.",
                    "label": 0
                },
                {
                    "sent": "And most of our data was.",
                    "label": 0
                },
                {
                    "sent": "There was mostly one or two words in between the verb and the particle, but whatever the adults were doing, their children were also doing.",
                    "label": 0
                },
                {
                    "sent": "The difference that we found was that in the top ten work, 10 vertical constructions that were used by the children, they had a lot of fell down, fell down, fell down, and the adults had pick up.",
                    "label": 0
                },
                {
                    "sent": "Pick up pick up.",
                    "label": 0
                },
                {
                    "sent": "So probably parenting children discussions there.",
                    "label": 0
                },
                {
                    "sent": "Is there something like not very familiar with public, but is there something like it?",
                    "label": 0
                },
                {
                    "sent": "Long term multi word expressions versus?",
                    "label": 0
                },
                {
                    "sent": "This incident creation that will just because something happens in the world and then you know people use it for a couple weeks or months and then it dies.",
                    "label": 0
                },
                {
                    "sent": "Is there a different use of those or happening?",
                    "label": 0
                },
                {
                    "sent": "Thank you so much.",
                    "label": 0
                },
                {
                    "sent": "That's another really interesting questions, so some people working on the area have been looking at diachronic corpora, so trying to see how an expression changes sense and how they incorporate this more idiomatic meaning, because at some point some of these expressions were quite understandable.",
                    "label": 0
                },
                {
                    "sent": "So, for example, kick the bucket.",
                    "label": 0
                },
                {
                    "sent": "Apparently the meaning of kick the bucket has to do it, pigs being slaughtered, and some buckets.",
                    "label": 0
                },
                {
                    "sent": "So probably people could understanding.",
                    "label": 0
                },
                {
                    "sent": "I don't know 1600s when they were using this expression for the first time, but now, for as it's completely idiomatic, we can't reconstruct the meaning.",
                    "label": 0
                },
                {
                    "sent": "And one of the things about multi word expressions is that they are very convenient way of expressing a situation in a very short format.",
                    "label": 0
                },
                {
                    "sent": "So in English it's beautiful because you can just compile.",
                    "label": 0
                },
                {
                    "sent": "You can aggregate as many nouns as you want and you have a compound and people probably understand so you have to have ways of automatically going through Corporation.",
                    "label": 0
                },
                {
                    "sent": "Finding this new expression and trying to find the meaning for them.",
                    "label": 0
                },
                {
                    "sent": "See if you can find at least the implicit relation that could.",
                    "label": 0
                },
                {
                    "sent": "Be between the words like a brick wall or if something completely different, but there is a study about it and the other thing is that you can think of OK, so let's just pick any sequence of towards and believe it's a compound.",
                    "label": 0
                },
                {
                    "sent": "If you go to chemical tags for example, you have compounds that are 30 words long.",
                    "label": 0
                },
                {
                    "sent": "So in in domain specific texts.",
                    "label": 0
                },
                {
                    "sent": "You have this huge huge words, so if you think that you have to construct all possible sequences from a corpus of up to 30 words long, then you are going to have a lot of work just trying to get enough statistics for each of the possible compounds.",
                    "label": 0
                },
                {
                    "sent": "I would like to.",
                    "label": 0
                },
                {
                    "sent": "Comment on one paper which is actually also by Carlson Sylvia.",
                    "label": 0
                },
                {
                    "sent": "It's a position statements.",
                    "label": 0
                },
                {
                    "sent": "Expressions.",
                    "label": 0
                },
                {
                    "sent": "I think they Seattle this year.",
                    "label": 0
                },
                {
                    "sent": "Says the time says without lexicons, module expression identification would never fly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean what the paper says is basically that.",
                    "label": 0
                },
                {
                    "sent": "You can put the long efforts.",
                    "label": 0
                },
                {
                    "sent": "Machine.",
                    "label": 0
                },
                {
                    "sent": "In the end, you can actually make the list of wonderful expressions in the language.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Usually this kind of a camera is on.",
                    "label": 0
                },
                {
                    "sent": "Things like.",
                    "label": 0
                },
                {
                    "sent": "Dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Question, I think if.",
                    "label": 0
                },
                {
                    "sent": "Into making this list and.",
                    "label": 0
                },
                {
                    "sent": "Michael Expression identification will actually become the simulation of liberal senators versus something that is in the lexicon.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much and that's a great food for thought to use a multiword expression there.",
                    "label": 0
                },
                {
                    "sent": "So I think what I completely agree with them an I have to say that I'm a computational linguist.",
                    "label": 0
                },
                {
                    "sent": "I'm not frustrated, I'm not a linguist unfortunately, but I'm a computational linguist, so I really believe that without some linguistic intuition with some linguistic knowledge, what you learn is something, but you don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "So most of these models are learning something and sometimes we can't interpret what they learn.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can create a model, but if you don't have any way of evaluating what the model is learning.",
                    "label": 0
                },
                {
                    "sent": "Then you may have garbage in the end, so I haven't rented their paper, but thanks for summarizing and what I would, I would say is that.",
                    "label": 0
                },
                {
                    "sent": "For many domains and many languages, we still need lists like that, so even for a computer science, depending on the topic that you are working on, you are going to have specific ways of describing this terms and what I think that this methods can do is to facilitate the work of a lexicographer or a terminal terminologies.",
                    "label": 0
                },
                {
                    "sent": "Because it's actually quite quite hard work and no matter how many words you put into a list of multi word expressions, there are always going to be more, and they're always going to be created.",
                    "label": 0
                },
                {
                    "sent": "So we need to somehow facilitates that, but without having the list of multi word expressions you can't evaluate how your models are doing so it's a chicken and egg kind of game.",
                    "label": 0
                },
                {
                    "sent": "So I believe my view is that we can help the lexicographers and they can help us evaluate these methods, but I would never do without lexicographers an without linguistic intuition there.",
                    "label": 0
                },
                {
                    "sent": "Specially when you look at syntax and what you can generate.",
                    "label": 0
                },
                {
                    "sent": "It's not clear that you that you can generate like standard language, so known multiword expressions and you really need syntactic knowledge and you need specific knowledge about the variance to be able to know what you can generate from a given expression.",
                    "label": 1
                },
                {
                    "sent": "So I think I agree with them and I I'm curious now to read what they're saying, but I think it's a provocation to just automatisering everything and believing whatever the model tells you is correct.",
                    "label": 0
                },
                {
                    "sent": "So be sceptical.",
                    "label": 0
                },
                {
                    "sent": "Active.",
                    "label": 0
                },
                {
                    "sent": "Lexical type of language knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Just what I would try to imagine fixed and looking forward to the Registrar's office to make something more standardized.",
                    "label": 0
                },
                {
                    "sent": "Moving forward.",
                    "label": 0
                },
                {
                    "sent": "Spoilers.",
                    "label": 0
                },
                {
                    "sent": "Compounds public events semantic software from PowerShell.",
                    "label": 0
                },
                {
                    "sent": "Just some next description or something post.",
                    "label": 0
                },
                {
                    "sent": "The description is right.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a machine.",
                    "label": 0
                },
                {
                    "sent": "Apology syntax and semantics.",
                    "label": 0
                },
                {
                    "sent": "And so the they have been involved in this cost project, which I don't know if everyone is familiar here.",
                    "label": 0
                },
                {
                    "sent": "So it was a really, really important initiative by the RCM cost action.",
                    "label": 0
                },
                {
                    "sent": "So I think Nikolaou here was involved in it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "And so there were lexicons created for multi word expressions or variables languages.",
                    "label": 0
                },
                {
                    "sent": "So if you saw his poster yesterday so now now you have for three domains specific vocabulary for multi word expressions for is laughing, but it's really hard work and it was.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot of languages were described by that.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested in multi word expressions for.",
                    "label": 0
                },
                {
                    "sent": "Languages of Europe.",
                    "label": 0
                },
                {
                    "sent": "The RCM cost action is a good place to look at.",
                    "label": 0
                }
            ]
        }
    }
}