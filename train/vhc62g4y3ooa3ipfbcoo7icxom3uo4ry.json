{
    "id": "vhc62g4y3ooa3ipfbcoo7icxom3uo4ry",
    "title": "An integrated generative and discriminative Bayesian model for binary classification",
    "info": {
        "author": [
            "Keith James Harris, Department of Computing Science, University of Glasgow"
        ],
        "published": "Nov. 8, 2010",
        "recorded": "October 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Bioinformatics->Computational Systems Biology"
        ]
    },
    "url": "http://videolectures.net/mlsb2010_harris_aig/",
    "segmentation": [
        [
            "So I'm going to start this stuff by motivating why such a model is a good idea.",
            "So as we saw earlier in Session 6."
        ],
        [
            "High dimensional datasets typically consists of 10s of thousands of covariates and usually a much smaller number of Sam."
        ],
        [
            "Apples and this is a big problem as analyzing such data is very challenging as the covariates usually are very highly correlated, which results in unstable parameter estimates and inaccurate prediction.",
            "If you use the standard methods."
        ],
        [
            "So, to alleviate this problem, our idea is that we have developed a statistical model that uses a small number of what we call meta covariates inferred from the data through a Gaussian mixture model rather than all the original covariates to classify this."
        ],
        [
            "Apples.",
            "Now the key novelty of our approach is that our meta covariates are formed considering predictor outcome correlations as well as predictor into predictor correlations.",
            "And there's a diagram that will hopefully illustrate that in a couple of slides."
        ],
        [
            "Time.",
            "So this possible benefit.",
            "So this idea was partly inspired by recent recent empirical research in machine learning by, for example, Chris Bishop there, showing the optimum predictive performance often corresponds to an intermediate tradeoff between the purely generative and purely discriminative approaches to class."
        ],
        [
            "Vacation.",
            "And what one suggestion?",
            "So one possible alternative method is to use sparsity.",
            "So the main advantage over using sparse classification model is that we can extract a much larger subset of covariates with essential predictive power and partition this subset into groups within which the covariates are real."
        ],
        [
            "Acted in some way.",
            "Moreover, are better covariates have a natural ordering and interpretation as increasingly predictive response relevant clusters?",
            "So hopefully these clusters will capture some biological information."
        ],
        [
            "As well.",
            "So this is the model overview of pictorial representation of what's going on in the example of gene expression data.",
            "So we have a large number of capital D features an N samples which are either have set a or set B, which is this case, are either control rats that were given a normal diet or rats that were given salt assault Dyett, so Co expression clusters are identified.",
            "Are represented by their meta covariate, which can be thought of as some sort of mean or average, and each cluster mean is assigned a weight according to its ability to distinguish between set A and set B data."
        ],
        [
            "So here's the model notation, because we're coupling together 2 models, there's a lot of parameters.",
            "Unfortunately in the model, so we have in in the usual setting we have our design matrix Capital X, which has N samples and D features, and the associated binary response vector which is labeled the vector T. So we have our Gaussian mixture model parameters, so we have a matrix of clustering.",
            "The parameters which we will denote by theater, which is K by N and associated with that will have a matrix of clustering variance parameters capital Sigma, which is also K by N and a vector of mixing coefficients pie which is K by one and we do inference using any EM algorithm technique.",
            "So we have a matrix of clustering latent variables.",
            "Capital said there's one for each.",
            "So there's a vector associated with each feature, which is D by K, which is 0 if it isn't a member of the case cluster and one otherwise, and we have the classification variables as well, so we have a intercept denoted by W. 0 vector of regression coefficients, which we'll denote by W, which is K by one, and we have the vector of classification auxiliary variables.",
            "Why which converts the binary model into a continuum?"
        ],
        [
            "Swan so this graph represents the conditional dependency structure of our model, and this also illustrates the novelty of our method.",
            "So by coupling the clustering model together with the classification model, we see that the cluster matrix of cluster means seater depends both on the design matrix Capital X and the response vector T through the classification.",
            "Accelerate variables, why?"
        ],
        [
            "So it's easy to read off the conditional dependency graph the joint distribution.",
            "So there's a discriminative component, which is the distribution of T&Y conditional on Theta W 0 vector W. So we've replaced the design matrix there by the metric over representation theater, we have the generative component, which is a, which is the distribution.",
            "Of X and said conditional on the clustering parameters and we have prior distributions for all the prime.",
            "All the model parameters."
        ],
        [
            "So to talk about the model components in a bit more detail so the generative component is just a standard Gaussian mixture model where we've assumed diagonal covariance matrix."
        ],
        [
            "Says so that the covariance parameters are all, so there's no conference parameters, so the discriminative component is just a standard product regression model, except we replace where the design matrix where there would be the vector XN with the reply with the metric overrepresentation theater N."
        ],
        [
            "And the prior distributions we assume conjugate prior distributions.",
            "So, and these prior distributions are chosen such that the hyperparameters Hu Zi, L zero and L are chosen so that they're fairly uninformative and vague."
        ],
        [
            "So as I mentioned earlier, we use an M algorithm approach to calculate the expectations of the latent variables and the maximum a posteriori estimates of the model parameters.",
            "So here we have the expectation of the binary binary clustering latent variables.",
            "This is exactly the same as if you did the.",
            "Eastep for the Gaussian mixture."
        ],
        [
            "Bottle, so that remains unchanged.",
            "The expectation of the classification auxiliary variables also remains unchanged.",
            "It's just the expectation of the truncated Gaussian.",
            "The only difference is that where we have vectors XN, we have the thread because we're representing with the data by the metric over."
        ],
        [
            "It's so.",
            "Probably the most interesting update is the update for the cluster means.",
            "So here on the top we have.",
            "So the term in brackets, the first term in brackets can be sort of as a model mismatch term when you exclude the case cluster and you multiply that by the regression coefficient WK and then you add on the standard contribution from the Gaussian mixture model.",
            "Uh, the denominator can be thought of as a position term that scales the numerator.",
            "So what you see is that when WCS 0 you have the same update as the Gaussian mixture model.",
            "But what it isn't there?",
            "Oh, the the model mismatch term will become more more important."
        ],
        [
            "So if the clustering variance parameters you have the same update as in the EM for the Gaussian mixture model."
        ],
        [
            "Save the mixing coefficients.",
            "It's the same."
        ],
        [
            "That's normal, so you have for the Intercept.",
            "It's the same as the appropriate even for the probate regression model, except you replace the design matrix X by the matrix ITA."
        ],
        [
            "Again, and the same with the vector of regression coefficients W. So instead of having, you usually would have something like X transpose X plus some prior inverse.",
            "We have fee to see to transpose plus prior.",
            "Contribution from the prior inverse and so on."
        ],
        [
            "Hub and what is the thing due to coupling the model we have to set the first component of the regression.",
            "That to 1:00 so that the model is identifiable."
        ],
        [
            "So I will now show an application of the in algorithm into applied to.",
            "So data from the British Heart Foundation at Glasgow Cardiovascular Unit at Glasgow University.",
            "About salt sensitive hypertension.",
            "So hypertension is a common precursor to cardiovascular disease and is often exacerbated by increased dietary intake of salt."
        ],
        [
            "So, uh, so at Glasgow they've read a strain of rats called the stroke prone, spontaneously hypertensive rat.",
            "And this is an excellent model of human essential hypertension that exhibits sensitivity."
        ],
        [
            "Salt.",
            "So by analyzing microarray data from this strain of rat assault insensitive strain cut which will denote by WKY and an intermediate congenic strain to a genes and pathways that influence salt sensitive hypertension can be elucidated."
        ],
        [
            "So we applied our method to the data after some preprocessing and we found a very highly influential cluster of 13 genes in the model with 20 clusters suggested by the Bayesian information criterion.",
            "So the left off graph shows the expression of all 13 genes.",
            "One thing to note is that on the left hand side is.",
            "No salt on the right hand side is salt and you can see that it's very heterogeneous.",
            "The pattern of expression between no salt and salt due to three different groups.",
            "Expression so just of the roar expressions is shown in the left middle graph and the alternative meta covariate representation is shown in the left bottom graph and what we see is that the meta covariate expression by taking into account the fact that we know that one of the outlying observation is in the salt, was given salt.",
            "It's sort of reduces the effect of the outlier.",
            "That pulls out the relationship."
        ],
        [
            "So, so we did a rank products analysis of this cluster.",
            "So what this graph represents is the genes the genes in cluster 13 overlaid with which strain there from SO2A is left top WKY is right top and the bad unhealthy strain is at the bottom.",
            "Red indicates overexpression and salt.",
            "Green indicates under expression and Gray is that.",
            "Not significantly differentially expressed, and we see that in the 2A and WKY strains, most of the genes of the cluster are significantly differentially expressed due to salt.",
            "But in the unhealthy SP strain, they're not differentially expressed."
        ],
        [
            "So of the story we got from the biologists was that the age of reason jeans were implicated, so Canonical pathway analysis of this highly influential cluster showed that the cluster was enriched for circadian rhythm genes, indicating that these genes are important in differentiating between rats that were given salt."
        ],
        [
            "And those which worked, and this is relevant, given that these nocturnal animals exhibited increased hypotension joining the night, and this difference was exacerbated by giving them salt."
        ],
        [
            "So go back to the RP analysis.",
            "We show we saw that.",
            "There were differences on salt loaded in the two M WKY strains, but not in the SP strain."
        ],
        [
            "And so the the biologists we worked with therefore hypothesize that the genes in the most influential meta Kovac cluster of protective against hypotension in response to an increase in dietary sodium.",
            "And you can do a similar analysis for the other clusters that were."
        ],
        [
            "Down to the bottle.",
            "So just to know we've extended the method to Gibbs sampling, it is straightforward to do because with the there's a. Yeah, the Gibbs sampler is sort of parallel to the algorithm approach is sort of the Bayesian equivalent, so all I know is that that each of the updates have very simple forms.",
            "So for the mixing coefficients you just have to sample from Adrish, lay distr."
        ],
        [
            "He should for the mean metric over parameters.",
            "See to KN, you just have to sample from the univariate normal with the mean being the update."
        ],
        [
            "The full conditional distribution for the clustering variance parameters is an inverse."
        ],
        [
            "Scabbard distribution.",
            "For The Intercept, it's it's another univariate normal with the M update being the."
        ],
        [
            "The being said, the update for the vector of regression coefficients that's multi."
        ],
        [
            "Noble the.",
            "Clustering latent variables can be sampled from a multinomial distribution where the number of trials is 1 and the probabilities of success PK are the responsibility."
        ],
        [
            "And the full conditional distribution for the classification auxiliary variable is just a truncated Gaussian distribution that you have to sample."
        ],
        [
            "So so.",
            "We could do predictive inference.",
            "Sing a traditional Monte Carlo approach.",
            "Though this entails so, we haven't got values of Theta for the new data point X star, so we need to sample them from their full conditional distribution."
        ],
        [
            "So this these are just the full conditional distributions that you'd get from the standard Gaussian mixture model, because for this case we don't know what the the observation T star is.",
            "So whether it's 0."
        ],
        [
            "Obama so we applied the Gibbs sampling.",
            "Method to head involves breast cancer data, which is a publicly available data set from patients carrying mutations in the predisposing genes.",
            "BRCA one or BRCA two and from patients.",
            "Not expected to carry either of these mutations.",
            "So the dimensionality was that there were 3000 just over 3000 genes and 22 samples which were split.",
            "Roughly evenly between the three groups, so we did a Wilcoxon test to provide ranking of the features based on their P value and set a threshold of 10%.",
            "The number of features was reduced to just over 600, but this was done to make the facilitate the classification classifier not for reasons of computation."
        ],
        [
            "So we used our method classified BRCA one versus the others and we compared our method to a Bayesian sparse probe it regression model.",
            "So a lot.",
            "So type model we initialized are Gibbs sampler used in REM algorithm and we ran both Gibbs samplers, 400,000 iterations and discarded the first half of the changes burden and we compared the methods using leave one out cross validation."
        ],
        [
            "So here is the plots of the posterior predictive probabilities.",
            "The white bars did out correctly classified samples and the black bars did out misclassified samples.",
            "What we see is that the covariate model performs slightly better than the sparse probit regression model because when the when it classifieds correctly, it's slightly more confident than the sparse probate regression model.",
            "But for the misclassified sample, it was slightly less confident in its prediction, so it's closer to 1/2."
        ],
        [
            "So conclusions, so our experimental results indicate that our Gibbs sampling approach of inferring Mexico virus in classification has competitive performance with some with a certain Bayesian sparse private regression model."
        ],
        [
            "Moreover, our approach could naturally be extended to multiclass classification."
        ],
        [
            "Future research will focus on applying our methodology to functional magnetic resonance imaging data and perhaps manipulating the model so it can be applied to species abundance data.",
            "In of bacteria, and we could also consider developing a base in sampler that could infer directly from the data the optimal number of clusters in R model via an infinite mixture model or reverse jump reversible jump type procedure."
        ],
        [
            "And here are some references, and that's the end of the talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start this stuff by motivating why such a model is a good idea.",
                    "label": 0
                },
                {
                    "sent": "So as we saw earlier in Session 6.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "High dimensional datasets typically consists of 10s of thousands of covariates and usually a much smaller number of Sam.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apples and this is a big problem as analyzing such data is very challenging as the covariates usually are very highly correlated, which results in unstable parameter estimates and inaccurate prediction.",
                    "label": 0
                },
                {
                    "sent": "If you use the standard methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, to alleviate this problem, our idea is that we have developed a statistical model that uses a small number of what we call meta covariates inferred from the data through a Gaussian mixture model rather than all the original covariates to classify this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples.",
                    "label": 0
                },
                {
                    "sent": "Now the key novelty of our approach is that our meta covariates are formed considering predictor outcome correlations as well as predictor into predictor correlations.",
                    "label": 1
                },
                {
                    "sent": "And there's a diagram that will hopefully illustrate that in a couple of slides.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "So this possible benefit.",
                    "label": 0
                },
                {
                    "sent": "So this idea was partly inspired by recent recent empirical research in machine learning by, for example, Chris Bishop there, showing the optimum predictive performance often corresponds to an intermediate tradeoff between the purely generative and purely discriminative approaches to class.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacation.",
                    "label": 0
                },
                {
                    "sent": "And what one suggestion?",
                    "label": 0
                },
                {
                    "sent": "So one possible alternative method is to use sparsity.",
                    "label": 0
                },
                {
                    "sent": "So the main advantage over using sparse classification model is that we can extract a much larger subset of covariates with essential predictive power and partition this subset into groups within which the covariates are real.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acted in some way.",
                    "label": 0
                },
                {
                    "sent": "Moreover, are better covariates have a natural ordering and interpretation as increasingly predictive response relevant clusters?",
                    "label": 1
                },
                {
                    "sent": "So hopefully these clusters will capture some biological information.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So this is the model overview of pictorial representation of what's going on in the example of gene expression data.",
                    "label": 0
                },
                {
                    "sent": "So we have a large number of capital D features an N samples which are either have set a or set B, which is this case, are either control rats that were given a normal diet or rats that were given salt assault Dyett, so Co expression clusters are identified.",
                    "label": 0
                },
                {
                    "sent": "Are represented by their meta covariate, which can be thought of as some sort of mean or average, and each cluster mean is assigned a weight according to its ability to distinguish between set A and set B data.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the model notation, because we're coupling together 2 models, there's a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately in the model, so we have in in the usual setting we have our design matrix Capital X, which has N samples and D features, and the associated binary response vector which is labeled the vector T. So we have our Gaussian mixture model parameters, so we have a matrix of clustering.",
                    "label": 0
                },
                {
                    "sent": "The parameters which we will denote by theater, which is K by N and associated with that will have a matrix of clustering variance parameters capital Sigma, which is also K by N and a vector of mixing coefficients pie which is K by one and we do inference using any EM algorithm technique.",
                    "label": 1
                },
                {
                    "sent": "So we have a matrix of clustering latent variables.",
                    "label": 1
                },
                {
                    "sent": "Capital said there's one for each.",
                    "label": 0
                },
                {
                    "sent": "So there's a vector associated with each feature, which is D by K, which is 0 if it isn't a member of the case cluster and one otherwise, and we have the classification variables as well, so we have a intercept denoted by W. 0 vector of regression coefficients, which we'll denote by W, which is K by one, and we have the vector of classification auxiliary variables.",
                    "label": 0
                },
                {
                    "sent": "Why which converts the binary model into a continuum?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Swan so this graph represents the conditional dependency structure of our model, and this also illustrates the novelty of our method.",
                    "label": 1
                },
                {
                    "sent": "So by coupling the clustering model together with the classification model, we see that the cluster matrix of cluster means seater depends both on the design matrix Capital X and the response vector T through the classification.",
                    "label": 0
                },
                {
                    "sent": "Accelerate variables, why?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's easy to read off the conditional dependency graph the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "So there's a discriminative component, which is the distribution of T&Y conditional on Theta W 0 vector W. So we've replaced the design matrix there by the metric over representation theater, we have the generative component, which is a, which is the distribution.",
                    "label": 0
                },
                {
                    "sent": "Of X and said conditional on the clustering parameters and we have prior distributions for all the prime.",
                    "label": 0
                },
                {
                    "sent": "All the model parameters.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to talk about the model components in a bit more detail so the generative component is just a standard Gaussian mixture model where we've assumed diagonal covariance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says so that the covariance parameters are all, so there's no conference parameters, so the discriminative component is just a standard product regression model, except we replace where the design matrix where there would be the vector XN with the reply with the metric overrepresentation theater N.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the prior distributions we assume conjugate prior distributions.",
                    "label": 0
                },
                {
                    "sent": "So, and these prior distributions are chosen such that the hyperparameters Hu Zi, L zero and L are chosen so that they're fairly uninformative and vague.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I mentioned earlier, we use an M algorithm approach to calculate the expectations of the latent variables and the maximum a posteriori estimates of the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So here we have the expectation of the binary binary clustering latent variables.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the same as if you did the.",
                    "label": 0
                },
                {
                    "sent": "Eastep for the Gaussian mixture.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottle, so that remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the classification auxiliary variables also remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "It's just the expectation of the truncated Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that where we have vectors XN, we have the thread because we're representing with the data by the metric over.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's so.",
                    "label": 0
                },
                {
                    "sent": "Probably the most interesting update is the update for the cluster means.",
                    "label": 0
                },
                {
                    "sent": "So here on the top we have.",
                    "label": 0
                },
                {
                    "sent": "So the term in brackets, the first term in brackets can be sort of as a model mismatch term when you exclude the case cluster and you multiply that by the regression coefficient WK and then you add on the standard contribution from the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "Uh, the denominator can be thought of as a position term that scales the numerator.",
                    "label": 0
                },
                {
                    "sent": "So what you see is that when WCS 0 you have the same update as the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "But what it isn't there?",
                    "label": 0
                },
                {
                    "sent": "Oh, the the model mismatch term will become more more important.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if the clustering variance parameters you have the same update as in the EM for the Gaussian mixture model.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Save the mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's normal, so you have for the Intercept.",
                    "label": 0
                },
                {
                    "sent": "It's the same as the appropriate even for the probate regression model, except you replace the design matrix X by the matrix ITA.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, and the same with the vector of regression coefficients W. So instead of having, you usually would have something like X transpose X plus some prior inverse.",
                    "label": 0
                },
                {
                    "sent": "We have fee to see to transpose plus prior.",
                    "label": 0
                },
                {
                    "sent": "Contribution from the prior inverse and so on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hub and what is the thing due to coupling the model we have to set the first component of the regression.",
                    "label": 0
                },
                {
                    "sent": "That to 1:00 so that the model is identifiable.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will now show an application of the in algorithm into applied to.",
                    "label": 0
                },
                {
                    "sent": "So data from the British Heart Foundation at Glasgow Cardiovascular Unit at Glasgow University.",
                    "label": 0
                },
                {
                    "sent": "About salt sensitive hypertension.",
                    "label": 0
                },
                {
                    "sent": "So hypertension is a common precursor to cardiovascular disease and is often exacerbated by increased dietary intake of salt.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, uh, so at Glasgow they've read a strain of rats called the stroke prone, spontaneously hypertensive rat.",
                    "label": 0
                },
                {
                    "sent": "And this is an excellent model of human essential hypertension that exhibits sensitivity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Salt.",
                    "label": 0
                },
                {
                    "sent": "So by analyzing microarray data from this strain of rat assault insensitive strain cut which will denote by WKY and an intermediate congenic strain to a genes and pathways that influence salt sensitive hypertension can be elucidated.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we applied our method to the data after some preprocessing and we found a very highly influential cluster of 13 genes in the model with 20 clusters suggested by the Bayesian information criterion.",
                    "label": 1
                },
                {
                    "sent": "So the left off graph shows the expression of all 13 genes.",
                    "label": 0
                },
                {
                    "sent": "One thing to note is that on the left hand side is.",
                    "label": 0
                },
                {
                    "sent": "No salt on the right hand side is salt and you can see that it's very heterogeneous.",
                    "label": 0
                },
                {
                    "sent": "The pattern of expression between no salt and salt due to three different groups.",
                    "label": 0
                },
                {
                    "sent": "Expression so just of the roar expressions is shown in the left middle graph and the alternative meta covariate representation is shown in the left bottom graph and what we see is that the meta covariate expression by taking into account the fact that we know that one of the outlying observation is in the salt, was given salt.",
                    "label": 0
                },
                {
                    "sent": "It's sort of reduces the effect of the outlier.",
                    "label": 0
                },
                {
                    "sent": "That pulls out the relationship.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we did a rank products analysis of this cluster.",
                    "label": 1
                },
                {
                    "sent": "So what this graph represents is the genes the genes in cluster 13 overlaid with which strain there from SO2A is left top WKY is right top and the bad unhealthy strain is at the bottom.",
                    "label": 1
                },
                {
                    "sent": "Red indicates overexpression and salt.",
                    "label": 0
                },
                {
                    "sent": "Green indicates under expression and Gray is that.",
                    "label": 0
                },
                {
                    "sent": "Not significantly differentially expressed, and we see that in the 2A and WKY strains, most of the genes of the cluster are significantly differentially expressed due to salt.",
                    "label": 0
                },
                {
                    "sent": "But in the unhealthy SP strain, they're not differentially expressed.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of the story we got from the biologists was that the age of reason jeans were implicated, so Canonical pathway analysis of this highly influential cluster showed that the cluster was enriched for circadian rhythm genes, indicating that these genes are important in differentiating between rats that were given salt.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And those which worked, and this is relevant, given that these nocturnal animals exhibited increased hypotension joining the night, and this difference was exacerbated by giving them salt.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So go back to the RP analysis.",
                    "label": 0
                },
                {
                    "sent": "We show we saw that.",
                    "label": 0
                },
                {
                    "sent": "There were differences on salt loaded in the two M WKY strains, but not in the SP strain.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the the biologists we worked with therefore hypothesize that the genes in the most influential meta Kovac cluster of protective against hypotension in response to an increase in dietary sodium.",
                    "label": 0
                },
                {
                    "sent": "And you can do a similar analysis for the other clusters that were.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down to the bottle.",
                    "label": 0
                },
                {
                    "sent": "So just to know we've extended the method to Gibbs sampling, it is straightforward to do because with the there's a. Yeah, the Gibbs sampler is sort of parallel to the algorithm approach is sort of the Bayesian equivalent, so all I know is that that each of the updates have very simple forms.",
                    "label": 0
                },
                {
                    "sent": "So for the mixing coefficients you just have to sample from Adrish, lay distr.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He should for the mean metric over parameters.",
                    "label": 0
                },
                {
                    "sent": "See to KN, you just have to sample from the univariate normal with the mean being the update.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The full conditional distribution for the clustering variance parameters is an inverse.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scabbard distribution.",
                    "label": 0
                },
                {
                    "sent": "For The Intercept, it's it's another univariate normal with the M update being the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The being said, the update for the vector of regression coefficients that's multi.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Noble the.",
                    "label": 0
                },
                {
                    "sent": "Clustering latent variables can be sampled from a multinomial distribution where the number of trials is 1 and the probabilities of success PK are the responsibility.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the full conditional distribution for the classification auxiliary variable is just a truncated Gaussian distribution that you have to sample.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "We could do predictive inference.",
                    "label": 0
                },
                {
                    "sent": "Sing a traditional Monte Carlo approach.",
                    "label": 0
                },
                {
                    "sent": "Though this entails so, we haven't got values of Theta for the new data point X star, so we need to sample them from their full conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this these are just the full conditional distributions that you'd get from the standard Gaussian mixture model, because for this case we don't know what the the observation T star is.",
                    "label": 0
                },
                {
                    "sent": "So whether it's 0.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Obama so we applied the Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "Method to head involves breast cancer data, which is a publicly available data set from patients carrying mutations in the predisposing genes.",
                    "label": 1
                },
                {
                    "sent": "BRCA one or BRCA two and from patients.",
                    "label": 1
                },
                {
                    "sent": "Not expected to carry either of these mutations.",
                    "label": 1
                },
                {
                    "sent": "So the dimensionality was that there were 3000 just over 3000 genes and 22 samples which were split.",
                    "label": 1
                },
                {
                    "sent": "Roughly evenly between the three groups, so we did a Wilcoxon test to provide ranking of the features based on their P value and set a threshold of 10%.",
                    "label": 0
                },
                {
                    "sent": "The number of features was reduced to just over 600, but this was done to make the facilitate the classification classifier not for reasons of computation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we used our method classified BRCA one versus the others and we compared our method to a Bayesian sparse probe it regression model.",
                    "label": 1
                },
                {
                    "sent": "So a lot.",
                    "label": 0
                },
                {
                    "sent": "So type model we initialized are Gibbs sampler used in REM algorithm and we ran both Gibbs samplers, 400,000 iterations and discarded the first half of the changes burden and we compared the methods using leave one out cross validation.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the plots of the posterior predictive probabilities.",
                    "label": 1
                },
                {
                    "sent": "The white bars did out correctly classified samples and the black bars did out misclassified samples.",
                    "label": 1
                },
                {
                    "sent": "What we see is that the covariate model performs slightly better than the sparse probit regression model because when the when it classifieds correctly, it's slightly more confident than the sparse probate regression model.",
                    "label": 0
                },
                {
                    "sent": "But for the misclassified sample, it was slightly less confident in its prediction, so it's closer to 1/2.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So conclusions, so our experimental results indicate that our Gibbs sampling approach of inferring Mexico virus in classification has competitive performance with some with a certain Bayesian sparse private regression model.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover, our approach could naturally be extended to multiclass classification.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future research will focus on applying our methodology to functional magnetic resonance imaging data and perhaps manipulating the model so it can be applied to species abundance data.",
                    "label": 0
                },
                {
                    "sent": "In of bacteria, and we could also consider developing a base in sampler that could infer directly from the data the optimal number of clusters in R model via an infinite mixture model or reverse jump reversible jump type procedure.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some references, and that's the end of the talk.",
                    "label": 0
                }
            ]
        }
    }
}