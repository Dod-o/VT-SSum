{
    "id": "abcibn4nteakg2uyhf67praw6f3cdi4j",
    "title": "Machine Learning Reductions",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_langford_mlr/",
    "segmentation": [
        [
            "Hi.",
            "OK, so one thing that I think is essential to understand about machine learning is sort of the state that the field is in.",
            "Um?",
            "I think that the state that the field is in is preliminary.",
            "So Sam got up and he told you about some useful way to go about trying to solve the problem an I'm going to tell you about a way that I found useful for solving learning problems, and it's a different way and not entirely compatible.",
            "And it's not clear at all what is what is going to end up being the right way.",
            "So part of the reason why I'm telling you this is that I'd like to encourage you to ask questions.",
            "It's entirely appropriate.",
            "Ask questions.",
            "It's entirely appropriate to ask hard questions.",
            "OK, so this is about learning reductions.",
            "This is some stuff that I've worked on with a number of other people."
        ],
        [
            "How's it go?",
            "OK so.",
            "Let's say that you're trying to solve some learning problems.",
            "Maybe maybe you work for some charity and one of the charities do is they send out mails to people and they say please give us money right?",
            "So the envelope and postage costs a little bit of money to charity and so there's kind of an optimization problem to solve here.",
            "Who are you going to Mail and ask money from if you're going to ask if you send Mail to everyone in the world, then maybe maybe your budget is broken.",
            "But if you figure out who to send money, who's likely to donate, and you send Mail to them, then maybe you do maybe come out positive.",
            "So we're going to do machine learning for some sort of charity.",
            "So maybe we could try to apply binary classification.",
            "We just try to predict is this person going to donate money or not right?",
            "And maybe we get a pretty good predictor and then we we Mail out the envelopes and we discover that we just barely break even in the real world, right?",
            "So it's kind of a bummer.",
            "Um?",
            "So there's a problem here."
        ],
        [
            "Here's another situation which comes up.",
            "Maybe we were working for a doctor.",
            "And the doctor wants some mechanism for predicting whether not somebody has cancer.",
            "So maybe you said to use a support vector machine.",
            "And.",
            "And then you apply your support vector machine.",
            "You get some sort of predictive without this cancer.",
            "And then you're going to the doctor.",
            "The doctor is not.",
            "I don't want to tell me whether not this cancer I want to tell me the probability that there's cancer, right?",
            "So then maybe you take your support vector machine so this is a little bit out of sequence in some sense, because people quotes for machines more thoroughly.",
            "But you can peer into inside of what support vector machine is doing.",
            "And you can extract some number called a margin.",
            "And then you can kind of.",
            "The margin is kind of like a confidence that you have.",
            "It's in your prediction and then you can return that and you call that a probability and then it might turn out that these probabilities just aren't well calibrated there, always you always getting probabilities that are near .5 that your prediction is correct or something like that.",
            "And then you lose because you know you're not managing to do with the Doctor wants.",
            "I guess in case you ever end up with cancer, you really want to get that right."
        ],
        [
            "So.",
            "The question is what's going wrong here?",
            "And this is a very common failure mode that a lot of learning algorithms fall into.",
            "This sort of certain set problems that we know how to attack.",
            "We have tools to attack.",
            "We go out and we try to apply these set tools too.",
            "The world's problems.",
            "It turns out there's a mismatch because the problem that the world is imposing is not the problem that your tool is actually made to solve.",
            "So there's sort of two ways we can try to go on beyond this.",
            "One of them is we try to design a new algorithm for every problem in the real world.",
            "Um?",
            "This is sort of great for learning research and the other approach is to try to figure out some way to reuse your old algorithms to solve these new problems.",
            "So I'm going to be talking about this approach, right?",
            "How do we take some sort of standardized learning machine and use it over and over again to solve a wide array of different kinds of problems?",
            "This is a question that somebody should ask.",
            "Alright, so somebody needs to test the system.",
            "If you push the button, does the camera actually focus on you?",
            "Sam test the system.",
            "John 10 #2 do everything that number one can.",
            "I don't know.",
            "We didn't get a camera though.",
            "Interesting, OK."
        ],
        [
            "Alright, so.",
            "What I'm telling you about is is another way to think about how to solve problems, and it has particular strengths and weaknesses of its own.",
            "So I think probably.",
            "The biggest weakness is related to this question, so if you're in a situation we have a very small amount of data.",
            "That means you're going to.",
            "You're going to have to have sort of some kind of strong prior information in order to succeed, and then the approach that Sam talked about earlier is probably pretty reasonable.",
            "Situation where you have a lot of data.",
            "Then often the approach I'm going to talk about will end up being useful to you.",
            "OK, so.",
            "What are these learning reductions?",
            "So we're going to think about how to reuse an algorithm to solve a bunch of different problems.",
            "So there's several things which are several properties of this approach.",
            "One that it's reductionist.",
            "Well, I'll just go through each of these individually."
        ],
        [
            "Oh OK, so there are a lot of good things which are reductionist.",
            "One of 'em is we figured out how to kind of reduce the process of computation to transistors.",
            "This turned out to be extremely useful to all of us.",
            "We figured out how to reduce the rendering of scenes to rendering triangles so.",
            "When people are building models that they render that they're often actually just turning everything into a zillion triangles and then rendering triangles.",
            "And much of science is also kind of taking a reductionist approach.",
            "Try to figure out some kind of core problem, figure how to solve for problem and we figure out how to use that solution over and over and over again to solve lots and lots of different problems.",
            "OK."
        ],
        [
            "So these reductions are sort of elemental.",
            "Need to know nothing, right?",
            "So.",
            "We're going to come up with some kind of mathematics which this describe.",
            "What is a good reduction?",
            "And in this definition of what is a good reduction is sort of not going to depend too much on the particular learning outcome that we reduced too.",
            "So will be able to apply a decision tree or support vector machine or unknown network, or logistic regression, or any of many different sub algorithms.",
            "So we're going to end up with a bunch of little components that we kind of plug together.",
            "This seems kind of minor, but it turns out to be extremely important.",
            "And the reason why it's really important is it turns out that writing good, fast code isn't that easy.",
            "So we get to sort of reuse good fast code, which gives you a big advantage.",
            "OK. OK, so I'm going to claim this approach is pretty easy.",
            "There's sort of three steps you want to fall.",
            "Follow in solving things by reduction.",
            "The first step is you going to identify your learning problem.",
            "So this is this is kind of an important step step that people actually get wrong pretty often for the reason that Sam said earlier.",
            "So you said that you know sometimes you lie to yourself about what the loss function is, right?",
            "Now.",
            "It's almost hard not to lie to yourself, but with the loss function is.",
            "But instead we want to do is we want to sit down and we want to really think about what our loss function is.",
            "So maybe I'll go back.",
            "To hear OK, so we're working for a charity and we're trying to Mail out envelopes.",
            "And we're going to ask for money and then get some variable amount of money back from the people that we ask.",
            "It could be zero, it could be $5.",
            "It could be $200, could be whatever.",
            "So what is our loss function here?",
            "Right?",
            "So our goal is to maximize the amount of money that we get back.",
            "So for any particular individual.",
            "Possibility we can choose to Mail the envelope and if we get no money back then we'll kind of have some sort of negative return.",
            "And if we get $5 back then we can get $5 minus the cost envelope back.",
            "And so forth, right?",
            "So this is it's not just about estimating whether or not we'll get money back if there's some sort of quantity involved here, so this is what I would call importance weighted classification.",
            "In this one.",
            "Similarly, it turns out maybe you're interested in conditional class probability estimation, right?",
            "Rather than just classification."
        ],
        [
            "OK, so.",
            "Maybe now we sat down and we thought about what the real problem is.",
            "Which you can probably do, or at least I hope it's probable is you can go out and you can just see it.",
            "Find a reduction in somebody's already made.",
            "And you can apply this reduction to some simple learning algorithms and you can just try it and see if it works right.",
            "So it would be very quick test.",
            "And if it's.",
            "Not necessarily premade.",
            "Then you have to actually create the reduction and maybe creating a reduction is as hard as creating one learning algorithm.",
            "Oh, and I guess once you find this reduction, you just.",
            "You just build your predictor using the reduction.",
            "OK, so this is."
        ],
        [
            "Enough of the overview stuff.",
            "And now what I would like to do, so I'd like to tell you how to take various learning problems and reduce them to binary classification.",
            "This is a question somebody should ask.",
            "Which is why binary classification.",
            "And the answer.",
            "Is it was because it was what I was familiar with?",
            "The approach that I'm telling you about this reductions approach you can reduce to anything that you want.",
            "Right, the only question is do you have a good tool for solving whatever you reduce to?",
            "So another very natural and compelling candidate would be squared error regression.",
            "OK, so."
        ],
        [
            "Importance weighted classification.",
            "The first of all, we need to define precisely what the problem that reducing from and two.",
            "So they're going to do to this classification.",
            "So the way we define a classification problem is, we say, suppose we have some probability measure on features across a single bit.",
            "Then we're going to try to find a classifier which Maps the features to that bit value is there.",
            "You said you put it over here.",
            "No.",
            "Interesting hello.",
            "In the case.",
            "Now would be really great if I had like a 3 way laser pointer and then I could.",
            "I don't know.",
            "OK, so.",
            "We have this this measure D. This is some probability measure.",
            "It could turn out that for every value of X is only unique value of zero or one which has a positive probability, right?",
            "And then then in that particular case is there exists some function from X201 which is perfect.",
            "It has 0 error rate.",
            "So the way we measure errors in classification is we say.",
            "What is the probability under a random draw from D that the classifier is wrong?",
            "It's very natural is 01 loss.",
            "So.",
            "When we're trying to do machine learning, we're going to be given a bunch of examples.",
            "Which are hopefully drawn from D and we can try to find some classifier C which has a small error rate.",
            "Maybe now you start to see why I reduced the classification.",
            "This is sort of I don't know.",
            "It seems like it's the most simple learning problem you could.",
            "You could have just predicting 1 bit.",
            "And the tricky thing, of course, is that.",
            "All we have are these examples.",
            "We don't actually know what the distribution is.",
            "So this is kind of impossible in general without some kind of prior information, but maybe it's maybe we do have some prior information, so so maybe it turns out to be possible for the problems that we actually care about."
        ],
        [
            "All right importance weighted classification is the same as binary classification except for one small difference.",
            "The distribution is over the same thing as before, but also there's this positive real number.",
            "Is this number I'll call the importance.",
            "So for example in the charity application, this would be so if you have zero return then it would be just the cost of the envelope.",
            "And for particular example, an if you get $5 back and the importance of the $5 minus the cost of envelope.",
            "Right?",
            "And then.",
            "We're going to have.",
            "Some should be crossed there.",
            "We're going to have an importance weighted data set, and now we're going to classifier.",
            "C is a small importance weighted loss, so importance weighted loss is just going to be.",
            "So I guess it's a basic fact of probabilities.",
            "I could rewrite this probability is the expectation of the indicator function of whether not the classifier was wrong.",
            "So indicated function is just one.",
            "If that argument is true.",
            "OK, so this is this is almost the same except now I have this importance value the little I.",
            "And I'm going to weight individual examples according to the importance value."
        ],
        [
            "Alright.",
            "So there's a very basic theorem.",
            "Is this theorem is so basic that nobody can claim it?",
            "The basic theorem is that for any classifier.",
            "And for any importance weight distribution.",
            "There's this other distribution, deprime.",
            "Which is the same as the old distribution, except now you kind of up wait by the importance.",
            "And he divided by the expected importance.",
            "And the claim is that.",
            "The binary error rate on D prime times expected importance equals the importance weighted error rate.",
            "So this is is a very basic insight.",
            "And it turns out that it's extremely useful.",
            "Because what this says is that if you find a good classifier C for D prime, then you're also finding a good classifier C. 4D.",
            "The original importance weight problem.",
            "The proof of this is pretty simple.",
            "Yes.",
            "So you have the importance weighted loss of C on D. You can just write out what this means, so we have so important weight loss is an expectation expectation is a sum where you wait according to the probability of each element.",
            "So this is the sum over all triples weighted according to the probability of the triple of the importance times without the classifier.",
            "Got things wrong?",
            "And then.",
            "It turns out you can pull the importance out.",
            "Like this?",
            "Just have the prime.",
            "'cause remember Deprime is just.",
            "This corner here, right?",
            "So this is ID divided by the expected importance.",
            "So it's D prime times expected importance.",
            "And then.",
            "This is.",
            "An expectation expected the prime of the indicated function classifier being wrong.",
            "So that's just the probability.",
            "It's just the definition of probability.",
            "And then.",
            "We can rewrite this is the.",
            "Binary error rate.",
            "And then just exactly the result, right?",
            "If our reductions were this simple would be a very very very nice.",
            "All this simple but."
        ],
        [
            "We hope they are.",
            "OK, so this theorem gives some sort of basic insight into how one problem is related to another problem, and then the question is how do we take advantage of that algorithmically?",
            "So there's actually what the theorem says that if you change the distribution from D to D prime and then you learn well on D prime just for 01 loss, you learn well, especially important weight loss in D. So it's all about how do we change D into D prime?",
            "Or how do we change samples from Dino samples from the prime 'cause?",
            "Remember, we only actually get samples.",
            "So one approach people often use is resampling an I guess the way this works is.",
            "You have a bunch of importance weights, one for each example.",
            "And you kind of play rouler so you just spin this around and.",
            "You know, maybe it lands on this example when you take that example and you spin it again and you get another example, maybe get the same one, but you just take it twice in that case.",
            "So.",
            "Heard of this can have some problems in practice and the reason why I can have problems is because a lot of our learning algorithms are kind of made an assumption that the samples that they observe are drawn independently.",
            "Right?",
            "And this process breaks that.",
            "So in particular, if my original samples were independent, they were they happened to be.",
            "Independent samples from D independent importance.",
            "Weighted samples.",
            "Then if I do resampling, it's no longer true.",
            "I guess it's easy to see this because if you have imagine just having a probability distribution on continuous space, right?",
            "So you never see the same sample twice, and if you do resampling on a finite sample set that you will observe the same sample twice.",
            "'cause obviously something is going weird."
        ],
        [
            "There's another trick.",
            "Which turns to fit the requirements that we like.",
            "It's just called rejection sampling and the idea here is we're going to pick some constant C which is larger than the importance weight.",
            "And then for every sample, we're going to flip a coin with a bias of I oversee.",
            "So if I is small then probably have heads is small.",
            "If the result is heads, would keep it in.",
            "Otherwise we're going to throw it away.",
            "This is some process and you can prove that if the original samples were independent, then after you run rejection sampling in your sample set, the remaining samples will also be independent.",
            "OK, so.",
            "Is something a little bit weird here.",
            "Which is that we're kind of throwing away some of the information.",
            "So maybe."
        ],
        [
            "Simple way to.",
            "Fix this.",
            "So here's kind of the most naive algorithm that you might imagine.",
            "The algorithm is called costing.",
            "It takes his input some importance, weighted data set, and some binary learning algorithm.",
            "And then it for maybe 10 iterations.",
            "He do rejection sampling.",
            "From the importance weights to get a binary data set with no importance weights.",
            "And then you learn a classifier on that binary data set.",
            "When you take the majority vote.",
            "Over each of these 10 different classifiers you learned.",
            "OK, so this is a very simple algorithm.",
            "Just repeated rejection sampling and then take a majority vote and learn classifiers.",
            "It turns out this album."
        ],
        [
            "Them.",
            "Works really great for this charity problem, so in 1998 the conference called KDD has some sort of championship.",
            "What were the problem was exactly?",
            "This is figuring out how to optimize, which you're going to envelopes to to get the most in terms of donations.",
            "And.",
            "You can.",
            "Apply.",
            "Uh.",
            "This costing reduction.",
            "And you can ask yourself, what is the total return that was received?",
            "And so the total return is measured in terms of profit, which is essentially equivalent to expected importance.",
            "And you can use different base classifiers, Naive Bayes, which Sam talked about.",
            "This boosted naive Bayes.",
            "This is just.",
            "Yeah, I'll get there in a second.",
            "This is a decision tree.",
            "This is a support vector machine which we'll hear about later.",
            "OK, so there's actually.",
            "Three things here is also something here and here and here and here that you can't see which has to do with.",
            "Suppose you just ignore the importance weights and you tell your classifier to learn.",
            "It turns out that all these classifiers tell you that.",
            "It's great to predict that nobody will donate.",
            "You have very high accuracy and then.",
            "And then you don't Mail out any envelopes and then you get no return.",
            "Write another thing that you can do is you can do that.",
            "Resampling.",
            "The first sampling process with green is he can do resampling and then you can apply your base classifier.",
            "Or you can do rejection sampling and it seems that this rejection sampling works pretty well.",
            "OK.",
            "So unfortunately we came up with this after 1998.",
            "Because I guess we would have won.",
            "You should always come take these things with a grain of salt because you know after the fact that these championships that are running machine learning.",
            "I think I actually great.",
            "After the fact, we get a kind of pour over.",
            "They said over and over again and try our best to overfit to it."
        ],
        [
            "Alright.",
            "Oh yeah.",
            "So this is a different data set.",
            "This is it's a similar task, it's another one of these.",
            "Optimizing.",
            "Mailing tasks.",
            "And now it turns out that you can actually the simple approach where we ignore the importance weights actually does give you some return when you're using a naive Bayes classifier.",
            "Um?",
            "And once again, the rejection sampling approach seems to work quite well."
        ],
        [
            "OK, so are there any questions about?",
            "Importance weighted approach.",
            "OK, so now I'm going to do is.",
            "I'm going to tell you how to reduce regression to classification.",
            "So this is a little bit of a weird thing to do because there's lots of great regressions that already exist.",
            "It's not clear this is.",
            "An essential problem needs to be solved, but you know, maybe we're feeling mathematical and we just want to know.",
            "If I have a good classifier, do I have a good regressor?",
            "Oh, by the way, you could think about going the other way right?",
            "You could reduce classification to regression.",
            "And this would be pretty easy because you just use the regressor to estimate the probability that the label is 1 given X, and then you just threshold is the probability above .5 or below .5.",
            "So give you some kind of reasonable estimate."
        ],
        [
            "So the other direction turned out to be a little more difficult, but it seems that there is actually.",
            "A fairly natural simple algorithm for doing this.",
            "OK, so first of all, what do I mean by regression?",
            "So you have to be kind of careful with this.",
            "'cause people mean different things.",
            "So what I mean by regression is we have some feature space and we have some Y label which is now in the range from zero to 1.",
            "So it could be point 3.5, one point 9, whatever.",
            "Uh.",
            "Then we're going to learn a regressor which is going to map.",
            "The features to some continuous value in 01.",
            "And the learning problem is going to be given.",
            "Examples which are hopefully drawn from D but may not be final regressor H, which is a small squared error.",
            "So squared error is actually a pretty interesting choice of error measure.",
            "And the reason why it's interesting is that the minimizer of this is the mean.",
            "So you want H of X to predict.",
            "The probability that the expected value of Y given X.",
            "So we're going to optimize the squared error.",
            "So now this is.",
            "This is the first step in the reduction.",
            "We've identified it some problem.",
            "The question is can we reduce this to binary classification?"
        ],
        [
            "Oh examples where we care about regression.",
            "Maybe we want to estimate the probability of.",
            "Of cancer, right?",
            "Given various measurements of a patient.",
            "Maybe we have some sort of big distributed system, so we have a bunch of sensors spread out all over the place and we want to estimate.",
            "Whether or not somebody pushed a button.",
            "For example, so each other different sensors might get different bits of information, and we could try to have each individual sensor make a prediction, but it predicts just a single bit.",
            "Somehow, combining a single bit is not not attractive because it doesn't have enough information to really.",
            "Maybe all of our sensors individually think nobody pushed the button, but if each of them happens to think that you know would probably .49 somebody pushed the button.",
            "If we knew all that information together.",
            "If we could get all that information together, then we would.",
            "Perhaps believe that somebody had pushed the button.",
            "Uh.",
            "Alright."
        ],
        [
            "OK. We're going to start with a basic observation.",
            "Let's pick some threshold interval between zero and one.",
            "Let's take.",
            "A sample, so this is a regression sample, which means Y varies between zero and one, and let's form an importance weighted sample of the form.",
            "Here my features is the Y value greater than the threshold or not?",
            "So this is the binary label.",
            "And.",
            "And then we have an importance weight, right?",
            "And important weight is going to be, how far is Y from the threshold.",
            "And the claim is that if C is perfect.",
            "Meaning it's going to give us the best possible prediction then that will mean.",
            "That when C of X = 1, the expected value of Y given X is going to be greater than T. And you can adjust.",
            "This is easy to workout, so if C of X is 1 and the classification is correct, it's doing the best possible.",
            "Then that implies.",
            "That OK, so we have two kinds of wise we have the wise which are greater than threshold and we have the wise which are less than the threshold.",
            "So when Y is greater than the threshold.",
            "The important weight is just going to be y -- T. And why is less than the threshold important so it will be T -- Y?",
            "So if it's correct to say that the classification is 1 important classification is one, that means that the importance on.",
            "The wise, which are above the thresholds.",
            "The expected importance in the wise above threshold is greater than expected importance in The Witcher below the thresholds.",
            "So you get this inequality.",
            "And then we can just.",
            "Subtract this from both sides and now T -- Y becomes y -- T here and now we just have the expected value, so we have indicated function of Y greater than T and it are function of why hold on?",
            "That's a bug.",
            "The inequality doesn't change science.",
            "OK, so this is indicator function of Y less than or equal to T. Indicator function of Y greater than T, which is everything and we multiply by the same thing in either case.",
            "So this is just expected value of y -- T. And teams are constants, so you end up getting this equation.",
            "OK, so this is.",
            "Another simple observation, and it tells us if you think about it how to do this?"
        ],
        [
            "So here's an algorithm.",
            "This is the algorithm which reduces squared error regression to binary classification.",
            "It's a very simple algorithm.",
            "What it does is.",
            "OK, so the name of T changed the P. What it does is it start with your regression data set.",
            "You choose some values of T. Some thresholds?",
            "And then you form importance weighted datasets.",
            "And then you're going to apply an importance weighted learning algorithm.",
            "You're going to learn a bunch of different binary classifiers.",
            "And then at Test time you're going to have some features you're going to ask each classifier how it predicts given the features.",
            "Maybe the predictions will look like this, maybe you know it's almost never the case that Y is greater than .99, so it's zero and turns out that.",
            "Even why is typically less than .5, which means the Sky predicts 0, but it is greater than .1 otherwise, typically larger .1.",
            "So this guy predicts one in this guy predicts one.",
            "And then the way we're going to form a regression estimate.",
            "Is by just looking.",
            "For where this threshold is, where do we switch from?",
            "Predicting ones to predicting zeros.",
            "And then maybe there's some discretization in the system, so maybe we predict the value which is in between these two, which would be .3.",
            "Now is the question in at least 10 minutes.",
            "Somebody ask a question.",
            "What happened to probability?",
            "Good good, there's a few problems with this algorithm."
        ],
        [
            "So there's some details here.",
            "I say there is reducing this to binary classification.",
            "And actually just reduced to importance weighted classification.",
            "But I told you how to reduce importance weighted to binary, so you can just apply that.",
            "Previous reduction and here's your answer, Sam.",
            "If it turns out.",
            "And this could happen very easily.",
            "It could be that you get 101000.",
            "So what you do is you just sort.",
            "So then you get all ones and then all zeros.",
            "That sounds like a hack, but it turns out that that's exactly what the theory tells you to do.",
            "And then I guess there's also an issue in sort of how you discretize Auntie.",
            "We tried using sort of a uniform grid and it works pretty well.",
            "It works a little bit better if you kind of discretize on demand.",
            "So maybe most of your Y values are up near .9 and then you want to put your discretization in that region.",
            "OK.",
            "Right, so here's some algorithm I told you there was some basic observation."
        ],
        [
            "And then what can we?",
            "So first question is, how well does this algorithm work?",
            "And I told you this is a little bit of a weird question to ask.",
            "Can we reduce regression to classification 'cause there are a lot of good regressors around?",
            "And some of these.",
            "So what this is doing is comparing.",
            "Various algorithms.",
            "To various algorithms under the program reduction so.",
            "This is naive Bayes with approving reduction.",
            "This is a decision tree with production.",
            "This is support vector machine with production.",
            "It's comparing these various algorithms.",
            "Two, what happens when you use other approaches people use for doing regression?",
            "And in particular.",
            "The kind of problems these are.",
            "R. Is there really classification problems?",
            "So when you're doing regression with classification, you're really trying to estimate the probability that y = 1 given X. OK, so some other approaches people use is they just try to apply a naive Bayes classifier straight it.",
            "Estimate probabilities.",
            "Some people try to use a naive Bayes classifier and then instead of reporting the actual probability of the Y = 1 given X, The Naive Bayes tells you that they try to fit some sort of function to that output.",
            "Typically a sigmoid.",
            "Sometimes people try to use a decision tree straight.",
            "I don't know if I don't think anybody is talking about Decision Tree here, but the way Decision Tree works is very simple.",
            "What happens is.",
            "You have.",
            "A bunch of features and 1st you have some test is the is the third feature above or below .2 right?",
            "And this puts your data and then you have another test is the 2nd feature above or below .7?",
            "Let's put your data again and again and again.",
            "You keep on splitting your data as you walk down some sort of tree until you get down to the leaves, and then in the leaves you have some samples remaining and you just ask yourself what fraction of the samples predict one way versus another.",
            "So this is some way to get a probabilistic estimate and the way you label it, you just.",
            "A particular example, SMB, you walk down the tree according to whether or not your first feature is above what was at .1.",
            "I don't think it was right.",
            "So you just walk down the tree to get to a leaf, and then you predict whatever proportion of the labels were one in that leaf.",
            "OK, so another approach people use is decision tree with bagging.",
            "How to describe begging.",
            "Begging is another very simple approach.",
            "It's like.",
            "It's like costing.",
            "Except importance weights are all one.",
            "And C is.",
            "Well.",
            "Now it's more like I guess it's like costing except.",
            "Instead of doing rejection sampling.",
            "You have importance weights all one and you use resampling rather rejection sampling.",
            "That's the simplest description.",
            "And then other people uses the.",
            "How to apply a support vector machine?",
            "This gives them a pretty good classifier and then to get a good prediction a good probabilistic prediction if it a signal into the margin which is some internal value can extract from a support vector machine.",
            "So I guess the way to read this is this.",
            "We're talking bout squared error, which means that smaller is better.",
            "And for each of these kinds of classifiers, there's several approaches and the ordered in the same way they are up here, right?",
            "So right there.",
            "Is naive Bayes with under the program reduction?",
            "And again.",
            "That's naive Bayes into the program reduction.",
            "This would be naive Bayes under SVM and this would be naive Bayes under.",
            "Sorry this would be a support vector machine or the production and this would be a decision tree into production.",
            "And if you take a look, you see this is working reasonably.",
            "I think that's about all you can conclude if you take a look at the exact numbers, it seems like it works a little bit better than other approaches, but maybe not dramatically better.",
            "OK.",
            "Right so.",
            "I like to try to do some theory.",
            "Question is what can we prove?"
        ],
        [
            "Not this algorithm.",
            "We want to actually prove that this algorithm is a valid reduction from regression to classification.",
            "So there's a trick which is very helpful in thinking about the proof.",
            "So we have a bunch of different classifiers and we could try to prove something with respect to kind of the collection of classifiers that we actually made.",
            "But maybe there's some way to just think about having one classifier.",
            "It turns out there's a trick which let's do this what we're going to do is we're going to think about.",
            "Adding the threshold T into.",
            "The feature space.",
            "And then we're going to think about.",
            "So we have a importance weighted.",
            "Data set for each threshold team going to think about taking a union, or maybe just a stochastic choice from each of these.",
            "And then we're going to learn one classifier.",
            "Respect to this one sample set.",
            "And then we're going to define CSUB T of X = C of X comity.",
            "So this is a trick we since we have a bunch of classifiers and we're learning them and calling them in parallel.",
            "We can just put the name of the classifier into the feature space.",
            "And then we can ask yourselves, what does small loss spec to this classifier?",
            "This one imply.",
            "Just a little bit of a cheat, because the algorithm that I told you in the other thing to analyze are slightly different, right?",
            "And whether this is reasonable to do in practice, we don't actually know, because.",
            "We didn't do the experiments.",
            "But it is extremely helpful.",
            "Theoretically.",
            "It makes the theorems much cleaner.",
            "You can prove about the same thing if you don't do this trick.",
            "But it's just it's much nicer if you do."
        ],
        [
            "OK.",
            "So now.",
            "We're going to prove a theorem, so the theorem says for every.",
            "Classifier so this 01.",
            "Here is the threshold right?",
            "So we have this one classifier trick for every classifier this form.",
            "And for every distribution.",
            "Send the typo.",
            "This should be a hard brackets because we're going to have the interval from zero to 1.",
            "For every distribution, if every regression problem.",
            "Squared error of our estimate is going to be bounded by.",
            "OK, so now we have.",
            "A distribution.",
            "D. Which is going to.",
            "Produce samples that we're going to apply this.",
            "One classifier trick 2, which is going to.",
            "Produce a distribution over importance weighted samples.",
            "Right and then via the.",
            "Costing reduction on just binary samples, so this is kind of a tricky thing, so we don't know what distribution D is.",
            "But the distribution D is implicitly inducing a distribution on just binary classification.",
            "And the way that you get at this this induce distribution.",
            "The way that you draw from this news distribution is you first draw from the original distribution D. And then you pick a random threshold T. Uniformly from 01.",
            "Then you just create the importance weighted sample.",
            "Then you transform the importance weighted sample into a binary sample using the cost reduction.",
            "OK, so.",
            "Right?",
            "So claim is the error rate of the classifier on this induced distribution.",
            "Minus.",
            "Something.",
            "Bounds of squared error.",
            "So if I just told you this theorem, which is true?",
            "Then it wouldn't actually be very interesting, and the reason why it wouldn't be very interesting is because we're trying to apply.",
            "Regression in kind of inherently noisy situations.",
            "So we need a theorem which is actually going to imply something when we have a lot of noise.",
            "Just saying I have one error rate bounds, another error rate isn't very convincing because this error.",
            "This error rate will be pretty large when you have a very noisy D. To what you do is you subtract off the smallest possible error rates.",
            "Sometimes people call this the Bayes error rate she ask yourself.",
            "Suppose it chose the best possible classifier that would have some binary error rate.",
            "And turns out that you can subtract this off.",
            "So that's this difference.",
            "Which bounds the squared error.",
            "The difference is a regret.",
            "Typically the way we think about the way we define regrets is how well did we do in comparison to how well we could have done.",
            "The claim is that the binary regret.",
            "Bounds the squared error regret.",
            "OK, so.",
            "Yeah.",
            "Nice theorem.",
            "How do you prove it?"
        ],
        [
            "So before I try to prove it mathematically, let me tell you why you should believe this is true.",
            "So let's think about an extreme case where it happens that we're just trying to estimate the.",
            "The probability that y = 1 given X.",
            "So if we're trying to estimate the probability that y = 1 given X, let's just think about some situations.",
            "One situation is where the probability that y = 1 given X is 1, right?",
            "And then.",
            "As we vary the threshold.",
            "The importance weighted loss will look like this line.",
            "Right, so the when the threshold is one that.",
            "And the probability equals one given X is 1, then the importance will be 0 because we importance weight is just.",
            "Magnitude of y -- T if Y and Tier 1 then important way to 0.",
            "Another situation is where the problem with y = 1 given X is 0, so we're always going to be.",
            "Why wow is always 0 and now the importance weight is going to look like this.",
            "And then the question is what happens in between?",
            "Suppose that the probability that y = 1 given access point 5.",
            "What happens so?",
            "What happens what you should expect to happen is a convex combination of this and this because you have some probability that Y is 1.",
            "And probably some probably that why is 0.",
            "Now it's not quite that, and the reason why it's not quite that.",
            "It's because.",
            "We're trying to prove a strong theorem.",
            "The strong theorem subtracts off the minimum possible.",
            "Error rate.",
            "Right so.",
            "Instead of having the green line be like so it's like so because we subtract off the minimum, which means that it always touches 0.",
            "OK, so.",
            "So these are some cases and now what we want to do is we want to think about.",
            "We don't think about an adversary who's going to choose how to make our classifiers screw up.",
            "And we're going to place a constraint on our adversary.",
            "The constraint is that the classifier cannot take too much importance weighted loss, or you cannot cause too much importance weighted loss.",
            "And the question is, what is the most efficient way for the adversary to disturb our prediction?",
            "Given his budget on the amount of importance weight loss he can incur.",
            "So every classifier has some importance importance weighted loss.",
            "So suppose that were just in this case here, right?",
            "So this classifier at this threshold will have this much importance weight loss.",
            "The server is winning predicts wrong.",
            "OK, so there's two observations which make this kind of trivial.",
            "The first observation is it would be very dumb for the adversary to scroop.",
            "On this side of the truth, and on this side of the truth.",
            "And the reason why is very dumb is because we had that sorting operation right.",
            "So if we screw up here then we're going to be turning 1 into a 0 interest group here going to 0 until one.",
            "After we sort these cancel each other out.",
            "So it's as if the adversary achieved nothing.",
            "OK, so this is that all of the mistakes are going to be on one side of the truth.",
            "In the worst case.",
            "And then.",
            "A second observation is it wouldn't make very much sense for the adversary to mess up here and to not mess up here.",
            "Because after we sort, these two are equivalent and importance weights the adversary pays.",
            "Is smaller here than it is over here.",
            "So it says that.",
            "The most efficient way for the adversary with the budget to to disturb our estimate.",
            "It is just choose one side of the truth and just start airing over and over and over again.",
            "Until he runs out of budget.",
            "And then the probing algorithm will report this.",
            "The truth will be here.",
            "And then.",
            "This is just something linear, so if we integrate that, we're going to get something squared.",
            "So the amount of area here is the amount of budget the adversary has.",
            "And it's going to induce some particular squared loss.",
            "OK.",
            "They've been marginally clear.",
            "We also have the proof in math.",
            "OK, so.",
            "Other questions.",
            "This is a good page to ask questions about.",
            "Alright.",
            "OK, so let's go back to this page.",
            "If you take a look at the importance weight.",
            "Here, the integration of the importance weight is just going to be half because it's it's half of this.",
            "Unit wrecked"
        ],
        [
            "We're going to square OK, so the expected importance is going to be less than equal to half.",
            "So we're going to do is just going to start writing down what the various definitions mean, so we have this term which is a regret with how well we did compared to how we could have done.",
            "And the claim is that.",
            "This equals this, so we're going to write down.",
            "The importance weighted loss, minus the minimum possible importance weighted loss.",
            "Now we're going to multiply by two, and the reason we're going to buy two.",
            "It's because you know the expected importance is less than 1/2, and then you can go back to.",
            "Costing theorem, I won't do it now and there was this relationship where the binary loss.",
            "Times expected importance equal importance weighted loss.",
            "One over half is 2.",
            "OK. What's happening here?",
            "So what we're going to do?",
            "He's going to change around the expectation.",
            "So it turns out that the way that you prove these reductions typically is all the action is kind of happening in the loss function an in the particular labels.",
            "And it doesn't really matter what your features are, so you can just condition on your features.",
            "So we're going to pull the expectation over X&T out of.",
            "The the full expectation, so just kind of reordering how we take the expectation.",
            "I feel like I'm going a little bit too fast.",
            "Think about what this means here.",
            "So this classifier with the smallest error rate.",
            "Is going to be the one which.",
            "OK so.",
            "If we predict.",
            "Uh.",
            "1.",
            "Then whenever the truth is zero, we're going to pay.",
            "This kind of importance weighted loss right?",
            "So this is just if the truth is 0, if Y is less than the threshold, we're going to pay T -- Y.",
            "Whenever we predict zero, if the truth is 1 if.",
            "Why is greater than T?",
            "The amount of loss that we're going to suffer just y -- T?",
            "Right and the best possible classifiers should pay the minimum of these two possible losses.",
            "OK."
        ],
        [
            "Oh or or conditioning on action T there's two possibilities, either the classifier C is going to predict zero is going to predict one and it's going to be right or so either going to be wrong in this prediction.",
            "So if it predicts perfectly, then the difference between the loss of the best classifier and the loss of our classifier is going to be 0 for this particular choice of X&T.",
            "If it predicts incorrectly, and that's not true, and in particular the difference will be this difference here.",
            "Which you should maybe be remembering from that observation we made earlier.",
            "Metro.",
            "When C is wrong.",
            "The difference between this and this is going to be this minus that magnitude.",
            "So we have a - and we have a - so we can just reorder things where we have a + T -- Y.",
            "And then we have.",
            "While it's singles T&Y greater than T. Which means this is just T minus expected value of Y given X.",
            "And then, if you recall, we had that 2 running around, so says the cost.",
            "Of the misprediction is T times expected value of Y given X. OK, so all that I've done right here is essentially.",
            "What was obvious from this graph all that I've done is I've told you that the importance weight, the amount of loss suffered for screwing up is something kind of linear in how far you are from the threshold.",
            "Right ptosis.",
            "If you screw up for this particular.",
            "T. You look at how far the truth is from the threshold."
        ],
        [
            "And then now we're just going to make the observations that I made earlier.",
            "Little bit more mathematical way.",
            "Um?",
            "Actually the same observations.",
            "So.",
            "A good adversary is only going to screw up on one side of the truth because the sorting cancels out things.",
            "As in the screw up is close to the truth as possible because the loss is linear in the distance of the threshold from the truth.",
            "And then.",
            "We have the truth.",
            "We're going to have the adversary induce a loss of Delta.",
            "And we're going to integrate this.",
            "From the truth to the truth plus Delta.",
            "Yeah, and then this integral turns out to be Delta squared.",
            "And that's essentially the proof.",
            "Alright, so.",
            "Are there questions about this theorem?",
            "Works proof.",
            "Alright."
        ],
        [
            "Is a simple modification of this.",
            "So if you go back to the probing algorithm and you modify it very slightly and you can do quantile regression.",
            "So when people are trying to do regression, the most common thing they do is squared error regression like we talked about first.",
            "Another thing which is maybe very useful.",
            "In which.",
            "People don't necessarily.",
            "Often times people use cordair regression when they really want to use quantile regression instead.",
            "So what does quantile regression quantile regression is?",
            "So it's going to be parameterized with some Q and 01.",
            "It wouldn't hurt to predict some value.",
            "White hats is the probability wise good in white hat equals Q.",
            "So if you take this probing algorithm and you just modify it slightly, it turns out that you can get quantile regression.",
            "The way you modify it is in terms of the importance weights.",
            "So now instead of having remember before we had the magnitude of y -- T, Now we just have Q when Y is greater than T and 1 -- Q when Y is less than T. And then you can go through and you can prove that this.",
            "The.",
            "Quantile.",
            "Loss.",
            "Is bounded by the binary.",
            "The quantile regret is bounded by the binary regret.",
            "The proof is essentially the same.",
            "So.",
            "Quantile regression is a little bit less used, and that means that people haven't had so much time to develop a lot of tools to solve it very well so."
        ],
        [
            "Perhaps it's unsurprising, then, that when you go and you apply this quantile regression reduction.",
            "Two various classifiers.",
            "It works pretty well.",
            "So for four different datasets and three different choices of quantile.",
            "We computed.",
            "Well, we applied this reduction too.",
            "I think the base classifiers were logistic regression and one was Decision Tree.",
            "OK, So what is this number here?",
            "This number is take the worst guy out of these four possibilities.",
            "So this is linear quantile regression.",
            "This is kernelized quantile regression.",
            "And then this quantity algorithm, applied logic, aggression, quantified decision tree.",
            "You take the worst of these four.",
            "And then.",
            "Divide the performance in terms of the absolute of the quantile loss by the worst, right?",
            "So this is a normalized performance.",
            "So what this is saying is that using quantum with the decision tree we got maybe 60% of the loss that we got using linear regression, yeah?",
            "Define SD quantile regression loss.",
            "Think I was hoping that you wouldn't ask that question because it turns out that I screwed up this theorem statement.",
            "This is actually what you get when you're trying to do median.",
            "If you for general quantile loss, what you do?",
            "But what is going to look like is essentially.",
            "This times the magnitude of y -- T and 1 -- Q times the magnitude of 1 -- T. Yes.",
            "This theorem does hold for the pinball loss Virginia like you just.",
            "Alright.",
            "Some experiments seem to work pretty well.",
            "A significant caveat for the kernel stuff which Alex probably interested in so.",
            "Their computational problems here, and the reason why there are compilation problems is because these are pretty big datasets.",
            "Uh.",
            "And.",
            "When you go and apply a reduction, the actual amount of computation which is going on in the reduction is relatively small.",
            "Most of the computation goes on inside of the learning algorithm that you actually are going to use right?",
            "And turns out these learning algorithms are reasonably well optimized.",
            "And they will often allies enough that they can just handle these large datasets.",
            "Uh.",
            "So I guess this algorithm here is very fast.",
            "It could handle large data set and this algorithm.",
            "It's a little bit unfair for us to be comparing to it because we couldn't get it to run with very large data set.",
            "I think we could only feed it about 3000 samples or so so.",
            "This is your copy out here when you look at the green X you should be understanding that you know there's like 50,000 samples here and it's only seeing 3000 of them.",
            "So I think for the sake of fairness, I should that we didn't know if you wanted to keep you so.",
            "I mean, you could use like anybody cares.",
            "Code mean.",
            "With that you could easily set it up.",
            "I agree so.",
            "Writing good, fast code takes some effort.",
            "We know how to do it in specific cases, and I guess it's just.",
            "Just if you're practically minded, the question is.",
            "Do you want to?",
            "Do you want to be?",
            "Writing good fast code over and over again, or you want to be trying to reuse it.",
            "And.",
            "Maybe initially try to reuse it and then over the long term you try to write good, fast code.",
            "That's a good solution."
        ],
        [
            "Alright, so I think I'm about done.",
            "Couple of caveats which you should understand so some people worry bout log loss like Sam was talking about log loss and it's very natural to ask yourselves.",
            "What does this imply about log loss and turns out that implies nothing interesting.",
            "And the difficulty is that log loss can be infinite.",
            "And we don't know how to reduce a loss that could be infinite sum of the time to a loss that is bounded.",
            "In fact, you can't do it if you allow the predictor that you induce to choose any particular value, right?",
            "'cause then some very small fraction of the time the adversary could just choose to cause it to totally screw up and suffer infinite loss.",
            "Then the expectation of some finite numbers in some infinite numbers is infinite number, and that's just pretty unfortunate for the mathematics.",
            "Another thing that people often try to do is they tried to use regression to estimates like, say, the relevance of web pages.",
            "Something like that and.",
            "And then maybe order web pages.",
            "According to this relevance value.",
            "And you can't prove.",
            "At least for several different natural ranking losses, you can't prove anything interesting, and the fundamental reason is.",
            "Let's say that you have one relevant web page and you have 100,000 year relevant web pages.",
            "This is fairly typical case.",
            "Then, an adversary who's smart is just going to screw up.",
            "The estimate of the relevance there.",
            "1.",
            "Web page right now is going to go down to rank number.",
            "100,000 and one.",
            "And you just get garbage.",
            "So the total amount of time that the adversary messes up is just one out of 100,000.",
            "But the result that you get is not useful."
        ],
        [
            "OK, so I think.",
            "Now the time we want to take a break, so I'll be hanging around.",
            "And if you have questions in the break then should feel free to come up and talk to me.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing that I think is essential to understand about machine learning is sort of the state that the field is in.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think that the state that the field is in is preliminary.",
                    "label": 0
                },
                {
                    "sent": "So Sam got up and he told you about some useful way to go about trying to solve the problem an I'm going to tell you about a way that I found useful for solving learning problems, and it's a different way and not entirely compatible.",
                    "label": 0
                },
                {
                    "sent": "And it's not clear at all what is what is going to end up being the right way.",
                    "label": 0
                },
                {
                    "sent": "So part of the reason why I'm telling you this is that I'd like to encourage you to ask questions.",
                    "label": 0
                },
                {
                    "sent": "It's entirely appropriate.",
                    "label": 0
                },
                {
                    "sent": "Ask questions.",
                    "label": 0
                },
                {
                    "sent": "It's entirely appropriate to ask hard questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is about learning reductions.",
                    "label": 0
                },
                {
                    "sent": "This is some stuff that I've worked on with a number of other people.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How's it go?",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Let's say that you're trying to solve some learning problems.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe you work for some charity and one of the charities do is they send out mails to people and they say please give us money right?",
                    "label": 0
                },
                {
                    "sent": "So the envelope and postage costs a little bit of money to charity and so there's kind of an optimization problem to solve here.",
                    "label": 0
                },
                {
                    "sent": "Who are you going to Mail and ask money from if you're going to ask if you send Mail to everyone in the world, then maybe maybe your budget is broken.",
                    "label": 0
                },
                {
                    "sent": "But if you figure out who to send money, who's likely to donate, and you send Mail to them, then maybe you do maybe come out positive.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do machine learning for some sort of charity.",
                    "label": 0
                },
                {
                    "sent": "So maybe we could try to apply binary classification.",
                    "label": 0
                },
                {
                    "sent": "We just try to predict is this person going to donate money or not right?",
                    "label": 0
                },
                {
                    "sent": "And maybe we get a pretty good predictor and then we we Mail out the envelopes and we discover that we just barely break even in the real world, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a bummer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there's a problem here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another situation which comes up.",
                    "label": 0
                },
                {
                    "sent": "Maybe we were working for a doctor.",
                    "label": 0
                },
                {
                    "sent": "And the doctor wants some mechanism for predicting whether not somebody has cancer.",
                    "label": 0
                },
                {
                    "sent": "So maybe you said to use a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And then you apply your support vector machine.",
                    "label": 0
                },
                {
                    "sent": "You get some sort of predictive without this cancer.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to the doctor.",
                    "label": 0
                },
                {
                    "sent": "The doctor is not.",
                    "label": 0
                },
                {
                    "sent": "I don't want to tell me whether not this cancer I want to tell me the probability that there's cancer, right?",
                    "label": 0
                },
                {
                    "sent": "So then maybe you take your support vector machine so this is a little bit out of sequence in some sense, because people quotes for machines more thoroughly.",
                    "label": 0
                },
                {
                    "sent": "But you can peer into inside of what support vector machine is doing.",
                    "label": 0
                },
                {
                    "sent": "And you can extract some number called a margin.",
                    "label": 0
                },
                {
                    "sent": "And then you can kind of.",
                    "label": 0
                },
                {
                    "sent": "The margin is kind of like a confidence that you have.",
                    "label": 0
                },
                {
                    "sent": "It's in your prediction and then you can return that and you call that a probability and then it might turn out that these probabilities just aren't well calibrated there, always you always getting probabilities that are near .5 that your prediction is correct or something like that.",
                    "label": 0
                },
                {
                    "sent": "And then you lose because you know you're not managing to do with the Doctor wants.",
                    "label": 0
                },
                {
                    "sent": "I guess in case you ever end up with cancer, you really want to get that right.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The question is what's going wrong here?",
                    "label": 0
                },
                {
                    "sent": "And this is a very common failure mode that a lot of learning algorithms fall into.",
                    "label": 0
                },
                {
                    "sent": "This sort of certain set problems that we know how to attack.",
                    "label": 0
                },
                {
                    "sent": "We have tools to attack.",
                    "label": 0
                },
                {
                    "sent": "We go out and we try to apply these set tools too.",
                    "label": 0
                },
                {
                    "sent": "The world's problems.",
                    "label": 0
                },
                {
                    "sent": "It turns out there's a mismatch because the problem that the world is imposing is not the problem that your tool is actually made to solve.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of two ways we can try to go on beyond this.",
                    "label": 0
                },
                {
                    "sent": "One of them is we try to design a new algorithm for every problem in the real world.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is sort of great for learning research and the other approach is to try to figure out some way to reuse your old algorithms to solve these new problems.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about this approach, right?",
                    "label": 0
                },
                {
                    "sent": "How do we take some sort of standardized learning machine and use it over and over again to solve a wide array of different kinds of problems?",
                    "label": 0
                },
                {
                    "sent": "This is a question that somebody should ask.",
                    "label": 0
                },
                {
                    "sent": "Alright, so somebody needs to test the system.",
                    "label": 0
                },
                {
                    "sent": "If you push the button, does the camera actually focus on you?",
                    "label": 0
                },
                {
                    "sent": "Sam test the system.",
                    "label": 0
                },
                {
                    "sent": "John 10 #2 do everything that number one can.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "We didn't get a camera though.",
                    "label": 0
                },
                {
                    "sent": "Interesting, OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "What I'm telling you about is is another way to think about how to solve problems, and it has particular strengths and weaknesses of its own.",
                    "label": 0
                },
                {
                    "sent": "So I think probably.",
                    "label": 0
                },
                {
                    "sent": "The biggest weakness is related to this question, so if you're in a situation we have a very small amount of data.",
                    "label": 0
                },
                {
                    "sent": "That means you're going to.",
                    "label": 0
                },
                {
                    "sent": "You're going to have to have sort of some kind of strong prior information in order to succeed, and then the approach that Sam talked about earlier is probably pretty reasonable.",
                    "label": 0
                },
                {
                    "sent": "Situation where you have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Then often the approach I'm going to talk about will end up being useful to you.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What are these learning reductions?",
                    "label": 0
                },
                {
                    "sent": "So we're going to think about how to reuse an algorithm to solve a bunch of different problems.",
                    "label": 0
                },
                {
                    "sent": "So there's several things which are several properties of this approach.",
                    "label": 0
                },
                {
                    "sent": "One that it's reductionist.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll just go through each of these individually.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh OK, so there are a lot of good things which are reductionist.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is we figured out how to kind of reduce the process of computation to transistors.",
                    "label": 0
                },
                {
                    "sent": "This turned out to be extremely useful to all of us.",
                    "label": 0
                },
                {
                    "sent": "We figured out how to reduce the rendering of scenes to rendering triangles so.",
                    "label": 0
                },
                {
                    "sent": "When people are building models that they render that they're often actually just turning everything into a zillion triangles and then rendering triangles.",
                    "label": 0
                },
                {
                    "sent": "And much of science is also kind of taking a reductionist approach.",
                    "label": 0
                },
                {
                    "sent": "Try to figure out some kind of core problem, figure how to solve for problem and we figure out how to use that solution over and over and over again to solve lots and lots of different problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these reductions are sort of elemental.",
                    "label": 0
                },
                {
                    "sent": "Need to know nothing, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to come up with some kind of mathematics which this describe.",
                    "label": 0
                },
                {
                    "sent": "What is a good reduction?",
                    "label": 0
                },
                {
                    "sent": "And in this definition of what is a good reduction is sort of not going to depend too much on the particular learning outcome that we reduced too.",
                    "label": 0
                },
                {
                    "sent": "So will be able to apply a decision tree or support vector machine or unknown network, or logistic regression, or any of many different sub algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we're going to end up with a bunch of little components that we kind of plug together.",
                    "label": 0
                },
                {
                    "sent": "This seems kind of minor, but it turns out to be extremely important.",
                    "label": 0
                },
                {
                    "sent": "And the reason why it's really important is it turns out that writing good, fast code isn't that easy.",
                    "label": 0
                },
                {
                    "sent": "So we get to sort of reuse good fast code, which gives you a big advantage.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so I'm going to claim this approach is pretty easy.",
                    "label": 0
                },
                {
                    "sent": "There's sort of three steps you want to fall.",
                    "label": 0
                },
                {
                    "sent": "Follow in solving things by reduction.",
                    "label": 0
                },
                {
                    "sent": "The first step is you going to identify your learning problem.",
                    "label": 0
                },
                {
                    "sent": "So this is this is kind of an important step step that people actually get wrong pretty often for the reason that Sam said earlier.",
                    "label": 0
                },
                {
                    "sent": "So you said that you know sometimes you lie to yourself about what the loss function is, right?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "It's almost hard not to lie to yourself, but with the loss function is.",
                    "label": 0
                },
                {
                    "sent": "But instead we want to do is we want to sit down and we want to really think about what our loss function is.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'll go back.",
                    "label": 0
                },
                {
                    "sent": "To hear OK, so we're working for a charity and we're trying to Mail out envelopes.",
                    "label": 0
                },
                {
                    "sent": "And we're going to ask for money and then get some variable amount of money back from the people that we ask.",
                    "label": 0
                },
                {
                    "sent": "It could be zero, it could be $5.",
                    "label": 0
                },
                {
                    "sent": "It could be $200, could be whatever.",
                    "label": 0
                },
                {
                    "sent": "So what is our loss function here?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So our goal is to maximize the amount of money that we get back.",
                    "label": 0
                },
                {
                    "sent": "So for any particular individual.",
                    "label": 0
                },
                {
                    "sent": "Possibility we can choose to Mail the envelope and if we get no money back then we'll kind of have some sort of negative return.",
                    "label": 0
                },
                {
                    "sent": "And if we get $5 back then we can get $5 minus the cost envelope back.",
                    "label": 0
                },
                {
                    "sent": "And so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So this is it's not just about estimating whether or not we'll get money back if there's some sort of quantity involved here, so this is what I would call importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "In this one.",
                    "label": 0
                },
                {
                    "sent": "Similarly, it turns out maybe you're interested in conditional class probability estimation, right?",
                    "label": 0
                },
                {
                    "sent": "Rather than just classification.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Maybe now we sat down and we thought about what the real problem is.",
                    "label": 0
                },
                {
                    "sent": "Which you can probably do, or at least I hope it's probable is you can go out and you can just see it.",
                    "label": 0
                },
                {
                    "sent": "Find a reduction in somebody's already made.",
                    "label": 0
                },
                {
                    "sent": "And you can apply this reduction to some simple learning algorithms and you can just try it and see if it works right.",
                    "label": 0
                },
                {
                    "sent": "So it would be very quick test.",
                    "label": 0
                },
                {
                    "sent": "And if it's.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily premade.",
                    "label": 0
                },
                {
                    "sent": "Then you have to actually create the reduction and maybe creating a reduction is as hard as creating one learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Oh, and I guess once you find this reduction, you just.",
                    "label": 0
                },
                {
                    "sent": "You just build your predictor using the reduction.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enough of the overview stuff.",
                    "label": 0
                },
                {
                    "sent": "And now what I would like to do, so I'd like to tell you how to take various learning problems and reduce them to binary classification.",
                    "label": 0
                },
                {
                    "sent": "This is a question somebody should ask.",
                    "label": 0
                },
                {
                    "sent": "Which is why binary classification.",
                    "label": 0
                },
                {
                    "sent": "And the answer.",
                    "label": 0
                },
                {
                    "sent": "Is it was because it was what I was familiar with?",
                    "label": 0
                },
                {
                    "sent": "The approach that I'm telling you about this reductions approach you can reduce to anything that you want.",
                    "label": 0
                },
                {
                    "sent": "Right, the only question is do you have a good tool for solving whatever you reduce to?",
                    "label": 0
                },
                {
                    "sent": "So another very natural and compelling candidate would be squared error regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "The first of all, we need to define precisely what the problem that reducing from and two.",
                    "label": 0
                },
                {
                    "sent": "So they're going to do to this classification.",
                    "label": 0
                },
                {
                    "sent": "So the way we define a classification problem is, we say, suppose we have some probability measure on features across a single bit.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to try to find a classifier which Maps the features to that bit value is there.",
                    "label": 0
                },
                {
                    "sent": "You said you put it over here.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Interesting hello.",
                    "label": 0
                },
                {
                    "sent": "In the case.",
                    "label": 0
                },
                {
                    "sent": "Now would be really great if I had like a 3 way laser pointer and then I could.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have this this measure D. This is some probability measure.",
                    "label": 0
                },
                {
                    "sent": "It could turn out that for every value of X is only unique value of zero or one which has a positive probability, right?",
                    "label": 0
                },
                {
                    "sent": "And then then in that particular case is there exists some function from X201 which is perfect.",
                    "label": 0
                },
                {
                    "sent": "It has 0 error rate.",
                    "label": 0
                },
                {
                    "sent": "So the way we measure errors in classification is we say.",
                    "label": 0
                },
                {
                    "sent": "What is the probability under a random draw from D that the classifier is wrong?",
                    "label": 0
                },
                {
                    "sent": "It's very natural is 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When we're trying to do machine learning, we're going to be given a bunch of examples.",
                    "label": 0
                },
                {
                    "sent": "Which are hopefully drawn from D and we can try to find some classifier C which has a small error rate.",
                    "label": 0
                },
                {
                    "sent": "Maybe now you start to see why I reduced the classification.",
                    "label": 0
                },
                {
                    "sent": "This is sort of I don't know.",
                    "label": 0
                },
                {
                    "sent": "It seems like it's the most simple learning problem you could.",
                    "label": 0
                },
                {
                    "sent": "You could have just predicting 1 bit.",
                    "label": 0
                },
                {
                    "sent": "And the tricky thing, of course, is that.",
                    "label": 0
                },
                {
                    "sent": "All we have are these examples.",
                    "label": 0
                },
                {
                    "sent": "We don't actually know what the distribution is.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of impossible in general without some kind of prior information, but maybe it's maybe we do have some prior information, so so maybe it turns out to be possible for the problems that we actually care about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All right importance weighted classification is the same as binary classification except for one small difference.",
                    "label": 0
                },
                {
                    "sent": "The distribution is over the same thing as before, but also there's this positive real number.",
                    "label": 0
                },
                {
                    "sent": "Is this number I'll call the importance.",
                    "label": 0
                },
                {
                    "sent": "So for example in the charity application, this would be so if you have zero return then it would be just the cost of the envelope.",
                    "label": 0
                },
                {
                    "sent": "And for particular example, an if you get $5 back and the importance of the $5 minus the cost of envelope.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We're going to have.",
                    "label": 0
                },
                {
                    "sent": "Some should be crossed there.",
                    "label": 0
                },
                {
                    "sent": "We're going to have an importance weighted data set, and now we're going to classifier.",
                    "label": 0
                },
                {
                    "sent": "C is a small importance weighted loss, so importance weighted loss is just going to be.",
                    "label": 0
                },
                {
                    "sent": "So I guess it's a basic fact of probabilities.",
                    "label": 0
                },
                {
                    "sent": "I could rewrite this probability is the expectation of the indicator function of whether not the classifier was wrong.",
                    "label": 0
                },
                {
                    "sent": "So indicated function is just one.",
                    "label": 0
                },
                {
                    "sent": "If that argument is true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is almost the same except now I have this importance value the little I.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to weight individual examples according to the importance value.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So there's a very basic theorem.",
                    "label": 0
                },
                {
                    "sent": "Is this theorem is so basic that nobody can claim it?",
                    "label": 0
                },
                {
                    "sent": "The basic theorem is that for any classifier.",
                    "label": 0
                },
                {
                    "sent": "And for any importance weight distribution.",
                    "label": 0
                },
                {
                    "sent": "There's this other distribution, deprime.",
                    "label": 0
                },
                {
                    "sent": "Which is the same as the old distribution, except now you kind of up wait by the importance.",
                    "label": 0
                },
                {
                    "sent": "And he divided by the expected importance.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that.",
                    "label": 0
                },
                {
                    "sent": "The binary error rate on D prime times expected importance equals the importance weighted error rate.",
                    "label": 0
                },
                {
                    "sent": "So this is is a very basic insight.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's extremely useful.",
                    "label": 0
                },
                {
                    "sent": "Because what this says is that if you find a good classifier C for D prime, then you're also finding a good classifier C. 4D.",
                    "label": 0
                },
                {
                    "sent": "The original importance weight problem.",
                    "label": 0
                },
                {
                    "sent": "The proof of this is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So you have the importance weighted loss of C on D. You can just write out what this means, so we have so important weight loss is an expectation expectation is a sum where you wait according to the probability of each element.",
                    "label": 0
                },
                {
                    "sent": "So this is the sum over all triples weighted according to the probability of the triple of the importance times without the classifier.",
                    "label": 0
                },
                {
                    "sent": "Got things wrong?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "It turns out you can pull the importance out.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Just have the prime.",
                    "label": 0
                },
                {
                    "sent": "'cause remember Deprime is just.",
                    "label": 0
                },
                {
                    "sent": "This corner here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is ID divided by the expected importance.",
                    "label": 0
                },
                {
                    "sent": "So it's D prime times expected importance.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "An expectation expected the prime of the indicated function classifier being wrong.",
                    "label": 0
                },
                {
                    "sent": "So that's just the probability.",
                    "label": 0
                },
                {
                    "sent": "It's just the definition of probability.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this is the.",
                    "label": 0
                },
                {
                    "sent": "Binary error rate.",
                    "label": 0
                },
                {
                    "sent": "And then just exactly the result, right?",
                    "label": 0
                },
                {
                    "sent": "If our reductions were this simple would be a very very very nice.",
                    "label": 0
                },
                {
                    "sent": "All this simple but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We hope they are.",
                    "label": 0
                },
                {
                    "sent": "OK, so this theorem gives some sort of basic insight into how one problem is related to another problem, and then the question is how do we take advantage of that algorithmically?",
                    "label": 0
                },
                {
                    "sent": "So there's actually what the theorem says that if you change the distribution from D to D prime and then you learn well on D prime just for 01 loss, you learn well, especially important weight loss in D. So it's all about how do we change D into D prime?",
                    "label": 0
                },
                {
                    "sent": "Or how do we change samples from Dino samples from the prime 'cause?",
                    "label": 0
                },
                {
                    "sent": "Remember, we only actually get samples.",
                    "label": 0
                },
                {
                    "sent": "So one approach people often use is resampling an I guess the way this works is.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of importance weights, one for each example.",
                    "label": 0
                },
                {
                    "sent": "And you kind of play rouler so you just spin this around and.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe it lands on this example when you take that example and you spin it again and you get another example, maybe get the same one, but you just take it twice in that case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Heard of this can have some problems in practice and the reason why I can have problems is because a lot of our learning algorithms are kind of made an assumption that the samples that they observe are drawn independently.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And this process breaks that.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if my original samples were independent, they were they happened to be.",
                    "label": 0
                },
                {
                    "sent": "Independent samples from D independent importance.",
                    "label": 0
                },
                {
                    "sent": "Weighted samples.",
                    "label": 0
                },
                {
                    "sent": "Then if I do resampling, it's no longer true.",
                    "label": 0
                },
                {
                    "sent": "I guess it's easy to see this because if you have imagine just having a probability distribution on continuous space, right?",
                    "label": 0
                },
                {
                    "sent": "So you never see the same sample twice, and if you do resampling on a finite sample set that you will observe the same sample twice.",
                    "label": 0
                },
                {
                    "sent": "'cause obviously something is going weird.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another trick.",
                    "label": 0
                },
                {
                    "sent": "Which turns to fit the requirements that we like.",
                    "label": 0
                },
                {
                    "sent": "It's just called rejection sampling and the idea here is we're going to pick some constant C which is larger than the importance weight.",
                    "label": 0
                },
                {
                    "sent": "And then for every sample, we're going to flip a coin with a bias of I oversee.",
                    "label": 0
                },
                {
                    "sent": "So if I is small then probably have heads is small.",
                    "label": 0
                },
                {
                    "sent": "If the result is heads, would keep it in.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we're going to throw it away.",
                    "label": 0
                },
                {
                    "sent": "This is some process and you can prove that if the original samples were independent, then after you run rejection sampling in your sample set, the remaining samples will also be independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Is something a little bit weird here.",
                    "label": 0
                },
                {
                    "sent": "Which is that we're kind of throwing away some of the information.",
                    "label": 0
                },
                {
                    "sent": "So maybe.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple way to.",
                    "label": 0
                },
                {
                    "sent": "Fix this.",
                    "label": 0
                },
                {
                    "sent": "So here's kind of the most naive algorithm that you might imagine.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is called costing.",
                    "label": 0
                },
                {
                    "sent": "It takes his input some importance, weighted data set, and some binary learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then it for maybe 10 iterations.",
                    "label": 0
                },
                {
                    "sent": "He do rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "From the importance weights to get a binary data set with no importance weights.",
                    "label": 0
                },
                {
                    "sent": "And then you learn a classifier on that binary data set.",
                    "label": 0
                },
                {
                    "sent": "When you take the majority vote.",
                    "label": 0
                },
                {
                    "sent": "Over each of these 10 different classifiers you learned.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just repeated rejection sampling and then take a majority vote and learn classifiers.",
                    "label": 0
                },
                {
                    "sent": "It turns out this album.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Them.",
                    "label": 0
                },
                {
                    "sent": "Works really great for this charity problem, so in 1998 the conference called KDD has some sort of championship.",
                    "label": 0
                },
                {
                    "sent": "What were the problem was exactly?",
                    "label": 0
                },
                {
                    "sent": "This is figuring out how to optimize, which you're going to envelopes to to get the most in terms of donations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Apply.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "This costing reduction.",
                    "label": 0
                },
                {
                    "sent": "And you can ask yourself, what is the total return that was received?",
                    "label": 0
                },
                {
                    "sent": "And so the total return is measured in terms of profit, which is essentially equivalent to expected importance.",
                    "label": 0
                },
                {
                    "sent": "And you can use different base classifiers, Naive Bayes, which Sam talked about.",
                    "label": 0
                },
                {
                    "sent": "This boosted naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll get there in a second.",
                    "label": 0
                },
                {
                    "sent": "This is a decision tree.",
                    "label": 0
                },
                {
                    "sent": "This is a support vector machine which we'll hear about later.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's actually.",
                    "label": 0
                },
                {
                    "sent": "Three things here is also something here and here and here and here that you can't see which has to do with.",
                    "label": 0
                },
                {
                    "sent": "Suppose you just ignore the importance weights and you tell your classifier to learn.",
                    "label": 0
                },
                {
                    "sent": "It turns out that all these classifiers tell you that.",
                    "label": 0
                },
                {
                    "sent": "It's great to predict that nobody will donate.",
                    "label": 0
                },
                {
                    "sent": "You have very high accuracy and then.",
                    "label": 0
                },
                {
                    "sent": "And then you don't Mail out any envelopes and then you get no return.",
                    "label": 0
                },
                {
                    "sent": "Write another thing that you can do is you can do that.",
                    "label": 0
                },
                {
                    "sent": "Resampling.",
                    "label": 0
                },
                {
                    "sent": "The first sampling process with green is he can do resampling and then you can apply your base classifier.",
                    "label": 0
                },
                {
                    "sent": "Or you can do rejection sampling and it seems that this rejection sampling works pretty well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately we came up with this after 1998.",
                    "label": 0
                },
                {
                    "sent": "Because I guess we would have won.",
                    "label": 0
                },
                {
                    "sent": "You should always come take these things with a grain of salt because you know after the fact that these championships that are running machine learning.",
                    "label": 0
                },
                {
                    "sent": "I think I actually great.",
                    "label": 0
                },
                {
                    "sent": "After the fact, we get a kind of pour over.",
                    "label": 0
                },
                {
                    "sent": "They said over and over again and try our best to overfit to it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is a different data set.",
                    "label": 0
                },
                {
                    "sent": "This is it's a similar task, it's another one of these.",
                    "label": 0
                },
                {
                    "sent": "Optimizing.",
                    "label": 0
                },
                {
                    "sent": "Mailing tasks.",
                    "label": 0
                },
                {
                    "sent": "And now it turns out that you can actually the simple approach where we ignore the importance weights actually does give you some return when you're using a naive Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And once again, the rejection sampling approach seems to work quite well.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so are there any questions about?",
                    "label": 0
                },
                {
                    "sent": "Importance weighted approach.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to do is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you how to reduce regression to classification.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit of a weird thing to do because there's lots of great regressions that already exist.",
                    "label": 0
                },
                {
                    "sent": "It's not clear this is.",
                    "label": 0
                },
                {
                    "sent": "An essential problem needs to be solved, but you know, maybe we're feeling mathematical and we just want to know.",
                    "label": 0
                },
                {
                    "sent": "If I have a good classifier, do I have a good regressor?",
                    "label": 0
                },
                {
                    "sent": "Oh, by the way, you could think about going the other way right?",
                    "label": 0
                },
                {
                    "sent": "You could reduce classification to regression.",
                    "label": 0
                },
                {
                    "sent": "And this would be pretty easy because you just use the regressor to estimate the probability that the label is 1 given X, and then you just threshold is the probability above .5 or below .5.",
                    "label": 0
                },
                {
                    "sent": "So give you some kind of reasonable estimate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other direction turned out to be a little more difficult, but it seems that there is actually.",
                    "label": 0
                },
                {
                    "sent": "A fairly natural simple algorithm for doing this.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, what do I mean by regression?",
                    "label": 0
                },
                {
                    "sent": "So you have to be kind of careful with this.",
                    "label": 0
                },
                {
                    "sent": "'cause people mean different things.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by regression is we have some feature space and we have some Y label which is now in the range from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So it could be point 3.5, one point 9, whatever.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to learn a regressor which is going to map.",
                    "label": 0
                },
                {
                    "sent": "The features to some continuous value in 01.",
                    "label": 0
                },
                {
                    "sent": "And the learning problem is going to be given.",
                    "label": 0
                },
                {
                    "sent": "Examples which are hopefully drawn from D but may not be final regressor H, which is a small squared error.",
                    "label": 0
                },
                {
                    "sent": "So squared error is actually a pretty interesting choice of error measure.",
                    "label": 0
                },
                {
                    "sent": "And the reason why it's interesting is that the minimizer of this is the mean.",
                    "label": 0
                },
                {
                    "sent": "So you want H of X to predict.",
                    "label": 0
                },
                {
                    "sent": "The probability that the expected value of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So we're going to optimize the squared error.",
                    "label": 0
                },
                {
                    "sent": "So now this is.",
                    "label": 0
                },
                {
                    "sent": "This is the first step in the reduction.",
                    "label": 0
                },
                {
                    "sent": "We've identified it some problem.",
                    "label": 0
                },
                {
                    "sent": "The question is can we reduce this to binary classification?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh examples where we care about regression.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to estimate the probability of.",
                    "label": 0
                },
                {
                    "sent": "Of cancer, right?",
                    "label": 0
                },
                {
                    "sent": "Given various measurements of a patient.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have some sort of big distributed system, so we have a bunch of sensors spread out all over the place and we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "Whether or not somebody pushed a button.",
                    "label": 0
                },
                {
                    "sent": "For example, so each other different sensors might get different bits of information, and we could try to have each individual sensor make a prediction, but it predicts just a single bit.",
                    "label": 0
                },
                {
                    "sent": "Somehow, combining a single bit is not not attractive because it doesn't have enough information to really.",
                    "label": 0
                },
                {
                    "sent": "Maybe all of our sensors individually think nobody pushed the button, but if each of them happens to think that you know would probably .49 somebody pushed the button.",
                    "label": 0
                },
                {
                    "sent": "If we knew all that information together.",
                    "label": 0
                },
                {
                    "sent": "If we could get all that information together, then we would.",
                    "label": 0
                },
                {
                    "sent": "Perhaps believe that somebody had pushed the button.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. We're going to start with a basic observation.",
                    "label": 0
                },
                {
                    "sent": "Let's pick some threshold interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Let's take.",
                    "label": 0
                },
                {
                    "sent": "A sample, so this is a regression sample, which means Y varies between zero and one, and let's form an importance weighted sample of the form.",
                    "label": 0
                },
                {
                    "sent": "Here my features is the Y value greater than the threshold or not?",
                    "label": 0
                },
                {
                    "sent": "So this is the binary label.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And then we have an importance weight, right?",
                    "label": 0
                },
                {
                    "sent": "And important weight is going to be, how far is Y from the threshold.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that if C is perfect.",
                    "label": 0
                },
                {
                    "sent": "Meaning it's going to give us the best possible prediction then that will mean.",
                    "label": 0
                },
                {
                    "sent": "That when C of X = 1, the expected value of Y given X is going to be greater than T. And you can adjust.",
                    "label": 0
                },
                {
                    "sent": "This is easy to workout, so if C of X is 1 and the classification is correct, it's doing the best possible.",
                    "label": 0
                },
                {
                    "sent": "Then that implies.",
                    "label": 0
                },
                {
                    "sent": "That OK, so we have two kinds of wise we have the wise which are greater than threshold and we have the wise which are less than the threshold.",
                    "label": 0
                },
                {
                    "sent": "So when Y is greater than the threshold.",
                    "label": 0
                },
                {
                    "sent": "The important weight is just going to be y -- T. And why is less than the threshold important so it will be T -- Y?",
                    "label": 1
                },
                {
                    "sent": "So if it's correct to say that the classification is 1 important classification is one, that means that the importance on.",
                    "label": 0
                },
                {
                    "sent": "The wise, which are above the thresholds.",
                    "label": 0
                },
                {
                    "sent": "The expected importance in the wise above threshold is greater than expected importance in The Witcher below the thresholds.",
                    "label": 0
                },
                {
                    "sent": "So you get this inequality.",
                    "label": 0
                },
                {
                    "sent": "And then we can just.",
                    "label": 0
                },
                {
                    "sent": "Subtract this from both sides and now T -- Y becomes y -- T here and now we just have the expected value, so we have indicated function of Y greater than T and it are function of why hold on?",
                    "label": 0
                },
                {
                    "sent": "That's a bug.",
                    "label": 0
                },
                {
                    "sent": "The inequality doesn't change science.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is indicator function of Y less than or equal to T. Indicator function of Y greater than T, which is everything and we multiply by the same thing in either case.",
                    "label": 0
                },
                {
                    "sent": "So this is just expected value of y -- T. And teams are constants, so you end up getting this equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Another simple observation, and it tells us if you think about it how to do this?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm which reduces squared error regression to binary classification.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "What it does is.",
                    "label": 0
                },
                {
                    "sent": "OK, so the name of T changed the P. What it does is it start with your regression data set.",
                    "label": 0
                },
                {
                    "sent": "You choose some values of T. Some thresholds?",
                    "label": 0
                },
                {
                    "sent": "And then you form importance weighted datasets.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to apply an importance weighted learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You're going to learn a bunch of different binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "And then at Test time you're going to have some features you're going to ask each classifier how it predicts given the features.",
                    "label": 0
                },
                {
                    "sent": "Maybe the predictions will look like this, maybe you know it's almost never the case that Y is greater than .99, so it's zero and turns out that.",
                    "label": 0
                },
                {
                    "sent": "Even why is typically less than .5, which means the Sky predicts 0, but it is greater than .1 otherwise, typically larger .1.",
                    "label": 0
                },
                {
                    "sent": "So this guy predicts one in this guy predicts one.",
                    "label": 0
                },
                {
                    "sent": "And then the way we're going to form a regression estimate.",
                    "label": 0
                },
                {
                    "sent": "Is by just looking.",
                    "label": 0
                },
                {
                    "sent": "For where this threshold is, where do we switch from?",
                    "label": 0
                },
                {
                    "sent": "Predicting ones to predicting zeros.",
                    "label": 0
                },
                {
                    "sent": "And then maybe there's some discretization in the system, so maybe we predict the value which is in between these two, which would be .3.",
                    "label": 0
                },
                {
                    "sent": "Now is the question in at least 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "Somebody ask a question.",
                    "label": 0
                },
                {
                    "sent": "What happened to probability?",
                    "label": 0
                },
                {
                    "sent": "Good good, there's a few problems with this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's some details here.",
                    "label": 0
                },
                {
                    "sent": "I say there is reducing this to binary classification.",
                    "label": 0
                },
                {
                    "sent": "And actually just reduced to importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "But I told you how to reduce importance weighted to binary, so you can just apply that.",
                    "label": 0
                },
                {
                    "sent": "Previous reduction and here's your answer, Sam.",
                    "label": 0
                },
                {
                    "sent": "If it turns out.",
                    "label": 0
                },
                {
                    "sent": "And this could happen very easily.",
                    "label": 0
                },
                {
                    "sent": "It could be that you get 101000.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you just sort.",
                    "label": 0
                },
                {
                    "sent": "So then you get all ones and then all zeros.",
                    "label": 0
                },
                {
                    "sent": "That sounds like a hack, but it turns out that that's exactly what the theory tells you to do.",
                    "label": 0
                },
                {
                    "sent": "And then I guess there's also an issue in sort of how you discretize Auntie.",
                    "label": 0
                },
                {
                    "sent": "We tried using sort of a uniform grid and it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "It works a little bit better if you kind of discretize on demand.",
                    "label": 0
                },
                {
                    "sent": "So maybe most of your Y values are up near .9 and then you want to put your discretization in that region.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so here's some algorithm I told you there was some basic observation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what can we?",
                    "label": 0
                },
                {
                    "sent": "So first question is, how well does this algorithm work?",
                    "label": 0
                },
                {
                    "sent": "And I told you this is a little bit of a weird question to ask.",
                    "label": 0
                },
                {
                    "sent": "Can we reduce regression to classification 'cause there are a lot of good regressors around?",
                    "label": 0
                },
                {
                    "sent": "And some of these.",
                    "label": 0
                },
                {
                    "sent": "So what this is doing is comparing.",
                    "label": 0
                },
                {
                    "sent": "Various algorithms.",
                    "label": 0
                },
                {
                    "sent": "To various algorithms under the program reduction so.",
                    "label": 0
                },
                {
                    "sent": "This is naive Bayes with approving reduction.",
                    "label": 0
                },
                {
                    "sent": "This is a decision tree with production.",
                    "label": 0
                },
                {
                    "sent": "This is support vector machine with production.",
                    "label": 0
                },
                {
                    "sent": "It's comparing these various algorithms.",
                    "label": 0
                },
                {
                    "sent": "Two, what happens when you use other approaches people use for doing regression?",
                    "label": 0
                },
                {
                    "sent": "And in particular.",
                    "label": 0
                },
                {
                    "sent": "The kind of problems these are.",
                    "label": 0
                },
                {
                    "sent": "R. Is there really classification problems?",
                    "label": 0
                },
                {
                    "sent": "So when you're doing regression with classification, you're really trying to estimate the probability that y = 1 given X. OK, so some other approaches people use is they just try to apply a naive Bayes classifier straight it.",
                    "label": 0
                },
                {
                    "sent": "Estimate probabilities.",
                    "label": 0
                },
                {
                    "sent": "Some people try to use a naive Bayes classifier and then instead of reporting the actual probability of the Y = 1 given X, The Naive Bayes tells you that they try to fit some sort of function to that output.",
                    "label": 0
                },
                {
                    "sent": "Typically a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people try to use a decision tree straight.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I don't think anybody is talking about Decision Tree here, but the way Decision Tree works is very simple.",
                    "label": 0
                },
                {
                    "sent": "What happens is.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "A bunch of features and 1st you have some test is the is the third feature above or below .2 right?",
                    "label": 0
                },
                {
                    "sent": "And this puts your data and then you have another test is the 2nd feature above or below .7?",
                    "label": 0
                },
                {
                    "sent": "Let's put your data again and again and again.",
                    "label": 0
                },
                {
                    "sent": "You keep on splitting your data as you walk down some sort of tree until you get down to the leaves, and then in the leaves you have some samples remaining and you just ask yourself what fraction of the samples predict one way versus another.",
                    "label": 0
                },
                {
                    "sent": "So this is some way to get a probabilistic estimate and the way you label it, you just.",
                    "label": 0
                },
                {
                    "sent": "A particular example, SMB, you walk down the tree according to whether or not your first feature is above what was at .1.",
                    "label": 0
                },
                {
                    "sent": "I don't think it was right.",
                    "label": 0
                },
                {
                    "sent": "So you just walk down the tree to get to a leaf, and then you predict whatever proportion of the labels were one in that leaf.",
                    "label": 0
                },
                {
                    "sent": "OK, so another approach people use is decision tree with bagging.",
                    "label": 0
                },
                {
                    "sent": "How to describe begging.",
                    "label": 0
                },
                {
                    "sent": "Begging is another very simple approach.",
                    "label": 0
                },
                {
                    "sent": "It's like.",
                    "label": 0
                },
                {
                    "sent": "It's like costing.",
                    "label": 0
                },
                {
                    "sent": "Except importance weights are all one.",
                    "label": 0
                },
                {
                    "sent": "And C is.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Now it's more like I guess it's like costing except.",
                    "label": 0
                },
                {
                    "sent": "Instead of doing rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "You have importance weights all one and you use resampling rather rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "That's the simplest description.",
                    "label": 0
                },
                {
                    "sent": "And then other people uses the.",
                    "label": 0
                },
                {
                    "sent": "How to apply a support vector machine?",
                    "label": 0
                },
                {
                    "sent": "This gives them a pretty good classifier and then to get a good prediction a good probabilistic prediction if it a signal into the margin which is some internal value can extract from a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So I guess the way to read this is this.",
                    "label": 0
                },
                {
                    "sent": "We're talking bout squared error, which means that smaller is better.",
                    "label": 0
                },
                {
                    "sent": "And for each of these kinds of classifiers, there's several approaches and the ordered in the same way they are up here, right?",
                    "label": 0
                },
                {
                    "sent": "So right there.",
                    "label": 0
                },
                {
                    "sent": "Is naive Bayes with under the program reduction?",
                    "label": 0
                },
                {
                    "sent": "And again.",
                    "label": 0
                },
                {
                    "sent": "That's naive Bayes into the program reduction.",
                    "label": 0
                },
                {
                    "sent": "This would be naive Bayes under SVM and this would be naive Bayes under.",
                    "label": 0
                },
                {
                    "sent": "Sorry this would be a support vector machine or the production and this would be a decision tree into production.",
                    "label": 0
                },
                {
                    "sent": "And if you take a look, you see this is working reasonably.",
                    "label": 0
                },
                {
                    "sent": "I think that's about all you can conclude if you take a look at the exact numbers, it seems like it works a little bit better than other approaches, but maybe not dramatically better.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "I like to try to do some theory.",
                    "label": 0
                },
                {
                    "sent": "Question is what can we prove?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not this algorithm.",
                    "label": 0
                },
                {
                    "sent": "We want to actually prove that this algorithm is a valid reduction from regression to classification.",
                    "label": 0
                },
                {
                    "sent": "So there's a trick which is very helpful in thinking about the proof.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of different classifiers and we could try to prove something with respect to kind of the collection of classifiers that we actually made.",
                    "label": 0
                },
                {
                    "sent": "But maybe there's some way to just think about having one classifier.",
                    "label": 0
                },
                {
                    "sent": "It turns out there's a trick which let's do this what we're going to do is we're going to think about.",
                    "label": 0
                },
                {
                    "sent": "Adding the threshold T into.",
                    "label": 0
                },
                {
                    "sent": "The feature space.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to think about.",
                    "label": 0
                },
                {
                    "sent": "So we have a importance weighted.",
                    "label": 0
                },
                {
                    "sent": "Data set for each threshold team going to think about taking a union, or maybe just a stochastic choice from each of these.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to learn one classifier.",
                    "label": 0
                },
                {
                    "sent": "Respect to this one sample set.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to define CSUB T of X = C of X comity.",
                    "label": 0
                },
                {
                    "sent": "So this is a trick we since we have a bunch of classifiers and we're learning them and calling them in parallel.",
                    "label": 0
                },
                {
                    "sent": "We can just put the name of the classifier into the feature space.",
                    "label": 0
                },
                {
                    "sent": "And then we can ask yourselves, what does small loss spec to this classifier?",
                    "label": 0
                },
                {
                    "sent": "This one imply.",
                    "label": 0
                },
                {
                    "sent": "Just a little bit of a cheat, because the algorithm that I told you in the other thing to analyze are slightly different, right?",
                    "label": 0
                },
                {
                    "sent": "And whether this is reasonable to do in practice, we don't actually know, because.",
                    "label": 0
                },
                {
                    "sent": "We didn't do the experiments.",
                    "label": 0
                },
                {
                    "sent": "But it is extremely helpful.",
                    "label": 0
                },
                {
                    "sent": "Theoretically.",
                    "label": 0
                },
                {
                    "sent": "It makes the theorems much cleaner.",
                    "label": 0
                },
                {
                    "sent": "You can prove about the same thing if you don't do this trick.",
                    "label": 0
                },
                {
                    "sent": "But it's just it's much nicer if you do.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We're going to prove a theorem, so the theorem says for every.",
                    "label": 0
                },
                {
                    "sent": "Classifier so this 01.",
                    "label": 0
                },
                {
                    "sent": "Here is the threshold right?",
                    "label": 0
                },
                {
                    "sent": "So we have this one classifier trick for every classifier this form.",
                    "label": 0
                },
                {
                    "sent": "And for every distribution.",
                    "label": 0
                },
                {
                    "sent": "Send the typo.",
                    "label": 0
                },
                {
                    "sent": "This should be a hard brackets because we're going to have the interval from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "For every distribution, if every regression problem.",
                    "label": 0
                },
                {
                    "sent": "Squared error of our estimate is going to be bounded by.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have.",
                    "label": 0
                },
                {
                    "sent": "A distribution.",
                    "label": 0
                },
                {
                    "sent": "D. Which is going to.",
                    "label": 0
                },
                {
                    "sent": "Produce samples that we're going to apply this.",
                    "label": 0
                },
                {
                    "sent": "One classifier trick 2, which is going to.",
                    "label": 0
                },
                {
                    "sent": "Produce a distribution over importance weighted samples.",
                    "label": 0
                },
                {
                    "sent": "Right and then via the.",
                    "label": 0
                },
                {
                    "sent": "Costing reduction on just binary samples, so this is kind of a tricky thing, so we don't know what distribution D is.",
                    "label": 0
                },
                {
                    "sent": "But the distribution D is implicitly inducing a distribution on just binary classification.",
                    "label": 0
                },
                {
                    "sent": "And the way that you get at this this induce distribution.",
                    "label": 0
                },
                {
                    "sent": "The way that you draw from this news distribution is you first draw from the original distribution D. And then you pick a random threshold T. Uniformly from 01.",
                    "label": 0
                },
                {
                    "sent": "Then you just create the importance weighted sample.",
                    "label": 0
                },
                {
                    "sent": "Then you transform the importance weighted sample into a binary sample using the cost reduction.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So claim is the error rate of the classifier on this induced distribution.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "Bounds of squared error.",
                    "label": 0
                },
                {
                    "sent": "So if I just told you this theorem, which is true?",
                    "label": 0
                },
                {
                    "sent": "Then it wouldn't actually be very interesting, and the reason why it wouldn't be very interesting is because we're trying to apply.",
                    "label": 0
                },
                {
                    "sent": "Regression in kind of inherently noisy situations.",
                    "label": 0
                },
                {
                    "sent": "So we need a theorem which is actually going to imply something when we have a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "Just saying I have one error rate bounds, another error rate isn't very convincing because this error.",
                    "label": 0
                },
                {
                    "sent": "This error rate will be pretty large when you have a very noisy D. To what you do is you subtract off the smallest possible error rates.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people call this the Bayes error rate she ask yourself.",
                    "label": 0
                },
                {
                    "sent": "Suppose it chose the best possible classifier that would have some binary error rate.",
                    "label": 0
                },
                {
                    "sent": "And turns out that you can subtract this off.",
                    "label": 0
                },
                {
                    "sent": "So that's this difference.",
                    "label": 0
                },
                {
                    "sent": "Which bounds the squared error.",
                    "label": 0
                },
                {
                    "sent": "The difference is a regret.",
                    "label": 0
                },
                {
                    "sent": "Typically the way we think about the way we define regrets is how well did we do in comparison to how well we could have done.",
                    "label": 0
                },
                {
                    "sent": "The claim is that the binary regret.",
                    "label": 0
                },
                {
                    "sent": "Bounds the squared error regret.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Nice theorem.",
                    "label": 0
                },
                {
                    "sent": "How do you prove it?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I try to prove it mathematically, let me tell you why you should believe this is true.",
                    "label": 0
                },
                {
                    "sent": "So let's think about an extreme case where it happens that we're just trying to estimate the.",
                    "label": 0
                },
                {
                    "sent": "The probability that y = 1 given X.",
                    "label": 0
                },
                {
                    "sent": "So if we're trying to estimate the probability that y = 1 given X, let's just think about some situations.",
                    "label": 0
                },
                {
                    "sent": "One situation is where the probability that y = 1 given X is 1, right?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "As we vary the threshold.",
                    "label": 0
                },
                {
                    "sent": "The importance weighted loss will look like this line.",
                    "label": 1
                },
                {
                    "sent": "Right, so the when the threshold is one that.",
                    "label": 0
                },
                {
                    "sent": "And the probability equals one given X is 1, then the importance will be 0 because we importance weight is just.",
                    "label": 0
                },
                {
                    "sent": "Magnitude of y -- T if Y and Tier 1 then important way to 0.",
                    "label": 0
                },
                {
                    "sent": "Another situation is where the problem with y = 1 given X is 0, so we're always going to be.",
                    "label": 0
                },
                {
                    "sent": "Why wow is always 0 and now the importance weight is going to look like this.",
                    "label": 0
                },
                {
                    "sent": "And then the question is what happens in between?",
                    "label": 0
                },
                {
                    "sent": "Suppose that the probability that y = 1 given access point 5.",
                    "label": 0
                },
                {
                    "sent": "What happens so?",
                    "label": 0
                },
                {
                    "sent": "What happens what you should expect to happen is a convex combination of this and this because you have some probability that Y is 1.",
                    "label": 0
                },
                {
                    "sent": "And probably some probably that why is 0.",
                    "label": 0
                },
                {
                    "sent": "Now it's not quite that, and the reason why it's not quite that.",
                    "label": 0
                },
                {
                    "sent": "It's because.",
                    "label": 0
                },
                {
                    "sent": "We're trying to prove a strong theorem.",
                    "label": 0
                },
                {
                    "sent": "The strong theorem subtracts off the minimum possible.",
                    "label": 0
                },
                {
                    "sent": "Error rate.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Instead of having the green line be like so it's like so because we subtract off the minimum, which means that it always touches 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So these are some cases and now what we want to do is we want to think about.",
                    "label": 0
                },
                {
                    "sent": "We don't think about an adversary who's going to choose how to make our classifiers screw up.",
                    "label": 0
                },
                {
                    "sent": "And we're going to place a constraint on our adversary.",
                    "label": 0
                },
                {
                    "sent": "The constraint is that the classifier cannot take too much importance weighted loss, or you cannot cause too much importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what is the most efficient way for the adversary to disturb our prediction?",
                    "label": 0
                },
                {
                    "sent": "Given his budget on the amount of importance weight loss he can incur.",
                    "label": 0
                },
                {
                    "sent": "So every classifier has some importance importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "So suppose that were just in this case here, right?",
                    "label": 0
                },
                {
                    "sent": "So this classifier at this threshold will have this much importance weight loss.",
                    "label": 0
                },
                {
                    "sent": "The server is winning predicts wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's two observations which make this kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "The first observation is it would be very dumb for the adversary to scroop.",
                    "label": 0
                },
                {
                    "sent": "On this side of the truth, and on this side of the truth.",
                    "label": 0
                },
                {
                    "sent": "And the reason why is very dumb is because we had that sorting operation right.",
                    "label": 0
                },
                {
                    "sent": "So if we screw up here then we're going to be turning 1 into a 0 interest group here going to 0 until one.",
                    "label": 0
                },
                {
                    "sent": "After we sort these cancel each other out.",
                    "label": 0
                },
                {
                    "sent": "So it's as if the adversary achieved nothing.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is that all of the mistakes are going to be on one side of the truth.",
                    "label": 0
                },
                {
                    "sent": "In the worst case.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "A second observation is it wouldn't make very much sense for the adversary to mess up here and to not mess up here.",
                    "label": 0
                },
                {
                    "sent": "Because after we sort, these two are equivalent and importance weights the adversary pays.",
                    "label": 0
                },
                {
                    "sent": "Is smaller here than it is over here.",
                    "label": 0
                },
                {
                    "sent": "So it says that.",
                    "label": 1
                },
                {
                    "sent": "The most efficient way for the adversary with the budget to to disturb our estimate.",
                    "label": 0
                },
                {
                    "sent": "It is just choose one side of the truth and just start airing over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "Until he runs out of budget.",
                    "label": 0
                },
                {
                    "sent": "And then the probing algorithm will report this.",
                    "label": 0
                },
                {
                    "sent": "The truth will be here.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This is just something linear, so if we integrate that, we're going to get something squared.",
                    "label": 0
                },
                {
                    "sent": "So the amount of area here is the amount of budget the adversary has.",
                    "label": 0
                },
                {
                    "sent": "And it's going to induce some particular squared loss.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "They've been marginally clear.",
                    "label": 0
                },
                {
                    "sent": "We also have the proof in math.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "This is a good page to ask questions about.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go back to this page.",
                    "label": 0
                },
                {
                    "sent": "If you take a look at the importance weight.",
                    "label": 0
                },
                {
                    "sent": "Here, the integration of the importance weight is just going to be half because it's it's half of this.",
                    "label": 0
                },
                {
                    "sent": "Unit wrecked",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to square OK, so the expected importance is going to be less than equal to half.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is just going to start writing down what the various definitions mean, so we have this term which is a regret with how well we did compared to how we could have done.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that.",
                    "label": 0
                },
                {
                    "sent": "This equals this, so we're going to write down.",
                    "label": 0
                },
                {
                    "sent": "The importance weighted loss, minus the minimum possible importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to multiply by two, and the reason we're going to buy two.",
                    "label": 0
                },
                {
                    "sent": "It's because you know the expected importance is less than 1/2, and then you can go back to.",
                    "label": 0
                },
                {
                    "sent": "Costing theorem, I won't do it now and there was this relationship where the binary loss.",
                    "label": 0
                },
                {
                    "sent": "Times expected importance equal importance weighted loss.",
                    "label": 0
                },
                {
                    "sent": "One over half is 2.",
                    "label": 0
                },
                {
                    "sent": "OK. What's happening here?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do?",
                    "label": 0
                },
                {
                    "sent": "He's going to change around the expectation.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the way that you prove these reductions typically is all the action is kind of happening in the loss function an in the particular labels.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't really matter what your features are, so you can just condition on your features.",
                    "label": 0
                },
                {
                    "sent": "So we're going to pull the expectation over X&T out of.",
                    "label": 0
                },
                {
                    "sent": "The the full expectation, so just kind of reordering how we take the expectation.",
                    "label": 0
                },
                {
                    "sent": "I feel like I'm going a little bit too fast.",
                    "label": 0
                },
                {
                    "sent": "Think about what this means here.",
                    "label": 0
                },
                {
                    "sent": "So this classifier with the smallest error rate.",
                    "label": 0
                },
                {
                    "sent": "Is going to be the one which.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "If we predict.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Then whenever the truth is zero, we're going to pay.",
                    "label": 0
                },
                {
                    "sent": "This kind of importance weighted loss right?",
                    "label": 0
                },
                {
                    "sent": "So this is just if the truth is 0, if Y is less than the threshold, we're going to pay T -- Y.",
                    "label": 0
                },
                {
                    "sent": "Whenever we predict zero, if the truth is 1 if.",
                    "label": 0
                },
                {
                    "sent": "Why is greater than T?",
                    "label": 0
                },
                {
                    "sent": "The amount of loss that we're going to suffer just y -- T?",
                    "label": 0
                },
                {
                    "sent": "Right and the best possible classifiers should pay the minimum of these two possible losses.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh or or conditioning on action T there's two possibilities, either the classifier C is going to predict zero is going to predict one and it's going to be right or so either going to be wrong in this prediction.",
                    "label": 0
                },
                {
                    "sent": "So if it predicts perfectly, then the difference between the loss of the best classifier and the loss of our classifier is going to be 0 for this particular choice of X&T.",
                    "label": 0
                },
                {
                    "sent": "If it predicts incorrectly, and that's not true, and in particular the difference will be this difference here.",
                    "label": 0
                },
                {
                    "sent": "Which you should maybe be remembering from that observation we made earlier.",
                    "label": 0
                },
                {
                    "sent": "Metro.",
                    "label": 0
                },
                {
                    "sent": "When C is wrong.",
                    "label": 0
                },
                {
                    "sent": "The difference between this and this is going to be this minus that magnitude.",
                    "label": 0
                },
                {
                    "sent": "So we have a - and we have a - so we can just reorder things where we have a + T -- Y.",
                    "label": 0
                },
                {
                    "sent": "And then we have.",
                    "label": 0
                },
                {
                    "sent": "While it's singles T&Y greater than T. Which means this is just T minus expected value of Y given X.",
                    "label": 0
                },
                {
                    "sent": "And then, if you recall, we had that 2 running around, so says the cost.",
                    "label": 0
                },
                {
                    "sent": "Of the misprediction is T times expected value of Y given X. OK, so all that I've done right here is essentially.",
                    "label": 0
                },
                {
                    "sent": "What was obvious from this graph all that I've done is I've told you that the importance weight, the amount of loss suffered for screwing up is something kind of linear in how far you are from the threshold.",
                    "label": 0
                },
                {
                    "sent": "Right ptosis.",
                    "label": 0
                },
                {
                    "sent": "If you screw up for this particular.",
                    "label": 0
                },
                {
                    "sent": "T. You look at how far the truth is from the threshold.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then now we're just going to make the observations that I made earlier.",
                    "label": 0
                },
                {
                    "sent": "Little bit more mathematical way.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Actually the same observations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A good adversary is only going to screw up on one side of the truth because the sorting cancels out things.",
                    "label": 0
                },
                {
                    "sent": "As in the screw up is close to the truth as possible because the loss is linear in the distance of the threshold from the truth.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We have the truth.",
                    "label": 0
                },
                {
                    "sent": "We're going to have the adversary induce a loss of Delta.",
                    "label": 0
                },
                {
                    "sent": "And we're going to integrate this.",
                    "label": 0
                },
                {
                    "sent": "From the truth to the truth plus Delta.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then this integral turns out to be Delta squared.",
                    "label": 0
                },
                {
                    "sent": "And that's essentially the proof.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Are there questions about this theorem?",
                    "label": 0
                },
                {
                    "sent": "Works proof.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a simple modification of this.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the probing algorithm and you modify it very slightly and you can do quantile regression.",
                    "label": 0
                },
                {
                    "sent": "So when people are trying to do regression, the most common thing they do is squared error regression like we talked about first.",
                    "label": 0
                },
                {
                    "sent": "Another thing which is maybe very useful.",
                    "label": 0
                },
                {
                    "sent": "In which.",
                    "label": 0
                },
                {
                    "sent": "People don't necessarily.",
                    "label": 0
                },
                {
                    "sent": "Often times people use cordair regression when they really want to use quantile regression instead.",
                    "label": 0
                },
                {
                    "sent": "So what does quantile regression quantile regression is?",
                    "label": 0
                },
                {
                    "sent": "So it's going to be parameterized with some Q and 01.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't hurt to predict some value.",
                    "label": 0
                },
                {
                    "sent": "White hats is the probability wise good in white hat equals Q.",
                    "label": 0
                },
                {
                    "sent": "So if you take this probing algorithm and you just modify it slightly, it turns out that you can get quantile regression.",
                    "label": 0
                },
                {
                    "sent": "The way you modify it is in terms of the importance weights.",
                    "label": 0
                },
                {
                    "sent": "So now instead of having remember before we had the magnitude of y -- T, Now we just have Q when Y is greater than T and 1 -- Q when Y is less than T. And then you can go through and you can prove that this.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Quantile.",
                    "label": 0
                },
                {
                    "sent": "Loss.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by the binary.",
                    "label": 0
                },
                {
                    "sent": "The quantile regret is bounded by the binary regret.",
                    "label": 0
                },
                {
                    "sent": "The proof is essentially the same.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Quantile regression is a little bit less used, and that means that people haven't had so much time to develop a lot of tools to solve it very well so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perhaps it's unsurprising, then, that when you go and you apply this quantile regression reduction.",
                    "label": 0
                },
                {
                    "sent": "Two various classifiers.",
                    "label": 0
                },
                {
                    "sent": "It works pretty well.",
                    "label": 0
                },
                {
                    "sent": "So for four different datasets and three different choices of quantile.",
                    "label": 0
                },
                {
                    "sent": "We computed.",
                    "label": 0
                },
                {
                    "sent": "Well, we applied this reduction too.",
                    "label": 0
                },
                {
                    "sent": "I think the base classifiers were logistic regression and one was Decision Tree.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is this number here?",
                    "label": 0
                },
                {
                    "sent": "This number is take the worst guy out of these four possibilities.",
                    "label": 0
                },
                {
                    "sent": "So this is linear quantile regression.",
                    "label": 0
                },
                {
                    "sent": "This is kernelized quantile regression.",
                    "label": 0
                },
                {
                    "sent": "And then this quantity algorithm, applied logic, aggression, quantified decision tree.",
                    "label": 0
                },
                {
                    "sent": "You take the worst of these four.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Divide the performance in terms of the absolute of the quantile loss by the worst, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a normalized performance.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that using quantum with the decision tree we got maybe 60% of the loss that we got using linear regression, yeah?",
                    "label": 0
                },
                {
                    "sent": "Define SD quantile regression loss.",
                    "label": 0
                },
                {
                    "sent": "Think I was hoping that you wouldn't ask that question because it turns out that I screwed up this theorem statement.",
                    "label": 0
                },
                {
                    "sent": "This is actually what you get when you're trying to do median.",
                    "label": 0
                },
                {
                    "sent": "If you for general quantile loss, what you do?",
                    "label": 0
                },
                {
                    "sent": "But what is going to look like is essentially.",
                    "label": 0
                },
                {
                    "sent": "This times the magnitude of y -- T and 1 -- Q times the magnitude of 1 -- T. Yes.",
                    "label": 0
                },
                {
                    "sent": "This theorem does hold for the pinball loss Virginia like you just.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Some experiments seem to work pretty well.",
                    "label": 0
                },
                {
                    "sent": "A significant caveat for the kernel stuff which Alex probably interested in so.",
                    "label": 0
                },
                {
                    "sent": "Their computational problems here, and the reason why there are compilation problems is because these are pretty big datasets.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When you go and apply a reduction, the actual amount of computation which is going on in the reduction is relatively small.",
                    "label": 0
                },
                {
                    "sent": "Most of the computation goes on inside of the learning algorithm that you actually are going to use right?",
                    "label": 0
                },
                {
                    "sent": "And turns out these learning algorithms are reasonably well optimized.",
                    "label": 0
                },
                {
                    "sent": "And they will often allies enough that they can just handle these large datasets.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So I guess this algorithm here is very fast.",
                    "label": 0
                },
                {
                    "sent": "It could handle large data set and this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit unfair for us to be comparing to it because we couldn't get it to run with very large data set.",
                    "label": 0
                },
                {
                    "sent": "I think we could only feed it about 3000 samples or so so.",
                    "label": 0
                },
                {
                    "sent": "This is your copy out here when you look at the green X you should be understanding that you know there's like 50,000 samples here and it's only seeing 3000 of them.",
                    "label": 0
                },
                {
                    "sent": "So I think for the sake of fairness, I should that we didn't know if you wanted to keep you so.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could use like anybody cares.",
                    "label": 0
                },
                {
                    "sent": "Code mean.",
                    "label": 0
                },
                {
                    "sent": "With that you could easily set it up.",
                    "label": 0
                },
                {
                    "sent": "I agree so.",
                    "label": 0
                },
                {
                    "sent": "Writing good, fast code takes some effort.",
                    "label": 0
                },
                {
                    "sent": "We know how to do it in specific cases, and I guess it's just.",
                    "label": 0
                },
                {
                    "sent": "Just if you're practically minded, the question is.",
                    "label": 0
                },
                {
                    "sent": "Do you want to?",
                    "label": 0
                },
                {
                    "sent": "Do you want to be?",
                    "label": 0
                },
                {
                    "sent": "Writing good fast code over and over again, or you want to be trying to reuse it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Maybe initially try to reuse it and then over the long term you try to write good, fast code.",
                    "label": 0
                },
                {
                    "sent": "That's a good solution.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I think I'm about done.",
                    "label": 0
                },
                {
                    "sent": "Couple of caveats which you should understand so some people worry bout log loss like Sam was talking about log loss and it's very natural to ask yourselves.",
                    "label": 0
                },
                {
                    "sent": "What does this imply about log loss and turns out that implies nothing interesting.",
                    "label": 0
                },
                {
                    "sent": "And the difficulty is that log loss can be infinite.",
                    "label": 0
                },
                {
                    "sent": "And we don't know how to reduce a loss that could be infinite sum of the time to a loss that is bounded.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can't do it if you allow the predictor that you induce to choose any particular value, right?",
                    "label": 0
                },
                {
                    "sent": "'cause then some very small fraction of the time the adversary could just choose to cause it to totally screw up and suffer infinite loss.",
                    "label": 0
                },
                {
                    "sent": "Then the expectation of some finite numbers in some infinite numbers is infinite number, and that's just pretty unfortunate for the mathematics.",
                    "label": 0
                },
                {
                    "sent": "Another thing that people often try to do is they tried to use regression to estimates like, say, the relevance of web pages.",
                    "label": 0
                },
                {
                    "sent": "Something like that and.",
                    "label": 0
                },
                {
                    "sent": "And then maybe order web pages.",
                    "label": 0
                },
                {
                    "sent": "According to this relevance value.",
                    "label": 0
                },
                {
                    "sent": "And you can't prove.",
                    "label": 0
                },
                {
                    "sent": "At least for several different natural ranking losses, you can't prove anything interesting, and the fundamental reason is.",
                    "label": 0
                },
                {
                    "sent": "Let's say that you have one relevant web page and you have 100,000 year relevant web pages.",
                    "label": 0
                },
                {
                    "sent": "This is fairly typical case.",
                    "label": 0
                },
                {
                    "sent": "Then, an adversary who's smart is just going to screw up.",
                    "label": 0
                },
                {
                    "sent": "The estimate of the relevance there.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Web page right now is going to go down to rank number.",
                    "label": 0
                },
                {
                    "sent": "100,000 and one.",
                    "label": 0
                },
                {
                    "sent": "And you just get garbage.",
                    "label": 0
                },
                {
                    "sent": "So the total amount of time that the adversary messes up is just one out of 100,000.",
                    "label": 0
                },
                {
                    "sent": "But the result that you get is not useful.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think.",
                    "label": 0
                },
                {
                    "sent": "Now the time we want to take a break, so I'll be hanging around.",
                    "label": 0
                },
                {
                    "sent": "And if you have questions in the break then should feel free to come up and talk to me.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}