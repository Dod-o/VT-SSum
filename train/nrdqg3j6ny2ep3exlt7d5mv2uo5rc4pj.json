{
    "id": "nrdqg3j6ny2ep3exlt7d5mv2uo5rc4pj",
    "title": "Autonomous Exploration in Reinforcement Learning",
    "info": {
        "author": [
            "Peter Auer, Chair for Information Technology, Montanuniversit\u00e4t Leoben"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_auer_learning/",
    "segmentation": [
        [
            "OK, so thank you for having me.",
            "This is trying to work with share how she hung limb and Chris Watkins and it's done.",
            "It's done within the complex project which is funded by the European Commission.",
            "So why I'm interested in autonomous autonomous X?"
        ],
        [
            "Operation.",
            "It appears that infants, animals explore their environment without apparent purpose, and they do it in a way which is appropriate to their current skill.",
            "The current level of skill, and they increase their skills overtime.",
            "By doing this exploration, and I want to show you a video which you like and which is quite nice.",
            "Which.",
            "It shows an infant.",
            "Playing by itself for no apparent reason, and apparently it learns something by doing that and the advantages, it doesn't need any supervision.",
            "Any feedback to be engaged and and learn?",
            "And this is something we would like also in learning systems so that they can learn without supervision and improve their skills and get better and learn more complicated skills.",
            "Maybe?",
            "Entropy in the room, yes.",
            "And.",
            "So this is the channel problem I'm interested in.",
            "To do any, any any analysis or analysis of algorithms or any reasoning, mathematical reasoning about autonomous exploration is difficult because."
        ],
        [
            "It's hard to quantify the problem is we have no specific task we want to solve, so we cannot easily measure how well autonomous exploration is working, and indeed there is, as far as I know, no agreed criterion for.",
            "What how to measure autonomous exploration?",
            "There's some some."
        ],
        [
            "Proposals in the literature, one by Schmidhuber, who argues that essentially what you want to do by autonomous exploration is that you want to basically choose actions which decrease your uncertainty.",
            "So, and this basically leads to using information gain as an intrinsic reward in if you want to think of reinforcement light setup.",
            "Problem is that information gain is how to measure.",
            "If you don't have a probabilistic model.",
            "So what he?",
            "Das is that he recites to use Kolmogorov type complexity stuff or algorithmic information theory to measure this.",
            "Then it becomes measurable.",
            "On the other hand, it does not come, it becomes uncomputable, so it's hard to use it in an algorithm.",
            "So finally it needs he needs to use some some approximation to this algorithmic information theoretic measures.",
            "So this was."
        ],
        [
            "Picked up by or the end and coworkers and.",
            "They use this to actually build a robot who learns in an environment and they are interested in how the robot can develop complex behavior, and they admit that it's hard to actually decide what complex behavior means.",
            "They have some ideas what it could, how it could be measured so it could be the internal complexity, so the representation inside the robot, which is maybe another very.",
            "Good measure because it really depends on the implementation.",
            "They say they could use information complexity, but then.",
            "Complex behavior is random behavior because it has to the most the highest entropy.",
            "Or you could use some psychological terms to describe behavioral complexity.",
            "Basically, relying on the on the developmental stages by peer shape, but this is also hard to charge on a robot because basically you need to sit there and see.",
            "That's nice behavior and I want to show you another video which actually does that.",
            "And.",
            "So this is there.",
            "Robot and their experimental setup.",
            "So they have this this playground so it looks very very similar actually to that previous video.",
            "But now we have have this this dog robot here.",
            "OK, and do a few things so you can punch this bag and he can bark to this other robot and after after training this robot for awhile so let's jump here.",
            "Maybe it actually learns to park the other other robot because these other robot parts back and he learns that if he punches the bag it moves and if he bites into something on the floor it's also some some interest in action so you can observe this behavior and say well let's looks nice interesting.",
            "But you cannot quantify it in any way, I think, so it's very hard.",
            "So yeah, they do this somehow, but I think it's not really convincing, or at least not to me.",
            "And therefore I."
        ],
        [
            "So the criticism is that.",
            "Although this information theoretic concept is is a nice one, and.",
            "Yeah, well founded 1.",
            "One of my criticism of this concept is that not everything is interesting or not.",
            "Everything that is complex is interesting, and what we actually want to have is that the autonomous exploration leads to something which at some point can be exploited.",
            "So that's what I call utility here, which is still a very vague term, but it means that we do an optimal autonomous exploration, but in some sense it can be explored exploited later.",
            "We don't know how and on which task, but.",
            "Yeah, that's the idea.",
            "Obviously this.",
            "Is wait and I make it a bit more concrete."
        ],
        [
            "Nine moving to him specific setup which we analyzed and which I think is kind of nice.",
            "So we we try to learn to navigate in an MDP so we don't have rewards so we only have the transition probabilities we have possibly infinite but discrete state space.",
            "And we want to learn to navigate in this space.",
            "So basically we want we have some central state start state is zero.",
            "We have a reset action to this state because if the state is infinitely, the agent might might just wander off and never return.",
            "So we need this reset action somehow and.",
            "So we want to learn to explore the vicinity of this start state.",
            "And basically find routes to all the states around this start state which are reachable in a certain number of steps.",
            "So we want to have a good map of this of this vicinity of the start state so that we can actually navigate in this in this vicinity.",
            "The question is how many exploration steps do we need to actually construct such a map?",
            "So by map I mean, for each state in the vicinity we have a policy which gets us to this stage reasonably quickly.",
            "The question is, how long does it take to learn this?",
            "How many exploration steps do we need and what is a good exploration algorithm?",
            "It turns out that it's not so easy and there's some, so this is now more formal.",
            "What we want to do so we have.",
            "So this is the time it takes policy \u03c0 to reach State S starting in the start state on average.",
            "So the average time to get there, and we are and our stars.",
            "The time of the optimal policy for this state, and we want so one question could be.",
            "How many exploration steps do we need to learn for all states S which are reachable in L steps by an optimal policy for this?",
            "For this step for this state so that we can learn policies for all these states.",
            "Turns out that this is not not a good question, or that's too hard because my essentially take infinitely many."
        ],
        [
            "Steps, so this is a counterexample.",
            "So this is just."
        ],
        [
            "Ark State this is the goal state and then we have these states in between.",
            "So this could be.",
            "This could be very many and for each of these states there's one action which gets us to the gold state.",
            "But these actions might be different for all these States and the first action takes us randomly to one of these very many states, which means that all these very many states are not reachable in L steps because we would basically need try very many times to start in a zero and.",
            "Take a random action to get to the state you actually want to get to.",
            "So all these states are not reachable.",
            "But we can reach this state in two steps by taking a random action and then taking the right action to get to this goal state.",
            "So basically this school state is separated from the start states by these intermediate states which are not not reachable.",
            "So this is not learnable because we both basically needs to learn the right action for all these intermediate states."
        ],
        [
            "It is not possible, so make it.",
            "Let's make it a little easier.",
            "So assume that we are allowing so that we define a set S such that this set S contains all states which are reachable in L steps, but using only policies which are defined on this set S. So as so we want to have a Poly so we have a state S which is basically the set of all reachable states.",
            "And reachable means it's reachable by a policy which is defined only on this reachable states.",
            "So we exclude basically the case that we have intermediate states which are not reachable.",
            "Unfortunately, this is still hard."
        ],
        [
            "And basically what we can prove is a lower bound which is exponentially in the number of steps squared of L."
        ],
        [
            "And the counterexample looks like this.",
            "It's a bit more complicated, so we have this.",
            "This tree in each node of this tree we have two L states.",
            "And but taking an action, we are transferred into this node and randomly and walk into one of these states.",
            "And if you take this action, we are transferred into this node and randomly into one of these into this states, if we take the right sequence of actions, we end up in this leaf state.",
            "In this leaf node.",
            "And from this leaf node we have a backwards tree which gets us to all the states in this blue boxes here.",
            "So since this the forward tree has a depth of L in each node, we have two L states.",
            "The total number of states in this blue box is L squared basically.",
            "So having a backwards tree we we can use a backward tree of lengths, log L basically to reach deterministically from.",
            "This leaves leaf node.",
            "All the states in this blue boxes.",
            "So which means that if we know the right sequence of actions we get here in L steps basically and then we have log L steps to get to any of these nodes in particular.",
            "Getting to a note in the forward direction is hard because there are too many and we are randomly transferred to one of these states.",
            "So we would would need basically 2L or some other constant number of trials to get into a specific state here.",
            "So which means that?",
            "To learn to read all these states in L steps, we would need to learn the right sequence through the tree here through the forward tree.",
            "And since there are many possible sequences, basically two to the L, it's also hard to learn.",
            "So this is also too hard problem.",
            "The problem is here that we first need to get to this state.",
            "So that we then can go get to this this particular state in this in this sequence here.",
            "So this is also kind of intermediate states, so this is a state we can we need to reach, and then we can reach the other states.",
            "But we cannot go directly to one of these states, so we again have these intermediate states which are hard to reach and we first need to learn a complicated policy to get there.",
            "So this is also too hard.",
            "So what we finally"
        ],
        [
            "Look for is that we ordered the States somehow or we assume that the States are ordered somehow, so we assume that there is a partial order on the state space.",
            "We don't need to know it, we just need.",
            "To assume that there is such an order and we use this order to define this set of reachable states incrementally.",
            "So which means that?",
            "S is now the set of all reachable states in the sense that as state S in this set is reach reachable by a policy which is defined only on smaller states.",
            "So now we can really incrementally beat this set S by an algorithm, so this makes the problem tractable now.",
            "OK, so."
        ],
        [
            "And it turns out that this is what we can actually do, so we can have an algorithm which runs in times L, ^3 S and eight to learn policies to all these reachable states reachable in this sense here.",
            "If you look closely, you observe that this that we have an S prime here, which might be super set of S. The reason is that since we are looking for police, since we can guarantee only that we find a policy which is slightly slower than the optimal policy, so which needs slightly more steps to reach the state, we might have, we might reach states which are actually further away from the start state then L, and these are also included in the S prime and we all learn all transitions to all these states.",
            "You're learning transitions to a larger set and this is reflected also in the bound.",
            "OK.",
            "Questions so far.",
            "Yep, it looks to me as if these exponentially many these combinatorial explosion almost sounds like a misspecification of your state space, because it doesn't matter which one of the states in that box I choose, I still can't reach the final goal, so why?",
            "Why you're having this level of resolution at all?"
        ],
        [
            "I'm not sure that I understand the question.",
            "Doesn't matter, you just go to the box and then you go to the next line.",
            "Yeah, but I need to get to the right box.",
            "Yes, but within the box you have.",
            "Can't you just collapse all these states?",
            "OK, I could use a different.",
            "I could use a different state space.",
            "Yeah, then might be different.",
            "Mia could be.",
            "Which action is taken?",
            "There's only one action against.",
            "Which of those states you end up in the well, it you could.",
            "You could identify a particular state you know to make that yeah, in the forward tree you don't need to.",
            "Or the transition is the same for all the states here.",
            "Look at the actions over the actions to solve with the same action you move here, but but still the states could have, so this is just for what for two actions.",
            "But if you have more actions that states could behave quite differently.",
            "So there might be, so this is just a part of the MDP to show that it's actually necessary to or that's difficult to explore it, but there might be some for all these states here.",
            "There might be something else going on with some more states.",
            "So you are right, if you have just this tree, it might make sense to collapse this, because I don't care.",
            "Then then it would go away, but you could assume that for each of these states you have some other states depending on on this state.",
            "So you have some other transitions from each of these state to other states which are not shown here and then it makes sense to move to a particular state.",
            "So.",
            "More questions.",
            "So maybe maybe I should elaborate a little bit on that.",
            "So the problem here is that it's hard.",
            "To distinguish reachable states from unreachable states, so this makes the problem hard.",
            "So if we go here and find out that by taking this action we get into states which are not reachable, we could forget about this action and then finding the right path through the tree would be easy.",
            "But during exploration, we don't know if this is a this reachable state, so these are the reachable States and this makes the exploration of defining the reachable States and exploration heart.",
            "So if the set of reachable states would be given ahead and it would be easy.",
            "But if it's not over there, if we actually need to find the reachable states, then the exploration problem is difficult.",
            "OK, now I'm."
        ],
        [
            "Move to standard reinforcement learning because I use it for proving proving this bound here.",
            "So yeah, because.",
            "Something I like, so standard reinforcement learning discrete finite state space transition probabilities."
        ],
        [
            "We assume that the rewards they reward reward function is given.",
            "This makes this makes the problem not not not much easier.",
            "So the main problem is that we don't know the transition transition probabilities."
        ],
        [
            "In yeah, so there's the discounted and undiscounted world.",
            "Um?",
            "For the discounted world, you know you care about the value function of a state.",
            "And the optimal value, the value of the optimal policy and undiscounted world you care about every tree board?",
            "And every true Ward naturally leads to."
        ],
        [
            "The notion of regret because you can compare other rewards collected by your algorithm to the rewards and optimal policy, would collect.",
            "And here the states which are visited by the optimal policy and the states that are visited by your algorithm could be different.",
            "So this is really trajectory of the optimal policy and this is a trajectory of your own algorithm and you're interested in how much worse is that is other rewards of your queue algorithm.",
            "So this is a very natural notion of reward for undiscounted reinforcement learning for discounted reinforcement learning.",
            "That's lot more difficult and."
        ],
        [
            "People usually do is that they look at pack MDP bounds so they count the number of steps where the policy where the value of the let's look at this.",
            "This is more easier where the Q value of the chosen action, the action chosen by the algorithm is epsilon worse than the optimal action.",
            "And this is a bit more strange, because here you assume that here you're looking only on the trajectory of the algorithm, so you're not comparing with the trajectory of an optimal algorithm, but we're looking at the trajectory of the of your algorithm and just ask for this trajectory.",
            "How often did you choose a bad action on this trajectory?",
            "Yeah so this, but this is basically what you can do with a discounted setting and."
        ],
        [
            "This also needs to some notion of regrets, which is a bit strange though, so let's look at."
        ],
        [
            "Yes, we get.",
            "We get bonds.",
            "So in the in the undiscounted case we get D. What is the diameter of the MDP, which basically is basically the complexity of the MDP?",
            "Which doesn't need to know, be known to the algorithm Times S * SQRT 80.",
            "So please time is number of actions as number of states.",
            "What we don't like is that the S is outside of the square root, so we would like much more if the S would be within the square root.",
            "Because if you translate this into a pack like bound, it says that the error scales like S ^2.",
            "S ^2 * A whereas when if the S would be within the square root, then it would scale like S * A, which is what you would expect somehow.",
            "For some reason we cannot prove that and it will give you a little bit of an indication why we cannot prove it.",
            "Yeah, for the discount in the discounted setting we get that they regret this bounded by something we like.",
            "The square root of X 80, so here there S is within the squirrel and we have Vmax which is the maximum value of the value function or an upper bound of that divided by 1 minus gamma where gamma is the discount factor.",
            "So the D does not appear in the discounted case because one minus gamma, summer place, the role of D. Because this is basically the horizon, how far do you look ahead?",
            "And this is somehow related to the D. How long does it take to move around in your MDP?",
            "It's not completely the same thing, but it's somehow related, so I will.",
            "I believe that this is the stronger notion.",
            "This is the weaker notion, which is easier to deal with.",
            "OK."
        ],
        [
            "So yeah, so to get pack MDP bounds from the regret bound, you can just plug in.",
            "So instead of T the number of.",
            "Times you choose a bad action and step the proof.",
            "If you look carefully and proof in the proofs then you can see that you can replace the T by the number of times you choose a bad action epsilon bear action.",
            "So you just solve gamma T equals epsilon T because epsilon is the accuracy you care for and T is now not the total number of the total time, but it's the number of steps where you do choose an epsilon correction.",
            "And if solve that then you get something like this.",
            "For example, which?",
            "Is the same as the bound by seat and cheapest Barry.",
            "As I see my last year, which also have impact MDP pound which has this dependency.",
            "OK, so this is just comment.",
            "That you can convert regret bounds in packing the bounds quite easily.",
            "And therefore you should work on the regret."
        ],
        [
            "Pounds.",
            "So how does the algorithm work?",
            "So we use an optimistic algorithm.",
            "Their optimistic algorithm assumes the best possible MVP, which is consistent with the data observed so far.",
            "Consistent needs to be defined and chooses the optimal policy for the best possible MVP.",
            "In this set of consistent MVP's.",
            "So there is standard approach.",
            "There's some some care is needed, so if you know about the bandit problem then independent problem you can in each trial choose the.",
            "ARM, which has the highest upper confidence value, which is the most optimistic value consistent with your data?",
            "But if you do MDP stand the rewards adelaid so you don't want to switch policy too often, so you need to keep going with the same policy for some time and this makes yeah.",
            "This is a little bit more complicated, just very little.",
            "So the main question is how to define consistent MVP's."
        ],
        [
            "And.",
            "What you need for the proof to go through is that with high probability, this set of consistent MVP's contains the true MVP, so you want to have for this consistent set.",
            "This kind of confidence set of MVP's, so you want to have a high probability that your true MVP's within this set, and to get good regret bounds you want this consistent set to be small because the smaller set is the tighter bounds you will get so."
        ],
        [
            "So how yeah, this is the intuition about optimistic MDP's.",
            "Yeah, if about optimistic policies.",
            "If you have an optimistic policy and you have small regrets, then you're fine.",
            "If you have high regret, the optimism was.",
            "Wrong and you learn something about your assumptions so you learn something more.",
            "So this is the basic intuition about optimistic policies, which is quite standard.",
            "How much time do I have?",
            "10 minutes to cook.",
            "Come on.",
            "It's not that bad."
        ],
        [
            "It gets worse.",
            "Do I need the temperature in the room this happened?",
            "Electric temperature.",
            "So I'm fine to increase it a little.",
            "So how do we choose this confidence, confident MDP?",
            "So yeah, very very.",
            "Easy way to do it is to put bounds on how much the optimistic and the transition probabilities of the optimistic NDPS might deviate from the empirical estimates and so and this is 1 possibility to do it and you can show that with high probability the true transition probabilities satisfy this condition.",
            "The only drawback is that we have the S here which gives us gives gives us this additional squared aspect and.",
            "In the bonds.",
            "And unfortunately, we don't know how to get rid of it and I will give you some indication why it's."
        ],
        [
            "Here.",
            "For the discounted case, we can do it better, and so the first one is quite simple.",
            "The second one is an interesting one, so this is not a very simple confidence interval because it has the distance between the optimistic transition probabilities and empirical transition probabilities, but multiplied with the optimistic value you would get if you use this probabilities transition probabilities.",
            "So it's kind of a weird notion.",
            "So you assume an optimistic MDP and it needs to satisfy this assumption.",
            "This is the optimal policy for the optimistic MDP, so the value of the optimistic policy for the value of the optimal policy for the optimistic MDP.",
            "And you can show that it's easy to show that that the true MVP satisfies this.",
            "So if you replace that image, the true values so that rule transition probability is the true value function, then this is satisfied.",
            "And for the discounted case, this is sufficient to prove the bounds.",
            "Quiet."
        ],
        [
            "Night gets little, so the basic inequality or the basic term you need to bound in the undiscounted case is this one.",
            "So this is the.",
            "Basically this is by how much do you overestimate the true rewards of the optimistic MDP, so this is the average reward of the optimistic MDP, and these are the true rewards and these are multiplied by the times you visit the states, so in each state to get a different reward depending on the action.",
            "And this is basically the overestimation where N is the expected number of visits.",
            "To the states.",
            "By the optimistic policy in the true MVP.",
            "Yeah.",
            "You can massage this a little and finally what comes out is that you need to bound.",
            "The hard part is this one.",
            "So you need to bound this term and this distant if this would be constant or a fixed vector, you could use a general kind of bound to get tight bounds, but since this depends on that.",
            "You cannot do that.",
            "So you, and therefore you need to do something more crude and estimating the one norm of distant Infinity norm of this.",
            "And this is because you're estimating the one norm here.",
            "You get this additional squared X Factor."
        ],
        [
            "In the discounted case, it's easier.",
            "So maybe I just point out the main things.",
            "Since I have very little time.",
            "Really have only 10 minutes left.",
            "Less now.",
            "I mean.",
            "Maybe you need to resolve this now.",
            "I have 6 minutes when you have since you started 10 minutes since you said 10 minutes.",
            "So what you so if you do the calculation, you find out that you get something here and this is what I use for their confidence in for the confident or for the consistent set of MDP's.",
            "So this can be dealed with the definition of the consistent MVP's and this is something which would be hard to deal with and this resembles some of this term here.",
            "But this is a recursion because this is the same like this.",
            "And we can just pull it on the other side and divide by 1 minus epsilon.",
            "And This is why we get tighter bounds.",
            "So because for in the discounted case we have this contraction, so we have this recursion and we can and we can use this to get the tighter bounds."
        ],
        [
            "OK."
        ],
        [
            "So I skip this so optimistic algorithm for autonomous exploration.",
            "So coming back to the original problem, finding policies to reach the reachable states.",
            "So the algorithm is very simple, so we keep a set of states which we already know that are reachable for, which you already know policy and the rest of the unknown states.",
            "So we optimistically choose the closest state among the unknown states.",
            "So for which we believe of given Arkansas optimistic estimates that we can reach this state, this state most quickly by some certain policy by some policy.",
            "Then we run this policy, if then we test if this policy actually satisfied gives us a quick transition to this state.",
            "So we run this policy for some time.",
            "So sometimes K, which is basically.",
            "Some logarithm of the confidence we want to get and just check if if we recently often get to the target state.",
            "And if you do, we know that this is a good policy and we keep it.",
            "If we don't reach this target states often enough and often enough means at least third of the times we try.",
            "With this we got.",
            "We repeat the loop and choose another state from the unknown from the unknown state.",
            "Why does this work?"
        ],
        [
            "Well, first thing is through the testing, we can guarantee that if the test is successful then we are sure that this policy is a good policy.",
            "So these constants are bit arbitrary and we can improve them.",
            "So this is just to make the proof.",
            "The argument is simple.",
            "So if in a third of the cases we we reached the States, then we are reasonably sure that we can reach this state in a Dell steps.",
            "The other question is how often?",
            "Do do we have iteration where we don't find a good policy?",
            "So basically I use useless iteration where we cannot find a good policy.",
            "And this can be.",
            "But this can be bounded by, by their regret.",
            "So if we don't find the good policy, then we can translate this.",
            "This trials to a modified MDP where we would get a reward if we reach the goal state.",
            "And since we don't reach the goals that very often, we suffer some regret compared to an optimal policy which would reach this goal state."
        ],
        [
            "Rent.",
            "Just basically plugging in the regret bound which looks a little different here because it's more complicated.",
            "It's more subtle, So what you see is that we only we don't get this.",
            "We get the square root is here and this is because we partition.",
            "Now we have we follow policy only for a fixed number of steps for L steps.",
            "No, after still 2 minutes.",
            "And if we partition the examples into L. Disjoint sets we can regain this independence and can get better bounds, so the prices that we need L times more examples.",
            "So This is why the so.",
            "This is why we need to divide the total number of examples by L. So, but this gives us this.",
            "We get the square root S and on the other hand we get the L cubed here.",
            "And yeah, if you do the calculation, you find that this is the number of exploration steps you need."
        ],
        [
            "And skip this."
        ],
        [
            "This and this is the summary, so autonomous exploration is interesting, I hope.",
            "At least the video should should convince you that this is indeed the case.",
            "It's it's difficult to formalize.",
            "Competitively.",
            "We analyzed the concrete model for autonomous exploration for reaching states.",
            "So which seems quite reasonable, because this is typically where you get a reward if you reach reaches certain state.",
            "There's few cases where you actually need to have a trajectory which gives you high reward.",
            "Yeah, so I think it's a reasonable general setting.",
            "Difficult cases cannot be learned, so you need to assume a bit more structure on your MDP.",
            "So we do it by incremental optimistic exploration.",
            "So we add states to the known to the set of known states.",
            "Once one at a time, and performance analysis is through regret analysis or regret analysis, which is great.",
            "And finally, apparently well, obviously there are other interesting learning modes, for example by invitation, so not everything is learned autonomously.",
            "It seems there's also a lot of imitation learning going on."
        ],
        [
            "And yeah, and so the question is, is autonomous exploration is really useful?",
            "Thank you.",
            "I work on the Markov process and this is the jewel of yours and I have been forced to consider that.",
            "Using probability anymore not using probabilities, probability are super probability in your case.",
            "Union B with A&B disjoint is not equal to pay of a plus because otherwise things were not working properly.",
            "I've used this same problem, no no no.",
            "Can I just ask?",
            "I mean you will you when you update your models.",
            "Is that you know that's after you've done your search for one states, then yes, all of that information update your.",
            "Yeah so the algorithm."
        ],
        [
            "Wood.",
            "Run through one complete iteration, so would do the case steps here.",
            "Yeah, and then it would update.",
            "In the simplest case, you could update more often, but yeah, in principle yes vote for the next day and then you look for the next stage right?",
            "Yeah, so yeah, you need to complete the test while you.",
            "In principle, if you find out that your experience during this update during these trials indicates that you actually cannot use reach the state, you could stop early, but.",
            "Yeah, I mean you could do that, yeah?",
            "Yeah, yeah you could do that, yeah?",
            "You could cut down on that, yeah?",
            "What is the message of the first part of your talk?",
            "Because you're trying to discover skills and then you showed at the beginning of the problem very difficult.",
            "Continue networking is ordering for the States and then you reach the solvable problem.",
            "So what is the message about?",
            "This connected to finding the skills.",
            "So what does this say to us at this point?",
            "Well, it says that.",
            "At least our algorithm works only if you build skills incrementally.",
            "So it means like a bottom up approach, yeah?",
            "So if you go back to the counter example, yeah, it's like a bottom up approach."
        ],
        [
            "So although this might be doable.",
            "You first need to learn this, which is actually.",
            "Further, yeah.",
            "This is so this and this is actually a complicated skill.",
            "Which is hard to learn.",
            "Is it true to consider the same question that he asked that these ordering of the states is like collapsing those boxes to no, not quite.",
            "Because it says that that.",
            "Not really, it really says that you can build skills incrementally.",
            "You learn one skill and then you can extend it by a little bit.",
            "That be that you first crudely learn the skills, and then you learn the refinement as you do it.",
            "Quite often in sports, for example.",
            "Yeah, yeah.",
            "So this will be the analogy to that.",
            "No, here, here, here you would need to jump here.",
            "So this is the first thing you can.",
            "You need to learn.",
            "You cannot learn the in between steps because yeah, yeah, yeah, you first need to go here and you need to learn a big step here and then you can go back with some reach all these.",
            "We went from this.",
            "You can derive all these skills so this this is not incremental.",
            "You really need to do this big jump here, which is hard to learn.",
            "So the in the other case where you have these incremental thing and even our algorithm does that.",
            "So if you learn skills for the certain level L and then you increase the L then you will refine the skills.",
            "Sandy didn't, and that's how you could extend to be sinetti, which which you know about.",
            "So you first learn is small with unity and then you increase the L and and re learn and use what you already know and and and refine what you already know.",
            "Do you can trust me?",
            "There's a break now, so yeah.",
            "So you know if you have a hierarchy and we want to be kind of hierarchy can be composed or task to hierarchy.",
            "Want there you only way that we can take advantage of this hierarchy in learning and speed up our learning is behind.",
            "If you have a state, if you have the abstraction abstraction hierarchy, just add more parameters and it's even worse than learning things.",
            "Can we have a relation between that and what have you?",
            "Not really, so this is not about abstraction.",
            "This is not about this abstraction, no, I'm afraid not.",
            "Not really, it's really just saying that you can incrementally add skills.",
            "It's not about abstraction.",
            "Tasks in which we can ignore?",
            "Yeah, no.",
            "This analysis is not about abstraction.",
            "But if you think of your, you know states that you can learn ideas.",
            "Just being states that describe rough skills and then the refinements of them are.",
            "So it's a bit like.",
            "I mean you build the hierarchy into your face.",
            "Well, it's hierarchical and in the sands of difficulty.",
            "But that's not what you meant.",
            "You meant you can use the same actions to reach a certain set of states.",
            "Is that if you have to use.",
            "Everything your state space every day then.",
            "Decomposing the tasks and skills is not going to help.",
            "Yeah.",
            "Well, it's not the way I think about abstraction about.",
            "I think about abstraction.",
            "If you can do the same thing to reach similar goals.",
            "That's not what we do here.",
            "Unfortunately.",
            "OK, well thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thank you for having me.",
                    "label": 0
                },
                {
                    "sent": "This is trying to work with share how she hung limb and Chris Watkins and it's done.",
                    "label": 0
                },
                {
                    "sent": "It's done within the complex project which is funded by the European Commission.",
                    "label": 0
                },
                {
                    "sent": "So why I'm interested in autonomous autonomous X?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operation.",
                    "label": 0
                },
                {
                    "sent": "It appears that infants, animals explore their environment without apparent purpose, and they do it in a way which is appropriate to their current skill.",
                    "label": 1
                },
                {
                    "sent": "The current level of skill, and they increase their skills overtime.",
                    "label": 0
                },
                {
                    "sent": "By doing this exploration, and I want to show you a video which you like and which is quite nice.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "It shows an infant.",
                    "label": 0
                },
                {
                    "sent": "Playing by itself for no apparent reason, and apparently it learns something by doing that and the advantages, it doesn't need any supervision.",
                    "label": 0
                },
                {
                    "sent": "Any feedback to be engaged and and learn?",
                    "label": 0
                },
                {
                    "sent": "And this is something we would like also in learning systems so that they can learn without supervision and improve their skills and get better and learn more complicated skills.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Entropy in the room, yes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is the channel problem I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "To do any, any any analysis or analysis of algorithms or any reasoning, mathematical reasoning about autonomous exploration is difficult because.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's hard to quantify the problem is we have no specific task we want to solve, so we cannot easily measure how well autonomous exploration is working, and indeed there is, as far as I know, no agreed criterion for.",
                    "label": 1
                },
                {
                    "sent": "What how to measure autonomous exploration?",
                    "label": 1
                },
                {
                    "sent": "There's some some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proposals in the literature, one by Schmidhuber, who argues that essentially what you want to do by autonomous exploration is that you want to basically choose actions which decrease your uncertainty.",
                    "label": 1
                },
                {
                    "sent": "So, and this basically leads to using information gain as an intrinsic reward in if you want to think of reinforcement light setup.",
                    "label": 1
                },
                {
                    "sent": "Problem is that information gain is how to measure.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So what he?",
                    "label": 0
                },
                {
                    "sent": "Das is that he recites to use Kolmogorov type complexity stuff or algorithmic information theory to measure this.",
                    "label": 0
                },
                {
                    "sent": "Then it becomes measurable.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it does not come, it becomes uncomputable, so it's hard to use it in an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So finally it needs he needs to use some some approximation to this algorithmic information theoretic measures.",
                    "label": 0
                },
                {
                    "sent": "So this was.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Picked up by or the end and coworkers and.",
                    "label": 0
                },
                {
                    "sent": "They use this to actually build a robot who learns in an environment and they are interested in how the robot can develop complex behavior, and they admit that it's hard to actually decide what complex behavior means.",
                    "label": 0
                },
                {
                    "sent": "They have some ideas what it could, how it could be measured so it could be the internal complexity, so the representation inside the robot, which is maybe another very.",
                    "label": 1
                },
                {
                    "sent": "Good measure because it really depends on the implementation.",
                    "label": 0
                },
                {
                    "sent": "They say they could use information complexity, but then.",
                    "label": 1
                },
                {
                    "sent": "Complex behavior is random behavior because it has to the most the highest entropy.",
                    "label": 1
                },
                {
                    "sent": "Or you could use some psychological terms to describe behavioral complexity.",
                    "label": 0
                },
                {
                    "sent": "Basically, relying on the on the developmental stages by peer shape, but this is also hard to charge on a robot because basically you need to sit there and see.",
                    "label": 0
                },
                {
                    "sent": "That's nice behavior and I want to show you another video which actually does that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is there.",
                    "label": 0
                },
                {
                    "sent": "Robot and their experimental setup.",
                    "label": 0
                },
                {
                    "sent": "So they have this this playground so it looks very very similar actually to that previous video.",
                    "label": 0
                },
                {
                    "sent": "But now we have have this this dog robot here.",
                    "label": 0
                },
                {
                    "sent": "OK, and do a few things so you can punch this bag and he can bark to this other robot and after after training this robot for awhile so let's jump here.",
                    "label": 0
                },
                {
                    "sent": "Maybe it actually learns to park the other other robot because these other robot parts back and he learns that if he punches the bag it moves and if he bites into something on the floor it's also some some interest in action so you can observe this behavior and say well let's looks nice interesting.",
                    "label": 0
                },
                {
                    "sent": "But you cannot quantify it in any way, I think, so it's very hard.",
                    "label": 0
                },
                {
                    "sent": "So yeah, they do this somehow, but I think it's not really convincing, or at least not to me.",
                    "label": 0
                },
                {
                    "sent": "And therefore I.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the criticism is that.",
                    "label": 0
                },
                {
                    "sent": "Although this information theoretic concept is is a nice one, and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well founded 1.",
                    "label": 0
                },
                {
                    "sent": "One of my criticism of this concept is that not everything is interesting or not.",
                    "label": 1
                },
                {
                    "sent": "Everything that is complex is interesting, and what we actually want to have is that the autonomous exploration leads to something which at some point can be exploited.",
                    "label": 1
                },
                {
                    "sent": "So that's what I call utility here, which is still a very vague term, but it means that we do an optimal autonomous exploration, but in some sense it can be explored exploited later.",
                    "label": 0
                },
                {
                    "sent": "We don't know how and on which task, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Obviously this.",
                    "label": 0
                },
                {
                    "sent": "Is wait and I make it a bit more concrete.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nine moving to him specific setup which we analyzed and which I think is kind of nice.",
                    "label": 0
                },
                {
                    "sent": "So we we try to learn to navigate in an MDP so we don't have rewards so we only have the transition probabilities we have possibly infinite but discrete state space.",
                    "label": 1
                },
                {
                    "sent": "And we want to learn to navigate in this space.",
                    "label": 0
                },
                {
                    "sent": "So basically we want we have some central state start state is zero.",
                    "label": 0
                },
                {
                    "sent": "We have a reset action to this state because if the state is infinitely, the agent might might just wander off and never return.",
                    "label": 0
                },
                {
                    "sent": "So we need this reset action somehow and.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn to explore the vicinity of this start state.",
                    "label": 0
                },
                {
                    "sent": "And basically find routes to all the states around this start state which are reachable in a certain number of steps.",
                    "label": 1
                },
                {
                    "sent": "So we want to have a good map of this of this vicinity of the start state so that we can actually navigate in this in this vicinity.",
                    "label": 0
                },
                {
                    "sent": "The question is how many exploration steps do we need to actually construct such a map?",
                    "label": 0
                },
                {
                    "sent": "So by map I mean, for each state in the vicinity we have a policy which gets us to this stage reasonably quickly.",
                    "label": 0
                },
                {
                    "sent": "The question is, how long does it take to learn this?",
                    "label": 1
                },
                {
                    "sent": "How many exploration steps do we need and what is a good exploration algorithm?",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's not so easy and there's some, so this is now more formal.",
                    "label": 0
                },
                {
                    "sent": "What we want to do so we have.",
                    "label": 0
                },
                {
                    "sent": "So this is the time it takes policy \u03c0 to reach State S starting in the start state on average.",
                    "label": 0
                },
                {
                    "sent": "So the average time to get there, and we are and our stars.",
                    "label": 0
                },
                {
                    "sent": "The time of the optimal policy for this state, and we want so one question could be.",
                    "label": 0
                },
                {
                    "sent": "How many exploration steps do we need to learn for all states S which are reachable in L steps by an optimal policy for this?",
                    "label": 0
                },
                {
                    "sent": "For this step for this state so that we can learn policies for all these states.",
                    "label": 0
                },
                {
                    "sent": "Turns out that this is not not a good question, or that's too hard because my essentially take infinitely many.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steps, so this is a counterexample.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ark State this is the goal state and then we have these states in between.",
                    "label": 0
                },
                {
                    "sent": "So this could be.",
                    "label": 0
                },
                {
                    "sent": "This could be very many and for each of these states there's one action which gets us to the gold state.",
                    "label": 0
                },
                {
                    "sent": "But these actions might be different for all these States and the first action takes us randomly to one of these very many states, which means that all these very many states are not reachable in L steps because we would basically need try very many times to start in a zero and.",
                    "label": 1
                },
                {
                    "sent": "Take a random action to get to the state you actually want to get to.",
                    "label": 0
                },
                {
                    "sent": "So all these states are not reachable.",
                    "label": 0
                },
                {
                    "sent": "But we can reach this state in two steps by taking a random action and then taking the right action to get to this goal state.",
                    "label": 0
                },
                {
                    "sent": "So basically this school state is separated from the start states by these intermediate states which are not not reachable.",
                    "label": 0
                },
                {
                    "sent": "So this is not learnable because we both basically needs to learn the right action for all these intermediate states.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is not possible, so make it.",
                    "label": 0
                },
                {
                    "sent": "Let's make it a little easier.",
                    "label": 0
                },
                {
                    "sent": "So assume that we are allowing so that we define a set S such that this set S contains all states which are reachable in L steps, but using only policies which are defined on this set S. So as so we want to have a Poly so we have a state S which is basically the set of all reachable states.",
                    "label": 0
                },
                {
                    "sent": "And reachable means it's reachable by a policy which is defined only on this reachable states.",
                    "label": 0
                },
                {
                    "sent": "So we exclude basically the case that we have intermediate states which are not reachable.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this is still hard.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically what we can prove is a lower bound which is exponentially in the number of steps squared of L.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the counterexample looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated, so we have this.",
                    "label": 0
                },
                {
                    "sent": "This tree in each node of this tree we have two L states.",
                    "label": 0
                },
                {
                    "sent": "And but taking an action, we are transferred into this node and randomly and walk into one of these states.",
                    "label": 0
                },
                {
                    "sent": "And if you take this action, we are transferred into this node and randomly into one of these into this states, if we take the right sequence of actions, we end up in this leaf state.",
                    "label": 0
                },
                {
                    "sent": "In this leaf node.",
                    "label": 0
                },
                {
                    "sent": "And from this leaf node we have a backwards tree which gets us to all the states in this blue boxes here.",
                    "label": 0
                },
                {
                    "sent": "So since this the forward tree has a depth of L in each node, we have two L states.",
                    "label": 0
                },
                {
                    "sent": "The total number of states in this blue box is L squared basically.",
                    "label": 0
                },
                {
                    "sent": "So having a backwards tree we we can use a backward tree of lengths, log L basically to reach deterministically from.",
                    "label": 0
                },
                {
                    "sent": "This leaves leaf node.",
                    "label": 0
                },
                {
                    "sent": "All the states in this blue boxes.",
                    "label": 0
                },
                {
                    "sent": "So which means that if we know the right sequence of actions we get here in L steps basically and then we have log L steps to get to any of these nodes in particular.",
                    "label": 0
                },
                {
                    "sent": "Getting to a note in the forward direction is hard because there are too many and we are randomly transferred to one of these states.",
                    "label": 0
                },
                {
                    "sent": "So we would would need basically 2L or some other constant number of trials to get into a specific state here.",
                    "label": 0
                },
                {
                    "sent": "So which means that?",
                    "label": 0
                },
                {
                    "sent": "To learn to read all these states in L steps, we would need to learn the right sequence through the tree here through the forward tree.",
                    "label": 1
                },
                {
                    "sent": "And since there are many possible sequences, basically two to the L, it's also hard to learn.",
                    "label": 0
                },
                {
                    "sent": "So this is also too hard problem.",
                    "label": 0
                },
                {
                    "sent": "The problem is here that we first need to get to this state.",
                    "label": 0
                },
                {
                    "sent": "So that we then can go get to this this particular state in this in this sequence here.",
                    "label": 0
                },
                {
                    "sent": "So this is also kind of intermediate states, so this is a state we can we need to reach, and then we can reach the other states.",
                    "label": 0
                },
                {
                    "sent": "But we cannot go directly to one of these states, so we again have these intermediate states which are hard to reach and we first need to learn a complicated policy to get there.",
                    "label": 0
                },
                {
                    "sent": "So this is also too hard.",
                    "label": 0
                },
                {
                    "sent": "So what we finally",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look for is that we ordered the States somehow or we assume that the States are ordered somehow, so we assume that there is a partial order on the state space.",
                    "label": 1
                },
                {
                    "sent": "We don't need to know it, we just need.",
                    "label": 0
                },
                {
                    "sent": "To assume that there is such an order and we use this order to define this set of reachable states incrementally.",
                    "label": 0
                },
                {
                    "sent": "So which means that?",
                    "label": 0
                },
                {
                    "sent": "S is now the set of all reachable states in the sense that as state S in this set is reach reachable by a policy which is defined only on smaller states.",
                    "label": 0
                },
                {
                    "sent": "So now we can really incrementally beat this set S by an algorithm, so this makes the problem tractable now.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that this is what we can actually do, so we can have an algorithm which runs in times L, ^3 S and eight to learn policies to all these reachable states reachable in this sense here.",
                    "label": 0
                },
                {
                    "sent": "If you look closely, you observe that this that we have an S prime here, which might be super set of S. The reason is that since we are looking for police, since we can guarantee only that we find a policy which is slightly slower than the optimal policy, so which needs slightly more steps to reach the state, we might have, we might reach states which are actually further away from the start state then L, and these are also included in the S prime and we all learn all transitions to all these states.",
                    "label": 0
                },
                {
                    "sent": "You're learning transitions to a larger set and this is reflected also in the bound.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Questions so far.",
                    "label": 0
                },
                {
                    "sent": "Yep, it looks to me as if these exponentially many these combinatorial explosion almost sounds like a misspecification of your state space, because it doesn't matter which one of the states in that box I choose, I still can't reach the final goal, so why?",
                    "label": 0
                },
                {
                    "sent": "Why you're having this level of resolution at all?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not sure that I understand the question.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter, you just go to the box and then you go to the next line.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I need to get to the right box.",
                    "label": 0
                },
                {
                    "sent": "Yes, but within the box you have.",
                    "label": 0
                },
                {
                    "sent": "Can't you just collapse all these states?",
                    "label": 0
                },
                {
                    "sent": "OK, I could use a different.",
                    "label": 0
                },
                {
                    "sent": "I could use a different state space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then might be different.",
                    "label": 0
                },
                {
                    "sent": "Mia could be.",
                    "label": 0
                },
                {
                    "sent": "Which action is taken?",
                    "label": 0
                },
                {
                    "sent": "There's only one action against.",
                    "label": 0
                },
                {
                    "sent": "Which of those states you end up in the well, it you could.",
                    "label": 0
                },
                {
                    "sent": "You could identify a particular state you know to make that yeah, in the forward tree you don't need to.",
                    "label": 0
                },
                {
                    "sent": "Or the transition is the same for all the states here.",
                    "label": 0
                },
                {
                    "sent": "Look at the actions over the actions to solve with the same action you move here, but but still the states could have, so this is just for what for two actions.",
                    "label": 0
                },
                {
                    "sent": "But if you have more actions that states could behave quite differently.",
                    "label": 0
                },
                {
                    "sent": "So there might be, so this is just a part of the MDP to show that it's actually necessary to or that's difficult to explore it, but there might be some for all these states here.",
                    "label": 0
                },
                {
                    "sent": "There might be something else going on with some more states.",
                    "label": 0
                },
                {
                    "sent": "So you are right, if you have just this tree, it might make sense to collapse this, because I don't care.",
                    "label": 0
                },
                {
                    "sent": "Then then it would go away, but you could assume that for each of these states you have some other states depending on on this state.",
                    "label": 0
                },
                {
                    "sent": "So you have some other transitions from each of these state to other states which are not shown here and then it makes sense to move to a particular state.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "So maybe maybe I should elaborate a little bit on that.",
                    "label": 0
                },
                {
                    "sent": "So the problem here is that it's hard.",
                    "label": 0
                },
                {
                    "sent": "To distinguish reachable states from unreachable states, so this makes the problem hard.",
                    "label": 0
                },
                {
                    "sent": "So if we go here and find out that by taking this action we get into states which are not reachable, we could forget about this action and then finding the right path through the tree would be easy.",
                    "label": 0
                },
                {
                    "sent": "But during exploration, we don't know if this is a this reachable state, so these are the reachable States and this makes the exploration of defining the reachable States and exploration heart.",
                    "label": 0
                },
                {
                    "sent": "So if the set of reachable states would be given ahead and it would be easy.",
                    "label": 0
                },
                {
                    "sent": "But if it's not over there, if we actually need to find the reachable states, then the exploration problem is difficult.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to standard reinforcement learning because I use it for proving proving this bound here.",
                    "label": 0
                },
                {
                    "sent": "So yeah, because.",
                    "label": 0
                },
                {
                    "sent": "Something I like, so standard reinforcement learning discrete finite state space transition probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We assume that the rewards they reward reward function is given.",
                    "label": 1
                },
                {
                    "sent": "This makes this makes the problem not not not much easier.",
                    "label": 0
                },
                {
                    "sent": "So the main problem is that we don't know the transition transition probabilities.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In yeah, so there's the discounted and undiscounted world.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For the discounted world, you know you care about the value function of a state.",
                    "label": 1
                },
                {
                    "sent": "And the optimal value, the value of the optimal policy and undiscounted world you care about every tree board?",
                    "label": 0
                },
                {
                    "sent": "And every true Ward naturally leads to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The notion of regret because you can compare other rewards collected by your algorithm to the rewards and optimal policy, would collect.",
                    "label": 1
                },
                {
                    "sent": "And here the states which are visited by the optimal policy and the states that are visited by your algorithm could be different.",
                    "label": 0
                },
                {
                    "sent": "So this is really trajectory of the optimal policy and this is a trajectory of your own algorithm and you're interested in how much worse is that is other rewards of your queue algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a very natural notion of reward for undiscounted reinforcement learning for discounted reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That's lot more difficult and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People usually do is that they look at pack MDP bounds so they count the number of steps where the policy where the value of the let's look at this.",
                    "label": 0
                },
                {
                    "sent": "This is more easier where the Q value of the chosen action, the action chosen by the algorithm is epsilon worse than the optimal action.",
                    "label": 1
                },
                {
                    "sent": "And this is a bit more strange, because here you assume that here you're looking only on the trajectory of the algorithm, so you're not comparing with the trajectory of an optimal algorithm, but we're looking at the trajectory of the of your algorithm and just ask for this trajectory.",
                    "label": 0
                },
                {
                    "sent": "How often did you choose a bad action on this trajectory?",
                    "label": 0
                },
                {
                    "sent": "Yeah so this, but this is basically what you can do with a discounted setting and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This also needs to some notion of regrets, which is a bit strange though, so let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we get.",
                    "label": 0
                },
                {
                    "sent": "We get bonds.",
                    "label": 0
                },
                {
                    "sent": "So in the in the undiscounted case we get D. What is the diameter of the MDP, which basically is basically the complexity of the MDP?",
                    "label": 0
                },
                {
                    "sent": "Which doesn't need to know, be known to the algorithm Times S * SQRT 80.",
                    "label": 0
                },
                {
                    "sent": "So please time is number of actions as number of states.",
                    "label": 0
                },
                {
                    "sent": "What we don't like is that the S is outside of the square root, so we would like much more if the S would be within the square root.",
                    "label": 0
                },
                {
                    "sent": "Because if you translate this into a pack like bound, it says that the error scales like S ^2.",
                    "label": 0
                },
                {
                    "sent": "S ^2 * A whereas when if the S would be within the square root, then it would scale like S * A, which is what you would expect somehow.",
                    "label": 0
                },
                {
                    "sent": "For some reason we cannot prove that and it will give you a little bit of an indication why we cannot prove it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for the discount in the discounted setting we get that they regret this bounded by something we like.",
                    "label": 0
                },
                {
                    "sent": "The square root of X 80, so here there S is within the squirrel and we have Vmax which is the maximum value of the value function or an upper bound of that divided by 1 minus gamma where gamma is the discount factor.",
                    "label": 0
                },
                {
                    "sent": "So the D does not appear in the discounted case because one minus gamma, summer place, the role of D. Because this is basically the horizon, how far do you look ahead?",
                    "label": 0
                },
                {
                    "sent": "And this is somehow related to the D. How long does it take to move around in your MDP?",
                    "label": 0
                },
                {
                    "sent": "It's not completely the same thing, but it's somehow related, so I will.",
                    "label": 0
                },
                {
                    "sent": "I believe that this is the stronger notion.",
                    "label": 0
                },
                {
                    "sent": "This is the weaker notion, which is easier to deal with.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so to get pack MDP bounds from the regret bound, you can just plug in.",
                    "label": 1
                },
                {
                    "sent": "So instead of T the number of.",
                    "label": 0
                },
                {
                    "sent": "Times you choose a bad action and step the proof.",
                    "label": 0
                },
                {
                    "sent": "If you look carefully and proof in the proofs then you can see that you can replace the T by the number of times you choose a bad action epsilon bear action.",
                    "label": 0
                },
                {
                    "sent": "So you just solve gamma T equals epsilon T because epsilon is the accuracy you care for and T is now not the total number of the total time, but it's the number of steps where you do choose an epsilon correction.",
                    "label": 0
                },
                {
                    "sent": "And if solve that then you get something like this.",
                    "label": 0
                },
                {
                    "sent": "For example, which?",
                    "label": 0
                },
                {
                    "sent": "Is the same as the bound by seat and cheapest Barry.",
                    "label": 0
                },
                {
                    "sent": "As I see my last year, which also have impact MDP pound which has this dependency.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just comment.",
                    "label": 0
                },
                {
                    "sent": "That you can convert regret bounds in packing the bounds quite easily.",
                    "label": 1
                },
                {
                    "sent": "And therefore you should work on the regret.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pounds.",
                    "label": 0
                },
                {
                    "sent": "So how does the algorithm work?",
                    "label": 0
                },
                {
                    "sent": "So we use an optimistic algorithm.",
                    "label": 1
                },
                {
                    "sent": "Their optimistic algorithm assumes the best possible MVP, which is consistent with the data observed so far.",
                    "label": 1
                },
                {
                    "sent": "Consistent needs to be defined and chooses the optimal policy for the best possible MVP.",
                    "label": 1
                },
                {
                    "sent": "In this set of consistent MVP's.",
                    "label": 0
                },
                {
                    "sent": "So there is standard approach.",
                    "label": 0
                },
                {
                    "sent": "There's some some care is needed, so if you know about the bandit problem then independent problem you can in each trial choose the.",
                    "label": 0
                },
                {
                    "sent": "ARM, which has the highest upper confidence value, which is the most optimistic value consistent with your data?",
                    "label": 0
                },
                {
                    "sent": "But if you do MDP stand the rewards adelaid so you don't want to switch policy too often, so you need to keep going with the same policy for some time and this makes yeah.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit more complicated, just very little.",
                    "label": 0
                },
                {
                    "sent": "So the main question is how to define consistent MVP's.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What you need for the proof to go through is that with high probability, this set of consistent MVP's contains the true MVP, so you want to have for this consistent set.",
                    "label": 1
                },
                {
                    "sent": "This kind of confidence set of MVP's, so you want to have a high probability that your true MVP's within this set, and to get good regret bounds you want this consistent set to be small because the smaller set is the tighter bounds you will get so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how yeah, this is the intuition about optimistic MDP's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if about optimistic policies.",
                    "label": 1
                },
                {
                    "sent": "If you have an optimistic policy and you have small regrets, then you're fine.",
                    "label": 1
                },
                {
                    "sent": "If you have high regret, the optimism was.",
                    "label": 0
                },
                {
                    "sent": "Wrong and you learn something about your assumptions so you learn something more.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic intuition about optimistic policies, which is quite standard.",
                    "label": 1
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "10 minutes to cook.",
                    "label": 0
                },
                {
                    "sent": "Come on.",
                    "label": 0
                },
                {
                    "sent": "It's not that bad.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It gets worse.",
                    "label": 0
                },
                {
                    "sent": "Do I need the temperature in the room this happened?",
                    "label": 0
                },
                {
                    "sent": "Electric temperature.",
                    "label": 0
                },
                {
                    "sent": "So I'm fine to increase it a little.",
                    "label": 0
                },
                {
                    "sent": "So how do we choose this confidence, confident MDP?",
                    "label": 0
                },
                {
                    "sent": "So yeah, very very.",
                    "label": 0
                },
                {
                    "sent": "Easy way to do it is to put bounds on how much the optimistic and the transition probabilities of the optimistic NDPS might deviate from the empirical estimates and so and this is 1 possibility to do it and you can show that with high probability the true transition probabilities satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "The only drawback is that we have the S here which gives us gives gives us this additional squared aspect and.",
                    "label": 0
                },
                {
                    "sent": "In the bonds.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately, we don't know how to get rid of it and I will give you some indication why it's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "For the discounted case, we can do it better, and so the first one is quite simple.",
                    "label": 0
                },
                {
                    "sent": "The second one is an interesting one, so this is not a very simple confidence interval because it has the distance between the optimistic transition probabilities and empirical transition probabilities, but multiplied with the optimistic value you would get if you use this probabilities transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a weird notion.",
                    "label": 0
                },
                {
                    "sent": "So you assume an optimistic MDP and it needs to satisfy this assumption.",
                    "label": 0
                },
                {
                    "sent": "This is the optimal policy for the optimistic MDP, so the value of the optimistic policy for the value of the optimal policy for the optimistic MDP.",
                    "label": 0
                },
                {
                    "sent": "And you can show that it's easy to show that that the true MVP satisfies this.",
                    "label": 0
                },
                {
                    "sent": "So if you replace that image, the true values so that rule transition probability is the true value function, then this is satisfied.",
                    "label": 0
                },
                {
                    "sent": "And for the discounted case, this is sufficient to prove the bounds.",
                    "label": 0
                },
                {
                    "sent": "Quiet.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Night gets little, so the basic inequality or the basic term you need to bound in the undiscounted case is this one.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Basically this is by how much do you overestimate the true rewards of the optimistic MDP, so this is the average reward of the optimistic MDP, and these are the true rewards and these are multiplied by the times you visit the states, so in each state to get a different reward depending on the action.",
                    "label": 0
                },
                {
                    "sent": "And this is basically the overestimation where N is the expected number of visits.",
                    "label": 0
                },
                {
                    "sent": "To the states.",
                    "label": 0
                },
                {
                    "sent": "By the optimistic policy in the true MVP.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You can massage this a little and finally what comes out is that you need to bound.",
                    "label": 0
                },
                {
                    "sent": "The hard part is this one.",
                    "label": 0
                },
                {
                    "sent": "So you need to bound this term and this distant if this would be constant or a fixed vector, you could use a general kind of bound to get tight bounds, but since this depends on that.",
                    "label": 0
                },
                {
                    "sent": "You cannot do that.",
                    "label": 0
                },
                {
                    "sent": "So you, and therefore you need to do something more crude and estimating the one norm of distant Infinity norm of this.",
                    "label": 0
                },
                {
                    "sent": "And this is because you're estimating the one norm here.",
                    "label": 0
                },
                {
                    "sent": "You get this additional squared X Factor.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the discounted case, it's easier.",
                    "label": 0
                },
                {
                    "sent": "So maybe I just point out the main things.",
                    "label": 0
                },
                {
                    "sent": "Since I have very little time.",
                    "label": 0
                },
                {
                    "sent": "Really have only 10 minutes left.",
                    "label": 0
                },
                {
                    "sent": "Less now.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe you need to resolve this now.",
                    "label": 0
                },
                {
                    "sent": "I have 6 minutes when you have since you started 10 minutes since you said 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "So what you so if you do the calculation, you find out that you get something here and this is what I use for their confidence in for the confident or for the consistent set of MDP's.",
                    "label": 0
                },
                {
                    "sent": "So this can be dealed with the definition of the consistent MVP's and this is something which would be hard to deal with and this resembles some of this term here.",
                    "label": 0
                },
                {
                    "sent": "But this is a recursion because this is the same like this.",
                    "label": 0
                },
                {
                    "sent": "And we can just pull it on the other side and divide by 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "And This is why we get tighter bounds.",
                    "label": 0
                },
                {
                    "sent": "So because for in the discounted case we have this contraction, so we have this recursion and we can and we can use this to get the tighter bounds.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I skip this so optimistic algorithm for autonomous exploration.",
                    "label": 1
                },
                {
                    "sent": "So coming back to the original problem, finding policies to reach the reachable states.",
                    "label": 1
                },
                {
                    "sent": "So the algorithm is very simple, so we keep a set of states which we already know that are reachable for, which you already know policy and the rest of the unknown states.",
                    "label": 0
                },
                {
                    "sent": "So we optimistically choose the closest state among the unknown states.",
                    "label": 0
                },
                {
                    "sent": "So for which we believe of given Arkansas optimistic estimates that we can reach this state, this state most quickly by some certain policy by some policy.",
                    "label": 0
                },
                {
                    "sent": "Then we run this policy, if then we test if this policy actually satisfied gives us a quick transition to this state.",
                    "label": 0
                },
                {
                    "sent": "So we run this policy for some time.",
                    "label": 0
                },
                {
                    "sent": "So sometimes K, which is basically.",
                    "label": 0
                },
                {
                    "sent": "Some logarithm of the confidence we want to get and just check if if we recently often get to the target state.",
                    "label": 0
                },
                {
                    "sent": "And if you do, we know that this is a good policy and we keep it.",
                    "label": 0
                },
                {
                    "sent": "If we don't reach this target states often enough and often enough means at least third of the times we try.",
                    "label": 0
                },
                {
                    "sent": "With this we got.",
                    "label": 0
                },
                {
                    "sent": "We repeat the loop and choose another state from the unknown from the unknown state.",
                    "label": 0
                },
                {
                    "sent": "Why does this work?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, first thing is through the testing, we can guarantee that if the test is successful then we are sure that this policy is a good policy.",
                    "label": 0
                },
                {
                    "sent": "So these constants are bit arbitrary and we can improve them.",
                    "label": 0
                },
                {
                    "sent": "So this is just to make the proof.",
                    "label": 0
                },
                {
                    "sent": "The argument is simple.",
                    "label": 0
                },
                {
                    "sent": "So if in a third of the cases we we reached the States, then we are reasonably sure that we can reach this state in a Dell steps.",
                    "label": 0
                },
                {
                    "sent": "The other question is how often?",
                    "label": 0
                },
                {
                    "sent": "Do do we have iteration where we don't find a good policy?",
                    "label": 0
                },
                {
                    "sent": "So basically I use useless iteration where we cannot find a good policy.",
                    "label": 0
                },
                {
                    "sent": "And this can be.",
                    "label": 0
                },
                {
                    "sent": "But this can be bounded by, by their regret.",
                    "label": 0
                },
                {
                    "sent": "So if we don't find the good policy, then we can translate this.",
                    "label": 0
                },
                {
                    "sent": "This trials to a modified MDP where we would get a reward if we reach the goal state.",
                    "label": 1
                },
                {
                    "sent": "And since we don't reach the goals that very often, we suffer some regret compared to an optimal policy which would reach this goal state.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rent.",
                    "label": 0
                },
                {
                    "sent": "Just basically plugging in the regret bound which looks a little different here because it's more complicated.",
                    "label": 1
                },
                {
                    "sent": "It's more subtle, So what you see is that we only we don't get this.",
                    "label": 0
                },
                {
                    "sent": "We get the square root is here and this is because we partition.",
                    "label": 0
                },
                {
                    "sent": "Now we have we follow policy only for a fixed number of steps for L steps.",
                    "label": 1
                },
                {
                    "sent": "No, after still 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "And if we partition the examples into L. Disjoint sets we can regain this independence and can get better bounds, so the prices that we need L times more examples.",
                    "label": 1
                },
                {
                    "sent": "So This is why the so.",
                    "label": 0
                },
                {
                    "sent": "This is why we need to divide the total number of examples by L. So, but this gives us this.",
                    "label": 0
                },
                {
                    "sent": "We get the square root S and on the other hand we get the L cubed here.",
                    "label": 0
                },
                {
                    "sent": "And yeah, if you do the calculation, you find that this is the number of exploration steps you need.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And skip this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This and this is the summary, so autonomous exploration is interesting, I hope.",
                    "label": 1
                },
                {
                    "sent": "At least the video should should convince you that this is indeed the case.",
                    "label": 0
                },
                {
                    "sent": "It's it's difficult to formalize.",
                    "label": 0
                },
                {
                    "sent": "Competitively.",
                    "label": 1
                },
                {
                    "sent": "We analyzed the concrete model for autonomous exploration for reaching states.",
                    "label": 0
                },
                {
                    "sent": "So which seems quite reasonable, because this is typically where you get a reward if you reach reaches certain state.",
                    "label": 1
                },
                {
                    "sent": "There's few cases where you actually need to have a trajectory which gives you high reward.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think it's a reasonable general setting.",
                    "label": 1
                },
                {
                    "sent": "Difficult cases cannot be learned, so you need to assume a bit more structure on your MDP.",
                    "label": 1
                },
                {
                    "sent": "So we do it by incremental optimistic exploration.",
                    "label": 1
                },
                {
                    "sent": "So we add states to the known to the set of known states.",
                    "label": 0
                },
                {
                    "sent": "Once one at a time, and performance analysis is through regret analysis or regret analysis, which is great.",
                    "label": 0
                },
                {
                    "sent": "And finally, apparently well, obviously there are other interesting learning modes, for example by invitation, so not everything is learned autonomously.",
                    "label": 0
                },
                {
                    "sent": "It seems there's also a lot of imitation learning going on.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yeah, and so the question is, is autonomous exploration is really useful?",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I work on the Markov process and this is the jewel of yours and I have been forced to consider that.",
                    "label": 0
                },
                {
                    "sent": "Using probability anymore not using probabilities, probability are super probability in your case.",
                    "label": 0
                },
                {
                    "sent": "Union B with A&B disjoint is not equal to pay of a plus because otherwise things were not working properly.",
                    "label": 0
                },
                {
                    "sent": "I've used this same problem, no no no.",
                    "label": 0
                },
                {
                    "sent": "Can I just ask?",
                    "label": 0
                },
                {
                    "sent": "I mean you will you when you update your models.",
                    "label": 0
                },
                {
                    "sent": "Is that you know that's after you've done your search for one states, then yes, all of that information update your.",
                    "label": 0
                },
                {
                    "sent": "Yeah so the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wood.",
                    "label": 0
                },
                {
                    "sent": "Run through one complete iteration, so would do the case steps here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then it would update.",
                    "label": 0
                },
                {
                    "sent": "In the simplest case, you could update more often, but yeah, in principle yes vote for the next day and then you look for the next stage right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah, you need to complete the test while you.",
                    "label": 0
                },
                {
                    "sent": "In principle, if you find out that your experience during this update during these trials indicates that you actually cannot use reach the state, you could stop early, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you could do that, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah you could do that, yeah?",
                    "label": 0
                },
                {
                    "sent": "You could cut down on that, yeah?",
                    "label": 0
                },
                {
                    "sent": "What is the message of the first part of your talk?",
                    "label": 0
                },
                {
                    "sent": "Because you're trying to discover skills and then you showed at the beginning of the problem very difficult.",
                    "label": 0
                },
                {
                    "sent": "Continue networking is ordering for the States and then you reach the solvable problem.",
                    "label": 0
                },
                {
                    "sent": "So what is the message about?",
                    "label": 0
                },
                {
                    "sent": "This connected to finding the skills.",
                    "label": 0
                },
                {
                    "sent": "So what does this say to us at this point?",
                    "label": 0
                },
                {
                    "sent": "Well, it says that.",
                    "label": 0
                },
                {
                    "sent": "At least our algorithm works only if you build skills incrementally.",
                    "label": 0
                },
                {
                    "sent": "So it means like a bottom up approach, yeah?",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the counter example, yeah, it's like a bottom up approach.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So although this might be doable.",
                    "label": 0
                },
                {
                    "sent": "You first need to learn this, which is actually.",
                    "label": 0
                },
                {
                    "sent": "Further, yeah.",
                    "label": 0
                },
                {
                    "sent": "This is so this and this is actually a complicated skill.",
                    "label": 0
                },
                {
                    "sent": "Which is hard to learn.",
                    "label": 0
                },
                {
                    "sent": "Is it true to consider the same question that he asked that these ordering of the states is like collapsing those boxes to no, not quite.",
                    "label": 0
                },
                {
                    "sent": "Because it says that that.",
                    "label": 0
                },
                {
                    "sent": "Not really, it really says that you can build skills incrementally.",
                    "label": 0
                },
                {
                    "sent": "You learn one skill and then you can extend it by a little bit.",
                    "label": 0
                },
                {
                    "sent": "That be that you first crudely learn the skills, and then you learn the refinement as you do it.",
                    "label": 0
                },
                {
                    "sent": "Quite often in sports, for example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So this will be the analogy to that.",
                    "label": 0
                },
                {
                    "sent": "No, here, here, here you would need to jump here.",
                    "label": 0
                },
                {
                    "sent": "So this is the first thing you can.",
                    "label": 0
                },
                {
                    "sent": "You need to learn.",
                    "label": 0
                },
                {
                    "sent": "You cannot learn the in between steps because yeah, yeah, yeah, you first need to go here and you need to learn a big step here and then you can go back with some reach all these.",
                    "label": 0
                },
                {
                    "sent": "We went from this.",
                    "label": 0
                },
                {
                    "sent": "You can derive all these skills so this this is not incremental.",
                    "label": 0
                },
                {
                    "sent": "You really need to do this big jump here, which is hard to learn.",
                    "label": 0
                },
                {
                    "sent": "So the in the other case where you have these incremental thing and even our algorithm does that.",
                    "label": 0
                },
                {
                    "sent": "So if you learn skills for the certain level L and then you increase the L then you will refine the skills.",
                    "label": 0
                },
                {
                    "sent": "Sandy didn't, and that's how you could extend to be sinetti, which which you know about.",
                    "label": 0
                },
                {
                    "sent": "So you first learn is small with unity and then you increase the L and and re learn and use what you already know and and and refine what you already know.",
                    "label": 0
                },
                {
                    "sent": "Do you can trust me?",
                    "label": 0
                },
                {
                    "sent": "There's a break now, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So you know if you have a hierarchy and we want to be kind of hierarchy can be composed or task to hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Want there you only way that we can take advantage of this hierarchy in learning and speed up our learning is behind.",
                    "label": 0
                },
                {
                    "sent": "If you have a state, if you have the abstraction abstraction hierarchy, just add more parameters and it's even worse than learning things.",
                    "label": 0
                },
                {
                    "sent": "Can we have a relation between that and what have you?",
                    "label": 0
                },
                {
                    "sent": "Not really, so this is not about abstraction.",
                    "label": 0
                },
                {
                    "sent": "This is not about this abstraction, no, I'm afraid not.",
                    "label": 0
                },
                {
                    "sent": "Not really, it's really just saying that you can incrementally add skills.",
                    "label": 0
                },
                {
                    "sent": "It's not about abstraction.",
                    "label": 0
                },
                {
                    "sent": "Tasks in which we can ignore?",
                    "label": 0
                },
                {
                    "sent": "Yeah, no.",
                    "label": 0
                },
                {
                    "sent": "This analysis is not about abstraction.",
                    "label": 0
                },
                {
                    "sent": "But if you think of your, you know states that you can learn ideas.",
                    "label": 0
                },
                {
                    "sent": "Just being states that describe rough skills and then the refinements of them are.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit like.",
                    "label": 0
                },
                {
                    "sent": "I mean you build the hierarchy into your face.",
                    "label": 0
                },
                {
                    "sent": "Well, it's hierarchical and in the sands of difficulty.",
                    "label": 0
                },
                {
                    "sent": "But that's not what you meant.",
                    "label": 0
                },
                {
                    "sent": "You meant you can use the same actions to reach a certain set of states.",
                    "label": 0
                },
                {
                    "sent": "Is that if you have to use.",
                    "label": 0
                },
                {
                    "sent": "Everything your state space every day then.",
                    "label": 0
                },
                {
                    "sent": "Decomposing the tasks and skills is not going to help.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not the way I think about abstraction about.",
                    "label": 0
                },
                {
                    "sent": "I think about abstraction.",
                    "label": 0
                },
                {
                    "sent": "If you can do the same thing to reach similar goals.",
                    "label": 0
                },
                {
                    "sent": "That's not what we do here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you.",
                    "label": 0
                }
            ]
        }
    }
}