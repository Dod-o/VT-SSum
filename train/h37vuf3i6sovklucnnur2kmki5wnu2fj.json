{
    "id": "h37vuf3i6sovklucnnur2kmki5wnu2fj",
    "title": "A Spectral Algorithm for Latent Dirichlet Allocation",
    "info": {
        "author": [
            "Daniel Hsu, Microsoft Research New England, Microsoft Research"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_hsu_algorithm/",
    "segmentation": [
        [
            "So we're going to be talking about LDA.",
            "This is the old version of LDA, the one from over 10 years ago.",
            "So this is a model for document corpus is we want to model documents as mixtures of bags of words, and these words are drawn from mixtures of topics.",
            "So in our model, the LDA model or simplified version of it.",
            "We have K different topics and different words and the distribution over topics for document is given from Adair Slay distribution.",
            "With a certain parameter Alpha, and then when you draw words from the document, you first draw a topic.",
            "One of these T1 through TL, and given the topic you draw word from distribution.",
            "These topic word distributions are given by beta one through beta K and these are just distributions over words and we're going to make a simplifying assumption that every document has L words.",
            "OK, so why are we looking at this really old model for topic modeling?",
            "One is we can maybe answer some questions about topic modeling, and two is still a very effective model.",
            "I think for a lot of applications and including modeling documents and also for these kind of mixed membership block models for Community detection."
        ],
        [
            "OK, so the main question we want to answer is whether we can learn LDA parameters from a corpus of documents and our answer is yes and there's a lot of empirical evidence for this.",
            "Given a lot of algorithms that have been developed over the years, we develop a new algorithm, But another question we can ask is how long do documents have to be in order to learn the parameters?",
            "And if you think about it, if you just had one word documents not very interesting, there's not a whole lot you can learn.",
            "So in that case you can't learn the parameters.",
            "He had just two word for document, though it turns out it's actually possible and this is some very recent work by Rucka and Moitra from Fox this year.",
            "And finally, if you have 3 words per document that was shown earlier this year in the Cold Paper, that is possible, at least when every document is just a mixture of 1 topic.",
            "So it's just only about one topic rather than the mixture of several topics.",
            "An in this work we show that will still with just three words per document, suffices to learn the parameters of LDA for any value of the parameter.",
            "An without any kind of separation conditions."
        ],
        [
            "OK, so how do we do this?",
            "Well, let me first state the main result.",
            "What we?",
            "Prove is that there's an efficient algorithm that, given a certain parameter A0 which is just the sum of the values of the dearsley parameter, and if you're given also the 3rd order across moments of the observations, the words and then this algorithm will recover the model parameters exactly.",
            "OK, and So what does it mean by having the 3rd order cross moments?",
            "Well, this is really just the frequencies that word triples in a document, so you look at how many times each triple of words appears in your document.",
            "Count those things up over all the documents in your corpus, and this gives you an estimate of the 3rd order across moments and so given this kind of result, you can also stay to sample complexity bound that's polynomial and all the relevant parameters of the problem.",
            "And it turns out the competition with this new algorithm is linear and just the number of non zero entries in the term document matrix.",
            "And also linear in the size of your vocabulary.",
            "So if you want to know more details please come by or poster W 66."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to be talking about LDA.",
                    "label": 0
                },
                {
                    "sent": "This is the old version of LDA, the one from over 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "So this is a model for document corpus is we want to model documents as mixtures of bags of words, and these words are drawn from mixtures of topics.",
                    "label": 0
                },
                {
                    "sent": "So in our model, the LDA model or simplified version of it.",
                    "label": 0
                },
                {
                    "sent": "We have K different topics and different words and the distribution over topics for document is given from Adair Slay distribution.",
                    "label": 0
                },
                {
                    "sent": "With a certain parameter Alpha, and then when you draw words from the document, you first draw a topic.",
                    "label": 0
                },
                {
                    "sent": "One of these T1 through TL, and given the topic you draw word from distribution.",
                    "label": 0
                },
                {
                    "sent": "These topic word distributions are given by beta one through beta K and these are just distributions over words and we're going to make a simplifying assumption that every document has L words.",
                    "label": 0
                },
                {
                    "sent": "OK, so why are we looking at this really old model for topic modeling?",
                    "label": 0
                },
                {
                    "sent": "One is we can maybe answer some questions about topic modeling, and two is still a very effective model.",
                    "label": 0
                },
                {
                    "sent": "I think for a lot of applications and including modeling documents and also for these kind of mixed membership block models for Community detection.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the main question we want to answer is whether we can learn LDA parameters from a corpus of documents and our answer is yes and there's a lot of empirical evidence for this.",
                    "label": 0
                },
                {
                    "sent": "Given a lot of algorithms that have been developed over the years, we develop a new algorithm, But another question we can ask is how long do documents have to be in order to learn the parameters?",
                    "label": 1
                },
                {
                    "sent": "And if you think about it, if you just had one word documents not very interesting, there's not a whole lot you can learn.",
                    "label": 0
                },
                {
                    "sent": "So in that case you can't learn the parameters.",
                    "label": 0
                },
                {
                    "sent": "He had just two word for document, though it turns out it's actually possible and this is some very recent work by Rucka and Moitra from Fox this year.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you have 3 words per document that was shown earlier this year in the Cold Paper, that is possible, at least when every document is just a mixture of 1 topic.",
                    "label": 0
                },
                {
                    "sent": "So it's just only about one topic rather than the mixture of several topics.",
                    "label": 1
                },
                {
                    "sent": "An in this work we show that will still with just three words per document, suffices to learn the parameters of LDA for any value of the parameter.",
                    "label": 1
                },
                {
                    "sent": "An without any kind of separation conditions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, let me first state the main result.",
                    "label": 1
                },
                {
                    "sent": "What we?",
                    "label": 1
                },
                {
                    "sent": "Prove is that there's an efficient algorithm that, given a certain parameter A0 which is just the sum of the values of the dearsley parameter, and if you're given also the 3rd order across moments of the observations, the words and then this algorithm will recover the model parameters exactly.",
                    "label": 1
                },
                {
                    "sent": "OK, and So what does it mean by having the 3rd order cross moments?",
                    "label": 0
                },
                {
                    "sent": "Well, this is really just the frequencies that word triples in a document, so you look at how many times each triple of words appears in your document.",
                    "label": 1
                },
                {
                    "sent": "Count those things up over all the documents in your corpus, and this gives you an estimate of the 3rd order across moments and so given this kind of result, you can also stay to sample complexity bound that's polynomial and all the relevant parameters of the problem.",
                    "label": 1
                },
                {
                    "sent": "And it turns out the competition with this new algorithm is linear and just the number of non zero entries in the term document matrix.",
                    "label": 0
                },
                {
                    "sent": "And also linear in the size of your vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know more details please come by or poster W 66.",
                    "label": 0
                }
            ]
        }
    }
}