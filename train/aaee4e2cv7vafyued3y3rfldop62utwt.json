{
    "id": "aaee4e2cv7vafyued3y3rfldop62utwt",
    "title": "Function Factorization Using Warped Gaussian Processes",
    "info": {
        "author": [
            "Mikkel N. Schmidt, Department of Engineering, University of Cambridge"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_schmidt_ffuw/",
    "segmentation": [
        [
            "So I'm Michael.",
            "I'm postdoc at University of Cambridge."
        ],
        [
            "And.",
            "I'm going to talk about a new idea called function Factorization an based on what Gaussian processes.",
            "So the idea of function factorization is basically a new approach to doing nonlinear regression when we know something more about the functions that we want to do regression on.",
            "So the basic setup is that we have some observed input output points, a data set consisting of outputs, N and inputs X, and we have a number of those observed.",
            "Now we want to estimate or compute some sort of function that Maps from the input SpaceX to the outer space R, and we want to do this in order to make predictions of what we might be at some new some new point X star.",
            "So that's basically the general setup and the key idea here is to try to approximate this.",
            "This may be complex regression function, why?",
            "To make this complex function which might be on a high dimensional space by a sum of products of similar functions over subspaces of the full space.",
            "That's sort of the key idea.",
            "And the motivation behind doing this?"
        ],
        [
            "It's sort of to try to take the best from two different worlds, so it's sort of generalize and combines ideas from matrix and tensor factorization, and ideas from Bashan on parametric regression.",
            "So major matrix sensor factorization are very useful models when data has a specific structure and it's basically just generalized generalized multiple linear model that's linear in a number of effectors.",
            "And patient regression here.",
            "Specifically, we look at warp Gaussian process regression, which we use us.",
            "Try to combine these two ideas into one framework.",
            "And.",
            "Obviously, Gaussian process regression provides a very flexible framework for doing regression and modeling functions."
        ],
        [
            "So in a generalized multiple linear model, data is described as our model is a number of factors, and these factors multiplied together an added together to form the output and so basically the output can be modeled as any combination of multiplications and additions of some underlying functions F here.",
            "And in addition to be very flexible framework that it can also for many types of data set, being an interpretable model because these factors can have some sort of physical meaning, we can try to understand what the factors mean based on the data.",
            "M."
        ],
        [
            "So this octopus leaning octopus like equation here shows the basic equation in function factorization.",
            "So the idea is to model and output Y of the function Y of X.",
            "As some of a number of components we have, in this case K components and these K additive components are a product.",
            "Over I factors and these factors are functions of inputs and these inputs are not the full input, but just a part of the input.",
            "An input defined over subspace of the full input space.",
            "So basically what we want to learn here instead of learning just one big complicated function why we want to learn a number K * I more simple functions FYK defined over these subspaces.",
            "And these subspaces can be defined in some meaningful way based on whatever the data is, right?",
            "They can either be like restrictions of the full subspace, or they don't have to be different.",
            "Some of them could be the same stuff is actually that would allow the model to have sort of.",
            "Stuff like product of the over the same subspace, which can be useful in some in some models alright?"
        ],
        [
            "So let me try to motivate why this model is interesting by trying to compare it with matrix factorization.",
            "So matrix factorization, the basic equation is what we have up here.",
            "Y equals matrix Y equals the product of two matrices.",
            "Here F1 and F2.",
            "Another way to look at it is to take a look at a single element in this in a matrix Y.",
            "We call this YN one and two.",
            "And in this basic matrix multiplication, that's modeled as a sum, in this case over 2 components, which are the products of two factors which are just vectors, FF&IIK.",
            "So we can look at this as we have some data points defined which lie on a grid defined by two indexes indices and one and two.",
            "And these data points are modeled as a sum of outer products of vectors.",
            "So that's sort of a general way to look at matrix factorization, and this extends as well to tensor factorizations, where we have where each other progress isn't isn't 3 dimensional outer products or high dimensional?"
        ],
        [
            "Send function factorization.",
            "The model is very similar.",
            "We now have data points which are just.",
            "Was just lying in any initial SpaceX, and here we just for just visualize it by some space divided into X1 and X2 and they don't have to run.",
            "It requires a lion and grid.",
            "And instead of modeling the data points as some of our products of vectors, we model this as sums of products of functions.",
            "So the basic equations are very similar.",
            "Instead of having.",
            "To learn in matrix factorization, a number of.",
            "Matrices or elements of matrices F. Here we learn functions F."
        ],
        [
            "So this slide is to try to compare function factorization to Gaussian process regression.",
            "I'm so.",
            "Function factorization is of course most relevant if you have data where you assume that there is some sort of factorized structuring the data we want to capture.",
            "So in this case I just took a very simple data set or data set, just consisting of two cosine functions.",
            "And just plotted here as a control plot over X1 and X2.",
            "And let's say we just observe this function as.",
            "It only is a subset of points, just points indicated here by the circles.",
            "So if we if we do Gaussian process regression on this data set, we've got a function out that matches, matches the input points quite well in the close vicinity of the observed data points, but as so Gaussian process regression matches very well along these two axis.",
            "Here where we actually have observations.",
            "But as we move away from.",
            "From where we have observations, the Gaussian process doesn't have any more information about what the function look like.",
            "Looks like there, so it reverts back to its mean, which is the false practices O mean in this case.",
            "So function factorization we simply just.",
            "We assume that there's this factorized structure.",
            "And so we fit these Gaussian processes on each.",
            "Each of these dimensions.",
            "And since we assume that this factor structure exists, this function factorization model will be able to match the function quite well even in regions far away from the observations.",
            "Of course, assumes that this factor structure really is there in the data."
        ],
        [
            "So in the.",
            "In the paper, we mainly focus on on modeling these latent functions by work Gaussian processes, but basically in this framework we could use any sort of prior over functions.",
            "So the most simple thing would be to just use a parametric family of functions.",
            "Of course that has limited flexibility, but that might be useful for if we have some more knowledge that we know the functions might be linear or quadratic whatever.",
            "We also use Gaussian processes which are very flexible and.",
            "But they're also sort of limited by the fact that the outputs are joint Gaussian.",
            "So what we did was we.",
            "You say even more flexible framework.",
            "The warp Gaussian process due to Snelson.",
            "I'm.",
            "In which we take a Gaussian process and map that through a nonlinear warping function.",
            "So this work caching process."
        ],
        [
            "Framework was proposed by Snelson in 1999 I guess.",
            "I'm.",
            "And.",
            "Is the basic setup is simply just to to model function F of X by taking mapping a Gaussian process GFX through some warp function so the work function H has?",
            "Number of parameters.",
            "And the Gaussian process is parameterized by mean function and a covariance function, and these can as well also have have parameters.",
            "And then all these promises of the word function and the current function as an artist.",
            "Learn jointly in this in this framework."
        ],
        [
            "So the way we do inference in the model is by Hamiltonian Markov chain Monte Carlo.",
            "Another proposed by Duane.",
            "So the idea is to simply take all the parameters in the model and just integrate them out in order to make predictions about.",
            "In order to make a predicted distribution at a new new point.",
            "So the premises model include the parameters of likelihood function, which could be something like a noise variance and or other parameters we might have and it includes the latent variables in the Gaussian process and the parameters of the covariance function and the parameters of this warping function.",
            "In order to do inference by Hamiltonian MCMC, we just need to be able to compute the gradients with respect to all the parameters.",
            "The gradient of the lock posterior distribution, and that's that's possible to do in this model.",
            "Alright, so so."
        ],
        [
            "Illustrate the results of this model.",
            "I did some experiments on datasets.",
            "Do tubulin Jacobson on a food sign data set.",
            "The data set consists of measurements of the color of beef as a changes during storage.",
            "The measurements that the different conditions are storage time and temperature and the oxygen content inside the storage facility and exposure to light and so on.",
            "And the idea is to try to develop better storage conditions for just to know how to store meat and the color is one of the most important factors for selling meat.",
            "So yes, it's very important for food science and so the idea here is to use this model to predict the color based on the condition."
        ],
        [
            "And the data is a.",
            "Is A5 way array data so it can be stored as a five ways tensor where each element gives the measurement of how read the meters measured on a non negative scale.",
            "This five weight sensor has 60% missing values and is.",
            "This data set has previously been analyzed by a central factorization model.",
            "The Parafac model, which has been modified by born Jacobson to handle missing data using M iterations.",
            "So to analyze this data set using function factorization, we don't need any special.",
            "Thing to handle the missing data, because there is no requirement that data has to be integrated, just it's just data points in some space.",
            "So we used a RBF covariance function and.",
            "Which was just parameters biolink scale and we use the warp function which map the.",
            "Map the Gaussian process onto a non negative to non negative real numbers which also has some parameters."
        ],
        [
            "And then we we we learned a factorization of this function is.",
            "Color of meat function.",
            "So the results we results using paraphrase, which we try to repeat the experiments of Bone Jacobson in their original work on this data set that we came to approximately the same results for a one component parafac model does quite quite badly, and two components modeled us quite well models the data quite well, and it also has to in food science that had sort of an intuitive explanation of what how these factors.",
            "What these factors meant and how.",
            "The two component model might be a good model in this case for three component model we see that the cross validation error on this data set is quite poor.",
            "Again, this is due to overfitting.",
            "So as a model increases it becomes too flexible and since the perfect model is just fitted, it overfits.",
            "In this case we also tried just doing Gaussian process regression.",
            "And it also gave quite good results.",
            "I'm the chairman.",
            "Over model is resulting from the original Born Jacobson paper, which is a restricted parafac model, and it also does reasonably well.",
            "This is very similar to the two factor 2 two component parafac model.",
            "So for the function factorization.",
            "Using just one component, it's basically the same results as Parafac model, which is also a special case of the function factorization framework.",
            "But when we get more components at, we get quite a lot better results, both due to the added flexibility of having more flexible model.",
            "But also we don't see any problems with overfitting, so we can we can have more components without overfitting because every all the parameters are integrated out.",
            "So basically we don't do fitting so we can't do overfitting.",
            "I'm.",
            "Yeah, so this."
        ],
        [
            "To summarize, So what we propose is a new approach to nonlinear regression, which is useful when we know something more about the data set, namely that it has some sort of factory structure, and we can use.",
            "To exploit that in the in the regression setting so it generalizes matrix and tensor factorization instead of factorizing a matrix as function as sorry as vectors reflectorized function as a part of the functions.",
            "It's based on what Gaussian process priors, but it would be equally well in the same framework.",
            "Grease other private server functions and we do full Bayesian inference using Hamiltonian MCMC and integrate all out old parameters and on this data set we've shown that we can outperform both terrifying and Gaussian process regression.",
            "Right?"
        ],
        [
            "Yeah, thank you.",
            "Before we go.",
            "So two years ago there was this paper.",
            "Which seems very very related.",
            "If not, can you state the difference between will see with you also that one?",
            "I'm not sure what's the author.",
            "Another person is basically.",
            "It's more or less the same thing on.",
            "I'm not familiar with that paper though.",
            "Tip this algorithm does a lot worse than.",
            "Compared to what?",
            "Actually, no, because.",
            "So the ones we compared to is Parafac model which was fitted model which has some problem for overfitting.",
            "So basically we can always do as well as perfect because we don't we can't overfit, so we always always going to be better than that.",
            "So the added flexibility.",
            "The only problem we might run into compared in comparison with the perfect model would be.",
            "The MCMC inference might be a bit difficult or too slow, so, but otherwise we wouldn't have any problems compared to paraphrase compression process regression.",
            "Again, this is this model is simply just more and more complex, so it more less control back to the Gaussian process regression model as well.",
            "You mentioned that the different functions could have different supports, right?",
            "So they're looking at different subsets of the features, yes?",
            "How would you go back to?",
            "Yeah, so in this in this the this beef data set each each function was fitted on each of these storage conditions.",
            "So one function was over the oxygen content and one function was over just the lights and so on.",
            "So it's just.",
            "Function of two variables or anything like that.",
            "So the interaction terms come come come into play because all these functions nonlinear functions are multiplied together in model, so that's the interaction between the.",
            "So basically taking out all the interactions and put them into multiplicative terms.",
            "And basically a sum of products of nonlinear functions can do any interaction terms.",
            "Serve you.",
            "I mean what sort of range of datasets would be looked at?",
            "Well, basically I started looking at a full size datasets because there are quite small and.",
            "It's not a secret that Hamiltonian retaining wall can be quite slow sometimes, so this is I think this idea is mainly interesting if you want to really like.",
            "Detailed analysis of possibly small data set.",
            "We want to have like full posterior distributions and try to understand what's going on in the model.",
            "So compared to the previous results on these types of datasets, just shown that, well, this sort of seems to be a linear relationship between the oxygen content in the storage and the color of the beef.",
            "But now we can see where we get out.",
            "Here is a full posterior distribution of this function that relates these two policies.",
            "So that's sort of adds to the understanding, and it because of the inference procedures that it doesn't scale that well up to large datasets.",
            "Looking at other scale things up.",
            "Yeah, I'm concentrating on small problems for now, but I think the main advantage here is basically that we can.",
            "Since this is a very sort of complicated model that has lots of promises, a lot of structures, so it's really.",
            "It's really important that we do a probation inference, and since that's intractable, we need to do some other sampling.",
            "Or maybe, maybe we could look at doing like expectation propagation, or something else in this subset models, but I'm not looking at it at the moment.",
            "Yep.",
            "Tried for instance back in your toy problem with two cows, two cosines.",
            "Have you tried to verify that you still recover something reasonable if the data points are sampled on sort of non natural axes?",
            "Like if you have coordinate transformations here, same problem.",
            "Just ask 1X2 and still get something done with this.",
            "I haven't, I haven't tried that specifically, but the main point is that you need to really.",
            "Important point is to specify these subspaces that have these over, which this factory structure exists.",
            "So if you specify that correctly where the data point is sample isn't that important.",
            "As long as you sort of get the right axis for the factorization, two to work on, so that's something you need to specify in advance, so that's kind of information.",
            "We need to have.",
            "Intuition that this is an important variable.",
            "Absolutely yes.",
            "Question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm Michael.",
                    "label": 0
                },
                {
                    "sent": "I'm postdoc at University of Cambridge.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about a new idea called function Factorization an based on what Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So the idea of function factorization is basically a new approach to doing nonlinear regression when we know something more about the functions that we want to do regression on.",
                    "label": 0
                },
                {
                    "sent": "So the basic setup is that we have some observed input output points, a data set consisting of outputs, N and inputs X, and we have a number of those observed.",
                    "label": 0
                },
                {
                    "sent": "Now we want to estimate or compute some sort of function that Maps from the input SpaceX to the outer space R, and we want to do this in order to make predictions of what we might be at some new some new point X star.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the general setup and the key idea here is to try to approximate this.",
                    "label": 0
                },
                {
                    "sent": "This may be complex regression function, why?",
                    "label": 1
                },
                {
                    "sent": "To make this complex function which might be on a high dimensional space by a sum of products of similar functions over subspaces of the full space.",
                    "label": 1
                },
                {
                    "sent": "That's sort of the key idea.",
                    "label": 0
                },
                {
                    "sent": "And the motivation behind doing this?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's sort of to try to take the best from two different worlds, so it's sort of generalize and combines ideas from matrix and tensor factorization, and ideas from Bashan on parametric regression.",
                    "label": 1
                },
                {
                    "sent": "So major matrix sensor factorization are very useful models when data has a specific structure and it's basically just generalized generalized multiple linear model that's linear in a number of effectors.",
                    "label": 0
                },
                {
                    "sent": "And patient regression here.",
                    "label": 1
                },
                {
                    "sent": "Specifically, we look at warp Gaussian process regression, which we use us.",
                    "label": 0
                },
                {
                    "sent": "Try to combine these two ideas into one framework.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Obviously, Gaussian process regression provides a very flexible framework for doing regression and modeling functions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a generalized multiple linear model, data is described as our model is a number of factors, and these factors multiplied together an added together to form the output and so basically the output can be modeled as any combination of multiplications and additions of some underlying functions F here.",
                    "label": 1
                },
                {
                    "sent": "And in addition to be very flexible framework that it can also for many types of data set, being an interpretable model because these factors can have some sort of physical meaning, we can try to understand what the factors mean based on the data.",
                    "label": 0
                },
                {
                    "sent": "M.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this octopus leaning octopus like equation here shows the basic equation in function factorization.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to model and output Y of the function Y of X.",
                    "label": 0
                },
                {
                    "sent": "As some of a number of components we have, in this case K components and these K additive components are a product.",
                    "label": 0
                },
                {
                    "sent": "Over I factors and these factors are functions of inputs and these inputs are not the full input, but just a part of the input.",
                    "label": 0
                },
                {
                    "sent": "An input defined over subspace of the full input space.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to learn here instead of learning just one big complicated function why we want to learn a number K * I more simple functions FYK defined over these subspaces.",
                    "label": 0
                },
                {
                    "sent": "And these subspaces can be defined in some meaningful way based on whatever the data is, right?",
                    "label": 0
                },
                {
                    "sent": "They can either be like restrictions of the full subspace, or they don't have to be different.",
                    "label": 0
                },
                {
                    "sent": "Some of them could be the same stuff is actually that would allow the model to have sort of.",
                    "label": 0
                },
                {
                    "sent": "Stuff like product of the over the same subspace, which can be useful in some in some models alright?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me try to motivate why this model is interesting by trying to compare it with matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "So matrix factorization, the basic equation is what we have up here.",
                    "label": 1
                },
                {
                    "sent": "Y equals matrix Y equals the product of two matrices.",
                    "label": 0
                },
                {
                    "sent": "Here F1 and F2.",
                    "label": 0
                },
                {
                    "sent": "Another way to look at it is to take a look at a single element in this in a matrix Y.",
                    "label": 0
                },
                {
                    "sent": "We call this YN one and two.",
                    "label": 0
                },
                {
                    "sent": "And in this basic matrix multiplication, that's modeled as a sum, in this case over 2 components, which are the products of two factors which are just vectors, FF&IIK.",
                    "label": 0
                },
                {
                    "sent": "So we can look at this as we have some data points defined which lie on a grid defined by two indexes indices and one and two.",
                    "label": 0
                },
                {
                    "sent": "And these data points are modeled as a sum of outer products of vectors.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a general way to look at matrix factorization, and this extends as well to tensor factorizations, where we have where each other progress isn't isn't 3 dimensional outer products or high dimensional?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Send function factorization.",
                    "label": 0
                },
                {
                    "sent": "The model is very similar.",
                    "label": 0
                },
                {
                    "sent": "We now have data points which are just.",
                    "label": 0
                },
                {
                    "sent": "Was just lying in any initial SpaceX, and here we just for just visualize it by some space divided into X1 and X2 and they don't have to run.",
                    "label": 0
                },
                {
                    "sent": "It requires a lion and grid.",
                    "label": 0
                },
                {
                    "sent": "And instead of modeling the data points as some of our products of vectors, we model this as sums of products of functions.",
                    "label": 0
                },
                {
                    "sent": "So the basic equations are very similar.",
                    "label": 0
                },
                {
                    "sent": "Instead of having.",
                    "label": 0
                },
                {
                    "sent": "To learn in matrix factorization, a number of.",
                    "label": 1
                },
                {
                    "sent": "Matrices or elements of matrices F. Here we learn functions F.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this slide is to try to compare function factorization to Gaussian process regression.",
                    "label": 1
                },
                {
                    "sent": "I'm so.",
                    "label": 0
                },
                {
                    "sent": "Function factorization is of course most relevant if you have data where you assume that there is some sort of factorized structuring the data we want to capture.",
                    "label": 0
                },
                {
                    "sent": "So in this case I just took a very simple data set or data set, just consisting of two cosine functions.",
                    "label": 0
                },
                {
                    "sent": "And just plotted here as a control plot over X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "And let's say we just observe this function as.",
                    "label": 0
                },
                {
                    "sent": "It only is a subset of points, just points indicated here by the circles.",
                    "label": 0
                },
                {
                    "sent": "So if we if we do Gaussian process regression on this data set, we've got a function out that matches, matches the input points quite well in the close vicinity of the observed data points, but as so Gaussian process regression matches very well along these two axis.",
                    "label": 0
                },
                {
                    "sent": "Here where we actually have observations.",
                    "label": 0
                },
                {
                    "sent": "But as we move away from.",
                    "label": 0
                },
                {
                    "sent": "From where we have observations, the Gaussian process doesn't have any more information about what the function look like.",
                    "label": 0
                },
                {
                    "sent": "Looks like there, so it reverts back to its mean, which is the false practices O mean in this case.",
                    "label": 0
                },
                {
                    "sent": "So function factorization we simply just.",
                    "label": 0
                },
                {
                    "sent": "We assume that there's this factorized structure.",
                    "label": 0
                },
                {
                    "sent": "And so we fit these Gaussian processes on each.",
                    "label": 0
                },
                {
                    "sent": "Each of these dimensions.",
                    "label": 0
                },
                {
                    "sent": "And since we assume that this factor structure exists, this function factorization model will be able to match the function quite well even in regions far away from the observations.",
                    "label": 0
                },
                {
                    "sent": "Of course, assumes that this factor structure really is there in the data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "In the paper, we mainly focus on on modeling these latent functions by work Gaussian processes, but basically in this framework we could use any sort of prior over functions.",
                    "label": 0
                },
                {
                    "sent": "So the most simple thing would be to just use a parametric family of functions.",
                    "label": 0
                },
                {
                    "sent": "Of course that has limited flexibility, but that might be useful for if we have some more knowledge that we know the functions might be linear or quadratic whatever.",
                    "label": 0
                },
                {
                    "sent": "We also use Gaussian processes which are very flexible and.",
                    "label": 1
                },
                {
                    "sent": "But they're also sort of limited by the fact that the outputs are joint Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So what we did was we.",
                    "label": 0
                },
                {
                    "sent": "You say even more flexible framework.",
                    "label": 0
                },
                {
                    "sent": "The warp Gaussian process due to Snelson.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "In which we take a Gaussian process and map that through a nonlinear warping function.",
                    "label": 0
                },
                {
                    "sent": "So this work caching process.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Framework was proposed by Snelson in 1999 I guess.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Is the basic setup is simply just to to model function F of X by taking mapping a Gaussian process GFX through some warp function so the work function H has?",
                    "label": 0
                },
                {
                    "sent": "Number of parameters.",
                    "label": 0
                },
                {
                    "sent": "And the Gaussian process is parameterized by mean function and a covariance function, and these can as well also have have parameters.",
                    "label": 1
                },
                {
                    "sent": "And then all these promises of the word function and the current function as an artist.",
                    "label": 0
                },
                {
                    "sent": "Learn jointly in this in this framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we do inference in the model is by Hamiltonian Markov chain Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "Another proposed by Duane.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to simply take all the parameters in the model and just integrate them out in order to make predictions about.",
                    "label": 0
                },
                {
                    "sent": "In order to make a predicted distribution at a new new point.",
                    "label": 0
                },
                {
                    "sent": "So the premises model include the parameters of likelihood function, which could be something like a noise variance and or other parameters we might have and it includes the latent variables in the Gaussian process and the parameters of the covariance function and the parameters of this warping function.",
                    "label": 0
                },
                {
                    "sent": "In order to do inference by Hamiltonian MCMC, we just need to be able to compute the gradients with respect to all the parameters.",
                    "label": 0
                },
                {
                    "sent": "The gradient of the lock posterior distribution, and that's that's possible to do in this model.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Illustrate the results of this model.",
                    "label": 0
                },
                {
                    "sent": "I did some experiments on datasets.",
                    "label": 0
                },
                {
                    "sent": "Do tubulin Jacobson on a food sign data set.",
                    "label": 0
                },
                {
                    "sent": "The data set consists of measurements of the color of beef as a changes during storage.",
                    "label": 1
                },
                {
                    "sent": "The measurements that the different conditions are storage time and temperature and the oxygen content inside the storage facility and exposure to light and so on.",
                    "label": 0
                },
                {
                    "sent": "And the idea is to try to develop better storage conditions for just to know how to store meat and the color is one of the most important factors for selling meat.",
                    "label": 0
                },
                {
                    "sent": "So yes, it's very important for food science and so the idea here is to use this model to predict the color based on the condition.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the data is a.",
                    "label": 0
                },
                {
                    "sent": "Is A5 way array data so it can be stored as a five ways tensor where each element gives the measurement of how read the meters measured on a non negative scale.",
                    "label": 0
                },
                {
                    "sent": "This five weight sensor has 60% missing values and is.",
                    "label": 1
                },
                {
                    "sent": "This data set has previously been analyzed by a central factorization model.",
                    "label": 1
                },
                {
                    "sent": "The Parafac model, which has been modified by born Jacobson to handle missing data using M iterations.",
                    "label": 1
                },
                {
                    "sent": "So to analyze this data set using function factorization, we don't need any special.",
                    "label": 1
                },
                {
                    "sent": "Thing to handle the missing data, because there is no requirement that data has to be integrated, just it's just data points in some space.",
                    "label": 0
                },
                {
                    "sent": "So we used a RBF covariance function and.",
                    "label": 0
                },
                {
                    "sent": "Which was just parameters biolink scale and we use the warp function which map the.",
                    "label": 0
                },
                {
                    "sent": "Map the Gaussian process onto a non negative to non negative real numbers which also has some parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we we we learned a factorization of this function is.",
                    "label": 0
                },
                {
                    "sent": "Color of meat function.",
                    "label": 0
                },
                {
                    "sent": "So the results we results using paraphrase, which we try to repeat the experiments of Bone Jacobson in their original work on this data set that we came to approximately the same results for a one component parafac model does quite quite badly, and two components modeled us quite well models the data quite well, and it also has to in food science that had sort of an intuitive explanation of what how these factors.",
                    "label": 0
                },
                {
                    "sent": "What these factors meant and how.",
                    "label": 0
                },
                {
                    "sent": "The two component model might be a good model in this case for three component model we see that the cross validation error on this data set is quite poor.",
                    "label": 0
                },
                {
                    "sent": "Again, this is due to overfitting.",
                    "label": 0
                },
                {
                    "sent": "So as a model increases it becomes too flexible and since the perfect model is just fitted, it overfits.",
                    "label": 0
                },
                {
                    "sent": "In this case we also tried just doing Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "And it also gave quite good results.",
                    "label": 0
                },
                {
                    "sent": "I'm the chairman.",
                    "label": 0
                },
                {
                    "sent": "Over model is resulting from the original Born Jacobson paper, which is a restricted parafac model, and it also does reasonably well.",
                    "label": 0
                },
                {
                    "sent": "This is very similar to the two factor 2 two component parafac model.",
                    "label": 0
                },
                {
                    "sent": "So for the function factorization.",
                    "label": 0
                },
                {
                    "sent": "Using just one component, it's basically the same results as Parafac model, which is also a special case of the function factorization framework.",
                    "label": 0
                },
                {
                    "sent": "But when we get more components at, we get quite a lot better results, both due to the added flexibility of having more flexible model.",
                    "label": 0
                },
                {
                    "sent": "But also we don't see any problems with overfitting, so we can we can have more components without overfitting because every all the parameters are integrated out.",
                    "label": 0
                },
                {
                    "sent": "So basically we don't do fitting so we can't do overfitting.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize, So what we propose is a new approach to nonlinear regression, which is useful when we know something more about the data set, namely that it has some sort of factory structure, and we can use.",
                    "label": 1
                },
                {
                    "sent": "To exploit that in the in the regression setting so it generalizes matrix and tensor factorization instead of factorizing a matrix as function as sorry as vectors reflectorized function as a part of the functions.",
                    "label": 1
                },
                {
                    "sent": "It's based on what Gaussian process priors, but it would be equally well in the same framework.",
                    "label": 0
                },
                {
                    "sent": "Grease other private server functions and we do full Bayesian inference using Hamiltonian MCMC and integrate all out old parameters and on this data set we've shown that we can outperform both terrifying and Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Before we go.",
                    "label": 0
                },
                {
                    "sent": "So two years ago there was this paper.",
                    "label": 0
                },
                {
                    "sent": "Which seems very very related.",
                    "label": 0
                },
                {
                    "sent": "If not, can you state the difference between will see with you also that one?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what's the author.",
                    "label": 0
                },
                {
                    "sent": "Another person is basically.",
                    "label": 0
                },
                {
                    "sent": "It's more or less the same thing on.",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with that paper though.",
                    "label": 0
                },
                {
                    "sent": "Tip this algorithm does a lot worse than.",
                    "label": 0
                },
                {
                    "sent": "Compared to what?",
                    "label": 0
                },
                {
                    "sent": "Actually, no, because.",
                    "label": 0
                },
                {
                    "sent": "So the ones we compared to is Parafac model which was fitted model which has some problem for overfitting.",
                    "label": 0
                },
                {
                    "sent": "So basically we can always do as well as perfect because we don't we can't overfit, so we always always going to be better than that.",
                    "label": 0
                },
                {
                    "sent": "So the added flexibility.",
                    "label": 0
                },
                {
                    "sent": "The only problem we might run into compared in comparison with the perfect model would be.",
                    "label": 0
                },
                {
                    "sent": "The MCMC inference might be a bit difficult or too slow, so, but otherwise we wouldn't have any problems compared to paraphrase compression process regression.",
                    "label": 0
                },
                {
                    "sent": "Again, this is this model is simply just more and more complex, so it more less control back to the Gaussian process regression model as well.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that the different functions could have different supports, right?",
                    "label": 0
                },
                {
                    "sent": "So they're looking at different subsets of the features, yes?",
                    "label": 0
                },
                {
                    "sent": "How would you go back to?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in this in this the this beef data set each each function was fitted on each of these storage conditions.",
                    "label": 0
                },
                {
                    "sent": "So one function was over the oxygen content and one function was over just the lights and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's just.",
                    "label": 0
                },
                {
                    "sent": "Function of two variables or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So the interaction terms come come come into play because all these functions nonlinear functions are multiplied together in model, so that's the interaction between the.",
                    "label": 0
                },
                {
                    "sent": "So basically taking out all the interactions and put them into multiplicative terms.",
                    "label": 0
                },
                {
                    "sent": "And basically a sum of products of nonlinear functions can do any interaction terms.",
                    "label": 0
                },
                {
                    "sent": "Serve you.",
                    "label": 0
                },
                {
                    "sent": "I mean what sort of range of datasets would be looked at?",
                    "label": 0
                },
                {
                    "sent": "Well, basically I started looking at a full size datasets because there are quite small and.",
                    "label": 0
                },
                {
                    "sent": "It's not a secret that Hamiltonian retaining wall can be quite slow sometimes, so this is I think this idea is mainly interesting if you want to really like.",
                    "label": 0
                },
                {
                    "sent": "Detailed analysis of possibly small data set.",
                    "label": 0
                },
                {
                    "sent": "We want to have like full posterior distributions and try to understand what's going on in the model.",
                    "label": 0
                },
                {
                    "sent": "So compared to the previous results on these types of datasets, just shown that, well, this sort of seems to be a linear relationship between the oxygen content in the storage and the color of the beef.",
                    "label": 0
                },
                {
                    "sent": "But now we can see where we get out.",
                    "label": 0
                },
                {
                    "sent": "Here is a full posterior distribution of this function that relates these two policies.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of adds to the understanding, and it because of the inference procedures that it doesn't scale that well up to large datasets.",
                    "label": 0
                },
                {
                    "sent": "Looking at other scale things up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm concentrating on small problems for now, but I think the main advantage here is basically that we can.",
                    "label": 0
                },
                {
                    "sent": "Since this is a very sort of complicated model that has lots of promises, a lot of structures, so it's really.",
                    "label": 0
                },
                {
                    "sent": "It's really important that we do a probation inference, and since that's intractable, we need to do some other sampling.",
                    "label": 0
                },
                {
                    "sent": "Or maybe, maybe we could look at doing like expectation propagation, or something else in this subset models, but I'm not looking at it at the moment.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Tried for instance back in your toy problem with two cows, two cosines.",
                    "label": 0
                },
                {
                    "sent": "Have you tried to verify that you still recover something reasonable if the data points are sampled on sort of non natural axes?",
                    "label": 0
                },
                {
                    "sent": "Like if you have coordinate transformations here, same problem.",
                    "label": 0
                },
                {
                    "sent": "Just ask 1X2 and still get something done with this.",
                    "label": 0
                },
                {
                    "sent": "I haven't, I haven't tried that specifically, but the main point is that you need to really.",
                    "label": 0
                },
                {
                    "sent": "Important point is to specify these subspaces that have these over, which this factory structure exists.",
                    "label": 0
                },
                {
                    "sent": "So if you specify that correctly where the data point is sample isn't that important.",
                    "label": 0
                },
                {
                    "sent": "As long as you sort of get the right axis for the factorization, two to work on, so that's something you need to specify in advance, so that's kind of information.",
                    "label": 0
                },
                {
                    "sent": "We need to have.",
                    "label": 0
                },
                {
                    "sent": "Intuition that this is an important variable.",
                    "label": 0
                },
                {
                    "sent": "Absolutely yes.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        }
    }
}