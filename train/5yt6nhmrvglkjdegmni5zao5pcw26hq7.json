{
    "id": "5yt6nhmrvglkjdegmni5zao5pcw26hq7",
    "title": "Covariate Shift by Kernel Mean Matching",
    "info": {
        "author": [
            "Arthur Gretton, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_gretton_cskm/",
    "segmentation": [
        [
            "Can approaches because these apply also to structured and more complex data?",
            "So I think this is relevant.",
            "The talk will be more by nature of a tutorial and somewhat informal.",
            "In particular, I'll sort of talk about some things that that arose from some earlier work that I did.",
            "In fact, with casting and others on the Verichip subsequent to our EPS paper on this.",
            "And in particular some findings about where and where one shouldn't use coverages."
        ],
        [
            "So first of all, just to set some.",
            "Variables up.",
            "We have a input domain of patterns X which might be something like ID or might be something more complex like strings and graphs.",
            "We have a training set which we received.",
            "NTR pairs XTR ytr from.",
            "I mean, these are distributed according to a. I likewise.",
            "Distributed according.",
            "Reduce it.",
            "And I'll go ahead.",
            "So the training distribution might be when the user is very focused.",
            "The test one when the user is less so.",
            "Gene expression profiles.",
            "We might want to use a classifier obtained from microwave data in one lab on data from another.",
            "In this case, again, differences in lab procedure might mean that there are differences in the distribution, so this is just sort of yeah, brief set of motivating examples of why one might need covariant shift.",
            "A first question to ask is, does this make sense?"
        ],
        [
            "Lol.",
            "And this is actually not.",
            "An unreasonable question in the sense that if you have two completely unlike distributions, then learning for one might tell you nothing about the other.",
            "Um?",
            "I'm going to restrict my situation in the."
        ],
        [
            "Case to a particular subclass of transfer learning problems.",
            "For which the conditional dependence of the labels given the data remains unchanged and the probability distributions of the patterns alone are allowed to change.",
            "And this is the setting of convention.",
            "So this is the setting I will focus on today."
        ],
        [
            "So classic example is this one by show Dara in 2000.",
            "Your training distribution in this case is a Gaussian of a high variance annual test is a Gaussian smaller variance.",
            "The labels.",
            "In this case we'll valued there a function polynomial function of the inputs.",
            "And corrupted by noise.",
            "So there is a noise epsilon.",
            "The.",
            "Task where doing here is to use linear regression to make a prediction of Y given X."
        ],
        [
            "So here are the training and test data.",
            "As you can see, the test data is a more complicated distribution.",
            "If we were just to fit the lines to both the training and test data, we can see that the line fit on the training data is a poor fit."
        ],
        [
            "Of the test prediction, and so you might think that you can do better than this because you know actually where the test data are.",
            "You should be able to predict the test data better using that knowledge.",
            "So this is."
        ],
        [
            "The example, so now I'm going to talk about the setup.",
            "So in classical learning what we might want to do is to minimize a regularised expected risk, which I have here.",
            "So this is a sum of two terms, the expectation of some loss.",
            "And a regularising term.",
            "So the last term here might be something like a log likelihood.",
            "It might be a square loss, it might be a hinge loss.",
            "At the feet of parameter is a parameter where optimizing over, so this could be the parameter of some models.",
            "It could be the direction you project on it and support vector machine and so on, and your task with minimizing the expression event."
        ],
        [
            "In the covariate shift setting, as I've said, you have a pair of distributions rather than a single one.",
            "So you're starting with a test distribution.",
            "You're trying to minimize the regularised risk on the test.",
            "You express this as a sum here, as as we did, but then we want to replace the expectation over the test distribution with an expectation over the reweighted loss for the training, and somehow this waiting beta is going to allow us to recover this.",
            "So the way."
        ],
        [
            "We do this.",
            "This is actually very straightforward by some simple algebra.",
            "You get that the correct waiting to use is this waiting.",
            "If you type that in, you get exactly this and you're done.",
            "So this would seem to have solved the problem.",
            "Ah, 01 thing to bear in mind, of course, is that we can't have the denominator zero when the numerator is 0, so I'm.",
            "Testing on this condition here, which is saying that wherever the.",
            "Test distribution has support.",
            "The training.",
            "Distribution must also have support, so that's important."
        ],
        [
            "So where I guess problems might arise in practice.",
            "Can be seen when you look at the variance of this expression.",
            "And so here I've written the variance of the loss function re weighted by the ratio of the test to training distributions.",
            "I expected that out until this expectation.",
            "Again."
        ],
        [
            "With some simple algebra you get an expectation of the squared loss function.",
            "Multiplied by this ratio here, which is your testing.",
            "So in the worst case."
        ],
        [
            "If this tested training ratio is bounded by B.",
            "Then your variance might rise as fast as the largest ratio of the tester training distributions.",
            "And if you think about it, this makes sense, because if your training distribution has almost no support as your test distribution, then it's quite likely that the variance of these weights is going to be very high.",
            "So one heuristic that people use is to use a training distribution, or when, when should have at least a training distribution with heavier."
        ],
        [
            "Else in the test distribution to avoid deflation.",
            "In this case I am just describing things in very general terms.",
            "So what did you have in life?",
            "Oh yeah, yeah, this is absolutely is that.",
            "OK, you know it's in a couple of slides.",
            "I bring up this point, yeah?",
            "Um?",
            "So OK, the next slide is just given these weights.",
            "How do you go about solving the learning problem?",
            "Um?"
        ],
        [
            "So the example I've used here is kind of regression, so assume that somehow rather you have access to the correct solution.",
            "The correct importance weights you want to now use these weights in their learning problem.",
            "So here the loss is just B squared loss.",
            "So Y minus this.",
            "Yeah, which is your prediction.",
            "This is some feature of your inputs X feature map pipe going to talk.",
            "More about that later.",
            "Beta is the vector you project on, so this is the prediction left."
        ],
        [
            "You minimize this expression.",
            "Here you have again this regularization term as we had in a couple of slides back.",
            "Here we have the weights which we've been given.",
            "And you can see that this amounts to minimizing the following expression.",
            "The K matrix is a matrix of the inner product between your feature vectors.",
            "The difference with respect to standard Ridge regression is that you now have these leader matrices.",
            "Is diagonal feeder matrix in the middle of your inner product here.",
            "So you can see that everything follows through and again with support vector machines with many learning methods.",
            "Reweighting in this way is just as simple and results in problems that you can solve as easily as you put himself the originals."
        ],
        [
            "So in the example in the toy example that we've seen, if we use the correct test training ratio, you can see that the prediction of the test data has improved in this case, and so it's closer to the idea that we would want, which is this red line, which is the prediction which was trained on a distribution on the test data."
        ],
        [
            "OK, so difficulty is shy, but up was that if you're using density estimation to try and get this ratio, you're going to run into trouble.",
            "So there are a few reasons.",
            "One, the workshop is about complex instruction data.",
            "It might be impossible even to meaningfully define distributions over, for instance graphs or strings or so on.",
            "Just because the dimension is high, or because the domain is hourly.",
            "Distributions on that domain are not well understood.",
            "Um?",
            "Another issue that arises is even if you do have a means of defining density estimates.",
            "For instance, if you're on the wheels, then you're doing a passing window estimate of the training and test distributions.",
            "You can end up with a very high variance.",
            "So again, going back a couple of sites you remember that this is Capital D parameter, which depends on the ratio of tested training.",
            "If your empirical estimate, for whatever reason makes that ratio large, then you could have a terribly high variance.",
            "But you might also have variants just because of the nature of the problem, because the training and test distributions are somewhat far away, and so the ratio just is big and there's nothing you can do about it."
        ],
        [
            "So there are a number of other ways that people go about computing this ratio in a sort of indirect way.",
            "And the advantages of these that.",
            "Each of them solves the problem, which under certain conditions will converge eventually to the right ratio.",
            "Of course, assuming that the problems are able to find it, which is not always true.",
            "But they basically make different regularising assumptions, and what that might mean is that the estimate of the ratio, even though it might not be as immediate, is doing a ratio of densities might have better variance properties, might have bias, which is is sort of higher, but which reduces variance and hence improves the estimate that you get when you learn on the related data using these.",
            "So just to go down the list.",
            "One approach that's been used which is rather elegant, which I'll talk a little bit about later, is that you actually train a classifier which distinguishes the training and test distributions, and as a byproduct of that you get an estimate of the ratio.",
            "So that's one way to do it.",
            "Another way is to minimize, for instance the KL divergent between training and test.",
            "That was done by Suyama itself.",
            "Another way is to minimize the two norm of the difference between the test training ratio and its target value, so that's kind of Oriental.",
            "These ones I won't talk about too much.",
            "The method that I'll focus on in the talk is using a measure of distance between distribution mappings called the maximum mean discrepancy.",
            "So this is defined on features of the distributions.",
            "In a reproducing kernel Hilbert space, and it's advantage is that it can be applied on structured data.",
            "Peace wraps and so on."
        ],
        [
            "So to describe how we do compare it, shift correction using the maximum mean discrepancy I need to start by describing the maximum in discrepancy."
        ],
        [
            "So this.",
            "Is a.",
            "Well, this one is actually a sort of very old approach to defining differences on distributions, which is to look at the supremum over a set of witness functions.",
            "Of the difference in expectations.",
            "So this is a nice illustration here where you have a Gaussian, a laptop distribution and you have a witness function F. And you can see that this has a large amplitude where the difference in probability mass of the two distributions is large, and so this function here is a function from some class which I haven't yet defined, but you can see that this is a function that would make this discrepancy large."
        ],
        [
            "So now I come to the question of defining this function class.",
            "So what we might want to do is find a class of functions such that this quantity here is a metric on distributions and this is a very classical problem.",
            "There are many solutions to it.",
            "I've listed a few years, the bounded continuous functions, functions of bounded variation, one which gives the Kolmogorov metric, the Lipschitz functions which give you the Earth movers.",
            "Distance is so in image processing these are popular.",
            "And all of these rich enough that this is a metric, so that means it's zero only when P&Q coincide.",
            "Oh"
        ],
        [
            "I mean, this is for it to be a metric for the purposes of.",
            "Perhaps changing this reply code.",
            "Yeah, for the purposes of comparing distributions, you might actually do have more expected function class so this.",
            "One can impose conditions for it to be a metric, but if one knows something about the classes of distributions comparing them, one might not wish that would be the case, yeah?",
            "It is a metric on the characteristic arcade chess is.",
            "I have some some entry sites which describe that I'm not sure if I'll get time for them, so I leave at the end of the talk.",
            "For the moment, I'll just say that they're not particularly exotic, so RK chess on the wheels with the Gaussian kernel is a characteristic of pages, and so using a Gaussian kernel will cause this to be a metric."
        ],
        [
            "So I've talked about unit both in a reproducing kernel Hilbert space.",
            "So for this I'm going to quickly define some Hilbert space terminology, so that's what I'm going to say on the next slide, makes sense.",
            "The reproducing kernel Hilbert spaces of the spaces of functions that can be written in this form.",
            "So basically linear combinations of positive definite kernel functions and the limits of these sequences.",
            "These are the reproducing kernel Hilbert spaces."
        ],
        [
            "Now, the positive definite kernel is an inner product between feature Maps.",
            "So coming back to this feature map idea I introduced when I talked about Ridge regression.",
            "Um?"
        ],
        [
            "And in particular.",
            "The OK just function at X can be written as an inner product between F and the feature methanex.",
            "So these are my notations that I'm going to be using."
        ],
        [
            "So we've defined the distance between P&Q as disagreement over a function class of differences in expectations.",
            "Then we make the following replacement.",
            "The expectation of F is inner product of F and the feature map of X which we defined.",
            "We then take the expectation of this feature up, which is itself a feature map, but this time it's a feature map of the distribution P. So just as we embedded a point in an arcade chest by T of X, we've embedded now a whole distribution in the arcade chest as U of X.",
            "So let's substitute these back.",
            "Now we."
        ],
        [
            "The inner product of F and the difference of view of X is.",
            "Now we."
        ],
        [
            "Find F to be a unit pole in the reproducing kernel Hilbert space.",
            "What this means is that we can replace the supremum over this unit bowl.",
            "As just the North so you can see this just by Cauchy Schwarz, so you get now the norm of the differences means.",
            "And."
        ],
        [
            "Then you can expand this, no doubt as an inner product and then make the reverse argument so the inner product of UX and UX is the expectation of the inner products of the feature Maps P of X.",
            "And the inner products of the P of X kernel function.",
            "So now we have a simple expression in terms of kernel functions of the distance between P&Q.",
            "So."
        ],
        [
            "Just to I guess we calculate the feature map is the expectation of the feature map of the feature map of the distribution.",
            "Is the expectation of the feature map of a point.",
            "You can also think of it as a convolution of the kernel at the distribution.",
            "So this is how we represent distributions in the arcade chest.",
            "The distance between these representations is the maximum mean discrepancy, or MMP."
        ],
        [
            "So I have to find now the measure of distance between distributions that I'm going to use.",
            "So now how do you do covariant shift using this distance?"
        ],
        [
            "The algorithm I'm going to use is going to be called kernel mean matching."
        ],
        [
            "For the following reason, what you want to do?",
            "Is to be weight your training.",
            "That in such a way that it's dark Ages embedding matches your test distribution.",
            "So if you can do that, then you're matching the representations of the distributions in a reproducing kernel Hilbert space, and this is like a way of doing this.",
            "We're waiting to close Internet.",
            "So the constraints that we've put here I just to ensure that this is a legitimate mapping of a probability distribution.",
            "Now, if the kernel is characteristic and if the support of the test distribution is contained in this part of the training distribution, then the solution of this optimization problem is just this.",
            "You get back your important something weights which, as we defined a few slides ago, are the best in the population sense.",
            "So what about non characteristic?"
        ],
        [
            "So.",
            "Again, you.",
            "Might wish well not to use characteristic kernels for a couple of reasons, one of which is that they might not be defined, or they might be impossible to compute.",
            "So characteristic kernels on graphs fall within this category, like it's computationally too expensive to compute them.",
            "Another reason you might not want to is because you might not care about matching distributions, but you might care only about matching certain moments of the distribution.",
            "So for instance.",
            "If you knew that your classifier that you were going to use with it only on the 1st three moments, then it doesn't make sense to match or to try to match distributions using a general metric.",
            "So basically, yeah, if you consider subclasses of distributions.",
            "It would make more sense to choose a feature space which concentrates on those properties of those subclasses, and not a general."
        ],
        [
            "So empirically this problem can be solved straightforwardly.",
            "I replaced here the population expectations with empirical ones, so this is the test distribution embedding computed with the empirical distribution.",
            "Here's the related training embedding and with minimizing the distance between that.",
            "That's a simple quadratic function here, so.",
            "The precise terms are not important.",
            "What's important is that you know you can."
        ],
        [
            "Readily.",
            "There are some constraints, so first of all we know the ratio of distributions is on some interval between obviously zero and the largest such ratios are Capital V. We also have this constraint, so this constraint is a somewhat relaxed version of that one.",
            "Where we allow a little bit of play around the meter equal."
        ],
        [
            "One constraint.",
            "The reason that we did that is because if we were given the important sampling ratio, says some fixed heater.",
            "Then the sample distance between this sum and its target isn't like.",
            "Yeah, the sum doesn't hit the target perfectly, but it's sort of distributed around the target asymptotically in a normal way, so it doesn't make sense to enforce that constraint exactly, but I need to enforce it with increasing precision as you see more samples."
        ],
        [
            "OK, so I've described a way to match distributions.",
            "If you remember from the important something thing we ran into this.",
            "Difficulty that the variance depends on the largest ratio of the test to training distributions.",
            "So again, I'm going to show here that we run into the same problem as you would expect when you're using this mean matching procedure instead."
        ],
        [
            "So you're given the correct input, it something better, so this comes to you, it's fixed.",
            "Then the distance computed empirically between the reweighted training map and the test map has this found here.",
            "So it's converging to 0, but it's converging at some rate.",
            "The important thing to note here is this capital D term comes back again.",
            "So again, if your test distribution is very different from your training distribution, then even though this converges at rate one on Route 10, it might converge from somewhere very far away from where you would like it to be.",
            "I'm going to just make an aside here that Carina Cortez has shown convergence of the kernel mean matched.",
            "We weighted training test error rather or expected risk to the expected risk on the test distribution, so that's.",
            "Carinas oh I should I.",
            "It might be an aesthetic when actually.",
            "No, no, I don't think it's good, but I mean I should check that I I yeah."
        ],
        [
            "So here is again now favorite example.",
            "We have these tests and training distribution.",
            "We're trying to fit lines to this code."
        ],
        [
            "For these data you have the kernel mean matching.",
            "Result is actually pretty good.",
            "We also run this."
        ],
        [
            "Many times.",
            "So here you can see the test train ratio compared to a couple of other methods.",
            "So one is kernel matching and one is a criterion by Sherwood era.",
            "And this is basically showing that you're able to improve performance over the roar ratio of training to test distributions.",
            "If you're prepared to regularize a bit, and if you're prepared to introduce a bit of bias.",
            "So this is, I think, an encouraging result."
        ],
        [
            "This problem.",
            "So I talked mainly about kind of matching.",
            "I want to just briefly cover another rather elegant way to solve this reweighting problem.",
            "So this is actually to use a classifier as a proxy for getting this weight.",
            "So you train a classifier which is to distinguish the training and test distributions.",
            "And as a side effect, with this you get weight which you can then use in your covariate shift correction.",
            "After this was proposed, well, a number of times by several people, most recently Baker later.",
            "The distribution here.",
            "These variables are a vector of variables for your testing training points, which are one where the training distribution is drawn from and which are zero when the test distribution is drawn from.",
            "So the again, the reason why you might want to do this by a classifier is because you can then perhaps define classifiers on more exotic spaces.",
            "And get weights which you would otherwise have trouble getting.",
            "If you are trying to do density estimation 1st and then take the ratio."
        ],
        [
            "So here is how one gets the ratio of tested training.",
            "From a classifier.",
            "So this is the probability of being drawn from the training sample given a training point in giving the parameter Theta, which you've learned, which is the Cedar police training test classifier.",
            "So basically the converted procedure here goes in two stages.",
            "First you learn the training test classifier, then you get your weights.",
            "Then you plug those into your learning algorithm, which turns the tables from the patterns.",
            "So the ways from the exits and then Yep, we're done.",
            "So the vehicle paper proposed a further extension to this which I mentioned here where they just make an enormous joint model over everything.",
            "So they say my model tells me first day choose something from the training or the test distribution and then I labeled accordingly.",
            "So actually, just in the lips conference Masashi Seyama told me that he had some difficulties with this.",
            "So his difficulty was that to cross validate the priors here, which have parameters associated.",
            "So when 60 bandwidth of the Gaussian.",
            "You need to actually see a label test sample, which then makes the whole thing redundant.",
            "Another difficulty that I think I have with it is that this idea of like choosing randomly from training or test distribution to start with is I think.",
            "Like not a satisfactory intuition for the process by which coverage is because.",
            "But anyway, I just put this here 'cause it's a method that's being used.",
            "The argument for this method is that you can have jointly learn the features for the training and test distribution and then potentially get stronger improvement than you would by doing it in two stages.",
            "So that was the argument that was made.",
            "So I have some experiments."
        ],
        [
            "Um?",
            "The first one, which I'm."
        ],
        [
            "Going to go into in somewhat more details and the others is the UCI breast cancer data set.",
            "We're using a Gaussian kernel for both the kernel mean matching and for the support vector machine that we train.",
            "We just fixing the bandwidth to be 5 and we're going to show performance as a function of the sea so they see is the thing that controls the tradeoff between loss and regularization, and as it set up here, a small C means that we prioritize with this.",
            "So the way that we generate our data so this is an artificial example.",
            "We have a data pool.",
            "We split it randomly into tests and training.",
            "The tests, the training data set will be of different sizes 'cause we're going to see performance as a function of training size.",
            "And this is the distribution we choose from.",
            "So you're basically selecting randomly from the from the pool of data according to this bias sampling."
        ],
        [
            "So first of all, the good result.",
            "Here for a small see what we see.",
            "Is that the performance?",
            "The two improvement that you get using kernel mean matching is very substantial, so here we have the average loss and you can see that across the board the covariant shift gives you smaller loss than the unweighted case which is in black.",
            "The Gray one.",
            "The important something one is actually using the underlying crew ratio.",
            "And you can see here that for the smaller sample sizes, it's doing much worse.",
            "This shouldn't be surprising because the selection procedure was set up in such a way as to create a large ratio between training and test.",
            "And so having some unregularized expression of that ratio is going to give you grief.",
            "So this is for a very small see, which means that we're actually prioritizing the smoothness of the classifier over the loss."
        ],
        [
            "Now let's see what happens when we use a property cross validated, see.",
            "So in this case, you'll see that the kind of matching actually makes everything a bit worse.",
            "So what's interesting is that for a very small value of C, you have basically pushed down the error to this value here.",
            "But if you were too well, you say well?",
            "Validated and carefully modeled, selected classifier, you would end up with the right answer.",
            "So why is it in fact that we were not getting improvements for this case?",
            "And bear in mind also that way we're also not getting improvements for the the one where we waited by the underlying crew.",
            "Density."
        ],
        [
            "Show so to to give a intuition as to why this might be, I'll return to our toy example again.",
            "This time, rather than fitting a straight line, I'm fitting kernel Ridge regression.",
            "And here you can see that basically.",
            "This is following the curve pretty much everywhere, and so the only thing that you get if you take some subset of that training data and up weighted and unweighted.",
            "Everything else is a reduced sample size and you're actually not helping yourself at all.",
            "So the moral to this story, I think, is that if you have.",
            "A properly trained and and powerful learning methods.",
            "But it's doubtful that you're going to get improvement using covariant shift of any kind.",
            "So.",
            "Using the waiting.",
            "Sorry Oh yeah.",
            "But Yep.",
            "Yeah.",
            "I mean, even in this one which was chosen."
        ],
        [
            "Pretty much at random, you can see actually the reweighting making things worse, so yeah.",
            "So.",
            "I mean, this was."
        ],
        [
            "Just went experiment so we should never generalize from a single data set.",
            "So now I'm going to show a whole bunch of datasets.",
            "So it's a mixture of regression and classification.",
            "Our sampling scheme is again similar to the one we use for the breast cancer data.",
            "We basically have a pool of data we sample from it with some bias.",
            "In this case, again a Gaussian bias on a specific access through this space.",
            "One thing that we're doing, which might not be the best thing to do, is that we are using the same kernel size for the kernel mean matching for the classifier regressor, so this might not be optimal.",
            "Right, so in all of these, all of these cases we generate the data by taking a pool of data sub something it randomly, but with some biased distribution that we have created.",
            "So this is an artificial benchmark.",
            "And then yeah, proceeding.",
            "Up the other thing that we do is we solve for the parameters of the regression or classification over.",
            "Then we cross validate on the unweighted data.",
            "So we cross validate on the unweighted training set rather than the weighted one.",
            "So this is again something I'm going to return to, but this might at this point already speculating a bit suspicious."
        ],
        [
            "But it needed to be.",
            "OK, so here are the benchmarks.",
            "So to guide you through the graph.",
            "These are where."
        ],
        [
            "Either important waiting or panel mean matching give some improvement.",
            "In some cases, in most cases both do.",
            "There's only one case, which is this case, where that doesn't happen.",
            "But that's something that's quite striking.",
            "Is that if one works in both."
        ],
        [
            "Here's some cases where both."
        ],
        [
            "Statistically worse.",
            "Is a case where you put it something is statistically worse in the one here currently, matching doesn't make a difference.",
            "And here's one where kind of matching is worse.",
            "So basically what we've shown here is that, yeah, I mean, the results are very mixed.",
            "I think what's interesting is that this this phenomenon that.",
            "When?"
        ],
        [
            "One works, both work, so this seems to suggest looking at a couple of slides back then that if for whatever reason.",
            "You cross validated over to core Sigrid or your learning algorithm is not the right one for that data.",
            "Then you're going to get an improvement, perhaps using the weighting, even though if you were more careful in your selection of algorithm or if you were more systematic in your consolidation, you would not.",
            "So this is a way to look at that, yeah?",
            "Um so when?"
        ],
        [
            "I guess final thought, which unfortunately we didn't explore very well.",
            "In our original list paper, is this model selection problem for covariate shift?",
            "So we have a distance measure between distributions.",
            "Is this measure as a function of the kernel?",
            "So what that means?",
            "Is it for different kernels?",
            "You get different distances.",
            "And for when you use these different distances, you get different performance.",
            "So here are a bunch of high dimensional datasets.",
            "What you should note here is, well, 1st that it's from someone elses paper so you can trust their experiment.",
            "And 2nd that the performance does show a sort of optimum.",
            "And in particular that this optimum matches the best performance that they young mentality get with their algorithm which has a built in cross validation.",
            "So what this seems to mean is that, again, if for whatever reason maybe waiting does work, you have to be very careful with kernel matching to choose a distance measure, which allows you to recover that reweighting effectively.",
            "Um?",
            "So this is something which I don't have a good answer for.",
            "In the case of enemy matching."
        ],
        [
            "Um?",
            "There have been considerable work, in fact on choosing parameters for different coverage of correction algorithms.",
            "One easy case is if there is a systematic drift in the data, which just means that you have like from past values of the covariate shift a very good idea of where it's going to go next, so that makes life a bit easier for you.",
            "Um?",
            "The.",
            "Indiana groups have been doing cross validation on the left out sample to obtain the parameters for their learning algorithms.",
            "Uh huh.",
            "So they.",
            "It's sort of.",
            "It's it's like it's not cross validation in in necessarily a completely systematic sense, or it's it's a kind of it's a different.",
            "Way to do it.",
            "So basically they have their estimate of their ratio and then they they use this as they use this ratio somewhere difference.",
            "They would plug it into a tail divergent so they would plug it into some other expression, but.",
            "Right exactly so this is a proxy for the ground truth, so I I just bringing this up as is what the way that they do their assassination.",
            "Um?",
            "In the main conference we talked about looking at the largest NMD over a set of panels.",
            "Or perhaps some some other optimization over a set which might or might not be weighted.",
            "This I don't know if it would be useful or not, but at least this is.",
            "This is something that one might use.",
            "The last point, I think is is quite an important one.",
            "So again, if you know something about your learning problem.",
            "For instance, that the learner uses particular features of the distribution, then you should use as features in make matching your needs, because that's that really makes a lot of sense.",
            "Yeah, yeah, exactly.",
            "I mean, it's like you should concentrate on actually matching those features that you're going to use and not on trying to match every feature you can get your hands on.",
            "So this is, I think.",
            "Yeah, another quite important thing to very light.",
            "The other thing is you remember that when I did my cross validation for the experiments I was using cross validation on the unweighted training set rather than the weighted one.",
            "So this does introduce a bias.",
            "But again, coming back to our Ridge regression examples because we were using powerful learning algorithms here, it seems that the cross validation he got on the weighted set pretty much did it change when we use the unweighted set.",
            "So yeah, so that was."
        ],
        [
            "OK, so in summary.",
            "I think, well, the.",
            "Currently, matching allows you to do covariant shift without doing first density estimates of the training and test distributions.",
            "It allows you to match only particular features of the distributions and so for instance, on structured domains like strings and graphs.",
            "This might be an approach to use.",
            "Where the performance improves in our sort of very generic benchmark experiments.",
            "Was mainly when the learning algorithm was simpler than the data would warrant.",
            "So in this case you can get very big performance improvements and this almost has the favor of local learning in the sense that you if you have a learning algorithm which is simpler than the one you should have.",
            "Then focusing in on your test data is going to help you.",
            "If the learning algorithm is as helpful as it needs to be, then it might still be useful to do per variant shift in the sense that it gives you a little bit more play in your cross validation.",
            "So reweighting based event shift just because if you haven't consolidated properly it, it might actually help you.",
            "Or if you're using an algorithm which is not completely suited to your data.",
            "At model selection."
        ],
        [
            "So.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can approaches because these apply also to structured and more complex data?",
                    "label": 0
                },
                {
                    "sent": "So I think this is relevant.",
                    "label": 0
                },
                {
                    "sent": "The talk will be more by nature of a tutorial and somewhat informal.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'll sort of talk about some things that that arose from some earlier work that I did.",
                    "label": 0
                },
                {
                    "sent": "In fact, with casting and others on the Verichip subsequent to our EPS paper on this.",
                    "label": 0
                },
                {
                    "sent": "And in particular some findings about where and where one shouldn't use coverages.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, just to set some.",
                    "label": 0
                },
                {
                    "sent": "Variables up.",
                    "label": 0
                },
                {
                    "sent": "We have a input domain of patterns X which might be something like ID or might be something more complex like strings and graphs.",
                    "label": 0
                },
                {
                    "sent": "We have a training set which we received.",
                    "label": 0
                },
                {
                    "sent": "NTR pairs XTR ytr from.",
                    "label": 1
                },
                {
                    "sent": "I mean, these are distributed according to a. I likewise.",
                    "label": 0
                },
                {
                    "sent": "Distributed according.",
                    "label": 0
                },
                {
                    "sent": "Reduce it.",
                    "label": 0
                },
                {
                    "sent": "And I'll go ahead.",
                    "label": 0
                },
                {
                    "sent": "So the training distribution might be when the user is very focused.",
                    "label": 0
                },
                {
                    "sent": "The test one when the user is less so.",
                    "label": 0
                },
                {
                    "sent": "Gene expression profiles.",
                    "label": 0
                },
                {
                    "sent": "We might want to use a classifier obtained from microwave data in one lab on data from another.",
                    "label": 0
                },
                {
                    "sent": "In this case, again, differences in lab procedure might mean that there are differences in the distribution, so this is just sort of yeah, brief set of motivating examples of why one might need covariant shift.",
                    "label": 0
                },
                {
                    "sent": "A first question to ask is, does this make sense?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lol.",
                    "label": 0
                },
                {
                    "sent": "And this is actually not.",
                    "label": 0
                },
                {
                    "sent": "An unreasonable question in the sense that if you have two completely unlike distributions, then learning for one might tell you nothing about the other.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm going to restrict my situation in the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case to a particular subclass of transfer learning problems.",
                    "label": 0
                },
                {
                    "sent": "For which the conditional dependence of the labels given the data remains unchanged and the probability distributions of the patterns alone are allowed to change.",
                    "label": 0
                },
                {
                    "sent": "And this is the setting of convention.",
                    "label": 0
                },
                {
                    "sent": "So this is the setting I will focus on today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So classic example is this one by show Dara in 2000.",
                    "label": 0
                },
                {
                    "sent": "Your training distribution in this case is a Gaussian of a high variance annual test is a Gaussian smaller variance.",
                    "label": 0
                },
                {
                    "sent": "The labels.",
                    "label": 0
                },
                {
                    "sent": "In this case we'll valued there a function polynomial function of the inputs.",
                    "label": 0
                },
                {
                    "sent": "And corrupted by noise.",
                    "label": 0
                },
                {
                    "sent": "So there is a noise epsilon.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Task where doing here is to use linear regression to make a prediction of Y given X.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the training and test data.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the test data is a more complicated distribution.",
                    "label": 0
                },
                {
                    "sent": "If we were just to fit the lines to both the training and test data, we can see that the line fit on the training data is a poor fit.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the test prediction, and so you might think that you can do better than this because you know actually where the test data are.",
                    "label": 0
                },
                {
                    "sent": "You should be able to predict the test data better using that knowledge.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The example, so now I'm going to talk about the setup.",
                    "label": 0
                },
                {
                    "sent": "So in classical learning what we might want to do is to minimize a regularised expected risk, which I have here.",
                    "label": 0
                },
                {
                    "sent": "So this is a sum of two terms, the expectation of some loss.",
                    "label": 0
                },
                {
                    "sent": "And a regularising term.",
                    "label": 0
                },
                {
                    "sent": "So the last term here might be something like a log likelihood.",
                    "label": 0
                },
                {
                    "sent": "It might be a square loss, it might be a hinge loss.",
                    "label": 0
                },
                {
                    "sent": "At the feet of parameter is a parameter where optimizing over, so this could be the parameter of some models.",
                    "label": 0
                },
                {
                    "sent": "It could be the direction you project on it and support vector machine and so on, and your task with minimizing the expression event.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the covariate shift setting, as I've said, you have a pair of distributions rather than a single one.",
                    "label": 1
                },
                {
                    "sent": "So you're starting with a test distribution.",
                    "label": 0
                },
                {
                    "sent": "You're trying to minimize the regularised risk on the test.",
                    "label": 0
                },
                {
                    "sent": "You express this as a sum here, as as we did, but then we want to replace the expectation over the test distribution with an expectation over the reweighted loss for the training, and somehow this waiting beta is going to allow us to recover this.",
                    "label": 0
                },
                {
                    "sent": "So the way.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do this.",
                    "label": 0
                },
                {
                    "sent": "This is actually very straightforward by some simple algebra.",
                    "label": 0
                },
                {
                    "sent": "You get that the correct waiting to use is this waiting.",
                    "label": 0
                },
                {
                    "sent": "If you type that in, you get exactly this and you're done.",
                    "label": 0
                },
                {
                    "sent": "So this would seem to have solved the problem.",
                    "label": 0
                },
                {
                    "sent": "Ah, 01 thing to bear in mind, of course, is that we can't have the denominator zero when the numerator is 0, so I'm.",
                    "label": 0
                },
                {
                    "sent": "Testing on this condition here, which is saying that wherever the.",
                    "label": 0
                },
                {
                    "sent": "Test distribution has support.",
                    "label": 0
                },
                {
                    "sent": "The training.",
                    "label": 0
                },
                {
                    "sent": "Distribution must also have support, so that's important.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where I guess problems might arise in practice.",
                    "label": 0
                },
                {
                    "sent": "Can be seen when you look at the variance of this expression.",
                    "label": 1
                },
                {
                    "sent": "And so here I've written the variance of the loss function re weighted by the ratio of the test to training distributions.",
                    "label": 0
                },
                {
                    "sent": "I expected that out until this expectation.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With some simple algebra you get an expectation of the squared loss function.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by this ratio here, which is your testing.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If this tested training ratio is bounded by B.",
                    "label": 0
                },
                {
                    "sent": "Then your variance might rise as fast as the largest ratio of the tester training distributions.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, this makes sense, because if your training distribution has almost no support as your test distribution, then it's quite likely that the variance of these weights is going to be very high.",
                    "label": 0
                },
                {
                    "sent": "So one heuristic that people use is to use a training distribution, or when, when should have at least a training distribution with heavier.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Else in the test distribution to avoid deflation.",
                    "label": 0
                },
                {
                    "sent": "In this case I am just describing things in very general terms.",
                    "label": 0
                },
                {
                    "sent": "So what did you have in life?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah, this is absolutely is that.",
                    "label": 0
                },
                {
                    "sent": "OK, you know it's in a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "I bring up this point, yeah?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So OK, the next slide is just given these weights.",
                    "label": 0
                },
                {
                    "sent": "How do you go about solving the learning problem?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the example I've used here is kind of regression, so assume that somehow rather you have access to the correct solution.",
                    "label": 0
                },
                {
                    "sent": "The correct importance weights you want to now use these weights in their learning problem.",
                    "label": 0
                },
                {
                    "sent": "So here the loss is just B squared loss.",
                    "label": 0
                },
                {
                    "sent": "So Y minus this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is your prediction.",
                    "label": 0
                },
                {
                    "sent": "This is some feature of your inputs X feature map pipe going to talk.",
                    "label": 0
                },
                {
                    "sent": "More about that later.",
                    "label": 0
                },
                {
                    "sent": "Beta is the vector you project on, so this is the prediction left.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You minimize this expression.",
                    "label": 0
                },
                {
                    "sent": "Here you have again this regularization term as we had in a couple of slides back.",
                    "label": 0
                },
                {
                    "sent": "Here we have the weights which we've been given.",
                    "label": 0
                },
                {
                    "sent": "And you can see that this amounts to minimizing the following expression.",
                    "label": 0
                },
                {
                    "sent": "The K matrix is a matrix of the inner product between your feature vectors.",
                    "label": 0
                },
                {
                    "sent": "The difference with respect to standard Ridge regression is that you now have these leader matrices.",
                    "label": 0
                },
                {
                    "sent": "Is diagonal feeder matrix in the middle of your inner product here.",
                    "label": 0
                },
                {
                    "sent": "So you can see that everything follows through and again with support vector machines with many learning methods.",
                    "label": 0
                },
                {
                    "sent": "Reweighting in this way is just as simple and results in problems that you can solve as easily as you put himself the originals.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the example in the toy example that we've seen, if we use the correct test training ratio, you can see that the prediction of the test data has improved in this case, and so it's closer to the idea that we would want, which is this red line, which is the prediction which was trained on a distribution on the test data.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so difficulty is shy, but up was that if you're using density estimation to try and get this ratio, you're going to run into trouble.",
                    "label": 0
                },
                {
                    "sent": "So there are a few reasons.",
                    "label": 0
                },
                {
                    "sent": "One, the workshop is about complex instruction data.",
                    "label": 0
                },
                {
                    "sent": "It might be impossible even to meaningfully define distributions over, for instance graphs or strings or so on.",
                    "label": 0
                },
                {
                    "sent": "Just because the dimension is high, or because the domain is hourly.",
                    "label": 0
                },
                {
                    "sent": "Distributions on that domain are not well understood.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Another issue that arises is even if you do have a means of defining density estimates.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you're on the wheels, then you're doing a passing window estimate of the training and test distributions.",
                    "label": 0
                },
                {
                    "sent": "You can end up with a very high variance.",
                    "label": 0
                },
                {
                    "sent": "So again, going back a couple of sites you remember that this is Capital D parameter, which depends on the ratio of tested training.",
                    "label": 0
                },
                {
                    "sent": "If your empirical estimate, for whatever reason makes that ratio large, then you could have a terribly high variance.",
                    "label": 0
                },
                {
                    "sent": "But you might also have variants just because of the nature of the problem, because the training and test distributions are somewhat far away, and so the ratio just is big and there's nothing you can do about it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are a number of other ways that people go about computing this ratio in a sort of indirect way.",
                    "label": 0
                },
                {
                    "sent": "And the advantages of these that.",
                    "label": 0
                },
                {
                    "sent": "Each of them solves the problem, which under certain conditions will converge eventually to the right ratio.",
                    "label": 0
                },
                {
                    "sent": "Of course, assuming that the problems are able to find it, which is not always true.",
                    "label": 0
                },
                {
                    "sent": "But they basically make different regularising assumptions, and what that might mean is that the estimate of the ratio, even though it might not be as immediate, is doing a ratio of densities might have better variance properties, might have bias, which is is sort of higher, but which reduces variance and hence improves the estimate that you get when you learn on the related data using these.",
                    "label": 0
                },
                {
                    "sent": "So just to go down the list.",
                    "label": 0
                },
                {
                    "sent": "One approach that's been used which is rather elegant, which I'll talk a little bit about later, is that you actually train a classifier which distinguishes the training and test distributions, and as a byproduct of that you get an estimate of the ratio.",
                    "label": 0
                },
                {
                    "sent": "So that's one way to do it.",
                    "label": 0
                },
                {
                    "sent": "Another way is to minimize, for instance the KL divergent between training and test.",
                    "label": 0
                },
                {
                    "sent": "That was done by Suyama itself.",
                    "label": 0
                },
                {
                    "sent": "Another way is to minimize the two norm of the difference between the test training ratio and its target value, so that's kind of Oriental.",
                    "label": 0
                },
                {
                    "sent": "These ones I won't talk about too much.",
                    "label": 0
                },
                {
                    "sent": "The method that I'll focus on in the talk is using a measure of distance between distribution mappings called the maximum mean discrepancy.",
                    "label": 0
                },
                {
                    "sent": "So this is defined on features of the distributions.",
                    "label": 0
                },
                {
                    "sent": "In a reproducing kernel Hilbert space, and it's advantage is that it can be applied on structured data.",
                    "label": 0
                },
                {
                    "sent": "Peace wraps and so on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to describe how we do compare it, shift correction using the maximum mean discrepancy I need to start by describing the maximum in discrepancy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Is a.",
                    "label": 0
                },
                {
                    "sent": "Well, this one is actually a sort of very old approach to defining differences on distributions, which is to look at the supremum over a set of witness functions.",
                    "label": 0
                },
                {
                    "sent": "Of the difference in expectations.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice illustration here where you have a Gaussian, a laptop distribution and you have a witness function F. And you can see that this has a large amplitude where the difference in probability mass of the two distributions is large, and so this function here is a function from some class which I haven't yet defined, but you can see that this is a function that would make this discrepancy large.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I come to the question of defining this function class.",
                    "label": 0
                },
                {
                    "sent": "So what we might want to do is find a class of functions such that this quantity here is a metric on distributions and this is a very classical problem.",
                    "label": 0
                },
                {
                    "sent": "There are many solutions to it.",
                    "label": 0
                },
                {
                    "sent": "I've listed a few years, the bounded continuous functions, functions of bounded variation, one which gives the Kolmogorov metric, the Lipschitz functions which give you the Earth movers.",
                    "label": 1
                },
                {
                    "sent": "Distance is so in image processing these are popular.",
                    "label": 0
                },
                {
                    "sent": "And all of these rich enough that this is a metric, so that means it's zero only when P&Q coincide.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, this is for it to be a metric for the purposes of.",
                    "label": 0
                },
                {
                    "sent": "Perhaps changing this reply code.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for the purposes of comparing distributions, you might actually do have more expected function class so this.",
                    "label": 0
                },
                {
                    "sent": "One can impose conditions for it to be a metric, but if one knows something about the classes of distributions comparing them, one might not wish that would be the case, yeah?",
                    "label": 0
                },
                {
                    "sent": "It is a metric on the characteristic arcade chess is.",
                    "label": 0
                },
                {
                    "sent": "I have some some entry sites which describe that I'm not sure if I'll get time for them, so I leave at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "For the moment, I'll just say that they're not particularly exotic, so RK chess on the wheels with the Gaussian kernel is a characteristic of pages, and so using a Gaussian kernel will cause this to be a metric.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've talked about unit both in a reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So for this I'm going to quickly define some Hilbert space terminology, so that's what I'm going to say on the next slide, makes sense.",
                    "label": 0
                },
                {
                    "sent": "The reproducing kernel Hilbert spaces of the spaces of functions that can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "So basically linear combinations of positive definite kernel functions and the limits of these sequences.",
                    "label": 1
                },
                {
                    "sent": "These are the reproducing kernel Hilbert spaces.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, the positive definite kernel is an inner product between feature Maps.",
                    "label": 1
                },
                {
                    "sent": "So coming back to this feature map idea I introduced when I talked about Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in particular.",
                    "label": 0
                },
                {
                    "sent": "The OK just function at X can be written as an inner product between F and the feature methanex.",
                    "label": 1
                },
                {
                    "sent": "So these are my notations that I'm going to be using.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've defined the distance between P&Q as disagreement over a function class of differences in expectations.",
                    "label": 0
                },
                {
                    "sent": "Then we make the following replacement.",
                    "label": 0
                },
                {
                    "sent": "The expectation of F is inner product of F and the feature map of X which we defined.",
                    "label": 0
                },
                {
                    "sent": "We then take the expectation of this feature up, which is itself a feature map, but this time it's a feature map of the distribution P. So just as we embedded a point in an arcade chest by T of X, we've embedded now a whole distribution in the arcade chest as U of X.",
                    "label": 0
                },
                {
                    "sent": "So let's substitute these back.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inner product of F and the difference of view of X is.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find F to be a unit pole in the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "What this means is that we can replace the supremum over this unit bowl.",
                    "label": 0
                },
                {
                    "sent": "As just the North so you can see this just by Cauchy Schwarz, so you get now the norm of the differences means.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can expand this, no doubt as an inner product and then make the reverse argument so the inner product of UX and UX is the expectation of the inner products of the feature Maps P of X.",
                    "label": 0
                },
                {
                    "sent": "And the inner products of the P of X kernel function.",
                    "label": 0
                },
                {
                    "sent": "So now we have a simple expression in terms of kernel functions of the distance between P&Q.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to I guess we calculate the feature map is the expectation of the feature map of the feature map of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Is the expectation of the feature map of a point.",
                    "label": 0
                },
                {
                    "sent": "You can also think of it as a convolution of the kernel at the distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is how we represent distributions in the arcade chest.",
                    "label": 0
                },
                {
                    "sent": "The distance between these representations is the maximum mean discrepancy, or MMP.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have to find now the measure of distance between distributions that I'm going to use.",
                    "label": 0
                },
                {
                    "sent": "So now how do you do covariant shift using this distance?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm I'm going to use is going to be called kernel mean matching.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the following reason, what you want to do?",
                    "label": 0
                },
                {
                    "sent": "Is to be weight your training.",
                    "label": 0
                },
                {
                    "sent": "That in such a way that it's dark Ages embedding matches your test distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you can do that, then you're matching the representations of the distributions in a reproducing kernel Hilbert space, and this is like a way of doing this.",
                    "label": 0
                },
                {
                    "sent": "We're waiting to close Internet.",
                    "label": 0
                },
                {
                    "sent": "So the constraints that we've put here I just to ensure that this is a legitimate mapping of a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Now, if the kernel is characteristic and if the support of the test distribution is contained in this part of the training distribution, then the solution of this optimization problem is just this.",
                    "label": 0
                },
                {
                    "sent": "You get back your important something weights which, as we defined a few slides ago, are the best in the population sense.",
                    "label": 0
                },
                {
                    "sent": "So what about non characteristic?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, you.",
                    "label": 0
                },
                {
                    "sent": "Might wish well not to use characteristic kernels for a couple of reasons, one of which is that they might not be defined, or they might be impossible to compute.",
                    "label": 0
                },
                {
                    "sent": "So characteristic kernels on graphs fall within this category, like it's computationally too expensive to compute them.",
                    "label": 0
                },
                {
                    "sent": "Another reason you might not want to is because you might not care about matching distributions, but you might care only about matching certain moments of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "If you knew that your classifier that you were going to use with it only on the 1st three moments, then it doesn't make sense to match or to try to match distributions using a general metric.",
                    "label": 0
                },
                {
                    "sent": "So basically, yeah, if you consider subclasses of distributions.",
                    "label": 0
                },
                {
                    "sent": "It would make more sense to choose a feature space which concentrates on those properties of those subclasses, and not a general.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So empirically this problem can be solved straightforwardly.",
                    "label": 0
                },
                {
                    "sent": "I replaced here the population expectations with empirical ones, so this is the test distribution embedding computed with the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "Here's the related training embedding and with minimizing the distance between that.",
                    "label": 0
                },
                {
                    "sent": "That's a simple quadratic function here, so.",
                    "label": 0
                },
                {
                    "sent": "The precise terms are not important.",
                    "label": 0
                },
                {
                    "sent": "What's important is that you know you can.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Readily.",
                    "label": 0
                },
                {
                    "sent": "There are some constraints, so first of all we know the ratio of distributions is on some interval between obviously zero and the largest such ratios are Capital V. We also have this constraint, so this constraint is a somewhat relaxed version of that one.",
                    "label": 0
                },
                {
                    "sent": "Where we allow a little bit of play around the meter equal.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One constraint.",
                    "label": 0
                },
                {
                    "sent": "The reason that we did that is because if we were given the important sampling ratio, says some fixed heater.",
                    "label": 0
                },
                {
                    "sent": "Then the sample distance between this sum and its target isn't like.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the sum doesn't hit the target perfectly, but it's sort of distributed around the target asymptotically in a normal way, so it doesn't make sense to enforce that constraint exactly, but I need to enforce it with increasing precision as you see more samples.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I've described a way to match distributions.",
                    "label": 0
                },
                {
                    "sent": "If you remember from the important something thing we ran into this.",
                    "label": 0
                },
                {
                    "sent": "Difficulty that the variance depends on the largest ratio of the test to training distributions.",
                    "label": 0
                },
                {
                    "sent": "So again, I'm going to show here that we run into the same problem as you would expect when you're using this mean matching procedure instead.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you're given the correct input, it something better, so this comes to you, it's fixed.",
                    "label": 0
                },
                {
                    "sent": "Then the distance computed empirically between the reweighted training map and the test map has this found here.",
                    "label": 0
                },
                {
                    "sent": "So it's converging to 0, but it's converging at some rate.",
                    "label": 0
                },
                {
                    "sent": "The important thing to note here is this capital D term comes back again.",
                    "label": 0
                },
                {
                    "sent": "So again, if your test distribution is very different from your training distribution, then even though this converges at rate one on Route 10, it might converge from somewhere very far away from where you would like it to be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just make an aside here that Carina Cortez has shown convergence of the kernel mean matched.",
                    "label": 0
                },
                {
                    "sent": "We weighted training test error rather or expected risk to the expected risk on the test distribution, so that's.",
                    "label": 0
                },
                {
                    "sent": "Carinas oh I should I.",
                    "label": 0
                },
                {
                    "sent": "It might be an aesthetic when actually.",
                    "label": 0
                },
                {
                    "sent": "No, no, I don't think it's good, but I mean I should check that I I yeah.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is again now favorite example.",
                    "label": 0
                },
                {
                    "sent": "We have these tests and training distribution.",
                    "label": 0
                },
                {
                    "sent": "We're trying to fit lines to this code.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For these data you have the kernel mean matching.",
                    "label": 1
                },
                {
                    "sent": "Result is actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "We also run this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many times.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the test train ratio compared to a couple of other methods.",
                    "label": 0
                },
                {
                    "sent": "So one is kernel matching and one is a criterion by Sherwood era.",
                    "label": 0
                },
                {
                    "sent": "And this is basically showing that you're able to improve performance over the roar ratio of training to test distributions.",
                    "label": 0
                },
                {
                    "sent": "If you're prepared to regularize a bit, and if you're prepared to introduce a bit of bias.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think, an encouraging result.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem.",
                    "label": 0
                },
                {
                    "sent": "So I talked mainly about kind of matching.",
                    "label": 0
                },
                {
                    "sent": "I want to just briefly cover another rather elegant way to solve this reweighting problem.",
                    "label": 0
                },
                {
                    "sent": "So this is actually to use a classifier as a proxy for getting this weight.",
                    "label": 0
                },
                {
                    "sent": "So you train a classifier which is to distinguish the training and test distributions.",
                    "label": 0
                },
                {
                    "sent": "And as a side effect, with this you get weight which you can then use in your covariate shift correction.",
                    "label": 0
                },
                {
                    "sent": "After this was proposed, well, a number of times by several people, most recently Baker later.",
                    "label": 0
                },
                {
                    "sent": "The distribution here.",
                    "label": 0
                },
                {
                    "sent": "These variables are a vector of variables for your testing training points, which are one where the training distribution is drawn from and which are zero when the test distribution is drawn from.",
                    "label": 0
                },
                {
                    "sent": "So the again, the reason why you might want to do this by a classifier is because you can then perhaps define classifiers on more exotic spaces.",
                    "label": 0
                },
                {
                    "sent": "And get weights which you would otherwise have trouble getting.",
                    "label": 0
                },
                {
                    "sent": "If you are trying to do density estimation 1st and then take the ratio.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is how one gets the ratio of tested training.",
                    "label": 0
                },
                {
                    "sent": "From a classifier.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability of being drawn from the training sample given a training point in giving the parameter Theta, which you've learned, which is the Cedar police training test classifier.",
                    "label": 0
                },
                {
                    "sent": "So basically the converted procedure here goes in two stages.",
                    "label": 0
                },
                {
                    "sent": "First you learn the training test classifier, then you get your weights.",
                    "label": 0
                },
                {
                    "sent": "Then you plug those into your learning algorithm, which turns the tables from the patterns.",
                    "label": 0
                },
                {
                    "sent": "So the ways from the exits and then Yep, we're done.",
                    "label": 0
                },
                {
                    "sent": "So the vehicle paper proposed a further extension to this which I mentioned here where they just make an enormous joint model over everything.",
                    "label": 0
                },
                {
                    "sent": "So they say my model tells me first day choose something from the training or the test distribution and then I labeled accordingly.",
                    "label": 0
                },
                {
                    "sent": "So actually, just in the lips conference Masashi Seyama told me that he had some difficulties with this.",
                    "label": 0
                },
                {
                    "sent": "So his difficulty was that to cross validate the priors here, which have parameters associated.",
                    "label": 0
                },
                {
                    "sent": "So when 60 bandwidth of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You need to actually see a label test sample, which then makes the whole thing redundant.",
                    "label": 0
                },
                {
                    "sent": "Another difficulty that I think I have with it is that this idea of like choosing randomly from training or test distribution to start with is I think.",
                    "label": 0
                },
                {
                    "sent": "Like not a satisfactory intuition for the process by which coverage is because.",
                    "label": 0
                },
                {
                    "sent": "But anyway, I just put this here 'cause it's a method that's being used.",
                    "label": 0
                },
                {
                    "sent": "The argument for this method is that you can have jointly learn the features for the training and test distribution and then potentially get stronger improvement than you would by doing it in two stages.",
                    "label": 0
                },
                {
                    "sent": "So that was the argument that was made.",
                    "label": 0
                },
                {
                    "sent": "So I have some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The first one, which I'm.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to go into in somewhat more details and the others is the UCI breast cancer data set.",
                    "label": 1
                },
                {
                    "sent": "We're using a Gaussian kernel for both the kernel mean matching and for the support vector machine that we train.",
                    "label": 0
                },
                {
                    "sent": "We just fixing the bandwidth to be 5 and we're going to show performance as a function of the sea so they see is the thing that controls the tradeoff between loss and regularization, and as it set up here, a small C means that we prioritize with this.",
                    "label": 0
                },
                {
                    "sent": "So the way that we generate our data so this is an artificial example.",
                    "label": 0
                },
                {
                    "sent": "We have a data pool.",
                    "label": 0
                },
                {
                    "sent": "We split it randomly into tests and training.",
                    "label": 0
                },
                {
                    "sent": "The tests, the training data set will be of different sizes 'cause we're going to see performance as a function of training size.",
                    "label": 0
                },
                {
                    "sent": "And this is the distribution we choose from.",
                    "label": 0
                },
                {
                    "sent": "So you're basically selecting randomly from the from the pool of data according to this bias sampling.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, the good result.",
                    "label": 0
                },
                {
                    "sent": "Here for a small see what we see.",
                    "label": 0
                },
                {
                    "sent": "Is that the performance?",
                    "label": 0
                },
                {
                    "sent": "The two improvement that you get using kernel mean matching is very substantial, so here we have the average loss and you can see that across the board the covariant shift gives you smaller loss than the unweighted case which is in black.",
                    "label": 0
                },
                {
                    "sent": "The Gray one.",
                    "label": 0
                },
                {
                    "sent": "The important something one is actually using the underlying crew ratio.",
                    "label": 0
                },
                {
                    "sent": "And you can see here that for the smaller sample sizes, it's doing much worse.",
                    "label": 1
                },
                {
                    "sent": "This shouldn't be surprising because the selection procedure was set up in such a way as to create a large ratio between training and test.",
                    "label": 0
                },
                {
                    "sent": "And so having some unregularized expression of that ratio is going to give you grief.",
                    "label": 0
                },
                {
                    "sent": "So this is for a very small see, which means that we're actually prioritizing the smoothness of the classifier over the loss.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see what happens when we use a property cross validated, see.",
                    "label": 0
                },
                {
                    "sent": "So in this case, you'll see that the kind of matching actually makes everything a bit worse.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting is that for a very small value of C, you have basically pushed down the error to this value here.",
                    "label": 0
                },
                {
                    "sent": "But if you were too well, you say well?",
                    "label": 0
                },
                {
                    "sent": "Validated and carefully modeled, selected classifier, you would end up with the right answer.",
                    "label": 0
                },
                {
                    "sent": "So why is it in fact that we were not getting improvements for this case?",
                    "label": 0
                },
                {
                    "sent": "And bear in mind also that way we're also not getting improvements for the the one where we waited by the underlying crew.",
                    "label": 0
                },
                {
                    "sent": "Density.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show so to to give a intuition as to why this might be, I'll return to our toy example again.",
                    "label": 0
                },
                {
                    "sent": "This time, rather than fitting a straight line, I'm fitting kernel Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "And here you can see that basically.",
                    "label": 0
                },
                {
                    "sent": "This is following the curve pretty much everywhere, and so the only thing that you get if you take some subset of that training data and up weighted and unweighted.",
                    "label": 0
                },
                {
                    "sent": "Everything else is a reduced sample size and you're actually not helping yourself at all.",
                    "label": 0
                },
                {
                    "sent": "So the moral to this story, I think, is that if you have.",
                    "label": 0
                },
                {
                    "sent": "A properly trained and and powerful learning methods.",
                    "label": 0
                },
                {
                    "sent": "But it's doubtful that you're going to get improvement using covariant shift of any kind.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Using the waiting.",
                    "label": 0
                },
                {
                    "sent": "Sorry Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "But Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, even in this one which was chosen.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much at random, you can see actually the reweighting making things worse, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, this was.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just went experiment so we should never generalize from a single data set.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to show a whole bunch of datasets.",
                    "label": 0
                },
                {
                    "sent": "So it's a mixture of regression and classification.",
                    "label": 1
                },
                {
                    "sent": "Our sampling scheme is again similar to the one we use for the breast cancer data.",
                    "label": 0
                },
                {
                    "sent": "We basically have a pool of data we sample from it with some bias.",
                    "label": 0
                },
                {
                    "sent": "In this case, again a Gaussian bias on a specific access through this space.",
                    "label": 0
                },
                {
                    "sent": "One thing that we're doing, which might not be the best thing to do, is that we are using the same kernel size for the kernel mean matching for the classifier regressor, so this might not be optimal.",
                    "label": 0
                },
                {
                    "sent": "Right, so in all of these, all of these cases we generate the data by taking a pool of data sub something it randomly, but with some biased distribution that we have created.",
                    "label": 0
                },
                {
                    "sent": "So this is an artificial benchmark.",
                    "label": 0
                },
                {
                    "sent": "And then yeah, proceeding.",
                    "label": 0
                },
                {
                    "sent": "Up the other thing that we do is we solve for the parameters of the regression or classification over.",
                    "label": 0
                },
                {
                    "sent": "Then we cross validate on the unweighted data.",
                    "label": 0
                },
                {
                    "sent": "So we cross validate on the unweighted training set rather than the weighted one.",
                    "label": 1
                },
                {
                    "sent": "So this is again something I'm going to return to, but this might at this point already speculating a bit suspicious.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it needed to be.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are the benchmarks.",
                    "label": 0
                },
                {
                    "sent": "So to guide you through the graph.",
                    "label": 0
                },
                {
                    "sent": "These are where.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either important waiting or panel mean matching give some improvement.",
                    "label": 0
                },
                {
                    "sent": "In some cases, in most cases both do.",
                    "label": 0
                },
                {
                    "sent": "There's only one case, which is this case, where that doesn't happen.",
                    "label": 0
                },
                {
                    "sent": "But that's something that's quite striking.",
                    "label": 0
                },
                {
                    "sent": "Is that if one works in both.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some cases where both.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Statistically worse.",
                    "label": 0
                },
                {
                    "sent": "Is a case where you put it something is statistically worse in the one here currently, matching doesn't make a difference.",
                    "label": 0
                },
                {
                    "sent": "And here's one where kind of matching is worse.",
                    "label": 0
                },
                {
                    "sent": "So basically what we've shown here is that, yeah, I mean, the results are very mixed.",
                    "label": 0
                },
                {
                    "sent": "I think what's interesting is that this this phenomenon that.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One works, both work, so this seems to suggest looking at a couple of slides back then that if for whatever reason.",
                    "label": 0
                },
                {
                    "sent": "You cross validated over to core Sigrid or your learning algorithm is not the right one for that data.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to get an improvement, perhaps using the weighting, even though if you were more careful in your selection of algorithm or if you were more systematic in your consolidation, you would not.",
                    "label": 0
                },
                {
                    "sent": "So this is a way to look at that, yeah?",
                    "label": 0
                },
                {
                    "sent": "Um so when?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess final thought, which unfortunately we didn't explore very well.",
                    "label": 0
                },
                {
                    "sent": "In our original list paper, is this model selection problem for covariate shift?",
                    "label": 1
                },
                {
                    "sent": "So we have a distance measure between distributions.",
                    "label": 0
                },
                {
                    "sent": "Is this measure as a function of the kernel?",
                    "label": 0
                },
                {
                    "sent": "So what that means?",
                    "label": 0
                },
                {
                    "sent": "Is it for different kernels?",
                    "label": 0
                },
                {
                    "sent": "You get different distances.",
                    "label": 0
                },
                {
                    "sent": "And for when you use these different distances, you get different performance.",
                    "label": 0
                },
                {
                    "sent": "So here are a bunch of high dimensional datasets.",
                    "label": 0
                },
                {
                    "sent": "What you should note here is, well, 1st that it's from someone elses paper so you can trust their experiment.",
                    "label": 0
                },
                {
                    "sent": "And 2nd that the performance does show a sort of optimum.",
                    "label": 0
                },
                {
                    "sent": "And in particular that this optimum matches the best performance that they young mentality get with their algorithm which has a built in cross validation.",
                    "label": 0
                },
                {
                    "sent": "So what this seems to mean is that, again, if for whatever reason maybe waiting does work, you have to be very careful with kernel matching to choose a distance measure, which allows you to recover that reweighting effectively.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is something which I don't have a good answer for.",
                    "label": 0
                },
                {
                    "sent": "In the case of enemy matching.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There have been considerable work, in fact on choosing parameters for different coverage of correction algorithms.",
                    "label": 0
                },
                {
                    "sent": "One easy case is if there is a systematic drift in the data, which just means that you have like from past values of the covariate shift a very good idea of where it's going to go next, so that makes life a bit easier for you.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Indiana groups have been doing cross validation on the left out sample to obtain the parameters for their learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "So they.",
                    "label": 0
                },
                {
                    "sent": "It's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's it's like it's not cross validation in in necessarily a completely systematic sense, or it's it's a kind of it's a different.",
                    "label": 0
                },
                {
                    "sent": "Way to do it.",
                    "label": 0
                },
                {
                    "sent": "So basically they have their estimate of their ratio and then they they use this as they use this ratio somewhere difference.",
                    "label": 0
                },
                {
                    "sent": "They would plug it into a tail divergent so they would plug it into some other expression, but.",
                    "label": 0
                },
                {
                    "sent": "Right exactly so this is a proxy for the ground truth, so I I just bringing this up as is what the way that they do their assassination.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "In the main conference we talked about looking at the largest NMD over a set of panels.",
                    "label": 0
                },
                {
                    "sent": "Or perhaps some some other optimization over a set which might or might not be weighted.",
                    "label": 0
                },
                {
                    "sent": "This I don't know if it would be useful or not, but at least this is.",
                    "label": 0
                },
                {
                    "sent": "This is something that one might use.",
                    "label": 0
                },
                {
                    "sent": "The last point, I think is is quite an important one.",
                    "label": 0
                },
                {
                    "sent": "So again, if you know something about your learning problem.",
                    "label": 1
                },
                {
                    "sent": "For instance, that the learner uses particular features of the distribution, then you should use as features in make matching your needs, because that's that really makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's like you should concentrate on actually matching those features that you're going to use and not on trying to match every feature you can get your hands on.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, another quite important thing to very light.",
                    "label": 0
                },
                {
                    "sent": "The other thing is you remember that when I did my cross validation for the experiments I was using cross validation on the unweighted training set rather than the weighted one.",
                    "label": 0
                },
                {
                    "sent": "So this does introduce a bias.",
                    "label": 0
                },
                {
                    "sent": "But again, coming back to our Ridge regression examples because we were using powerful learning algorithms here, it seems that the cross validation he got on the weighted set pretty much did it change when we use the unweighted set.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that was.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in summary.",
                    "label": 0
                },
                {
                    "sent": "I think, well, the.",
                    "label": 0
                },
                {
                    "sent": "Currently, matching allows you to do covariant shift without doing first density estimates of the training and test distributions.",
                    "label": 0
                },
                {
                    "sent": "It allows you to match only particular features of the distributions and so for instance, on structured domains like strings and graphs.",
                    "label": 1
                },
                {
                    "sent": "This might be an approach to use.",
                    "label": 0
                },
                {
                    "sent": "Where the performance improves in our sort of very generic benchmark experiments.",
                    "label": 0
                },
                {
                    "sent": "Was mainly when the learning algorithm was simpler than the data would warrant.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can get very big performance improvements and this almost has the favor of local learning in the sense that you if you have a learning algorithm which is simpler than the one you should have.",
                    "label": 0
                },
                {
                    "sent": "Then focusing in on your test data is going to help you.",
                    "label": 0
                },
                {
                    "sent": "If the learning algorithm is as helpful as it needs to be, then it might still be useful to do per variant shift in the sense that it gives you a little bit more play in your cross validation.",
                    "label": 0
                },
                {
                    "sent": "So reweighting based event shift just because if you haven't consolidated properly it, it might actually help you.",
                    "label": 0
                },
                {
                    "sent": "Or if you're using an algorithm which is not completely suited to your data.",
                    "label": 0
                },
                {
                    "sent": "At model selection.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}