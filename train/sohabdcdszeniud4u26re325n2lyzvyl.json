{
    "id": "sohabdcdszeniud4u26re325n2lyzvyl",
    "title": "Kernel methods for integrating biological data",
    "info": {
        "author": [
            "Dick de Ridder, The Delft Bioinformatics Lab, Delft University of Technology (TU Delft)"
        ],
        "published": "Oct. 14, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/prib2010_deridder_kmi/",
    "segmentation": [
        [
            "Well, thank you very much for inviting me to this nice place.",
            "And these lovely this lovely weather.",
            "I don't know whether you arrange that.",
            "So this tutorial is going to be about kernel methods for integrating biological data and to a much lesser extent the news talk is going to be based on our own work, so it's basically an overview of methods around to combine kernels to solve biological problems, and at the very end there will be some some work that we did.",
            "Together with Mark Holtzman.",
            "Done sorry, then an embassy student which got published last year.",
            "So we work in Delft in the Dalton versus Knology.",
            "And what we call the biometrics lab.",
            "And when I was invited to to give this tutorial, I had to come up with an abstract.",
            "And so I thought of this title.",
            "And then when I look back to make the actual slides, I realized that this stated more solution rather than the problem.",
            "So I thought maybe in the in the presentation I should start by first explaining what the problem is or what the typical problem is."
        ],
        [
            "So our last few years I've worked with Biotechnologies quite a lot and that's because Delta University.",
            "We have strong Department of Biotechnology.",
            "And they are involved in.",
            "A lot of them are involved in manipulating microorganisms to do interesting stuff.",
            "An interesting stuff here usually means producing interesting things, for example here this this weird fungus, Aspergillus Niger produces citric acid, which doesn't sound very sexy.",
            "But actually it's a book project and gets produced in terms of billions of litres each year.",
            "The bacteria could use to produce cheese, yogurt, milk, penicillin, and a personal favorite of mine and most health researchers sacrifices cervisia, which is used to make beer and the like and brads, but mainly beer."
        ],
        [
            "OK, so one problem that we run into and actually.",
            "We're going to hear about more about that later is just got back industrial protein production.",
            "We work together with an industrial partner.",
            "Who used one of these microorganisms to produce enzymes, which are then used in industry to make better beer, better, better, better, whatever?",
            "And they used to sell factory.",
            "So basically they build big vats of these of these microorganisms and they want them to produce a certain protein.",
            "And if you want to easily collect that protein, you want the Organism to excrete the primary protein, right?",
            "So not that you don't have to 1st kill the microorganism, break it apart and separate the protein that you're interested in.",
            "So you need to do two things.",
            "You need to take this microorganism.",
            "We need to glue in a new gene.",
            "Integrate and Eugene.",
            "Include a strong promoter in front of it so that you know that it will be transcribed.",
            "So that you will get the highly express gene.",
            "That's the first budget in the second part you need is that you want this protein to actually be secreted by the cell so that you can collect it on the outside."
        ],
        [
            "And this is really a large industry, so there's a couple of these enzymes and proteins are routinely made now in these large factories, but if they want to come up with a new interest in new proteins, new enzymes, what they do is they start in the lab.",
            "They do a test phase in which they actually integrate this gene.",
            "I'll see whether it's created and that whole test phase is very tedious and costly.",
            "So the question that they posed to us this, can we predict what proteins can be successfully successfully expressed so created?",
            "OK."
        ],
        [
            "So that makes it, I think, about informatics problem rather than the biological problem.",
            "Basically, the biologists are too expensive or too lazy, and then they need a computer or a computer scientist to help them out to solve things.",
            "So this expression is relatively easy.",
            "I think they've got that down so they they know that if they put in a gene and they put in a promoter sequence in front of that will almost always work.",
            "But secretion is very hard to get right, and there is a sort of basic idea of how this would work, although I'm always amazed by the affected biologists tend to put up models like this and sort of claim that they understand how it works.",
            "And then there's tons of exceptions to the rule and so on.",
            "So the basic machinery is that these proteins that developers are RNA that gets transcribed and translated into proteins, and I've got transported through the endoplasmic reticulum and then to the Golgi complex, gets packaged in vesicles and then released to the outside of the cell.",
            "And actually, the proteins carry signal sequences, sort of stamps or zip codes if you will that tell the cell users to say where the protein has to go.",
            "But the signal sequences are not unique, so there's a sort of vague consensus on what they should look like.",
            "But as many exceptions, there's a lot of alternative routes, and the whole model, so it completely goes overboard once you start considering proteins that are not normally executed by the by the Organism."
        ],
        [
            "So that makes it a pattern recognition biometrics problem.",
            "We don't have a model.",
            "We don't have a good idea what we should do, and then when all else fails, we turn to pattern recognition."
        ],
        [
            "OK, so they already did a lot of groundwork at this at this company.",
            "They had to have a data set of 680, almost 700 proteins for which they attempted secretion.",
            "So there's a.",
            "Objects are proteins.",
            "The labels are weathered.",
            "Secretion was detected and this is a bit of a weird level because they look for a certain band on the gel or this is a pretty subjective way of saying whether this accretion or not, but we that's all we have for the moment.",
            "They couldn't find back to original gels.",
            "And more about this problem will be told tomorrow by bus down from there.",
            "Who did some initial work to see what you can do with this data set?",
            "OK, so two questions remain.",
            "What is the target?",
            "What is the problem?",
            "We're actually trying to solve?",
            "And what are the features?",
            "What is a good representation for the service this brings?"
        ],
        [
            "Well, actually what the biologists are going to do with this is that there.",
            "Sorry.",
            "They're going to use any predictions that come out of this pattern recognition approach to go back to the lab and then try to see whether the predictions actually hold true.",
            "Right, so much like meals, for example, with the transcription factors, we don't want to suggest to them that we can give the answer, but we want to give them at least a couple of suggestions and we hope that many of them are correct, right?",
            "So we're more interested in an area under the RC curve.",
            "You probably all know which blocks the false positive rate against a true positive rate, and we hope to be somewhere over here where we predict.",
            "A large number of true positives of proteins that can be secreted at a very low number of false positives.",
            "Proteins that that fail.",
            "Actually, what this what this axis conveniently hide is that they run actually from zero to one, but there's many more possible false positives than true positives, so many more.",
            "Proteins does fail to be accreted than proteins that can be excluded, so this number the actual number could actually be something like 10,000 and over.",
            "Here it could be maybe a couple of 100.",
            "So that even if you're over here in terms of an RC, it looks good, but it means you would have to do thousands of experiments to get a few 100 proteins.",
            "And they're also not interested in that.",
            "So then we can use a partial area under the curve.",
            "Only look at the the personal number of true positives you get until a certain maximum number of experiments.",
            "You have to do.",
            "OK, so that's all."
        ],
        [
            "Literally straight forward.",
            "Now the features, right?",
            "So in many many applications of pattern recognition at least once I used to work on the features you have to choose for quite straightforward if you want to do face recognition, you take a camera, you take a picture of someone's face.",
            "That's about it then.",
            "Maybe you have to do some feature extraction, but the basic elements that you have there so the basics, the measurements that you have to do is pretty clear.",
            "I think in problems such as these it's much much harder, so we have.",
            "I only thing we have available that we have that we can work with our DNA sequences so we can translate these into protein sequences.",
            "And then we can come up with a number of characteristics which we think might be predictive of this secretion.",
            "So we might think that long sequences are much harder to secrete.",
            "Or they may be easier to screen because maybe they fold and so are do not interact as much with the rest of the machinery.",
            "It could be that certain amino acids make a protein easier.",
            "Easier to secrete or less easy to create.",
            "At least not exactly.",
            "Amino acids at certain positions, but just the composition of the protein in terms of amino acids.",
            "Or that subsets of linear assets like the Group of basic amino acids chartered charged amino acids and so on help predict that?",
            "Right?",
            "Even even further removed from the actual sequence, we can calculate things like Golden Adaptation Index, which is a measure for how likely it is that this gene will be highly expressed.",
            "Hydrophobicity felta protein does it.",
            "Does it dissolve well, and so on.",
            "And becomes less and less easy to capture this in numbers, right?",
            "The things that we normally work with."
        ],
        [
            "If I bought it here this.",
            "Their standard picture of pattern matching problem looks like we have two measurements, 2 features X one X2 usually seem to be real numbers."
        ],
        [
            "So it becomes harder and harder to translate all these things into real numbers."
        ],
        [
            "We can even go step further and use other predictors outputs as possible inputs for our system.",
            "For example, the presence of a signal sequence there specialized particular that some celery localization, whether it's predicted that this this protein, if it's predicted that this protein will stay inside the cell, is very unlikely to go outside.",
            "And so on.",
            "So what we have is a very heterogeneous bunch of data sources.",
            "And prior knowledge.",
            "So we have knowledge which echo information and data.",
            "And they all need to be integrated in some way to solve this problem."
        ],
        [
            "OK, so that brings me to the first topic, integrated bioinformatics.",
            "I'll give you a quick overview.",
            "This was sort of a sketch of a single problem in integrated buying."
        ],
        [
            "But in general, integrated by Infomatics is much.",
            "I think it's much like what about new called this data based approach that we have a bunch of data and prior knowledge and we like to mine the data and use everything we have to come up with.",
            "Global pictures often goes on rather than having detailed kinetic models for single serve parts of this.",
            "So I'm firmly in the ocean on the left side of new Sir graph at the moment.",
            "Although I may move a bit towards the right now the folder stock.",
            "So the overall goal of integrated graphics to construct such networks of biochemical interactions.",
            "Making use of everything that's around and then using these, these networks are starting points for doing further research, right?",
            "So you look at these networks.",
            "You might notice certain modules that have certain functions an wanted to.",
            "Proteins that do not have yet functional attached.",
            "Well then you can test whether these modules have that function.",
            "And basically integrated mathematics deals with answering 2 main types of questions to construct these networks.",
            "That one is that given information on interactions between proteins RNA.",
            "DNA, whatever.",
            "Can we predict new interactions without having access to data, or given that we have a certain functional annotation for some protein or molecule, can we derive a functional annotation for a different molecule which is connected to that?"
        ],
        [
            "Actually, the level at which these interactions can occur or multiple levels is staggering, right?",
            "So the default picture.",
            "I think Neil talk mostly about these levels and then this type of interaction, right?",
            "Gene gets transcribed into RNA.",
            "That has translated into protein which may influence the regulation of another gene.",
            "But in fact, the number of the amount of crosstalk and interactions between all kinds of different levels is huge.",
            "So these proteins, connectors, enzymes which influence the rate of production of metabolites.",
            "Small chemical compounds.",
            "These metabolise interacting with proteins, activate or deactivate them.",
            "Proteins form complexes.",
            "RNA can influence the degradation of other types of RNA, and I think there's probably even levels in here that were completely missing at the moment.",
            "Right, realizing this RNA interference is also just very recently discovered, it likely that we're just missing entire levels of regulations, so.",
            "So that's not a simple problem."
        ],
        [
            "I think for the moment, at least in this field that focuses on two type of two types of interactions.",
            "One, you might call genetic interactions, which is sort of a catch all term, meaning there's some functional relation between.",
            "Jeans, for example, a gene might influence the transcription of another gene or two genes might when you knock him out simultaneously, might lead to a different phenotype.",
            "For example, the cell dying or something like that.",
            "And a lot of work has been performed on protein protein interaction, and that's mainly because there's a lot of data available for that, and protein interaction is important to understand signaling.",
            "Like to be 53 I think, but also transport and complex formation.",
            "And for our particular problem transport would be interesting."
        ],
        [
            "So we have other availability, really vast amount of data.",
            "These days, every biological paper that produces or that presents a high throughput measurement data set.",
            "Uh also is accompanied by a submission of the data set into a database.",
            "So if you now go to the IBI website array, expressive thinks called and Geo website that.",
            "NCBI in the states.",
            "You can download download thousands of microarrays on all kinds of different phenotypes.",
            "The same starts holding for metabolite data, protein data, and so on.",
            "There's measurement databases, soft protein interaction, MIPS and DIP, and so on.",
            "So it's really easy to get access to lots and lots of partial information on all these processes."
        ],
        [
            "Besides that, we have a lot of prior knowledge people have already curated lots of these views on the data.",
            "So we have databases that contain annotations, pathways such as gag or REACT.",
            "I think reactor.",
            "A chemical reactions, even literature, all contains information on what this network should look like."
        ],
        [
            "OK, so we have all this data.",
            "We have a particular problem Now what we do?",
            "Well, if you think about it is predicting of these interactions right?",
            "So I did these interactions on these functions.",
            "That is really, in essence, pattern recognition problem.",
            "We have a bunch of measurements and data and we as output we try to predict the presence or absence of this interaction or function.",
            "So we have an input, a classifier, an output."
        ],
        [
            "Now, as I said, we have very heterogeneous data sources, right?",
            "There's lots of different types of data around, so somehow we have to integrate that data into a single classifier or single output at least.",
            "Well, the standard approach to do that is just.",
            "We might call this feature Fusion, but that's basically the simplest approach.",
            "You've just been pulling all your features together in a feature vector.",
            "Combining that linearly or non linearly in a classifier to produce a single output."
        ],
        [
            "The extreme opposite of that would be to apply single classifiers to each individual data source, and these classifiers can be optimized for their data source.",
            "So for example, for certain real value measurement you might use a linear classifier an for.",
            "For another string based measurement, you might use nearest neighbor type approach.",
            "And as long as these classifiers all outputs something similar, for example of posterior probability or approximation of that, then you can combine these into a single output by maybe fixed rule or again a classifier which combines all the inputs.",
            "So here all these individual features are first translated into a sort of vote on the possibility of having certain output, and then these votes are aggregated into a single output."
        ],
        [
            "There's a couple of problems with both these approaches.",
            "For early integration, you need to convert all the features that you have into the single representation, right?",
            "So usually real values, and for a lot of data that's quite straightforward, but for example for sequences it's already usually becomes a bit awkward.",
            "I had to go back here."
        ],
        [
            "Yeah, so here we have an example of a feature called Gravy.",
            "Which is a sort of hybrid hydrophobicity index which you can calculate over the entire protein sequence.",
            "And we might have the idea that's that it's important that certain areas of the protein are either highly hydrophilic or hydrophobic.",
            "And then we already have to come up with some measurement like OK, there's some threshold, and then we count the total area of these curves outside, above or below the threshold.",
            "I think there's a normalization issue because this is the this runs from the length of the protein sequence, so for longer protein sequences shot after normalizing tone.",
            "So this is already quite involved."
        ],
        [
            "Late integration is more straightforward, but still there you have to choose the combination mechanism and from being around people who do classifier combination, I can I can vouch that for every problem there's probably an optimal optimal way of combining classifiers.",
            "But a different way, so not easy to tell beforehand what you should do.",
            "Um?",
            "And all knowledge of the fact that this data etc genius is sort of lost in the process."
        ],
        [
            "So it may be that one of these classifiers bases its prediction on high quality, low throughput measurements, right?",
            "So really trustworthy data, whereas another classifier performs its predictions based on, well, sort of model.",
            "Probably vaguely connected evidence.",
            "But as soon as you get here to these outputs, all that knowledge that they have different back."
        ],
        [
            "Is lost."
        ],
        [
            "So when people start to think about it and I think it's probably just going to term intermediate integration so well, actually what you want to do is first translate all these features, all these measurements into common language.",
            "An intermediate representation that allows you to apply a single classifier.",
            "And if you think about this, a couple of candidates to to allow that.",
            "Probably there's more but common languages that we did.",
            "You're probably familiar with our probabilities.",
            "Similarities with these similarities or kernels."
        ],
        [
            "So if you translate all the knowledge that you have into probabilities, then you might end up with something like a Bayesian network or basing classifier, right?",
            "So an example here from the forgotten Sky.",
            "Yeah, that's why she tried to predict functional relationships between genes, and this is too hard to read.",
            "But does everything down here, from biochemical essays to to experimentally verified complexes, which then are aggregated into some sort of probability of physical Association.",
            "This is genetic or functional Association and this is stuff like coexpression and so on, which is down again aggregated into one vertex on function."
        ],
        [
            "But you can also combine P value so if you have some statistical test.",
            "That gives you some score and wish you can convert into a P value.",
            "Then you could combine them using for example Fisher's method into a new overall TP value, which follow some guys squared distribution and then by applying different weights you can assign different different levels of importance to the different features that originated these P values.",
            "So these features test another, well known ones is that score.",
            "If you can transform everything, everything does that scare.",
            "You can just send these scores and they will follow again normal distribution, from which you can derive one single value."
        ],
        [
            "These similarities can also be used to integrate data.",
            "This is usually quite intuitively easier than the P value approach in that if you ask people what is the probability of this occurring or what is the probability that a protein with such a sequence will be treated, you will likely get a blank face.",
            "We've asked him how much do these two proteins differ.",
            "Then it's easy to come up with an answer, right?",
            "You blast them against each other, or you come up with some.",
            "Measurement of distance between your sequences.",
            "And then using these sequences using these distances, you can build classifiers, do clustering and so.",
            "So one example could be and these are not real examples, but just sort of quickly made these up in Matlab is where you have.",
            "You can calculate a correlation matrix here for how for motif binding, so we have a bunch of motifs we check for each gene, which of which of these motifs can bind.",
            "How we calculate correlation between these multi binding factors?",
            "For the same genes we might have gene expression data and we might have to Gene ontology annotation information which we can also turn into a correlation measure.",
            "And then we could convert these correlations into a single combined distance matrix, which we can then use for classification or clustering."
        ],
        [
            "OK, I'm finally on the final way of integrating this information.",
            "The one I'll be talking about for the rest of the material is by integrating kernels, and I think of kernels have sort of well founded similarities, right?",
            "So it's basically basically doing what we did here, but without all the hand waving and ad hoc approaches.",
            "So for our problem in predicting protein secretion.",
            "We might take the amino acid composition just listed as a vector of how many amino acids, or the frequency of amino acids in a certain protein and put that in a vector kernel.",
            "Or we might take a signal sequence.",
            "So first part of the protein and apply a weighted Rico nor which compares strings.",
            "So all kinds of different information can be on different forms of information, different types of data we can apply specific kernels and then combine these to apply a single support vector machine or other type of learning method.",
            "Is this clear so far?",
            "Any questions?"
        ],
        [
            "OK, so how many of you have never not worked with, not read about support vector machines?",
            "OK, I can go through this really quickly.",
            "So the.",
            "The whole idea of these kernels came up when people in the early 90s started working on ideas by Pop Nick in from the 60s and 70s where he came up with a linear classifier and said, well, if you want to have an optimally generalizing classifier in some sense we have to maximize the margin.",
            "Around this classifier, right the maximum margin classifier and you can actually show that boils down to minimizing the weight factor in this linear classifier subject to the constraints that all points are on the correct side of the classifier."
        ],
        [
            "OK well."
        ],
        [
            "I'll go as you all know this.",
            "I'll go through this quickly.",
            "What helped was that there was an easy sort of this is an optimization problem.",
            "There's an easy dual formulation of this optimization problem where you can ride the weights here.",
            "They turn out to be the possible to write them in terms of a linear combination of the training objects and the training labels multiplied by some factor, and only the support factors here.",
            "The ones on the margin get a weight factor larger than 0."
        ],
        [
            "OK. Then the kernel trick which was.",
            "But later added."
        ],
        [
            "People say, well, this is now all written in terms of DOT products between vectors.",
            "Here training vectors and here dot product between training vector and test samples at."
        ],
        [
            "So what we can do is we can replace these dot product dot product between these vectors mapped into some feature space."
        ],
        [
            "And then actually, under certain circumstances we can replace it by a general kernel function as long as it fulfills some constraints and immersive virtual kernel that he talked about that Neil talked about.",
            "We can replace this inner product by this kernel function and whole machinery still works, so we still have a classifier linear classifier in some space."
        ],
        [
            "And holiday of this function is that it would map the data into space, which makes it easier for us to classify and one standard examples is that if we have this problem which clearly cannot be separated by linear classifier.",
            "But we apply a transformation into a space where in which the first 2 features are the squares of the original features.",
            "Then all of a sudden it becomes possible to apply a linear classifier here, which corresponds to quadratic classifier over here."
        ],
        [
            "Right?",
            "So if we can come up with a decent kernel.",
            "Then we have this standard classification algorithm which allows us to get a nonlinear classifier in the original space.",
            "Example for this problem, if you have a polynomial kernel which basically the dot product you add one, you raise the whole thing to the power D for D is 1, you get a linear kernel which doesn't separate the data for these two you get this dashed thing, which is still not very good for these free you get a polynomial for power polynomial which is able to separate the two classes.",
            "This is the exponentiated quadratic kernel, which, when applied to this data you can also get all kinds of different shapes of classifiers from a linear one.",
            "If you set this Sigma very large to one which is highly nonlinear, if you said it very small."
        ],
        [
            "OK, I think.",
            "This is all pretty well known yesterday.",
            "You don't actually have to know this feature mapping to construct this kernel.",
            "Most people now do the other way around.",
            "Their first come up with interesting notion of dissimilarity, constructed kernel and then try to see whether it leads to positive definite kernel matrix for any possible input.",
            "If you can prove that, then you have a kernel no matter what feature space it may correspond to.",
            "And since then, there's lots of classifiers that have been formulated in terms of Colonel Sanders, nearest mean classifiers, nearest neighbor classifiers, Fisher linear discriminant analysis has been kernelized.",
            "Elsu"
        ],
        [
            "Classifiers so how do we train these things well?",
            "That's actually and I would realize that Gaussian processes do this much more elegantly, but we do this using standard optimization package for solving quadratic problems.",
            "To optimize this this way, factor and this bias term, and then the other parameters there a bit of a hassle, so there we have to do something like cross validation to find a peaceful support vector machine.",
            "This trade off parameter between errors and margin, and the kernel parameters and usually for a single problem.",
            "I don't have 30 yet, but later when we get to the combination that.",
            "Can be a problem.",
            "And it's not usually the whole the whole performance of the algorithm is not that sensitive to the exact choice of.",
            "For example, C and Sigma in this in this example.",
            "So if you're somewhere around here you do you do reasonably well, but if you have a bad choice, you can end up getting pretty poor performance."
        ],
        [
            "OK, so since the classifier there have been lots of other kernelized machine learning type algorithms, support vector regression, which already I think proposed by public.",
            "But I worked out by by smaller show up.",
            "Or at least print this paper.",
            "This picture is from smaller show up.",
            "If you come up with with a decent loss function here so you don't have a quadratic loss function But a loss function which is insensitive in a certain tube and then linear around that.",
            "Then again, you can write this regression as a very similar optimization problem and apply the same machinery As for the support."
        ],
        [
            "Classifier to do regression.",
            "And depending on the width of the tube, you approximate your data too.",
            "The better or worse.",
            "This is very hard to see, but here is.",
            "Distill the two curves that you can see.",
            "They they give the width of the tube, and then actually the approximated function runs between.",
            "Runs between here like this, so here you have a very wide approximation and here you have very.",
            "Tight approximation."
        ],
        [
            "And then came kernel clustering."
        ],
        [
            "And kind of PCA, which is very hard to interpret for me at least.",
            "What happens if you have a first degree for the polynomial kernel and then a PCA in that space?",
            "Colonel LDS African record Canonical correlation analysis.",
            "So basically there's a whole zoo of."
        ],
        [
            "Applicable algorithms based on kernels, right?",
            "So once we have a decent set of kernels or decent kernel for our problem, then there's this whole stack of literature and actually implementations that we can turn to solve a problem.",
            "Any questions about this?",
            "Question."
        ],
        [
            "Instrumental break.",
            "Yeah.",
            "OK, so as I said, if you have good kernels then there's the whole machinery to solve the problem you have.",
            "So the kernels that were proposed earliest where.",
            "Linear kernels polynomial kernels exponentiated quadratic kernels right?",
            "So simple kernels on vector representations of your original objects?"
        ],
        [
            "So from now on, every now and again I'll show a slide that about applications of these kernels to this protein secretion prediction problem that Boston line made over the last couple of days.",
            "So this probably not highly optimized, but we just played around a bit to see what we could get away with.",
            "So this is support vector classifier on the 680 and somewhat proteins where we predict whether they succeeded or not.",
            "We do 10 fold cross validation and this is the average.",
            "Every other girl forget.",
            "We have a bunch of features which must, now we'll discuss in more depth and we can divide them, sort of roughly into three subsets, one based on sequence amino acid composition, one based on sequence, but derived from that.",
            "So predictions of hydrophobicity and hydrophilicity.",
            "And then sort of a bunch of various features such as sequence length, Golden adaptation index, isoelectric point, stuff like that.",
            "So if we just take all the features, put them into one big feature vector, and apply a linear kernel, we already do quite well.",
            "We got this.",
            "89.73 area under the curve.",
            "If you apply an RBF kernel, we do slightly better and then you actually see that in this case it would pay off to look at the subsets, because this sequence.",
            "If we just looked at you in the amino acid composition and apply an RBF kernel on that, we do actually best among these options.",
            "So in some way here the other features.",
            "Make that we do worse, although I must say this is all highly non significant.",
            "If we repeat this experiment for sure likely to be off by bit."
        ],
        [
            "OK, so one standard approach for almost any other data type that you can come up with is the empirical kernel map.",
            "I think it's important though.",
            "Right, so as long as I said the nice thing about these similarities that I talked about earlier is that they're quite intuitive.",
            "So lots of times people can come up with.",
            "Nice, I just received this similarities and quite often these are highly non Euclidean, even non metric so they cannot be used directly as a kernel.",
            "But what we can do is that we we have a bunch of examples which might go template set.",
            "We just calculate the dissimilarity between our object and each of these objects in the template set.",
            "Turn that into a vector and then apply a linear or whatever kernel you want to these vectors.",
            "Right?",
            "So actually here you use transformation into a vector and then application of standard kernel as a trick to get away with using these non Euclidean metric type of these similarities and you'll see that trick get applied in more standard kernels later on as well.",
            "So one example is the last kernel.",
            "Wish you have a bunch of proteins and then you calculate you have two proteins which would like to calculate the kernel for.",
            "So you blast the first protein against the template set.",
            "You get lost the second kernel second protein against the template set you calculate the minors laggy value of this lost and if they if these miners lucky values concur for objects in the template set then you might conclude that A&B are therefore likely to be very similar.",
            "So you get a high kernel value."
        ],
        [
            "Well, this kernel kernels kernels that allow you to combine or play around with kernels.",
            "Kernel addition is most well known among and that's the one I'll discuss later in terms of combination.",
            "Um?",
            "Where just a weighted weighted sum of individual kernels you can pull in vice multiplications of kernels.",
            "You can have a generalized RBF kernel.",
            "It's called so first you use your kernel to transform the dissimilarity.",
            "The similarity between.",
            "Two objects into a distance, measure again and then plug this distance measure back into the into the kernel.",
            "And you can normalize kernels, and in fact this becomes important also for the combination, because these similarities can have wildly different ranges, so you can come up with a way of normalizing."
        ],
        [
            "An interesting kernel is a convolution kernel.",
            "This specific application on next slide.",
            "This is when you have kernels that operates on submit.",
            "Operate on some parts of sequences, for example, or structures, but you don't know beforehand which subpart, So what you do is that you try all possible decompositions into subparts, so that's this summation here.",
            "So we convolve a number of colonels and colonels on these two objects an we can decompose these two objects into.",
            "Subobjects.",
            "And then for each possible decomposition, we apply the first kernel to the first subset, Subpart 2nd, going through the 2nd part, and so on, and we multiply all these kernels with some of these multiplications and at the convolution kernel.",
            "Now this of course if you would implement this naively would be horrible.",
            "So specific implementations have highly optimized algorithms to be able to avoid having to do this all explicitly."
        ],
        [
            "So one example is the local alignment kernel.",
            "So local alignment is defined as a local alignment of a certain length and then the sum over all possible lengths.",
            "And this local alignment kernel offset and length is a convolution kernel where you start out with what's called a trivial kernel.",
            "And then a letter alignment kernel which.",
            "Gives you a certain score based on the substitution cost.",
            "If both these subparts have exactly rank one, so when you're comparing individual letters in sequence and otherwise it gives you 0, so this is a way of quickly weirding out all possible lots of possible decompositions of the sequence.",
            "So this is a sort of match going, or if there's a, if there's a.",
            "If you're looking at single letters, then this calculates the score of a match at that position.",
            "And then there's a gap Colonel.",
            "That's also a possibility of a gap, and you get a score for death based on the length of the gap.",
            "So if you have to skip a lot of letters, then you will get a.",
            "Larger score.",
            "And then you apply this.",
            "You have N of these alignments and minus one possible gaps.",
            "So first and alignment possible gap, possible alignment, possible gap and so on.",
            "Your end game with this trivial kernel that gives you one number for how well two sequences align in terms of.",
            "How many gaps are there and how many?",
            "How the letters are the letters in the sequences can be compared in terms of this substitution cost?"
        ],
        [
            "I think Colonel Colonel is it also is a barrel as Colonel in many of these problems.",
            "We tried to predict, for example, an interaction between proteins are objects is actually a protein pair rather than individual proteins.",
            "But we have a lot of information available about individual proteins.",
            "So what we don't need to do is combine this information about the individual proteins into.",
            "An overall outcome.",
            "So I'm just pairwise kernel or this is very hard to read.",
            "Again, the green you take the you you compare.",
            "The combine.",
            "You multiply the kernels for.",
            "So you want to start over again to calculate a kernel value for two pairs, you calculate kernels for each individual possible bear between the two pairs.",
            "So rather than.",
            "I just you would calculate it broken or for this combination the green one do this green one the blue one and a blue one will multiply the two green one.",
            "So there is like the probabilities for the case when this protein matches this protein.",
            "This protein matches this protein.",
            "And the other case when this protein actually matches this protein and this protein matches this one, that's the blue gazing you had to do to deal with the two possibilities."
        ],
        [
            "OK couple more specific kernels so that set kernels which operates on which operate on sets.",
            "Say that you have a certain domain, for example.",
            "Words right then you could have a distribution of sets which could be document you might have a certain probability distribution over this domain.",
            "For example, a measure of the uniqueness of the word like.",
            "Can populate would be very unlikely word, so if you find 2 documents with the same word with that word then they might be similar.",
            "Westward would be much more likely to occur, so would not be count that much towards the similarity.",
            "And I'm given two sets of example two documents.",
            "You can calculate intersections, complements an agreements.",
            "So how?",
            "How much today, how much of the words intersect and how much do they agree on the difference?"
        ],
        [
            "String kernels which are quite applicable in love by informatics problems.",
            "Because we deal with a lot of sequences, the basic approach is called the spectrum kernel, where we have addiction where we create a Dictionary of all possible occurrences of substrings of a string and then, like the empirical kernel map, we construct a vector with the number of occurrences of those substrings and can apply again standard kernel to combine those.",
            "Do other smart implementations of this as well using using trees so that you don't have to go through all possible.",
            "You don't actually have to construct these entire vectors, because these can become quite long.",
            "And this versions with gaps and mismatches, there's motif kernels where you look for specific sets of up substrings, rather than all possible substrings, and so on."
        ],
        [
            "A weighted degree kernel as a sort of special case, which also takes the position into account, so this previous one just looked at all possible matches between substrings, no matter where they occurred, but waiting to be Colonel count the number of matching words at identical positions, and then this concept by the length of the match.",
            "So as an example here in this this case where we look for matches up to length 4. Who would have five matches of a single letter?",
            "So.",
            "Kind of would be five matches times a weight of 4 -- 1 + 1.",
            "We're looking at single letters now, so 4 * 5.",
            "We have two cases where 2 letters match contribution of.",
            "2 * 2.",
            "An one case we're freeloaders match.",
            "Combinations 2 * 1.",
            "So the total kernel becomes 20 + 4 + 1.",
            "I think I made an error here.",
            "This should be free, so this is actually 627.",
            "OK, more on this kernel by Anna talk connects to more often by like Christian Whitmer.",
            "Who do you have a special version of this kernel or?",
            "Version of this kernel which is specialized."
        ],
        [
            "OK, so we all be applied to this.",
            "The string kernels and a number of different kernels to our protein secretion prediction problem again.",
            "And one of them turned out to be slightly better than the result we have until now.",
            "But again, not not very really significantly, and also a bit.",
            "Worrying me with the length of the subsequence, the maximum length such consent to one.",
            "So actually it's still counting just individual amino acids.",
            "We tried to see whether we can.",
            "We could do something with the signal sequence that's supposed to be at the beginning of each product using a way to do equal.",
            "But the standard approach doesn't really give us good results yet, so we might start looking at more clever ways of doing that."
        ],
        [
            "OK, and then there's a whole bunch of more advanced kernels in which you can compare graphs or based on, for example, the number of subgraphs that occur.",
            "Or or the number of random walks that you take on the graph.",
            "You can create kernels based on generative models, for example kernels based on comparing full hidden Markov models and so on and so on."
        ],
        [
            "Right, so we have this zoo of kernels, all of which may be applicable to our problem.",
            "And applicable to different data sources.",
            "Now the question is how do we combine them to come up with one single representation to put into our algorithms?"
        ],
        [
            "So any questions about this so far?",
            "So you mentioned at some point that you don't have to really enforce the kernels to be positive.",
            "See, see my definite.",
            "It's no problem.",
            "Still up."
        ],
        [
            "All the machinery did it is around or do you in the end with some kind of transformation to make it?",
            "Now, So what I said was that was, I think here that if he uses empirical kernel map then then why you then tried to enforce it to be a kernel by first calculating this dissimilarity for number of represent representatives in template sets and then you again by a standard kernel which makes it into a positive definite kernel.",
            "But this dissimilarity can be almost anything.",
            "OK, but for any of the order.",
            "Kernels that you mentioned you can easily show that they are all well.",
            "I cannot do this.",
            "Clever people have.",
            "So I take their word for it, but yes, that's the idea.",
            "Still, in practice we sometimes have been able to get away with just calculating mean.",
            "There's a difference between proving that the kernel function will give a positive definite kernel matrix for any possible input set, or just having a positive definite kernel matrix for solving a particular problem.",
            "So we have been able to get away with using a similarity measure, which we cannot prove.",
            "We could not prove that was positive definite, but still gave a positive definite kernel matrix, so we could apply the support vector machine.",
            "Any other questions?",
            "Example that you keep showing the secret."
        ],
        [
            "Yeah, strange that.",
            "Chat.",
            "Very good results come just with their counting amino acids and you can't do any better.",
            "There's something about this problem, or is this that the data is so small?",
            "Well, the data is small, but still you would expect that would hold for all features I guess right?",
            "So the I don't see why the why that would.",
            "That would say that I mean no assets.",
            "Would that based on Justin?",
            "To your increase.",
            "Nationality yes, but I think.",
            "This is where you're getting.",
            "Well, I'm not sure it's a technical reason though.",
            "Maybe there is just not more information in this in this data than just counting individual amino acids.",
            "We are now trying to convince the kernels which are situated.",
            "Between.",
            "Similarity.",
            "So why is it the same?",
            "Why is why is getting a kernel which have similarities maybe got mapped?",
            "My problems within yourself?",
            "I did.",
            "Good question.",
            "I'm not sure.",
            "You could always map to a separate space with the kernel.",
            "That said diagonal reason you want similarities.",
            "You want to generalize well, so this argument about.",
            "Spaces.",
            "To do to show that you have the power to separate anything.",
            "But in practice the reason you want the similarities to be high between simple things is Even so that you do it in awhile regularised.",
            "Completely intentional.",
            "Wow, maybe, maybe not.",
            "You would like the classes to be separated, but not all objects would be separate.",
            "Think of a similar closer they classified with the same label.",
            "If they happen to have different labels, they wouldn't be separable.",
            "You want them to then.",
            "No.",
            "So your example of over there it's possible to separate all the objects using a linear diagonal.",
            "You have colors.",
            "You take the lift, the obvious settle down to zero.",
            "You will separate everything, but you'll overfit, and that's an example where conflicts.",
            "But somewhere in the middle.",
            "Moving the similarity between everything so there's no similarity.",
            "Then you get separation as you have similarity, you'll lose separation potentially.",
            "So what is our design principle now?",
            "The balance between.",
            "Balance between good and evil.",
            "All the kinds of misunderstandings follow from the fact that I can know is in no case a similarity measure.",
            "It's that rather nothing else but what is dissimilarity measure is the ingredient medic produced by a camel.",
            "But candle is.",
            "An inner product in some linear space into which the respective metric space is embedded.",
            "Panel will depend on the choice of the North element.",
            "Now did not you see?",
            "In all, the kinds of understanding or misunderstandings are produced by this fact.",
            "You see, Kendall is no similarity measure in the general case.",
            "Of course, if you take the usual RN and dimensional Euclidean space of real letters.",
            "In this case, the so called radial basis kernel.",
            "Yes, of course.",
            "This kind of kettles is an natural similarity measure, but not in the general case, use it.",
            "Sentence for.",
            "To try to get it really, really, really deep misunderstanding, which is actually for machine learning as a whole, you see not specifically for bioinformatics.",
            "People wrongly understand the the term of account.",
            "Try to interpret that as a senior editor.",
            "Of course, in many cases it is so, but not under general case.",
            "But any candle produces a metric is a secondary function.",
            "Their characteristic will be invariant to the choice of the null element you see.",
            "And it is this why people ask questions about kernels, a graph, what it is.",
            "But the most popular answer kennels.",
            "Yes, they are actually have the sense of similarity measures, but not a kernel.",
            "I'm not sure that answered the question, but.",
            "So I think that Murphy do end up correctly that you ask her what do I try to optimize?",
            "Well, what is the goal of this or folders of these kernels?",
            "Yeah, I think that's still unsolved, so people come up with different kernels for different problems, and there's ways of combining them and still, like in lots of transmission approaches, you have to try a number to see what works.",
            "So the idea that you that you say OK, well, I am looking at strings and this helps similarity between strings.",
            "I've gotta in code in such a way.",
            "Do those exercises well.",
            "The more similar, the more similarity I encode in my kernel the better.",
            "Necessary.",
            "Now, so there is a trade off with the with the generalization, yeah.",
            "So that's why I think in most approaches you'll see she still see that people try and see what works."
        ],
        [
            "OK."
        ],
        [
            "So the final part about combination.",
            "I said that the easiest way to combine kernels is another people.",
            "The way that people have been mostly studying is to use a weighted sum of normalized kernel matrices.",
            "This normalization is often very important because these kernels have quite different ranges, so if you want to throw them into a pool and then you should first normalize it.",
            "Can a simplest approach to do that is to just wait and more equally, just some of the kernels?"
        ],
        [
            "And if we do that for for our problem, then we get slightly better performance when we combine these three feature subsets than we did before.",
            "So this is a free RBF kernels and then combined into one overall kernel.",
            "Or just slightly better.",
            "So now the question is, can we also optimize these weights by which we combine these kernels, right?"
        ],
        [
            "And the answer is yes.",
            "That's when people have been studying for the last almost 10 years, and there's three basic approaches to filter approach to rapper approaching without my called the embedded approach.",
            "Let me filter approach you.",
            "This is much like the filter approaches in feature selection and better recognition where you come up with some of the derived criterion which is not directly classifier performance, but.",
            "Criteria which you think is closely related to classifier performance and try to optimize that.",
            "So a famous example of this would work, but Christianity on kernel alignment.",
            "We said well, I've come up with some criterion here for the combined kernel, I want to be.",
            "I want it to be as closely as possible, like the closest possible to the ideal kernel.",
            "And it's ideal kernel matrix is the outer product of the glass labels, so you would like here.",
            "Also references with you just said you would like to similarities within the.",
            "If I can get away with that term, the similarities within the classes to be as large as possible and this similarities between the classes to be yes large possible as well so.",
            "Small, small glasses that are far apart.",
            "That's what I would like to achieve.",
            "And then you can try to optimize the weights to optimize this kernel alignment."
        ],
        [
            "Second approach will be wrapper approach.",
            "For example, using a grid search over possible weight settings with cross validation as a performance measure.",
            "So using the actual.",
            "Or fax machine performance as a target or using evolutionary algorithms or using gradient descent.",
            "And this is the work by Chappelle that Neil also referred to where he came up with the gradient.",
            "I think an estimated gradient of the performance of the classifier and then did gradient descent.",
            "On this on this estimated error to optimize their weights.",
            "So this is a looper optimized.",
            "You change the weights to optimize to calculate support vector machines again, and you keep updating weights."
        ],
        [
            "Jobmaster example we did this.",
            "We have already combined these three sub feature subsets into one that gives us a 91% area under curve an by a very simple grid search.",
            "We didn't have alot of time unfortunately.",
            "We came up with this weight which actually shows that the derived kernel to kernel on the drive feature should be thrown out.",
            "We got a 92 nearly 92% AUC so slight improvement again, probably not significant."
        ],
        [
            "So did you do this?",
            "This great search on on the actual outcome or the other kind of an outer loop cross validation again too?",
            "I think I think this you did this.",
            "It's supposed to be an outer loop.",
            "Oh OK, so very simple.",
            "It has to be better, right?",
            "So it cannot be worse because yeah.",
            "The one with equal weights is kind of in and out here.",
            "Well, there's a landscape that you search over and the one the one point of equal weights 111.",
            "That's in depth so.",
            "Definition it's gonna be.",
            "So we realized this problem of the double loop when we started last week.",
            "And we also tried to add a couple of these specific kernels and they didn't give any better results in this specific example.",
            "So we're the spectrum kernel and weighted degree kernel.",
            "When we combine them, we got a slightly worse result again."
        ],
        [
            "OK, and the third approach to combination so we've had to filter approach in a wrapper approach.",
            "It approaches what you might call the embedded approach, in which you try to solve the entire optimization problem at once.",
            "So remember that this was so this coming by very quickly the original formulation of the dual formulation of the support vector classifier problem."
        ],
        [
            "And Lockheed proposed to just put the combined kernel in here, treat them, use the weights as additional parameters, and then minimize this thing over the weights, adding a couple of constraints.",
            "And he showed how you can see this as a semidefinite programming problem, which sounds like there's a lot of easy solutions for it, but still is very hard.",
            "And then later people came up with faster approximations for faster versions of this, right?",
            "So this is now often being done.",
            "This is known as multiple kernel learning."
        ],
        [
            "Interesting, Lee, later it was shown by my boss and also shipped out again that if you look at the original SVM primal problem.",
            "That looks like this are minimizing the weight.",
            "The normal norm squared and two number to weight factor and at the same time minimizing the number of errors he makes."
        ],
        [
            "That if you translate back."
        ],
        [
            "Just you formulation into a prime or you end up with sort of blocks version of that where you minimize.",
            "And L1 norm over a per kernel L2 norm.",
            "Basically what this boils down to is that there's an additional one normal similar to assume regression where you try to force weights kernel weights in fact to zero and kernels with non zero way to support kernels.",
            "The entire mathematical derivations it's quite hard."
        ],
        [
            "But they showed later.",
            "Which showed later that it actually looks like this that you have and in a problem where they try to predict protein localization.",
            "We use 69 different kernels in a couple of groups.",
            "Interestingly, this access your starters or .2 so these kernels didn't make the cuts.",
            "They perform very well.",
            "Very bad, very poorly.",
            "These are the scores they get with each individual kernel.",
            "This is the combined score they get and then this is what the weights look like.",
            "So you see that these weights are actually very sparse.",
            "There's not not all these kernels are used, just a few ones.",
            "Are you?",
            "So that's a sparsity and forged by this implicit L1 norm in the in the optimization."
        ],
        [
            "OK, and more recently people in the machine learning community have been studying this an extending this, so there's I summarize a lot of work now in one slide.",
            "But there are references are there.",
            "They move from 1:00 AM to gym or general LP norm where you can specify P and then they show that in some cases this this one original multiple kernel learning on some problems even performs worse than the original support vector machine.",
            "But if you don't play around with this P you can start doing better.",
            "So here the black Lines sports computer, the lower redline.",
            "That's the multiple kernel learning by playing around the speed you can do slightly better and you move through less sparse solutions.",
            "And there's other extensions to multiple Conan, localising, feature space, nonlinear kernel combinations, etc etc."
        ],
        [
            "OK, how much time do I have?",
            "Tell me a couple of example applications.",
            "So one is predicting about actually the first well about predicting protein interaction, but in very completely different ways.",
            "So this was work by Mark Holzman, Master student of mine in 2008.",
            "Where he basically collected a large number of features having to do with these proteins, myologie go expression colocalization.",
            "So on 49 kernels into 49 features in total and the goal was to predict protein interaction and he came up with an evolutionary algorithm to optimize this with the weights.",
            "And while the main finding was actually that after a lot of computation, we were just slightly better than just summing these kernels, so it's not very.",
            "I very happy finding we were slightly better, but he also found that this multiple kernel learning without any data tricks like regularization.",
            "There's multiple kernel learning even performed worse than just summation.",
            "Right, and that's something you don't read very often in the newspapers, and Colonel alignment was even much worse."
        ],
        [
            "So I think I'll skip this is horribly complete."
        ],
        [
            "This figure I'll skip this one here.",
            "You also see better sparsity of the multiple kernel learning approach, where these are.",
            "These are the weights of the various kernels that we use.",
            "The evolutionary algorithm sort of spreads all the weight around.",
            "And you see also that these are five different runs of the algorithm, so for different runs you get reasonably different solutions based on cross validation, splits and so on.",
            "The multiple kernel learning is more consistent in assigning the weights and also assigned lots of legacy features.",
            "A zero wait, so this is the sparsity at work here."
        ],
        [
            "A second example where this is recent work in by Japanese group is where they also tried to predict protein interaction, but now using literature, so the input now is not at all measurements, it's just sentences in this form taking furniture F1 is activated by Jack, two in the presence of P21 rest.",
            "Richard already preprocessed this data to the extent that they have sentences which identified two proteins and now they would like to find whether this sentence says that these proteins interact or not.",
            "So they applied boxes.",
            "I think I'm not at all unexpected bosses, but they have two different passes, one which gives a quick and dirty result, and one which really decomposes the sentence into a hole.",
            "Free."
        ],
        [
            "And then I have very complicated setup too.",
            "Really apply three different kernels.",
            "The output of Apple trees of each of these parsers.",
            "So a bag of words, kernel which is set kernel one kernel which count the number of common subtrees anagrafe kernel.",
            "So they normalize them and then combine them and then again apply support vector classifier and they show that combining at least a number of these.",
            "These things make sense.",
            "But again, here they showed up.",
            "So the individual individual kernels catch you do around 5559% performance combining the tree and the graph kernel gets you to 62% score 62% score, but combining everything lower score again."
        ],
        [
            "And a final example, just to give you an impression that you can do completely different things with this.",
            "Is a reaction kernel which was proposed earlier this year by by a group in Finland.",
            "I think you owe is also here.",
            "Not sure in this room, but he will.",
            "He will present a similar work here on Thursday as well.",
            "But the input is now a sequence representation, not exactly the sequence, but the representation of the sequence of an enzyme, and you try to predict an entire reaction that can be catalyzed by the enzyme.",
            "So I'm not just a description of the enzyme that entire reaction, so a bunch of molecule graphs.",
            "So this becomes this is no longer a standard classification problem.",
            "It's a structured output prediction problem where you have a density over all possible reactions and so on.",
            "But actually there's now less than kernels that you can come up with that compare molecule graphs based on the number of common subgraphs they have.",
            "Alright, so this shows up fully that that once you have this mechanism in place you can you can come up with really wild kernels to try and solve your problem."
        ],
        [
            "OK."
        ],
        [
            "Conclusion So integrated informatics we we try to combine all the knowledge and measurements we have about a certain problem to come up with predictions of interactions and functional annotation.",
            "Now we have a lot of heterogeneous data, so this calls for intermediate integration.",
            "Probably this made the case that we this is a right way of approaching such problems.",
            "Colonels are very good vehicles for this.",
            "Mainly because a lot of people in the machine learning community are doing very hard work to give us new algorithms and.",
            "And a combination methods and also there's a wide variety of kernels that you can apply to biological problems.",
            "Um?",
            "And it's very easy to start playing around with.",
            "There used to be a wide variety of tools, but recently the winner Richard Group has come up with sort of collection into one single package called Shogun Toolbox.",
            "You can download it and start playing around with it very quickly."
        ],
        [
            "So this Colonel combination that's still not free lunch normalization is important, but kernel combinations can overtrain as well.",
            "I think of in our quickly hacked together example.",
            "We already saw that.",
            "The kernel classifier parameters have a large impact that require computing computationally intensive procedures to set.",
            "So maybe Gaussian processes are the way to go then.",
            "I'm also there.",
            "I also heard that you have to be able to solve nonlinear.",
            "Do nonlinear optimization problems right?",
            "Yeah.",
            "And filled the whole.",
            "The whole approach is computationally intensive, especially for genome wide data set.",
            "So for protein interactions you want to predict on 10s of thousands of.",
            "Proteins which gives you 100 million protein pairs, and so on, right?",
            "So this is not easily solved.",
            "You really need to think about how you do it.",
            "And as I think that a couple of examples I've showed is that.",
            "Especially on the data points that it's critical to choose to write kernel, and I think there's no no Handbook yet on how to choose right kernel.",
            "How to choose the right combination of kernels.",
            "But they keep it simple.",
            "Stupid principle applies, and very often this simple summation already works quite well.",
            "So that's just let me think the first thing you try."
        ],
        [
            "OK, that's it.",
            "Thank you very much for that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, thank you very much for inviting me to this nice place.",
                    "label": 0
                },
                {
                    "sent": "And these lovely this lovely weather.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you arrange that.",
                    "label": 0
                },
                {
                    "sent": "So this tutorial is going to be about kernel methods for integrating biological data and to a much lesser extent the news talk is going to be based on our own work, so it's basically an overview of methods around to combine kernels to solve biological problems, and at the very end there will be some some work that we did.",
                    "label": 1
                },
                {
                    "sent": "Together with Mark Holtzman.",
                    "label": 0
                },
                {
                    "sent": "Done sorry, then an embassy student which got published last year.",
                    "label": 0
                },
                {
                    "sent": "So we work in Delft in the Dalton versus Knology.",
                    "label": 0
                },
                {
                    "sent": "And what we call the biometrics lab.",
                    "label": 0
                },
                {
                    "sent": "And when I was invited to to give this tutorial, I had to come up with an abstract.",
                    "label": 0
                },
                {
                    "sent": "And so I thought of this title.",
                    "label": 0
                },
                {
                    "sent": "And then when I look back to make the actual slides, I realized that this stated more solution rather than the problem.",
                    "label": 0
                },
                {
                    "sent": "So I thought maybe in the in the presentation I should start by first explaining what the problem is or what the typical problem is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our last few years I've worked with Biotechnologies quite a lot and that's because Delta University.",
                    "label": 0
                },
                {
                    "sent": "We have strong Department of Biotechnology.",
                    "label": 0
                },
                {
                    "sent": "And they are involved in.",
                    "label": 0
                },
                {
                    "sent": "A lot of them are involved in manipulating microorganisms to do interesting stuff.",
                    "label": 0
                },
                {
                    "sent": "An interesting stuff here usually means producing interesting things, for example here this this weird fungus, Aspergillus Niger produces citric acid, which doesn't sound very sexy.",
                    "label": 1
                },
                {
                    "sent": "But actually it's a book project and gets produced in terms of billions of litres each year.",
                    "label": 0
                },
                {
                    "sent": "The bacteria could use to produce cheese, yogurt, milk, penicillin, and a personal favorite of mine and most health researchers sacrifices cervisia, which is used to make beer and the like and brads, but mainly beer.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one problem that we run into and actually.",
                    "label": 0
                },
                {
                    "sent": "We're going to hear about more about that later is just got back industrial protein production.",
                    "label": 1
                },
                {
                    "sent": "We work together with an industrial partner.",
                    "label": 0
                },
                {
                    "sent": "Who used one of these microorganisms to produce enzymes, which are then used in industry to make better beer, better, better, better, whatever?",
                    "label": 0
                },
                {
                    "sent": "And they used to sell factory.",
                    "label": 0
                },
                {
                    "sent": "So basically they build big vats of these of these microorganisms and they want them to produce a certain protein.",
                    "label": 0
                },
                {
                    "sent": "And if you want to easily collect that protein, you want the Organism to excrete the primary protein, right?",
                    "label": 0
                },
                {
                    "sent": "So not that you don't have to 1st kill the microorganism, break it apart and separate the protein that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So you need to do two things.",
                    "label": 0
                },
                {
                    "sent": "You need to take this microorganism.",
                    "label": 0
                },
                {
                    "sent": "We need to glue in a new gene.",
                    "label": 0
                },
                {
                    "sent": "Integrate and Eugene.",
                    "label": 0
                },
                {
                    "sent": "Include a strong promoter in front of it so that you know that it will be transcribed.",
                    "label": 1
                },
                {
                    "sent": "So that you will get the highly express gene.",
                    "label": 0
                },
                {
                    "sent": "That's the first budget in the second part you need is that you want this protein to actually be secreted by the cell so that you can collect it on the outside.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is really a large industry, so there's a couple of these enzymes and proteins are routinely made now in these large factories, but if they want to come up with a new interest in new proteins, new enzymes, what they do is they start in the lab.",
                    "label": 0
                },
                {
                    "sent": "They do a test phase in which they actually integrate this gene.",
                    "label": 0
                },
                {
                    "sent": "I'll see whether it's created and that whole test phase is very tedious and costly.",
                    "label": 1
                },
                {
                    "sent": "So the question that they posed to us this, can we predict what proteins can be successfully successfully expressed so created?",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that makes it, I think, about informatics problem rather than the biological problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, the biologists are too expensive or too lazy, and then they need a computer or a computer scientist to help them out to solve things.",
                    "label": 0
                },
                {
                    "sent": "So this expression is relatively easy.",
                    "label": 1
                },
                {
                    "sent": "I think they've got that down so they they know that if they put in a gene and they put in a promoter sequence in front of that will almost always work.",
                    "label": 0
                },
                {
                    "sent": "But secretion is very hard to get right, and there is a sort of basic idea of how this would work, although I'm always amazed by the affected biologists tend to put up models like this and sort of claim that they understand how it works.",
                    "label": 0
                },
                {
                    "sent": "And then there's tons of exceptions to the rule and so on.",
                    "label": 0
                },
                {
                    "sent": "So the basic machinery is that these proteins that developers are RNA that gets transcribed and translated into proteins, and I've got transported through the endoplasmic reticulum and then to the Golgi complex, gets packaged in vesicles and then released to the outside of the cell.",
                    "label": 0
                },
                {
                    "sent": "And actually, the proteins carry signal sequences, sort of stamps or zip codes if you will that tell the cell users to say where the protein has to go.",
                    "label": 0
                },
                {
                    "sent": "But the signal sequences are not unique, so there's a sort of vague consensus on what they should look like.",
                    "label": 1
                },
                {
                    "sent": "But as many exceptions, there's a lot of alternative routes, and the whole model, so it completely goes overboard once you start considering proteins that are not normally executed by the by the Organism.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that makes it a pattern recognition biometrics problem.",
                    "label": 1
                },
                {
                    "sent": "We don't have a model.",
                    "label": 0
                },
                {
                    "sent": "We don't have a good idea what we should do, and then when all else fails, we turn to pattern recognition.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so they already did a lot of groundwork at this at this company.",
                    "label": 0
                },
                {
                    "sent": "They had to have a data set of 680, almost 700 proteins for which they attempted secretion.",
                    "label": 1
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "Objects are proteins.",
                    "label": 0
                },
                {
                    "sent": "The labels are weathered.",
                    "label": 1
                },
                {
                    "sent": "Secretion was detected and this is a bit of a weird level because they look for a certain band on the gel or this is a pretty subjective way of saying whether this accretion or not, but we that's all we have for the moment.",
                    "label": 0
                },
                {
                    "sent": "They couldn't find back to original gels.",
                    "label": 0
                },
                {
                    "sent": "And more about this problem will be told tomorrow by bus down from there.",
                    "label": 0
                },
                {
                    "sent": "Who did some initial work to see what you can do with this data set?",
                    "label": 0
                },
                {
                    "sent": "OK, so two questions remain.",
                    "label": 0
                },
                {
                    "sent": "What is the target?",
                    "label": 0
                },
                {
                    "sent": "What is the problem?",
                    "label": 0
                },
                {
                    "sent": "We're actually trying to solve?",
                    "label": 0
                },
                {
                    "sent": "And what are the features?",
                    "label": 0
                },
                {
                    "sent": "What is a good representation for the service this brings?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, actually what the biologists are going to do with this is that there.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "They're going to use any predictions that come out of this pattern recognition approach to go back to the lab and then try to see whether the predictions actually hold true.",
                    "label": 0
                },
                {
                    "sent": "Right, so much like meals, for example, with the transcription factors, we don't want to suggest to them that we can give the answer, but we want to give them at least a couple of suggestions and we hope that many of them are correct, right?",
                    "label": 0
                },
                {
                    "sent": "So we're more interested in an area under the RC curve.",
                    "label": 0
                },
                {
                    "sent": "You probably all know which blocks the false positive rate against a true positive rate, and we hope to be somewhere over here where we predict.",
                    "label": 1
                },
                {
                    "sent": "A large number of true positives of proteins that can be secreted at a very low number of false positives.",
                    "label": 0
                },
                {
                    "sent": "Proteins that that fail.",
                    "label": 0
                },
                {
                    "sent": "Actually, what this what this axis conveniently hide is that they run actually from zero to one, but there's many more possible false positives than true positives, so many more.",
                    "label": 0
                },
                {
                    "sent": "Proteins does fail to be accreted than proteins that can be excluded, so this number the actual number could actually be something like 10,000 and over.",
                    "label": 0
                },
                {
                    "sent": "Here it could be maybe a couple of 100.",
                    "label": 0
                },
                {
                    "sent": "So that even if you're over here in terms of an RC, it looks good, but it means you would have to do thousands of experiments to get a few 100 proteins.",
                    "label": 0
                },
                {
                    "sent": "And they're also not interested in that.",
                    "label": 1
                },
                {
                    "sent": "So then we can use a partial area under the curve.",
                    "label": 0
                },
                {
                    "sent": "Only look at the the personal number of true positives you get until a certain maximum number of experiments.",
                    "label": 0
                },
                {
                    "sent": "You have to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's all.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Literally straight forward.",
                    "label": 0
                },
                {
                    "sent": "Now the features, right?",
                    "label": 0
                },
                {
                    "sent": "So in many many applications of pattern recognition at least once I used to work on the features you have to choose for quite straightforward if you want to do face recognition, you take a camera, you take a picture of someone's face.",
                    "label": 0
                },
                {
                    "sent": "That's about it then.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have to do some feature extraction, but the basic elements that you have there so the basics, the measurements that you have to do is pretty clear.",
                    "label": 0
                },
                {
                    "sent": "I think in problems such as these it's much much harder, so we have.",
                    "label": 0
                },
                {
                    "sent": "I only thing we have available that we have that we can work with our DNA sequences so we can translate these into protein sequences.",
                    "label": 0
                },
                {
                    "sent": "And then we can come up with a number of characteristics which we think might be predictive of this secretion.",
                    "label": 0
                },
                {
                    "sent": "So we might think that long sequences are much harder to secrete.",
                    "label": 0
                },
                {
                    "sent": "Or they may be easier to screen because maybe they fold and so are do not interact as much with the rest of the machinery.",
                    "label": 0
                },
                {
                    "sent": "It could be that certain amino acids make a protein easier.",
                    "label": 0
                },
                {
                    "sent": "Easier to secrete or less easy to create.",
                    "label": 0
                },
                {
                    "sent": "At least not exactly.",
                    "label": 0
                },
                {
                    "sent": "Amino acids at certain positions, but just the composition of the protein in terms of amino acids.",
                    "label": 0
                },
                {
                    "sent": "Or that subsets of linear assets like the Group of basic amino acids chartered charged amino acids and so on help predict that?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Even even further removed from the actual sequence, we can calculate things like Golden Adaptation Index, which is a measure for how likely it is that this gene will be highly expressed.",
                    "label": 0
                },
                {
                    "sent": "Hydrophobicity felta protein does it.",
                    "label": 0
                },
                {
                    "sent": "Does it dissolve well, and so on.",
                    "label": 0
                },
                {
                    "sent": "And becomes less and less easy to capture this in numbers, right?",
                    "label": 0
                },
                {
                    "sent": "The things that we normally work with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I bought it here this.",
                    "label": 0
                },
                {
                    "sent": "Their standard picture of pattern matching problem looks like we have two measurements, 2 features X one X2 usually seem to be real numbers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it becomes harder and harder to translate all these things into real numbers.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can even go step further and use other predictors outputs as possible inputs for our system.",
                    "label": 0
                },
                {
                    "sent": "For example, the presence of a signal sequence there specialized particular that some celery localization, whether it's predicted that this this protein, if it's predicted that this protein will stay inside the cell, is very unlikely to go outside.",
                    "label": 1
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So what we have is a very heterogeneous bunch of data sources.",
                    "label": 1
                },
                {
                    "sent": "And prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "So we have knowledge which echo information and data.",
                    "label": 1
                },
                {
                    "sent": "And they all need to be integrated in some way to solve this problem.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that brings me to the first topic, integrated bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a quick overview.",
                    "label": 0
                },
                {
                    "sent": "This was sort of a sketch of a single problem in integrated buying.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in general, integrated by Infomatics is much.",
                    "label": 0
                },
                {
                    "sent": "I think it's much like what about new called this data based approach that we have a bunch of data and prior knowledge and we like to mine the data and use everything we have to come up with.",
                    "label": 0
                },
                {
                    "sent": "Global pictures often goes on rather than having detailed kinetic models for single serve parts of this.",
                    "label": 0
                },
                {
                    "sent": "So I'm firmly in the ocean on the left side of new Sir graph at the moment.",
                    "label": 0
                },
                {
                    "sent": "Although I may move a bit towards the right now the folder stock.",
                    "label": 0
                },
                {
                    "sent": "So the overall goal of integrated graphics to construct such networks of biochemical interactions.",
                    "label": 1
                },
                {
                    "sent": "Making use of everything that's around and then using these, these networks are starting points for doing further research, right?",
                    "label": 0
                },
                {
                    "sent": "So you look at these networks.",
                    "label": 0
                },
                {
                    "sent": "You might notice certain modules that have certain functions an wanted to.",
                    "label": 0
                },
                {
                    "sent": "Proteins that do not have yet functional attached.",
                    "label": 0
                },
                {
                    "sent": "Well then you can test whether these modules have that function.",
                    "label": 0
                },
                {
                    "sent": "And basically integrated mathematics deals with answering 2 main types of questions to construct these networks.",
                    "label": 0
                },
                {
                    "sent": "That one is that given information on interactions between proteins RNA.",
                    "label": 0
                },
                {
                    "sent": "DNA, whatever.",
                    "label": 0
                },
                {
                    "sent": "Can we predict new interactions without having access to data, or given that we have a certain functional annotation for some protein or molecule, can we derive a functional annotation for a different molecule which is connected to that?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the level at which these interactions can occur or multiple levels is staggering, right?",
                    "label": 0
                },
                {
                    "sent": "So the default picture.",
                    "label": 0
                },
                {
                    "sent": "I think Neil talk mostly about these levels and then this type of interaction, right?",
                    "label": 0
                },
                {
                    "sent": "Gene gets transcribed into RNA.",
                    "label": 0
                },
                {
                    "sent": "That has translated into protein which may influence the regulation of another gene.",
                    "label": 0
                },
                {
                    "sent": "But in fact, the number of the amount of crosstalk and interactions between all kinds of different levels is huge.",
                    "label": 0
                },
                {
                    "sent": "So these proteins, connectors, enzymes which influence the rate of production of metabolites.",
                    "label": 0
                },
                {
                    "sent": "Small chemical compounds.",
                    "label": 0
                },
                {
                    "sent": "These metabolise interacting with proteins, activate or deactivate them.",
                    "label": 0
                },
                {
                    "sent": "Proteins form complexes.",
                    "label": 0
                },
                {
                    "sent": "RNA can influence the degradation of other types of RNA, and I think there's probably even levels in here that were completely missing at the moment.",
                    "label": 0
                },
                {
                    "sent": "Right, realizing this RNA interference is also just very recently discovered, it likely that we're just missing entire levels of regulations, so.",
                    "label": 0
                },
                {
                    "sent": "So that's not a simple problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think for the moment, at least in this field that focuses on two type of two types of interactions.",
                    "label": 0
                },
                {
                    "sent": "One, you might call genetic interactions, which is sort of a catch all term, meaning there's some functional relation between.",
                    "label": 1
                },
                {
                    "sent": "Jeans, for example, a gene might influence the transcription of another gene or two genes might when you knock him out simultaneously, might lead to a different phenotype.",
                    "label": 0
                },
                {
                    "sent": "For example, the cell dying or something like that.",
                    "label": 0
                },
                {
                    "sent": "And a lot of work has been performed on protein protein interaction, and that's mainly because there's a lot of data available for that, and protein interaction is important to understand signaling.",
                    "label": 0
                },
                {
                    "sent": "Like to be 53 I think, but also transport and complex formation.",
                    "label": 1
                },
                {
                    "sent": "And for our particular problem transport would be interesting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have other availability, really vast amount of data.",
                    "label": 0
                },
                {
                    "sent": "These days, every biological paper that produces or that presents a high throughput measurement data set.",
                    "label": 0
                },
                {
                    "sent": "Uh also is accompanied by a submission of the data set into a database.",
                    "label": 0
                },
                {
                    "sent": "So if you now go to the IBI website array, expressive thinks called and Geo website that.",
                    "label": 0
                },
                {
                    "sent": "NCBI in the states.",
                    "label": 0
                },
                {
                    "sent": "You can download download thousands of microarrays on all kinds of different phenotypes.",
                    "label": 0
                },
                {
                    "sent": "The same starts holding for metabolite data, protein data, and so on.",
                    "label": 0
                },
                {
                    "sent": "There's measurement databases, soft protein interaction, MIPS and DIP, and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's really easy to get access to lots and lots of partial information on all these processes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Besides that, we have a lot of prior knowledge people have already curated lots of these views on the data.",
                    "label": 1
                },
                {
                    "sent": "So we have databases that contain annotations, pathways such as gag or REACT.",
                    "label": 1
                },
                {
                    "sent": "I think reactor.",
                    "label": 0
                },
                {
                    "sent": "A chemical reactions, even literature, all contains information on what this network should look like.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have all this data.",
                    "label": 0
                },
                {
                    "sent": "We have a particular problem Now what we do?",
                    "label": 0
                },
                {
                    "sent": "Well, if you think about it is predicting of these interactions right?",
                    "label": 0
                },
                {
                    "sent": "So I did these interactions on these functions.",
                    "label": 0
                },
                {
                    "sent": "That is really, in essence, pattern recognition problem.",
                    "label": 1
                },
                {
                    "sent": "We have a bunch of measurements and data and we as output we try to predict the presence or absence of this interaction or function.",
                    "label": 0
                },
                {
                    "sent": "So we have an input, a classifier, an output.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, as I said, we have very heterogeneous data sources, right?",
                    "label": 0
                },
                {
                    "sent": "There's lots of different types of data around, so somehow we have to integrate that data into a single classifier or single output at least.",
                    "label": 0
                },
                {
                    "sent": "Well, the standard approach to do that is just.",
                    "label": 1
                },
                {
                    "sent": "We might call this feature Fusion, but that's basically the simplest approach.",
                    "label": 0
                },
                {
                    "sent": "You've just been pulling all your features together in a feature vector.",
                    "label": 0
                },
                {
                    "sent": "Combining that linearly or non linearly in a classifier to produce a single output.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The extreme opposite of that would be to apply single classifiers to each individual data source, and these classifiers can be optimized for their data source.",
                    "label": 0
                },
                {
                    "sent": "So for example, for certain real value measurement you might use a linear classifier an for.",
                    "label": 0
                },
                {
                    "sent": "For another string based measurement, you might use nearest neighbor type approach.",
                    "label": 0
                },
                {
                    "sent": "And as long as these classifiers all outputs something similar, for example of posterior probability or approximation of that, then you can combine these into a single output by maybe fixed rule or again a classifier which combines all the inputs.",
                    "label": 0
                },
                {
                    "sent": "So here all these individual features are first translated into a sort of vote on the possibility of having certain output, and then these votes are aggregated into a single output.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a couple of problems with both these approaches.",
                    "label": 0
                },
                {
                    "sent": "For early integration, you need to convert all the features that you have into the single representation, right?",
                    "label": 1
                },
                {
                    "sent": "So usually real values, and for a lot of data that's quite straightforward, but for example for sequences it's already usually becomes a bit awkward.",
                    "label": 0
                },
                {
                    "sent": "I had to go back here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so here we have an example of a feature called Gravy.",
                    "label": 0
                },
                {
                    "sent": "Which is a sort of hybrid hydrophobicity index which you can calculate over the entire protein sequence.",
                    "label": 0
                },
                {
                    "sent": "And we might have the idea that's that it's important that certain areas of the protein are either highly hydrophilic or hydrophobic.",
                    "label": 0
                },
                {
                    "sent": "And then we already have to come up with some measurement like OK, there's some threshold, and then we count the total area of these curves outside, above or below the threshold.",
                    "label": 0
                },
                {
                    "sent": "I think there's a normalization issue because this is the this runs from the length of the protein sequence, so for longer protein sequences shot after normalizing tone.",
                    "label": 0
                },
                {
                    "sent": "So this is already quite involved.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Late integration is more straightforward, but still there you have to choose the combination mechanism and from being around people who do classifier combination, I can I can vouch that for every problem there's probably an optimal optimal way of combining classifiers.",
                    "label": 0
                },
                {
                    "sent": "But a different way, so not easy to tell beforehand what you should do.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And all knowledge of the fact that this data etc genius is sort of lost in the process.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it may be that one of these classifiers bases its prediction on high quality, low throughput measurements, right?",
                    "label": 0
                },
                {
                    "sent": "So really trustworthy data, whereas another classifier performs its predictions based on, well, sort of model.",
                    "label": 0
                },
                {
                    "sent": "Probably vaguely connected evidence.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you get here to these outputs, all that knowledge that they have different back.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is lost.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when people start to think about it and I think it's probably just going to term intermediate integration so well, actually what you want to do is first translate all these features, all these measurements into common language.",
                    "label": 0
                },
                {
                    "sent": "An intermediate representation that allows you to apply a single classifier.",
                    "label": 1
                },
                {
                    "sent": "And if you think about this, a couple of candidates to to allow that.",
                    "label": 0
                },
                {
                    "sent": "Probably there's more but common languages that we did.",
                    "label": 0
                },
                {
                    "sent": "You're probably familiar with our probabilities.",
                    "label": 0
                },
                {
                    "sent": "Similarities with these similarities or kernels.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you translate all the knowledge that you have into probabilities, then you might end up with something like a Bayesian network or basing classifier, right?",
                    "label": 0
                },
                {
                    "sent": "So an example here from the forgotten Sky.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why she tried to predict functional relationships between genes, and this is too hard to read.",
                    "label": 0
                },
                {
                    "sent": "But does everything down here, from biochemical essays to to experimentally verified complexes, which then are aggregated into some sort of probability of physical Association.",
                    "label": 0
                },
                {
                    "sent": "This is genetic or functional Association and this is stuff like coexpression and so on, which is down again aggregated into one vertex on function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can also combine P value so if you have some statistical test.",
                    "label": 0
                },
                {
                    "sent": "That gives you some score and wish you can convert into a P value.",
                    "label": 0
                },
                {
                    "sent": "Then you could combine them using for example Fisher's method into a new overall TP value, which follow some guys squared distribution and then by applying different weights you can assign different different levels of importance to the different features that originated these P values.",
                    "label": 0
                },
                {
                    "sent": "So these features test another, well known ones is that score.",
                    "label": 0
                },
                {
                    "sent": "If you can transform everything, everything does that scare.",
                    "label": 0
                },
                {
                    "sent": "You can just send these scores and they will follow again normal distribution, from which you can derive one single value.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These similarities can also be used to integrate data.",
                    "label": 0
                },
                {
                    "sent": "This is usually quite intuitively easier than the P value approach in that if you ask people what is the probability of this occurring or what is the probability that a protein with such a sequence will be treated, you will likely get a blank face.",
                    "label": 0
                },
                {
                    "sent": "We've asked him how much do these two proteins differ.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy to come up with an answer, right?",
                    "label": 0
                },
                {
                    "sent": "You blast them against each other, or you come up with some.",
                    "label": 0
                },
                {
                    "sent": "Measurement of distance between your sequences.",
                    "label": 0
                },
                {
                    "sent": "And then using these sequences using these distances, you can build classifiers, do clustering and so.",
                    "label": 0
                },
                {
                    "sent": "So one example could be and these are not real examples, but just sort of quickly made these up in Matlab is where you have.",
                    "label": 0
                },
                {
                    "sent": "You can calculate a correlation matrix here for how for motif binding, so we have a bunch of motifs we check for each gene, which of which of these motifs can bind.",
                    "label": 0
                },
                {
                    "sent": "How we calculate correlation between these multi binding factors?",
                    "label": 0
                },
                {
                    "sent": "For the same genes we might have gene expression data and we might have to Gene ontology annotation information which we can also turn into a correlation measure.",
                    "label": 0
                },
                {
                    "sent": "And then we could convert these correlations into a single combined distance matrix, which we can then use for classification or clustering.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm finally on the final way of integrating this information.",
                    "label": 0
                },
                {
                    "sent": "The one I'll be talking about for the rest of the material is by integrating kernels, and I think of kernels have sort of well founded similarities, right?",
                    "label": 1
                },
                {
                    "sent": "So it's basically basically doing what we did here, but without all the hand waving and ad hoc approaches.",
                    "label": 0
                },
                {
                    "sent": "So for our problem in predicting protein secretion.",
                    "label": 0
                },
                {
                    "sent": "We might take the amino acid composition just listed as a vector of how many amino acids, or the frequency of amino acids in a certain protein and put that in a vector kernel.",
                    "label": 0
                },
                {
                    "sent": "Or we might take a signal sequence.",
                    "label": 1
                },
                {
                    "sent": "So first part of the protein and apply a weighted Rico nor which compares strings.",
                    "label": 0
                },
                {
                    "sent": "So all kinds of different information can be on different forms of information, different types of data we can apply specific kernels and then combine these to apply a single support vector machine or other type of learning method.",
                    "label": 0
                },
                {
                    "sent": "Is this clear so far?",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how many of you have never not worked with, not read about support vector machines?",
                    "label": 1
                },
                {
                    "sent": "OK, I can go through this really quickly.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The whole idea of these kernels came up when people in the early 90s started working on ideas by Pop Nick in from the 60s and 70s where he came up with a linear classifier and said, well, if you want to have an optimally generalizing classifier in some sense we have to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "Around this classifier, right the maximum margin classifier and you can actually show that boils down to minimizing the weight factor in this linear classifier subject to the constraints that all points are on the correct side of the classifier.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK well.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll go as you all know this.",
                    "label": 0
                },
                {
                    "sent": "I'll go through this quickly.",
                    "label": 0
                },
                {
                    "sent": "What helped was that there was an easy sort of this is an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There's an easy dual formulation of this optimization problem where you can ride the weights here.",
                    "label": 0
                },
                {
                    "sent": "They turn out to be the possible to write them in terms of a linear combination of the training objects and the training labels multiplied by some factor, and only the support factors here.",
                    "label": 0
                },
                {
                    "sent": "The ones on the margin get a weight factor larger than 0.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Then the kernel trick which was.",
                    "label": 0
                },
                {
                    "sent": "But later added.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People say, well, this is now all written in terms of DOT products between vectors.",
                    "label": 0
                },
                {
                    "sent": "Here training vectors and here dot product between training vector and test samples at.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we can do is we can replace these dot product dot product between these vectors mapped into some feature space.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then actually, under certain circumstances we can replace it by a general kernel function as long as it fulfills some constraints and immersive virtual kernel that he talked about that Neil talked about.",
                    "label": 0
                },
                {
                    "sent": "We can replace this inner product by this kernel function and whole machinery still works, so we still have a classifier linear classifier in some space.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And holiday of this function is that it would map the data into space, which makes it easier for us to classify and one standard examples is that if we have this problem which clearly cannot be separated by linear classifier.",
                    "label": 1
                },
                {
                    "sent": "But we apply a transformation into a space where in which the first 2 features are the squares of the original features.",
                    "label": 0
                },
                {
                    "sent": "Then all of a sudden it becomes possible to apply a linear classifier here, which corresponds to quadratic classifier over here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So if we can come up with a decent kernel.",
                    "label": 0
                },
                {
                    "sent": "Then we have this standard classification algorithm which allows us to get a nonlinear classifier in the original space.",
                    "label": 1
                },
                {
                    "sent": "Example for this problem, if you have a polynomial kernel which basically the dot product you add one, you raise the whole thing to the power D for D is 1, you get a linear kernel which doesn't separate the data for these two you get this dashed thing, which is still not very good for these free you get a polynomial for power polynomial which is able to separate the two classes.",
                    "label": 0
                },
                {
                    "sent": "This is the exponentiated quadratic kernel, which, when applied to this data you can also get all kinds of different shapes of classifiers from a linear one.",
                    "label": 0
                },
                {
                    "sent": "If you set this Sigma very large to one which is highly nonlinear, if you said it very small.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think.",
                    "label": 0
                },
                {
                    "sent": "This is all pretty well known yesterday.",
                    "label": 0
                },
                {
                    "sent": "You don't actually have to know this feature mapping to construct this kernel.",
                    "label": 1
                },
                {
                    "sent": "Most people now do the other way around.",
                    "label": 0
                },
                {
                    "sent": "Their first come up with interesting notion of dissimilarity, constructed kernel and then try to see whether it leads to positive definite kernel matrix for any possible input.",
                    "label": 1
                },
                {
                    "sent": "If you can prove that, then you have a kernel no matter what feature space it may correspond to.",
                    "label": 1
                },
                {
                    "sent": "And since then, there's lots of classifiers that have been formulated in terms of Colonel Sanders, nearest mean classifiers, nearest neighbor classifiers, Fisher linear discriminant analysis has been kernelized.",
                    "label": 0
                },
                {
                    "sent": "Elsu",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifiers so how do we train these things well?",
                    "label": 0
                },
                {
                    "sent": "That's actually and I would realize that Gaussian processes do this much more elegantly, but we do this using standard optimization package for solving quadratic problems.",
                    "label": 0
                },
                {
                    "sent": "To optimize this this way, factor and this bias term, and then the other parameters there a bit of a hassle, so there we have to do something like cross validation to find a peaceful support vector machine.",
                    "label": 0
                },
                {
                    "sent": "This trade off parameter between errors and margin, and the kernel parameters and usually for a single problem.",
                    "label": 0
                },
                {
                    "sent": "I don't have 30 yet, but later when we get to the combination that.",
                    "label": 0
                },
                {
                    "sent": "Can be a problem.",
                    "label": 0
                },
                {
                    "sent": "And it's not usually the whole the whole performance of the algorithm is not that sensitive to the exact choice of.",
                    "label": 0
                },
                {
                    "sent": "For example, C and Sigma in this in this example.",
                    "label": 0
                },
                {
                    "sent": "So if you're somewhere around here you do you do reasonably well, but if you have a bad choice, you can end up getting pretty poor performance.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so since the classifier there have been lots of other kernelized machine learning type algorithms, support vector regression, which already I think proposed by public.",
                    "label": 0
                },
                {
                    "sent": "But I worked out by by smaller show up.",
                    "label": 0
                },
                {
                    "sent": "Or at least print this paper.",
                    "label": 0
                },
                {
                    "sent": "This picture is from smaller show up.",
                    "label": 0
                },
                {
                    "sent": "If you come up with with a decent loss function here so you don't have a quadratic loss function But a loss function which is insensitive in a certain tube and then linear around that.",
                    "label": 0
                },
                {
                    "sent": "Then again, you can write this regression as a very similar optimization problem and apply the same machinery As for the support.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifier to do regression.",
                    "label": 0
                },
                {
                    "sent": "And depending on the width of the tube, you approximate your data too.",
                    "label": 0
                },
                {
                    "sent": "The better or worse.",
                    "label": 0
                },
                {
                    "sent": "This is very hard to see, but here is.",
                    "label": 0
                },
                {
                    "sent": "Distill the two curves that you can see.",
                    "label": 0
                },
                {
                    "sent": "They they give the width of the tube, and then actually the approximated function runs between.",
                    "label": 0
                },
                {
                    "sent": "Runs between here like this, so here you have a very wide approximation and here you have very.",
                    "label": 0
                },
                {
                    "sent": "Tight approximation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then came kernel clustering.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And kind of PCA, which is very hard to interpret for me at least.",
                    "label": 0
                },
                {
                    "sent": "What happens if you have a first degree for the polynomial kernel and then a PCA in that space?",
                    "label": 0
                },
                {
                    "sent": "Colonel LDS African record Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "So basically there's a whole zoo of.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applicable algorithms based on kernels, right?",
                    "label": 0
                },
                {
                    "sent": "So once we have a decent set of kernels or decent kernel for our problem, then there's this whole stack of literature and actually implementations that we can turn to solve a problem.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instrumental break.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said, if you have good kernels then there's the whole machinery to solve the problem you have.",
                    "label": 0
                },
                {
                    "sent": "So the kernels that were proposed earliest where.",
                    "label": 0
                },
                {
                    "sent": "Linear kernels polynomial kernels exponentiated quadratic kernels right?",
                    "label": 0
                },
                {
                    "sent": "So simple kernels on vector representations of your original objects?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from now on, every now and again I'll show a slide that about applications of these kernels to this protein secretion prediction problem that Boston line made over the last couple of days.",
                    "label": 0
                },
                {
                    "sent": "So this probably not highly optimized, but we just played around a bit to see what we could get away with.",
                    "label": 0
                },
                {
                    "sent": "So this is support vector classifier on the 680 and somewhat proteins where we predict whether they succeeded or not.",
                    "label": 0
                },
                {
                    "sent": "We do 10 fold cross validation and this is the average.",
                    "label": 0
                },
                {
                    "sent": "Every other girl forget.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of features which must, now we'll discuss in more depth and we can divide them, sort of roughly into three subsets, one based on sequence amino acid composition, one based on sequence, but derived from that.",
                    "label": 0
                },
                {
                    "sent": "So predictions of hydrophobicity and hydrophilicity.",
                    "label": 0
                },
                {
                    "sent": "And then sort of a bunch of various features such as sequence length, Golden adaptation index, isoelectric point, stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So if we just take all the features, put them into one big feature vector, and apply a linear kernel, we already do quite well.",
                    "label": 0
                },
                {
                    "sent": "We got this.",
                    "label": 0
                },
                {
                    "sent": "89.73 area under the curve.",
                    "label": 0
                },
                {
                    "sent": "If you apply an RBF kernel, we do slightly better and then you actually see that in this case it would pay off to look at the subsets, because this sequence.",
                    "label": 0
                },
                {
                    "sent": "If we just looked at you in the amino acid composition and apply an RBF kernel on that, we do actually best among these options.",
                    "label": 0
                },
                {
                    "sent": "So in some way here the other features.",
                    "label": 0
                },
                {
                    "sent": "Make that we do worse, although I must say this is all highly non significant.",
                    "label": 0
                },
                {
                    "sent": "If we repeat this experiment for sure likely to be off by bit.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one standard approach for almost any other data type that you can come up with is the empirical kernel map.",
                    "label": 1
                },
                {
                    "sent": "I think it's important though.",
                    "label": 0
                },
                {
                    "sent": "Right, so as long as I said the nice thing about these similarities that I talked about earlier is that they're quite intuitive.",
                    "label": 0
                },
                {
                    "sent": "So lots of times people can come up with.",
                    "label": 0
                },
                {
                    "sent": "Nice, I just received this similarities and quite often these are highly non Euclidean, even non metric so they cannot be used directly as a kernel.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is that we we have a bunch of examples which might go template set.",
                    "label": 0
                },
                {
                    "sent": "We just calculate the dissimilarity between our object and each of these objects in the template set.",
                    "label": 0
                },
                {
                    "sent": "Turn that into a vector and then apply a linear or whatever kernel you want to these vectors.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So actually here you use transformation into a vector and then application of standard kernel as a trick to get away with using these non Euclidean metric type of these similarities and you'll see that trick get applied in more standard kernels later on as well.",
                    "label": 0
                },
                {
                    "sent": "So one example is the last kernel.",
                    "label": 0
                },
                {
                    "sent": "Wish you have a bunch of proteins and then you calculate you have two proteins which would like to calculate the kernel for.",
                    "label": 1
                },
                {
                    "sent": "So you blast the first protein against the template set.",
                    "label": 0
                },
                {
                    "sent": "You get lost the second kernel second protein against the template set you calculate the minors laggy value of this lost and if they if these miners lucky values concur for objects in the template set then you might conclude that A&B are therefore likely to be very similar.",
                    "label": 0
                },
                {
                    "sent": "So you get a high kernel value.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, this kernel kernels kernels that allow you to combine or play around with kernels.",
                    "label": 0
                },
                {
                    "sent": "Kernel addition is most well known among and that's the one I'll discuss later in terms of combination.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Where just a weighted weighted sum of individual kernels you can pull in vice multiplications of kernels.",
                    "label": 0
                },
                {
                    "sent": "You can have a generalized RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "It's called so first you use your kernel to transform the dissimilarity.",
                    "label": 0
                },
                {
                    "sent": "The similarity between.",
                    "label": 0
                },
                {
                    "sent": "Two objects into a distance, measure again and then plug this distance measure back into the into the kernel.",
                    "label": 0
                },
                {
                    "sent": "And you can normalize kernels, and in fact this becomes important also for the combination, because these similarities can have wildly different ranges, so you can come up with a way of normalizing.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An interesting kernel is a convolution kernel.",
                    "label": 0
                },
                {
                    "sent": "This specific application on next slide.",
                    "label": 0
                },
                {
                    "sent": "This is when you have kernels that operates on submit.",
                    "label": 0
                },
                {
                    "sent": "Operate on some parts of sequences, for example, or structures, but you don't know beforehand which subpart, So what you do is that you try all possible decompositions into subparts, so that's this summation here.",
                    "label": 1
                },
                {
                    "sent": "So we convolve a number of colonels and colonels on these two objects an we can decompose these two objects into.",
                    "label": 0
                },
                {
                    "sent": "Subobjects.",
                    "label": 0
                },
                {
                    "sent": "And then for each possible decomposition, we apply the first kernel to the first subset, Subpart 2nd, going through the 2nd part, and so on, and we multiply all these kernels with some of these multiplications and at the convolution kernel.",
                    "label": 0
                },
                {
                    "sent": "Now this of course if you would implement this naively would be horrible.",
                    "label": 0
                },
                {
                    "sent": "So specific implementations have highly optimized algorithms to be able to avoid having to do this all explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one example is the local alignment kernel.",
                    "label": 0
                },
                {
                    "sent": "So local alignment is defined as a local alignment of a certain length and then the sum over all possible lengths.",
                    "label": 0
                },
                {
                    "sent": "And this local alignment kernel offset and length is a convolution kernel where you start out with what's called a trivial kernel.",
                    "label": 1
                },
                {
                    "sent": "And then a letter alignment kernel which.",
                    "label": 0
                },
                {
                    "sent": "Gives you a certain score based on the substitution cost.",
                    "label": 0
                },
                {
                    "sent": "If both these subparts have exactly rank one, so when you're comparing individual letters in sequence and otherwise it gives you 0, so this is a way of quickly weirding out all possible lots of possible decompositions of the sequence.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of match going, or if there's a, if there's a.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at single letters, then this calculates the score of a match at that position.",
                    "label": 0
                },
                {
                    "sent": "And then there's a gap Colonel.",
                    "label": 0
                },
                {
                    "sent": "That's also a possibility of a gap, and you get a score for death based on the length of the gap.",
                    "label": 0
                },
                {
                    "sent": "So if you have to skip a lot of letters, then you will get a.",
                    "label": 0
                },
                {
                    "sent": "Larger score.",
                    "label": 0
                },
                {
                    "sent": "And then you apply this.",
                    "label": 0
                },
                {
                    "sent": "You have N of these alignments and minus one possible gaps.",
                    "label": 0
                },
                {
                    "sent": "So first and alignment possible gap, possible alignment, possible gap and so on.",
                    "label": 0
                },
                {
                    "sent": "Your end game with this trivial kernel that gives you one number for how well two sequences align in terms of.",
                    "label": 0
                },
                {
                    "sent": "How many gaps are there and how many?",
                    "label": 0
                },
                {
                    "sent": "How the letters are the letters in the sequences can be compared in terms of this substitution cost?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think Colonel Colonel is it also is a barrel as Colonel in many of these problems.",
                    "label": 0
                },
                {
                    "sent": "We tried to predict, for example, an interaction between proteins are objects is actually a protein pair rather than individual proteins.",
                    "label": 1
                },
                {
                    "sent": "But we have a lot of information available about individual proteins.",
                    "label": 0
                },
                {
                    "sent": "So what we don't need to do is combine this information about the individual proteins into.",
                    "label": 0
                },
                {
                    "sent": "An overall outcome.",
                    "label": 1
                },
                {
                    "sent": "So I'm just pairwise kernel or this is very hard to read.",
                    "label": 0
                },
                {
                    "sent": "Again, the green you take the you you compare.",
                    "label": 0
                },
                {
                    "sent": "The combine.",
                    "label": 0
                },
                {
                    "sent": "You multiply the kernels for.",
                    "label": 0
                },
                {
                    "sent": "So you want to start over again to calculate a kernel value for two pairs, you calculate kernels for each individual possible bear between the two pairs.",
                    "label": 0
                },
                {
                    "sent": "So rather than.",
                    "label": 0
                },
                {
                    "sent": "I just you would calculate it broken or for this combination the green one do this green one the blue one and a blue one will multiply the two green one.",
                    "label": 0
                },
                {
                    "sent": "So there is like the probabilities for the case when this protein matches this protein.",
                    "label": 0
                },
                {
                    "sent": "This protein matches this protein.",
                    "label": 0
                },
                {
                    "sent": "And the other case when this protein actually matches this protein and this protein matches this one, that's the blue gazing you had to do to deal with the two possibilities.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK couple more specific kernels so that set kernels which operates on which operate on sets.",
                    "label": 1
                },
                {
                    "sent": "Say that you have a certain domain, for example.",
                    "label": 1
                },
                {
                    "sent": "Words right then you could have a distribution of sets which could be document you might have a certain probability distribution over this domain.",
                    "label": 1
                },
                {
                    "sent": "For example, a measure of the uniqueness of the word like.",
                    "label": 1
                },
                {
                    "sent": "Can populate would be very unlikely word, so if you find 2 documents with the same word with that word then they might be similar.",
                    "label": 0
                },
                {
                    "sent": "Westward would be much more likely to occur, so would not be count that much towards the similarity.",
                    "label": 0
                },
                {
                    "sent": "And I'm given two sets of example two documents.",
                    "label": 0
                },
                {
                    "sent": "You can calculate intersections, complements an agreements.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "How much today, how much of the words intersect and how much do they agree on the difference?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "String kernels which are quite applicable in love by informatics problems.",
                    "label": 0
                },
                {
                    "sent": "Because we deal with a lot of sequences, the basic approach is called the spectrum kernel, where we have addiction where we create a Dictionary of all possible occurrences of substrings of a string and then, like the empirical kernel map, we construct a vector with the number of occurrences of those substrings and can apply again standard kernel to combine those.",
                    "label": 1
                },
                {
                    "sent": "Do other smart implementations of this as well using using trees so that you don't have to go through all possible.",
                    "label": 0
                },
                {
                    "sent": "You don't actually have to construct these entire vectors, because these can become quite long.",
                    "label": 0
                },
                {
                    "sent": "And this versions with gaps and mismatches, there's motif kernels where you look for specific sets of up substrings, rather than all possible substrings, and so on.",
                    "label": 1
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A weighted degree kernel as a sort of special case, which also takes the position into account, so this previous one just looked at all possible matches between substrings, no matter where they occurred, but waiting to be Colonel count the number of matching words at identical positions, and then this concept by the length of the match.",
                    "label": 1
                },
                {
                    "sent": "So as an example here in this this case where we look for matches up to length 4. Who would have five matches of a single letter?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Kind of would be five matches times a weight of 4 -- 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "We're looking at single letters now, so 4 * 5.",
                    "label": 0
                },
                {
                    "sent": "We have two cases where 2 letters match contribution of.",
                    "label": 0
                },
                {
                    "sent": "2 * 2.",
                    "label": 0
                },
                {
                    "sent": "An one case we're freeloaders match.",
                    "label": 0
                },
                {
                    "sent": "Combinations 2 * 1.",
                    "label": 1
                },
                {
                    "sent": "So the total kernel becomes 20 + 4 + 1.",
                    "label": 0
                },
                {
                    "sent": "I think I made an error here.",
                    "label": 0
                },
                {
                    "sent": "This should be free, so this is actually 627.",
                    "label": 0
                },
                {
                    "sent": "OK, more on this kernel by Anna talk connects to more often by like Christian Whitmer.",
                    "label": 0
                },
                {
                    "sent": "Who do you have a special version of this kernel or?",
                    "label": 0
                },
                {
                    "sent": "Version of this kernel which is specialized.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we all be applied to this.",
                    "label": 0
                },
                {
                    "sent": "The string kernels and a number of different kernels to our protein secretion prediction problem again.",
                    "label": 1
                },
                {
                    "sent": "And one of them turned out to be slightly better than the result we have until now.",
                    "label": 0
                },
                {
                    "sent": "But again, not not very really significantly, and also a bit.",
                    "label": 0
                },
                {
                    "sent": "Worrying me with the length of the subsequence, the maximum length such consent to one.",
                    "label": 0
                },
                {
                    "sent": "So actually it's still counting just individual amino acids.",
                    "label": 0
                },
                {
                    "sent": "We tried to see whether we can.",
                    "label": 0
                },
                {
                    "sent": "We could do something with the signal sequence that's supposed to be at the beginning of each product using a way to do equal.",
                    "label": 0
                },
                {
                    "sent": "But the standard approach doesn't really give us good results yet, so we might start looking at more clever ways of doing that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then there's a whole bunch of more advanced kernels in which you can compare graphs or based on, for example, the number of subgraphs that occur.",
                    "label": 0
                },
                {
                    "sent": "Or or the number of random walks that you take on the graph.",
                    "label": 0
                },
                {
                    "sent": "You can create kernels based on generative models, for example kernels based on comparing full hidden Markov models and so on and so on.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so we have this zoo of kernels, all of which may be applicable to our problem.",
                    "label": 1
                },
                {
                    "sent": "And applicable to different data sources.",
                    "label": 1
                },
                {
                    "sent": "Now the question is how do we combine them to come up with one single representation to put into our algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any questions about this so far?",
                    "label": 0
                },
                {
                    "sent": "So you mentioned at some point that you don't have to really enforce the kernels to be positive.",
                    "label": 0
                },
                {
                    "sent": "See, see my definite.",
                    "label": 0
                },
                {
                    "sent": "It's no problem.",
                    "label": 0
                },
                {
                    "sent": "Still up.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All the machinery did it is around or do you in the end with some kind of transformation to make it?",
                    "label": 0
                },
                {
                    "sent": "Now, So what I said was that was, I think here that if he uses empirical kernel map then then why you then tried to enforce it to be a kernel by first calculating this dissimilarity for number of represent representatives in template sets and then you again by a standard kernel which makes it into a positive definite kernel.",
                    "label": 1
                },
                {
                    "sent": "But this dissimilarity can be almost anything.",
                    "label": 0
                },
                {
                    "sent": "OK, but for any of the order.",
                    "label": 0
                },
                {
                    "sent": "Kernels that you mentioned you can easily show that they are all well.",
                    "label": 0
                },
                {
                    "sent": "I cannot do this.",
                    "label": 0
                },
                {
                    "sent": "Clever people have.",
                    "label": 0
                },
                {
                    "sent": "So I take their word for it, but yes, that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Still, in practice we sometimes have been able to get away with just calculating mean.",
                    "label": 0
                },
                {
                    "sent": "There's a difference between proving that the kernel function will give a positive definite kernel matrix for any possible input set, or just having a positive definite kernel matrix for solving a particular problem.",
                    "label": 0
                },
                {
                    "sent": "So we have been able to get away with using a similarity measure, which we cannot prove.",
                    "label": 0
                },
                {
                    "sent": "We could not prove that was positive definite, but still gave a positive definite kernel matrix, so we could apply the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Example that you keep showing the secret.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, strange that.",
                    "label": 0
                },
                {
                    "sent": "Chat.",
                    "label": 0
                },
                {
                    "sent": "Very good results come just with their counting amino acids and you can't do any better.",
                    "label": 0
                },
                {
                    "sent": "There's something about this problem, or is this that the data is so small?",
                    "label": 0
                },
                {
                    "sent": "Well, the data is small, but still you would expect that would hold for all features I guess right?",
                    "label": 0
                },
                {
                    "sent": "So the I don't see why the why that would.",
                    "label": 0
                },
                {
                    "sent": "That would say that I mean no assets.",
                    "label": 0
                },
                {
                    "sent": "Would that based on Justin?",
                    "label": 0
                },
                {
                    "sent": "To your increase.",
                    "label": 0
                },
                {
                    "sent": "Nationality yes, but I think.",
                    "label": 0
                },
                {
                    "sent": "This is where you're getting.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not sure it's a technical reason though.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is just not more information in this in this data than just counting individual amino acids.",
                    "label": 0
                },
                {
                    "sent": "We are now trying to convince the kernels which are situated.",
                    "label": 0
                },
                {
                    "sent": "Between.",
                    "label": 0
                },
                {
                    "sent": "Similarity.",
                    "label": 0
                },
                {
                    "sent": "So why is it the same?",
                    "label": 0
                },
                {
                    "sent": "Why is why is getting a kernel which have similarities maybe got mapped?",
                    "label": 0
                },
                {
                    "sent": "My problems within yourself?",
                    "label": 0
                },
                {
                    "sent": "I did.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "You could always map to a separate space with the kernel.",
                    "label": 0
                },
                {
                    "sent": "That said diagonal reason you want similarities.",
                    "label": 0
                },
                {
                    "sent": "You want to generalize well, so this argument about.",
                    "label": 0
                },
                {
                    "sent": "Spaces.",
                    "label": 0
                },
                {
                    "sent": "To do to show that you have the power to separate anything.",
                    "label": 0
                },
                {
                    "sent": "But in practice the reason you want the similarities to be high between simple things is Even so that you do it in awhile regularised.",
                    "label": 0
                },
                {
                    "sent": "Completely intentional.",
                    "label": 0
                },
                {
                    "sent": "Wow, maybe, maybe not.",
                    "label": 0
                },
                {
                    "sent": "You would like the classes to be separated, but not all objects would be separate.",
                    "label": 0
                },
                {
                    "sent": "Think of a similar closer they classified with the same label.",
                    "label": 0
                },
                {
                    "sent": "If they happen to have different labels, they wouldn't be separable.",
                    "label": 0
                },
                {
                    "sent": "You want them to then.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So your example of over there it's possible to separate all the objects using a linear diagonal.",
                    "label": 0
                },
                {
                    "sent": "You have colors.",
                    "label": 0
                },
                {
                    "sent": "You take the lift, the obvious settle down to zero.",
                    "label": 0
                },
                {
                    "sent": "You will separate everything, but you'll overfit, and that's an example where conflicts.",
                    "label": 0
                },
                {
                    "sent": "But somewhere in the middle.",
                    "label": 0
                },
                {
                    "sent": "Moving the similarity between everything so there's no similarity.",
                    "label": 0
                },
                {
                    "sent": "Then you get separation as you have similarity, you'll lose separation potentially.",
                    "label": 0
                },
                {
                    "sent": "So what is our design principle now?",
                    "label": 0
                },
                {
                    "sent": "The balance between.",
                    "label": 0
                },
                {
                    "sent": "Balance between good and evil.",
                    "label": 0
                },
                {
                    "sent": "All the kinds of misunderstandings follow from the fact that I can know is in no case a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "It's that rather nothing else but what is dissimilarity measure is the ingredient medic produced by a camel.",
                    "label": 0
                },
                {
                    "sent": "But candle is.",
                    "label": 0
                },
                {
                    "sent": "An inner product in some linear space into which the respective metric space is embedded.",
                    "label": 0
                },
                {
                    "sent": "Panel will depend on the choice of the North element.",
                    "label": 0
                },
                {
                    "sent": "Now did not you see?",
                    "label": 0
                },
                {
                    "sent": "In all, the kinds of understanding or misunderstandings are produced by this fact.",
                    "label": 0
                },
                {
                    "sent": "You see, Kendall is no similarity measure in the general case.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you take the usual RN and dimensional Euclidean space of real letters.",
                    "label": 0
                },
                {
                    "sent": "In this case, the so called radial basis kernel.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course.",
                    "label": 0
                },
                {
                    "sent": "This kind of kettles is an natural similarity measure, but not in the general case, use it.",
                    "label": 0
                },
                {
                    "sent": "Sentence for.",
                    "label": 0
                },
                {
                    "sent": "To try to get it really, really, really deep misunderstanding, which is actually for machine learning as a whole, you see not specifically for bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "People wrongly understand the the term of account.",
                    "label": 0
                },
                {
                    "sent": "Try to interpret that as a senior editor.",
                    "label": 0
                },
                {
                    "sent": "Of course, in many cases it is so, but not under general case.",
                    "label": 0
                },
                {
                    "sent": "But any candle produces a metric is a secondary function.",
                    "label": 0
                },
                {
                    "sent": "Their characteristic will be invariant to the choice of the null element you see.",
                    "label": 0
                },
                {
                    "sent": "And it is this why people ask questions about kernels, a graph, what it is.",
                    "label": 0
                },
                {
                    "sent": "But the most popular answer kennels.",
                    "label": 0
                },
                {
                    "sent": "Yes, they are actually have the sense of similarity measures, but not a kernel.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that answered the question, but.",
                    "label": 0
                },
                {
                    "sent": "So I think that Murphy do end up correctly that you ask her what do I try to optimize?",
                    "label": 0
                },
                {
                    "sent": "Well, what is the goal of this or folders of these kernels?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's still unsolved, so people come up with different kernels for different problems, and there's ways of combining them and still, like in lots of transmission approaches, you have to try a number to see what works.",
                    "label": 0
                },
                {
                    "sent": "So the idea that you that you say OK, well, I am looking at strings and this helps similarity between strings.",
                    "label": 0
                },
                {
                    "sent": "I've gotta in code in such a way.",
                    "label": 0
                },
                {
                    "sent": "Do those exercises well.",
                    "label": 0
                },
                {
                    "sent": "The more similar, the more similarity I encode in my kernel the better.",
                    "label": 0
                },
                {
                    "sent": "Necessary.",
                    "label": 0
                },
                {
                    "sent": "Now, so there is a trade off with the with the generalization, yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's why I think in most approaches you'll see she still see that people try and see what works.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the final part about combination.",
                    "label": 0
                },
                {
                    "sent": "I said that the easiest way to combine kernels is another people.",
                    "label": 1
                },
                {
                    "sent": "The way that people have been mostly studying is to use a weighted sum of normalized kernel matrices.",
                    "label": 1
                },
                {
                    "sent": "This normalization is often very important because these kernels have quite different ranges, so if you want to throw them into a pool and then you should first normalize it.",
                    "label": 0
                },
                {
                    "sent": "Can a simplest approach to do that is to just wait and more equally, just some of the kernels?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we do that for for our problem, then we get slightly better performance when we combine these three feature subsets than we did before.",
                    "label": 0
                },
                {
                    "sent": "So this is a free RBF kernels and then combined into one overall kernel.",
                    "label": 0
                },
                {
                    "sent": "Or just slightly better.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, can we also optimize these weights by which we combine these kernels, right?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "That's when people have been studying for the last almost 10 years, and there's three basic approaches to filter approach to rapper approaching without my called the embedded approach.",
                    "label": 0
                },
                {
                    "sent": "Let me filter approach you.",
                    "label": 0
                },
                {
                    "sent": "This is much like the filter approaches in feature selection and better recognition where you come up with some of the derived criterion which is not directly classifier performance, but.",
                    "label": 1
                },
                {
                    "sent": "Criteria which you think is closely related to classifier performance and try to optimize that.",
                    "label": 0
                },
                {
                    "sent": "So a famous example of this would work, but Christianity on kernel alignment.",
                    "label": 1
                },
                {
                    "sent": "We said well, I've come up with some criterion here for the combined kernel, I want to be.",
                    "label": 1
                },
                {
                    "sent": "I want it to be as closely as possible, like the closest possible to the ideal kernel.",
                    "label": 0
                },
                {
                    "sent": "And it's ideal kernel matrix is the outer product of the glass labels, so you would like here.",
                    "label": 1
                },
                {
                    "sent": "Also references with you just said you would like to similarities within the.",
                    "label": 0
                },
                {
                    "sent": "If I can get away with that term, the similarities within the classes to be as large as possible and this similarities between the classes to be yes large possible as well so.",
                    "label": 0
                },
                {
                    "sent": "Small, small glasses that are far apart.",
                    "label": 0
                },
                {
                    "sent": "That's what I would like to achieve.",
                    "label": 0
                },
                {
                    "sent": "And then you can try to optimize the weights to optimize this kernel alignment.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second approach will be wrapper approach.",
                    "label": 0
                },
                {
                    "sent": "For example, using a grid search over possible weight settings with cross validation as a performance measure.",
                    "label": 1
                },
                {
                    "sent": "So using the actual.",
                    "label": 0
                },
                {
                    "sent": "Or fax machine performance as a target or using evolutionary algorithms or using gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And this is the work by Chappelle that Neil also referred to where he came up with the gradient.",
                    "label": 1
                },
                {
                    "sent": "I think an estimated gradient of the performance of the classifier and then did gradient descent.",
                    "label": 0
                },
                {
                    "sent": "On this on this estimated error to optimize their weights.",
                    "label": 0
                },
                {
                    "sent": "So this is a looper optimized.",
                    "label": 0
                },
                {
                    "sent": "You change the weights to optimize to calculate support vector machines again, and you keep updating weights.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jobmaster example we did this.",
                    "label": 0
                },
                {
                    "sent": "We have already combined these three sub feature subsets into one that gives us a 91% area under curve an by a very simple grid search.",
                    "label": 0
                },
                {
                    "sent": "We didn't have alot of time unfortunately.",
                    "label": 0
                },
                {
                    "sent": "We came up with this weight which actually shows that the derived kernel to kernel on the drive feature should be thrown out.",
                    "label": 0
                },
                {
                    "sent": "We got a 92 nearly 92% AUC so slight improvement again, probably not significant.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So did you do this?",
                    "label": 0
                },
                {
                    "sent": "This great search on on the actual outcome or the other kind of an outer loop cross validation again too?",
                    "label": 0
                },
                {
                    "sent": "I think I think this you did this.",
                    "label": 0
                },
                {
                    "sent": "It's supposed to be an outer loop.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so very simple.",
                    "label": 0
                },
                {
                    "sent": "It has to be better, right?",
                    "label": 0
                },
                {
                    "sent": "So it cannot be worse because yeah.",
                    "label": 0
                },
                {
                    "sent": "The one with equal weights is kind of in and out here.",
                    "label": 0
                },
                {
                    "sent": "Well, there's a landscape that you search over and the one the one point of equal weights 111.",
                    "label": 0
                },
                {
                    "sent": "That's in depth so.",
                    "label": 0
                },
                {
                    "sent": "Definition it's gonna be.",
                    "label": 0
                },
                {
                    "sent": "So we realized this problem of the double loop when we started last week.",
                    "label": 0
                },
                {
                    "sent": "And we also tried to add a couple of these specific kernels and they didn't give any better results in this specific example.",
                    "label": 0
                },
                {
                    "sent": "So we're the spectrum kernel and weighted degree kernel.",
                    "label": 0
                },
                {
                    "sent": "When we combine them, we got a slightly worse result again.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the third approach to combination so we've had to filter approach in a wrapper approach.",
                    "label": 0
                },
                {
                    "sent": "It approaches what you might call the embedded approach, in which you try to solve the entire optimization problem at once.",
                    "label": 0
                },
                {
                    "sent": "So remember that this was so this coming by very quickly the original formulation of the dual formulation of the support vector classifier problem.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Lockheed proposed to just put the combined kernel in here, treat them, use the weights as additional parameters, and then minimize this thing over the weights, adding a couple of constraints.",
                    "label": 0
                },
                {
                    "sent": "And he showed how you can see this as a semidefinite programming problem, which sounds like there's a lot of easy solutions for it, but still is very hard.",
                    "label": 0
                },
                {
                    "sent": "And then later people came up with faster approximations for faster versions of this, right?",
                    "label": 0
                },
                {
                    "sent": "So this is now often being done.",
                    "label": 0
                },
                {
                    "sent": "This is known as multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting, Lee, later it was shown by my boss and also shipped out again that if you look at the original SVM primal problem.",
                    "label": 0
                },
                {
                    "sent": "That looks like this are minimizing the weight.",
                    "label": 0
                },
                {
                    "sent": "The normal norm squared and two number to weight factor and at the same time minimizing the number of errors he makes.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That if you translate back.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just you formulation into a prime or you end up with sort of blocks version of that where you minimize.",
                    "label": 0
                },
                {
                    "sent": "And L1 norm over a per kernel L2 norm.",
                    "label": 1
                },
                {
                    "sent": "Basically what this boils down to is that there's an additional one normal similar to assume regression where you try to force weights kernel weights in fact to zero and kernels with non zero way to support kernels.",
                    "label": 1
                },
                {
                    "sent": "The entire mathematical derivations it's quite hard.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But they showed later.",
                    "label": 0
                },
                {
                    "sent": "Which showed later that it actually looks like this that you have and in a problem where they try to predict protein localization.",
                    "label": 0
                },
                {
                    "sent": "We use 69 different kernels in a couple of groups.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, this access your starters or .2 so these kernels didn't make the cuts.",
                    "label": 0
                },
                {
                    "sent": "They perform very well.",
                    "label": 0
                },
                {
                    "sent": "Very bad, very poorly.",
                    "label": 0
                },
                {
                    "sent": "These are the scores they get with each individual kernel.",
                    "label": 0
                },
                {
                    "sent": "This is the combined score they get and then this is what the weights look like.",
                    "label": 0
                },
                {
                    "sent": "So you see that these weights are actually very sparse.",
                    "label": 0
                },
                {
                    "sent": "There's not not all these kernels are used, just a few ones.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "So that's a sparsity and forged by this implicit L1 norm in the in the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and more recently people in the machine learning community have been studying this an extending this, so there's I summarize a lot of work now in one slide.",
                    "label": 0
                },
                {
                    "sent": "But there are references are there.",
                    "label": 0
                },
                {
                    "sent": "They move from 1:00 AM to gym or general LP norm where you can specify P and then they show that in some cases this this one original multiple kernel learning on some problems even performs worse than the original support vector machine.",
                    "label": 1
                },
                {
                    "sent": "But if you don't play around with this P you can start doing better.",
                    "label": 0
                },
                {
                    "sent": "So here the black Lines sports computer, the lower redline.",
                    "label": 1
                },
                {
                    "sent": "That's the multiple kernel learning by playing around the speed you can do slightly better and you move through less sparse solutions.",
                    "label": 1
                },
                {
                    "sent": "And there's other extensions to multiple Conan, localising, feature space, nonlinear kernel combinations, etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, how much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Tell me a couple of example applications.",
                    "label": 0
                },
                {
                    "sent": "So one is predicting about actually the first well about predicting protein interaction, but in very completely different ways.",
                    "label": 1
                },
                {
                    "sent": "So this was work by Mark Holzman, Master student of mine in 2008.",
                    "label": 0
                },
                {
                    "sent": "Where he basically collected a large number of features having to do with these proteins, myologie go expression colocalization.",
                    "label": 0
                },
                {
                    "sent": "So on 49 kernels into 49 features in total and the goal was to predict protein interaction and he came up with an evolutionary algorithm to optimize this with the weights.",
                    "label": 1
                },
                {
                    "sent": "And while the main finding was actually that after a lot of computation, we were just slightly better than just summing these kernels, so it's not very.",
                    "label": 0
                },
                {
                    "sent": "I very happy finding we were slightly better, but he also found that this multiple kernel learning without any data tricks like regularization.",
                    "label": 0
                },
                {
                    "sent": "There's multiple kernel learning even performed worse than just summation.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's something you don't read very often in the newspapers, and Colonel alignment was even much worse.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I'll skip this is horribly complete.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This figure I'll skip this one here.",
                    "label": 0
                },
                {
                    "sent": "You also see better sparsity of the multiple kernel learning approach, where these are.",
                    "label": 0
                },
                {
                    "sent": "These are the weights of the various kernels that we use.",
                    "label": 0
                },
                {
                    "sent": "The evolutionary algorithm sort of spreads all the weight around.",
                    "label": 0
                },
                {
                    "sent": "And you see also that these are five different runs of the algorithm, so for different runs you get reasonably different solutions based on cross validation, splits and so on.",
                    "label": 0
                },
                {
                    "sent": "The multiple kernel learning is more consistent in assigning the weights and also assigned lots of legacy features.",
                    "label": 0
                },
                {
                    "sent": "A zero wait, so this is the sparsity at work here.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A second example where this is recent work in by Japanese group is where they also tried to predict protein interaction, but now using literature, so the input now is not at all measurements, it's just sentences in this form taking furniture F1 is activated by Jack, two in the presence of P21 rest.",
                    "label": 1
                },
                {
                    "sent": "Richard already preprocessed this data to the extent that they have sentences which identified two proteins and now they would like to find whether this sentence says that these proteins interact or not.",
                    "label": 0
                },
                {
                    "sent": "So they applied boxes.",
                    "label": 0
                },
                {
                    "sent": "I think I'm not at all unexpected bosses, but they have two different passes, one which gives a quick and dirty result, and one which really decomposes the sentence into a hole.",
                    "label": 0
                },
                {
                    "sent": "Free.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I have very complicated setup too.",
                    "label": 0
                },
                {
                    "sent": "Really apply three different kernels.",
                    "label": 0
                },
                {
                    "sent": "The output of Apple trees of each of these parsers.",
                    "label": 0
                },
                {
                    "sent": "So a bag of words, kernel which is set kernel one kernel which count the number of common subtrees anagrafe kernel.",
                    "label": 1
                },
                {
                    "sent": "So they normalize them and then combine them and then again apply support vector classifier and they show that combining at least a number of these.",
                    "label": 0
                },
                {
                    "sent": "These things make sense.",
                    "label": 0
                },
                {
                    "sent": "But again, here they showed up.",
                    "label": 0
                },
                {
                    "sent": "So the individual individual kernels catch you do around 5559% performance combining the tree and the graph kernel gets you to 62% score 62% score, but combining everything lower score again.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And a final example, just to give you an impression that you can do completely different things with this.",
                    "label": 0
                },
                {
                    "sent": "Is a reaction kernel which was proposed earlier this year by by a group in Finland.",
                    "label": 0
                },
                {
                    "sent": "I think you owe is also here.",
                    "label": 0
                },
                {
                    "sent": "Not sure in this room, but he will.",
                    "label": 0
                },
                {
                    "sent": "He will present a similar work here on Thursday as well.",
                    "label": 0
                },
                {
                    "sent": "But the input is now a sequence representation, not exactly the sequence, but the representation of the sequence of an enzyme, and you try to predict an entire reaction that can be catalyzed by the enzyme.",
                    "label": 1
                },
                {
                    "sent": "So I'm not just a description of the enzyme that entire reaction, so a bunch of molecule graphs.",
                    "label": 0
                },
                {
                    "sent": "So this becomes this is no longer a standard classification problem.",
                    "label": 0
                },
                {
                    "sent": "It's a structured output prediction problem where you have a density over all possible reactions and so on.",
                    "label": 1
                },
                {
                    "sent": "But actually there's now less than kernels that you can come up with that compare molecule graphs based on the number of common subgraphs they have.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this shows up fully that that once you have this mechanism in place you can you can come up with really wild kernels to try and solve your problem.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion So integrated informatics we we try to combine all the knowledge and measurements we have about a certain problem to come up with predictions of interactions and functional annotation.",
                    "label": 0
                },
                {
                    "sent": "Now we have a lot of heterogeneous data, so this calls for intermediate integration.",
                    "label": 1
                },
                {
                    "sent": "Probably this made the case that we this is a right way of approaching such problems.",
                    "label": 1
                },
                {
                    "sent": "Colonels are very good vehicles for this.",
                    "label": 0
                },
                {
                    "sent": "Mainly because a lot of people in the machine learning community are doing very hard work to give us new algorithms and.",
                    "label": 1
                },
                {
                    "sent": "And a combination methods and also there's a wide variety of kernels that you can apply to biological problems.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it's very easy to start playing around with.",
                    "label": 0
                },
                {
                    "sent": "There used to be a wide variety of tools, but recently the winner Richard Group has come up with sort of collection into one single package called Shogun Toolbox.",
                    "label": 0
                },
                {
                    "sent": "You can download it and start playing around with it very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this Colonel combination that's still not free lunch normalization is important, but kernel combinations can overtrain as well.",
                    "label": 1
                },
                {
                    "sent": "I think of in our quickly hacked together example.",
                    "label": 0
                },
                {
                    "sent": "We already saw that.",
                    "label": 1
                },
                {
                    "sent": "The kernel classifier parameters have a large impact that require computing computationally intensive procedures to set.",
                    "label": 0
                },
                {
                    "sent": "So maybe Gaussian processes are the way to go then.",
                    "label": 0
                },
                {
                    "sent": "I'm also there.",
                    "label": 0
                },
                {
                    "sent": "I also heard that you have to be able to solve nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Do nonlinear optimization problems right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And filled the whole.",
                    "label": 0
                },
                {
                    "sent": "The whole approach is computationally intensive, especially for genome wide data set.",
                    "label": 0
                },
                {
                    "sent": "So for protein interactions you want to predict on 10s of thousands of.",
                    "label": 0
                },
                {
                    "sent": "Proteins which gives you 100 million protein pairs, and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So this is not easily solved.",
                    "label": 0
                },
                {
                    "sent": "You really need to think about how you do it.",
                    "label": 0
                },
                {
                    "sent": "And as I think that a couple of examples I've showed is that.",
                    "label": 0
                },
                {
                    "sent": "Especially on the data points that it's critical to choose to write kernel, and I think there's no no Handbook yet on how to choose right kernel.",
                    "label": 0
                },
                {
                    "sent": "How to choose the right combination of kernels.",
                    "label": 1
                },
                {
                    "sent": "But they keep it simple.",
                    "label": 0
                },
                {
                    "sent": "Stupid principle applies, and very often this simple summation already works quite well.",
                    "label": 0
                },
                {
                    "sent": "So that's just let me think the first thing you try.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for that.",
                    "label": 0
                }
            ]
        }
    }
}