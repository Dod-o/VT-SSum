{
    "id": "ycm5cnyg45bz6suphcdbetamfxlabagj",
    "title": "Best Paper - Information-Theoretic Metric Learning",
    "info": {
        "author": [
            "Brian Kulis, University of Texas at Austin"
        ],
        "published": "June 22, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/icml07_kulis_itml/",
    "segmentation": [
        [
            "Alright thanks.",
            "So this is this is joint work done with a number of people at University of Texas Jason Davis, Prateek Jain, Super Sarah and Inderjit Dhillon?"
        ],
        [
            "So in case you weren't here for the last talk or you were in the bathroom or something, so metric learning is an important problem in machine learning these days.",
            "Very popular problem.",
            "After all, the notion of the distance between objects is very very critical to many problems in machine learning, classification, clustering and so on.",
            "And having an appropriate way to measure the distance between two objects ultimately determines whether or not your learning algorithm will succeed or fail.",
            "So it's a very reasonable thing to want to be able to learn some notion of distance between objects using class information or some sort of constraints that you may have on your domain or on your data.",
            "And it's of course useful wherever distances are used.",
            "Now there's a lot of approaches that exist for metric learning, and this is.",
            "Certainly not an exhaustive list, but these are some of the papers that are most similar to the approach that we have, which is in learning a Mahalanobis distance."
        ],
        [
            "Now what makes our approach?",
            "I think different from other existing techniques for metric learning is that we have a.",
            "So we have a very basic and straightforward framework.",
            "For learning a Mahalanobis distance, but it has a lot of nice features.",
            "So first of all is very simple to implement.",
            "It's very scalable to very large datasets, were able to incorporate many different kinds of constraints.",
            "So not just simple similarity, dissimilarity constraints, but more complicated relative distance constraints and so on.",
            "We can also look at a variant of the algorithm which does online metric learning and that has some provable regret bounds associated with it.",
            "And finally, we'll talk a little bit about kernelization of the algorithm, so metric learning in kernel space an in that case, what we're going to be doing is essentially learning a certain kind of kernel function.",
            "As far as I know."
        ],
        [
            "Existing all existing methods.",
            "Fail on one of those.",
            "One of those those features.",
            "So as I said, we'll be learning a Mahalanobis distance, which is which I've given up at the top here.",
            "Mahalanobis distance.",
            "If a were equal to the identity, then we would just be looking at the squared Euclidean distance between X&Y.",
            "But by sticking in this positive definite matrix A which is D by D, where D is the dimensionality of the data.",
            "We allow stretching and an rotation of your input data in an arbitrary way.",
            "So for example on the left.",
            "We have some data which I don't know if you can tell, but there's sort of four blobs.",
            "The upper left and bottom left or red and the upper right and bottom right are blue, and we want to make sure that that when we learn a distance function, the red points are close to each other and farther from the blue point, so we could stretch out the X axis and squish the Y axis, and by defining an appropriate A, what we do is.",
            "We're going to be measuring squared Euclidean distance in this map space, so that's that's what a Mahalanobis distance captures."
        ],
        [
            "Now, in order to do metric learning we need to introduce some sort of constraints, so we need to say that we want to learn a distance function that satisfies some pre given or some input constraints to the problem.",
            "And as I said, we can handle a wide variety of constraints, but the kind of constraints that I'll talk about in this talk just for simplicity are similarity and dissimilarity constraints.",
            "Which are are related, but not quite the same as the equivalent constraints from the previous talk.",
            "So the goal of a metric learning algorithm is we want to learn some metric DA.",
            "And we want that metric to be close, close to some starting metric day, not.",
            "But we want it to satisfy the constraints that we've given to the problem.",
            "So this this first of all we need, we need some notion of what denad is.",
            "We assume that some baseline baseline metric could be.",
            "If not, was the identity, then that's just going to be squared Euclidean distance?",
            "Or we could use the inverse of the covariance of the data.",
            "We have some baseline.",
            "That's RDA not.",
            "And Secondly, we need some notion of what it means for two Mahalanobis distances to be close to one another."
        ],
        [
            "So this is probably the most important slide of the talk, so this is sort of our approach.",
            "We're going to look at a very simple bijection between the set of Mahalanobis distances and set of multivariate Gaussians.",
            "So if you recall the PDF of a multivariate Gaussian is parameterized by a mean and a matrix A, which will say is the inverse of the covariance matrix.",
            "An IF we fix the mean.",
            "So we fixed mu, say at the origin or something.",
            "Then the set of multivariate Gaussians is just parameterized by A and so there's a bijection between the set of multivariate Gaussians and the set of Mahalanobis distances.",
            "So what we're going to do is we're going to compare two distance functions by looking at their corresponding Gaussians.",
            "So we take DAA.",
            "We look at the corresponding Gaussian parameterized by a.",
            "We look at day.",
            "Now we look at its corresponding gassy and and then we compare those two Gaussians and our notion of divergent or closeness is given by the differential relative entropy, which is a.",
            "A standard way to compare."
        ],
        [
            "22 probability distributions.",
            "So more formally, we can write the problem as I have on the slide which is minimized.",
            "The differential relative entropy between the Gaussian parameterized by a not an the gasoline parameterized by a subject to the constraints.",
            "These similarity and dissimilarity constraints, and we also require that a be positive definite.",
            "Now if you look at this, it's a little bit a little bit nasty, so it's not clear immediately how you might go about solving it.",
            "So what we do is we appeal to a result which shows that if we fix the means of two multivariate Gaussians, we can actually rewrite the differential relative entropy as 1/2 times the log determinant divergance between A and a, not where the log determinant divergance.",
            "Is matrix divergance defined at the bottom of the slide?"
        ],
        [
            "So we can.",
            "We can just directly map from the relative entropy formulation of the KL formulation.",
            "To what I call the log determinant formulation.",
            "So the objective is just rewritten and then the constraints.",
            "The distance constraints.",
            "I just turned into standard form, so it's clear that there are linear constraints on a, so we map it over to this equivalent formulation.",
            "The log determinant formulation.",
            "And now we can try to solve this problem.",
            "Now if you look at what the definition of the log determinant divergences.",
            "It looks almost like an SDP.",
            "It's got it has this trace term, but then there's this this logdet term in there so that.",
            "That will mean that we can't.",
            "We can't quite use semi definite programming.",
            "So we need to develop an algorithm for."
        ],
        [
            "Solving this and I don't want to go into too much detail, but we use the method of Bregman projections, which, very briefly is a dual coordinate descent method.",
            "Essentially, what you do is you have a Mahalanobis matrix at each iteration, and you project.",
            "That Mahalanobis matrix onto a single constraint?",
            "So you choose a constraint, you project it and then and then you do some correction and you do this iteratively and as it turns out, the update.",
            "This projection can be done in a very simple way.",
            "It turns out to be a rank 1 update, so the the cost of doing a single projection is ordered D ^2 D is the."
        ],
        [
            "Mention aliti of the data.",
            "And briefly, some of the nice things about this algorithm.",
            "First, it's very scalable, since we have these simple projections and we don't have to worry about enforcing the positive definiteness constraint because the projection does that for us automatically and there's no.",
            "There's no line search or anything, it's all closed form, and no eigenvector computation.",
            "It's very simple."
        ],
        [
            "At the beginning of the talk I mentioned.",
            "Two nice aspects of the of the approach.",
            "One was this connection to kernel learning and one was a connection to online metric learning.",
            "So I'd like to talk about that now for a couple of slides.",
            "On the left I have the log determinant formulation.",
            "I didn't mention it before, but we also can add Slack variables to the formulation to guarantee the existence of a solution, which we do in general.",
            "So OK, so on the left we have the log determinant formulation, which is over D by D matrices.",
            "Now it might be the case that the dimensionality of the data is very high.",
            "In which case maybe it's not feasible to work with a directly.",
            "So on the right here is what I call the kernel formulation which is over Now N by N matrices, where N is the number of data points in your training set.",
            "And the constraints are slightly different.",
            "But it turns out that if.",
            "K Star is the optimal solution to your this kernel problem.",
            "An efca stars the optimal solution to the kernel problem, and a star is the optimal solution to the log determinant problem.",
            "Then K star equals X transpose a star X, where X is your.",
            "Input data your input data vectors.",
            "So what this will allow us to do is work with the kernel formulation when the dimensionality of the data."
        ],
        [
            "Is very high, so so to make that maybe a little bit more clear if we want to.",
            "If we want to learn a metric in kernel space.",
            "So say we have some kernel function Phi X transpose, VY and now we want to learn.",
            "We want to learn some DNA of Phoenix, Phi Y.",
            "Now if you expand that just using the definition of the Mahalanobis distance.",
            "What what it's equivalent to is learning a kernel function of the form Phi X transpose a few Y.",
            "So the question is, how can we learn given the fact that we can compute Phoenix transfers fee, why?",
            "How do we compute Phi X transpose ACY?",
            "And if you solve this kernel formulation, you can actually do this and what we show is that the learned kernel.",
            "Has this form, so it's the original kernel plus a quadratic term which is.",
            "Based on the Zion XJR.",
            "Data points that are part of your constraints."
        ],
        [
            "So another, another aspect of the algorithm of this formulation, which is which is nice, is that we can also do metric learning in an online manner.",
            "We can we can, we can.",
            "Do a variant of this projection method that I talked about a few slides ago in an online setting.",
            "So in the the set up for that is we have.",
            "We have we want to, at every iteration we have some Mahalanobis matrix A at step step T say and we receive a pair of points.",
            "And we predict the distance between those two points and and then we receive the true distance and incur some loss for how far off we were from the from the true distance.",
            "Then we update our Mahalanobis metric for the next iteration and what we want to do is we want to minimize the total loss the sum of the losses at.",
            "At all iterations and what's typically done in online learning is you want to prove some sort of regret bound.",
            "That is, you want to show that you don't do much worse than the best possible offline algorithm.",
            "So we were able to prove a bound.",
            "So LOML is the loss of the online metric learning algorithm, and LA Star is the loss of the best offline metric algorithm.",
            "So note that LA Star is not the loss of like our offline information theoretic metric learning algorithm.",
            "It's like the best possible offline algorithm that could exist."
        ],
        [
            "In theory.",
            "So in the in the few minutes I have left, I'll just present some empirical results of this approach.",
            "What we did is we set up.",
            "We did your typical K nearest neighbor classifier.",
            "So we have a training data set.",
            "We generate constraints from that training data set.",
            "We learn the metric and then we do K nearest neighbor over the test data.",
            "And some of the details are written up on this slide.",
            "We compare it against the number of different existing approaches, including some baseline metrics like the squared Euclidean distance and using the inverse covariance."
        ],
        [
            "For your Mahalanobis distance.",
            "So first we look at some of the standard UCI datasets that everybody looks at.",
            "Now the leftmost.",
            "The leftmost bar is ITM accent, which is information theoretic metric learning with a not equal to the identity matrix.",
            "And what we see is that in general that that algorithm is performing quite comparably to other methods.",
            "And and is often is within the confidence intervals, is always sort of performing the best."
        ],
        [
            "We looked at an application of metric learning to nearest neighbor software support, so there's a system developed at you T called clarify the way it works is every execution of a program is.",
            "Is like one datapoint an IT and monitors things like function calls and so forth.",
            "And so if you run your program and it crashes, you might want to say, oh, you know what other program executions are similar to this that are from, say, some training set so I can figure out why my program crashed.",
            "So we looked at so we looked at metric learning in this context and looked at a few different programs, LA Tech and an MP3 player database manager.",
            "In this Linux kernel application.",
            "And again, we found that we do quite quite well as compared to the other methods, especially on the LA Tech data set where we seem to be doing very well.",
            "In terms of timing, I don't have the results here, but we're also very, very fast, so in general I think we're the fastest of the methods that we looked at."
        ],
        [
            "So to wrap up, we so the formulation for this problem is that of minimizing the differential relative entropy of two Gaussians corresponding to.",
            "Mahalanobis distance MMDA and Mohammed Mahalanobis distance DA, not an.",
            "That's that's straightforward setup has a lot of nice advantages.",
            "We can.",
            "We have a very simple and scalable algorithm based on logdet projections, and it's able to incorporate many different constraints.",
            "We concur.",
            "Analyze it.",
            "We can prove regret bounds and empirically.",
            "It seems to be quite competitive with existing.",
            "Techniques.",
            "Thanks.",
            "Complete likelihood of a Gaussian, i.e.",
            "You have locked it term in dogs head term.",
            "Add it to the melanosis distance.",
            "Can you still have bound?",
            "I don't know if you can have bands I don't, so how would you?",
            "So this distance is is in the exponential of the Gaussian, but the Gaussian also has allowed PET in front.",
            "So if you take the log of the full Gaussian, the log likelihood of a multi dimensional Gaussian, you get the Molyneaux Bridge distance plus or minus some log 10.",
            "You use that as your loss.",
            "Do you still get done?",
            "I see so I don't know honestly.",
            "You know, we haven't really thought about that, but.",
            "It's possible, but I really don't know.",
            "Yeah.",
            "It's about the online version of the algorithm, sure.",
            "The complexity of the online update.",
            "You mentioned the request lands that they depend on how you tune the learning rate, right?",
            "If one does it right.",
            "What is the bounds look like?",
            "I see OK, so first first question was what is the running time and the running time is going to be ordered D squared to do the online update, which is the same as as doing the offline Bregman projections the same.",
            "It's basically the same kind of update now.",
            "Yeah, as far as the the online learning bounds are concerned, we do have this right.",
            "We have this sort of dependency on the learning rate.",
            "Which which is adaptive for the algorithm.",
            "Now what we observe is that that learning rate does does not go very low, so our bounds are quite good empirically.",
            "Now it would be nice to remove the dependence on on the learning rate in the bound, but we're not.",
            "We're not entirely sure how to do that.",
            "I mean, this is sort of like a like a first pass at.",
            "Yeah, you know which norms enter into the bound, which norms of the target matrix A. Norms in the.",
            "There's a pair of dual norms, right?",
            "To these balls.",
            "I.",
            "Well, I mean the bound itself.",
            "Has a logdet term in it.",
            "So that's sort of how you know if you look at the bound, it's like the loss of the online is less than or equal to the loss of the offline plus logdet terms.",
            "Yeah, I mean.",
            "Wondering?",
            "Yep.",
            "No, it's not.",
            "It's not limited to trans transductive setting.",
            "We're actually learning the kernel function itself, so you can do anything, anything that you could do with any other kernel function you could do with this kernel function.",
            "At this point.",
            "Metrics.",
            "Well, you're not just learning a kernel matrix here.",
            "I mean, you're learning, you know something of the form like a new kernel function.",
            "You know the new kernel function between X&Y equals the old kernel function plus a term that depends on the training data.",
            "So you can think of it as like you know you have just your starting kernel function and then something else that has some impact on your kernel function.",
            "So we've looked a little bit at actually the looking at.",
            "How well like this kernel learning?",
            "Aspect you know how well it does on some different learning tasks in it, it seems to do quite well, but we haven't done a whole lot of comparison with other kernel learning methods.",
            "OK, cool, thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright thanks.",
                    "label": 0
                },
                {
                    "sent": "So this is this is joint work done with a number of people at University of Texas Jason Davis, Prateek Jain, Super Sarah and Inderjit Dhillon?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in case you weren't here for the last talk or you were in the bathroom or something, so metric learning is an important problem in machine learning these days.",
                    "label": 1
                },
                {
                    "sent": "Very popular problem.",
                    "label": 0
                },
                {
                    "sent": "After all, the notion of the distance between objects is very very critical to many problems in machine learning, classification, clustering and so on.",
                    "label": 0
                },
                {
                    "sent": "And having an appropriate way to measure the distance between two objects ultimately determines whether or not your learning algorithm will succeed or fail.",
                    "label": 0
                },
                {
                    "sent": "So it's a very reasonable thing to want to be able to learn some notion of distance between objects using class information or some sort of constraints that you may have on your domain or on your data.",
                    "label": 0
                },
                {
                    "sent": "And it's of course useful wherever distances are used.",
                    "label": 1
                },
                {
                    "sent": "Now there's a lot of approaches that exist for metric learning, and this is.",
                    "label": 0
                },
                {
                    "sent": "Certainly not an exhaustive list, but these are some of the papers that are most similar to the approach that we have, which is in learning a Mahalanobis distance.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what makes our approach?",
                    "label": 1
                },
                {
                    "sent": "I think different from other existing techniques for metric learning is that we have a.",
                    "label": 0
                },
                {
                    "sent": "So we have a very basic and straightforward framework.",
                    "label": 1
                },
                {
                    "sent": "For learning a Mahalanobis distance, but it has a lot of nice features.",
                    "label": 0
                },
                {
                    "sent": "So first of all is very simple to implement.",
                    "label": 0
                },
                {
                    "sent": "It's very scalable to very large datasets, were able to incorporate many different kinds of constraints.",
                    "label": 0
                },
                {
                    "sent": "So not just simple similarity, dissimilarity constraints, but more complicated relative distance constraints and so on.",
                    "label": 0
                },
                {
                    "sent": "We can also look at a variant of the algorithm which does online metric learning and that has some provable regret bounds associated with it.",
                    "label": 0
                },
                {
                    "sent": "And finally, we'll talk a little bit about kernelization of the algorithm, so metric learning in kernel space an in that case, what we're going to be doing is essentially learning a certain kind of kernel function.",
                    "label": 1
                },
                {
                    "sent": "As far as I know.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Existing all existing methods.",
                    "label": 0
                },
                {
                    "sent": "Fail on one of those.",
                    "label": 0
                },
                {
                    "sent": "One of those those features.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we'll be learning a Mahalanobis distance, which is which I've given up at the top here.",
                    "label": 0
                },
                {
                    "sent": "Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "If a were equal to the identity, then we would just be looking at the squared Euclidean distance between X&Y.",
                    "label": 0
                },
                {
                    "sent": "But by sticking in this positive definite matrix A which is D by D, where D is the dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "We allow stretching and an rotation of your input data in an arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "So for example on the left.",
                    "label": 0
                },
                {
                    "sent": "We have some data which I don't know if you can tell, but there's sort of four blobs.",
                    "label": 0
                },
                {
                    "sent": "The upper left and bottom left or red and the upper right and bottom right are blue, and we want to make sure that that when we learn a distance function, the red points are close to each other and farther from the blue point, so we could stretch out the X axis and squish the Y axis, and by defining an appropriate A, what we do is.",
                    "label": 0
                },
                {
                    "sent": "We're going to be measuring squared Euclidean distance in this map space, so that's that's what a Mahalanobis distance captures.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in order to do metric learning we need to introduce some sort of constraints, so we need to say that we want to learn a distance function that satisfies some pre given or some input constraints to the problem.",
                    "label": 0
                },
                {
                    "sent": "And as I said, we can handle a wide variety of constraints, but the kind of constraints that I'll talk about in this talk just for simplicity are similarity and dissimilarity constraints.",
                    "label": 0
                },
                {
                    "sent": "Which are are related, but not quite the same as the equivalent constraints from the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So the goal of a metric learning algorithm is we want to learn some metric DA.",
                    "label": 1
                },
                {
                    "sent": "And we want that metric to be close, close to some starting metric day, not.",
                    "label": 1
                },
                {
                    "sent": "But we want it to satisfy the constraints that we've given to the problem.",
                    "label": 0
                },
                {
                    "sent": "So this this first of all we need, we need some notion of what denad is.",
                    "label": 0
                },
                {
                    "sent": "We assume that some baseline baseline metric could be.",
                    "label": 0
                },
                {
                    "sent": "If not, was the identity, then that's just going to be squared Euclidean distance?",
                    "label": 0
                },
                {
                    "sent": "Or we could use the inverse of the covariance of the data.",
                    "label": 0
                },
                {
                    "sent": "We have some baseline.",
                    "label": 0
                },
                {
                    "sent": "That's RDA not.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we need some notion of what it means for two Mahalanobis distances to be close to one another.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is probably the most important slide of the talk, so this is sort of our approach.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at a very simple bijection between the set of Mahalanobis distances and set of multivariate Gaussians.",
                    "label": 1
                },
                {
                    "sent": "So if you recall the PDF of a multivariate Gaussian is parameterized by a mean and a matrix A, which will say is the inverse of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "An IF we fix the mean.",
                    "label": 0
                },
                {
                    "sent": "So we fixed mu, say at the origin or something.",
                    "label": 0
                },
                {
                    "sent": "Then the set of multivariate Gaussians is just parameterized by A and so there's a bijection between the set of multivariate Gaussians and the set of Mahalanobis distances.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to compare two distance functions by looking at their corresponding Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So we take DAA.",
                    "label": 0
                },
                {
                    "sent": "We look at the corresponding Gaussian parameterized by a.",
                    "label": 0
                },
                {
                    "sent": "We look at day.",
                    "label": 0
                },
                {
                    "sent": "Now we look at its corresponding gassy and and then we compare those two Gaussians and our notion of divergent or closeness is given by the differential relative entropy, which is a.",
                    "label": 0
                },
                {
                    "sent": "A standard way to compare.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "22 probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So more formally, we can write the problem as I have on the slide which is minimized.",
                    "label": 0
                },
                {
                    "sent": "The differential relative entropy between the Gaussian parameterized by a not an the gasoline parameterized by a subject to the constraints.",
                    "label": 0
                },
                {
                    "sent": "These similarity and dissimilarity constraints, and we also require that a be positive definite.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at this, it's a little bit a little bit nasty, so it's not clear immediately how you might go about solving it.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we appeal to a result which shows that if we fix the means of two multivariate Gaussians, we can actually rewrite the differential relative entropy as 1/2 times the log determinant divergance between A and a, not where the log determinant divergance.",
                    "label": 0
                },
                {
                    "sent": "Is matrix divergance defined at the bottom of the slide?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can just directly map from the relative entropy formulation of the KL formulation.",
                    "label": 0
                },
                {
                    "sent": "To what I call the log determinant formulation.",
                    "label": 0
                },
                {
                    "sent": "So the objective is just rewritten and then the constraints.",
                    "label": 0
                },
                {
                    "sent": "The distance constraints.",
                    "label": 0
                },
                {
                    "sent": "I just turned into standard form, so it's clear that there are linear constraints on a, so we map it over to this equivalent formulation.",
                    "label": 0
                },
                {
                    "sent": "The log determinant formulation.",
                    "label": 0
                },
                {
                    "sent": "And now we can try to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at what the definition of the log determinant divergences.",
                    "label": 0
                },
                {
                    "sent": "It looks almost like an SDP.",
                    "label": 0
                },
                {
                    "sent": "It's got it has this trace term, but then there's this this logdet term in there so that.",
                    "label": 0
                },
                {
                    "sent": "That will mean that we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't quite use semi definite programming.",
                    "label": 0
                },
                {
                    "sent": "So we need to develop an algorithm for.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solving this and I don't want to go into too much detail, but we use the method of Bregman projections, which, very briefly is a dual coordinate descent method.",
                    "label": 1
                },
                {
                    "sent": "Essentially, what you do is you have a Mahalanobis matrix at each iteration, and you project.",
                    "label": 0
                },
                {
                    "sent": "That Mahalanobis matrix onto a single constraint?",
                    "label": 0
                },
                {
                    "sent": "So you choose a constraint, you project it and then and then you do some correction and you do this iteratively and as it turns out, the update.",
                    "label": 0
                },
                {
                    "sent": "This projection can be done in a very simple way.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be a rank 1 update, so the the cost of doing a single projection is ordered D ^2 D is the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mention aliti of the data.",
                    "label": 0
                },
                {
                    "sent": "And briefly, some of the nice things about this algorithm.",
                    "label": 0
                },
                {
                    "sent": "First, it's very scalable, since we have these simple projections and we don't have to worry about enforcing the positive definiteness constraint because the projection does that for us automatically and there's no.",
                    "label": 0
                },
                {
                    "sent": "There's no line search or anything, it's all closed form, and no eigenvector computation.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the beginning of the talk I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Two nice aspects of the of the approach.",
                    "label": 0
                },
                {
                    "sent": "One was this connection to kernel learning and one was a connection to online metric learning.",
                    "label": 1
                },
                {
                    "sent": "So I'd like to talk about that now for a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "On the left I have the log determinant formulation.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention it before, but we also can add Slack variables to the formulation to guarantee the existence of a solution, which we do in general.",
                    "label": 0
                },
                {
                    "sent": "So OK, so on the left we have the log determinant formulation, which is over D by D matrices.",
                    "label": 0
                },
                {
                    "sent": "Now it might be the case that the dimensionality of the data is very high.",
                    "label": 0
                },
                {
                    "sent": "In which case maybe it's not feasible to work with a directly.",
                    "label": 0
                },
                {
                    "sent": "So on the right here is what I call the kernel formulation which is over Now N by N matrices, where N is the number of data points in your training set.",
                    "label": 0
                },
                {
                    "sent": "And the constraints are slightly different.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that if.",
                    "label": 0
                },
                {
                    "sent": "K Star is the optimal solution to your this kernel problem.",
                    "label": 1
                },
                {
                    "sent": "An efca stars the optimal solution to the kernel problem, and a star is the optimal solution to the log determinant problem.",
                    "label": 0
                },
                {
                    "sent": "Then K star equals X transpose a star X, where X is your.",
                    "label": 0
                },
                {
                    "sent": "Input data your input data vectors.",
                    "label": 0
                },
                {
                    "sent": "So what this will allow us to do is work with the kernel formulation when the dimensionality of the data.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is very high, so so to make that maybe a little bit more clear if we want to.",
                    "label": 0
                },
                {
                    "sent": "If we want to learn a metric in kernel space.",
                    "label": 1
                },
                {
                    "sent": "So say we have some kernel function Phi X transpose, VY and now we want to learn.",
                    "label": 0
                },
                {
                    "sent": "We want to learn some DNA of Phoenix, Phi Y.",
                    "label": 0
                },
                {
                    "sent": "Now if you expand that just using the definition of the Mahalanobis distance.",
                    "label": 1
                },
                {
                    "sent": "What what it's equivalent to is learning a kernel function of the form Phi X transpose a few Y.",
                    "label": 1
                },
                {
                    "sent": "So the question is, how can we learn given the fact that we can compute Phoenix transfers fee, why?",
                    "label": 0
                },
                {
                    "sent": "How do we compute Phi X transpose ACY?",
                    "label": 0
                },
                {
                    "sent": "And if you solve this kernel formulation, you can actually do this and what we show is that the learned kernel.",
                    "label": 0
                },
                {
                    "sent": "Has this form, so it's the original kernel plus a quadratic term which is.",
                    "label": 0
                },
                {
                    "sent": "Based on the Zion XJR.",
                    "label": 0
                },
                {
                    "sent": "Data points that are part of your constraints.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another, another aspect of the algorithm of this formulation, which is which is nice, is that we can also do metric learning in an online manner.",
                    "label": 0
                },
                {
                    "sent": "We can we can, we can.",
                    "label": 0
                },
                {
                    "sent": "Do a variant of this projection method that I talked about a few slides ago in an online setting.",
                    "label": 0
                },
                {
                    "sent": "So in the the set up for that is we have.",
                    "label": 0
                },
                {
                    "sent": "We have we want to, at every iteration we have some Mahalanobis matrix A at step step T say and we receive a pair of points.",
                    "label": 1
                },
                {
                    "sent": "And we predict the distance between those two points and and then we receive the true distance and incur some loss for how far off we were from the from the true distance.",
                    "label": 0
                },
                {
                    "sent": "Then we update our Mahalanobis metric for the next iteration and what we want to do is we want to minimize the total loss the sum of the losses at.",
                    "label": 0
                },
                {
                    "sent": "At all iterations and what's typically done in online learning is you want to prove some sort of regret bound.",
                    "label": 0
                },
                {
                    "sent": "That is, you want to show that you don't do much worse than the best possible offline algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we were able to prove a bound.",
                    "label": 0
                },
                {
                    "sent": "So LOML is the loss of the online metric learning algorithm, and LA Star is the loss of the best offline metric algorithm.",
                    "label": 1
                },
                {
                    "sent": "So note that LA Star is not the loss of like our offline information theoretic metric learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's like the best possible offline algorithm that could exist.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In theory.",
                    "label": 0
                },
                {
                    "sent": "So in the in the few minutes I have left, I'll just present some empirical results of this approach.",
                    "label": 0
                },
                {
                    "sent": "What we did is we set up.",
                    "label": 0
                },
                {
                    "sent": "We did your typical K nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "So we have a training data set.",
                    "label": 0
                },
                {
                    "sent": "We generate constraints from that training data set.",
                    "label": 0
                },
                {
                    "sent": "We learn the metric and then we do K nearest neighbor over the test data.",
                    "label": 0
                },
                {
                    "sent": "And some of the details are written up on this slide.",
                    "label": 0
                },
                {
                    "sent": "We compare it against the number of different existing approaches, including some baseline metrics like the squared Euclidean distance and using the inverse covariance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For your Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "So first we look at some of the standard UCI datasets that everybody looks at.",
                    "label": 0
                },
                {
                    "sent": "Now the leftmost.",
                    "label": 0
                },
                {
                    "sent": "The leftmost bar is ITM accent, which is information theoretic metric learning with a not equal to the identity matrix.",
                    "label": 1
                },
                {
                    "sent": "And what we see is that in general that that algorithm is performing quite comparably to other methods.",
                    "label": 0
                },
                {
                    "sent": "And and is often is within the confidence intervals, is always sort of performing the best.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at an application of metric learning to nearest neighbor software support, so there's a system developed at you T called clarify the way it works is every execution of a program is.",
                    "label": 0
                },
                {
                    "sent": "Is like one datapoint an IT and monitors things like function calls and so forth.",
                    "label": 0
                },
                {
                    "sent": "And so if you run your program and it crashes, you might want to say, oh, you know what other program executions are similar to this that are from, say, some training set so I can figure out why my program crashed.",
                    "label": 0
                },
                {
                    "sent": "So we looked at so we looked at metric learning in this context and looked at a few different programs, LA Tech and an MP3 player database manager.",
                    "label": 0
                },
                {
                    "sent": "In this Linux kernel application.",
                    "label": 0
                },
                {
                    "sent": "And again, we found that we do quite quite well as compared to the other methods, especially on the LA Tech data set where we seem to be doing very well.",
                    "label": 0
                },
                {
                    "sent": "In terms of timing, I don't have the results here, but we're also very, very fast, so in general I think we're the fastest of the methods that we looked at.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to wrap up, we so the formulation for this problem is that of minimizing the differential relative entropy of two Gaussians corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Mahalanobis distance MMDA and Mohammed Mahalanobis distance DA, not an.",
                    "label": 0
                },
                {
                    "sent": "That's that's straightforward setup has a lot of nice advantages.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We have a very simple and scalable algorithm based on logdet projections, and it's able to incorporate many different constraints.",
                    "label": 1
                },
                {
                    "sent": "We concur.",
                    "label": 0
                },
                {
                    "sent": "Analyze it.",
                    "label": 1
                },
                {
                    "sent": "We can prove regret bounds and empirically.",
                    "label": 0
                },
                {
                    "sent": "It seems to be quite competitive with existing.",
                    "label": 1
                },
                {
                    "sent": "Techniques.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Complete likelihood of a Gaussian, i.e.",
                    "label": 0
                },
                {
                    "sent": "You have locked it term in dogs head term.",
                    "label": 0
                },
                {
                    "sent": "Add it to the melanosis distance.",
                    "label": 0
                },
                {
                    "sent": "Can you still have bound?",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can have bands I don't, so how would you?",
                    "label": 0
                },
                {
                    "sent": "So this distance is is in the exponential of the Gaussian, but the Gaussian also has allowed PET in front.",
                    "label": 0
                },
                {
                    "sent": "So if you take the log of the full Gaussian, the log likelihood of a multi dimensional Gaussian, you get the Molyneaux Bridge distance plus or minus some log 10.",
                    "label": 0
                },
                {
                    "sent": "You use that as your loss.",
                    "label": 0
                },
                {
                    "sent": "Do you still get done?",
                    "label": 0
                },
                {
                    "sent": "I see so I don't know honestly.",
                    "label": 0
                },
                {
                    "sent": "You know, we haven't really thought about that, but.",
                    "label": 0
                },
                {
                    "sent": "It's possible, but I really don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's about the online version of the algorithm, sure.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the online update.",
                    "label": 0
                },
                {
                    "sent": "You mentioned the request lands that they depend on how you tune the learning rate, right?",
                    "label": 0
                },
                {
                    "sent": "If one does it right.",
                    "label": 0
                },
                {
                    "sent": "What is the bounds look like?",
                    "label": 0
                },
                {
                    "sent": "I see OK, so first first question was what is the running time and the running time is going to be ordered D squared to do the online update, which is the same as as doing the offline Bregman projections the same.",
                    "label": 0
                },
                {
                    "sent": "It's basically the same kind of update now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as far as the the online learning bounds are concerned, we do have this right.",
                    "label": 0
                },
                {
                    "sent": "We have this sort of dependency on the learning rate.",
                    "label": 0
                },
                {
                    "sent": "Which which is adaptive for the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now what we observe is that that learning rate does does not go very low, so our bounds are quite good empirically.",
                    "label": 0
                },
                {
                    "sent": "Now it would be nice to remove the dependence on on the learning rate in the bound, but we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not entirely sure how to do that.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is sort of like a like a first pass at.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you know which norms enter into the bound, which norms of the target matrix A. Norms in the.",
                    "label": 0
                },
                {
                    "sent": "There's a pair of dual norms, right?",
                    "label": 0
                },
                {
                    "sent": "To these balls.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean the bound itself.",
                    "label": 0
                },
                {
                    "sent": "Has a logdet term in it.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of how you know if you look at the bound, it's like the loss of the online is less than or equal to the loss of the offline plus logdet terms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Wondering?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not limited to trans transductive setting.",
                    "label": 0
                },
                {
                    "sent": "We're actually learning the kernel function itself, so you can do anything, anything that you could do with any other kernel function you could do with this kernel function.",
                    "label": 0
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "Metrics.",
                    "label": 0
                },
                {
                    "sent": "Well, you're not just learning a kernel matrix here.",
                    "label": 0
                },
                {
                    "sent": "I mean, you're learning, you know something of the form like a new kernel function.",
                    "label": 0
                },
                {
                    "sent": "You know the new kernel function between X&Y equals the old kernel function plus a term that depends on the training data.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as like you know you have just your starting kernel function and then something else that has some impact on your kernel function.",
                    "label": 0
                },
                {
                    "sent": "So we've looked a little bit at actually the looking at.",
                    "label": 0
                },
                {
                    "sent": "How well like this kernel learning?",
                    "label": 0
                },
                {
                    "sent": "Aspect you know how well it does on some different learning tasks in it, it seems to do quite well, but we haven't done a whole lot of comparison with other kernel learning methods.",
                    "label": 0
                },
                {
                    "sent": "OK, cool, thank you again.",
                    "label": 0
                }
            ]
        }
    }
}