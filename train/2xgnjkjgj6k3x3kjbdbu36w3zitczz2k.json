{
    "id": "2xgnjkjgj6k3x3kjbdbu36w3zitczz2k",
    "title": "Learning Coverage Functions and Private Release of Marginals",
    "info": {
        "author": [
            "Pravesh Kothari, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_kothari_learning/",
    "segmentation": [
        [
            "OK, I'm going to talk about learning coverage functions and applications to private data release."
        ],
        [
            "Define coverage function.",
            "I'll be more descriptive so that the slides are not that important.",
            "Alright, so we think of functions which define onset of all subsets of 1st and natural numbers and take positive real values.",
            "So such a function is set to be a coverage function."
        ],
        [
            "If there is a ground set U."
        ],
        [
            "And a collection of subsets of you, say even a 2 N so."
        ],
        [
            "For every S, the subset of 1st N natural numbers, the value of F at."
        ],
        [
            "This is defined as the number of elements in the Union of AI so that I is an S."
        ],
        [
            "Equivalently, we can look at these functions as defined on the hypercube, minus one, one to the N. By associating every subset of 1st and natural numbers to the indicator vector in the natural way."
        ],
        [
            "For much of this talk we will be dealing with additive error approximations and therefore we will consider coverage functions that are normalized in range 201.",
            "I'll explicitly point out when we won't need this normalization."
        ],
        [
            "A more useful characterization of coverage functions for us would be."
        ],
        [
            "Writing them as non negative linear."
        ],
        [
            "Combinations of monotone disjunctions."
        ],
        [
            "And by our normalization that we discussed before, the sum of these nonnegative coefficients is bounded above by 1."
        ],
        [
            "Alright, so why do we care about coverage functions?",
            "First of all, they form an important subclass of submodular functions which are a topic of intense research.",
            "In the past few years in machine learning community.",
            "Of perhaps more directly, they appear in algorithmic game theory and economix, where they are studied as valuation functions are used to model utility functions and prices.",
            "An more naturally they appear in differential privacy in the problem of private data release and this part I'll discuss in more detail in this talk."
        ],
        [
            "Alright, so the main point of investigation in this work is the learnability of coverage functions from random examples.",
            "And we will also see applications to private data release problems as a as an application of learning algorithms."
        ],
        [
            "So let me first define the first model of interest to us.",
            "This is the pack model of learning with L1 error.",
            "This is just a generalization of the standard pack learning to learning real valued function classes.",
            "So here the learner gets random label examples from a target coverage functions F and the job of the learner is to return a hypothesis, say edge, so that the error of edge, which is the L1 error of edge with respect to F, is bounded above by epsilon.",
            "The expectation here is with respect to the distribution on the examples that you fixed."
        ],
        [
            "So in this model, we show that if you work under the uniform distribution, then coverage functions are learnable in time, linear in N and polynomial in one over epsilon.",
            "An users examples that are logarithmic in N and polynomial in one over epsilon.",
            "It is.",
            "It is nice to compare this to the previous work on learning submodular functions, which is a more general class.",
            "And a number of works, which perhaps is not clear to you at the back.",
            "A number of works have considered this problem of learning submodular functions, and the best time bound in this setting is 2 to the one over epsilon squared times Poly, and an in fact one can show that this exponential dependence on one over epsilon.",
            "There're parameter is necessary and cannot be removed for the general case of submodular functions.",
            "That"
        ],
        [
            "This model of interest to us is the probably mostly approximately correct model defined by Balkaran Harvey in 2011.",
            "In this model, the learner gets to see again random label examples from a target coverage function and since we will see that it's OK to think of non negative functions here, we don't need the normalization.",
            "So the learner, the job of the learner is to return the hypothesis edge which multiplicatively approximates the target coverage function F on all but a Delta fraction of the probability mass.",
            "So you need to approximate age much approximate F within a factor of 1 plus gamma on all but a Delta probability mass of the domain.",
            "So since we deal with multiplicative error, this is a more challenging model of learning than the standard pack model of learning I described earlier."
        ],
        [
            "And in this model again, we show that if you fix a distribution to the uniform distribution on the hypercube, then coverage functions are learnable in time.",
            "Linear in an polynomial in one over epsilon.",
            "An samples loggers McKinnon and polynomial in or epsilon.",
            "Again, one can compare this to previous work on learning submodular functions in this stronger pimc model of learning and the 1st result in this direction was again by Balkan hardware in 2011, who showed that if one can obtain in polynomial time an algorithm that returns is square root N factor approximation to the target coverage for target submodular function.",
            "More recently, working on the uniform distribution, Feldman and Vondrak showed that one can obtain an algorithm that runs in time 2 to the one over Delta Gamma squared and polynomial in N. Again, this exponential dependence is necessary.",
            "So one can observe that our algorithm for learning coverage functions that the Mac model is the first fully polynomial algorithm for learning in the pimc challenging pimc model of learning.",
            "Oh yes, there's a distribution independent.",
            "Yeah, so the film and one result is the uniform distribution.",
            "While that is distribution independent."
        ],
        [
            "Next, we consider the model of learning with adversarial letter.",
            "So that is the agnostic model of learning.",
            "Again, generalized to learning for real valued function classes.",
            "And here again learner gets random label examples, But this time the labels come from an arbitrary function F instead of a target coverage function and the job the learner is to construct a hypothesis edge so that the error of edge with respect to F is at most OP plus epsilon."
        ],
        [
            "The top is the error of the best fitting coverage function.",
            "So since F is not guaranteed to be covered itself, we we have this relaxed goal of making it off at most hopeless epsilon."
        ],
        [
            "In this model, we show that if you fix a distribution to be any product distribution, or more generally any symmetric distribution, then coverage functions are learnable in time and samples and to the of log one over epsilon.",
            "So let me remind you what these symmetric distributions are.",
            "These are distributions."
        ],
        [
            "Find on the hypercube such that the PDF of this distribution is a symmetric function that is, at any X the value of the PDF is a function of just the Hamming weight of X.",
            "It can be shown that this time bound of enter the of log on where epsilon is actually tight.",
            "Even in the case of uniform distribution for agnostic learning, and this follows just from a reduction, a simple reduction from learning sparse parities with noise."
        ],
        [
            "Since all our algorithms are based on distribution assumptions, it is perhaps instructive node that these distribution assumptions are actually necessary.",
            "Indeed, if you actually have a fully polynomial algorithm that is polynomial in N and one over epsilon time algorithm to even pack learn coverage functions.",
            "Then you obtain a polynomial in N and one or epsilon time algorithm 2 pack.",
            "Learn disjoint DNS.",
            "So this."
        ],
        [
            "Join DNS are a important class and well said it class in learning theory and the best known algorithm runs in time end to the end to the 1/3 due to a result by Clive Answer Video in 2001."
        ],
        [
            "So next I'll describe our applications to private data in less problems.",
            "So let me start with some background for our purpose, the database D would be just a subset of the hypercube minus one to the N, and we will."
        ],
        [
            "Interested in releasing counting queries on the database so counting query is just a Boolean function.",
            "That is, it takes any point in minus one to the N and Maps into a Boolean 01 value.",
            "And the job so and so on are on a query queue on the database."
        ],
        [
            "D. The answer is supposed to be the fraction of elements of D that satisfy the query function Q.",
            "So."
        ],
        [
            "What is our goal here?",
            "Well, we are given a query class, so some interesting query class and we are we are required to answer queries accurately from the query class and at the same time maintain the privacy of the participants of the database D."
        ],
        [
            "So what do we mean by privacy when we use this formulation of cryptographic notion of privacy defined by the work at all called as differential privacy, and the actual definition is not very important for the purpose of this talk.",
            "Roughly, one could think of this as an algorithm which accesses the database is differentially private.",
            "If on databases that differing just one element, the behavior of the algorithm is not too different.",
            "Alright."
        ],
        [
            "So with this we can state our goal more clearly.",
            "Now we are interested in releasing the class of marginal queries and bimodule queries.",
            "I mean the set of all query functions Q which are monotone conjunctions."
        ],
        [
            "So this is a simple and natural query class and in fact."
        ],
        [
            "Actually, part of many reported databases such that US sensors and Department of Labor and IRS Statistiques."
        ],
        [
            "And is also very well studied in the differential privacy literature and number of works have focused on private data release problems on marginal really marginal problem of marginal queries."
        ],
        [
            "So that's our goal is to construct a differentially private algorithm that takes an input as the database D and construct a private summary such that using their summary we can answer all monotone conjunction counting queries within a bounded error of, say, beta."
        ],
        [
            "So if we assume that.",
            "Other error beta is worst case.",
            "That is the error beta is required to be small on all modern conjunction counting queries.",
            "Then it turns out that the best on algorithm runs in time and to the square root, and.",
            "So do this, and since this is like farming efficient people, relax this problem."
        ],
        [
            "Website all considered the problem of releasing marginal queries with low average error.",
            "So instead of worst case error they consider the proper using the queries with average low average error."
        ],
        [
            "And a number of follow-up works also focused on this relaxed model of private queries."
        ],
        [
            "And in this."
        ],
        [
            "Model book title showed that if Alpha Bar is the desired level of average error that you require, then one can give an algorithm that runs in time and to the of one over Alpha squared."
        ],
        [
            "Visual we improve on this result and we show that our pack learning algorithm actually implies a polynomial in N1 over Alpha and one White Kappa algorithm for releasing monotone conjunction counting queries with low average error.",
            "So this is a fully polynomial time algorithm for the same problem."
        ],
        [
            "So in many applications, on the other hand, we are not interested in releasing all possible modern conjunction counting queries, and in fact we just want to release a short marginals so just fixed length marginals of length K. This."
        ],
        [
            "Problem is called as the problem of using K marginals.",
            "And again, if you desire that you release all, gave a marginals with low worst case error then one cannot do better than some enter the square root K."
        ],
        [
            "So we relax the problem again and think of the setting where we want to release all length K monitor in conjunction counting queries with low average error.",
            "We know that the previous result that I talked about where we were producing low average error overall conjunctions will not give us anything in this setting because the error on a conjunctions of length fixed length may be arbitrarily high, even though the total average is controlled."
        ],
        [
            "So in this setting, we show that our algorithm for agnostic learning on symmetric distributions implies a private queries algorithm for K marginals with average error Alpha bar that runs in time enter the order log of one over Alpha bar."
        ],
        [
            "So let me briefly explain, this connection between learning algorithms and private queries.",
            "So say you want to release queries from a Class C. So for a fixed database D you can think of a function that Maps this query Class C into a number between zero and one.",
            "Basically, the answer is for the queries.",
            "So each query is mapped to an answer.",
            "You"
        ],
        [
            "Then take the collection of all such accounting query functions, one for each database, and construct what is known as accounting.",
            "Very class corresponding to a query function set C."
        ],
        [
            "It turns out that if you select the Class C as the class of modern conjunctions, then the query counting query class turns out to be the class of coverage functions.",
            "Now this observation has been used in private data.",
            "In these problems implicitly before we make this connection explicit, and by a simple reduction, we know that learning on the distribution, the query class QFC implies private release of marginals with low average error where the average is taken with respect or distribution on the Class C. So that's the basis for our learning to private data release algorithms."
        ],
        [
            "In the remaining part of the talk, Let Me explain to you some ideas behind our back end team at learning algorithms."
        ],
        [
            "So our pack learning algorithm on a uniform distribution is based on uncovering simple properties of the full spectrum of coverage functions."
        ],
        [
            "So we observe first of all that the Fourier spectrum of coverage functions has this morning city property, which means that if you collect in disguise P&B, so that is completely included inside B in the foyer, coefficient of FT is larger in magnitude than the four coefficient of F at V."
        ],
        [
            "So why is this useful?",
            "Well, in general, given random examples of function, it is hard to find the large four coefficients, but one can use this modality observation to find a large for your coefficients.",
            "Just some random examples efficiently."
        ],
        [
            "Secondly, we observe that the L1 spectral norm of coverage functions is bounded about just a constant 2."
        ],
        [
            "So let me remind you whatever spectral Norm is it is the sum of absolute values of the Fourier coefficients of any function.",
            "What does this have to do with this algorithm?",
            "Well, it tells."
        ],
        [
            "Is that the number of large for your coefficients cannot be too large, too many, they're going to be too many large for coefficients.",
            "Combined together, we can obtain the not too many large four coefficients of the coverage function, and by standard techniques we can show that if we return the linear combination of this large four coefficients, there is a good approximation to the unknown coverage function."
        ],
        [
            "Next, we show that coverage functions have are efficiently approximated by hunters.",
            "What this means is that if you take a coverage function C, then there's another coverage function, say C prime, which depends.",
            "First of all only on one or epsilon squared variables.",
            "An epsilon approximates the target, correct function C."
        ],
        [
            "This observation helps in actually reducing the sample complexity of our algorithms.",
            "In particular, it implies attribute efficient algorithms for pack learning."
        ],
        [
            "So as a result, we obtained our pack learning algorithm that I described to you earlier."
        ],
        [
            "Next, let me describe some ideas behind about Pymatuning algorithm.",
            "Now the high level plan of us is to reduce the Mac running 2PAC learning, but then P Mac learning has a stronger notion of multiplicative error while pack learn English with additive error.",
            "So there is not.",
            "There is no direct correspondence between their message.",
            "The main simple idea is that if the values of the target coverage function are large, then additive error will imply multiplicative error.",
            "As such, this may not be true.",
            "So what we do is we did."
        ],
        [
            "Oppose the target coverage function into various regions so."
        ],
        [
            "But in each region, the value of the coverage function in that region is large compared to the maximum of the coverage function in that region."
        ],
        [
            "Specifically speaking, what we prove is that if we have a coverage function C, then either on one minus Delta by two fraction of the domain C takes a value, which is someone by four times the maximum.",
            "Or there is an index I such that if you take the set of all X is such that XI is minus one, then F is large and all such access.",
            "It is very easy to see that one can use this observation to do what I described in the previous slide, because if you are in the first setting where the values of C are large and all but the Delta by two fraction of the domain, then you're already done because you can do your PAC learning algorithm here.",
            "An additive error will imply multiplicative error.",
            "Otherwise you know that on half the XSF is large and you can use your pack learning algorithm on those half access and recurse on the other half.",
            "So to prove."
        ],
        [
            "Is here and we used to strongly two well known and useful results, the first being the concentration of Lipschitz submodular functions, proved by Boucheron Lugosi and massage in 2000.",
            "And we again use the observation of Phi Gay from 2006, where he shows that for every monotone submodular function F the average of F is at least 1/4 of its maximum."
        ],
        [
            "Alright, as a result, we obtain that.",
            "As a result, we obtain our P Max learning algorithm, that is Skype to you earlier."
        ],
        [
            "So let me summarize our contributions.",
            "Before show that there is a fully polynomial pack and pimc learning algorithm for coverage functions on the uniform distribution.",
            "Be sure that on product and symmetric distributions there's an agnostic learning algorithm for coverage functions that runs in time and to the of log one epsilon.",
            "And be sure that distribution assumptions are actually necessary in that on arbitrary distributions one can reduce the problem of learning disjoint DNF's to the problem of learning coverage functions.",
            "As an application, we give faster private data in these algorithms for releasing marginal counting queries."
        ],
        [
            "Learning on more general distributions remains an open problem."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to talk about learning coverage functions and applications to private data release.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Define coverage function.",
                    "label": 0
                },
                {
                    "sent": "I'll be more descriptive so that the slides are not that important.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we think of functions which define onset of all subsets of 1st and natural numbers and take positive real values.",
                    "label": 0
                },
                {
                    "sent": "So such a function is set to be a coverage function.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If there is a ground set U.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a collection of subsets of you, say even a 2 N so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For every S, the subset of 1st N natural numbers, the value of F at.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is defined as the number of elements in the Union of AI so that I is an S.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equivalently, we can look at these functions as defined on the hypercube, minus one, one to the N. By associating every subset of 1st and natural numbers to the indicator vector in the natural way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For much of this talk we will be dealing with additive error approximations and therefore we will consider coverage functions that are normalized in range 201.",
                    "label": 0
                },
                {
                    "sent": "I'll explicitly point out when we won't need this normalization.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A more useful characterization of coverage functions for us would be.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Writing them as non negative linear.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combinations of monotone disjunctions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by our normalization that we discussed before, the sum of these nonnegative coefficients is bounded above by 1.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so why do we care about coverage functions?",
                    "label": 0
                },
                {
                    "sent": "First of all, they form an important subclass of submodular functions which are a topic of intense research.",
                    "label": 0
                },
                {
                    "sent": "In the past few years in machine learning community.",
                    "label": 0
                },
                {
                    "sent": "Of perhaps more directly, they appear in algorithmic game theory and economix, where they are studied as valuation functions are used to model utility functions and prices.",
                    "label": 0
                },
                {
                    "sent": "An more naturally they appear in differential privacy in the problem of private data release and this part I'll discuss in more detail in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the main point of investigation in this work is the learnability of coverage functions from random examples.",
                    "label": 0
                },
                {
                    "sent": "And we will also see applications to private data release problems as a as an application of learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me first define the first model of interest to us.",
                    "label": 0
                },
                {
                    "sent": "This is the pack model of learning with L1 error.",
                    "label": 0
                },
                {
                    "sent": "This is just a generalization of the standard pack learning to learning real valued function classes.",
                    "label": 0
                },
                {
                    "sent": "So here the learner gets random label examples from a target coverage functions F and the job of the learner is to return a hypothesis, say edge, so that the error of edge, which is the L1 error of edge with respect to F, is bounded above by epsilon.",
                    "label": 0
                },
                {
                    "sent": "The expectation here is with respect to the distribution on the examples that you fixed.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this model, we show that if you work under the uniform distribution, then coverage functions are learnable in time, linear in N and polynomial in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "An users examples that are logarithmic in N and polynomial in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "It is nice to compare this to the previous work on learning submodular functions, which is a more general class.",
                    "label": 0
                },
                {
                    "sent": "And a number of works, which perhaps is not clear to you at the back.",
                    "label": 0
                },
                {
                    "sent": "A number of works have considered this problem of learning submodular functions, and the best time bound in this setting is 2 to the one over epsilon squared times Poly, and an in fact one can show that this exponential dependence on one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "There're parameter is necessary and cannot be removed for the general case of submodular functions.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model of interest to us is the probably mostly approximately correct model defined by Balkaran Harvey in 2011.",
                    "label": 0
                },
                {
                    "sent": "In this model, the learner gets to see again random label examples from a target coverage function and since we will see that it's OK to think of non negative functions here, we don't need the normalization.",
                    "label": 0
                },
                {
                    "sent": "So the learner, the job of the learner is to return the hypothesis edge which multiplicatively approximates the target coverage function F on all but a Delta fraction of the probability mass.",
                    "label": 0
                },
                {
                    "sent": "So you need to approximate age much approximate F within a factor of 1 plus gamma on all but a Delta probability mass of the domain.",
                    "label": 0
                },
                {
                    "sent": "So since we deal with multiplicative error, this is a more challenging model of learning than the standard pack model of learning I described earlier.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this model again, we show that if you fix a distribution to the uniform distribution on the hypercube, then coverage functions are learnable in time.",
                    "label": 0
                },
                {
                    "sent": "Linear in an polynomial in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "An samples loggers McKinnon and polynomial in or epsilon.",
                    "label": 0
                },
                {
                    "sent": "Again, one can compare this to previous work on learning submodular functions in this stronger pimc model of learning and the 1st result in this direction was again by Balkan hardware in 2011, who showed that if one can obtain in polynomial time an algorithm that returns is square root N factor approximation to the target coverage for target submodular function.",
                    "label": 0
                },
                {
                    "sent": "More recently, working on the uniform distribution, Feldman and Vondrak showed that one can obtain an algorithm that runs in time 2 to the one over Delta Gamma squared and polynomial in N. Again, this exponential dependence is necessary.",
                    "label": 0
                },
                {
                    "sent": "So one can observe that our algorithm for learning coverage functions that the Mac model is the first fully polynomial algorithm for learning in the pimc challenging pimc model of learning.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, there's a distribution independent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the film and one result is the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "While that is distribution independent.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we consider the model of learning with adversarial letter.",
                    "label": 0
                },
                {
                    "sent": "So that is the agnostic model of learning.",
                    "label": 0
                },
                {
                    "sent": "Again, generalized to learning for real valued function classes.",
                    "label": 0
                },
                {
                    "sent": "And here again learner gets random label examples, But this time the labels come from an arbitrary function F instead of a target coverage function and the job the learner is to construct a hypothesis edge so that the error of edge with respect to F is at most OP plus epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The top is the error of the best fitting coverage function.",
                    "label": 0
                },
                {
                    "sent": "So since F is not guaranteed to be covered itself, we we have this relaxed goal of making it off at most hopeless epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this model, we show that if you fix a distribution to be any product distribution, or more generally any symmetric distribution, then coverage functions are learnable in time and samples and to the of log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So let me remind you what these symmetric distributions are.",
                    "label": 0
                },
                {
                    "sent": "These are distributions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find on the hypercube such that the PDF of this distribution is a symmetric function that is, at any X the value of the PDF is a function of just the Hamming weight of X.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that this time bound of enter the of log on where epsilon is actually tight.",
                    "label": 0
                },
                {
                    "sent": "Even in the case of uniform distribution for agnostic learning, and this follows just from a reduction, a simple reduction from learning sparse parities with noise.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since all our algorithms are based on distribution assumptions, it is perhaps instructive node that these distribution assumptions are actually necessary.",
                    "label": 0
                },
                {
                    "sent": "Indeed, if you actually have a fully polynomial algorithm that is polynomial in N and one over epsilon time algorithm to even pack learn coverage functions.",
                    "label": 0
                },
                {
                    "sent": "Then you obtain a polynomial in N and one or epsilon time algorithm 2 pack.",
                    "label": 0
                },
                {
                    "sent": "Learn disjoint DNS.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Join DNS are a important class and well said it class in learning theory and the best known algorithm runs in time end to the end to the 1/3 due to a result by Clive Answer Video in 2001.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next I'll describe our applications to private data in less problems.",
                    "label": 0
                },
                {
                    "sent": "So let me start with some background for our purpose, the database D would be just a subset of the hypercube minus one to the N, and we will.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interested in releasing counting queries on the database so counting query is just a Boolean function.",
                    "label": 0
                },
                {
                    "sent": "That is, it takes any point in minus one to the N and Maps into a Boolean 01 value.",
                    "label": 0
                },
                {
                    "sent": "And the job so and so on are on a query queue on the database.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "D. The answer is supposed to be the fraction of elements of D that satisfy the query function Q.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is our goal here?",
                    "label": 0
                },
                {
                    "sent": "Well, we are given a query class, so some interesting query class and we are we are required to answer queries accurately from the query class and at the same time maintain the privacy of the participants of the database D.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we mean by privacy when we use this formulation of cryptographic notion of privacy defined by the work at all called as differential privacy, and the actual definition is not very important for the purpose of this talk.",
                    "label": 0
                },
                {
                    "sent": "Roughly, one could think of this as an algorithm which accesses the database is differentially private.",
                    "label": 0
                },
                {
                    "sent": "If on databases that differing just one element, the behavior of the algorithm is not too different.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with this we can state our goal more clearly.",
                    "label": 0
                },
                {
                    "sent": "Now we are interested in releasing the class of marginal queries and bimodule queries.",
                    "label": 0
                },
                {
                    "sent": "I mean the set of all query functions Q which are monotone conjunctions.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a simple and natural query class and in fact.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, part of many reported databases such that US sensors and Department of Labor and IRS Statistiques.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And is also very well studied in the differential privacy literature and number of works have focused on private data release problems on marginal really marginal problem of marginal queries.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's our goal is to construct a differentially private algorithm that takes an input as the database D and construct a private summary such that using their summary we can answer all monotone conjunction counting queries within a bounded error of, say, beta.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we assume that.",
                    "label": 0
                },
                {
                    "sent": "Other error beta is worst case.",
                    "label": 0
                },
                {
                    "sent": "That is the error beta is required to be small on all modern conjunction counting queries.",
                    "label": 1
                },
                {
                    "sent": "Then it turns out that the best on algorithm runs in time and to the square root, and.",
                    "label": 0
                },
                {
                    "sent": "So do this, and since this is like farming efficient people, relax this problem.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Website all considered the problem of releasing marginal queries with low average error.",
                    "label": 0
                },
                {
                    "sent": "So instead of worst case error they consider the proper using the queries with average low average error.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a number of follow-up works also focused on this relaxed model of private queries.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model book title showed that if Alpha Bar is the desired level of average error that you require, then one can give an algorithm that runs in time and to the of one over Alpha squared.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visual we improve on this result and we show that our pack learning algorithm actually implies a polynomial in N1 over Alpha and one White Kappa algorithm for releasing monotone conjunction counting queries with low average error.",
                    "label": 0
                },
                {
                    "sent": "So this is a fully polynomial time algorithm for the same problem.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in many applications, on the other hand, we are not interested in releasing all possible modern conjunction counting queries, and in fact we just want to release a short marginals so just fixed length marginals of length K. This.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is called as the problem of using K marginals.",
                    "label": 0
                },
                {
                    "sent": "And again, if you desire that you release all, gave a marginals with low worst case error then one cannot do better than some enter the square root K.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we relax the problem again and think of the setting where we want to release all length K monitor in conjunction counting queries with low average error.",
                    "label": 0
                },
                {
                    "sent": "We know that the previous result that I talked about where we were producing low average error overall conjunctions will not give us anything in this setting because the error on a conjunctions of length fixed length may be arbitrarily high, even though the total average is controlled.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this setting, we show that our algorithm for agnostic learning on symmetric distributions implies a private queries algorithm for K marginals with average error Alpha bar that runs in time enter the order log of one over Alpha bar.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me briefly explain, this connection between learning algorithms and private queries.",
                    "label": 0
                },
                {
                    "sent": "So say you want to release queries from a Class C. So for a fixed database D you can think of a function that Maps this query Class C into a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Basically, the answer is for the queries.",
                    "label": 0
                },
                {
                    "sent": "So each query is mapped to an answer.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then take the collection of all such accounting query functions, one for each database, and construct what is known as accounting.",
                    "label": 0
                },
                {
                    "sent": "Very class corresponding to a query function set C.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that if you select the Class C as the class of modern conjunctions, then the query counting query class turns out to be the class of coverage functions.",
                    "label": 0
                },
                {
                    "sent": "Now this observation has been used in private data.",
                    "label": 0
                },
                {
                    "sent": "In these problems implicitly before we make this connection explicit, and by a simple reduction, we know that learning on the distribution, the query class QFC implies private release of marginals with low average error where the average is taken with respect or distribution on the Class C. So that's the basis for our learning to private data release algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the remaining part of the talk, Let Me explain to you some ideas behind our back end team at learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our pack learning algorithm on a uniform distribution is based on uncovering simple properties of the full spectrum of coverage functions.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we observe first of all that the Fourier spectrum of coverage functions has this morning city property, which means that if you collect in disguise P&B, so that is completely included inside B in the foyer, coefficient of FT is larger in magnitude than the four coefficient of F at V.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Well, in general, given random examples of function, it is hard to find the large four coefficients, but one can use this modality observation to find a large for your coefficients.",
                    "label": 0
                },
                {
                    "sent": "Just some random examples efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secondly, we observe that the L1 spectral norm of coverage functions is bounded about just a constant 2.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me remind you whatever spectral Norm is it is the sum of absolute values of the Fourier coefficients of any function.",
                    "label": 1
                },
                {
                    "sent": "What does this have to do with this algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, it tells.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the number of large for your coefficients cannot be too large, too many, they're going to be too many large for coefficients.",
                    "label": 0
                },
                {
                    "sent": "Combined together, we can obtain the not too many large four coefficients of the coverage function, and by standard techniques we can show that if we return the linear combination of this large four coefficients, there is a good approximation to the unknown coverage function.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we show that coverage functions have are efficiently approximated by hunters.",
                    "label": 0
                },
                {
                    "sent": "What this means is that if you take a coverage function C, then there's another coverage function, say C prime, which depends.",
                    "label": 0
                },
                {
                    "sent": "First of all only on one or epsilon squared variables.",
                    "label": 0
                },
                {
                    "sent": "An epsilon approximates the target, correct function C.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This observation helps in actually reducing the sample complexity of our algorithms.",
                    "label": 0
                },
                {
                    "sent": "In particular, it implies attribute efficient algorithms for pack learning.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a result, we obtained our pack learning algorithm that I described to you earlier.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, let me describe some ideas behind about Pymatuning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the high level plan of us is to reduce the Mac running 2PAC learning, but then P Mac learning has a stronger notion of multiplicative error while pack learn English with additive error.",
                    "label": 0
                },
                {
                    "sent": "So there is not.",
                    "label": 0
                },
                {
                    "sent": "There is no direct correspondence between their message.",
                    "label": 0
                },
                {
                    "sent": "The main simple idea is that if the values of the target coverage function are large, then additive error will imply multiplicative error.",
                    "label": 0
                },
                {
                    "sent": "As such, this may not be true.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we did.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oppose the target coverage function into various regions so.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in each region, the value of the coverage function in that region is large compared to the maximum of the coverage function in that region.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Specifically speaking, what we prove is that if we have a coverage function C, then either on one minus Delta by two fraction of the domain C takes a value, which is someone by four times the maximum.",
                    "label": 0
                },
                {
                    "sent": "Or there is an index I such that if you take the set of all X is such that XI is minus one, then F is large and all such access.",
                    "label": 0
                },
                {
                    "sent": "It is very easy to see that one can use this observation to do what I described in the previous slide, because if you are in the first setting where the values of C are large and all but the Delta by two fraction of the domain, then you're already done because you can do your PAC learning algorithm here.",
                    "label": 0
                },
                {
                    "sent": "An additive error will imply multiplicative error.",
                    "label": 1
                },
                {
                    "sent": "Otherwise you know that on half the XSF is large and you can use your pack learning algorithm on those half access and recurse on the other half.",
                    "label": 0
                },
                {
                    "sent": "So to prove.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is here and we used to strongly two well known and useful results, the first being the concentration of Lipschitz submodular functions, proved by Boucheron Lugosi and massage in 2000.",
                    "label": 0
                },
                {
                    "sent": "And we again use the observation of Phi Gay from 2006, where he shows that for every monotone submodular function F the average of F is at least 1/4 of its maximum.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, as a result, we obtain that.",
                    "label": 0
                },
                {
                    "sent": "As a result, we obtain our P Max learning algorithm, that is Skype to you earlier.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me summarize our contributions.",
                    "label": 0
                },
                {
                    "sent": "Before show that there is a fully polynomial pack and pimc learning algorithm for coverage functions on the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "Be sure that on product and symmetric distributions there's an agnostic learning algorithm for coverage functions that runs in time and to the of log one epsilon.",
                    "label": 0
                },
                {
                    "sent": "And be sure that distribution assumptions are actually necessary in that on arbitrary distributions one can reduce the problem of learning disjoint DNF's to the problem of learning coverage functions.",
                    "label": 0
                },
                {
                    "sent": "As an application, we give faster private data in these algorithms for releasing marginal counting queries.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning on more general distributions remains an open problem.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}