{
    "id": "hf54556wnz232hroc77ogwyszhfdnmtt",
    "title": "On the Usefulness of Similarity based Projection Spaces for Transfer Learning",
    "info": {
        "author": [
            "Emilie Morvant, Laboratoire d\u2019Informatique Fondamentale de Marseille"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Pattern Recognition",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/simbad2011_morvant_transfer/",
    "segmentation": [
        [
            "So."
        ],
        [
            "Prison first.",
            "The motivation behind this work super ever.",
            "Binary classification task consisting in labeling images as there is a person or there is not a person.",
            "For this task we have some labeled images extracted from a particular corpus, for example, local producer of the web, usually in supervised classification.",
            "We are able to learn a low error classifier.",
            "An image is extracted from the same corpus.",
            "In other words, the test distribution and the training distribution are the same.",
            "However, in domain adaptation this true distribution are different.",
            "For example, if the test images are extracted from videos, then the quality of the classifier of the model is normal, guaranteed."
        ],
        [
            "So first I would present the theory of domain adaptation.",
            "Then I will present the theory of learning with good similarity functions and its associated explicit projection space.",
            "Then I would present our contribution consisting in modifying the projection space for domain adaptation.",
            "After I will present some experimental results and before conclude I will present an extension of this work with some theoretical aspects for."
        ],
        [
            "Amateur.",
            "So first what is?"
        ],
        [
            "I'm an adaptation.",
            "Like in supervised classification, we have an input space and the level sets.",
            "Here we consider only binary problem.",
            "One key difference in domain adaptation is that we have two different distribution.",
            "The first one is the source domain and can be seen as the training distribution.",
            "The second one is the target domain and can be seen as the test distribution.",
            "We also can define the error of any parties on the two domains.",
            "We can define the source domain error.",
            "And the target domain error as the expectation of the hypothesis to commit an error on the considered domain.",
            "And in fact, in domain adaptation we want to infer a model with a low target domain error."
        ],
        [
            "We consider the case where we are only liable for the source domain for the target domain.",
            "We have no information about the level and the main issue.",
            "The main issue is then if we run.",
            "If the classifier is learn on the source domain, how does it perform on the target domain?",
            "To answer to this question where the following intuition here, here we have our two domains.",
            "The source running ahead in red and the target one in blue.",
            "With the rebels to see intuition and the first figure, if we run a low error classifier on the red data."
        ],
        [
            "We clearly see that it commits a lot of error on the blue data."
        ],
        [
            "And the second figure where we can remark that the two domains are very close, then running a low error classifier on the red data is equivalent to obtain a low error classifier on the blue data with this info."
        ],
        [
            "Michelle.",
            "Band aid and associate a proven the following theorem.",
            "Here we have a bound over the error that we want to minimize a bundle where the target domain error.",
            "This band is constituted by the summer free terms."
        ],
        [
            "The first one is the classic error in supervised classification.",
            "In fact, it's the error under source domain."
        ],
        [
            "The second term is the distance between the two domains, but without the level, it's the distance between the two marginal distribution of the two domains.",
            "And intuitively, this distance can be seen as follow.",
            "Again, are two domains the source one inward, the target, one in blue without the level here and the low distance is equivalent to be notable to separate the two during the two distributions?"
        ],
        [
            "The last term is the error or what is called a joint optimal classifier.",
            "In fact, it's the best error that we can expect to obtain under under two domains at the same time, and this value can be seen as a measure of adaptation ability in the current space.",
            "So for doing domain adaptation, we suppose that this value that is value is low."
        ],
        [
            "So finally, for domain adaptation we want to minimize this bound for building a new projection space where the two domains tend to be closed.",
            "And we propose to use classifier based on good similarity function that I press."
        ],
        [
            "Don't know."
        ],
        [
            "Here we are in the theory proposed by Bellco, where a similarity function is any pairwise function that compares two instances and returns value between minus one and one.",
            "And we say that the frame similarity function is good for binary classification problem.",
            "If it verifies these two points.",
            "Intuitively we have a set of reasonable points denoted by her.",
            "These points are kind of super points and we want that for a lot of examples.",
            "There are on average more similar to reasonable points to the same class than the reasonable points to the opposite class."
        ],
        [
            "Given the set of reasonable points, we can define an explicit projection space where an instance is represented by its vector of similarities to each reasonable points, and it's in this space that we can learn a linear linear classifier with good generalization guarantees.",
            "The last important thing is that this notion of good similarity function generalizes the notion of kernel in the sense that.",
            "A kernel is a good similarity function, but a good similarity function, maybe not symmetric and not positive semidefinite, and in fact it's this property that we want to use to modify the explicit projection space for domain adaptation."
        ],
        [
            "So our first contribution, consisting in modifying this space app, really deep."
        ],
        [
            "I recall first that for domain adaptation task we want to be performing under target domain.",
            "So we propose to insert some kind of target information in the similarity function.",
            "For this we normalize the similarities curls too according to the source sample and the target sample in order to our viziru mean and the unit variance for each reasonable points.",
            "And we see in experimental results that this kind of normalization can improve performance is for the others to ask others doing."
        ],
        [
            "Station desk.",
            "Our second contribution consists in recognizing the construction of the projection space during the learning process."
        ],
        [
            "Air will start from the the domain adaptation bound proposed by Bendavid.",
            "We want to minimize this bound."
        ],
        [
            "And for minimizing the first term of the bond for minimizing the the source domain error, we simply use the linear program proposed by Bellco for learning with good similarity function.",
            "Here we can remark that we don't know approving the set or food event reasonable points, so we take a set of inis, another potential with the number points, an serving.",
            "This program is equivalent to learn a classifier and to select the relevant reasonable point by Astro City into them and underweight.",
            "Alpha G."
        ],
        [
            "The second term of the bond is the distance between the marginal distribution of the two domains are recalled that allow distance is equivalent to be not able to separate the two domain.",
            "The two distribution.",
            "So we take a set a set of Pearl which associate to a source point, a target point, and we want to build a new projection space for you, where two points for a pair tend to be not separable.",
            "In other words, we want that the divergent between the return value by the epitasis tend to be low, and in fact this divergents can be bound by this term."
        ],
        [
            "And this term can be seen as a distance in your new projection space.",
            "And this projection space is defined by the writing of the similarity function by their long wait Alpha G. And we can do this because of the non PSD and Unsimilar earned and symmetric requirement for good similarity function."
        ],
        [
            "And finally we take this red term has a new regularization term to add in the problem proposed by Bellco for the similarity."
        ],
        [
            "Function and we obtain this global optimization problem on the first line, we can recognize the problem proposed by Barco for learning with good similarity function and on the second line we can recognize organization term for each of our per set."
        ],
        [
            "Then the last question, and is how can we validate the parameters, the writing and the selection of the pairs?"
        ],
        [
            "Here we have no information about the the label on the target domain, so we cannot do a cross validation process, so we propose to use a kind of reverse validation defined by robust classifier age and this process."
        ],
        [
            "Is the following error again our two domains, their source running red with ladders, the target one in route without level?"
        ],
        [
            "We don't.",
            "Our classifier with the regularization term."
        ],
        [
            "Then we only focus on the target sample."
        ],
        [
            "However, this sample with our classifier we obtain Absurdo Holabird, a target sample and with this adorable targets, and."
        ],
        [
            "We learn the Rovers classifier here without domain adaptation."
        ],
        [
            "And we can evaluate this classifier this robust classifier on the source domain, with the idea that if two domains are related, then the robust classifier performs well on the source domain."
        ],
        [
            "So now I will present some experimental results."
        ],
        [
            "Here we consider two different similarity function.",
            "The first one is the classical Gaussian kernel, the second one is it's normalization.",
            "We want to compare the performances of the two are similarity function with and without the organization term.",
            "In other words, with and without domain adaptation.",
            "We consider two different problem.",
            "The first one is a toy program called intertwining mounts, where one moon correspond to one class and we consider one source domain an A different target domain according to 8 rotation, eight different rotation angles.",
            "Our second program is a real image annotation task.",
            "It will take as a source domain, some images extracted from particular copious called Pascal VOC 2007, and we consider us target domain.",
            "Some images extracted from videos and these videos are coming from the Corpus Nine 2007."
        ],
        [
            "Here we can observe the 1st result for the type problem.",
            "We can see the correct classification percentage according to the target domain or rotation angle.",
            "The first thing is that the regularization term in red and green improve the results.",
            "The second thing is that the normalized similarity function in red improve performances from the artist ask.",
            "I want to say.",
            "Why so to try?"
        ],
        [
            "To translate this question, we propose to estimate the goodness of the similarity function on the target domain.",
            "So here we start from the definition and back on and our record that we want.",
            "I want that.",
            "For a lot of example, there are on average more similar to points of the same class than than the point of the opposite class, so we propose to evaluate the goodness of similarity function as the proportion epsilon of example that not verified this property according to emerging Gala.",
            "An assimilative function is better if this proportion is lower.",
            "And here we can observe in red the normalized the similarity function and in blue the Gaussian kernel for the freezers task we clearly see that the true true similarity function are very similar, and for the five orders task, the red similarity to normalize one is better.",
            "So it confirms the previous result."
        ],
        [
            "Air, it's the results for the really major adaptation task.",
            "Here again, the first thing is that the regularization term improved the results, and here in fact we are difficult domain adaptation tasks.",
            "So the normalized similarity function is preferred."
        ],
        [
            "No, it's the end of the contribution of the published paper, and I now say some words about something which color aspect."
        ],
        [
            "For my chart, for the method with the regularization term, we can.",
            "The 1st results is an analysis of the sparsity of the infirm model.",
            "We can prove this lemma here.",
            "We have in fact abound for the optimal solution of our program, and who can say the following thing.",
            "That is, the sparsity of the models depends on the parameters anhana value BR and this value is in fact rated variety to the difference between coordinates in the projection space.",
            "And in fact, if the two domains are far, then this value tends to be I and it can imply an increase of the sparsity.",
            "In some experiments we have, we have seen that the gain ratio is between for the size for the model is between 2:00 and 2:00."
        ],
        [
            "The sudden results is about generalization abilities.",
            "For our method, we have in fact investigated the notion of algorithmic robustness proposed by Sue, and model had called 2010, where the idea is it's easier if a testing sample is similar to a training sample, then the testing error is close to the training error.",
            "I've heard this this notion is still a conjecture for them, an adaptation.",
            "Guys, despite this drawback, we can prove that her method is robust under source domain and with the help of the bond proposed by David for domain adaptation, we can derive the directly this bound order target error."
        ],
        [
            "So to conclude, we can say that good similarity function Alpers too to build around production space for domain adaptation and were proposed two way to to modify this space.",
            "The first one is on a pre renormalization of similarity function according to the target domain, and that future works.",
            "We aim to find a better way to design A similarity function for domain adaptation.",
            "Our second method is based on the addition of new regularization term from moving closer to two domains during the learning process and I just want to add that, well extended this work through an iterative approach and this approach can improve the performances and can lighten the search of different parameters so."
        ],
        [
            "Thank you.",
            "Fact here is that we.",
            "Symptomatic presentation than classical discriminatory because people tell us immediately positive function.",
            "So we cannot exactly say that we are closed this year in order.",
            "But one thing is that we restriction of the cauldron with double points so.",
            "Percentage points of some areas of decision during the space and in the meditation.",
            "We think that.",
            "We must select some points that are between the source and the target domain so they are against time to give an intuition can between the two domains to give time to define the new vision program is that it has to go to London to the target domain.",
            "Find an.",
            "Fun fact is that we have some demented thought for what we want to do is to find the difference in specs, and we mean that you want to find some subspace where the domains are similar, which replaces reduce with the position space.",
            "I mean in the dimension and weight, focusing anew subspace environments smart space where we can think so.",
            "To find this particular points while we projected security as well, in order to to get clothes on.",
            "So in another, exactly to be close or not close to this abortion.",
            "I want to find space where the source on target.",
            "I'm going to try to run the message I put his things so we tried to define the lyrics which placed back in this case, and the process faces the projection to the similarities cost on some points, which maybe not another segment.",
            "Only with this sort of thing.",
            "Thank you in practical applications.",
            "Customer is very useful or might be great.",
            "Very useful, but it's difficult to to understand automatically assumptions of this specific clustering technique like doors in practical problems.",
            "So stressing I would like to add some comments on on which assumptions do you.",
            "You assume in your method and.",
            "Because the problem is not just the expressions assumption in summer months away, but being able to express these in practical problems where maybe the language or the domain language is different and then ultimately straightforward feedback.",
            "So if you want to say something more about this problem.",
            "It's clearly depends on under the problem in the consumer crap.",
            "I'm OK and then we try to propose a general method, but the I think the main important thing is that this notion of per set and with this despair set, if we know this, this set appropriates it's seems to be easy to to do to make them an adaptation, because we.",
            "We know which point we want to to move closer and if we don't know this this set we have to choose this set between all of the points and it's clearly are intractable.",
            "That's why we have proposed an iterative approach that tends to auto corrector its selection, but it's very hard to control the this this program.",
            "So the main, I think the main important.",
            "People disease is that the last term of the bond proposed by Ben David, the hair of the joint optimal.",
            "A classifier is a.",
            "As to be a low for for doing a domain adaptation, because if it can be impossible to do for to do.",
            "Domain adaptation for some problem, I think.",
            "I think you have to.",
            "Let's give a handful."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prison first.",
                    "label": 0
                },
                {
                    "sent": "The motivation behind this work super ever.",
                    "label": 0
                },
                {
                    "sent": "Binary classification task consisting in labeling images as there is a person or there is not a person.",
                    "label": 1
                },
                {
                    "sent": "For this task we have some labeled images extracted from a particular corpus, for example, local producer of the web, usually in supervised classification.",
                    "label": 1
                },
                {
                    "sent": "We are able to learn a low error classifier.",
                    "label": 1
                },
                {
                    "sent": "An image is extracted from the same corpus.",
                    "label": 1
                },
                {
                    "sent": "In other words, the test distribution and the training distribution are the same.",
                    "label": 0
                },
                {
                    "sent": "However, in domain adaptation this true distribution are different.",
                    "label": 0
                },
                {
                    "sent": "For example, if the test images are extracted from videos, then the quality of the classifier of the model is normal, guaranteed.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I would present the theory of domain adaptation.",
                    "label": 1
                },
                {
                    "sent": "Then I will present the theory of learning with good similarity functions and its associated explicit projection space.",
                    "label": 1
                },
                {
                    "sent": "Then I would present our contribution consisting in modifying the projection space for domain adaptation.",
                    "label": 1
                },
                {
                    "sent": "After I will present some experimental results and before conclude I will present an extension of this work with some theoretical aspects for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amateur.",
                    "label": 0
                },
                {
                    "sent": "So first what is?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm an adaptation.",
                    "label": 0
                },
                {
                    "sent": "Like in supervised classification, we have an input space and the level sets.",
                    "label": 1
                },
                {
                    "sent": "Here we consider only binary problem.",
                    "label": 1
                },
                {
                    "sent": "One key difference in domain adaptation is that we have two different distribution.",
                    "label": 1
                },
                {
                    "sent": "The first one is the source domain and can be seen as the training distribution.",
                    "label": 1
                },
                {
                    "sent": "The second one is the target domain and can be seen as the test distribution.",
                    "label": 0
                },
                {
                    "sent": "We also can define the error of any parties on the two domains.",
                    "label": 1
                },
                {
                    "sent": "We can define the source domain error.",
                    "label": 0
                },
                {
                    "sent": "And the target domain error as the expectation of the hypothesis to commit an error on the considered domain.",
                    "label": 1
                },
                {
                    "sent": "And in fact, in domain adaptation we want to infer a model with a low target domain error.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider the case where we are only liable for the source domain for the target domain.",
                    "label": 1
                },
                {
                    "sent": "We have no information about the level and the main issue.",
                    "label": 0
                },
                {
                    "sent": "The main issue is then if we run.",
                    "label": 0
                },
                {
                    "sent": "If the classifier is learn on the source domain, how does it perform on the target domain?",
                    "label": 1
                },
                {
                    "sent": "To answer to this question where the following intuition here, here we have our two domains.",
                    "label": 0
                },
                {
                    "sent": "The source running ahead in red and the target one in blue.",
                    "label": 0
                },
                {
                    "sent": "With the rebels to see intuition and the first figure, if we run a low error classifier on the red data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We clearly see that it commits a lot of error on the blue data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second figure where we can remark that the two domains are very close, then running a low error classifier on the red data is equivalent to obtain a low error classifier on the blue data with this info.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Michelle.",
                    "label": 0
                },
                {
                    "sent": "Band aid and associate a proven the following theorem.",
                    "label": 0
                },
                {
                    "sent": "Here we have a bound over the error that we want to minimize a bundle where the target domain error.",
                    "label": 0
                },
                {
                    "sent": "This band is constituted by the summer free terms.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one is the classic error in supervised classification.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's the error under source domain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second term is the distance between the two domains, but without the level, it's the distance between the two marginal distribution of the two domains.",
                    "label": 0
                },
                {
                    "sent": "And intuitively, this distance can be seen as follow.",
                    "label": 0
                },
                {
                    "sent": "Again, are two domains the source one inward, the target, one in blue without the level here and the low distance is equivalent to be notable to separate the two during the two distributions?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last term is the error or what is called a joint optimal classifier.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's the best error that we can expect to obtain under under two domains at the same time, and this value can be seen as a measure of adaptation ability in the current space.",
                    "label": 1
                },
                {
                    "sent": "So for doing domain adaptation, we suppose that this value that is value is low.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, for domain adaptation we want to minimize this bound for building a new projection space where the two domains tend to be closed.",
                    "label": 0
                },
                {
                    "sent": "And we propose to use classifier based on good similarity function that I press.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't know.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we are in the theory proposed by Bellco, where a similarity function is any pairwise function that compares two instances and returns value between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "And we say that the frame similarity function is good for binary classification problem.",
                    "label": 1
                },
                {
                    "sent": "If it verifies these two points.",
                    "label": 1
                },
                {
                    "sent": "Intuitively we have a set of reasonable points denoted by her.",
                    "label": 1
                },
                {
                    "sent": "These points are kind of super points and we want that for a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "There are on average more similar to reasonable points to the same class than the reasonable points to the opposite class.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given the set of reasonable points, we can define an explicit projection space where an instance is represented by its vector of similarities to each reasonable points, and it's in this space that we can learn a linear linear classifier with good generalization guarantees.",
                    "label": 1
                },
                {
                    "sent": "The last important thing is that this notion of good similarity function generalizes the notion of kernel in the sense that.",
                    "label": 0
                },
                {
                    "sent": "A kernel is a good similarity function, but a good similarity function, maybe not symmetric and not positive semidefinite, and in fact it's this property that we want to use to modify the explicit projection space for domain adaptation.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our first contribution, consisting in modifying this space app, really deep.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I recall first that for domain adaptation task we want to be performing under target domain.",
                    "label": 1
                },
                {
                    "sent": "So we propose to insert some kind of target information in the similarity function.",
                    "label": 1
                },
                {
                    "sent": "For this we normalize the similarities curls too according to the source sample and the target sample in order to our viziru mean and the unit variance for each reasonable points.",
                    "label": 0
                },
                {
                    "sent": "And we see in experimental results that this kind of normalization can improve performance is for the others to ask others doing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station desk.",
                    "label": 0
                },
                {
                    "sent": "Our second contribution consists in recognizing the construction of the projection space during the learning process.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Air will start from the the domain adaptation bound proposed by Bendavid.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize this bound.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for minimizing the first term of the bond for minimizing the the source domain error, we simply use the linear program proposed by Bellco for learning with good similarity function.",
                    "label": 1
                },
                {
                    "sent": "Here we can remark that we don't know approving the set or food event reasonable points, so we take a set of inis, another potential with the number points, an serving.",
                    "label": 0
                },
                {
                    "sent": "This program is equivalent to learn a classifier and to select the relevant reasonable point by Astro City into them and underweight.",
                    "label": 0
                },
                {
                    "sent": "Alpha G.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second term of the bond is the distance between the marginal distribution of the two domains are recalled that allow distance is equivalent to be not able to separate the two domain.",
                    "label": 0
                },
                {
                    "sent": "The two distribution.",
                    "label": 0
                },
                {
                    "sent": "So we take a set a set of Pearl which associate to a source point, a target point, and we want to build a new projection space for you, where two points for a pair tend to be not separable.",
                    "label": 1
                },
                {
                    "sent": "In other words, we want that the divergent between the return value by the epitasis tend to be low, and in fact this divergents can be bound by this term.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this term can be seen as a distance in your new projection space.",
                    "label": 0
                },
                {
                    "sent": "And this projection space is defined by the writing of the similarity function by their long wait Alpha G. And we can do this because of the non PSD and Unsimilar earned and symmetric requirement for good similarity function.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we take this red term has a new regularization term to add in the problem proposed by Bellco for the similarity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function and we obtain this global optimization problem on the first line, we can recognize the problem proposed by Barco for learning with good similarity function and on the second line we can recognize organization term for each of our per set.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the last question, and is how can we validate the parameters, the writing and the selection of the pairs?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have no information about the the label on the target domain, so we cannot do a cross validation process, so we propose to use a kind of reverse validation defined by robust classifier age and this process.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the following error again our two domains, their source running red with ladders, the target one in route without level?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "Our classifier with the regularization term.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we only focus on the target sample.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, this sample with our classifier we obtain Absurdo Holabird, a target sample and with this adorable targets, and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn the Rovers classifier here without domain adaptation.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can evaluate this classifier this robust classifier on the source domain, with the idea that if two domains are related, then the robust classifier performs well on the source domain.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I will present some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we consider two different similarity function.",
                    "label": 0
                },
                {
                    "sent": "The first one is the classical Gaussian kernel, the second one is it's normalization.",
                    "label": 1
                },
                {
                    "sent": "We want to compare the performances of the two are similarity function with and without the organization term.",
                    "label": 1
                },
                {
                    "sent": "In other words, with and without domain adaptation.",
                    "label": 1
                },
                {
                    "sent": "We consider two different problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is a toy program called intertwining mounts, where one moon correspond to one class and we consider one source domain an A different target domain according to 8 rotation, eight different rotation angles.",
                    "label": 1
                },
                {
                    "sent": "Our second program is a real image annotation task.",
                    "label": 0
                },
                {
                    "sent": "It will take as a source domain, some images extracted from particular copious called Pascal VOC 2007, and we consider us target domain.",
                    "label": 0
                },
                {
                    "sent": "Some images extracted from videos and these videos are coming from the Corpus Nine 2007.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we can observe the 1st result for the type problem.",
                    "label": 0
                },
                {
                    "sent": "We can see the correct classification percentage according to the target domain or rotation angle.",
                    "label": 1
                },
                {
                    "sent": "The first thing is that the regularization term in red and green improve the results.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that the normalized similarity function in red improve performances from the artist ask.",
                    "label": 0
                },
                {
                    "sent": "I want to say.",
                    "label": 0
                },
                {
                    "sent": "Why so to try?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To translate this question, we propose to estimate the goodness of the similarity function on the target domain.",
                    "label": 1
                },
                {
                    "sent": "So here we start from the definition and back on and our record that we want.",
                    "label": 0
                },
                {
                    "sent": "I want that.",
                    "label": 0
                },
                {
                    "sent": "For a lot of example, there are on average more similar to points of the same class than than the point of the opposite class, so we propose to evaluate the goodness of similarity function as the proportion epsilon of example that not verified this property according to emerging Gala.",
                    "label": 1
                },
                {
                    "sent": "An assimilative function is better if this proportion is lower.",
                    "label": 0
                },
                {
                    "sent": "And here we can observe in red the normalized the similarity function and in blue the Gaussian kernel for the freezers task we clearly see that the true true similarity function are very similar, and for the five orders task, the red similarity to normalize one is better.",
                    "label": 0
                },
                {
                    "sent": "So it confirms the previous result.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Air, it's the results for the really major adaptation task.",
                    "label": 0
                },
                {
                    "sent": "Here again, the first thing is that the regularization term improved the results, and here in fact we are difficult domain adaptation tasks.",
                    "label": 0
                },
                {
                    "sent": "So the normalized similarity function is preferred.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, it's the end of the contribution of the published paper, and I now say some words about something which color aspect.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For my chart, for the method with the regularization term, we can.",
                    "label": 0
                },
                {
                    "sent": "The 1st results is an analysis of the sparsity of the infirm model.",
                    "label": 0
                },
                {
                    "sent": "We can prove this lemma here.",
                    "label": 0
                },
                {
                    "sent": "We have in fact abound for the optimal solution of our program, and who can say the following thing.",
                    "label": 1
                },
                {
                    "sent": "That is, the sparsity of the models depends on the parameters anhana value BR and this value is in fact rated variety to the difference between coordinates in the projection space.",
                    "label": 1
                },
                {
                    "sent": "And in fact, if the two domains are far, then this value tends to be I and it can imply an increase of the sparsity.",
                    "label": 0
                },
                {
                    "sent": "In some experiments we have, we have seen that the gain ratio is between for the size for the model is between 2:00 and 2:00.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sudden results is about generalization abilities.",
                    "label": 0
                },
                {
                    "sent": "For our method, we have in fact investigated the notion of algorithmic robustness proposed by Sue, and model had called 2010, where the idea is it's easier if a testing sample is similar to a training sample, then the testing error is close to the training error.",
                    "label": 1
                },
                {
                    "sent": "I've heard this this notion is still a conjecture for them, an adaptation.",
                    "label": 0
                },
                {
                    "sent": "Guys, despite this drawback, we can prove that her method is robust under source domain and with the help of the bond proposed by David for domain adaptation, we can derive the directly this bound order target error.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we can say that good similarity function Alpers too to build around production space for domain adaptation and were proposed two way to to modify this space.",
                    "label": 1
                },
                {
                    "sent": "The first one is on a pre renormalization of similarity function according to the target domain, and that future works.",
                    "label": 1
                },
                {
                    "sent": "We aim to find a better way to design A similarity function for domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "Our second method is based on the addition of new regularization term from moving closer to two domains during the learning process and I just want to add that, well extended this work through an iterative approach and this approach can improve the performances and can lighten the search of different parameters so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Fact here is that we.",
                    "label": 0
                },
                {
                    "sent": "Symptomatic presentation than classical discriminatory because people tell us immediately positive function.",
                    "label": 0
                },
                {
                    "sent": "So we cannot exactly say that we are closed this year in order.",
                    "label": 0
                },
                {
                    "sent": "But one thing is that we restriction of the cauldron with double points so.",
                    "label": 0
                },
                {
                    "sent": "Percentage points of some areas of decision during the space and in the meditation.",
                    "label": 0
                },
                {
                    "sent": "We think that.",
                    "label": 0
                },
                {
                    "sent": "We must select some points that are between the source and the target domain so they are against time to give an intuition can between the two domains to give time to define the new vision program is that it has to go to London to the target domain.",
                    "label": 0
                },
                {
                    "sent": "Find an.",
                    "label": 0
                },
                {
                    "sent": "Fun fact is that we have some demented thought for what we want to do is to find the difference in specs, and we mean that you want to find some subspace where the domains are similar, which replaces reduce with the position space.",
                    "label": 0
                },
                {
                    "sent": "I mean in the dimension and weight, focusing anew subspace environments smart space where we can think so.",
                    "label": 0
                },
                {
                    "sent": "To find this particular points while we projected security as well, in order to to get clothes on.",
                    "label": 0
                },
                {
                    "sent": "So in another, exactly to be close or not close to this abortion.",
                    "label": 0
                },
                {
                    "sent": "I want to find space where the source on target.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to run the message I put his things so we tried to define the lyrics which placed back in this case, and the process faces the projection to the similarities cost on some points, which maybe not another segment.",
                    "label": 0
                },
                {
                    "sent": "Only with this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "Thank you in practical applications.",
                    "label": 0
                },
                {
                    "sent": "Customer is very useful or might be great.",
                    "label": 0
                },
                {
                    "sent": "Very useful, but it's difficult to to understand automatically assumptions of this specific clustering technique like doors in practical problems.",
                    "label": 0
                },
                {
                    "sent": "So stressing I would like to add some comments on on which assumptions do you.",
                    "label": 0
                },
                {
                    "sent": "You assume in your method and.",
                    "label": 0
                },
                {
                    "sent": "Because the problem is not just the expressions assumption in summer months away, but being able to express these in practical problems where maybe the language or the domain language is different and then ultimately straightforward feedback.",
                    "label": 0
                },
                {
                    "sent": "So if you want to say something more about this problem.",
                    "label": 0
                },
                {
                    "sent": "It's clearly depends on under the problem in the consumer crap.",
                    "label": 0
                },
                {
                    "sent": "I'm OK and then we try to propose a general method, but the I think the main important thing is that this notion of per set and with this despair set, if we know this, this set appropriates it's seems to be easy to to do to make them an adaptation, because we.",
                    "label": 0
                },
                {
                    "sent": "We know which point we want to to move closer and if we don't know this this set we have to choose this set between all of the points and it's clearly are intractable.",
                    "label": 0
                },
                {
                    "sent": "That's why we have proposed an iterative approach that tends to auto corrector its selection, but it's very hard to control the this this program.",
                    "label": 0
                },
                {
                    "sent": "So the main, I think the main important.",
                    "label": 0
                },
                {
                    "sent": "People disease is that the last term of the bond proposed by Ben David, the hair of the joint optimal.",
                    "label": 0
                },
                {
                    "sent": "A classifier is a.",
                    "label": 0
                },
                {
                    "sent": "As to be a low for for doing a domain adaptation, because if it can be impossible to do for to do.",
                    "label": 0
                },
                {
                    "sent": "Domain adaptation for some problem, I think.",
                    "label": 0
                },
                {
                    "sent": "I think you have to.",
                    "label": 0
                },
                {
                    "sent": "Let's give a handful.",
                    "label": 0
                }
            ]
        }
    }
}