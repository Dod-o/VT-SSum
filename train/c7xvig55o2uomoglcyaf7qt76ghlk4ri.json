{
    "id": "c7xvig55o2uomoglcyaf7qt76ghlk4ri",
    "title": "Information Theoretic Model Validation by Approximate Optimization",
    "info": {
        "author": [
            "Joachim M. Buhmann, Institute of Computational Science, ETH Zurich"
        ],
        "published": "Jan. 5, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Information Theory"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_buhmann_itm/",
    "segmentation": [
        [
            "Thanks Andreas, let me start by thanking the organizers for the opportunity to present that work here.",
            "What I would like to advocate is an information theoretic approach to optimization, and in particular I'm asking the question when data have perturbed the optimization problem, how?"
        ],
        [
            "Sisely, you should actually approximate.",
            "I will start out with motivating why information theory is a good approach for optimization.",
            "Then I introduce the approximation capacity for a cost function and I give you a number of examples.",
            "Probably I will not get through the list where it actually works to determine how much you should regularize your problem and what kind of cost function you actually should optimize.",
            "And then I have some conclusions."
        ],
        [
            "So what is the just to give you an overview of what my thinking is?",
            "What is the Channel Central Challenge in pattern recognition?",
            "Is it finding the right model or validating the model in my hypothesis?",
            "Is that validating of pattern recognition models is the more fundamental challenge because we have many models today and garbage collection on models when they are not predicting as well as you like them to do is absolutely mandatory.",
            "So you look for.",
            "A model validation procedure which, on one hand, prefers noise tolerant and expressive models over brittle and sophisti simplistic ones and this relates to the stability versus informativeness tradeoff.",
            "You want to have stable models and they should also be not simplistic but informative and information theory gives you some kind of an approach which you could use in order to.",
            "To find the right compromise between stability and informativeness and in the end I hope to convince you that this will give you something like context sensitive information."
        ],
        [
            "So let me remind you what the situation is and how the analogy between information theory on one hand and pattern recognition on the other hand can be seen.",
            "I would like to highlight three different information theoretic components.",
            "There is the sender side where codebook vectors are defined and it's a set of the subset of all possible substrings, and I consider that my hypothesis class.",
            "Then you have these noisy channel where the codebook vector is communicated from the sender to the receiver.",
            "And the receiver then decodes, that is the received bitstring by minimizing the Hamming distance and what comes out is a criterion for error free communication.",
            "And it basically means you have to look at the mutual information and you have to.",
            "You choose your code according to that criterion.",
            "Now what are the what are the parallel components which you could use in pattern recognition?",
            "Well, in pattern recognition you usually have some kind of an optimization problem to find good structures in your data.",
            "And I like to associate with that hypothesis class an approximation set as my hypothesis class.",
            "So it's it's an approximation set is the interpretations for my data, which is good according to some cost function.",
            "The analogue to the noisy channel in optimization is then a perturbed optimization problem.",
            "So I get some data I optimize, but then the solution should work on a second data set which is drawn from the same probability distribution.",
            "That's sort of the two sample scenario for generalization.",
            "And in the end you do also decoding and the decoding is by approximate optimization on a test instance.",
            "So these are the components which I will use in order to make an analogy between what we know from information theory and to apply it to pattern recognition problems.",
            "What comes out is that model validation on the pattern recognition side is again governed by a mutual information which actually tells you how precisely you should approximate a model, and that also means how complex this model can be in the light of the data.",
            "So in essence, what I would like to derive is is a systematic way of getting optimized regularization constraints."
        ],
        [
            "So what are the hypothesis in pattern recognition?",
            "I've worked a lot on class string, so these are the type of partitioning's you are looking for.",
            "Usually they are in very high dimensions and it's getting more complicated, but you might also be interested in trees.",
            "Phylogenetic trees for example, or language trees.",
            "Then you have these kind of tree structures with partitions with little motricity as a constraint.",
            "Or you might have graphical models which are more complicated in that context.",
            "These are the type of hypothesis classes I would like to address, so they might be usually they are discrete in these types of cases, but you can also imagine that you have continuous hypothesis classes.",
            "A characteristic for these situations is that usually the hypothesis class is much smaller than your data space.",
            "That means a generative approach will fail, because I will never have enough observations to actually identify the probability distribution over my data space.",
            "But I still from a very small sample of data.",
            "For example, only a few images I could hope to actually find.",
            "A good subset of hypothesis to identify in my.",
            "Lucian space.",
            "So to some degree I'm not believing for these types of problems into the generative modeling approach.",
            "I would advocate a discriminative approach where you focus on on the resolution in the hypothesis class."
        ],
        [
            "So let me now give you a pictorial view of what I would like to achieve, when you when you go to information theory and you assume this is the space of strings.",
            "Then you try with random Shannon's random codebook vector idea.",
            "You cover these set of strings with your codebook vectors and then you select the particular codebook vector.",
            "This is the red one here and this is the message you want to submit across the channel and then you have.",
            "Since this codebook vector is perturbed, you have the task you have to solve the task to actually identify the closest codebook vector in that big list of.",
            "Of in that codebook, now when you want to apply the same picture to pattern recognition problems and to rely on approximation sets, the situation is similar.",
            "I have these different pattern recognition problems which cover the space of my hypothesis and then I have to identify one of these.",
            "I have to choose one of these pattern recognition problems.",
            "The minimizer is here and I use this as a message.",
            "And the receiver then has to identify this approximation set with this representative, where the red this Red Cross identifies the minimizer for this particular problem.",
            "So in Shannon situation, we are minimizing the Hamming distance in general, in pattern recognition we are minimizing some kind of a criterion which we are interested in when interpreting our data."
        ],
        [
            "So let me give him a little bit more mathematical structure and then I will come to a specific case to make it clear in the context of of graph cut.",
            "So given out some data, that's my input space on my data space and the goal is to learn structure from the data, in particular to interpret the data relative to a hypothesis class and what I mean by that is that I have a set of functions C which map my data into some output space and that could be.",
            "The the the binary space the bullions to the power of N or a set of labels one 2K.",
            "For my end different objects.",
            "And I also assume that I have a cost function and the cost function assigns to a hypothesis and data non negative real number which is the quality of that particular hypothesis.",
            "So this cost function will play.",
            "Now the essential role of the communication carrier between a sender and the receiver.",
            "So that's the idea which I would like to."
        ],
        [
            "To explore.",
            "Now."
        ],
        [
            "How can I actually find now the analog?",
            "Of the codebook vectors, when I do optimization, I remember I have one set of measurements X, but I still want to cover my hypothesis class so that I can actually explore different alternatives the way you do."
        ],
        [
            "So that is, you ask.",
            "What are?",
            "A set of transformations under which your cost function is actually variant.",
            "So what I mean by that?",
            "I define our set of transformations.",
            "Sigma and I require that the costs for the hypothesis C and the data X is the same as if I transform my data with the transformation Sigma.",
            "And equivalently, I also transform my hypothesis.",
            "So that corresponds for example, in the case of of graph cuts, I permute my my the index of my labels and if I also permute, then correspondingly my hypothesis, I get the same description of my of my.",
            "Of my solutions.",
            "I have the minimizer.",
            "If I permute my data, then the minimizer is just shifted in the hypothesis class.",
            "So I with these transformations I can generate a coverage of my hypothesis class when I have only one set of measurements.",
            "Now, what is the approximation set?",
            "The approximation set is essentially the set of hypothesis, which are gamma close to the global minimizer.",
            "See Bottom is my minimizer.",
            "And I like to admit all hypothesis which are gamma close to the minimum.",
            "Now information theory is now used to determine what is the optimal gamma.",
            "On one hand, I want to have a very small gamma because I want to precisely estimate my hypothesis.",
            "On the other hand, if I to precisely estimate my hypothesis, I will be confused because the data have fluctuations and I then can no longer identify.",
            "These approximations set in a stable way.",
            "So that's."
        ],
        [
            "That's the idea.",
            "And the question is now, how could you actually in very concrete cases, generate these different code problems?",
            "These optimization problems, which we then want to use for coding.",
            "Now if I have a combinatorial optimization problem, then very naturally you can use the permutation of the combinatorial components.",
            "So for example you permute the labeling of your vertices in the graph and this would give you then a solution.",
            "Off that, for example, graph cut problem which is somewhere else in your hypothesis class.",
            "If you have localization problems like estimating the mean of a Gaussian or something like this, the natural transformation would be shifting the data.",
            "So you have some data, you shift them, know the data appear.",
            "If you have an orientation problem and I will come back to that in one of the examples like singular value decomposition, then the rotations are the transformations which allow you to cover your hypothesis class.",
            "So you have these data set.",
            "Here you rotate it, it gives you a new different problem to identify."
        ],
        [
            "So let me now come to an example to make it a little bit more concrete.",
            "You have graph cut.",
            "Here the hypothesis class are or cuts.",
            "And here I have a pictorial description of what the solution is.",
            "I just cut it in two different into different."
        ],
        [
            "Graphs.",
            "No, if you do that.",
            "Here is again this graph cut solution and you permute.",
            "Here is adjacency matrix with the solution vector.",
            "Here if you permute your labels here, you have a different adjacency matrix in a different representation of your cut.",
            "So the messages which I would like to actually communicate is the transformation.",
            "Of this of this solution here.",
            "And then I relat.",
            "The spacing of these transformations to the fluctuations in my data and this gives me a precise criterion.",
            "How I can estimate?",
            "How precisely I can estimate the underlying hypothesis so I do these?",
            "I apply these transformations in this case for graph cut.",
            "These are permutations.",
            "Apply them a number of times two to the N road different times and that is sort of my vocabulary to talk about statistically distinctive hypothesis.",
            "Now when I get now my test graph, I always need two types of instances.",
            "If I get my test graph, the test graph is different I have.",
            "Links added I now have some links.",
            "Cut.",
            "The adjacency matrix is different.",
            "This would be the minimal cut here and now.",
            "You can guess whether this minimal cut corresponds to this representation of to this representation and you see already it's much more similar to this solution to this one, and this is essentially the trick which we use for for for communicating the transformation."
        ],
        [
            "So what is the protocol now in terms of sender receiver type of communication setting?",
            "You have a sender, you have a receiver and rather than a channel I have a problem generator.",
            "Now the problem generator gives me my training data X1 with the respective cost function to the sender and the receiver and they now have to establish a protocol so the sender minimizes that the receiver minimizes it and they agree upon a number of different transformations, Sigma one up to Sigma two to the NRO different transformations.",
            "So these are the messages they can communicate."
        ],
        [
            "Now during communication, what happens is the following process.",
            "The sender selects one of these transformations and sends it to the problem generator.",
            "Now the problem generator draws from the same probability distribution of the data.",
            "A second data set X2.",
            "It applies these transformation to X2.",
            "Sigma S is applied now 2X2 and and the problem generator sends these new data with their transformation to the receiver.",
            "Now the receiver has the problem that on one hand the fluctuations are different.",
            "And on the other hand, he doesn't know what the transformation is.",
            "So if the transformations are spaced very far away in the hypothesis class and the fluctuations are small, then you have no problem with able to identify that.",
            "If the transformations are very fine spaced and the fluctuations are large, you will constantly confused what's going on.",
            "And that's exactly the mechanism which I used to identify.",
            "How much you can communicate so in the last step, the receiver has to guess what the transformation was, and that allows you for this.",
            "For this communication protocol, to calculate an error rate and the error rate is whenever Sigma had is not equal to Sigma S. Now what we have to do is in order to make this call."
        ],
        [
            "But if we have to measure this error rate, So what the what the receiver now has to do is to compare sets of hypothesis.",
            "I'm now telling you what the decoding procedure is based on this decoding procedure.",
            "We can calculate the error rate.",
            "So I need a mapping which actually allows me to compare these two different hypothesis generated on X1 and data X2.",
            "Never mind you can identify for all the cases I I'm discussing this side with an identity mapping and the decoding goes in the following way.",
            "The approximation set on the second data set X~ two.",
            "That's what the receiver actually receives.",
            "Is intersected with an approximation set where the training data have been transformed with the transformation Sigma and you maximize over all possible sigmas.",
            "So you try to find the transformation which makes the approximation set.",
            "There's a receiver has calculated as similar as possible to the approximation sets.",
            "Which you have in your in your in your possible set of transformations."
        ],
        [
            "So."
        ],
        [
            "This is the decoding procedure.",
            "That's how I calculate my Sigma head.",
            "Now what you have to do is what we basically have now have to calculate what is the error rate under this protocol.",
            "Anet will sensitively depend how many of these transformations an I have chosen an where I have I have a disk."
        ],
        [
            "And then so let me introduce some sets.",
            "The joint approximation set, Delta CJ is the intersection where the sender has chosen Sigma J, but the sender has chosen Sigma J and that has been applied by the problem generator to my test data.",
            "But you compare it with the transformation Sigma J here on the.",
            "And the training data.",
            "So this is this intersection set and the error events are when the transformation Sigma J and Sigma S are different compared to the case where I have correctly identified as the best overlapping approximation set with the transformation Sigma is so whenever Delta CJ is larger than the cardinality of Delta, CSI will decode according to my decoding protocol I will."
        ],
        [
            "Dinero so how do you calculate now these errors you have to you have to determine the conditional probability.",
            "Sigma head is not equal to Sigma S my message and you do that by using the Union bound and then you have to calculate the probability that this cardinality is larger than this cardinality here and at that point for the clustering applications in particular.",
            "Also the K cut application.",
            "We use random transformations of Sigma.",
            "We just use random permutations.",
            "Is our code messages in an analogy to Shannon's random coding argument?"
        ],
        [
            "Now let me just skip through that what you can do is you can approximate these.",
            "These probability of error by a quantity I gamma and this quantity I gamma is essentially a mutual information between these approximation sets.",
            "The random transformation decouples the intersection between two approximation sets and that's why I have here the product of two cardinalities, whereas in the denominator you still have this intersection of the statistically coupled.",
            "Two approximation sets when you use the same transformation."
        ],
        [
            "So if you take the limit of these error rate and you require that you are communicating with 0 error, you have the condition that your rate row for this for this communication scenario should be smaller than this mutual information which comes out the logarithm of of these cardinalities gives you three different terms.",
            "When you analyze that the first term is the entropy on the receiver side on the sender side, the second term is the entropy on the receiver side.",
            "And the third term, which is negative here, is the joint entropy, and this gives you the mutual information between these approximation sets on the sender and the receiver side.",
            "So again, for communication, I have to maximize mutual information and that gives me the maximal amount of of rate.",
            "In order to have."
        ],
        [
            "0 error zero error communication.",
            "Now how can I use that to actually rank different cost functions?",
            "The ranking of different cost functions is now done in the following.",
            "I have these protocol which basically allows me to measure how precisely a particular cost function can resolve my hypothesis class.",
            "So if I use this communication protocol and you have your cost function an I have my cost function, we just measure according to that protocol how many bits.",
            "We can get across this noisy channel, which is which is the problem generator and both cost functions are exposed to the same fluctuations in the data.",
            "So if you want to have a robust if you want to have a robust model, basically what you have to do is you have to have a rich model which on the other hand is also very insensitive to the fluctuations apedale separation between fluctuations and signal will give you a higher communication rate and that is the ranking criterion which I'm using.",
            "For finding out what the good models are."
        ],
        [
            "Now you might say, how can you kind of?",
            "Can you test whether this is a reasonable idea?",
            "We sort of tried out the simplest case in order to embed this new theory or bring it in contact with information theory and what you do for that is, you minimize the Hamming distance when you when you have a hypothesis class of bit strings, and in that case when we compare bit strings and behind the random.",
            "Coding scenario of Shannon.",
            "We basically recover under the extreme aliti of that mutual information formula formula.",
            "We recover the capacity of the binary symmetric channel.",
            "So in a setting which corresponds to Shannon's random coding scenario, this new theory allows you to recover the cases from information."
        ],
        [
            "Theory.",
            "Now, when you actually want to use it for estimating mixtures of Gaussians.",
            "How much time do I have?",
            "If I if I want to use it for recovering mixtures of Gaussians, I can I can basically use the K means cost function have an approximation control in terms of a temperature and if I lower the temperature when I have two Gaussian sources and I have a number of different hypothetical means for my K means problem, I see such a such a bifurcation pattern.",
            "If you now ask what is the generalization error, that's basically the costs which you have to pay for when you take a solution on the first data set, and you apply it to the second data set.",
            "Then, depending on this on this regularity of your hypothesis class on this temperature, this cost has a minimum here, and this minimum exactly coincides with a maximum of that mutual information.",
            "So the criterion which I derive gives you exactly the right model in in this context."
        ],
        [
            "Now if you are if you are.",
            "If you have 500 data points and you are in handle dimensions and the data comes from 2 Gaussians but you but you invest for Gaussians in your hypothesis class and you vary the temperature, then you see for a large temperature regime you get 2 Gaussians.",
            "Then for a very tiny one in that green area you get 3000.",
            "Then you get for Gaussians.",
            "So what is the mutual information telling you?",
            "The mutual information tells you that you should exactly stop here.",
            "This is the minimum of the transfer cost, so to speak.",
            "The generalization error and that means also in this case you get the right model order for this in our."
        ],
        [
            "Real.",
            "So if you are in very high dimensions and you have very few data points you actually see for estimating two Gaussians phase diagram where you have an unsplit phase, you have a split phase where you actually estimate precisely the two Gaussians and you have a random phase where you estimate two Gaussians.",
            "But their means have nothing to do with the data source.",
            "Under these circumstances you also see that you can only that you only can code in that.",
            "Ordered phase but not in the unsplit phase here.",
            "You have no structure, and here you have random structure.",
            "Random structure is not really."
        ],
        [
            "So you don't get any anything for that.",
            "Let me come to the to the final example.",
            "So far I have talked about discrete optimization and how you regularize here is a situation where I have a Boolean matrix.",
            "We took that from some applications in security and it's very noisy.",
            "Now you want to denoise via low rank approximation to calculated by SVD.",
            "So here you have a rank 5 approximation.",
            "If you then denoise by rounding, this is what you get.",
            "Now how can you find the optimal rank for this?",
            "For these denoising experiment we use again the same scenario we use SVD as a coding way."
        ],
        [
            "If you do that and you calculate what the approximation capacity is for the optimal beta, it has a clear maximum here, and if you look what the what, the clear maximum corresponds to as a function of rank.",
            "It corresponds exactly to the point where you have the minimum reconstruction error.",
            "So that means this criterion allows you also to precisely cut off the spectrum of SVD under this noise source."
        ],
        [
            "So.",
            "And probably running out of time.",
            "OK, so we also applied this in a relatively complicated scenario.",
            "For role based access control, this is a problem in C."
        ],
        [
            "Security.",
            "It basically relies on a matrix decomposition which."
        ],
        [
            "Which then gives you also the same type of ranking for model order selection.",
            "So I don't."
        ],
        [
            "To go into that.",
            "Except that also in this, in this scenario, when you when you when you pick according to the generalization error, the approximation capacity, the maximum of the approximation capacity, you'll stop temperature.",
            "You get very low.",
            "Generalization error, so as a model selection criterion this this concept allows you to select the model order precisely in these in these."
        ],
        [
            "In this setting, so let me conclude by saying the following noise in the data basically quantizes the mathematical structures which are involved here.",
            "You have hypothesis classes.",
            "You have different choices for your metrics and so on.",
            "This scenario allows you to quantify how these different mathematical structures are actually sensitive, how they are influenced by noise, and whenever they are sensitive to noise.",
            "Then they will reduce the.",
            "The.",
            "The capacity of this of this channel that means you can project out those those models, whether you parameterized them with different hypothesis class, is obvious.",
            "Different cost functions in a way that they are noise robust, since the noise quantizes these structures, you have symbols and you use them for coding, so that's the underlying idea.",
            "If you have the criterion of error free communication then this determines for every cost function.",
            "And approximation capacity and you can.",
            "You can basically rank these cost functions according to how high their approximation capacity."
        ],
        [
            "And So what will be future work?",
            "First of all, I would like to explore more in detail.",
            "The this is an asymptotic theory because I use asymptotic arguments for for determining the channel capacity.",
            "I'm still working on a lower bound for these for this argument, but according to the.",
            "Theory of jointly typically code words in information theory.",
            "You can actually use the same type of techniques.",
            "Here we have.",
            "We are in the process of applying it to model reduction for dynamical systems.",
            "Again, you would like to know in a set of ODS or PDS, how precisely you should regularize them by approximating them, and this coding scenario should also be able to tell you what the right approximation quality is, and I would like to conclude with a statement which I think is.",
            "Is at the core of machine learning and how it interacts with computer science.",
            "The.",
            "In all of these experiments, in all of these studies, the statistical complexity has to be related in some way to the computational to the algorithmic complexity, and the challenge is to find out whether you, on the statistical side, can still find predictive models by your approximation by your best possible approximation quality in such a way that you find them efficiently.",
            "So, since since I'm working.",
            "A lot in practically in applied problems the question the user most often asks is.",
            "Is your model robust?",
            "Now this theory allows to quantify precisely what robustness means, and it it trades of robustness against complexity.",
            "Richness of the description, and so as a constraint we want to have good generalizing models which at the same time I efficiently computable and let me close with that.",
            "Thank you.",
            "Formation capacity so yeah.",
            "This thing comes in here so that.",
            "Very busy or something similar.",
            "Well the theories so far is at the level of random coding theory, so I use random permutations for covering my hypothesis class, whereas in communication theory you do not want to randomly cover your bid space, you want to have an efficient decoder.",
            "But I mean that even right now causing you to work in a large dimensional space, yes.",
            "So the dimensionality comes from the from the from the set of transformations you have to, you have to consider.",
            "So when I applied this theory to the to the Hemming to minimizing the Hamming distance, I took this one bit string and I randomly permuted these bits to get my codebook vectors.",
            "So I was basically using the theory of types to generate a coverage of my of my hypothesis class and in the same way I do that for discrete optimization problem, except that the bitstring now is the minimizer of that one problem.",
            "Which I shift around in the hypothesis class.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks Andreas, let me start by thanking the organizers for the opportunity to present that work here.",
                    "label": 0
                },
                {
                    "sent": "What I would like to advocate is an information theoretic approach to optimization, and in particular I'm asking the question when data have perturbed the optimization problem, how?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sisely, you should actually approximate.",
                    "label": 0
                },
                {
                    "sent": "I will start out with motivating why information theory is a good approach for optimization.",
                    "label": 1
                },
                {
                    "sent": "Then I introduce the approximation capacity for a cost function and I give you a number of examples.",
                    "label": 1
                },
                {
                    "sent": "Probably I will not get through the list where it actually works to determine how much you should regularize your problem and what kind of cost function you actually should optimize.",
                    "label": 0
                },
                {
                    "sent": "And then I have some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the just to give you an overview of what my thinking is?",
                    "label": 0
                },
                {
                    "sent": "What is the Channel Central Challenge in pattern recognition?",
                    "label": 1
                },
                {
                    "sent": "Is it finding the right model or validating the model in my hypothesis?",
                    "label": 1
                },
                {
                    "sent": "Is that validating of pattern recognition models is the more fundamental challenge because we have many models today and garbage collection on models when they are not predicting as well as you like them to do is absolutely mandatory.",
                    "label": 0
                },
                {
                    "sent": "So you look for.",
                    "label": 0
                },
                {
                    "sent": "A model validation procedure which, on one hand, prefers noise tolerant and expressive models over brittle and sophisti simplistic ones and this relates to the stability versus informativeness tradeoff.",
                    "label": 1
                },
                {
                    "sent": "You want to have stable models and they should also be not simplistic but informative and information theory gives you some kind of an approach which you could use in order to.",
                    "label": 0
                },
                {
                    "sent": "To find the right compromise between stability and informativeness and in the end I hope to convince you that this will give you something like context sensitive information.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me remind you what the situation is and how the analogy between information theory on one hand and pattern recognition on the other hand can be seen.",
                    "label": 0
                },
                {
                    "sent": "I would like to highlight three different information theoretic components.",
                    "label": 0
                },
                {
                    "sent": "There is the sender side where codebook vectors are defined and it's a set of the subset of all possible substrings, and I consider that my hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Then you have these noisy channel where the codebook vector is communicated from the sender to the receiver.",
                    "label": 0
                },
                {
                    "sent": "And the receiver then decodes, that is the received bitstring by minimizing the Hamming distance and what comes out is a criterion for error free communication.",
                    "label": 1
                },
                {
                    "sent": "And it basically means you have to look at the mutual information and you have to.",
                    "label": 0
                },
                {
                    "sent": "You choose your code according to that criterion.",
                    "label": 0
                },
                {
                    "sent": "Now what are the what are the parallel components which you could use in pattern recognition?",
                    "label": 0
                },
                {
                    "sent": "Well, in pattern recognition you usually have some kind of an optimization problem to find good structures in your data.",
                    "label": 1
                },
                {
                    "sent": "And I like to associate with that hypothesis class an approximation set as my hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So it's it's an approximation set is the interpretations for my data, which is good according to some cost function.",
                    "label": 1
                },
                {
                    "sent": "The analogue to the noisy channel in optimization is then a perturbed optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So I get some data I optimize, but then the solution should work on a second data set which is drawn from the same probability distribution.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the two sample scenario for generalization.",
                    "label": 1
                },
                {
                    "sent": "And in the end you do also decoding and the decoding is by approximate optimization on a test instance.",
                    "label": 0
                },
                {
                    "sent": "So these are the components which I will use in order to make an analogy between what we know from information theory and to apply it to pattern recognition problems.",
                    "label": 0
                },
                {
                    "sent": "What comes out is that model validation on the pattern recognition side is again governed by a mutual information which actually tells you how precisely you should approximate a model, and that also means how complex this model can be in the light of the data.",
                    "label": 0
                },
                {
                    "sent": "So in essence, what I would like to derive is is a systematic way of getting optimized regularization constraints.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the hypothesis in pattern recognition?",
                    "label": 0
                },
                {
                    "sent": "I've worked a lot on class string, so these are the type of partitioning's you are looking for.",
                    "label": 0
                },
                {
                    "sent": "Usually they are in very high dimensions and it's getting more complicated, but you might also be interested in trees.",
                    "label": 0
                },
                {
                    "sent": "Phylogenetic trees for example, or language trees.",
                    "label": 0
                },
                {
                    "sent": "Then you have these kind of tree structures with partitions with little motricity as a constraint.",
                    "label": 0
                },
                {
                    "sent": "Or you might have graphical models which are more complicated in that context.",
                    "label": 0
                },
                {
                    "sent": "These are the type of hypothesis classes I would like to address, so they might be usually they are discrete in these types of cases, but you can also imagine that you have continuous hypothesis classes.",
                    "label": 0
                },
                {
                    "sent": "A characteristic for these situations is that usually the hypothesis class is much smaller than your data space.",
                    "label": 1
                },
                {
                    "sent": "That means a generative approach will fail, because I will never have enough observations to actually identify the probability distribution over my data space.",
                    "label": 0
                },
                {
                    "sent": "But I still from a very small sample of data.",
                    "label": 0
                },
                {
                    "sent": "For example, only a few images I could hope to actually find.",
                    "label": 0
                },
                {
                    "sent": "A good subset of hypothesis to identify in my.",
                    "label": 0
                },
                {
                    "sent": "Lucian space.",
                    "label": 0
                },
                {
                    "sent": "So to some degree I'm not believing for these types of problems into the generative modeling approach.",
                    "label": 0
                },
                {
                    "sent": "I would advocate a discriminative approach where you focus on on the resolution in the hypothesis class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me now give you a pictorial view of what I would like to achieve, when you when you go to information theory and you assume this is the space of strings.",
                    "label": 1
                },
                {
                    "sent": "Then you try with random Shannon's random codebook vector idea.",
                    "label": 0
                },
                {
                    "sent": "You cover these set of strings with your codebook vectors and then you select the particular codebook vector.",
                    "label": 0
                },
                {
                    "sent": "This is the red one here and this is the message you want to submit across the channel and then you have.",
                    "label": 0
                },
                {
                    "sent": "Since this codebook vector is perturbed, you have the task you have to solve the task to actually identify the closest codebook vector in that big list of.",
                    "label": 0
                },
                {
                    "sent": "Of in that codebook, now when you want to apply the same picture to pattern recognition problems and to rely on approximation sets, the situation is similar.",
                    "label": 0
                },
                {
                    "sent": "I have these different pattern recognition problems which cover the space of my hypothesis and then I have to identify one of these.",
                    "label": 0
                },
                {
                    "sent": "I have to choose one of these pattern recognition problems.",
                    "label": 0
                },
                {
                    "sent": "The minimizer is here and I use this as a message.",
                    "label": 0
                },
                {
                    "sent": "And the receiver then has to identify this approximation set with this representative, where the red this Red Cross identifies the minimizer for this particular problem.",
                    "label": 1
                },
                {
                    "sent": "So in Shannon situation, we are minimizing the Hamming distance in general, in pattern recognition we are minimizing some kind of a criterion which we are interested in when interpreting our data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give him a little bit more mathematical structure and then I will come to a specific case to make it clear in the context of of graph cut.",
                    "label": 0
                },
                {
                    "sent": "So given out some data, that's my input space on my data space and the goal is to learn structure from the data, in particular to interpret the data relative to a hypothesis class and what I mean by that is that I have a set of functions C which map my data into some output space and that could be.",
                    "label": 1
                },
                {
                    "sent": "The the the binary space the bullions to the power of N or a set of labels one 2K.",
                    "label": 0
                },
                {
                    "sent": "For my end different objects.",
                    "label": 0
                },
                {
                    "sent": "And I also assume that I have a cost function and the cost function assigns to a hypothesis and data non negative real number which is the quality of that particular hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So this cost function will play.",
                    "label": 0
                },
                {
                    "sent": "Now the essential role of the communication carrier between a sender and the receiver.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea which I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To explore.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can I actually find now the analog?",
                    "label": 0
                },
                {
                    "sent": "Of the codebook vectors, when I do optimization, I remember I have one set of measurements X, but I still want to cover my hypothesis class so that I can actually explore different alternatives the way you do.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that is, you ask.",
                    "label": 0
                },
                {
                    "sent": "What are?",
                    "label": 0
                },
                {
                    "sent": "A set of transformations under which your cost function is actually variant.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by that?",
                    "label": 0
                },
                {
                    "sent": "I define our set of transformations.",
                    "label": 1
                },
                {
                    "sent": "Sigma and I require that the costs for the hypothesis C and the data X is the same as if I transform my data with the transformation Sigma.",
                    "label": 0
                },
                {
                    "sent": "And equivalently, I also transform my hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So that corresponds for example, in the case of of graph cuts, I permute my my the index of my labels and if I also permute, then correspondingly my hypothesis, I get the same description of my of my.",
                    "label": 0
                },
                {
                    "sent": "Of my solutions.",
                    "label": 0
                },
                {
                    "sent": "I have the minimizer.",
                    "label": 0
                },
                {
                    "sent": "If I permute my data, then the minimizer is just shifted in the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So I with these transformations I can generate a coverage of my hypothesis class when I have only one set of measurements.",
                    "label": 1
                },
                {
                    "sent": "Now, what is the approximation set?",
                    "label": 1
                },
                {
                    "sent": "The approximation set is essentially the set of hypothesis, which are gamma close to the global minimizer.",
                    "label": 0
                },
                {
                    "sent": "See Bottom is my minimizer.",
                    "label": 0
                },
                {
                    "sent": "And I like to admit all hypothesis which are gamma close to the minimum.",
                    "label": 0
                },
                {
                    "sent": "Now information theory is now used to determine what is the optimal gamma.",
                    "label": 0
                },
                {
                    "sent": "On one hand, I want to have a very small gamma because I want to precisely estimate my hypothesis.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if I to precisely estimate my hypothesis, I will be confused because the data have fluctuations and I then can no longer identify.",
                    "label": 0
                },
                {
                    "sent": "These approximations set in a stable way.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the idea.",
                    "label": 0
                },
                {
                    "sent": "And the question is now, how could you actually in very concrete cases, generate these different code problems?",
                    "label": 1
                },
                {
                    "sent": "These optimization problems, which we then want to use for coding.",
                    "label": 1
                },
                {
                    "sent": "Now if I have a combinatorial optimization problem, then very naturally you can use the permutation of the combinatorial components.",
                    "label": 1
                },
                {
                    "sent": "So for example you permute the labeling of your vertices in the graph and this would give you then a solution.",
                    "label": 0
                },
                {
                    "sent": "Off that, for example, graph cut problem which is somewhere else in your hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "If you have localization problems like estimating the mean of a Gaussian or something like this, the natural transformation would be shifting the data.",
                    "label": 0
                },
                {
                    "sent": "So you have some data, you shift them, know the data appear.",
                    "label": 0
                },
                {
                    "sent": "If you have an orientation problem and I will come back to that in one of the examples like singular value decomposition, then the rotations are the transformations which allow you to cover your hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So you have these data set.",
                    "label": 0
                },
                {
                    "sent": "Here you rotate it, it gives you a new different problem to identify.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me now come to an example to make it a little bit more concrete.",
                    "label": 0
                },
                {
                    "sent": "You have graph cut.",
                    "label": 0
                },
                {
                    "sent": "Here the hypothesis class are or cuts.",
                    "label": 0
                },
                {
                    "sent": "And here I have a pictorial description of what the solution is.",
                    "label": 0
                },
                {
                    "sent": "I just cut it in two different into different.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphs.",
                    "label": 0
                },
                {
                    "sent": "No, if you do that.",
                    "label": 0
                },
                {
                    "sent": "Here is again this graph cut solution and you permute.",
                    "label": 0
                },
                {
                    "sent": "Here is adjacency matrix with the solution vector.",
                    "label": 0
                },
                {
                    "sent": "Here if you permute your labels here, you have a different adjacency matrix in a different representation of your cut.",
                    "label": 0
                },
                {
                    "sent": "So the messages which I would like to actually communicate is the transformation.",
                    "label": 0
                },
                {
                    "sent": "Of this of this solution here.",
                    "label": 0
                },
                {
                    "sent": "And then I relat.",
                    "label": 0
                },
                {
                    "sent": "The spacing of these transformations to the fluctuations in my data and this gives me a precise criterion.",
                    "label": 0
                },
                {
                    "sent": "How I can estimate?",
                    "label": 0
                },
                {
                    "sent": "How precisely I can estimate the underlying hypothesis so I do these?",
                    "label": 0
                },
                {
                    "sent": "I apply these transformations in this case for graph cut.",
                    "label": 0
                },
                {
                    "sent": "These are permutations.",
                    "label": 0
                },
                {
                    "sent": "Apply them a number of times two to the N road different times and that is sort of my vocabulary to talk about statistically distinctive hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Now when I get now my test graph, I always need two types of instances.",
                    "label": 0
                },
                {
                    "sent": "If I get my test graph, the test graph is different I have.",
                    "label": 0
                },
                {
                    "sent": "Links added I now have some links.",
                    "label": 0
                },
                {
                    "sent": "Cut.",
                    "label": 0
                },
                {
                    "sent": "The adjacency matrix is different.",
                    "label": 0
                },
                {
                    "sent": "This would be the minimal cut here and now.",
                    "label": 0
                },
                {
                    "sent": "You can guess whether this minimal cut corresponds to this representation of to this representation and you see already it's much more similar to this solution to this one, and this is essentially the trick which we use for for for communicating the transformation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the protocol now in terms of sender receiver type of communication setting?",
                    "label": 0
                },
                {
                    "sent": "You have a sender, you have a receiver and rather than a channel I have a problem generator.",
                    "label": 0
                },
                {
                    "sent": "Now the problem generator gives me my training data X1 with the respective cost function to the sender and the receiver and they now have to establish a protocol so the sender minimizes that the receiver minimizes it and they agree upon a number of different transformations, Sigma one up to Sigma two to the NRO different transformations.",
                    "label": 0
                },
                {
                    "sent": "So these are the messages they can communicate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now during communication, what happens is the following process.",
                    "label": 0
                },
                {
                    "sent": "The sender selects one of these transformations and sends it to the problem generator.",
                    "label": 1
                },
                {
                    "sent": "Now the problem generator draws from the same probability distribution of the data.",
                    "label": 1
                },
                {
                    "sent": "A second data set X2.",
                    "label": 0
                },
                {
                    "sent": "It applies these transformation to X2.",
                    "label": 0
                },
                {
                    "sent": "Sigma S is applied now 2X2 and and the problem generator sends these new data with their transformation to the receiver.",
                    "label": 1
                },
                {
                    "sent": "Now the receiver has the problem that on one hand the fluctuations are different.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, he doesn't know what the transformation is.",
                    "label": 0
                },
                {
                    "sent": "So if the transformations are spaced very far away in the hypothesis class and the fluctuations are small, then you have no problem with able to identify that.",
                    "label": 0
                },
                {
                    "sent": "If the transformations are very fine spaced and the fluctuations are large, you will constantly confused what's going on.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the mechanism which I used to identify.",
                    "label": 0
                },
                {
                    "sent": "How much you can communicate so in the last step, the receiver has to guess what the transformation was, and that allows you for this.",
                    "label": 0
                },
                {
                    "sent": "For this communication protocol, to calculate an error rate and the error rate is whenever Sigma had is not equal to Sigma S. Now what we have to do is in order to make this call.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if we have to measure this error rate, So what the what the receiver now has to do is to compare sets of hypothesis.",
                    "label": 1
                },
                {
                    "sent": "I'm now telling you what the decoding procedure is based on this decoding procedure.",
                    "label": 0
                },
                {
                    "sent": "We can calculate the error rate.",
                    "label": 0
                },
                {
                    "sent": "So I need a mapping which actually allows me to compare these two different hypothesis generated on X1 and data X2.",
                    "label": 0
                },
                {
                    "sent": "Never mind you can identify for all the cases I I'm discussing this side with an identity mapping and the decoding goes in the following way.",
                    "label": 0
                },
                {
                    "sent": "The approximation set on the second data set X~ two.",
                    "label": 0
                },
                {
                    "sent": "That's what the receiver actually receives.",
                    "label": 0
                },
                {
                    "sent": "Is intersected with an approximation set where the training data have been transformed with the transformation Sigma and you maximize over all possible sigmas.",
                    "label": 0
                },
                {
                    "sent": "So you try to find the transformation which makes the approximation set.",
                    "label": 1
                },
                {
                    "sent": "There's a receiver has calculated as similar as possible to the approximation sets.",
                    "label": 0
                },
                {
                    "sent": "Which you have in your in your in your possible set of transformations.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the decoding procedure.",
                    "label": 0
                },
                {
                    "sent": "That's how I calculate my Sigma head.",
                    "label": 0
                },
                {
                    "sent": "Now what you have to do is what we basically have now have to calculate what is the error rate under this protocol.",
                    "label": 0
                },
                {
                    "sent": "Anet will sensitively depend how many of these transformations an I have chosen an where I have I have a disk.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then so let me introduce some sets.",
                    "label": 0
                },
                {
                    "sent": "The joint approximation set, Delta CJ is the intersection where the sender has chosen Sigma J, but the sender has chosen Sigma J and that has been applied by the problem generator to my test data.",
                    "label": 0
                },
                {
                    "sent": "But you compare it with the transformation Sigma J here on the.",
                    "label": 0
                },
                {
                    "sent": "And the training data.",
                    "label": 0
                },
                {
                    "sent": "So this is this intersection set and the error events are when the transformation Sigma J and Sigma S are different compared to the case where I have correctly identified as the best overlapping approximation set with the transformation Sigma is so whenever Delta CJ is larger than the cardinality of Delta, CSI will decode according to my decoding protocol I will.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dinero so how do you calculate now these errors you have to you have to determine the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Sigma head is not equal to Sigma S my message and you do that by using the Union bound and then you have to calculate the probability that this cardinality is larger than this cardinality here and at that point for the clustering applications in particular.",
                    "label": 0
                },
                {
                    "sent": "Also the K cut application.",
                    "label": 0
                },
                {
                    "sent": "We use random transformations of Sigma.",
                    "label": 0
                },
                {
                    "sent": "We just use random permutations.",
                    "label": 0
                },
                {
                    "sent": "Is our code messages in an analogy to Shannon's random coding argument?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me just skip through that what you can do is you can approximate these.",
                    "label": 0
                },
                {
                    "sent": "These probability of error by a quantity I gamma and this quantity I gamma is essentially a mutual information between these approximation sets.",
                    "label": 1
                },
                {
                    "sent": "The random transformation decouples the intersection between two approximation sets and that's why I have here the product of two cardinalities, whereas in the denominator you still have this intersection of the statistically coupled.",
                    "label": 0
                },
                {
                    "sent": "Two approximation sets when you use the same transformation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you take the limit of these error rate and you require that you are communicating with 0 error, you have the condition that your rate row for this for this communication scenario should be smaller than this mutual information which comes out the logarithm of of these cardinalities gives you three different terms.",
                    "label": 0
                },
                {
                    "sent": "When you analyze that the first term is the entropy on the receiver side on the sender side, the second term is the entropy on the receiver side.",
                    "label": 0
                },
                {
                    "sent": "And the third term, which is negative here, is the joint entropy, and this gives you the mutual information between these approximation sets on the sender and the receiver side.",
                    "label": 0
                },
                {
                    "sent": "So again, for communication, I have to maximize mutual information and that gives me the maximal amount of of rate.",
                    "label": 0
                },
                {
                    "sent": "In order to have.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "0 error zero error communication.",
                    "label": 0
                },
                {
                    "sent": "Now how can I use that to actually rank different cost functions?",
                    "label": 0
                },
                {
                    "sent": "The ranking of different cost functions is now done in the following.",
                    "label": 0
                },
                {
                    "sent": "I have these protocol which basically allows me to measure how precisely a particular cost function can resolve my hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So if I use this communication protocol and you have your cost function an I have my cost function, we just measure according to that protocol how many bits.",
                    "label": 0
                },
                {
                    "sent": "We can get across this noisy channel, which is which is the problem generator and both cost functions are exposed to the same fluctuations in the data.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have a robust if you want to have a robust model, basically what you have to do is you have to have a rich model which on the other hand is also very insensitive to the fluctuations apedale separation between fluctuations and signal will give you a higher communication rate and that is the ranking criterion which I'm using.",
                    "label": 0
                },
                {
                    "sent": "For finding out what the good models are.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you might say, how can you kind of?",
                    "label": 0
                },
                {
                    "sent": "Can you test whether this is a reasonable idea?",
                    "label": 0
                },
                {
                    "sent": "We sort of tried out the simplest case in order to embed this new theory or bring it in contact with information theory and what you do for that is, you minimize the Hamming distance when you when you have a hypothesis class of bit strings, and in that case when we compare bit strings and behind the random.",
                    "label": 0
                },
                {
                    "sent": "Coding scenario of Shannon.",
                    "label": 0
                },
                {
                    "sent": "We basically recover under the extreme aliti of that mutual information formula formula.",
                    "label": 1
                },
                {
                    "sent": "We recover the capacity of the binary symmetric channel.",
                    "label": 1
                },
                {
                    "sent": "So in a setting which corresponds to Shannon's random coding scenario, this new theory allows you to recover the cases from information.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theory.",
                    "label": 0
                },
                {
                    "sent": "Now, when you actually want to use it for estimating mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "If I if I want to use it for recovering mixtures of Gaussians, I can I can basically use the K means cost function have an approximation control in terms of a temperature and if I lower the temperature when I have two Gaussian sources and I have a number of different hypothetical means for my K means problem, I see such a such a bifurcation pattern.",
                    "label": 0
                },
                {
                    "sent": "If you now ask what is the generalization error, that's basically the costs which you have to pay for when you take a solution on the first data set, and you apply it to the second data set.",
                    "label": 0
                },
                {
                    "sent": "Then, depending on this on this regularity of your hypothesis class on this temperature, this cost has a minimum here, and this minimum exactly coincides with a maximum of that mutual information.",
                    "label": 0
                },
                {
                    "sent": "So the criterion which I derive gives you exactly the right model in in this context.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you are if you are.",
                    "label": 0
                },
                {
                    "sent": "If you have 500 data points and you are in handle dimensions and the data comes from 2 Gaussians but you but you invest for Gaussians in your hypothesis class and you vary the temperature, then you see for a large temperature regime you get 2 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Then for a very tiny one in that green area you get 3000.",
                    "label": 0
                },
                {
                    "sent": "Then you get for Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So what is the mutual information telling you?",
                    "label": 0
                },
                {
                    "sent": "The mutual information tells you that you should exactly stop here.",
                    "label": 0
                },
                {
                    "sent": "This is the minimum of the transfer cost, so to speak.",
                    "label": 0
                },
                {
                    "sent": "The generalization error and that means also in this case you get the right model order for this in our.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real.",
                    "label": 0
                },
                {
                    "sent": "So if you are in very high dimensions and you have very few data points you actually see for estimating two Gaussians phase diagram where you have an unsplit phase, you have a split phase where you actually estimate precisely the two Gaussians and you have a random phase where you estimate two Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But their means have nothing to do with the data source.",
                    "label": 0
                },
                {
                    "sent": "Under these circumstances you also see that you can only that you only can code in that.",
                    "label": 0
                },
                {
                    "sent": "Ordered phase but not in the unsplit phase here.",
                    "label": 0
                },
                {
                    "sent": "You have no structure, and here you have random structure.",
                    "label": 0
                },
                {
                    "sent": "Random structure is not really.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you don't get any anything for that.",
                    "label": 0
                },
                {
                    "sent": "Let me come to the to the final example.",
                    "label": 0
                },
                {
                    "sent": "So far I have talked about discrete optimization and how you regularize here is a situation where I have a Boolean matrix.",
                    "label": 0
                },
                {
                    "sent": "We took that from some applications in security and it's very noisy.",
                    "label": 0
                },
                {
                    "sent": "Now you want to denoise via low rank approximation to calculated by SVD.",
                    "label": 0
                },
                {
                    "sent": "So here you have a rank 5 approximation.",
                    "label": 0
                },
                {
                    "sent": "If you then denoise by rounding, this is what you get.",
                    "label": 0
                },
                {
                    "sent": "Now how can you find the optimal rank for this?",
                    "label": 0
                },
                {
                    "sent": "For these denoising experiment we use again the same scenario we use SVD as a coding way.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do that and you calculate what the approximation capacity is for the optimal beta, it has a clear maximum here, and if you look what the what, the clear maximum corresponds to as a function of rank.",
                    "label": 0
                },
                {
                    "sent": "It corresponds exactly to the point where you have the minimum reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "So that means this criterion allows you also to precisely cut off the spectrum of SVD under this noise source.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And probably running out of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we also applied this in a relatively complicated scenario.",
                    "label": 0
                },
                {
                    "sent": "For role based access control, this is a problem in C.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Security.",
                    "label": 0
                },
                {
                    "sent": "It basically relies on a matrix decomposition which.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which then gives you also the same type of ranking for model order selection.",
                    "label": 0
                },
                {
                    "sent": "So I don't.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To go into that.",
                    "label": 0
                },
                {
                    "sent": "Except that also in this, in this scenario, when you when you when you pick according to the generalization error, the approximation capacity, the maximum of the approximation capacity, you'll stop temperature.",
                    "label": 1
                },
                {
                    "sent": "You get very low.",
                    "label": 0
                },
                {
                    "sent": "Generalization error, so as a model selection criterion this this concept allows you to select the model order precisely in these in these.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this setting, so let me conclude by saying the following noise in the data basically quantizes the mathematical structures which are involved here.",
                    "label": 0
                },
                {
                    "sent": "You have hypothesis classes.",
                    "label": 0
                },
                {
                    "sent": "You have different choices for your metrics and so on.",
                    "label": 0
                },
                {
                    "sent": "This scenario allows you to quantify how these different mathematical structures are actually sensitive, how they are influenced by noise, and whenever they are sensitive to noise.",
                    "label": 0
                },
                {
                    "sent": "Then they will reduce the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The capacity of this of this channel that means you can project out those those models, whether you parameterized them with different hypothesis class, is obvious.",
                    "label": 1
                },
                {
                    "sent": "Different cost functions in a way that they are noise robust, since the noise quantizes these structures, you have symbols and you use them for coding, so that's the underlying idea.",
                    "label": 1
                },
                {
                    "sent": "If you have the criterion of error free communication then this determines for every cost function.",
                    "label": 0
                },
                {
                    "sent": "And approximation capacity and you can.",
                    "label": 0
                },
                {
                    "sent": "You can basically rank these cost functions according to how high their approximation capacity.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what will be future work?",
                    "label": 1
                },
                {
                    "sent": "First of all, I would like to explore more in detail.",
                    "label": 0
                },
                {
                    "sent": "The this is an asymptotic theory because I use asymptotic arguments for for determining the channel capacity.",
                    "label": 0
                },
                {
                    "sent": "I'm still working on a lower bound for these for this argument, but according to the.",
                    "label": 0
                },
                {
                    "sent": "Theory of jointly typically code words in information theory.",
                    "label": 0
                },
                {
                    "sent": "You can actually use the same type of techniques.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "We are in the process of applying it to model reduction for dynamical systems.",
                    "label": 1
                },
                {
                    "sent": "Again, you would like to know in a set of ODS or PDS, how precisely you should regularize them by approximating them, and this coding scenario should also be able to tell you what the right approximation quality is, and I would like to conclude with a statement which I think is.",
                    "label": 0
                },
                {
                    "sent": "Is at the core of machine learning and how it interacts with computer science.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "In all of these experiments, in all of these studies, the statistical complexity has to be related in some way to the computational to the algorithmic complexity, and the challenge is to find out whether you, on the statistical side, can still find predictive models by your approximation by your best possible approximation quality in such a way that you find them efficiently.",
                    "label": 0
                },
                {
                    "sent": "So, since since I'm working.",
                    "label": 0
                },
                {
                    "sent": "A lot in practically in applied problems the question the user most often asks is.",
                    "label": 0
                },
                {
                    "sent": "Is your model robust?",
                    "label": 0
                },
                {
                    "sent": "Now this theory allows to quantify precisely what robustness means, and it it trades of robustness against complexity.",
                    "label": 0
                },
                {
                    "sent": "Richness of the description, and so as a constraint we want to have good generalizing models which at the same time I efficiently computable and let me close with that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Formation capacity so yeah.",
                    "label": 0
                },
                {
                    "sent": "This thing comes in here so that.",
                    "label": 0
                },
                {
                    "sent": "Very busy or something similar.",
                    "label": 0
                },
                {
                    "sent": "Well the theories so far is at the level of random coding theory, so I use random permutations for covering my hypothesis class, whereas in communication theory you do not want to randomly cover your bid space, you want to have an efficient decoder.",
                    "label": 0
                },
                {
                    "sent": "But I mean that even right now causing you to work in a large dimensional space, yes.",
                    "label": 0
                },
                {
                    "sent": "So the dimensionality comes from the from the from the set of transformations you have to, you have to consider.",
                    "label": 0
                },
                {
                    "sent": "So when I applied this theory to the to the Hemming to minimizing the Hamming distance, I took this one bit string and I randomly permuted these bits to get my codebook vectors.",
                    "label": 0
                },
                {
                    "sent": "So I was basically using the theory of types to generate a coverage of my of my hypothesis class and in the same way I do that for discrete optimization problem, except that the bitstring now is the minimizer of that one problem.",
                    "label": 0
                },
                {
                    "sent": "Which I shift around in the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}