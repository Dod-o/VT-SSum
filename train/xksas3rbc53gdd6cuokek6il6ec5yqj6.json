{
    "id": "xksas3rbc53gdd6cuokek6il6ec5yqj6",
    "title": "Efficient Computation of Recursive Principal Component Analysis",
    "info": {
        "author": [
            "Alessandro Sperduti, Department of Pure and Applied Mathematics, University of Padua"
        ],
        "published": "Jan. 28, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/ecml07_sperduti_ecr/",
    "segmentation": [
        [
            "To give you an idea."
        ],
        [
            "Of how festival structured information is quite important and I guess everyone agrees that there are many applications where data comes naturally with the structure, so it's important to use such structure.",
            "Then I will introduce PCA for structures and I will try to explain the basic concept at least for sequences.",
            "So to give you basic idea of what I'm working on.",
            "And we also discussed the case of trees and graphs, but mainly giving the final result and then I will go to the contribution of the paper of this presentation, which is about efficiency.",
            "Because when you have structured data then it's quite easy to have lots of data.",
            "The complexity is going much higher, so it's very important to be able to have techniques which are efficient.",
            "So that we all vote data can be used and also the techniques.",
            "So I will try to push your analysis of where efficiency can be gained and I will propose some techniques to increase efficiency in computing PCA for."
        ],
        [
            "Structures.",
            "So many awards involve structured data, so basically."
        ],
        [
            "Only an object is composed of different atomic components."
        ],
        [
            "And there are relationships which connect."
        ],
        [
            "The atomic components and these relationships and also the atomic components can be decorated with symbolic and or not."
        ],
        [
            "Recall information, so there are."
        ],
        [
            "Any applications so chemistry so more actuals in chemistry can be represented by molecular graph, where the principle information attached to the atomic components is the name of the item.",
            "But you can also add some numerical information regarding chemical fish."
        ],
        [
            "Color, property and of course the first talk was mentioning XML trees where you have to."
        ],
        [
            "Send this very natural description of XML documents as trees or also natural language processing where you may have passed reason.",
            "Then the problem may be to try to understand, for example, what is the semantic role of each portion of the graph.",
            "So many applications and many problems when you try to address these kind of problems because you don't want to discount the information about structure, which is the usual way.",
            "Of doing it.",
            "So basically when you process structure with traditional approaches you do a preprocessing of the structure.",
            "You extract some apio in defined structural feature and then you apply standard neural networks, upper vector machine and probabilistic methods everything.",
            "But if the processing has been done not in a proper way, if you are losing some info."
        ],
        [
            "Mission, then your loss.",
            "OK, so PCA is quite interesting and important approach for vectors since basically they define the direction of maximum variance of a set of vectors.",
            "So it's actually they are trying to capture the information in the data so they are orthogonal directions and follow maximum variance of the data.",
            "So they can use to compress information.",
            "So instead of representing the full data sets, you can project the data set in some principle components, and so hopefully you retain much of the information in the data set, so it's used the impact recognition to reduce the dimensionality of the data or to remove noise, and also is important for machine learning application, because this is going to reduce the dimensionality of the data and so the space of free parameters that you need to.",
            "A tune using data, so it's important in order to avoid overfitting, blah blah blah and all this."
        ],
        [
            "Stuff so OK, now we want to deal with more complex structures like sequences.",
            "I will represent sequences as Lisanna computer scientist.",
            "So I have deformed in this from this point of view, but I would say it's fair representation, so this is going to be the item at times ego item at time one, and so on.",
            "OK, then we may consider trees, darks and also on directed graphs.",
            "For example in chemistry.",
            "Chemical compound can be represented by a directed graph where you have also annotations on edges, which is the balance update bond.",
            "So PCA for structures, why it may be useful for data structure becausw.",
            "The idea is that if we were able to define PCA for structure, then it would it be possible to embeds very different objects of different size and shape in a vectorial space, and then if we are able also to have a small vectorial space, then this is quite retaining as much information as possible.",
            "Now this is important because we can.",
            "Apply all the standard techniques we know for Victoria space for clustering, classification or regression and so on.",
            "Same supervised learning."
        ],
        [
            "It can be applied so in the past there were some proposals, mainly involving sequences.",
            "And please, for example for sequences the idea was to try to encode sequence through a linear dynamical system where XT is the input of time T&Y T -- 1 is the story at time T9, one of the inputs so.",
            "40s are these basically an extension of these dynamical systems?",
            "But all these proposal where not really guaranteeing the computation of the principle components, so they were iterative techniques, not proof of convergence, and so also they seems not to scale very well.",
            "So as soon as you have more than 100 sequences or trees.",
            "No deeper than 3 levels then difficult to learn this kind of."
        ],
        [
            "Solutions so is it possible to define a direct procedure for computing PCA for structure?"
        ],
        [
            "Well, I did this proposal.",
            "So the idea is first of all we need to decide.",
            "OK, tag and that should be matching order 2.",
            "States that we are actually doing something similar to PCA with factors, so this is called the data encoding."
        ],
        [
            "Property I will discuss this much more details in the following.",
            "Then the idea is to define a linear machine.",
            "Basically the nonlinear dynamical system which is operating in extended state space.",
            "So the idea is OK. We want to reduce dimensionality.",
            "But in order to do that the trick is first of all to forget about this target.",
            "So first of all we build.",
            "Linear machine which is able to process the full info."
        ],
        [
            "Mitchell and the only after death, the idea is to consider all the eigenstates of this extended state space, and then we can compress the state space using standard PCA and then combining this with some.",
            "My pictures, which are defined on the extended linear machine."
        ],
        [
            "The Weeknd compose all these components and get our solution."
        ],
        [
            "OK, so let me give you an example for sequences so the reference model is this one.",
            "So we have linear model.",
            "This is the input of time T and this is a metrics which is transforming the inputs and this is the state at time T -- 1 and this is metrics which is transforming their state space.",
            "So the basic idea is that I'm kieu with the new information.",
            "You transform it and you combine it with the transformed istorie till time T -- 1 and then you get the new state space at time T. OK, so basically what we want to do is to define my pixels.",
            "These two metrics in such a way that the information which is contained in this state space is as preserved as.",
            "A possible, which means that if we feed this dynamical system with a sequence of inputs, then starting from the state space, it should be possible to reconstruct backwards recursively the encoded sequences using the transpose of these two metrics is just like with PCA.",
            "When you project in a subspace.",
            "Then if you want to reconstruct the original inputs, you transform.",
            "Eat with the transpose.",
            "There is also another issue for using the transpose.",
            "Here is that in this way you don't need to learn other parameters, so the parameter stays the same and it's also very stable from an American point of view because we have not to invert matrixes so it's quite robust, so the decoding requirement is actually telling you that if you have back to state at time T. And you multiply for the transpose of the X metrics.",
            "Then you get the input of Time Team.",
            "If you do the same with metrics, which is us with the transpose of the matrix which is associated with the E story, then you get the instigating T -- 1, then recursively you can get this vector and the codes the input assigned T -- 1 and this starts at time T -- 2 and so on.",
            "So what we want to do is to find these two matrixes of the smallest size.",
            "Such that the reconstruction arrow, so the inputs assigned T and the reconstruction reconstructed inputs at time T but backwards from the end backwards to T is minimized OK?"
        ],
        [
            "So it's possible to find an exact solution, which means that it's possible to find that 0 error solution, just noting that if we substitute the equation for the state vector inside the two equations so that I was discussing before the reconstruction requirements, then we end up having these two equations and so it's not difficult to see that this.",
            "Our equation, which constitutes a sufficient conditions for these two equations to be true.",
            "So this means that basically if we consider the transformed inputs and the transformed history, then these two vectors must be orthogonal.",
            "So means that in order to retain all the information you need to project the current input in a subspace which is different.",
            "I mean, it's Octogonal today information where you saw the story.",
            "OK, so let me just recall that this BT is the transformation of the input Z 30 is the transformation of."
        ],
        [
            "Face space and the sum of them is the new state OK?"
        ],
        [
            "OK, then what we can do is OK. Now the problem is how can we find a state space which is small enough to be useful and also preserving as much information as possible.",
            "So the trick here is OK, first of all, let's forget about the fact that you want a small space we just designed by NTH space which is.",
            "So large that we can store all the Aga sequences up to the maximum length of our sequence.",
            "OK, so the idea is to consider a state space where the components are partitioned in subsets.",
            "Every subset is actually encoding an input OK, so the input is a vector, so if they.",
            "The vector has dimension K. Let's say OK here I have a space which is partitioned in subset of K components.",
            "OK, so the first K components are used just doing codes they input.",
            "All the other components are used to store historical information.",
            "OK, so the second subset is storing the information time T -- 1 and so on OK?",
            "So then our state space is just given by the sum of these two vectors which are octogonal.",
            "So we are storing the input information in a subspace which is orthogonal with respect to the information in the story, and so if we recall that this vector is obtained by this transformation and if we look at the definition of these vectors is scared that these metrics here is.",
            "Just shifting everything on the right of K components.",
            "OK, so this is just a pushdown linear operator.",
            "If you understand this as a stuck."
        ],
        [
            "OK.",
            "So we can define.",
            "Very easy way of doing this.",
            "Shift.",
            "Just metrics like that.",
            "And then also recalling that the vector DT is transformation of inputs."
        ],
        [
            "This is just what is doing.",
            "This is just projecting the input in the first K components of our space."
        ],
        [
            "So.",
            "We just have this very simple definition for the metrics.",
            "So what I have reached now now I have a definition for this, map it X which satisfy the decoding data decoding property and.",
            "But the problem is that now this is 2 large dynamical system."
        ],
        [
            "So that week now is to collect all the states vectors of this very large system as Rosina metrics and then basically I compute the PCA of these metrics.",
            "So just to give an example, suppose that's I have this sequence in inputs at least, so I."
        ],
        [
            "After reading the first symbol and then put it in the goal.",
            "So now this is a state becauses this state representing the input in first position without any history.",
            "OK. Then, if I want to read the second item, the machine what is doing is shifting these state space to the right."
        ],
        [
            "And inserting the new input."
        ],
        [
            "In first position, so the new state is this one."
        ],
        [
            "And so on, so it keeps."
        ],
        [
            "Shifting."
        ],
        [
            "Inserting"
        ],
        [
            "15 and"
        ],
        [
            "Inserting"
        ],
        [
            "Shifting adding 13 OK Now if I."
        ],
        [
            "Other another sequence I want to collect different sequences and compute the PCA of all the sequences.",
            "I keep doing that."
        ],
        [
            "Why we do the input?",
            "This is the state."
        ],
        [
            "Space then I shift."
        ],
        [
            "And I serve."
        ],
        [
            "Achieve."
        ],
        [
            "And."
        ],
        [
            "Insert I shift."
        ],
        [
            "And I insert so these are all the states of my dynamical system.",
            "When reading these two sequences.",
            "So what can we do now?"
        ],
        [
            "How is to just compute the principal components of these rows and then we get the compression so we get a new way of representing the states in a more compact, compact way.",
            "And if we preserve all the components so that we have a perfect reconstruction reconstruction of this."
        ],
        [
            "State space we can redefine our dynamical system in this way, so getting much smaller then amical system which is doing the job OK.",
            "I hope at least that I give you an intuition or."
        ],
        [
            "Of what is going OK, so the same click can be played with trees with trees we have a different dynamical system where instead of adding just one history we went.",
            "We have one nice story for each possible child, so we have more matrixes and Step 2 is a bit more involved, cause for sequences you don't have much explosion in possibilities because you have only.",
            "Unordered set of items.",
            "Foltys you may have an exponential explosions of possibilities because you have all the different branching at each nodes.",
            "So basically just using this streak encoding apriori all possible paths is not going to work.",
            "Going to have two large extended system, so the trick is to just use parts which are.",
            "Well, actually present in your data, so in this way you avoid the explosion and then we also need of course to define array push operator for each possible child.",
            "So with PC is really more tricky to do everything, but it's possible to do it and the solution is similar to the."
        ],
        [
            "A sequence case it's also possible.",
            "Partially to address graphs, there are two main problems with grass festival cycles, so if you have cycles, your dynamical system may not converge to a stable position.",
            "Stable solutions, and even if you are able to have stable dynamical systems, the problem is that when you decode state vector photographs, so you generate a new vertex and you don't know whether this is a new vertex or an old vertex that is reached by another.",
            "Age by a different path.",
            "So what we proposed here was to according Creek.",
            "Basically.",
            "The idea is when a numerate the set of vertexes following a given convention and then representing either director or undirected graph as an inverted, although at least where there is information."
        ],
        [
            "Bought concerning the label attached to the vertex and the edges which are entering that vertex.",
            "So in this way is basically possible to encode the information about the graph, even if following a specific visit of the vertexes.",
            "So I'm not claiming that the problem is fully solved.",
            "Otherwise I would solve.",
            "I would demonstrate that P is equal to NP, so of course this is not going to be possible, But the idea is that when you choose.",
            "A conventional way of investigating the vector exists, then you fix a view of your graphs and you can do principal component analysis.",
            "According to this view.",
            "So there are some technical reasons forward thinking for defining the shift operator in this way.",
            "I don't want to go through all this, but basically again you get solution which is very similar to the."
        ],
        [
            "Out of cases.",
            "OK, now computational problem.",
            "So basically PCA is doing a angling engine analysis of the correlation or covariance covariance matrix of the X metrics, and so this can be computationally quite demanding when you have a large data set.",
            "So there are two problems of code space.",
            "The axe metrics can be quite large.",
            "You have one row for each single component.",
            "For each vertex of the data set, then several columns for each element of the encoding scheme depends on how you encode your input items.",
            "So if you suppose that every item is encoded by K bits, then for sequences you have K bits for each time step.",
            "For trees you have K columns for each path, which is present in the data and program director graphs we have less than K bits because there are some optimization can be done.",
            "But still I mean this number are quite large important, so it's very important to try to reduce the dimension of the X matrix and time.",
            "Of course, performing the computation of PCA is more than quadratic with the size of the metrics, so if we are able to shrink the size of the matrix X, we are also able to save alot."
        ],
        [
            "Out of time in computation.",
            "So let's consider the structure of the metric facts.",
            "So the number of columns which we said already is the timing by two factor.",
            "The sides of the representation of each possible component and their adopted structural encoding scheme.",
            "Second observation, if the components are finite and discrete, then it is quite probable that different substructure different structure will have one or more components in common.",
            "So you have you may have sub trees which are shared by many trees and so on.",
            "So this means if this happen means that rose in X that correspond to this common components will be identical.",
            "So you may take advantage of that.",
            "And so we can exploit this by removing the identical goals.",
            "So to reduce the size of the metrics.",
            "But we need also to do a small trick to use a small trick in order to consider the fact that we have replica of rules.",
            "And of course, it's also possible to exploit the smallest dimension of the Matrix X.",
            "So of course these are linear space, so the rank of the matrix is equal to the minimum minimum dimension."
        ],
        [
            "Only two, so this can be exploited.",
            "So basically the three observation turned out to be the reason for defining these three techniques.",
            "The first Wally one is defining a minimal state space.",
            "This actually allows you to save both space and time.",
            "Second one, representing unique substitute by substructure by minimal dog also saves you space and time and then the last one.",
            "You can exploit QR decomposition.",
            "This is mainly saving time."
        ],
        [
            "Not much space.",
            "OK, first approach first technique defining a minimal state space.",
            "So what does it mean?",
            "So the idea is that for each structural component, only items which occur associated to it in the data set are explicitly represented, so this is equivalent to remove zero columns from my ex.",
            "So let's consider the previous example.",
            "So we had these two sequences and this was the X matrix.",
            "Here you have all zeros.",
            "Basically because there is no.",
            "Information, so if you look at the items and how they approve in the different position, you see that in the first position all the symbols ABCD or code.",
            "OK, so you need to represent all of them.",
            "So you need to discriminate all of them.",
            "So if you want to have every symbol to be presented without any assumption about the metrics between symbols, you need to have one bit.",
            "On for each one of these symbols, all the others 01 N encoding.",
            "So means that you need 4 bit snow that we present all the events which are occurring at position one.",
            "OK if you look at the second column you see that this is never reaching position to so you don't need for beats.",
            "You just need 3 bits.",
            "The same here.",
            "Here you need just to beat San.",
            "Here you need just one bit becausw never.",
            "The last position of your memory of your stock will need to be present BC or D so you don't need to use all the possibilities.",
            "You just need to add the possibility to be presented in a.",
            "Is there OK?",
            "So of course all this is data dependent according to the data you have, then you get a different."
        ],
        [
            "Distribution, yeah.",
            "Second key that he is OK, let's suppose that we are encoding the trees.",
            "Then we can use quite efficient algorithm for compressing the forest in an adult, taking also in consideration the frequency of the substructure while the frequency is important because there is this property that we can exploit the fact that if you have a metric, say.",
            "We chase all these columns which are different each other and they have this multiplicity in a metric, say.",
            "Then if we consider a metrics where all the columns appear only once and you multiply for the square of the frequency, then they have the same correlation or covariance metrics.",
            "So this means that they have the same principle components.",
            "So, but the dimension of this matrix is much."
        ],
        [
            "Smaller.",
            "Lost QR decomposition.",
            "So basically according to which dimension, the number of goals or number of columns is the smallest one we can select the metrics, so that is more efficient to compute a QR decomposition of the matrix and then performing an SVD decomposition of the metrics so."
        ],
        [
            "It's possible to save some time, so I am going to present the experiments on data coming from chemistry and trees generated by this contract."
        ],
        [
            "Free grammar, so basically for graphs data set is 394 graphs and for total or 80,000 items the extended stay space is 3115.",
            "For the grammar there are 421 distinct parts, three over total of bit less than 50,000 nodes with 1193 different parts so.",
            "Just to be an idea, they maximum sites here is 70 autumns averages 24 atoms and here you may have up to 90 nodes and the depth maybe up to 19, so there may be quite large, so OK, so let's fix the world.",
            "See what's happened when we compute the minimal state space.",
            "So from 3115 we can we use for the graphs the state space.",
            "1500 and the 40s from 7000 to roughly less than 3000, so using the first technique we were able to reduce the number of columns.",
            "So if we try to compute SVD from the full matrix, then this is the number of seconds which are needed, so it for this architecture for this computer.",
            "And if we compute first the.",
            "While the composition, then the SVD, you see that there is some improvement in time.",
            "However, the largest improvement is if we also consider substructure which are repeated.",
            "So this means that if we consider the second technique, which is computing a graph which are shared, then we can reduce the number of rules.",
            "And this means that we can get a quite drastic reduction in computation, especially for trees where there are many subkeys in common and also applying the QR decomposition.",
            "You see, we can still gain something.",
            "So this is the starting point.",
            "And these are the final time.",
            "So you see that the first sample in the case of trees, it's possible in 63 seconds to perform the principal components to compute the principal component."
        ],
        [
            "Of this data set, so summary PCA of structure is interesting because it's going to allow us to device reduce the representation vectorial representation which can be used for any kind of machine learning approach which I defined on vectors or whatever in order to be applicable in practice, computation must be efficient, so the main contribution of this paper of this presentation was try to show.",
            "That there are some savings which can be done.",
            "The problem is when we have data with vocabulary which is quite large.",
            "In that case, if you don't want to do assumption about similarity of symbols like words and so on, then the input size may be too large, and so it might be very difficult to compute PCA.",
            "So there's much work to do in this 30 OK.",
            "Thank you very much.",
            "This factor that explains the most part of viruses.",
            "So what happens when you have this?",
            "Reducing space on reduces."
        ],
        [
            "This component or factor and added as exploratory data analysis is really difficult because we can reconstruct what it does mean, so they provisional results well.",
            "The first of all you have a linear system, so this means that if you for example.",
            "Compose one structure in the state space in different vectors which are octogonal.",
            "Then you can compute the reducer representation of each and combine the reducer representation and then you get this reducer representation for the full structure.",
            "So this means that there is really the possibility to interpret the data, but also you can use any, for example factor analysis.",
            "Uh, for the X metric so you don't need to use PCA, so any technique which is linear and which is using the X metrics can be applied and so."
        ],
        [
            "Are they using these?",
            "I mean, the only important thing is that these metrics is isometrics is not annoying your transformation.",
            "So if you prefer some other technique which is linear, then you can apply it.",
            "Magic.",
            "So long as we can also compute a special.",
            "So and also we can be used.",
            "So my question here is worsening ship between those of broad spectrum and so usually you apply the Laplacian and so on to a single metrics, not to a set of metrics.",
            "So the here the idea is to try to capture the variance in all the different graphs and basically the technique which."
        ],
        [
            "These apply to graphs.",
            "Is really inserting the adjacency matrix rule by rule inside the system dynamically?",
            "If we can close the browser pension for each run and then compare the difference between the other value on the web, maybe we can also well, not really because you are using a different representations, so here everything is put together and you focus on differences, so you are really capturing the variance in the representation of the graphs.",
            "And so I don't think that you are going to get the same result.",
            "Question.",
            "Potential loss of information where you marked on their linear days.",
            "Just wonder whether it is possible to.",
            "Transport space.",
            "Loss.",
            "Yes, because if."
        ],
        [
            "To keep all the principal components here so the subspace which is embedding although."
        ],
        [
            "The vectors this is just a linear transformation, so you get the full information and actually this technique is volunteering to give you the minimum only if you keep all the components.",
            "So I have no formal proof that if you remove some of the principal components then you get the small less arrow you can get.",
            "This is only four.",
            "You know this is.",
            "Stool softener cheese yes."
        ],
        [
            "No Sir there.",
            "Yeah is that 40s?",
            "Yeah, so the idea is that, for example whole trees.",
            "Every role here will be a part in your tree in your set of trees, and so you are actually encoding all the different parts and you can preserve the full information so.",
            "There's no loss information unless you drop some of the principle."
        ],
        [
            "And actually some preliminary work on using these representations in chemistry is giving us quite good results, so it's possible to improve a lot from a situation where you have the expected which is giving you some feature and you apply like kernel machines for prediction and the case where you don't need the expected, you just have the molecular graphs, you compute principal component of the molecular graph, in this case, prepping all the information.",
            "And applying the same machine that you applied here."
        ],
        [
            "And So what you get see is that quite significant improvements in prediction.",
            "Without the needs of going through the process of selecting features which are meaningful for the task and so on, and actually we were also really trying to reduce the number of components it was performing quite well even with not so many components.",
            "Of course, this might be dependent on the problem, so I'm not claiming that this is true in general.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give you an idea.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of how festival structured information is quite important and I guess everyone agrees that there are many applications where data comes naturally with the structure, so it's important to use such structure.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce PCA for structures and I will try to explain the basic concept at least for sequences.",
                    "label": 1
                },
                {
                    "sent": "So to give you basic idea of what I'm working on.",
                    "label": 0
                },
                {
                    "sent": "And we also discussed the case of trees and graphs, but mainly giving the final result and then I will go to the contribution of the paper of this presentation, which is about efficiency.",
                    "label": 0
                },
                {
                    "sent": "Because when you have structured data then it's quite easy to have lots of data.",
                    "label": 0
                },
                {
                    "sent": "The complexity is going much higher, so it's very important to be able to have techniques which are efficient.",
                    "label": 0
                },
                {
                    "sent": "So that we all vote data can be used and also the techniques.",
                    "label": 0
                },
                {
                    "sent": "So I will try to push your analysis of where efficiency can be gained and I will propose some techniques to increase efficiency in computing PCA for.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structures.",
                    "label": 0
                },
                {
                    "sent": "So many awards involve structured data, so basically.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only an object is composed of different atomic components.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are relationships which connect.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The atomic components and these relationships and also the atomic components can be decorated with symbolic and or not.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recall information, so there are.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any applications so chemistry so more actuals in chemistry can be represented by molecular graph, where the principle information attached to the atomic components is the name of the item.",
                    "label": 0
                },
                {
                    "sent": "But you can also add some numerical information regarding chemical fish.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Color, property and of course the first talk was mentioning XML trees where you have to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Send this very natural description of XML documents as trees or also natural language processing where you may have passed reason.",
                    "label": 1
                },
                {
                    "sent": "Then the problem may be to try to understand, for example, what is the semantic role of each portion of the graph.",
                    "label": 0
                },
                {
                    "sent": "So many applications and many problems when you try to address these kind of problems because you don't want to discount the information about structure, which is the usual way.",
                    "label": 0
                },
                {
                    "sent": "Of doing it.",
                    "label": 0
                },
                {
                    "sent": "So basically when you process structure with traditional approaches you do a preprocessing of the structure.",
                    "label": 0
                },
                {
                    "sent": "You extract some apio in defined structural feature and then you apply standard neural networks, upper vector machine and probabilistic methods everything.",
                    "label": 0
                },
                {
                    "sent": "But if the processing has been done not in a proper way, if you are losing some info.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission, then your loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so PCA is quite interesting and important approach for vectors since basically they define the direction of maximum variance of a set of vectors.",
                    "label": 1
                },
                {
                    "sent": "So it's actually they are trying to capture the information in the data so they are orthogonal directions and follow maximum variance of the data.",
                    "label": 0
                },
                {
                    "sent": "So they can use to compress information.",
                    "label": 0
                },
                {
                    "sent": "So instead of representing the full data sets, you can project the data set in some principle components, and so hopefully you retain much of the information in the data set, so it's used the impact recognition to reduce the dimensionality of the data or to remove noise, and also is important for machine learning application, because this is going to reduce the dimensionality of the data and so the space of free parameters that you need to.",
                    "label": 0
                },
                {
                    "sent": "A tune using data, so it's important in order to avoid overfitting, blah blah blah and all this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stuff so OK, now we want to deal with more complex structures like sequences.",
                    "label": 0
                },
                {
                    "sent": "I will represent sequences as Lisanna computer scientist.",
                    "label": 0
                },
                {
                    "sent": "So I have deformed in this from this point of view, but I would say it's fair representation, so this is going to be the item at times ego item at time one, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, then we may consider trees, darks and also on directed graphs.",
                    "label": 0
                },
                {
                    "sent": "For example in chemistry.",
                    "label": 0
                },
                {
                    "sent": "Chemical compound can be represented by a directed graph where you have also annotations on edges, which is the balance update bond.",
                    "label": 0
                },
                {
                    "sent": "So PCA for structures, why it may be useful for data structure becausw.",
                    "label": 1
                },
                {
                    "sent": "The idea is that if we were able to define PCA for structure, then it would it be possible to embeds very different objects of different size and shape in a vectorial space, and then if we are able also to have a small vectorial space, then this is quite retaining as much information as possible.",
                    "label": 0
                },
                {
                    "sent": "Now this is important because we can.",
                    "label": 0
                },
                {
                    "sent": "Apply all the standard techniques we know for Victoria space for clustering, classification or regression and so on.",
                    "label": 0
                },
                {
                    "sent": "Same supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can be applied so in the past there were some proposals, mainly involving sequences.",
                    "label": 0
                },
                {
                    "sent": "And please, for example for sequences the idea was to try to encode sequence through a linear dynamical system where XT is the input of time T&Y T -- 1 is the story at time T9, one of the inputs so.",
                    "label": 0
                },
                {
                    "sent": "40s are these basically an extension of these dynamical systems?",
                    "label": 1
                },
                {
                    "sent": "But all these proposal where not really guaranteeing the computation of the principle components, so they were iterative techniques, not proof of convergence, and so also they seems not to scale very well.",
                    "label": 1
                },
                {
                    "sent": "So as soon as you have more than 100 sequences or trees.",
                    "label": 0
                },
                {
                    "sent": "No deeper than 3 levels then difficult to learn this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions so is it possible to define a direct procedure for computing PCA for structure?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I did this proposal.",
                    "label": 0
                },
                {
                    "sent": "So the idea is first of all we need to decide.",
                    "label": 0
                },
                {
                    "sent": "OK, tag and that should be matching order 2.",
                    "label": 0
                },
                {
                    "sent": "States that we are actually doing something similar to PCA with factors, so this is called the data encoding.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Property I will discuss this much more details in the following.",
                    "label": 0
                },
                {
                    "sent": "Then the idea is to define a linear machine.",
                    "label": 1
                },
                {
                    "sent": "Basically the nonlinear dynamical system which is operating in extended state space.",
                    "label": 0
                },
                {
                    "sent": "So the idea is OK. We want to reduce dimensionality.",
                    "label": 0
                },
                {
                    "sent": "But in order to do that the trick is first of all to forget about this target.",
                    "label": 0
                },
                {
                    "sent": "So first of all we build.",
                    "label": 0
                },
                {
                    "sent": "Linear machine which is able to process the full info.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mitchell and the only after death, the idea is to consider all the eigenstates of this extended state space, and then we can compress the state space using standard PCA and then combining this with some.",
                    "label": 0
                },
                {
                    "sent": "My pictures, which are defined on the extended linear machine.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Weeknd compose all these components and get our solution.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me give you an example for sequences so the reference model is this one.",
                    "label": 0
                },
                {
                    "sent": "So we have linear model.",
                    "label": 0
                },
                {
                    "sent": "This is the input of time T and this is a metrics which is transforming the inputs and this is the state at time T -- 1 and this is metrics which is transforming their state space.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that I'm kieu with the new information.",
                    "label": 0
                },
                {
                    "sent": "You transform it and you combine it with the transformed istorie till time T -- 1 and then you get the new state space at time T. OK, so basically what we want to do is to define my pixels.",
                    "label": 0
                },
                {
                    "sent": "These two metrics in such a way that the information which is contained in this state space is as preserved as.",
                    "label": 0
                },
                {
                    "sent": "A possible, which means that if we feed this dynamical system with a sequence of inputs, then starting from the state space, it should be possible to reconstruct backwards recursively the encoded sequences using the transpose of these two metrics is just like with PCA.",
                    "label": 1
                },
                {
                    "sent": "When you project in a subspace.",
                    "label": 0
                },
                {
                    "sent": "Then if you want to reconstruct the original inputs, you transform.",
                    "label": 0
                },
                {
                    "sent": "Eat with the transpose.",
                    "label": 0
                },
                {
                    "sent": "There is also another issue for using the transpose.",
                    "label": 0
                },
                {
                    "sent": "Here is that in this way you don't need to learn other parameters, so the parameter stays the same and it's also very stable from an American point of view because we have not to invert matrixes so it's quite robust, so the decoding requirement is actually telling you that if you have back to state at time T. And you multiply for the transpose of the X metrics.",
                    "label": 0
                },
                {
                    "sent": "Then you get the input of Time Team.",
                    "label": 0
                },
                {
                    "sent": "If you do the same with metrics, which is us with the transpose of the matrix which is associated with the E story, then you get the instigating T -- 1, then recursively you can get this vector and the codes the input assigned T -- 1 and this starts at time T -- 2 and so on.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do is to find these two matrixes of the smallest size.",
                    "label": 0
                },
                {
                    "sent": "Such that the reconstruction arrow, so the inputs assigned T and the reconstruction reconstructed inputs at time T but backwards from the end backwards to T is minimized OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's possible to find an exact solution, which means that it's possible to find that 0 error solution, just noting that if we substitute the equation for the state vector inside the two equations so that I was discussing before the reconstruction requirements, then we end up having these two equations and so it's not difficult to see that this.",
                    "label": 0
                },
                {
                    "sent": "Our equation, which constitutes a sufficient conditions for these two equations to be true.",
                    "label": 1
                },
                {
                    "sent": "So this means that basically if we consider the transformed inputs and the transformed history, then these two vectors must be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So means that in order to retain all the information you need to project the current input in a subspace which is different.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's Octogonal today information where you saw the story.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just recall that this BT is the transformation of the input Z 30 is the transformation of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Face space and the sum of them is the new state OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, then what we can do is OK. Now the problem is how can we find a state space which is small enough to be useful and also preserving as much information as possible.",
                    "label": 0
                },
                {
                    "sent": "So the trick here is OK, first of all, let's forget about the fact that you want a small space we just designed by NTH space which is.",
                    "label": 0
                },
                {
                    "sent": "So large that we can store all the Aga sequences up to the maximum length of our sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is to consider a state space where the components are partitioned in subsets.",
                    "label": 0
                },
                {
                    "sent": "Every subset is actually encoding an input OK, so the input is a vector, so if they.",
                    "label": 0
                },
                {
                    "sent": "The vector has dimension K. Let's say OK here I have a space which is partitioned in subset of K components.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first K components are used just doing codes they input.",
                    "label": 0
                },
                {
                    "sent": "All the other components are used to store historical information.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second subset is storing the information time T -- 1 and so on OK?",
                    "label": 0
                },
                {
                    "sent": "So then our state space is just given by the sum of these two vectors which are octogonal.",
                    "label": 0
                },
                {
                    "sent": "So we are storing the input information in a subspace which is orthogonal with respect to the information in the story, and so if we recall that this vector is obtained by this transformation and if we look at the definition of these vectors is scared that these metrics here is.",
                    "label": 0
                },
                {
                    "sent": "Just shifting everything on the right of K components.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a pushdown linear operator.",
                    "label": 0
                },
                {
                    "sent": "If you understand this as a stuck.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we can define.",
                    "label": 0
                },
                {
                    "sent": "Very easy way of doing this.",
                    "label": 0
                },
                {
                    "sent": "Shift.",
                    "label": 0
                },
                {
                    "sent": "Just metrics like that.",
                    "label": 0
                },
                {
                    "sent": "And then also recalling that the vector DT is transformation of inputs.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just what is doing.",
                    "label": 0
                },
                {
                    "sent": "This is just projecting the input in the first K components of our space.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We just have this very simple definition for the metrics.",
                    "label": 0
                },
                {
                    "sent": "So what I have reached now now I have a definition for this, map it X which satisfy the decoding data decoding property and.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that now this is 2 large dynamical system.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that week now is to collect all the states vectors of this very large system as Rosina metrics and then basically I compute the PCA of these metrics.",
                    "label": 0
                },
                {
                    "sent": "So just to give an example, suppose that's I have this sequence in inputs at least, so I.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After reading the first symbol and then put it in the goal.",
                    "label": 0
                },
                {
                    "sent": "So now this is a state becauses this state representing the input in first position without any history.",
                    "label": 0
                },
                {
                    "sent": "OK. Then, if I want to read the second item, the machine what is doing is shifting these state space to the right.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And inserting the new input.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In first position, so the new state is this one.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on, so it keeps.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shifting.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inserting",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "15 and",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inserting",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shifting adding 13 OK Now if I.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other another sequence I want to collect different sequences and compute the PCA of all the sequences.",
                    "label": 0
                },
                {
                    "sent": "I keep doing that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why we do the input?",
                    "label": 0
                },
                {
                    "sent": "This is the state.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space then I shift.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I serve.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Achieve.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Insert I shift.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I insert so these are all the states of my dynamical system.",
                    "label": 0
                },
                {
                    "sent": "When reading these two sequences.",
                    "label": 0
                },
                {
                    "sent": "So what can we do now?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How is to just compute the principal components of these rows and then we get the compression so we get a new way of representing the states in a more compact, compact way.",
                    "label": 0
                },
                {
                    "sent": "And if we preserve all the components so that we have a perfect reconstruction reconstruction of this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "State space we can redefine our dynamical system in this way, so getting much smaller then amical system which is doing the job OK.",
                    "label": 0
                },
                {
                    "sent": "I hope at least that I give you an intuition or.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of what is going OK, so the same click can be played with trees with trees we have a different dynamical system where instead of adding just one history we went.",
                    "label": 0
                },
                {
                    "sent": "We have one nice story for each possible child, so we have more matrixes and Step 2 is a bit more involved, cause for sequences you don't have much explosion in possibilities because you have only.",
                    "label": 0
                },
                {
                    "sent": "Unordered set of items.",
                    "label": 0
                },
                {
                    "sent": "Foltys you may have an exponential explosions of possibilities because you have all the different branching at each nodes.",
                    "label": 0
                },
                {
                    "sent": "So basically just using this streak encoding apriori all possible paths is not going to work.",
                    "label": 0
                },
                {
                    "sent": "Going to have two large extended system, so the trick is to just use parts which are.",
                    "label": 0
                },
                {
                    "sent": "Well, actually present in your data, so in this way you avoid the explosion and then we also need of course to define array push operator for each possible child.",
                    "label": 0
                },
                {
                    "sent": "So with PC is really more tricky to do everything, but it's possible to do it and the solution is similar to the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sequence case it's also possible.",
                    "label": 0
                },
                {
                    "sent": "Partially to address graphs, there are two main problems with grass festival cycles, so if you have cycles, your dynamical system may not converge to a stable position.",
                    "label": 0
                },
                {
                    "sent": "Stable solutions, and even if you are able to have stable dynamical systems, the problem is that when you decode state vector photographs, so you generate a new vertex and you don't know whether this is a new vertex or an old vertex that is reached by another.",
                    "label": 0
                },
                {
                    "sent": "Age by a different path.",
                    "label": 0
                },
                {
                    "sent": "So what we proposed here was to according Creek.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "The idea is when a numerate the set of vertexes following a given convention and then representing either director or undirected graph as an inverted, although at least where there is information.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bought concerning the label attached to the vertex and the edges which are entering that vertex.",
                    "label": 0
                },
                {
                    "sent": "So in this way is basically possible to encode the information about the graph, even if following a specific visit of the vertexes.",
                    "label": 0
                },
                {
                    "sent": "So I'm not claiming that the problem is fully solved.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I would solve.",
                    "label": 0
                },
                {
                    "sent": "I would demonstrate that P is equal to NP, so of course this is not going to be possible, But the idea is that when you choose.",
                    "label": 0
                },
                {
                    "sent": "A conventional way of investigating the vector exists, then you fix a view of your graphs and you can do principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "According to this view.",
                    "label": 0
                },
                {
                    "sent": "So there are some technical reasons forward thinking for defining the shift operator in this way.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go through all this, but basically again you get solution which is very similar to the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of cases.",
                    "label": 0
                },
                {
                    "sent": "OK, now computational problem.",
                    "label": 0
                },
                {
                    "sent": "So basically PCA is doing a angling engine analysis of the correlation or covariance covariance matrix of the X metrics, and so this can be computationally quite demanding when you have a large data set.",
                    "label": 0
                },
                {
                    "sent": "So there are two problems of code space.",
                    "label": 0
                },
                {
                    "sent": "The axe metrics can be quite large.",
                    "label": 0
                },
                {
                    "sent": "You have one row for each single component.",
                    "label": 0
                },
                {
                    "sent": "For each vertex of the data set, then several columns for each element of the encoding scheme depends on how you encode your input items.",
                    "label": 0
                },
                {
                    "sent": "So if you suppose that every item is encoded by K bits, then for sequences you have K bits for each time step.",
                    "label": 0
                },
                {
                    "sent": "For trees you have K columns for each path, which is present in the data and program director graphs we have less than K bits because there are some optimization can be done.",
                    "label": 0
                },
                {
                    "sent": "But still I mean this number are quite large important, so it's very important to try to reduce the dimension of the X matrix and time.",
                    "label": 0
                },
                {
                    "sent": "Of course, performing the computation of PCA is more than quadratic with the size of the metrics, so if we are able to shrink the size of the matrix X, we are also able to save alot.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of time in computation.",
                    "label": 0
                },
                {
                    "sent": "So let's consider the structure of the metric facts.",
                    "label": 0
                },
                {
                    "sent": "So the number of columns which we said already is the timing by two factor.",
                    "label": 0
                },
                {
                    "sent": "The sides of the representation of each possible component and their adopted structural encoding scheme.",
                    "label": 0
                },
                {
                    "sent": "Second observation, if the components are finite and discrete, then it is quite probable that different substructure different structure will have one or more components in common.",
                    "label": 0
                },
                {
                    "sent": "So you have you may have sub trees which are shared by many trees and so on.",
                    "label": 0
                },
                {
                    "sent": "So this means if this happen means that rose in X that correspond to this common components will be identical.",
                    "label": 0
                },
                {
                    "sent": "So you may take advantage of that.",
                    "label": 0
                },
                {
                    "sent": "And so we can exploit this by removing the identical goals.",
                    "label": 0
                },
                {
                    "sent": "So to reduce the size of the metrics.",
                    "label": 1
                },
                {
                    "sent": "But we need also to do a small trick to use a small trick in order to consider the fact that we have replica of rules.",
                    "label": 1
                },
                {
                    "sent": "And of course, it's also possible to exploit the smallest dimension of the Matrix X.",
                    "label": 0
                },
                {
                    "sent": "So of course these are linear space, so the rank of the matrix is equal to the minimum minimum dimension.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only two, so this can be exploited.",
                    "label": 0
                },
                {
                    "sent": "So basically the three observation turned out to be the reason for defining these three techniques.",
                    "label": 0
                },
                {
                    "sent": "The first Wally one is defining a minimal state space.",
                    "label": 0
                },
                {
                    "sent": "This actually allows you to save both space and time.",
                    "label": 0
                },
                {
                    "sent": "Second one, representing unique substitute by substructure by minimal dog also saves you space and time and then the last one.",
                    "label": 0
                },
                {
                    "sent": "You can exploit QR decomposition.",
                    "label": 0
                },
                {
                    "sent": "This is mainly saving time.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not much space.",
                    "label": 0
                },
                {
                    "sent": "OK, first approach first technique defining a minimal state space.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that for each structural component, only items which occur associated to it in the data set are explicitly represented, so this is equivalent to remove zero columns from my ex.",
                    "label": 0
                },
                {
                    "sent": "So let's consider the previous example.",
                    "label": 0
                },
                {
                    "sent": "So we had these two sequences and this was the X matrix.",
                    "label": 0
                },
                {
                    "sent": "Here you have all zeros.",
                    "label": 0
                },
                {
                    "sent": "Basically because there is no.",
                    "label": 0
                },
                {
                    "sent": "Information, so if you look at the items and how they approve in the different position, you see that in the first position all the symbols ABCD or code.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need to represent all of them.",
                    "label": 0
                },
                {
                    "sent": "So you need to discriminate all of them.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have every symbol to be presented without any assumption about the metrics between symbols, you need to have one bit.",
                    "label": 0
                },
                {
                    "sent": "On for each one of these symbols, all the others 01 N encoding.",
                    "label": 0
                },
                {
                    "sent": "So means that you need 4 bit snow that we present all the events which are occurring at position one.",
                    "label": 0
                },
                {
                    "sent": "OK if you look at the second column you see that this is never reaching position to so you don't need for beats.",
                    "label": 0
                },
                {
                    "sent": "You just need 3 bits.",
                    "label": 0
                },
                {
                    "sent": "The same here.",
                    "label": 0
                },
                {
                    "sent": "Here you need just to beat San.",
                    "label": 0
                },
                {
                    "sent": "Here you need just one bit becausw never.",
                    "label": 0
                },
                {
                    "sent": "The last position of your memory of your stock will need to be present BC or D so you don't need to use all the possibilities.",
                    "label": 0
                },
                {
                    "sent": "You just need to add the possibility to be presented in a.",
                    "label": 0
                },
                {
                    "sent": "Is there OK?",
                    "label": 0
                },
                {
                    "sent": "So of course all this is data dependent according to the data you have, then you get a different.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution, yeah.",
                    "label": 0
                },
                {
                    "sent": "Second key that he is OK, let's suppose that we are encoding the trees.",
                    "label": 0
                },
                {
                    "sent": "Then we can use quite efficient algorithm for compressing the forest in an adult, taking also in consideration the frequency of the substructure while the frequency is important because there is this property that we can exploit the fact that if you have a metric, say.",
                    "label": 0
                },
                {
                    "sent": "We chase all these columns which are different each other and they have this multiplicity in a metric, say.",
                    "label": 0
                },
                {
                    "sent": "Then if we consider a metrics where all the columns appear only once and you multiply for the square of the frequency, then they have the same correlation or covariance metrics.",
                    "label": 0
                },
                {
                    "sent": "So this means that they have the same principle components.",
                    "label": 0
                },
                {
                    "sent": "So, but the dimension of this matrix is much.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Smaller.",
                    "label": 0
                },
                {
                    "sent": "Lost QR decomposition.",
                    "label": 0
                },
                {
                    "sent": "So basically according to which dimension, the number of goals or number of columns is the smallest one we can select the metrics, so that is more efficient to compute a QR decomposition of the matrix and then performing an SVD decomposition of the metrics so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's possible to save some time, so I am going to present the experiments on data coming from chemistry and trees generated by this contract.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free grammar, so basically for graphs data set is 394 graphs and for total or 80,000 items the extended stay space is 3115.",
                    "label": 0
                },
                {
                    "sent": "For the grammar there are 421 distinct parts, three over total of bit less than 50,000 nodes with 1193 different parts so.",
                    "label": 0
                },
                {
                    "sent": "Just to be an idea, they maximum sites here is 70 autumns averages 24 atoms and here you may have up to 90 nodes and the depth maybe up to 19, so there may be quite large, so OK, so let's fix the world.",
                    "label": 0
                },
                {
                    "sent": "See what's happened when we compute the minimal state space.",
                    "label": 0
                },
                {
                    "sent": "So from 3115 we can we use for the graphs the state space.",
                    "label": 0
                },
                {
                    "sent": "1500 and the 40s from 7000 to roughly less than 3000, so using the first technique we were able to reduce the number of columns.",
                    "label": 0
                },
                {
                    "sent": "So if we try to compute SVD from the full matrix, then this is the number of seconds which are needed, so it for this architecture for this computer.",
                    "label": 0
                },
                {
                    "sent": "And if we compute first the.",
                    "label": 0
                },
                {
                    "sent": "While the composition, then the SVD, you see that there is some improvement in time.",
                    "label": 0
                },
                {
                    "sent": "However, the largest improvement is if we also consider substructure which are repeated.",
                    "label": 0
                },
                {
                    "sent": "So this means that if we consider the second technique, which is computing a graph which are shared, then we can reduce the number of rules.",
                    "label": 0
                },
                {
                    "sent": "And this means that we can get a quite drastic reduction in computation, especially for trees where there are many subkeys in common and also applying the QR decomposition.",
                    "label": 0
                },
                {
                    "sent": "You see, we can still gain something.",
                    "label": 0
                },
                {
                    "sent": "So this is the starting point.",
                    "label": 0
                },
                {
                    "sent": "And these are the final time.",
                    "label": 0
                },
                {
                    "sent": "So you see that the first sample in the case of trees, it's possible in 63 seconds to perform the principal components to compute the principal component.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this data set, so summary PCA of structure is interesting because it's going to allow us to device reduce the representation vectorial representation which can be used for any kind of machine learning approach which I defined on vectors or whatever in order to be applicable in practice, computation must be efficient, so the main contribution of this paper of this presentation was try to show.",
                    "label": 0
                },
                {
                    "sent": "That there are some savings which can be done.",
                    "label": 0
                },
                {
                    "sent": "The problem is when we have data with vocabulary which is quite large.",
                    "label": 0
                },
                {
                    "sent": "In that case, if you don't want to do assumption about similarity of symbols like words and so on, then the input size may be too large, and so it might be very difficult to compute PCA.",
                    "label": 0
                },
                {
                    "sent": "So there's much work to do in this 30 OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "This factor that explains the most part of viruses.",
                    "label": 0
                },
                {
                    "sent": "So what happens when you have this?",
                    "label": 0
                },
                {
                    "sent": "Reducing space on reduces.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This component or factor and added as exploratory data analysis is really difficult because we can reconstruct what it does mean, so they provisional results well.",
                    "label": 0
                },
                {
                    "sent": "The first of all you have a linear system, so this means that if you for example.",
                    "label": 0
                },
                {
                    "sent": "Compose one structure in the state space in different vectors which are octogonal.",
                    "label": 0
                },
                {
                    "sent": "Then you can compute the reducer representation of each and combine the reducer representation and then you get this reducer representation for the full structure.",
                    "label": 0
                },
                {
                    "sent": "So this means that there is really the possibility to interpret the data, but also you can use any, for example factor analysis.",
                    "label": 0
                },
                {
                    "sent": "Uh, for the X metric so you don't need to use PCA, so any technique which is linear and which is using the X metrics can be applied and so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are they using these?",
                    "label": 0
                },
                {
                    "sent": "I mean, the only important thing is that these metrics is isometrics is not annoying your transformation.",
                    "label": 0
                },
                {
                    "sent": "So if you prefer some other technique which is linear, then you can apply it.",
                    "label": 0
                },
                {
                    "sent": "Magic.",
                    "label": 0
                },
                {
                    "sent": "So long as we can also compute a special.",
                    "label": 0
                },
                {
                    "sent": "So and also we can be used.",
                    "label": 0
                },
                {
                    "sent": "So my question here is worsening ship between those of broad spectrum and so usually you apply the Laplacian and so on to a single metrics, not to a set of metrics.",
                    "label": 0
                },
                {
                    "sent": "So the here the idea is to try to capture the variance in all the different graphs and basically the technique which.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These apply to graphs.",
                    "label": 0
                },
                {
                    "sent": "Is really inserting the adjacency matrix rule by rule inside the system dynamically?",
                    "label": 0
                },
                {
                    "sent": "If we can close the browser pension for each run and then compare the difference between the other value on the web, maybe we can also well, not really because you are using a different representations, so here everything is put together and you focus on differences, so you are really capturing the variance in the representation of the graphs.",
                    "label": 0
                },
                {
                    "sent": "And so I don't think that you are going to get the same result.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Potential loss of information where you marked on their linear days.",
                    "label": 0
                },
                {
                    "sent": "Just wonder whether it is possible to.",
                    "label": 0
                },
                {
                    "sent": "Transport space.",
                    "label": 0
                },
                {
                    "sent": "Loss.",
                    "label": 0
                },
                {
                    "sent": "Yes, because if.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To keep all the principal components here so the subspace which is embedding although.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The vectors this is just a linear transformation, so you get the full information and actually this technique is volunteering to give you the minimum only if you keep all the components.",
                    "label": 0
                },
                {
                    "sent": "So I have no formal proof that if you remove some of the principal components then you get the small less arrow you can get.",
                    "label": 0
                },
                {
                    "sent": "This is only four.",
                    "label": 0
                },
                {
                    "sent": "You know this is.",
                    "label": 0
                },
                {
                    "sent": "Stool softener cheese yes.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No Sir there.",
                    "label": 0
                },
                {
                    "sent": "Yeah is that 40s?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the idea is that, for example whole trees.",
                    "label": 0
                },
                {
                    "sent": "Every role here will be a part in your tree in your set of trees, and so you are actually encoding all the different parts and you can preserve the full information so.",
                    "label": 0
                },
                {
                    "sent": "There's no loss information unless you drop some of the principle.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually some preliminary work on using these representations in chemistry is giving us quite good results, so it's possible to improve a lot from a situation where you have the expected which is giving you some feature and you apply like kernel machines for prediction and the case where you don't need the expected, you just have the molecular graphs, you compute principal component of the molecular graph, in this case, prepping all the information.",
                    "label": 0
                },
                {
                    "sent": "And applying the same machine that you applied here.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what you get see is that quite significant improvements in prediction.",
                    "label": 0
                },
                {
                    "sent": "Without the needs of going through the process of selecting features which are meaningful for the task and so on, and actually we were also really trying to reduce the number of components it was performing quite well even with not so many components.",
                    "label": 0
                },
                {
                    "sent": "Of course, this might be dependent on the problem, so I'm not claiming that this is true in general.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}