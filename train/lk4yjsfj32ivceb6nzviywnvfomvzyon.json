{
    "id": "lk4yjsfj32ivceb6nzviywnvfomvzyon",
    "title": "Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparse Learning",
    "info": {
        "author": [
            "Ryota Tomioka, Toyota Technological Institute at Chicago"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_tomioka_slc/",
    "segmentation": [
        [
            "This is joint work with digestive vickyann massage, Lisa Gamma.",
            "It's about method we are proposing for learning spark like everyone has been doing today using augmented Lagrangian method.",
            "So let's get to the problem.",
            "So as we all know, we're interested."
        ],
        [
            "In this kind of minimization problem, where the first term is the data fit through measures, how well you explain the data and the second term is the regularizer.",
            "For example, in the case of lemon law, so we have a squared norm squared loss here and some of absolute coefficient in Lambda is the regularization constant.",
            "So first thing to notice here is that we make separation of the loss function FL and the design matrix A which is M * N. M is the number of samples and N is number of unknowns and often machine learning.",
            "Setting A is given and we don't know until the problem is given.",
            "But we can discuss properties of the loss function like we're going to assume that if L has Lipschitz continuous gradient, so we can discuss properties of FL but HMB arbitrarily.",
            "Evil, so we want to make this separation and the second thing to note it is that we're interested in general algorithms for general loss functions in general regularizers so the scope is different from developing method for specific loss in combination of specific loss in specific regularizer."
        ],
        [
            "So OK.",
            "So why is this problem difficult?",
            "So conventionally many people thought.",
            "Of course it's difficult because it's non differentiable.",
            "So two types of approach has emerged.",
            "The first is the upper bound.",
            "The non differentiable regularization term by a differentiable.",
            "So it's a quadratic function.",
            "For example, this method is called focus or motorization minimization or iteratively reweighted least squares, or also eat a trick.",
            "An this first method.",
            "Otherwise you can try to explicitly handles non differentiability, so for some gradient.",
            "And here we have a different view that it's not that non differentiability, but it's the coupling between variables introduced by the design matrix A.",
            "So in fact, if design matrix is identity."
        ],
        [
            "Our end dimensional minimization problem can be decomposed into an 1 dimensional minimization problem, so this is very easy to solve and the solution is known to be soft thresholding operation which has this nice form of thresholding.",
            "But from minus, Lambda, Lambda and it.",
            "It's shrink to Darrell outside this interval, so this is analytical.",
            "This is super easy to solve, so let's state this assumption clearly.",
            "So we're interested in.",
            "Class of."
        ],
        [
            "The Lambda regularizers that this minimization problem we call thresholding can be obtained analytically and we call this approximation with respect to free Lambda is easy."
        ],
        [
            "So is the outline of the talk we have come down to hear an next.",
            "I'm going to talk about a well known method called iterative shrinkage thresholding and then we go to.",
            "In contrast to the proposed method called deal augmented like Ranch End and the proposed method is double loop algorithm and the theoretical result come in two parts.",
            "So the first part is assuming that the immunization itself exactly we get super linear convergence and of course we cannot do that in practice so.",
            "We have also a theorem for when we solve the inner problem to find out tolerance and we have some empirical results comparing to recent methods for doing this and we go to."
        ],
        [
            "So it's right there strangers threshold method is very simple, so you start from some initial solution.",
            "At every step you take the gradient with stiff sites, ITA and then shrink the solution like the arrows point to the minimum author loss function.",
            "So you first forget about the regularization term and take the gradient step of the last term and then you shrink data created in straight things.",
            "The gradient and shrink.",
            "So it's very easy to implement.",
            "You only need to compute gradient.",
            "But it's."
        ],
        [
            "Bad if the design matrix A is poorly conditioned.",
            "So in this case the gradient doesn't point is a minimum of the loss term.",
            "You want to minimize, so this is also known as forward backward splitting or thresh."
        ],
        [
            "Sorted and we very iteration.",
            "So this is very well known an now we come to the proposed deal augmented Lagrangian method.",
            "So when you."
        ],
        [
            "To minimize again, the sum of loss term and the regularization term, and for convenience, we call the sum of two terms F of W and in the primal what we're doing is proximal minimization.",
            "So at every step we minimize the sum of the objective function here plus quadratic proxy term.",
            "So the proxy term measures the distance from the."
        ],
        [
            "Iterate so it tries to keep the iterate close to the last one.",
            "So and this, either parameter controls how strong this second term is.",
            "So in the beginning we strongly constrained, but as you go on and on, this term has to decrease.",
            "So this parimeter ITA is decreased is increased.",
            "But actually different from Interpoint method or barrier method.",
            "This either parameter doesn't go to Infinity because as you can see this is only.",
            "Constrain this step relative to the last step.",
            "So that means that if ITA is small, your step is small, but you are going to converge to the solution.",
            "It's only relative to the left hip and the good thing about this formulation is that it's very easy to analyze because it's very simple.",
            "You have only two terms right.",
            "For example, you can immediately see that the function value at the next step is smaller than the function value at the current SIP by this amount.",
            "So this is a step size, so at every step you improve, so this is clear from the definition.",
            "But as you can see it's not practical because you're already having hard time minimizing this function.",
            "So why can't you minimize?",
            "Why can you minimize the sum of?",
            "Just adding a quadratic term.",
            "So the the practical version is in the deal.",
            "So if you look at the dual problem.",
            "So this is just the Fenchel dual standard Fenchel dual of this problem with equality constraint that is constraining like combining these connecting these two terms and if we do augmented like runs method with respect to this equality constraint, it's well known that is equivalent to doing proximal minimization in the primal.",
            "So you can find this connection in very.",
            "Ethical paper of Rockefeller 76th but interesting here, in the context of sparse learning, is that the method looks very similar to iterative shrinkage thresholding.",
            "So at every step what you do is to go into this direction with Alpha with step size.",
            "ITA an you shrink, you go into the direction of Alpha and shrink, so the difference is that the direction Alpha is not the gradient direction.",
            "It is obtained by solving.",
            "Some minimization, this is the immunization problem I already mentioned.",
            "I don't show the form of this function because lack of faith, but manipulation of this function is easy.",
            "It's smooth minimization problem.",
            "You can do Newton or conjugate gradient, or quasi Newton or whatever.",
            "And this is easy to solve.",
            "And another difference against IST is that the step size 8A T is increased by this property so.",
            "At every step you increase the steps."
        ],
        [
            "Ice.",
            "So.",
            "To contrast the difference against ISD, the diff."
        ],
        [
            "Truth comes from the way you get rid of the coupling, so I already stated that the couplings are bad, so the difference is like to do the how to do this proximal minimization so?"
        ],
        [
            "This is hard because the variables are coupled here, so the ISD method linearly approximate this like evil term here by a linear gradient at the current step.",
            "So this bound is tightest at this point, but not at all tide at the next step you want to go.",
            "So what the proposed algorithm do is you construct parametric lower bound that you can control by the Parimeter Alpha.",
            "So by optimizing the inner minimization problem, you get a linear bound.",
            "It's linear, but it's tightest at the next step instead of the current step.",
            "So that is how you can solve this proximal minimization problem exactly.",
            "Of course, do some finite tolerance.",
            "So you can see that as an.",
            "The condition of the problem comes."
        ],
        [
            "Worse and worse, IST becomes more and more like oscillating.",
            "And of course you have to choose a stepsize like very carefully, but the algorithm seems to be more and more stable.",
            "And actually, that's because the solution is going to be sparse at the very end.",
            "So, um."
        ],
        [
            "We're going to go into the theory part, so.",
            "Let's define WT the sequence generated by the proposed algorithm.",
            "So, assuming in the first theorem that we can minimize the inner minimization exactly, that means the gradient of this five function that we want to minimize is zero at the next step, and a bizarre is a unique minimizer of the objective, and what we assume is that the revivial in terms of the function value we want to minimize is lower bounded by the distance from the true minimizer, so.",
            "This is related to assuming strong convexity, but it's only relative to the minimizer of the problem, and we only need this guarantee for all the points generated by the algorithm.",
            "So if the minimizer is unique, this condition is trivial for finite number of steps you take, and the theorem states that the distance to the minimizer is reduced by this factor at every iteration.",
            "And this is strictly smaller than zero.",
            "Sorry, it's strictly smaller than one an because we increase the stepsize parameter.",
            "ITA this rate is going to be smaller and smaller.",
            "That means that this algorithm converges super linearly.",
            "OK, so we go to the."
        ],
        [
            "Our commitment station fitting, so here gamma or one over gamma is let's it constant of the gradient of the loss function, and here this is something we can compute before hand, because if we know the loss function without looking at the data, we can compute the lipsticks content.",
            "So this is a very practical stopping condition."
        ],
        [
            "Stating that the norm of the.",
            "The grading of the immunization problem has to be smaller than this amount, which is inversely proportional to the square root of the eater parameter.",
            "So of course if we have larger ITA we are going to take bigger steps.",
            "Eyes that means that the inner minimization problem is going to get harder, but it's only square root to the step size either, and the theorem states that under the same condition as in theorem one, we get super linear convergence.",
            "And again, this is strictly smaller than one, and if we increase the stepsize Eater, we're going to get super linear conversions.",
            "And of course, because we have this square root here, the rate is slower, the convergence is slower than the exact case, and if you really want to get the same rate as the exact case, you can minimize the internalization to the 1 / E to so that is harder.",
            "But if you like do a little more effort in the inner minimization, you can obtain the same rate.",
            "So let's look at the."
        ],
        [
            "Sense of the proof.",
            "It's very simple because the proxamol minimization view.",
            "So the next step minimizes this thing.",
            "So by just taking the differential of this guy, we know that the step vector, which is the difference of the two points, lies in the subgradient of the loss function or of the objective function at the next step.",
            "So in another word we know that this guy is bounding this objective function from below, so if we.",
            "So after this inequality it's very simple because we have a bound on the left hand side and we only have to do a little work with the right hand side.",
            "So this was the exact case and in the.",
            "In exact case approximate case, the situation is limited."
        ],
        [
            "Order because this guy, which was the subgradient in the exact case.",
            "It's not this upgraded at this step anymore, because it's approximation in the minimization.",
            "But we can guarantee that if we.",
            "Three, if we have this additional term, we can.",
            "This additional term is proportional to the norm squared norm of the inner minimization.",
            "So if we can solve immunization exactly this term drops out and it's the same as the theorem one.",
            "But this is a cost of approximately meditation an if we have this cost, we can lower bounded function again and we can do the same proof.",
            "And we have a bound on this norm by definition, so this is very easy to prove.",
            "So, um."
        ],
        [
            "Here are some empirical results.",
            "This is L1 regularize logistic regression problem.",
            "We have 1000 examples, an 16,000 unknown variables and we so the blue one is the proposed algorithm and the green and red are the theoretical bound obtain.",
            "So actually they are pretty loose.",
            "But they're making sense.",
            "And we compare it to three methods, Vista is.",
            "Two step.",
            "Question.",
            "OK, so on the left we show a number of iterations.",
            "And on the right we show number of CPU time and spend an on the top.",
            "We have the distance from the true minimum, so this is something we proved an this is the residual in terms of the function value.",
            "Thank you for the question.",
            "So we have a bound on the residual from the truth in front in the distance so.",
            "The method we compare if Vista is a two step variant of IST which has theoretical bound with respect to the distance from the function value residual.",
            "An often wise LBF's algorithm is using subgradient an it.",
            "The magenta one in Sparsa is recently proposed variant of ISD that use clever choice of stepsize.",
            "So now seems to be faster than any methods compared to here.",
            "And it obtains a solution like roughly and 10 seconds to fairly precise solution and you can see that the number of iterations used by Dell is extremely small.",
            "It only uses like 10 outer iterations.",
            "While other method tends to use more iterations, of course the cost per iteration is higher for the proposed method, but it's also fast in terms of the CPU time spent after all.",
            "So to summarize, the first point was why sparse learning is difficult."
        ],
        [
            "And our point is that it's not the non differentiability, but it's a coupling that the design matrix A introduces an.",
            "So we have developed this method and actually we didn't have time to discuss the cost of immunization, but the cost of immunization is roughly proportional to the number of active variables.",
            "That means that even if you have millions of variables you want to optimize if like your active variable is like 100, we only need to compute these hundred variables like.",
            "That also applies to the case of trace norm regularization.",
            "If you have like 1000 * 1000 matrix, if you only have rank 10, then.",
            "Rank 10 is part that counts in the optimization.",
            "So sparsely actually help, like speed up the optimization, it doesn't harm us.",
            "So."
        ],
        [
            "The second point was this could be a take home message, so if you are tempted to introduce a linear approximation in your minimization, use a linear parametric lower bound instead, because you can like you can adjust the lower bound so that it becomes Titus at the point you want.",
            "Then don't use just fixed link."
        ],
        [
            "Approximation and we have some super linear convergence result and this was obtained because we use some characteristic property of sparse learning, which is that approximation with respect to file Lambda is easy to do an we have shown some promising empirical results compared to these conventional methods."
        ],
        [
            "Thank you very much.",
            "Yes.",
            "Activated yes, what we have to do in the inner minimization is this kind of minimization an.",
            "So this is a convex conjugate of the loss function, and this is the square of the soft thresholding.",
            "That means that also in the inner minimization we are soft thresholding all the time and we.",
            "Possible philben plus.",
            "Yes, we need to check whether they are active or not, but like for example, when you compute the gradient or the Hessian of this guy only the active for example only the active columns of A comes in.",
            "So that depends on Lambda.",
            "So if your if your Lambda is very small, there is of course going to be a lot of active components, but if you're aiming for a sparse solution then this is going to be efficient.",
            "Yes.",
            "Help sparse.",
            "I think this was like 4% of the whole, so it's fairly sparse.",
            "So total Commander is a.",
            "16 Yeah, yeah.",
            "Several hundreds."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with digestive vickyann massage, Lisa Gamma.",
                    "label": 0
                },
                {
                    "sent": "It's about method we are proposing for learning spark like everyone has been doing today using augmented Lagrangian method.",
                    "label": 0
                },
                {
                    "sent": "So let's get to the problem.",
                    "label": 0
                },
                {
                    "sent": "So as we all know, we're interested.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this kind of minimization problem, where the first term is the data fit through measures, how well you explain the data and the second term is the regularizer.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of lemon law, so we have a squared norm squared loss here and some of absolute coefficient in Lambda is the regularization constant.",
                    "label": 1
                },
                {
                    "sent": "So first thing to notice here is that we make separation of the loss function FL and the design matrix A which is M * N. M is the number of samples and N is number of unknowns and often machine learning.",
                    "label": 0
                },
                {
                    "sent": "Setting A is given and we don't know until the problem is given.",
                    "label": 0
                },
                {
                    "sent": "But we can discuss properties of the loss function like we're going to assume that if L has Lipschitz continuous gradient, so we can discuss properties of FL but HMB arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "Evil, so we want to make this separation and the second thing to note it is that we're interested in general algorithms for general loss functions in general regularizers so the scope is different from developing method for specific loss in combination of specific loss in specific regularizer.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So why is this problem difficult?",
                    "label": 0
                },
                {
                    "sent": "So conventionally many people thought.",
                    "label": 0
                },
                {
                    "sent": "Of course it's difficult because it's non differentiable.",
                    "label": 0
                },
                {
                    "sent": "So two types of approach has emerged.",
                    "label": 0
                },
                {
                    "sent": "The first is the upper bound.",
                    "label": 1
                },
                {
                    "sent": "The non differentiable regularization term by a differentiable.",
                    "label": 1
                },
                {
                    "sent": "So it's a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "For example, this method is called focus or motorization minimization or iteratively reweighted least squares, or also eat a trick.",
                    "label": 1
                },
                {
                    "sent": "An this first method.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can try to explicitly handles non differentiability, so for some gradient.",
                    "label": 0
                },
                {
                    "sent": "And here we have a different view that it's not that non differentiability, but it's the coupling between variables introduced by the design matrix A.",
                    "label": 1
                },
                {
                    "sent": "So in fact, if design matrix is identity.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our end dimensional minimization problem can be decomposed into an 1 dimensional minimization problem, so this is very easy to solve and the solution is known to be soft thresholding operation which has this nice form of thresholding.",
                    "label": 0
                },
                {
                    "sent": "But from minus, Lambda, Lambda and it.",
                    "label": 0
                },
                {
                    "sent": "It's shrink to Darrell outside this interval, so this is analytical.",
                    "label": 0
                },
                {
                    "sent": "This is super easy to solve, so let's state this assumption clearly.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Class of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Lambda regularizers that this minimization problem we call thresholding can be obtained analytically and we call this approximation with respect to free Lambda is easy.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is the outline of the talk we have come down to hear an next.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about a well known method called iterative shrinkage thresholding and then we go to.",
                    "label": 0
                },
                {
                    "sent": "In contrast to the proposed method called deal augmented like Ranch End and the proposed method is double loop algorithm and the theoretical result come in two parts.",
                    "label": 0
                },
                {
                    "sent": "So the first part is assuming that the immunization itself exactly we get super linear convergence and of course we cannot do that in practice so.",
                    "label": 0
                },
                {
                    "sent": "We have also a theorem for when we solve the inner problem to find out tolerance and we have some empirical results comparing to recent methods for doing this and we go to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's right there strangers threshold method is very simple, so you start from some initial solution.",
                    "label": 1
                },
                {
                    "sent": "At every step you take the gradient with stiff sites, ITA and then shrink the solution like the arrows point to the minimum author loss function.",
                    "label": 1
                },
                {
                    "sent": "So you first forget about the regularization term and take the gradient step of the last term and then you shrink data created in straight things.",
                    "label": 0
                },
                {
                    "sent": "The gradient and shrink.",
                    "label": 0
                },
                {
                    "sent": "So it's very easy to implement.",
                    "label": 1
                },
                {
                    "sent": "You only need to compute gradient.",
                    "label": 0
                },
                {
                    "sent": "But it's.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bad if the design matrix A is poorly conditioned.",
                    "label": 1
                },
                {
                    "sent": "So in this case the gradient doesn't point is a minimum of the loss term.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize, so this is also known as forward backward splitting or thresh.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorted and we very iteration.",
                    "label": 0
                },
                {
                    "sent": "So this is very well known an now we come to the proposed deal augmented Lagrangian method.",
                    "label": 0
                },
                {
                    "sent": "So when you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To minimize again, the sum of loss term and the regularization term, and for convenience, we call the sum of two terms F of W and in the primal what we're doing is proximal minimization.",
                    "label": 0
                },
                {
                    "sent": "So at every step we minimize the sum of the objective function here plus quadratic proxy term.",
                    "label": 0
                },
                {
                    "sent": "So the proxy term measures the distance from the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Iterate so it tries to keep the iterate close to the last one.",
                    "label": 0
                },
                {
                    "sent": "So and this, either parameter controls how strong this second term is.",
                    "label": 0
                },
                {
                    "sent": "So in the beginning we strongly constrained, but as you go on and on, this term has to decrease.",
                    "label": 0
                },
                {
                    "sent": "So this parimeter ITA is decreased is increased.",
                    "label": 1
                },
                {
                    "sent": "But actually different from Interpoint method or barrier method.",
                    "label": 0
                },
                {
                    "sent": "This either parameter doesn't go to Infinity because as you can see this is only.",
                    "label": 0
                },
                {
                    "sent": "Constrain this step relative to the last step.",
                    "label": 0
                },
                {
                    "sent": "So that means that if ITA is small, your step is small, but you are going to converge to the solution.",
                    "label": 1
                },
                {
                    "sent": "It's only relative to the left hip and the good thing about this formulation is that it's very easy to analyze because it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You have only two terms right.",
                    "label": 0
                },
                {
                    "sent": "For example, you can immediately see that the function value at the next step is smaller than the function value at the current SIP by this amount.",
                    "label": 0
                },
                {
                    "sent": "So this is a step size, so at every step you improve, so this is clear from the definition.",
                    "label": 1
                },
                {
                    "sent": "But as you can see it's not practical because you're already having hard time minimizing this function.",
                    "label": 0
                },
                {
                    "sent": "So why can't you minimize?",
                    "label": 0
                },
                {
                    "sent": "Why can you minimize the sum of?",
                    "label": 0
                },
                {
                    "sent": "Just adding a quadratic term.",
                    "label": 0
                },
                {
                    "sent": "So the the practical version is in the deal.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So this is just the Fenchel dual standard Fenchel dual of this problem with equality constraint that is constraining like combining these connecting these two terms and if we do augmented like runs method with respect to this equality constraint, it's well known that is equivalent to doing proximal minimization in the primal.",
                    "label": 0
                },
                {
                    "sent": "So you can find this connection in very.",
                    "label": 0
                },
                {
                    "sent": "Ethical paper of Rockefeller 76th but interesting here, in the context of sparse learning, is that the method looks very similar to iterative shrinkage thresholding.",
                    "label": 0
                },
                {
                    "sent": "So at every step what you do is to go into this direction with Alpha with step size.",
                    "label": 0
                },
                {
                    "sent": "ITA an you shrink, you go into the direction of Alpha and shrink, so the difference is that the direction Alpha is not the gradient direction.",
                    "label": 0
                },
                {
                    "sent": "It is obtained by solving.",
                    "label": 0
                },
                {
                    "sent": "Some minimization, this is the immunization problem I already mentioned.",
                    "label": 0
                },
                {
                    "sent": "I don't show the form of this function because lack of faith, but manipulation of this function is easy.",
                    "label": 0
                },
                {
                    "sent": "It's smooth minimization problem.",
                    "label": 0
                },
                {
                    "sent": "You can do Newton or conjugate gradient, or quasi Newton or whatever.",
                    "label": 0
                },
                {
                    "sent": "And this is easy to solve.",
                    "label": 1
                },
                {
                    "sent": "And another difference against IST is that the step size 8A T is increased by this property so.",
                    "label": 0
                },
                {
                    "sent": "At every step you increase the steps.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To contrast the difference against ISD, the diff.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Truth comes from the way you get rid of the coupling, so I already stated that the couplings are bad, so the difference is like to do the how to do this proximal minimization so?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is hard because the variables are coupled here, so the ISD method linearly approximate this like evil term here by a linear gradient at the current step.",
                    "label": 1
                },
                {
                    "sent": "So this bound is tightest at this point, but not at all tide at the next step you want to go.",
                    "label": 0
                },
                {
                    "sent": "So what the proposed algorithm do is you construct parametric lower bound that you can control by the Parimeter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So by optimizing the inner minimization problem, you get a linear bound.",
                    "label": 0
                },
                {
                    "sent": "It's linear, but it's tightest at the next step instead of the current step.",
                    "label": 1
                },
                {
                    "sent": "So that is how you can solve this proximal minimization problem exactly.",
                    "label": 0
                },
                {
                    "sent": "Of course, do some finite tolerance.",
                    "label": 0
                },
                {
                    "sent": "So you can see that as an.",
                    "label": 0
                },
                {
                    "sent": "The condition of the problem comes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worse and worse, IST becomes more and more like oscillating.",
                    "label": 0
                },
                {
                    "sent": "And of course you have to choose a stepsize like very carefully, but the algorithm seems to be more and more stable.",
                    "label": 0
                },
                {
                    "sent": "And actually, that's because the solution is going to be sparse at the very end.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to go into the theory part, so.",
                    "label": 0
                },
                {
                    "sent": "Let's define WT the sequence generated by the proposed algorithm.",
                    "label": 1
                },
                {
                    "sent": "So, assuming in the first theorem that we can minimize the inner minimization exactly, that means the gradient of this five function that we want to minimize is zero at the next step, and a bizarre is a unique minimizer of the objective, and what we assume is that the revivial in terms of the function value we want to minimize is lower bounded by the distance from the true minimizer, so.",
                    "label": 1
                },
                {
                    "sent": "This is related to assuming strong convexity, but it's only relative to the minimizer of the problem, and we only need this guarantee for all the points generated by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if the minimizer is unique, this condition is trivial for finite number of steps you take, and the theorem states that the distance to the minimizer is reduced by this factor at every iteration.",
                    "label": 0
                },
                {
                    "sent": "And this is strictly smaller than zero.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it's strictly smaller than one an because we increase the stepsize parameter.",
                    "label": 0
                },
                {
                    "sent": "ITA this rate is going to be smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "That means that this algorithm converges super linearly.",
                    "label": 0
                },
                {
                    "sent": "OK, so we go to the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our commitment station fitting, so here gamma or one over gamma is let's it constant of the gradient of the loss function, and here this is something we can compute before hand, because if we know the loss function without looking at the data, we can compute the lipsticks content.",
                    "label": 0
                },
                {
                    "sent": "So this is a very practical stopping condition.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stating that the norm of the.",
                    "label": 0
                },
                {
                    "sent": "The grading of the immunization problem has to be smaller than this amount, which is inversely proportional to the square root of the eater parameter.",
                    "label": 0
                },
                {
                    "sent": "So of course if we have larger ITA we are going to take bigger steps.",
                    "label": 1
                },
                {
                    "sent": "Eyes that means that the inner minimization problem is going to get harder, but it's only square root to the step size either, and the theorem states that under the same condition as in theorem one, we get super linear convergence.",
                    "label": 1
                },
                {
                    "sent": "And again, this is strictly smaller than one, and if we increase the stepsize Eater, we're going to get super linear conversions.",
                    "label": 0
                },
                {
                    "sent": "And of course, because we have this square root here, the rate is slower, the convergence is slower than the exact case, and if you really want to get the same rate as the exact case, you can minimize the internalization to the 1 / E to so that is harder.",
                    "label": 1
                },
                {
                    "sent": "But if you like do a little more effort in the inner minimization, you can obtain the same rate.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sense of the proof.",
                    "label": 0
                },
                {
                    "sent": "It's very simple because the proxamol minimization view.",
                    "label": 0
                },
                {
                    "sent": "So the next step minimizes this thing.",
                    "label": 0
                },
                {
                    "sent": "So by just taking the differential of this guy, we know that the step vector, which is the difference of the two points, lies in the subgradient of the loss function or of the objective function at the next step.",
                    "label": 0
                },
                {
                    "sent": "So in another word we know that this guy is bounding this objective function from below, so if we.",
                    "label": 0
                },
                {
                    "sent": "So after this inequality it's very simple because we have a bound on the left hand side and we only have to do a little work with the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So this was the exact case and in the.",
                    "label": 0
                },
                {
                    "sent": "In exact case approximate case, the situation is limited.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order because this guy, which was the subgradient in the exact case.",
                    "label": 0
                },
                {
                    "sent": "It's not this upgraded at this step anymore, because it's approximation in the minimization.",
                    "label": 0
                },
                {
                    "sent": "But we can guarantee that if we.",
                    "label": 0
                },
                {
                    "sent": "Three, if we have this additional term, we can.",
                    "label": 0
                },
                {
                    "sent": "This additional term is proportional to the norm squared norm of the inner minimization.",
                    "label": 0
                },
                {
                    "sent": "So if we can solve immunization exactly this term drops out and it's the same as the theorem one.",
                    "label": 0
                },
                {
                    "sent": "But this is a cost of approximately meditation an if we have this cost, we can lower bounded function again and we can do the same proof.",
                    "label": 0
                },
                {
                    "sent": "And we have a bound on this norm by definition, so this is very easy to prove.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some empirical results.",
                    "label": 0
                },
                {
                    "sent": "This is L1 regularize logistic regression problem.",
                    "label": 0
                },
                {
                    "sent": "We have 1000 examples, an 16,000 unknown variables and we so the blue one is the proposed algorithm and the green and red are the theoretical bound obtain.",
                    "label": 0
                },
                {
                    "sent": "So actually they are pretty loose.",
                    "label": 0
                },
                {
                    "sent": "But they're making sense.",
                    "label": 0
                },
                {
                    "sent": "And we compare it to three methods, Vista is.",
                    "label": 0
                },
                {
                    "sent": "Two step.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the left we show a number of iterations.",
                    "label": 0
                },
                {
                    "sent": "And on the right we show number of CPU time and spend an on the top.",
                    "label": 0
                },
                {
                    "sent": "We have the distance from the true minimum, so this is something we proved an this is the residual in terms of the function value.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the question.",
                    "label": 0
                },
                {
                    "sent": "So we have a bound on the residual from the truth in front in the distance so.",
                    "label": 0
                },
                {
                    "sent": "The method we compare if Vista is a two step variant of IST which has theoretical bound with respect to the distance from the function value residual.",
                    "label": 0
                },
                {
                    "sent": "An often wise LBF's algorithm is using subgradient an it.",
                    "label": 0
                },
                {
                    "sent": "The magenta one in Sparsa is recently proposed variant of ISD that use clever choice of stepsize.",
                    "label": 0
                },
                {
                    "sent": "So now seems to be faster than any methods compared to here.",
                    "label": 0
                },
                {
                    "sent": "And it obtains a solution like roughly and 10 seconds to fairly precise solution and you can see that the number of iterations used by Dell is extremely small.",
                    "label": 0
                },
                {
                    "sent": "It only uses like 10 outer iterations.",
                    "label": 0
                },
                {
                    "sent": "While other method tends to use more iterations, of course the cost per iteration is higher for the proposed method, but it's also fast in terms of the CPU time spent after all.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, the first point was why sparse learning is difficult.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our point is that it's not the non differentiability, but it's a coupling that the design matrix A introduces an.",
                    "label": 0
                },
                {
                    "sent": "So we have developed this method and actually we didn't have time to discuss the cost of immunization, but the cost of immunization is roughly proportional to the number of active variables.",
                    "label": 1
                },
                {
                    "sent": "That means that even if you have millions of variables you want to optimize if like your active variable is like 100, we only need to compute these hundred variables like.",
                    "label": 0
                },
                {
                    "sent": "That also applies to the case of trace norm regularization.",
                    "label": 0
                },
                {
                    "sent": "If you have like 1000 * 1000 matrix, if you only have rank 10, then.",
                    "label": 0
                },
                {
                    "sent": "Rank 10 is part that counts in the optimization.",
                    "label": 0
                },
                {
                    "sent": "So sparsely actually help, like speed up the optimization, it doesn't harm us.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second point was this could be a take home message, so if you are tempted to introduce a linear approximation in your minimization, use a linear parametric lower bound instead, because you can like you can adjust the lower bound so that it becomes Titus at the point you want.",
                    "label": 0
                },
                {
                    "sent": "Then don't use just fixed link.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximation and we have some super linear convergence result and this was obtained because we use some characteristic property of sparse learning, which is that approximation with respect to file Lambda is easy to do an we have shown some promising empirical results compared to these conventional methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Activated yes, what we have to do in the inner minimization is this kind of minimization an.",
                    "label": 1
                },
                {
                    "sent": "So this is a convex conjugate of the loss function, and this is the square of the soft thresholding.",
                    "label": 1
                },
                {
                    "sent": "That means that also in the inner minimization we are soft thresholding all the time and we.",
                    "label": 0
                },
                {
                    "sent": "Possible philben plus.",
                    "label": 0
                },
                {
                    "sent": "Yes, we need to check whether they are active or not, but like for example, when you compute the gradient or the Hessian of this guy only the active for example only the active columns of A comes in.",
                    "label": 0
                },
                {
                    "sent": "So that depends on Lambda.",
                    "label": 0
                },
                {
                    "sent": "So if your if your Lambda is very small, there is of course going to be a lot of active components, but if you're aiming for a sparse solution then this is going to be efficient.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Help sparse.",
                    "label": 1
                },
                {
                    "sent": "I think this was like 4% of the whole, so it's fairly sparse.",
                    "label": 0
                },
                {
                    "sent": "So total Commander is a.",
                    "label": 0
                },
                {
                    "sent": "16 Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Several hundreds.",
                    "label": 0
                }
            ]
        }
    }
}