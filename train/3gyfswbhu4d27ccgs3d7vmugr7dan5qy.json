{
    "id": "3gyfswbhu4d27ccgs3d7vmugr7dan5qy",
    "title": "How could networks of neurons learn to carry out probabilistic inference?",
    "info": {
        "author": [
            "Wolfgang Maass, Institute for Theoretical Computer Science"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/mlss2010_maass_hcnon/",
    "segmentation": [
        [
            "Yeah, I think it's very nice that you organized this summer school here in the last few years at NIPS.",
            "I really like most this particular interaction of machine learning and cognitive science, and starting also growing into neuroscience research, and I think it's really needed to move this type of research combination to Europe.",
            "I think it's much more still centered in the US.",
            "When in principle, the cognitive neuro scientists, especially Josh Tenenbaum, had laid out the problem for us.",
            "Know they have very nice models to explain behavior or about human mind.",
            "But as Josh mentioned, the really burning question is how could neurons really implement these type of probabilistic inference operations and represent probability distributions and convert.",
            "A very nice introduction also telling us how difficult it is to say anything with certainty in neuroscience.",
            "I think the mostly fundamental question early unanswered in neuroscience.",
            "On other hand, I think also it's not a situation where we can simply stand back and wait until the guys have had extra meant results be cause I think they need also be provided or provoked by theories and which they then refute or confirm then.",
            "And I think many of their theories which they have in the back of the head like synfire chains or so.",
            "There simply not up to speed now with regard to inside for machine learning.",
            "Another insight, and I think it's a duty or chance of this community.",
            "Also particular machine learning part really to provide ideas and models for neuro scientist and interact with them.",
            "So for example we benefited a lot from the European projects where they bring machine learning people and extramental scientists together and I think in this regard Europe in Europe we may actually have an advantage of are related.",
            "Research."
        ],
        [
            "So I have here a little bit less sophisticated summary of the vector or hardware that behave in the brain.",
            "So for computer scientists really provocative, know this.",
            "This picture size to millimeter 6 sheet of neurons, the greymatter.",
            "And this has about 10:50 runs.",
            "It consumes about 50.",
            "Button is apparently much better than any artificial computing computing machine that we have currently, even once to understand know something about the structure of this circuitry.",
            "This is here.",
            "This typical structure is laid out six layers which know sometimes it says no.",
            "It's the same.",
            "But if you take a closer look, it's everywhere, slightly different.",
            "Know how thick this particular pens are then.",
            "And then when you look inside and no longer here, every neuron body is 1 dark spot then.",
            "But the difference?",
            "The neurons are very different.",
            "Also there about 100 genetically different types of neurons there.",
            "And so this network is really very complicated and so one of these streams is to find out what is generic competition function of such no microcircuit as when calls them as a matter of convenience, but it's really it's a continuous sheet number should not think.",
            "That this is made out of."
        ],
        [
            "Internationally called them, but anyway, so this is another way how people have looked at it.",
            "And so this is no.",
            "Figure which I took from a very nice review paper by Rodney Douglas and Kevin Martin from Etoh, Zurich, who have been one of the key people who proposed really look.",
            "Also at the function of this generic cortical microcircuits, and this is, I think still the latest proposition in the sense in terms of function.",
            "Now you see here, this layers in principle symbolically and are mainly think of this.",
            "Is having two arrays of soft winner take all circuits there?",
            "This is their main hypothesis and I think this most of the data are based from neuron otomy some Physiology also and I think it's widely known that the interaction of excitation emission is really crucial, but doesn't quite know.",
            "But the loss and the computation goals of this interaction are then and I think the goal of my talk is to present some recent.",
            "Hypotheses, ideas, and then I think you see all the simultaneously.",
            "The difficulty of any such hypotheses, and perhaps you come up."
        ],
        [
            "And with better ideas.",
            "So actually I want to interpret this whole fanatic all circuit also re direction of this workshop.",
            "Namely, I would like to see this no more stochastic winner take all circuit and I think there's also a lot of experimental evidence becausw no one is no, these are spontaneously active even if you don't, you close your eyes and your visual cortex is almost as active now as when you have your eyes open.",
            "And also there's this frustrating large trial to try their ability, which people know who obsessed.",
            "Understanding the brain but understanding the neural codes of particular neurons are very bothered by this because this suggest to them that this is not a deterministic machinery's would like to see the brain but from the perspective of probabilistic inference this could just tell us that each trial is a sampling from posterior distributions and the brain really aims to represent in these circuits distributions there and each time we make it extra mental your sampling from this posterior there and so therefore from this perspective this tried to try probability.",
            "Could actually be a senchal part of the function there, and I think this is 1 ingredient which this community might bring into the resigns.",
            "Because I think missing simply kind of convincing computation models which can deal and live with this kind of variability."
        ],
        [
            "So announcing the title that I want to talk about about hypothesis how probabilistic inference could be truly encoded in the brain.",
            "But how it could emerge from plasticity from learning there and learning and synaptic plasticity is another can of worms, where which is extremely complicated.",
            "So this is just a drawing from a textbook of the presynaptic bouton.",
            "You might see no there there, this vesicles and when Hera presynaptic spike comes.",
            "One of these vesicles flashlight fuses here with the membrane.",
            "This neurotransmitter is left out and then you know the some of these moves to the synaptic cleft and over there and it opens receptors there and so this is an extremely complicated device and I think typically invalid.",
            "See everything that you see in structures.",
            "Also adaptive or plastic answer when seasonal.",
            "How many different components there are that could in principle plastic and most of them are plastic.",
            "This is a figure from a paper of Eric and Dell's group in New York which shows the postsynaptic site and it indicates.",
            "Cartoon this cascade of process is really triggered when you have no something which might be viewed as synaptic plasticity because at the moment receptors are just techno by first relation or so, which is kind of a current state then, but it initiates particularly calcium.",
            "Different processes goes to the cell body.",
            "In the end it reads had particular parts of the DNA and these are moved back to the synapses and then.",
            "Insert no additional receptors in into this membrane there, so it's an extremely complicated process and I think one has to assume that all of the existing data only give us the tip of an iceberg of knowledge about this is much more complicated and the data depending on which slipped acid, what is animal are there awake anesta size know about is the type of slice and all these questions there and I think one should not as a theoretician look at this particular not take from any theoretical book of paper.",
            "They're kind of a textbook learning rule and say this is now what I have to live with.",
            "I think it's really difficult different.",
            "I think situation is we don't understand the system and I think we should have functional ideas how the system might work, how it makes sense on organization level, and then come up with predictions or questions with instrumentalists can then decide.",
            "Then I think this is really the way however."
        ],
        [
            "Can expect to make more progress in this direction, but anyway, conventionally SDP, which has been discovered in 98 by three groups independently, one of them group so female neuroscientist Kirsty Grant is generous in France and.",
            "Found out this is the first time that extreme interest.",
            "Learn to induce synaptic plasticity in a kind of seem realistic, no physiological way as they say, and not by kind of shocks.",
            "No of current snow which inserted and so he have to neurons and ideally want to have one electrode in the postsynaptic neuron and running the presynaptic neuron and so then you can trigger spiking of the presynaptic neuron and this causes a certain postsynaptic potential.",
            "So this is coming and going this pointer.",
            "And then check out the postsynaptic potential in the postsynaptic neuron and then is already Conrad mentioned.",
            "Depending on the relative timing on pre and post synaptic firing the strength of the synapse goes up or goes down.",
            "Here in this drawing.",
            "Thank you.",
            "But he's the green part is where it goes up is and this is where the presynaptic neuron fires before the postsynaptic neuron.",
            "Nothing corner mention already.",
            "This is, I think, from the perspective of probabilistic inference is great because it shows already that this device tries to find causes.",
            "By this post synaptic neuron fired.",
            "It doesn't go for correlation, it goes for causal analysis of the things around it in circuitry.",
            "And if this presynaptic neuron fires after the postsynaptic neuron is definitely not cause it, and so this at the moment looks know.",
            "Now he feels like a bad connection or useless connection and then this weight goes down then really also.",
            "We use the office set definition of the science very, very good.",
            "Good part of mental mentions.",
            "So this is no 1/2 of the community draws it this way.",
            "The other one had the other way and it's kind of typical because I think there's not a single question on which neuro scientist can agree and not even on this question how to draw this here.",
            "So this is postsynaptic firing time minus presynaptic firing time.",
            "And so Conrad just the other way around.",
            "Then.",
            "And hopefully Elvis no Droid in this way then.",
            "So this is a very nice.",
            "Result and unfortunately then also people found out that if you do the similar experiment in different animal or different area, or even at a different synapse on the same number in there, you find different loss there.",
            "So therefore it's not.",
            "As I mentioned, Universal Law, but it's something it's representative of a family of these type of plasticity processes and we don't know what other loss behind this family and the variation of this."
        ],
        [
            "And so we've not looked at this not from perspective for theoreticians or the red curve is obviously an SDP curve designed by through attrition, and so this is the one that worked best.",
            "Works best from the perspective of our theory.",
            "And then this dash banisters, one which is no curved around, which works in the computer simulation almost as good then.",
            "So now for now, the question is now with an H. Had traveled North to reproduce this rectangle, or whether you know there's a reason also for having this, but also.",
            "Israeli Conrad had shown the data all extremely noisy.",
            "Yousef datapoint know all around clustering, so therefore there is not a clean car for know that you ever get from from experiments there, and these are data from a lap of Jesper system who is had moved from from US to use L2 years ago.",
            "He's one of the main and researchers in synaptic plasticity and he also looks at all the kind of no bells and whistles and unpleasant aspects of synaptic plasticity.",
            "And hypnosis was a 2001 paper but he showed by simply doing this pairing of pre and post synaptic firing at different rates.",
            "You get completely different results, so if you do it rarely, only at a.",
            "Every 10 seconds you get depression no matter what is the relationship between post and pre and post synaptic.",
            "Frank if you do it quite often at 40 Hertz then you get no potentiation of weights no matter what the temp relationship is.",
            "And there's a certain middle area only about 20 Hertz where you get the curve of the type node as it had shown you before there.",
            "So when seasonal this is one of these many other parameters, know the frequency of this pairing, which also controls the synaptic plasticity.",
            "And again tells us know how complex this phenomenon.",
            "There's also something which just came out of hand enough from perspective of our theory, but it's it's actually it's a family of results.",
            "It's less studied, namely that also the synapses are not the only plastic parts of a neuron.",
            "Probably actually every component of neuron is plastic, but its excitability or its firing threshold in kind of simple model is also something which is.",
            "Adepts there and the results are so called intrinsic excitability tell you when you simply exciting the room or the seven electrode injecting current that afterwards it fires more easily, so therefore it becomes kind of a feeling.",
            "It's becomes more important and then has to say something all the time.",
            "Then they also other processes that go with a different direction, but this is something which turns out in the context of our model.",
            "If this existed and follow certain laws it could be useful for really inserting priors.",
            "Into such cortical microcircuits, which I will propose could possibly learn from tissue plasticity to become models for the Corporation computations and for implementing based theory."
        ],
        [
            "I should also mention that I find it intriguing is better than their researchers know.",
            "Really experts for memory search and they think know for example burn that re its intrinsic excitability which primarily contains our memory traces there.",
            "So this is.",
            "I think there's also know how much how little we know about really encoding of memory traces.",
            "If you have completely different views than one is not intrinsic excitability of neurons, the kind of common view is not at the plasticity of synapses is really responsible for this and it.",
            "M as theology often is, it's maybe a combination of both effects there."
        ],
        [
            "So this is now an hour cortical motive of aquatic microcircuit know which I will base my talk on and we have no more complex models also which have in many aspects more biologically realistic features.",
            "But this is kind of the embryo in the most simple kind of toy version of what we propose could be a model.",
            "If you inserted into cortical circuits, induced could do something which we can interpret very well from the perspective of probabilistic inference.",
            "Information processing and so this is now here.",
            "Kind of a stochastic winner.",
            "Take all circuit.",
            "I have a new instead wanted said K and I assume that they are have some lateral inhibition.",
            "So therefore whenever by neuron fires it makes sure the other ones don't fire right at the same moment.",
            "So maybe 10 millisecond later now they can have their vote then.",
            "But for the moment they should sell should be silent and so therefore one can view this simply as a representation of a certain probability distribution of multi normal distribution over capital K many values.",
            "And so kind of which neuron fires could be viewed now as a sampling from this probability distribution and the code for this probability distribution is in one hand now encoded in the excitability of this insurance, and which is.",
            "This is the membrane potential which is normally in the mathematical model simply the bias here.",
            "Basically in the other part of it snow probability distributions in code in the weights from these neurons by to this sentence there which I hear this.",
            "In this is WKI then very similar to this indices.",
            "Now how Conrad mentor uses in this is actually an here.",
            "This fairing law is 1 which is no.",
            "Of course no, it suggest.",
            "Soft competition, but it happens to be actually something which no this exponential term of the membrane potential is something which comes up in many papers where they really try to fit new models to really recordings there.",
            "So they usually come up with such exponential terms there.",
            "So this is a model which is not not perceive fitted to data, but it's better fitted to data than most integrated for anyone model segment works with a theoretician, Ann Fortune.",
            "It's very simple.",
            "And it's also it's really probabilistic Norman, so therefore I think it looks like the right player, enough power building up our stage snow for probabilistic inference.",
            "In such circuitry.",
            "You also for this simplest version of this model detail which I want to discuss here.",
            "I assume that the input actually uses exactly the same coding as this one, so I view this as you know, kind of a certain population of neurons in each of them fires.",
            "No one of them fires, and so that they won't probably communicating.",
            "What is the current value?",
            "Office external multinomial variable there.",
            "So therefore this is just a nice symmetry North that this input coding is exactly the same as output coding.",
            "So these are both probability distribution Stan.",
            "So therefore the since you this codings are so nice, you could now you have another network of the same one which simply takes the output as one of its input components there.",
            "So this is model.",
            "Any questions about this is I think hopefully."
        ],
        [
            "Very simple and also this normalization here is happening which we assume some of this inhibition takes care of, which is, I think kind of a bad fashion from perspective of neuroscience because inhibition is has much more structure than many different types of inhibitory neurons with different connection profiles, different dynamic behavior and probably actually nature can squeeze out more functionality out of not just doing here straight normalization there, but this is something also weather data as Poss.",
            "Enough so at the moment I just know kind of know, explain away inefficient over assuming it does."
        ],
        [
            "This normalization then and simply before I discuss the theory, I simply want to demonstrate no one computer extra meant where we see no.",
            "How could no health care insurance in principle emerged in the brain then and actually.",
            "Also, I think the theory may provide a partial answer to this question, that one of you asked why did they ever managed to find this helper in the runs.",
            "So therefore I think this theory would propose whatever you have a large enough population and you show help Paris and other things.",
            "No one of them will probably become an expert for help very then.",
            "If it's enough important enough for this system, so here now I didn't have images of Halle Berry Butthead here.",
            "Patterns to dependence of this type and Nancy snow.",
            "Here he is kind of limited with Mccluster here at the top here my the bottom here on the left side here on the right hand side and so these are the way how this inputs are for different inputs presented now through this population code included."
        ],
        [
            "Network and once you know this, further samples, no one sees you know these two look very different in the sense that, but they both have more cluster here at the top here at the bottom and so and we want to find out whether the network and on its own find out what is the hidden cause of this patterns and the hidden cause of this patterns.",
            "Obviously this for different cluster centers, but from every single input notes very hard to infer about.",
            "The cluster centers are because you the maximum probability.",
            "I think this was .04.",
            "Pics of the great drawn then and so therefore no, you have this large tried to try durability as one sees also in real neural data, so there's no one gets this know simply as a population vector Norlin era race then and also each pixel is encoded by population.",
            "Coding is only binary value and if it has value one if it's black, not one of these two spike trends.",
            "One of these factions is active, it fires at a certain hurts, maybe 40 Hertz for certain time.",
            "If it's right the other one of this pair of neurons?",
            "Which encode this pixel fires there.",
            "So this is something which little bit makes note of analysis easy, because we know no matter how many pixels are black, the number of neurons which fire he is always the same then, because no matter whether the pixel is bigger, abide always one of this turn on fire.",
            "So this is certainly here.",
            "No model simplification and we have another models which don't require this.",
            "But anyway let's live, it is for the moment and so this is now how these four different patterns actually are presented to this circuit then.",
            "So this.",
            "800 spike trends and so this you know we have no color coded this for different sources.",
            "The question is now when we now simply know send this bike trends for certain time.",
            "To this winner.",
            "Take all circuit that have shown you.",
            "Could there be learning rules which enable these neurons to pick up?",
            "What are the underlying patterns Nobel various help very northern."
        ],
        [
            "Is a latent variable here, and so this is the results how it turns out.",
            "Then after sending in such, no, simply no randomly drawn patterns, each one for 50 milliseconds.",
            "So it's about every neuron that has a higher frame rate, fires one or maybe two spikes in such a period, or maybe none.",
            "And these are this output neurons before learning, and so these are the output neurons after learning.",
            "And so this colors as something we put on.",
            "Because this is a new one which tends to fire all the time.",
            "When you know this fourth kind of pattern type of pattern was on the hidden source of us, know that this was in the lower part.",
            "This and so these are currently was the one bit I think the part of the left or so, and so these are now simply this weight vectors that emerged here and then see something interesting because we have this nice discussion about generative versus discriminative model.",
            "So this network doesn't have any generative component.",
            "There's nothing propagated backwards everetts.",
            "Very simple now computer science designed to feed forward network.",
            "But we still see it forms a generative model because these are really know, kind of.",
            "It doesn't try to remember my hard one of these patterns.",
            "It tries to capture what is the kind of the typical statistic nature of these inputs and it simply guess is that this is a Gaussian here, which is centered around here.",
            "And this was exactly the hidden variable.",
            "Know how we generated this pictures here and we saw here these are this way to vectors of this for nutrients in this case.",
            "On this competitive layer, and each of them found their own kind of latent variable and built in their weights automatically.",
            "An internal model for this latent variables.",
            "To be only, of course, in these weights, no, they don't have a 2D structure, we simply projected them back into this 2D space, know where they get the spike, transform the, so this is hopefully clear that.",
            "But this is not the way how the generative models that they have built for the sources of the spike patterns here right now.",
            "This one from seeing or many patterns like this, it has built up a generative model which looks like this here, and this is here.",
            "But this learning rule about excitability does it, simply it learns that there different priors because I forgot to mention before this patterns for showed.",
            "The different probabilities here, so we have shown this this one four times as often as this pattern of these latent variable here and so, therefore automatically in this bias is operating normally in the.",
            "Potential function applied to this WK0 from this weighted sum.",
            "When CC that these values release.",
            "Converge to what?",
            "Are they really value probability values here?",
            ".1 in here .4 then.",
            "So therefore once easier that is very simple.",
            "Network is in principle able from this stream of really marking or very noisy data to recover both generative models for each of these four input sources and also civil tendencies.",
            "Their priors here.",
            "And I want to explain now which learning rules are used for this and also what is the theory which allows us to understand know why this happens here.",
            "But any questions so far to what's happening here?",
            "So I guess when we argue about whether when it was this generative or not, but it's doing some kind of clustering.",
            "Yeah, fun, good quality clustering over months to you.",
            "Also should say no.",
            "I think we're not the first, nor proposing this simple kind of generative models without backwards propagation.",
            "I think there was a paper by Ghahremani and hipness so basic over their head.",
            "For mixture of Gaussians also proposed this, but it thinks something which I find not quite nice becausw I think not all this backwards propagation of feedback connection.",
            "Ideas know that machine learning people like no in this context of generative models, I think they have a hard time to fitting them with any data there, and I think for other reasons.",
            "Also simply the timing is not right.",
            "So I'm very suspicious of all these explicit reactive models here than in men.",
            "See also that the retail have models are really only at that part of the model.",
            "That's essentially cause their guide.",
            "The learning we will see in the end.",
            "No, there really not the learning goal of this learning of the system is to fit their internal models to the world.",
            "Stimulated, they get the answer for this.",
            "No, they used relative model simply to maximize log likelihood implicitly of the inputs there, but it's really happening implicitly.",
            "We will see here and doesn't have to be so blunt and really propagate things and then matching.",
            "Does it fit to the input or not?",
            "Then, which would be something I think neurons would not."
        ],
        [
            "Like so much.",
            "So now let's try it now to go to the anymore questions to the what's happening here.",
            "OK, so this is how the first step.",
            "You know how we can understand what's happening here, and I think the key discovery behind this is possibly that Yvonne sets up this learning rules in a particular way which doesn't depend so much on the shape of this, but really depends primarily on how much we, in larger weight.",
            "If it's in the kind of know right window.",
            "Versus how much we.",
            "Downgrade evades know when this prior post synaptic.",
            "Difference has a wrong value there, so this is namely that you can achieve here principle that this weights converge to a certain lack of a conditional probability, and I think this is for me really something.",
            "If nature would we do this?",
            "It's very nice fast and because it allows us suddenly to create a link between the Sue of synaptic plasticity and things we can understand on the level of probabilistic analysis there and so therefore the claim is here know that even uses this particular learning rule where.",
            "If you put in, say, did you see it?",
            "You have to eat to the minus the current value of the weight, and if you simply availed gets decreased it simply by a certain unit minus one.",
            "It's decreased.",
            "And of course in the background there were some learning rates.",
            "Not like in every online learning.",
            "There in one can prove they actually quite simply that this with regard to expected weight changes.",
            "This lock probability is the only equilibrium of a wait no and so therefore.",
            "This is in our starting point and so this is nice because now we can ask yourself, know what is this condition probability good for, or if it maybe if you want to did kind of induce a different types of probabilistic computation in the network, what should be the variation of the learning rule which let's them the weights converge to different log probability.",
            "And as you probably have guessed, know in principle it's fast theoreticians.",
            "It's much more pleasant to have things converge to locks of probabilities becausw know in the end now.",
            "Yes, we're based here.",
            "You have to multiply probabilities and neurons and lots of things said.",
            "If you really know and yes, I'm theoreticians also claimed that do multiplication, but it's not something which comes out, not without some tweaking so, but but everybody agrees.",
            "No simply, the membrane potential is a great tool for adding things then and so therefore obviously you can do multiplications easily in lock domain then and so this is probably one reason why this theory works nice with a sibling room model where you have this additive.",
            "Membrane potential and working with lock probabilities.",
            "Unpleasant technical aspect.",
            "Which is he also know here, hidden in this last line there look probabilities tend to be negative, which is no not consistent with our assumption.",
            "We want to hear talk about excitatory connections there.",
            "So what we do in our computer experiments is simply we.",
            "We restrict the range of this lock probabilities from minus 520.",
            "So we simply cut it off at minus five, and then when we go through really network simulation, we shift everything by 5 upwards.",
            "So therefore minus 5.",
            "To 0 is shifted to zero to five, which is in our positive range of weights then and so we kind of virtually always from the perspective of probability theory we know we use this non negative range.",
            "We want to interpret this as a kind of a circuit effect in terms of the room models we shifted upwards there.",
            "So this is 1 unpleasant aspect of this.",
            "I don't know how to get around it."
        ],
        [
            "But maybe you have some idea later.",
            "So what I want to claim this, and actually that this simple network that had shown you this soft stochastic winner take all circuit when really can prove that in principle by this simple learning would have shown you automatically learns to carry out base theorem there.",
            "So it learns both prior and this likelihood.",
            "And so here you know this is not the K. Is the neuron set K the case output in the rent and the probability that it nor in the posterior distribution the case output.",
            "Fires which could be interpreted.",
            "Snow is no sampling from this posterior.",
            "Says know that V as a network belief.",
            "Now that the case hidden source is no really cost.",
            "This current input, and so this is based on the current weights hitting within this network there and so this is no cause emerges from the product of prior and hear the likelihood.",
            "And since you see here we take the exponent of this memory potential automatically E to this.",
            "WK0 is kind of corresponding to this prior, and this term is corresponding to this likelihood, as I will show you in the next slide then.",
            "And so therefore.",
            "This is something which, in principle, if something like this would exist in nature.",
            "Of course I think would solve lots of problems, you just have a complex remain.",
            "You have all this local agent, it's kinetically coded for inputs.",
            "Come in there, and so a lot of variability there.",
            "Not two areas in the brain which get exactly the same inputs there right?",
            "So in principle all the symmetries is broken by simply by looking at the input and output of local circuitry and possibly but it's local circuit.",
            "It could then do the same computation, and if it has useful computation it could be everywhere useful there.",
            "And I think 1 interesting hypothesis, but speaking from perspective, this workshop is.",
            "This could just be learn to carry out basic theory.",
            "Every single work that every single model of this of this complex."
        ],
        [
            "In network there.",
            "So this is a little bit looking at what is here in principle about types of likelihood distributions are supported by the simple model that have shown here and so this mixtures of multinomial's there where this one can simply know if you write again let me start here, then simply fall.",
            "So coming to formally to generative model.",
            "So if you look at the joint probability that the case neuron of these fires for the current spike input by.",
            "Coming from these neurons here, this is no, by the way, how the models defined.",
            "It's not given by this distribution.",
            "See is just some normalization factor.",
            "If you now want to marginalized out this case here K here all these different latent variables.",
            "This is now there really.",
            "The generative model of the input, which this whole network is building, only input up to here, then and now.",
            "If you're further analyze this now, but it really says that if you now look at this happen concretely, this is the prior.",
            "And this now can be viewed simply as a multi normal distribution over this original external variables.",
            "There each of them is encoded in the population coding in the binary code, then button altogether in a mathematically just boils down to this multi normal distribution.",
            "So this is each no.",
            "Actor and so this this is reporting an extra one of them in on simply this questions.",
            "But is the current value of this external variable X Ray?",
            "And if this is on then this factor into this product.",
            "Otherwise it's not simply London, so simply this is just a complicated way of writing multi normal distribution and see if it sees human has also interpretation of what this weight from this new and I to this known K is.",
            "Namely that E 2 WKI would be this probability.",
            "Which this factor contributes to this product in this multi normal distribution there will show later.",
            "This theory is not unfortunately not nailed down to mixtures of multi normals, but when can take mixture of anything which factorizes nor an exponentially family.",
            "For example everything there can be used and probably mixtures of products of person distributions is something which is more adequate here for the setup of really working with spike trends and so it's just a little bit more complicated mathematics.",
            "So therefore I wanted to stick.",
            "To the simpler case then.",
            "This is no, but this model is simply not kind of designed.",
            "No in principle, but it represents every multinomial distribution came with no capital K factors case.",
            "The number of neurons here can be represented by this, and what I would argue is that no matter what input you sent here into this circuit, it always tries its best job candidate to fit a multinomial mixture of multinomial's to the input distribution that it's getting there.",
            "This is really."
        ],
        [
            "The function in the summer surprising mathematical insight which underlies here, is that actually, if the actual distribution is something very far away from a mixture of multi normal, so it's a mixture of whatever Gaussian, so so it still tries to fit it as good as it can with a mixture of normals and as good as it can means here mathematically as good as any am can do it there.",
            "And so I think this is for me was a surprising insight, in principle is very simple.",
            "Local learning rules can be rigorously proven to approximate expectation maximization, which as you probably have heard about it, it's the most powerful tool really.",
            "For unsupervised learning there right, and I think it's a new, I think hope or perspective that possibly simply neural networks in synaptic plasticity could carry something out of this type then, because it would provide much more understandable functionality than any more local operation.",
            "You stage algorithm does this.",
            "Do a sort of local EM where you're like these things where you actually do.",
            "Interleave E&M sex much closer?",
            "Or is it?",
            "Yeah, let me go through this.",
            "I have, you know this few lines.",
            "Then and then we can come back to the family when approximate stochastic online EM then and then we can prove from principle know there's like drunken in the in the background and that this instead, which is a single application of SDP makes moves the parameter vector in the direction of this M step there.",
            "So this is the best one can do so.",
            "But let me go through this here just so because I think interesting.",
            "I think nobody.",
            "No, no group before had no really brought in synaptic plasticity in the context of of am then and so therefore because this SCM or send them for Spike base TM then.",
            "So when were drunk?",
            "Improve is maybe let's start with the easy step.",
            "The step is simply when you get now a complex spike train, you have to figure out what is best, guess what was hidden source of this current spike trends that this circuit is getting there and so this is simply the eastep know in the formal way.",
            "Eastep simply means you make the best case on the basis of the data that you have so far.",
            "Then now the M step is know when now one of these neurons fires in this competitive layer.",
            "In synaptic plasticity, recipes only applied to this number in there, which is the winner kind of competition in this one trial, there in this corresponds to the M step becausw know kind of understanding the Eve way that if a similar input comes again it makes this more likely to win again this competition there, so it moves a little bit.",
            "This kind of this neuron snow in this competitive layer which all the group for what could be possible general hidden sources for explaining this data.",
            "For kind of becoming more the expert for this type of input, or for this cluster Now, as Bennett has suggested, if I want to see this more visually, then and so this is something which one can also prove more formally.",
            "But I don't want to go in this analysis, but I think in the end I think the vision I think is here for me.",
            "I think the most hopeful aspect of this that we can because usually know if you have these things now.",
            "Without this EM theory in the background.",
            "You would just think not if these two steps are iterated know this thing just may underthrew wait space around there but am guarantee issue that you know you're making progress with regard to an optimization function, which I will make explicit on the next slide then and so therefore it's going to.",
            "So, at least to a local optimum of this objective there, and I think this is very nice to have here.",
            "A complex distributed network, but we can still understand possible its functionality is together trying to optimize a certain objective function there, which then of course theoretician makes it easier to understand the whole process, and I'm not sure whether you answered your question in the mean time."
        ],
        [
            "OK yeah, so this actually know this two guys here better Nessler Michelle Pfeiffer, who came up with this.",
            "So this was very compressed in NIPS paper and we're working on an understandable version of this snow in about a month.",
            "It should be done.",
            "You can send the email if you are interested, could send it to you and so one can either no value in this optimization function that simply the cool bar library versions between the extra distribution of spike inputs in this generative models.",
            "Which has been built with the help of the current setting of, albeit vectors of all these neurons.",
            "They try to minimize this and or equivalent.",
            "You could say the log likelihood of inputs is going to be maximised.",
            "Then by this whole process and so this is very nice because when really sees that it tries to do something which we can under Intrepid on the probabilistic level then.",
            "Any questions to this?"
        ],
        [
            "So these two guys, now we have an awful camera, but they are very talented PhD students and their Hebrides.",
            "Workout series that in principle one can move away.",
            "This two kind of most obvious kind of unpleasant aspects of the simple Model 1 is that one had no.",
            "I looked here.",
            "The mix of multi normals so Steven happen shows has shown that mixed.",
            "Of anything from the polynomial, exponential family is just as good then this is, I think, but it's going to stay at this level.",
            "Mixtures of something.",
            "This is just the capability of this particular symbol model.",
            "Otherwise we would get into all kinds of trouble when he's with false machine learning and so on.",
            "And last freezing has shown something very nice, because in real life."
        ],
        [
            "Never have really a network nor in neural system where you have here a bunch of input neurons and here bunch of output neurons and you have complete connectivity.",
            "Rather, as I mentioned, this is a continuous sheets in between two layers.",
            "You simply have these two sheets and you have many local connections from Muncie to the other one and you have lots of local letter inhibition but you don't have anything perfect.",
            "No, they don't exist.",
            "These models that exist here and so therefore last piercing has developed."
        ],
        [
            "Nice extension of this here, which allows us to deal with this situation there, which is now becoming something where we can really applied now to any circuitry which you know kind of is simply models explicitly no neural circuitry, but becomes out his nose is mankind are longer characterize so easily, but it's subjective distribution.",
            "Then this becomes more difficult in this set up there, but this is something I think it goes a little bit beyond this, but I think this."
        ],
        [
            "Spill make this more realistic or hopefully also.",
            "Verifiable or disputable from the perspective of the system, so then we became Bolton still softener.",
            "Maybe let's do something.",
            "Also, if you're understanding about learning poses a possible, let's do something cool like no Ben had would have done there working with real data set like this aimless data.",
            "Set of handwritten digits.",
            "But we made our life difficult for the neurons because neurons don't have a supervisor in the brain, so they have to figure out themselves.",
            "No, but the images are so now the database comes with annotations.",
            "No kind of supervised.",
            "We left them out.",
            "We never told the neurons know what which digit was which one.",
            "We simply showed it to them and we use the same kind of know.",
            "Naive way of encode pixels by each pixel by two spike trains.",
            "One of them is always on then and then we showed this."
        ],
        [
            "And so this is not we had hundred neurons on this competitive layer competing each of them finding their kind of the Hill Berry in the world.",
            "And you see that of course they have no idea.",
            "Know that kind of this vertical line has the same meaning for us as a slanted line.",
            "So therefore you have no experts know becoming experts for various new kind of degrees of slanted lines, then right?",
            "And you have find here the same for for any other digit know their different ways of writing.",
            "Three and you have different experts for coming over, but I think that's what's nice.",
            "I think to see Snowman, see see that in principle, self organization process can take place then, so that all of these hundred neurons there find their way of a certain kind of proportion of the input, which then can kind of model for which they build a generative model.",
            "And each time when something comes up which is similar to.",
            "The thing OK, this is now my turn.",
            "I think it's me then and you have some of them who couldn't kind find away.",
            "So we have a certain variability.",
            "Also you have noticed right to try their ability.",
            "So this by the various will simply how we showed this with the network.",
            "So it learns pretty fast for 300 seconds of political time and we showed them 30 millisecond is encoding of 1 handwritten digit that is 30 millisecond another one.",
            "And so this is just flies by then.",
            "And they have to find out of the zoo of Spike tread patterns, which are very, you know, have lots of our ability within this class.",
            "What they want to focus on.",
            "So this is quite nice, and I think because this tells us that in principle.",
            "So now with network of sparkle neurons and the simple learning rules, we can do something which is not so completely."
        ],
        [
            "Not only stupid and toy like in principle from the perspective of a neuroscience perspective, when sees that this experiment demonstrates the possibility of explaining something which I find very interesting, which is simply called perceptual learning.",
            "Then if you try to look at a certain types of no visual tasks, say you look at often to see whether 2 lines are parallel, not parallel after while doing this you become very.",
            "You become better at this then.",
            "Even if you don't have any supervisor which ever tells you whether your answers became better or not, then you simply become better at this thing.",
            "At the same time.",
            "Also, when animal records from the rooms one finds after perceptual learning, they have a sparser response then.",
            "So it's not that activity goes up, but it goes down.",
            "Then the other hand you have.",
            "Let's try to tribalism reproducible kind of firing.",
            "And you can interpret this as known, have become experts for this particular patents which occur as part of this task, that they have to solve them.",
            "And so this is simply here obviously, but occurs here.",
            "You have this hundred neurons, and here we showed them now five different digits and always know one of them fired actually, and even although no for each sample, the burner, on average, 10 of them.",
            "Experts of this but a single one of these 10 sub experts even think this is my way of writing of London.",
            "And so therefore you have in principle this pass firing.",
            "So this of course no, no, not really neuroscience becausw.",
            "Is not in a second brain which gets too."
        ],
        [
            "Visual input encoded this way, so therefore when we ask us have it be sync Now really about it?",
            "Not so much from the machine learning perspective, but simply from the perspective of computational models for the brain where we want to think of large networks of transportation models, which each there's something useful.",
            "In this context, we're sitting in the brain.",
            "I think this is comes much closer to this, and so let me go through this year.",
            "So this is no big head again, no large number of inputs 500 but only 100 are sold.",
            "And this actually something which is unusual for this kind of experiments.",
            "We always found that our results get better if you have a larger input dimension, which is insofar interesting as a real neuron gets about is about 3000 presynaptic pyramids helps there, so they always work in the range of thousands verse.",
            "Almost all computer simulation results.",
            "So far we know how to maybe 20 or 50 or so 100 presynaptic neuron.",
            "So this seems to be methods which really work much better if you have large number of presynaptic neurons.",
            "So what we did here is so these are just for some spike trends you're not supposed to see any pattern.",
            "There are no patterns in principle, but what we did here we froze certain for some patterns here and repeated them.",
            "So here there's red pattern is 1, which is repeated here again then.",
            "Although part of this is also spontaneous noise, so this was all together with 20 Hertz for some spy kids only these patterns is red here.",
            "Spikes, by the way.",
            "So this is all this time, so you may not be able to read it.",
            "This is now.",
            "Here's one second from here to here, and so there's no numbers.",
            "100 neurons are shown, and so 15 Hertz is a frozen person pattern here, which know the red dots.",
            "Here.",
            "No 3/4 of this dots in here are red, and there should repeat here.",
            "Also in this time window again when we show again this patterns, but 55.",
            "So 1/4 of this spikes here black and so these are simply spontaneous 5 Hertz noise which is superimposed so therefore.",
            "This patterns only agrees here only on 3/4 of its spikes there, and so therefore we had not created several of such patterns here.",
            "Now as many as we have colors know which can be shown to be different on a on a graph, then on a figure then I think we have 5 here manage then and then we set up here no this network with no 6.",
            "Lawrence, Ian and we showed this interesting movie.",
            "Now to this network for 22nd there.",
            "And but emerged there were there.",
            "Now five of this Lawrence.",
            "Each of them became an expert for one of this embedded patterns.",
            "So here this one neuron, which is then also hear this.",
            "But it spikes are marked with green.",
            "It starts to fire shortly after this pattern becomes shown then, and fire still a little bit while afterwards, and it goes down.",
            "It's hiding all the time all the time until the next time the screen pattern comes up here, so we can see here that really this five neurons, each of them they have detected their head buried in or in the world then, or particular pattern which occur to number of times in different contexts with different noise.",
            "In different variations and they only fire in the future when this pattern comes up here, there were only five patterns embedded into this versus 6 neurons, so this six one is this year.",
            "This new number for where the spikes are marked in black, and so this always fires when there is no pattern on them.",
            "In the Super we would now in certain now start inserting 6 pattern into this bike trans.",
            "This probably would become an expert of this.",
            "This new one doesn't have season founder kind of reason of coding in the world.",
            "It's still groping around and it's firing.",
            "None of the other ones are sure that they have found their help very than in the input and.",
            "So they suggested firing probability of these neurons.",
            "Notice this E to the UK's then which are always discussed before and here I I no longer know, played around with the prior.",
            "So so this was only for demonstration purposes in the first inductor experiments here they were about equally distributed there and."
        ],
        [
            "So an interesting perspective, I think from the perspective.",
            "Apologies also if we not only let this intervals between these patterns vary.",
            "But if you also apply now time warping to this frozen for some pattern here, like here, this red is probably only know maybe 2030 millisecond lemon.",
            "Here it's I don't know 80 or 100 millisecond long then and so principle.",
            "Now this is something for many models for computing with spatial temporal patterns with spikes of it spiking romances.",
            "Non trivial problem advance easier.",
            "This network which was just trained on the regular patterns had never seen during training know this kind of time for pattern there just as reliable here this revenue and fires here for this long version is for the short version there in this actually also in summer little bit naive Fenton Simple explanation for this big cause.",
            "This network is not really geared to create spatio temporal internal models is really only creates spatial internal models so therefore it tries to guess.",
            "What is what is kind of an average no set of neurons which is typically active in the thread pattern is on then.",
            "So really it extracts the SPF average spatial pattern out of this spatial temporal patterns there and I don't want to argue that this is happening.",
            "All there isn't a brain about temporal processing, so you would have to use additional circuitry and we have also variations of this simple network.",
            "Simply if you now you have interconnections on this output layer, so then it becomes an HMM, and so therefore it makes prediction.",
            "If the first is new and said one has fired, then I am.",
            "Neurons had three now and I have a hard, larger chance of being active again.",
            "Then someone can expand this model also into something which really does know kind of more sophisticated processing on temporal level then yeah.",
            "It's a good question.",
            "I think the problem becomes and really what is the interconnections among this set ignorance there?"
        ],
        [
            "Yeah, so here it was about this.",
            "So if it has interconnections among these neurons, excitatory connections there, that kind of 1 gets into trouble with now theory.",
            "Now from the probabilistic perspective, which doesn't get into the way of this generative weights, and so this is, I think the theoretical problem there.",
            "Altogether."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I think it's very nice that you organized this summer school here in the last few years at NIPS.",
                    "label": 0
                },
                {
                    "sent": "I really like most this particular interaction of machine learning and cognitive science, and starting also growing into neuroscience research, and I think it's really needed to move this type of research combination to Europe.",
                    "label": 0
                },
                {
                    "sent": "I think it's much more still centered in the US.",
                    "label": 0
                },
                {
                    "sent": "When in principle, the cognitive neuro scientists, especially Josh Tenenbaum, had laid out the problem for us.",
                    "label": 0
                },
                {
                    "sent": "Know they have very nice models to explain behavior or about human mind.",
                    "label": 0
                },
                {
                    "sent": "But as Josh mentioned, the really burning question is how could neurons really implement these type of probabilistic inference operations and represent probability distributions and convert.",
                    "label": 1
                },
                {
                    "sent": "A very nice introduction also telling us how difficult it is to say anything with certainty in neuroscience.",
                    "label": 0
                },
                {
                    "sent": "I think the mostly fundamental question early unanswered in neuroscience.",
                    "label": 0
                },
                {
                    "sent": "On other hand, I think also it's not a situation where we can simply stand back and wait until the guys have had extra meant results be cause I think they need also be provided or provoked by theories and which they then refute or confirm then.",
                    "label": 0
                },
                {
                    "sent": "And I think many of their theories which they have in the back of the head like synfire chains or so.",
                    "label": 0
                },
                {
                    "sent": "There simply not up to speed now with regard to inside for machine learning.",
                    "label": 0
                },
                {
                    "sent": "Another insight, and I think it's a duty or chance of this community.",
                    "label": 0
                },
                {
                    "sent": "Also particular machine learning part really to provide ideas and models for neuro scientist and interact with them.",
                    "label": 0
                },
                {
                    "sent": "So for example we benefited a lot from the European projects where they bring machine learning people and extramental scientists together and I think in this regard Europe in Europe we may actually have an advantage of are related.",
                    "label": 0
                },
                {
                    "sent": "Research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have here a little bit less sophisticated summary of the vector or hardware that behave in the brain.",
                    "label": 1
                },
                {
                    "sent": "So for computer scientists really provocative, know this.",
                    "label": 1
                },
                {
                    "sent": "This picture size to millimeter 6 sheet of neurons, the greymatter.",
                    "label": 0
                },
                {
                    "sent": "And this has about 10:50 runs.",
                    "label": 0
                },
                {
                    "sent": "It consumes about 50.",
                    "label": 0
                },
                {
                    "sent": "Button is apparently much better than any artificial computing computing machine that we have currently, even once to understand know something about the structure of this circuitry.",
                    "label": 0
                },
                {
                    "sent": "This is here.",
                    "label": 0
                },
                {
                    "sent": "This typical structure is laid out six layers which know sometimes it says no.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                },
                {
                    "sent": "But if you take a closer look, it's everywhere, slightly different.",
                    "label": 0
                },
                {
                    "sent": "Know how thick this particular pens are then.",
                    "label": 0
                },
                {
                    "sent": "And then when you look inside and no longer here, every neuron body is 1 dark spot then.",
                    "label": 0
                },
                {
                    "sent": "But the difference?",
                    "label": 0
                },
                {
                    "sent": "The neurons are very different.",
                    "label": 0
                },
                {
                    "sent": "Also there about 100 genetically different types of neurons there.",
                    "label": 0
                },
                {
                    "sent": "And so this network is really very complicated and so one of these streams is to find out what is generic competition function of such no microcircuit as when calls them as a matter of convenience, but it's really it's a continuous sheet number should not think.",
                    "label": 0
                },
                {
                    "sent": "That this is made out of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Internationally called them, but anyway, so this is another way how people have looked at it.",
                    "label": 0
                },
                {
                    "sent": "And so this is no.",
                    "label": 0
                },
                {
                    "sent": "Figure which I took from a very nice review paper by Rodney Douglas and Kevin Martin from Etoh, Zurich, who have been one of the key people who proposed really look.",
                    "label": 1
                },
                {
                    "sent": "Also at the function of this generic cortical microcircuits, and this is, I think still the latest proposition in the sense in terms of function.",
                    "label": 1
                },
                {
                    "sent": "Now you see here, this layers in principle symbolically and are mainly think of this.",
                    "label": 0
                },
                {
                    "sent": "Is having two arrays of soft winner take all circuits there?",
                    "label": 0
                },
                {
                    "sent": "This is their main hypothesis and I think this most of the data are based from neuron otomy some Physiology also and I think it's widely known that the interaction of excitation emission is really crucial, but doesn't quite know.",
                    "label": 0
                },
                {
                    "sent": "But the loss and the computation goals of this interaction are then and I think the goal of my talk is to present some recent.",
                    "label": 0
                },
                {
                    "sent": "Hypotheses, ideas, and then I think you see all the simultaneously.",
                    "label": 0
                },
                {
                    "sent": "The difficulty of any such hypotheses, and perhaps you come up.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with better ideas.",
                    "label": 0
                },
                {
                    "sent": "So actually I want to interpret this whole fanatic all circuit also re direction of this workshop.",
                    "label": 0
                },
                {
                    "sent": "Namely, I would like to see this no more stochastic winner take all circuit and I think there's also a lot of experimental evidence becausw no one is no, these are spontaneously active even if you don't, you close your eyes and your visual cortex is almost as active now as when you have your eyes open.",
                    "label": 0
                },
                {
                    "sent": "And also there's this frustrating large trial to try their ability, which people know who obsessed.",
                    "label": 0
                },
                {
                    "sent": "Understanding the brain but understanding the neural codes of particular neurons are very bothered by this because this suggest to them that this is not a deterministic machinery's would like to see the brain but from the perspective of probabilistic inference this could just tell us that each trial is a sampling from posterior distributions and the brain really aims to represent in these circuits distributions there and each time we make it extra mental your sampling from this posterior there and so therefore from this perspective this tried to try probability.",
                    "label": 0
                },
                {
                    "sent": "Could actually be a senchal part of the function there, and I think this is 1 ingredient which this community might bring into the resigns.",
                    "label": 0
                },
                {
                    "sent": "Because I think missing simply kind of convincing computation models which can deal and live with this kind of variability.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So announcing the title that I want to talk about about hypothesis how probabilistic inference could be truly encoded in the brain.",
                    "label": 0
                },
                {
                    "sent": "But how it could emerge from plasticity from learning there and learning and synaptic plasticity is another can of worms, where which is extremely complicated.",
                    "label": 1
                },
                {
                    "sent": "So this is just a drawing from a textbook of the presynaptic bouton.",
                    "label": 0
                },
                {
                    "sent": "You might see no there there, this vesicles and when Hera presynaptic spike comes.",
                    "label": 0
                },
                {
                    "sent": "One of these vesicles flashlight fuses here with the membrane.",
                    "label": 0
                },
                {
                    "sent": "This neurotransmitter is left out and then you know the some of these moves to the synaptic cleft and over there and it opens receptors there and so this is an extremely complicated device and I think typically invalid.",
                    "label": 0
                },
                {
                    "sent": "See everything that you see in structures.",
                    "label": 0
                },
                {
                    "sent": "Also adaptive or plastic answer when seasonal.",
                    "label": 0
                },
                {
                    "sent": "How many different components there are that could in principle plastic and most of them are plastic.",
                    "label": 0
                },
                {
                    "sent": "This is a figure from a paper of Eric and Dell's group in New York which shows the postsynaptic site and it indicates.",
                    "label": 0
                },
                {
                    "sent": "Cartoon this cascade of process is really triggered when you have no something which might be viewed as synaptic plasticity because at the moment receptors are just techno by first relation or so, which is kind of a current state then, but it initiates particularly calcium.",
                    "label": 0
                },
                {
                    "sent": "Different processes goes to the cell body.",
                    "label": 0
                },
                {
                    "sent": "In the end it reads had particular parts of the DNA and these are moved back to the synapses and then.",
                    "label": 0
                },
                {
                    "sent": "Insert no additional receptors in into this membrane there, so it's an extremely complicated process and I think one has to assume that all of the existing data only give us the tip of an iceberg of knowledge about this is much more complicated and the data depending on which slipped acid, what is animal are there awake anesta size know about is the type of slice and all these questions there and I think one should not as a theoretician look at this particular not take from any theoretical book of paper.",
                    "label": 0
                },
                {
                    "sent": "They're kind of a textbook learning rule and say this is now what I have to live with.",
                    "label": 0
                },
                {
                    "sent": "I think it's really difficult different.",
                    "label": 0
                },
                {
                    "sent": "I think situation is we don't understand the system and I think we should have functional ideas how the system might work, how it makes sense on organization level, and then come up with predictions or questions with instrumentalists can then decide.",
                    "label": 0
                },
                {
                    "sent": "Then I think this is really the way however.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can expect to make more progress in this direction, but anyway, conventionally SDP, which has been discovered in 98 by three groups independently, one of them group so female neuroscientist Kirsty Grant is generous in France and.",
                    "label": 0
                },
                {
                    "sent": "Found out this is the first time that extreme interest.",
                    "label": 1
                },
                {
                    "sent": "Learn to induce synaptic plasticity in a kind of seem realistic, no physiological way as they say, and not by kind of shocks.",
                    "label": 0
                },
                {
                    "sent": "No of current snow which inserted and so he have to neurons and ideally want to have one electrode in the postsynaptic neuron and running the presynaptic neuron and so then you can trigger spiking of the presynaptic neuron and this causes a certain postsynaptic potential.",
                    "label": 0
                },
                {
                    "sent": "So this is coming and going this pointer.",
                    "label": 0
                },
                {
                    "sent": "And then check out the postsynaptic potential in the postsynaptic neuron and then is already Conrad mentioned.",
                    "label": 0
                },
                {
                    "sent": "Depending on the relative timing on pre and post synaptic firing the strength of the synapse goes up or goes down.",
                    "label": 0
                },
                {
                    "sent": "Here in this drawing.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "But he's the green part is where it goes up is and this is where the presynaptic neuron fires before the postsynaptic neuron.",
                    "label": 0
                },
                {
                    "sent": "Nothing corner mention already.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, from the perspective of probabilistic inference is great because it shows already that this device tries to find causes.",
                    "label": 0
                },
                {
                    "sent": "By this post synaptic neuron fired.",
                    "label": 0
                },
                {
                    "sent": "It doesn't go for correlation, it goes for causal analysis of the things around it in circuitry.",
                    "label": 0
                },
                {
                    "sent": "And if this presynaptic neuron fires after the postsynaptic neuron is definitely not cause it, and so this at the moment looks know.",
                    "label": 0
                },
                {
                    "sent": "Now he feels like a bad connection or useless connection and then this weight goes down then really also.",
                    "label": 1
                },
                {
                    "sent": "We use the office set definition of the science very, very good.",
                    "label": 1
                },
                {
                    "sent": "Good part of mental mentions.",
                    "label": 0
                },
                {
                    "sent": "So this is no 1/2 of the community draws it this way.",
                    "label": 0
                },
                {
                    "sent": "The other one had the other way and it's kind of typical because I think there's not a single question on which neuro scientist can agree and not even on this question how to draw this here.",
                    "label": 0
                },
                {
                    "sent": "So this is postsynaptic firing time minus presynaptic firing time.",
                    "label": 0
                },
                {
                    "sent": "And so Conrad just the other way around.",
                    "label": 0
                },
                {
                    "sent": "Then.",
                    "label": 0
                },
                {
                    "sent": "And hopefully Elvis no Droid in this way then.",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice.",
                    "label": 0
                },
                {
                    "sent": "Result and unfortunately then also people found out that if you do the similar experiment in different animal or different area, or even at a different synapse on the same number in there, you find different loss there.",
                    "label": 0
                },
                {
                    "sent": "So therefore it's not.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, Universal Law, but it's something it's representative of a family of these type of plasticity processes and we don't know what other loss behind this family and the variation of this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we've not looked at this not from perspective for theoreticians or the red curve is obviously an SDP curve designed by through attrition, and so this is the one that worked best.",
                    "label": 0
                },
                {
                    "sent": "Works best from the perspective of our theory.",
                    "label": 0
                },
                {
                    "sent": "And then this dash banisters, one which is no curved around, which works in the computer simulation almost as good then.",
                    "label": 0
                },
                {
                    "sent": "So now for now, the question is now with an H. Had traveled North to reproduce this rectangle, or whether you know there's a reason also for having this, but also.",
                    "label": 0
                },
                {
                    "sent": "Israeli Conrad had shown the data all extremely noisy.",
                    "label": 1
                },
                {
                    "sent": "Yousef datapoint know all around clustering, so therefore there is not a clean car for know that you ever get from from experiments there, and these are data from a lap of Jesper system who is had moved from from US to use L2 years ago.",
                    "label": 0
                },
                {
                    "sent": "He's one of the main and researchers in synaptic plasticity and he also looks at all the kind of no bells and whistles and unpleasant aspects of synaptic plasticity.",
                    "label": 0
                },
                {
                    "sent": "And hypnosis was a 2001 paper but he showed by simply doing this pairing of pre and post synaptic firing at different rates.",
                    "label": 0
                },
                {
                    "sent": "You get completely different results, so if you do it rarely, only at a.",
                    "label": 0
                },
                {
                    "sent": "Every 10 seconds you get depression no matter what is the relationship between post and pre and post synaptic.",
                    "label": 0
                },
                {
                    "sent": "Frank if you do it quite often at 40 Hertz then you get no potentiation of weights no matter what the temp relationship is.",
                    "label": 0
                },
                {
                    "sent": "And there's a certain middle area only about 20 Hertz where you get the curve of the type node as it had shown you before there.",
                    "label": 0
                },
                {
                    "sent": "So when seasonal this is one of these many other parameters, know the frequency of this pairing, which also controls the synaptic plasticity.",
                    "label": 0
                },
                {
                    "sent": "And again tells us know how complex this phenomenon.",
                    "label": 0
                },
                {
                    "sent": "There's also something which just came out of hand enough from perspective of our theory, but it's it's actually it's a family of results.",
                    "label": 0
                },
                {
                    "sent": "It's less studied, namely that also the synapses are not the only plastic parts of a neuron.",
                    "label": 0
                },
                {
                    "sent": "Probably actually every component of neuron is plastic, but its excitability or its firing threshold in kind of simple model is also something which is.",
                    "label": 0
                },
                {
                    "sent": "Adepts there and the results are so called intrinsic excitability tell you when you simply exciting the room or the seven electrode injecting current that afterwards it fires more easily, so therefore it becomes kind of a feeling.",
                    "label": 0
                },
                {
                    "sent": "It's becomes more important and then has to say something all the time.",
                    "label": 0
                },
                {
                    "sent": "Then they also other processes that go with a different direction, but this is something which turns out in the context of our model.",
                    "label": 0
                },
                {
                    "sent": "If this existed and follow certain laws it could be useful for really inserting priors.",
                    "label": 0
                },
                {
                    "sent": "Into such cortical microcircuits, which I will propose could possibly learn from tissue plasticity to become models for the Corporation computations and for implementing based theory.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I should also mention that I find it intriguing is better than their researchers know.",
                    "label": 0
                },
                {
                    "sent": "Really experts for memory search and they think know for example burn that re its intrinsic excitability which primarily contains our memory traces there.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "I think there's also know how much how little we know about really encoding of memory traces.",
                    "label": 0
                },
                {
                    "sent": "If you have completely different views than one is not intrinsic excitability of neurons, the kind of common view is not at the plasticity of synapses is really responsible for this and it.",
                    "label": 1
                },
                {
                    "sent": "M as theology often is, it's maybe a combination of both effects there.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is now an hour cortical motive of aquatic microcircuit know which I will base my talk on and we have no more complex models also which have in many aspects more biologically realistic features.",
                    "label": 0
                },
                {
                    "sent": "But this is kind of the embryo in the most simple kind of toy version of what we propose could be a model.",
                    "label": 0
                },
                {
                    "sent": "If you inserted into cortical circuits, induced could do something which we can interpret very well from the perspective of probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "Information processing and so this is now here.",
                    "label": 0
                },
                {
                    "sent": "Kind of a stochastic winner.",
                    "label": 0
                },
                {
                    "sent": "Take all circuit.",
                    "label": 0
                },
                {
                    "sent": "I have a new instead wanted said K and I assume that they are have some lateral inhibition.",
                    "label": 0
                },
                {
                    "sent": "So therefore whenever by neuron fires it makes sure the other ones don't fire right at the same moment.",
                    "label": 0
                },
                {
                    "sent": "So maybe 10 millisecond later now they can have their vote then.",
                    "label": 0
                },
                {
                    "sent": "But for the moment they should sell should be silent and so therefore one can view this simply as a representation of a certain probability distribution of multi normal distribution over capital K many values.",
                    "label": 0
                },
                {
                    "sent": "And so kind of which neuron fires could be viewed now as a sampling from this probability distribution and the code for this probability distribution is in one hand now encoded in the excitability of this insurance, and which is.",
                    "label": 0
                },
                {
                    "sent": "This is the membrane potential which is normally in the mathematical model simply the bias here.",
                    "label": 1
                },
                {
                    "sent": "Basically in the other part of it snow probability distributions in code in the weights from these neurons by to this sentence there which I hear this.",
                    "label": 1
                },
                {
                    "sent": "In this is WKI then very similar to this indices.",
                    "label": 0
                },
                {
                    "sent": "Now how Conrad mentor uses in this is actually an here.",
                    "label": 0
                },
                {
                    "sent": "This fairing law is 1 which is no.",
                    "label": 0
                },
                {
                    "sent": "Of course no, it suggest.",
                    "label": 0
                },
                {
                    "sent": "Soft competition, but it happens to be actually something which no this exponential term of the membrane potential is something which comes up in many papers where they really try to fit new models to really recordings there.",
                    "label": 0
                },
                {
                    "sent": "So they usually come up with such exponential terms there.",
                    "label": 0
                },
                {
                    "sent": "So this is a model which is not not perceive fitted to data, but it's better fitted to data than most integrated for anyone model segment works with a theoretician, Ann Fortune.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "And it's also it's really probabilistic Norman, so therefore I think it looks like the right player, enough power building up our stage snow for probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "In such circuitry.",
                    "label": 0
                },
                {
                    "sent": "You also for this simplest version of this model detail which I want to discuss here.",
                    "label": 0
                },
                {
                    "sent": "I assume that the input actually uses exactly the same coding as this one, so I view this as you know, kind of a certain population of neurons in each of them fires.",
                    "label": 0
                },
                {
                    "sent": "No one of them fires, and so that they won't probably communicating.",
                    "label": 0
                },
                {
                    "sent": "What is the current value?",
                    "label": 0
                },
                {
                    "sent": "Office external multinomial variable there.",
                    "label": 0
                },
                {
                    "sent": "So therefore this is just a nice symmetry North that this input coding is exactly the same as output coding.",
                    "label": 0
                },
                {
                    "sent": "So these are both probability distribution Stan.",
                    "label": 0
                },
                {
                    "sent": "So therefore the since you this codings are so nice, you could now you have another network of the same one which simply takes the output as one of its input components there.",
                    "label": 0
                },
                {
                    "sent": "So this is model.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this is I think hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple and also this normalization here is happening which we assume some of this inhibition takes care of, which is, I think kind of a bad fashion from perspective of neuroscience because inhibition is has much more structure than many different types of inhibitory neurons with different connection profiles, different dynamic behavior and probably actually nature can squeeze out more functionality out of not just doing here straight normalization there, but this is something also weather data as Poss.",
                    "label": 0
                },
                {
                    "sent": "Enough so at the moment I just know kind of know, explain away inefficient over assuming it does.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This normalization then and simply before I discuss the theory, I simply want to demonstrate no one computer extra meant where we see no.",
                    "label": 0
                },
                {
                    "sent": "How could no health care insurance in principle emerged in the brain then and actually.",
                    "label": 1
                },
                {
                    "sent": "Also, I think the theory may provide a partial answer to this question, that one of you asked why did they ever managed to find this helper in the runs.",
                    "label": 0
                },
                {
                    "sent": "So therefore I think this theory would propose whatever you have a large enough population and you show help Paris and other things.",
                    "label": 0
                },
                {
                    "sent": "No one of them will probably become an expert for help very then.",
                    "label": 0
                },
                {
                    "sent": "If it's enough important enough for this system, so here now I didn't have images of Halle Berry Butthead here.",
                    "label": 0
                },
                {
                    "sent": "Patterns to dependence of this type and Nancy snow.",
                    "label": 1
                },
                {
                    "sent": "Here he is kind of limited with Mccluster here at the top here my the bottom here on the left side here on the right hand side and so these are the way how this inputs are for different inputs presented now through this population code included.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network and once you know this, further samples, no one sees you know these two look very different in the sense that, but they both have more cluster here at the top here at the bottom and so and we want to find out whether the network and on its own find out what is the hidden cause of this patterns and the hidden cause of this patterns.",
                    "label": 1
                },
                {
                    "sent": "Obviously this for different cluster centers, but from every single input notes very hard to infer about.",
                    "label": 0
                },
                {
                    "sent": "The cluster centers are because you the maximum probability.",
                    "label": 0
                },
                {
                    "sent": "I think this was .04.",
                    "label": 1
                },
                {
                    "sent": "Pics of the great drawn then and so therefore no, you have this large tried to try durability as one sees also in real neural data, so there's no one gets this know simply as a population vector Norlin era race then and also each pixel is encoded by population.",
                    "label": 0
                },
                {
                    "sent": "Coding is only binary value and if it has value one if it's black, not one of these two spike trends.",
                    "label": 0
                },
                {
                    "sent": "One of these factions is active, it fires at a certain hurts, maybe 40 Hertz for certain time.",
                    "label": 0
                },
                {
                    "sent": "If it's right the other one of this pair of neurons?",
                    "label": 0
                },
                {
                    "sent": "Which encode this pixel fires there.",
                    "label": 0
                },
                {
                    "sent": "So this is something which little bit makes note of analysis easy, because we know no matter how many pixels are black, the number of neurons which fire he is always the same then, because no matter whether the pixel is bigger, abide always one of this turn on fire.",
                    "label": 0
                },
                {
                    "sent": "So this is certainly here.",
                    "label": 1
                },
                {
                    "sent": "No model simplification and we have another models which don't require this.",
                    "label": 0
                },
                {
                    "sent": "But anyway let's live, it is for the moment and so this is now how these four different patterns actually are presented to this circuit then.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "800 spike trends and so this you know we have no color coded this for different sources.",
                    "label": 0
                },
                {
                    "sent": "The question is now when we now simply know send this bike trends for certain time.",
                    "label": 0
                },
                {
                    "sent": "To this winner.",
                    "label": 0
                },
                {
                    "sent": "Take all circuit that have shown you.",
                    "label": 0
                },
                {
                    "sent": "Could there be learning rules which enable these neurons to pick up?",
                    "label": 0
                },
                {
                    "sent": "What are the underlying patterns Nobel various help very northern.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a latent variable here, and so this is the results how it turns out.",
                    "label": 0
                },
                {
                    "sent": "Then after sending in such, no, simply no randomly drawn patterns, each one for 50 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So it's about every neuron that has a higher frame rate, fires one or maybe two spikes in such a period, or maybe none.",
                    "label": 0
                },
                {
                    "sent": "And these are this output neurons before learning, and so these are the output neurons after learning.",
                    "label": 0
                },
                {
                    "sent": "And so this colors as something we put on.",
                    "label": 0
                },
                {
                    "sent": "Because this is a new one which tends to fire all the time.",
                    "label": 0
                },
                {
                    "sent": "When you know this fourth kind of pattern type of pattern was on the hidden source of us, know that this was in the lower part.",
                    "label": 0
                },
                {
                    "sent": "This and so these are currently was the one bit I think the part of the left or so, and so these are now simply this weight vectors that emerged here and then see something interesting because we have this nice discussion about generative versus discriminative model.",
                    "label": 1
                },
                {
                    "sent": "So this network doesn't have any generative component.",
                    "label": 0
                },
                {
                    "sent": "There's nothing propagated backwards everetts.",
                    "label": 0
                },
                {
                    "sent": "Very simple now computer science designed to feed forward network.",
                    "label": 0
                },
                {
                    "sent": "But we still see it forms a generative model because these are really know, kind of.",
                    "label": 0
                },
                {
                    "sent": "It doesn't try to remember my hard one of these patterns.",
                    "label": 0
                },
                {
                    "sent": "It tries to capture what is the kind of the typical statistic nature of these inputs and it simply guess is that this is a Gaussian here, which is centered around here.",
                    "label": 0
                },
                {
                    "sent": "And this was exactly the hidden variable.",
                    "label": 1
                },
                {
                    "sent": "Know how we generated this pictures here and we saw here these are this way to vectors of this for nutrients in this case.",
                    "label": 0
                },
                {
                    "sent": "On this competitive layer, and each of them found their own kind of latent variable and built in their weights automatically.",
                    "label": 0
                },
                {
                    "sent": "An internal model for this latent variables.",
                    "label": 0
                },
                {
                    "sent": "To be only, of course, in these weights, no, they don't have a 2D structure, we simply projected them back into this 2D space, know where they get the spike, transform the, so this is hopefully clear that.",
                    "label": 0
                },
                {
                    "sent": "But this is not the way how the generative models that they have built for the sources of the spike patterns here right now.",
                    "label": 0
                },
                {
                    "sent": "This one from seeing or many patterns like this, it has built up a generative model which looks like this here, and this is here.",
                    "label": 0
                },
                {
                    "sent": "But this learning rule about excitability does it, simply it learns that there different priors because I forgot to mention before this patterns for showed.",
                    "label": 0
                },
                {
                    "sent": "The different probabilities here, so we have shown this this one four times as often as this pattern of these latent variable here and so, therefore automatically in this bias is operating normally in the.",
                    "label": 0
                },
                {
                    "sent": "Potential function applied to this WK0 from this weighted sum.",
                    "label": 0
                },
                {
                    "sent": "When CC that these values release.",
                    "label": 0
                },
                {
                    "sent": "Converge to what?",
                    "label": 0
                },
                {
                    "sent": "Are they really value probability values here?",
                    "label": 0
                },
                {
                    "sent": ".1 in here .4 then.",
                    "label": 1
                },
                {
                    "sent": "So therefore once easier that is very simple.",
                    "label": 0
                },
                {
                    "sent": "Network is in principle able from this stream of really marking or very noisy data to recover both generative models for each of these four input sources and also civil tendencies.",
                    "label": 0
                },
                {
                    "sent": "Their priors here.",
                    "label": 0
                },
                {
                    "sent": "And I want to explain now which learning rules are used for this and also what is the theory which allows us to understand know why this happens here.",
                    "label": 0
                },
                {
                    "sent": "But any questions so far to what's happening here?",
                    "label": 0
                },
                {
                    "sent": "So I guess when we argue about whether when it was this generative or not, but it's doing some kind of clustering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, fun, good quality clustering over months to you.",
                    "label": 0
                },
                {
                    "sent": "Also should say no.",
                    "label": 0
                },
                {
                    "sent": "I think we're not the first, nor proposing this simple kind of generative models without backwards propagation.",
                    "label": 0
                },
                {
                    "sent": "I think there was a paper by Ghahremani and hipness so basic over their head.",
                    "label": 0
                },
                {
                    "sent": "For mixture of Gaussians also proposed this, but it thinks something which I find not quite nice becausw I think not all this backwards propagation of feedback connection.",
                    "label": 0
                },
                {
                    "sent": "Ideas know that machine learning people like no in this context of generative models, I think they have a hard time to fitting them with any data there, and I think for other reasons.",
                    "label": 0
                },
                {
                    "sent": "Also simply the timing is not right.",
                    "label": 0
                },
                {
                    "sent": "So I'm very suspicious of all these explicit reactive models here than in men.",
                    "label": 1
                },
                {
                    "sent": "See also that the retail have models are really only at that part of the model.",
                    "label": 0
                },
                {
                    "sent": "That's essentially cause their guide.",
                    "label": 0
                },
                {
                    "sent": "The learning we will see in the end.",
                    "label": 0
                },
                {
                    "sent": "No, there really not the learning goal of this learning of the system is to fit their internal models to the world.",
                    "label": 0
                },
                {
                    "sent": "Stimulated, they get the answer for this.",
                    "label": 0
                },
                {
                    "sent": "No, they used relative model simply to maximize log likelihood implicitly of the inputs there, but it's really happening implicitly.",
                    "label": 0
                },
                {
                    "sent": "We will see here and doesn't have to be so blunt and really propagate things and then matching.",
                    "label": 0
                },
                {
                    "sent": "Does it fit to the input or not?",
                    "label": 0
                },
                {
                    "sent": "Then, which would be something I think neurons would not.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like so much.",
                    "label": 0
                },
                {
                    "sent": "So now let's try it now to go to the anymore questions to the what's happening here.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is how the first step.",
                    "label": 0
                },
                {
                    "sent": "You know how we can understand what's happening here, and I think the key discovery behind this is possibly that Yvonne sets up this learning rules in a particular way which doesn't depend so much on the shape of this, but really depends primarily on how much we, in larger weight.",
                    "label": 0
                },
                {
                    "sent": "If it's in the kind of know right window.",
                    "label": 0
                },
                {
                    "sent": "Versus how much we.",
                    "label": 0
                },
                {
                    "sent": "Downgrade evades know when this prior post synaptic.",
                    "label": 0
                },
                {
                    "sent": "Difference has a wrong value there, so this is namely that you can achieve here principle that this weights converge to a certain lack of a conditional probability, and I think this is for me really something.",
                    "label": 0
                },
                {
                    "sent": "If nature would we do this?",
                    "label": 0
                },
                {
                    "sent": "It's very nice fast and because it allows us suddenly to create a link between the Sue of synaptic plasticity and things we can understand on the level of probabilistic analysis there and so therefore the claim is here know that even uses this particular learning rule where.",
                    "label": 1
                },
                {
                    "sent": "If you put in, say, did you see it?",
                    "label": 0
                },
                {
                    "sent": "You have to eat to the minus the current value of the weight, and if you simply availed gets decreased it simply by a certain unit minus one.",
                    "label": 0
                },
                {
                    "sent": "It's decreased.",
                    "label": 0
                },
                {
                    "sent": "And of course in the background there were some learning rates.",
                    "label": 0
                },
                {
                    "sent": "Not like in every online learning.",
                    "label": 0
                },
                {
                    "sent": "There in one can prove they actually quite simply that this with regard to expected weight changes.",
                    "label": 0
                },
                {
                    "sent": "This lock probability is the only equilibrium of a wait no and so therefore.",
                    "label": 0
                },
                {
                    "sent": "This is in our starting point and so this is nice because now we can ask yourself, know what is this condition probability good for, or if it maybe if you want to did kind of induce a different types of probabilistic computation in the network, what should be the variation of the learning rule which let's them the weights converge to different log probability.",
                    "label": 0
                },
                {
                    "sent": "And as you probably have guessed, know in principle it's fast theoreticians.",
                    "label": 0
                },
                {
                    "sent": "It's much more pleasant to have things converge to locks of probabilities becausw know in the end now.",
                    "label": 0
                },
                {
                    "sent": "Yes, we're based here.",
                    "label": 0
                },
                {
                    "sent": "You have to multiply probabilities and neurons and lots of things said.",
                    "label": 0
                },
                {
                    "sent": "If you really know and yes, I'm theoreticians also claimed that do multiplication, but it's not something which comes out, not without some tweaking so, but but everybody agrees.",
                    "label": 0
                },
                {
                    "sent": "No simply, the membrane potential is a great tool for adding things then and so therefore obviously you can do multiplications easily in lock domain then and so this is probably one reason why this theory works nice with a sibling room model where you have this additive.",
                    "label": 0
                },
                {
                    "sent": "Membrane potential and working with lock probabilities.",
                    "label": 0
                },
                {
                    "sent": "Unpleasant technical aspect.",
                    "label": 0
                },
                {
                    "sent": "Which is he also know here, hidden in this last line there look probabilities tend to be negative, which is no not consistent with our assumption.",
                    "label": 0
                },
                {
                    "sent": "We want to hear talk about excitatory connections there.",
                    "label": 0
                },
                {
                    "sent": "So what we do in our computer experiments is simply we.",
                    "label": 0
                },
                {
                    "sent": "We restrict the range of this lock probabilities from minus 520.",
                    "label": 0
                },
                {
                    "sent": "So we simply cut it off at minus five, and then when we go through really network simulation, we shift everything by 5 upwards.",
                    "label": 0
                },
                {
                    "sent": "So therefore minus 5.",
                    "label": 0
                },
                {
                    "sent": "To 0 is shifted to zero to five, which is in our positive range of weights then and so we kind of virtually always from the perspective of probability theory we know we use this non negative range.",
                    "label": 1
                },
                {
                    "sent": "We want to interpret this as a kind of a circuit effect in terms of the room models we shifted upwards there.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 unpleasant aspect of this.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to get around it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But maybe you have some idea later.",
                    "label": 0
                },
                {
                    "sent": "So what I want to claim this, and actually that this simple network that had shown you this soft stochastic winner take all circuit when really can prove that in principle by this simple learning would have shown you automatically learns to carry out base theorem there.",
                    "label": 1
                },
                {
                    "sent": "So it learns both prior and this likelihood.",
                    "label": 1
                },
                {
                    "sent": "And so here you know this is not the K. Is the neuron set K the case output in the rent and the probability that it nor in the posterior distribution the case output.",
                    "label": 0
                },
                {
                    "sent": "Fires which could be interpreted.",
                    "label": 0
                },
                {
                    "sent": "Snow is no sampling from this posterior.",
                    "label": 0
                },
                {
                    "sent": "Says know that V as a network belief.",
                    "label": 0
                },
                {
                    "sent": "Now that the case hidden source is no really cost.",
                    "label": 0
                },
                {
                    "sent": "This current input, and so this is based on the current weights hitting within this network there and so this is no cause emerges from the product of prior and hear the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And since you see here we take the exponent of this memory potential automatically E to this.",
                    "label": 0
                },
                {
                    "sent": "WK0 is kind of corresponding to this prior, and this term is corresponding to this likelihood, as I will show you in the next slide then.",
                    "label": 0
                },
                {
                    "sent": "And so therefore.",
                    "label": 0
                },
                {
                    "sent": "This is something which, in principle, if something like this would exist in nature.",
                    "label": 0
                },
                {
                    "sent": "Of course I think would solve lots of problems, you just have a complex remain.",
                    "label": 0
                },
                {
                    "sent": "You have all this local agent, it's kinetically coded for inputs.",
                    "label": 0
                },
                {
                    "sent": "Come in there, and so a lot of variability there.",
                    "label": 0
                },
                {
                    "sent": "Not two areas in the brain which get exactly the same inputs there right?",
                    "label": 0
                },
                {
                    "sent": "So in principle all the symmetries is broken by simply by looking at the input and output of local circuitry and possibly but it's local circuit.",
                    "label": 0
                },
                {
                    "sent": "It could then do the same computation, and if it has useful computation it could be everywhere useful there.",
                    "label": 0
                },
                {
                    "sent": "And I think 1 interesting hypothesis, but speaking from perspective, this workshop is.",
                    "label": 0
                },
                {
                    "sent": "This could just be learn to carry out basic theory.",
                    "label": 0
                },
                {
                    "sent": "Every single work that every single model of this of this complex.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In network there.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit looking at what is here in principle about types of likelihood distributions are supported by the simple model that have shown here and so this mixtures of multinomial's there where this one can simply know if you write again let me start here, then simply fall.",
                    "label": 0
                },
                {
                    "sent": "So coming to formally to generative model.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the joint probability that the case neuron of these fires for the current spike input by.",
                    "label": 1
                },
                {
                    "sent": "Coming from these neurons here, this is no, by the way, how the models defined.",
                    "label": 0
                },
                {
                    "sent": "It's not given by this distribution.",
                    "label": 0
                },
                {
                    "sent": "See is just some normalization factor.",
                    "label": 0
                },
                {
                    "sent": "If you now want to marginalized out this case here K here all these different latent variables.",
                    "label": 0
                },
                {
                    "sent": "This is now there really.",
                    "label": 1
                },
                {
                    "sent": "The generative model of the input, which this whole network is building, only input up to here, then and now.",
                    "label": 1
                },
                {
                    "sent": "If you're further analyze this now, but it really says that if you now look at this happen concretely, this is the prior.",
                    "label": 0
                },
                {
                    "sent": "And this now can be viewed simply as a multi normal distribution over this original external variables.",
                    "label": 0
                },
                {
                    "sent": "There each of them is encoded in the population coding in the binary code, then button altogether in a mathematically just boils down to this multi normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is each no.",
                    "label": 1
                },
                {
                    "sent": "Actor and so this this is reporting an extra one of them in on simply this questions.",
                    "label": 0
                },
                {
                    "sent": "But is the current value of this external variable X Ray?",
                    "label": 0
                },
                {
                    "sent": "And if this is on then this factor into this product.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's not simply London, so simply this is just a complicated way of writing multi normal distribution and see if it sees human has also interpretation of what this weight from this new and I to this known K is.",
                    "label": 0
                },
                {
                    "sent": "Namely that E 2 WKI would be this probability.",
                    "label": 0
                },
                {
                    "sent": "Which this factor contributes to this product in this multi normal distribution there will show later.",
                    "label": 0
                },
                {
                    "sent": "This theory is not unfortunately not nailed down to mixtures of multi normals, but when can take mixture of anything which factorizes nor an exponentially family.",
                    "label": 0
                },
                {
                    "sent": "For example everything there can be used and probably mixtures of products of person distributions is something which is more adequate here for the setup of really working with spike trends and so it's just a little bit more complicated mathematics.",
                    "label": 0
                },
                {
                    "sent": "So therefore I wanted to stick.",
                    "label": 0
                },
                {
                    "sent": "To the simpler case then.",
                    "label": 0
                },
                {
                    "sent": "This is no, but this model is simply not kind of designed.",
                    "label": 0
                },
                {
                    "sent": "No in principle, but it represents every multinomial distribution came with no capital K factors case.",
                    "label": 0
                },
                {
                    "sent": "The number of neurons here can be represented by this, and what I would argue is that no matter what input you sent here into this circuit, it always tries its best job candidate to fit a multinomial mixture of multinomial's to the input distribution that it's getting there.",
                    "label": 0
                },
                {
                    "sent": "This is really.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The function in the summer surprising mathematical insight which underlies here, is that actually, if the actual distribution is something very far away from a mixture of multi normal, so it's a mixture of whatever Gaussian, so so it still tries to fit it as good as it can with a mixture of normals and as good as it can means here mathematically as good as any am can do it there.",
                    "label": 1
                },
                {
                    "sent": "And so I think this is for me was a surprising insight, in principle is very simple.",
                    "label": 0
                },
                {
                    "sent": "Local learning rules can be rigorously proven to approximate expectation maximization, which as you probably have heard about it, it's the most powerful tool really.",
                    "label": 0
                },
                {
                    "sent": "For unsupervised learning there right, and I think it's a new, I think hope or perspective that possibly simply neural networks in synaptic plasticity could carry something out of this type then, because it would provide much more understandable functionality than any more local operation.",
                    "label": 0
                },
                {
                    "sent": "You stage algorithm does this.",
                    "label": 0
                },
                {
                    "sent": "Do a sort of local EM where you're like these things where you actually do.",
                    "label": 0
                },
                {
                    "sent": "Interleave E&M sex much closer?",
                    "label": 0
                },
                {
                    "sent": "Or is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me go through this.",
                    "label": 0
                },
                {
                    "sent": "I have, you know this few lines.",
                    "label": 1
                },
                {
                    "sent": "Then and then we can come back to the family when approximate stochastic online EM then and then we can prove from principle know there's like drunken in the in the background and that this instead, which is a single application of SDP makes moves the parameter vector in the direction of this M step there.",
                    "label": 0
                },
                {
                    "sent": "So this is the best one can do so.",
                    "label": 0
                },
                {
                    "sent": "But let me go through this here just so because I think interesting.",
                    "label": 0
                },
                {
                    "sent": "I think nobody.",
                    "label": 0
                },
                {
                    "sent": "No, no group before had no really brought in synaptic plasticity in the context of of am then and so therefore because this SCM or send them for Spike base TM then.",
                    "label": 0
                },
                {
                    "sent": "So when were drunk?",
                    "label": 0
                },
                {
                    "sent": "Improve is maybe let's start with the easy step.",
                    "label": 1
                },
                {
                    "sent": "The step is simply when you get now a complex spike train, you have to figure out what is best, guess what was hidden source of this current spike trends that this circuit is getting there and so this is simply the eastep know in the formal way.",
                    "label": 0
                },
                {
                    "sent": "Eastep simply means you make the best case on the basis of the data that you have so far.",
                    "label": 1
                },
                {
                    "sent": "Then now the M step is know when now one of these neurons fires in this competitive layer.",
                    "label": 0
                },
                {
                    "sent": "In synaptic plasticity, recipes only applied to this number in there, which is the winner kind of competition in this one trial, there in this corresponds to the M step becausw know kind of understanding the Eve way that if a similar input comes again it makes this more likely to win again this competition there, so it moves a little bit.",
                    "label": 0
                },
                {
                    "sent": "This kind of this neuron snow in this competitive layer which all the group for what could be possible general hidden sources for explaining this data.",
                    "label": 0
                },
                {
                    "sent": "For kind of becoming more the expert for this type of input, or for this cluster Now, as Bennett has suggested, if I want to see this more visually, then and so this is something which one can also prove more formally.",
                    "label": 0
                },
                {
                    "sent": "But I don't want to go in this analysis, but I think in the end I think the vision I think is here for me.",
                    "label": 0
                },
                {
                    "sent": "I think the most hopeful aspect of this that we can because usually know if you have these things now.",
                    "label": 0
                },
                {
                    "sent": "Without this EM theory in the background.",
                    "label": 1
                },
                {
                    "sent": "You would just think not if these two steps are iterated know this thing just may underthrew wait space around there but am guarantee issue that you know you're making progress with regard to an optimization function, which I will make explicit on the next slide then and so therefore it's going to.",
                    "label": 0
                },
                {
                    "sent": "So, at least to a local optimum of this objective there, and I think this is very nice to have here.",
                    "label": 0
                },
                {
                    "sent": "A complex distributed network, but we can still understand possible its functionality is together trying to optimize a certain objective function there, which then of course theoretician makes it easier to understand the whole process, and I'm not sure whether you answered your question in the mean time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK yeah, so this actually know this two guys here better Nessler Michelle Pfeiffer, who came up with this.",
                    "label": 0
                },
                {
                    "sent": "So this was very compressed in NIPS paper and we're working on an understandable version of this snow in about a month.",
                    "label": 0
                },
                {
                    "sent": "It should be done.",
                    "label": 0
                },
                {
                    "sent": "You can send the email if you are interested, could send it to you and so one can either no value in this optimization function that simply the cool bar library versions between the extra distribution of spike inputs in this generative models.",
                    "label": 1
                },
                {
                    "sent": "Which has been built with the help of the current setting of, albeit vectors of all these neurons.",
                    "label": 0
                },
                {
                    "sent": "They try to minimize this and or equivalent.",
                    "label": 0
                },
                {
                    "sent": "You could say the log likelihood of inputs is going to be maximised.",
                    "label": 0
                },
                {
                    "sent": "Then by this whole process and so this is very nice because when really sees that it tries to do something which we can under Intrepid on the probabilistic level then.",
                    "label": 0
                },
                {
                    "sent": "Any questions to this?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these two guys, now we have an awful camera, but they are very talented PhD students and their Hebrides.",
                    "label": 0
                },
                {
                    "sent": "Workout series that in principle one can move away.",
                    "label": 0
                },
                {
                    "sent": "This two kind of most obvious kind of unpleasant aspects of the simple Model 1 is that one had no.",
                    "label": 0
                },
                {
                    "sent": "I looked here.",
                    "label": 0
                },
                {
                    "sent": "The mix of multi normals so Steven happen shows has shown that mixed.",
                    "label": 0
                },
                {
                    "sent": "Of anything from the polynomial, exponential family is just as good then this is, I think, but it's going to stay at this level.",
                    "label": 0
                },
                {
                    "sent": "Mixtures of something.",
                    "label": 0
                },
                {
                    "sent": "This is just the capability of this particular symbol model.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we would get into all kinds of trouble when he's with false machine learning and so on.",
                    "label": 0
                },
                {
                    "sent": "And last freezing has shown something very nice, because in real life.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never have really a network nor in neural system where you have here a bunch of input neurons and here bunch of output neurons and you have complete connectivity.",
                    "label": 0
                },
                {
                    "sent": "Rather, as I mentioned, this is a continuous sheets in between two layers.",
                    "label": 0
                },
                {
                    "sent": "You simply have these two sheets and you have many local connections from Muncie to the other one and you have lots of local letter inhibition but you don't have anything perfect.",
                    "label": 0
                },
                {
                    "sent": "No, they don't exist.",
                    "label": 0
                },
                {
                    "sent": "These models that exist here and so therefore last piercing has developed.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice extension of this here, which allows us to deal with this situation there, which is now becoming something where we can really applied now to any circuitry which you know kind of is simply models explicitly no neural circuitry, but becomes out his nose is mankind are longer characterize so easily, but it's subjective distribution.",
                    "label": 0
                },
                {
                    "sent": "Then this becomes more difficult in this set up there, but this is something I think it goes a little bit beyond this, but I think this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spill make this more realistic or hopefully also.",
                    "label": 0
                },
                {
                    "sent": "Verifiable or disputable from the perspective of the system, so then we became Bolton still softener.",
                    "label": 0
                },
                {
                    "sent": "Maybe let's do something.",
                    "label": 0
                },
                {
                    "sent": "Also, if you're understanding about learning poses a possible, let's do something cool like no Ben had would have done there working with real data set like this aimless data.",
                    "label": 0
                },
                {
                    "sent": "Set of handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "But we made our life difficult for the neurons because neurons don't have a supervisor in the brain, so they have to figure out themselves.",
                    "label": 0
                },
                {
                    "sent": "No, but the images are so now the database comes with annotations.",
                    "label": 0
                },
                {
                    "sent": "No kind of supervised.",
                    "label": 0
                },
                {
                    "sent": "We left them out.",
                    "label": 0
                },
                {
                    "sent": "We never told the neurons know what which digit was which one.",
                    "label": 0
                },
                {
                    "sent": "We simply showed it to them and we use the same kind of know.",
                    "label": 0
                },
                {
                    "sent": "Naive way of encode pixels by each pixel by two spike trains.",
                    "label": 0
                },
                {
                    "sent": "One of them is always on then and then we showed this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is not we had hundred neurons on this competitive layer competing each of them finding their kind of the Hill Berry in the world.",
                    "label": 0
                },
                {
                    "sent": "And you see that of course they have no idea.",
                    "label": 0
                },
                {
                    "sent": "Know that kind of this vertical line has the same meaning for us as a slanted line.",
                    "label": 0
                },
                {
                    "sent": "So therefore you have no experts know becoming experts for various new kind of degrees of slanted lines, then right?",
                    "label": 0
                },
                {
                    "sent": "And you have find here the same for for any other digit know their different ways of writing.",
                    "label": 0
                },
                {
                    "sent": "Three and you have different experts for coming over, but I think that's what's nice.",
                    "label": 0
                },
                {
                    "sent": "I think to see Snowman, see see that in principle, self organization process can take place then, so that all of these hundred neurons there find their way of a certain kind of proportion of the input, which then can kind of model for which they build a generative model.",
                    "label": 0
                },
                {
                    "sent": "And each time when something comes up which is similar to.",
                    "label": 0
                },
                {
                    "sent": "The thing OK, this is now my turn.",
                    "label": 0
                },
                {
                    "sent": "I think it's me then and you have some of them who couldn't kind find away.",
                    "label": 0
                },
                {
                    "sent": "So we have a certain variability.",
                    "label": 0
                },
                {
                    "sent": "Also you have noticed right to try their ability.",
                    "label": 0
                },
                {
                    "sent": "So this by the various will simply how we showed this with the network.",
                    "label": 0
                },
                {
                    "sent": "So it learns pretty fast for 300 seconds of political time and we showed them 30 millisecond is encoding of 1 handwritten digit that is 30 millisecond another one.",
                    "label": 0
                },
                {
                    "sent": "And so this is just flies by then.",
                    "label": 0
                },
                {
                    "sent": "And they have to find out of the zoo of Spike tread patterns, which are very, you know, have lots of our ability within this class.",
                    "label": 0
                },
                {
                    "sent": "What they want to focus on.",
                    "label": 0
                },
                {
                    "sent": "So this is quite nice, and I think because this tells us that in principle.",
                    "label": 0
                },
                {
                    "sent": "So now with network of sparkle neurons and the simple learning rules, we can do something which is not so completely.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not only stupid and toy like in principle from the perspective of a neuroscience perspective, when sees that this experiment demonstrates the possibility of explaining something which I find very interesting, which is simply called perceptual learning.",
                    "label": 0
                },
                {
                    "sent": "Then if you try to look at a certain types of no visual tasks, say you look at often to see whether 2 lines are parallel, not parallel after while doing this you become very.",
                    "label": 0
                },
                {
                    "sent": "You become better at this then.",
                    "label": 0
                },
                {
                    "sent": "Even if you don't have any supervisor which ever tells you whether your answers became better or not, then you simply become better at this thing.",
                    "label": 0
                },
                {
                    "sent": "At the same time.",
                    "label": 0
                },
                {
                    "sent": "Also, when animal records from the rooms one finds after perceptual learning, they have a sparser response then.",
                    "label": 0
                },
                {
                    "sent": "So it's not that activity goes up, but it goes down.",
                    "label": 0
                },
                {
                    "sent": "Then the other hand you have.",
                    "label": 0
                },
                {
                    "sent": "Let's try to tribalism reproducible kind of firing.",
                    "label": 0
                },
                {
                    "sent": "And you can interpret this as known, have become experts for this particular patents which occur as part of this task, that they have to solve them.",
                    "label": 0
                },
                {
                    "sent": "And so this is simply here obviously, but occurs here.",
                    "label": 0
                },
                {
                    "sent": "You have this hundred neurons, and here we showed them now five different digits and always know one of them fired actually, and even although no for each sample, the burner, on average, 10 of them.",
                    "label": 0
                },
                {
                    "sent": "Experts of this but a single one of these 10 sub experts even think this is my way of writing of London.",
                    "label": 0
                },
                {
                    "sent": "And so therefore you have in principle this pass firing.",
                    "label": 0
                },
                {
                    "sent": "So this of course no, no, not really neuroscience becausw.",
                    "label": 0
                },
                {
                    "sent": "Is not in a second brain which gets too.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visual input encoded this way, so therefore when we ask us have it be sync Now really about it?",
                    "label": 0
                },
                {
                    "sent": "Not so much from the machine learning perspective, but simply from the perspective of computational models for the brain where we want to think of large networks of transportation models, which each there's something useful.",
                    "label": 0
                },
                {
                    "sent": "In this context, we're sitting in the brain.",
                    "label": 0
                },
                {
                    "sent": "I think this is comes much closer to this, and so let me go through this year.",
                    "label": 0
                },
                {
                    "sent": "So this is no big head again, no large number of inputs 500 but only 100 are sold.",
                    "label": 0
                },
                {
                    "sent": "And this actually something which is unusual for this kind of experiments.",
                    "label": 0
                },
                {
                    "sent": "We always found that our results get better if you have a larger input dimension, which is insofar interesting as a real neuron gets about is about 3000 presynaptic pyramids helps there, so they always work in the range of thousands verse.",
                    "label": 0
                },
                {
                    "sent": "Almost all computer simulation results.",
                    "label": 0
                },
                {
                    "sent": "So far we know how to maybe 20 or 50 or so 100 presynaptic neuron.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be methods which really work much better if you have large number of presynaptic neurons.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is so these are just for some spike trends you're not supposed to see any pattern.",
                    "label": 0
                },
                {
                    "sent": "There are no patterns in principle, but what we did here we froze certain for some patterns here and repeated them.",
                    "label": 0
                },
                {
                    "sent": "So here there's red pattern is 1, which is repeated here again then.",
                    "label": 0
                },
                {
                    "sent": "Although part of this is also spontaneous noise, so this was all together with 20 Hertz for some spy kids only these patterns is red here.",
                    "label": 0
                },
                {
                    "sent": "Spikes, by the way.",
                    "label": 0
                },
                {
                    "sent": "So this is all this time, so you may not be able to read it.",
                    "label": 0
                },
                {
                    "sent": "This is now.",
                    "label": 0
                },
                {
                    "sent": "Here's one second from here to here, and so there's no numbers.",
                    "label": 0
                },
                {
                    "sent": "100 neurons are shown, and so 15 Hertz is a frozen person pattern here, which know the red dots.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "No 3/4 of this dots in here are red, and there should repeat here.",
                    "label": 0
                },
                {
                    "sent": "Also in this time window again when we show again this patterns, but 55.",
                    "label": 0
                },
                {
                    "sent": "So 1/4 of this spikes here black and so these are simply spontaneous 5 Hertz noise which is superimposed so therefore.",
                    "label": 0
                },
                {
                    "sent": "This patterns only agrees here only on 3/4 of its spikes there, and so therefore we had not created several of such patterns here.",
                    "label": 0
                },
                {
                    "sent": "Now as many as we have colors know which can be shown to be different on a on a graph, then on a figure then I think we have 5 here manage then and then we set up here no this network with no 6.",
                    "label": 0
                },
                {
                    "sent": "Lawrence, Ian and we showed this interesting movie.",
                    "label": 0
                },
                {
                    "sent": "Now to this network for 22nd there.",
                    "label": 0
                },
                {
                    "sent": "And but emerged there were there.",
                    "label": 0
                },
                {
                    "sent": "Now five of this Lawrence.",
                    "label": 0
                },
                {
                    "sent": "Each of them became an expert for one of this embedded patterns.",
                    "label": 0
                },
                {
                    "sent": "So here this one neuron, which is then also hear this.",
                    "label": 0
                },
                {
                    "sent": "But it spikes are marked with green.",
                    "label": 0
                },
                {
                    "sent": "It starts to fire shortly after this pattern becomes shown then, and fire still a little bit while afterwards, and it goes down.",
                    "label": 0
                },
                {
                    "sent": "It's hiding all the time all the time until the next time the screen pattern comes up here, so we can see here that really this five neurons, each of them they have detected their head buried in or in the world then, or particular pattern which occur to number of times in different contexts with different noise.",
                    "label": 0
                },
                {
                    "sent": "In different variations and they only fire in the future when this pattern comes up here, there were only five patterns embedded into this versus 6 neurons, so this six one is this year.",
                    "label": 0
                },
                {
                    "sent": "This new number for where the spikes are marked in black, and so this always fires when there is no pattern on them.",
                    "label": 0
                },
                {
                    "sent": "In the Super we would now in certain now start inserting 6 pattern into this bike trans.",
                    "label": 0
                },
                {
                    "sent": "This probably would become an expert of this.",
                    "label": 0
                },
                {
                    "sent": "This new one doesn't have season founder kind of reason of coding in the world.",
                    "label": 0
                },
                {
                    "sent": "It's still groping around and it's firing.",
                    "label": 0
                },
                {
                    "sent": "None of the other ones are sure that they have found their help very than in the input and.",
                    "label": 0
                },
                {
                    "sent": "So they suggested firing probability of these neurons.",
                    "label": 0
                },
                {
                    "sent": "Notice this E to the UK's then which are always discussed before and here I I no longer know, played around with the prior.",
                    "label": 0
                },
                {
                    "sent": "So so this was only for demonstration purposes in the first inductor experiments here they were about equally distributed there and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an interesting perspective, I think from the perspective.",
                    "label": 0
                },
                {
                    "sent": "Apologies also if we not only let this intervals between these patterns vary.",
                    "label": 1
                },
                {
                    "sent": "But if you also apply now time warping to this frozen for some pattern here, like here, this red is probably only know maybe 2030 millisecond lemon.",
                    "label": 0
                },
                {
                    "sent": "Here it's I don't know 80 or 100 millisecond long then and so principle.",
                    "label": 0
                },
                {
                    "sent": "Now this is something for many models for computing with spatial temporal patterns with spikes of it spiking romances.",
                    "label": 0
                },
                {
                    "sent": "Non trivial problem advance easier.",
                    "label": 0
                },
                {
                    "sent": "This network which was just trained on the regular patterns had never seen during training know this kind of time for pattern there just as reliable here this revenue and fires here for this long version is for the short version there in this actually also in summer little bit naive Fenton Simple explanation for this big cause.",
                    "label": 0
                },
                {
                    "sent": "This network is not really geared to create spatio temporal internal models is really only creates spatial internal models so therefore it tries to guess.",
                    "label": 0
                },
                {
                    "sent": "What is what is kind of an average no set of neurons which is typically active in the thread pattern is on then.",
                    "label": 0
                },
                {
                    "sent": "So really it extracts the SPF average spatial pattern out of this spatial temporal patterns there and I don't want to argue that this is happening.",
                    "label": 0
                },
                {
                    "sent": "All there isn't a brain about temporal processing, so you would have to use additional circuitry and we have also variations of this simple network.",
                    "label": 0
                },
                {
                    "sent": "Simply if you now you have interconnections on this output layer, so then it becomes an HMM, and so therefore it makes prediction.",
                    "label": 0
                },
                {
                    "sent": "If the first is new and said one has fired, then I am.",
                    "label": 0
                },
                {
                    "sent": "Neurons had three now and I have a hard, larger chance of being active again.",
                    "label": 0
                },
                {
                    "sent": "Then someone can expand this model also into something which really does know kind of more sophisticated processing on temporal level then yeah.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "I think the problem becomes and really what is the interconnections among this set ignorance there?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so here it was about this.",
                    "label": 0
                },
                {
                    "sent": "So if it has interconnections among these neurons, excitatory connections there, that kind of 1 gets into trouble with now theory.",
                    "label": 0
                },
                {
                    "sent": "Now from the probabilistic perspective, which doesn't get into the way of this generative weights, and so this is, I think the theoretical problem there.",
                    "label": 0
                },
                {
                    "sent": "Altogether.",
                    "label": 0
                }
            ]
        }
    }
}