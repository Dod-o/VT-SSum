{
    "id": "43dwu5o7z5whceq774vll6cs5skbaikn",
    "title": "Bayesian models of cross-situational word learning",
    "info": {
        "author": [
            "Michael Frank, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlcs07_frank_bmc/",
    "segmentation": [
        [
            "OK, so I'm going to be talking about Bayesian models of cross situational word learning and this is joint work with no Goodman and Josh Tenenbaum at MIT.",
            "I."
        ],
        [
            "So this talk is a little bit different than some of the other talks that we've been hearing here, in particular because this work is focused on connecting the words that an infant learner here is to the world outside.",
            "And this is just the first step at one of the numerous problems that a learner faces and trying to connect language with the world.",
            "But hopefully it gives some insight into some of these larger problems.",
            "So here's an example of word learning in action.",
            "Paper."
        ],
        [
            "So the way we can define the problem of word learning here, then, is that we've got a whole different set of situations.",
            "So situation as I'm defining it is an utterance combined with the objects that are present in the world at the time of the utterance.",
            "So for instance, in the first situation here, we've got the words blue rings, but two objects at least.",
            "I'm going to simplify and say two objects are present rings and also big bird.",
            "So in anyone situation, the problem here is that children may hear many words and see many objects.",
            "They have to figure out which one Maps to the to the other at."
        ],
        [
            "Plus, this seems like a pretty easy problem.",
            "You know, maybe you should just plot which ones go with the other, which words go with which objects at which times and it should work out pretty well.",
            "You could call this a cross situational strategy for learning word, word, object mappings and this kind of strategy.",
            "This proposal has a really great pedigree in cognitive science.",
            "People like Steve Pinker and Lila Gleitman have proposed it and it sounded like a really great idea.",
            "Didn't seem too difficult.",
            "The trouble is when you actually try to work it out.",
            "If you just use a statistic like Co occurrence frequency, it doesn't work very well.",
            "You end up with the word you mapping to the object ring just on the basis of frequ."
        ],
        [
            "See.",
            "So you actually end up with a little bit of a computational problem.",
            "Going back to this issue of cross situational word learning, you may actually have to apply.",
            "More sophisticated computational machinery to get this lexical learning task off the ground, there have been a number of attempts to do this in the literature.",
            "One early, influential one was made by Jeff Siskind, who used a deductive inference model to try to learn not just word object meanings mappings, but also full Lambda calculus expressions from cross situational observation.",
            "Trouble with this model is that it's hard to find a corpus or a real world situation that's annotated with.",
            "Lambda calculus expressions.",
            "So although there are a lot of really interesting features of the model, it may not be.",
            "Maybe the right place to start.",
            "More recent work has been by Chenyuan colleagues and this is I guess the starting point for our work.",
            "They used a machine translation model to estimate Association probabilities between words and objects and they got some fairly good success on relatively basic."
        ],
        [
            "The same corpus that we used so the structure of my presentation today is going to be to talk about a few facts of word learning.",
            "Empirical facts from the language acquisition literature that I think are desiderata for any model of word learning to cover.",
            "Then I'm going to talk a little bit about our model, which is a beige and word learner and give an extension of that model to the task of learning social cues.",
            "This is an interesting task because it's A kind of chicken and egg problem.",
            "Like other problems in language acquisition, there are certain social cues that are useful for determining the speakers reference, but it's hard to figure out what their which ones are useful unless you know ahead of time some of the words that the speaker is using to refer.",
            "Then finally, with these two models laid out, I'm going to talk a little bit.",
            "About how each of them covers the experimental result."
        ],
        [
            "That I started with.",
            "So the three phenomena that I'm going to talk about here are mutual exclusivity, fast mapping, and the use of social cues.",
            "So mutual exclusivity is a much discussed phenomenon in Word learning.",
            "Basically, the phenomenon is simple.",
            "You've got a young child interacting with the caregiver on the table between them.",
            "There's a known object, a ball, and an unknown object, let's call it attacks.",
            "The caregiver says something.",
            "Or in this case, may be an experiment or even says something like.",
            "Give me the Dax and the result that's found over and over again is that by 18 to 24 months children will map a novel word onto a novel object, and there have been a whole bunch of different.",
            "Proposals for how this happens.",
            "Constraints on the lexicons say a principle of mutual exclusivity that says each object has only one word that Maps to it, or more pragmatic principles that might account for this sort of phenomenon.",
            "But this is the basic empirical phenomenon.",
            "Second thing that I want to put up as a desideratum for a model of word learning is that it should be able to accomplish the task of fast mapping.",
            "So this refers to the unambiguous learning situation where the child and the caregiver both jointly attending to a particular novel object.",
            "The caregiver says something like this one is a Koba, and the result here is that three and four year olds can learn words from just this one learning situation, so they don't need a whole bunch of repeated situations in which.",
            "In which the object is named, and this was a very important result in Word learning, essentially because it seemed to rule out a whole lot of reinforcement based accounts of word learning.",
            "If all you needed was one situation to learn, maybe you didn't need kind of repeated beating over the head with the meaning of a word to get it right.",
            "On so finally another extremely relevant result from word learning is the use of social cues.",
            "So by 18 months children are able to distinguish between ambiguous reference of a sentence using just social cues.",
            "In other words, here we have the experimenter or caregiver attending using eye gaze to one of two novel objects, and saying, look at the Modi and the child will in a later test correctly recognized that the blue blue object here is a Modi had not the green one below.",
            "That's a tax."
        ],
        [
            "So I let me show you guys.",
            "Now our model.",
            "And I'll show the model corpus some baseline comparison models and."
        ],
        [
            "Results on the corpus.",
            "Our model is set up in the generative modeling framework that we've seen here already.",
            "The basic idea is that for each situation, each learning situation, we have a set of objects and a set of words, and these are observed by the model, so these are unordered sets.",
            "For simplicity, we're not going to build in any kind of linear ordering and syntax, although that is conceivably something that we could do in this kind of model.",
            "It's something we actually would like to do.",
            "Then there are two unobserved variables, which are model is going to have to.",
            "Infer eventually the first one is UN observed changes for each situation, and that's the thing that the speaker thing or things that the speaker intends to refer to in their utterance.",
            "So that's I hear, and it mediates between the objects that are observed and the words that are said the second it doesn't change is constant across situations, and that's the lexecon that the speaker has.",
            "The lexecon is in this case simply a set of word object pairings that indicate that word should be altered when the object is intended to.",
            "Refer to."
        ],
        [
            "You intend to refer to that object, so let me walk you through an example of how you might generate a typical situation in this model.",
            "So the first thing you do is you observe some set of objects, let's say a bag balhannah bike.",
            "Next you choose some subset of that intention.",
            "It could be an empty subset.",
            "You could be talking about something totally different, but choose some subset that you intend to refer to.",
            "In this case, it will be the ball we next set up our sentence.",
            "Our words by assuming some set of slots for words to be uttered.",
            "At this point we filled the slots for words with two different types of words, referring words and non referring words.",
            "So referring words are generated directly from the intention by intend to talk about the ball.",
            "I will talk about the ball and I do that by looking up ball in the lexecon and uttering one of the words that's associated with it.",
            "So in this case I've only got one pairing between the ball object Anna Word and the word is ball.",
            "So I uttered that word in the case that I would have two words.",
            "Maybe I could call that a ball or globe.",
            "I would pick uniformly between ball and globe in my attempt to describe what I intend to refer to.",
            "Then I populate the rest of my words with non referring words which are in this very simple abstractions.",
            "They're just junk, they're just any other words that I've heard that could be function words.",
            "They could even be verbs, but we're just going to pick those uniformly over all the words we've heard in the world.",
            "This allows us to do something a little bit tricky, which is to build in a very minor form class distinction here we've got referring words which are sort of down like.",
            "And non referring words which are pretty much everything else and we can say hey, you know if you use a word to refer if you have it in your lexecon, it's a little bit less likely that you'll use it as a non referring word later on.",
            "So it's a little bit less likely that you'll say ball when there's no ball around then then that you'll say look."
        ],
        [
            "For instance.",
            "So once we've set up this generative model were then in a position to invert it.",
            "In other words, to score the probability of some lexecon given a set of words and objects.",
            "A set of situations, and we can do that by evaluating the probability of the words given the lexecon objects times prior probability on lexicons, and we define the simplest prior that we can define, which is I. Parsimony prior on lexicons.",
            "It's just an exponential over the size of the lexecon.",
            "Once we've set up the scoring function, we can then do Bayesian inference.",
            "We can search around in the space of lexicons to try to find the lexicon which best fits our corpus, and we do this using stochastic search with simulated tempering.",
            "I'm happy to answer questions about the inference method afterwards, but essentially an any type of search would do so long as it got us a reasonable solution.",
            "Within this model, we're interested in really is the setup of the model, not the inference method."
        ],
        [
            "So the corpus we used is the same one.",
            "I pulled the video clip from at the beginning, it's 210 minute clips from the child.",
            "These corpus from the Rollins section.",
            "This is the same corpus that was used by Chenyuan, his colleagues and the corpus describes an interaction between mom and infant.",
            "Pairs of moms and infants, the infants around six months old.",
            "Excuse me and there are about 2500 words total in the corpus, so it's quite small there.",
            "24 objects which are being played with all of them are toys that are pulled from a chest.",
            "This nicely sidesteps one of the major problems that young learners have to figure out when they're assigning words to objects in the world.",
            "That is, is this dog the same as that dog?",
            "Or they both dogs and they pull take the same label?",
            "In this case we have a whole bunch of unique toys so we don't have to solve the problem of referential uncertainty.",
            "We're just going to assume that each toy is a salient object, which can take a label.",
            "So we ran our model on this corpus and then for."
        ],
        [
            "Comparison we also ran a number of other models we use the Co occurrence frequency model, simple common currency frequency statistic that I described at the beginning.",
            "We also looked at pointwise mutual information and a number of other simple conditional probability statistics that are similar to these, so I won't talk about them.",
            "We also implemented a translation model in the style of you in Ballard's translation model.",
            "This is a simple empty model based on IBM Model 1 basically.",
            "Estimates the Association probability of objects given words using the EM algorithm.",
            "So for each of these comparison models, what you end up with is a matrix of Association probabilities, frequencies, mutual informations, or associations from the M model.",
            "Then you can essentially twiddle a threshold nob to control the permissiveness of the lexecon.",
            "Do you take everything, or do you cut off the lexecon into certain?",
            "Frequency or Association threshold in order to include fewer items and our model likewise has this kind of.",
            "Adjustment parameter that we can control which is our prior parameter.",
            "How much the model is penalized for adding extra pairings to the."
        ],
        [
            "Lexicons So what we did to assess our results is to manipulate these parameters across their full range for each model.",
            "So this is a little bit of a non standard graph, so I'll take a second to explain it.",
            "Basically what we're plotting is for each value along that parameter.",
            "Recall by the precision of the lexicons that was achieved, so these look sort of like RC curves, but they're not exactly standard RFC curves, because at each point we're evaluating the entire lexecon.",
            "So if we up the threshold, we may incorrectly remove a good word object pairing from Mexicana and precision and recall may actually go down as you see in some places on the.",
            "For instance on the green line.",
            "So what you notice here is that the two simple statistical models Co occurrence frequency and mutual information.",
            "Which are both in blue on the left.",
            "Don't do so well.",
            "They both get relatively low F scores and basically this is because they are extremely permissive.",
            "You can get every word object pairing that was correct in your lexicon into your lexecon, but only at the cost of including nearly everything that was associated at all.",
            "On in contrast, both the translation model and the basian model do quite a bit better.",
            "So what we see for both of those is a fairly reasonable tradeoff between precision and recall here, although for a fairly large regime of values, the Bayesian model does outperform the translation model.",
            "One thing to note here is that the scores are to some extent a little bit arbitrary.",
            "This is basically dependent on what kind of gold standard you choose for your lexecon.",
            "A number of different standards you could choose.",
            "For instance, you could say the gold standard lexicon is the one learn learned by adult learners who are viewing this corpus.",
            "So even words that are never used with a particular toy could still be correct in principle.",
            "There none of these models could learn one, so we went.",
            "We backed off from that slightly using a gold standard that was.",
            "All of the correct word object pairings, including plurals, but not including those that were never associated in the corpus.",
            "So that's why RF scores are relatively low overall, it's because it's basically.",
            "There are a lot of pairings in here that might be extremely tough for any model to learn.",
            "We could sort of arbitrarily move those scores, but the basic take home point here is that.",
            "There is a.",
            "There's a sort of a substantial increase in performance.",
            "Especially in precision for from the Bayesian model.",
            "So just to."
        ],
        [
            "You guys a little more of an intuitive feel for how our model did, I'm showing on the left the best lexecon we found by search.",
            "Basically we're getting about 11 out of 14 correct.",
            "Another nice feature of this kind of model is that we can also plot out the most likely intention of the speaker in each sentence.",
            "So in a sentence like look at the Moo cow in the presence of a cow girl anibare, we can say that the speaker was probably trying to intend intending to refer to the cow.",
            "Also, unlike the baseline models, one really nice feature of our model is how extensible it is.",
            "Because we've written down an arbitrary generative model, we can then."
        ],
        [
            "Add an extension to it, and if it doesn't mess up the entire inference scheme, we should be able to add other cues.",
            "So in the remainder of my talk I'm going to say something about 11 possible extension that is to the task of learning social cues."
        ],
        [
            "So we went back through our corpus and coded the social cues for each utterance we quoted where the infants hands were what objects the infants hands were on, what object their eyes were on their mouth, and what object they were touching, but maybe not with their hands.",
            "Then we did the same for moms, hands, eyes, and touch.",
            "Mom never put anything in her mouth, so that wasn't a prob."
        ],
        [
            "And so that gave us a feature value matrix with essentially a zero or one for whether the infant is looking or mom was looking at each object, and we assumed in our model that each of these cues could be caused either by a certain base rate of of that social cue, or they could be generated by the fact that.",
            "Object was in the intention to refer was was part of the subset of objects that was being referred to."
        ],
        [
            "And So what that gave us was a an extension here where we also observed social cues and then we had a set of unobserved variables, relevance and base rate, which are distributions over the different social features.",
            "So we can find out now which features were relevant to determining reference and which queues just had high base rates but were not informative as to the intention of the speaker."
        ],
        [
            "And here just some preliminary results from that model.",
            "Basically, we haven't beaten our corpus performance from the previous model using the social model, where we get, you know, approximately equivalent performance.",
            "On the other hand, the model does go through and find the appropriate features from the corpus.",
            "In particular, it finds that the infants the object of the infants, gays, and the caregivers hand are two extremely frequent and salient cues for referential intent in this corpus.",
            "Whereas things like the infant's hand or the infants mouth end up having a fairly high base rate but extremely low relevance, and so one nice feature of this model is that it allows us to infer the caregivers intention even when they don't say a noun.",
            "So the mom says see that, and we can then infer that if the infant is looking and the caregivers hands on a cow toy, it's the cow toy that she's talking about.",
            "On so this is kind of a nice way of maybe bootstrapping into pronouns or more interesting kinds of words then just."
        ],
        [
            "Straight nouns.",
            "So to return to where I started, I mentioned a couple of different experimental results that would be nice to cover in a model."
        ],
        [
            "Word learning.",
            "The first one was mutual exclusivity.",
            "The tendency of children to map a novel name to a novel reference, and so we simulated that in this model by training the model on it or giving the model corpus of artificial data and presenting it with a situation in which there was a novel word used in the presence of a novel object and also a familiar object when it was part of the corpus.",
            "We then proposed four possible lexicons, shown here by these four boxes, which have links indicating the new words word.",
            "I mean we're in object mappings that were learned in that case, and we scored each one both on the current situation and the old corpus.",
            "So what we find is that in the case that we learn nothing, we have no way of explaining this current situation.",
            "We don't know why the word Dax was ordered.",
            "It must be a non referential word that we've never heard before, and so our situation scores quite low probability.",
            "On the other hand, we score quite well on the old corpus because our lexecon remains parsimonious.",
            "We score as well as we did before seeing this situation.",
            "In contrast, if we move on to learning both links, we now have an extremely good explanation for why we heard the word Dax.",
            "It's because both of these objects actually mean Dax, and so it's clear that whatever the caregiver was trying to intend to, and we don't know what that was, she probably would have said Dax so, so that's a very good explanation.",
            "The problem is now over our corpus.",
            "Things get a lot worse, so every time we saw a ball in the past.",
            "We now could have heard the word Dax instead of hearing the word ball.",
            "It just so happened and there's a big coincidence that we heard ball every time we never heard Dax before right now.",
            "So that's the way our generative process, the likelihood of the model actually penalizes this extra mapping that wasn't observed.",
            "So then we only have to decide between mapping Dax to ball and Dax Dax, and we see the same penalty over the corpus in the case that we map Dax to ball.",
            "Just minus a little bit for the prior so our model shows a kind of soft mutual exclusivity in its preference for the correct mapping here.",
            "So it shows the correct experimental phenomenon without the addition of any kind of external external principle or external constraint on the structure of the lexecon.",
            "This is nice because it seems intuitively implausible and seemed implausible too many researchers to build in constraints on the lexecon, especially when they seem counter productive in the long run.",
            "So for instance.",
            "If you learn that ball is called ball, it shouldn't be impossible to learn that it's called an object too, or the dog is an animal as well, so the learning of superordinates and subordinate category labels always seemed like a puzzle for accounts and mutual exclusivity, but should be perfectly easy to explain under this type of soft mutual."
        ],
        [
            "Activity.",
            "Just to go quickly through the example of fast mapping.",
            "We see here the situation in which there are three non referential words used.",
            "This is and then a new novel where that's never been heard before Koba, in the presence of a novel object to Koba, and again, what we see is that the over both the current situation and the corpus the model prefers to map the novel object to the novel word.",
            "Even just from this single instance, and the reason for that is this kind of light syntax that we've essentially built in.",
            "We can't map cobata any of these other words.",
            "Because they've been used non referentially throughout the rest of the corpus, and so if we suddenly assume there in our lexecon suddenly assumed that this means Koba exclusively, then every other time that we heard this in our life, we have to wonder why didn't we see akoba there?",
            "So what's happening here is that our model is actually making use of some of these kinds of explaining away principles that discussed earlier.",
            "We're seeing under evaluation of the hypothesis in light of the full corpus."
        ],
        [
            "So finally I mentioned the use of social cues from the work of Kathy Hirsh Pasek and Roberta Golinkoff, among others.",
            "So the phenomenon here is that in the presence of a novel word and two novel objects, the young learners as young as around 18 months are able to figure out that proper link to make is indicated by the direction of, for instance, of the caregivers, eyes, and so the model will show this as well, basically because of the salient feature being.",
            "Salient social feature being on the correct object.",
            "The model infers that the correct you know the correct object.",
            "Modi is in the intention to refer and thus learns that that mapping preferentially as opposed to the others.",
            "So the mapping of the model also seems is able to learn word meanings based on social cues alone and not just based on this kind of pure brute force cross situational observation."
        ],
        [
            "So to conclude what I've shown here is a Bayesian model of cross situational word learning.",
            "This is hard computational problem and our model performed best over a small segment of corpus data.",
            "We hope to extend it to a larger corpus, but this is one that's been discussed in the literature, so it allowed for comparison.",
            "In addition, one feature of this model that's different is that it allows us to parse sentence is an interpret the speaker's intent, so it allows us to figure out what the speaker is talking about.",
            "And that may pave the way for figuring out some more complex parts of the language that we're learning.",
            "Then introduced a social model which can learn which social cues are relevant to reference, sort of.",
            "Breaking out of this chicken and egg problem by learning jointly both the relevance of particular social cues and the lexecon.",
            "Finally, I showed an artificial data coverage of certain experiments that have been influential in Word learning, including mutual exclusivity, fast mapping.",
            "I and learning words by use of social cues."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to be talking about Bayesian models of cross situational word learning and this is joint work with no Goodman and Josh Tenenbaum at MIT.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk is a little bit different than some of the other talks that we've been hearing here, in particular because this work is focused on connecting the words that an infant learner here is to the world outside.",
                    "label": 0
                },
                {
                    "sent": "And this is just the first step at one of the numerous problems that a learner faces and trying to connect language with the world.",
                    "label": 0
                },
                {
                    "sent": "But hopefully it gives some insight into some of these larger problems.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of word learning in action.",
                    "label": 0
                },
                {
                    "sent": "Paper.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we can define the problem of word learning here, then, is that we've got a whole different set of situations.",
                    "label": 1
                },
                {
                    "sent": "So situation as I'm defining it is an utterance combined with the objects that are present in the world at the time of the utterance.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the first situation here, we've got the words blue rings, but two objects at least.",
                    "label": 1
                },
                {
                    "sent": "I'm going to simplify and say two objects are present rings and also big bird.",
                    "label": 0
                },
                {
                    "sent": "So in anyone situation, the problem here is that children may hear many words and see many objects.",
                    "label": 0
                },
                {
                    "sent": "They have to figure out which one Maps to the to the other at.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus, this seems like a pretty easy problem.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe you should just plot which ones go with the other, which words go with which objects at which times and it should work out pretty well.",
                    "label": 0
                },
                {
                    "sent": "You could call this a cross situational strategy for learning word, word, object mappings and this kind of strategy.",
                    "label": 0
                },
                {
                    "sent": "This proposal has a really great pedigree in cognitive science.",
                    "label": 0
                },
                {
                    "sent": "People like Steve Pinker and Lila Gleitman have proposed it and it sounded like a really great idea.",
                    "label": 0
                },
                {
                    "sent": "Didn't seem too difficult.",
                    "label": 0
                },
                {
                    "sent": "The trouble is when you actually try to work it out.",
                    "label": 0
                },
                {
                    "sent": "If you just use a statistic like Co occurrence frequency, it doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "You end up with the word you mapping to the object ring just on the basis of frequ.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "So you actually end up with a little bit of a computational problem.",
                    "label": 0
                },
                {
                    "sent": "Going back to this issue of cross situational word learning, you may actually have to apply.",
                    "label": 1
                },
                {
                    "sent": "More sophisticated computational machinery to get this lexical learning task off the ground, there have been a number of attempts to do this in the literature.",
                    "label": 0
                },
                {
                    "sent": "One early, influential one was made by Jeff Siskind, who used a deductive inference model to try to learn not just word object meanings mappings, but also full Lambda calculus expressions from cross situational observation.",
                    "label": 0
                },
                {
                    "sent": "Trouble with this model is that it's hard to find a corpus or a real world situation that's annotated with.",
                    "label": 0
                },
                {
                    "sent": "Lambda calculus expressions.",
                    "label": 0
                },
                {
                    "sent": "So although there are a lot of really interesting features of the model, it may not be.",
                    "label": 0
                },
                {
                    "sent": "Maybe the right place to start.",
                    "label": 0
                },
                {
                    "sent": "More recent work has been by Chenyuan colleagues and this is I guess the starting point for our work.",
                    "label": 0
                },
                {
                    "sent": "They used a machine translation model to estimate Association probabilities between words and objects and they got some fairly good success on relatively basic.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same corpus that we used so the structure of my presentation today is going to be to talk about a few facts of word learning.",
                    "label": 1
                },
                {
                    "sent": "Empirical facts from the language acquisition literature that I think are desiderata for any model of word learning to cover.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to talk a little bit about our model, which is a beige and word learner and give an extension of that model to the task of learning social cues.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting task because it's A kind of chicken and egg problem.",
                    "label": 0
                },
                {
                    "sent": "Like other problems in language acquisition, there are certain social cues that are useful for determining the speakers reference, but it's hard to figure out what their which ones are useful unless you know ahead of time some of the words that the speaker is using to refer.",
                    "label": 0
                },
                {
                    "sent": "Then finally, with these two models laid out, I'm going to talk a little bit.",
                    "label": 0
                },
                {
                    "sent": "About how each of them covers the experimental result.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That I started with.",
                    "label": 0
                },
                {
                    "sent": "So the three phenomena that I'm going to talk about here are mutual exclusivity, fast mapping, and the use of social cues.",
                    "label": 1
                },
                {
                    "sent": "So mutual exclusivity is a much discussed phenomenon in Word learning.",
                    "label": 0
                },
                {
                    "sent": "Basically, the phenomenon is simple.",
                    "label": 0
                },
                {
                    "sent": "You've got a young child interacting with the caregiver on the table between them.",
                    "label": 0
                },
                {
                    "sent": "There's a known object, a ball, and an unknown object, let's call it attacks.",
                    "label": 0
                },
                {
                    "sent": "The caregiver says something.",
                    "label": 0
                },
                {
                    "sent": "Or in this case, may be an experiment or even says something like.",
                    "label": 0
                },
                {
                    "sent": "Give me the Dax and the result that's found over and over again is that by 18 to 24 months children will map a novel word onto a novel object, and there have been a whole bunch of different.",
                    "label": 1
                },
                {
                    "sent": "Proposals for how this happens.",
                    "label": 0
                },
                {
                    "sent": "Constraints on the lexicons say a principle of mutual exclusivity that says each object has only one word that Maps to it, or more pragmatic principles that might account for this sort of phenomenon.",
                    "label": 0
                },
                {
                    "sent": "But this is the basic empirical phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Second thing that I want to put up as a desideratum for a model of word learning is that it should be able to accomplish the task of fast mapping.",
                    "label": 0
                },
                {
                    "sent": "So this refers to the unambiguous learning situation where the child and the caregiver both jointly attending to a particular novel object.",
                    "label": 1
                },
                {
                    "sent": "The caregiver says something like this one is a Koba, and the result here is that three and four year olds can learn words from just this one learning situation, so they don't need a whole bunch of repeated situations in which.",
                    "label": 0
                },
                {
                    "sent": "In which the object is named, and this was a very important result in Word learning, essentially because it seemed to rule out a whole lot of reinforcement based accounts of word learning.",
                    "label": 1
                },
                {
                    "sent": "If all you needed was one situation to learn, maybe you didn't need kind of repeated beating over the head with the meaning of a word to get it right.",
                    "label": 0
                },
                {
                    "sent": "On so finally another extremely relevant result from word learning is the use of social cues.",
                    "label": 0
                },
                {
                    "sent": "So by 18 months children are able to distinguish between ambiguous reference of a sentence using just social cues.",
                    "label": 0
                },
                {
                    "sent": "In other words, here we have the experimenter or caregiver attending using eye gaze to one of two novel objects, and saying, look at the Modi and the child will in a later test correctly recognized that the blue blue object here is a Modi had not the green one below.",
                    "label": 0
                },
                {
                    "sent": "That's a tax.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I let me show you guys.",
                    "label": 0
                },
                {
                    "sent": "Now our model.",
                    "label": 0
                },
                {
                    "sent": "And I'll show the model corpus some baseline comparison models and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results on the corpus.",
                    "label": 0
                },
                {
                    "sent": "Our model is set up in the generative modeling framework that we've seen here already.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is that for each situation, each learning situation, we have a set of objects and a set of words, and these are observed by the model, so these are unordered sets.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, we're not going to build in any kind of linear ordering and syntax, although that is conceivably something that we could do in this kind of model.",
                    "label": 0
                },
                {
                    "sent": "It's something we actually would like to do.",
                    "label": 0
                },
                {
                    "sent": "Then there are two unobserved variables, which are model is going to have to.",
                    "label": 0
                },
                {
                    "sent": "Infer eventually the first one is UN observed changes for each situation, and that's the thing that the speaker thing or things that the speaker intends to refer to in their utterance.",
                    "label": 0
                },
                {
                    "sent": "So that's I hear, and it mediates between the objects that are observed and the words that are said the second it doesn't change is constant across situations, and that's the lexecon that the speaker has.",
                    "label": 0
                },
                {
                    "sent": "The lexecon is in this case simply a set of word object pairings that indicate that word should be altered when the object is intended to.",
                    "label": 0
                },
                {
                    "sent": "Refer to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You intend to refer to that object, so let me walk you through an example of how you might generate a typical situation in this model.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you do is you observe some set of objects, let's say a bag balhannah bike.",
                    "label": 0
                },
                {
                    "sent": "Next you choose some subset of that intention.",
                    "label": 0
                },
                {
                    "sent": "It could be an empty subset.",
                    "label": 0
                },
                {
                    "sent": "You could be talking about something totally different, but choose some subset that you intend to refer to.",
                    "label": 0
                },
                {
                    "sent": "In this case, it will be the ball we next set up our sentence.",
                    "label": 0
                },
                {
                    "sent": "Our words by assuming some set of slots for words to be uttered.",
                    "label": 0
                },
                {
                    "sent": "At this point we filled the slots for words with two different types of words, referring words and non referring words.",
                    "label": 0
                },
                {
                    "sent": "So referring words are generated directly from the intention by intend to talk about the ball.",
                    "label": 0
                },
                {
                    "sent": "I will talk about the ball and I do that by looking up ball in the lexecon and uttering one of the words that's associated with it.",
                    "label": 0
                },
                {
                    "sent": "So in this case I've only got one pairing between the ball object Anna Word and the word is ball.",
                    "label": 0
                },
                {
                    "sent": "So I uttered that word in the case that I would have two words.",
                    "label": 0
                },
                {
                    "sent": "Maybe I could call that a ball or globe.",
                    "label": 0
                },
                {
                    "sent": "I would pick uniformly between ball and globe in my attempt to describe what I intend to refer to.",
                    "label": 0
                },
                {
                    "sent": "Then I populate the rest of my words with non referring words which are in this very simple abstractions.",
                    "label": 0
                },
                {
                    "sent": "They're just junk, they're just any other words that I've heard that could be function words.",
                    "label": 0
                },
                {
                    "sent": "They could even be verbs, but we're just going to pick those uniformly over all the words we've heard in the world.",
                    "label": 0
                },
                {
                    "sent": "This allows us to do something a little bit tricky, which is to build in a very minor form class distinction here we've got referring words which are sort of down like.",
                    "label": 0
                },
                {
                    "sent": "And non referring words which are pretty much everything else and we can say hey, you know if you use a word to refer if you have it in your lexecon, it's a little bit less likely that you'll use it as a non referring word later on.",
                    "label": 0
                },
                {
                    "sent": "So it's a little bit less likely that you'll say ball when there's no ball around then then that you'll say look.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "So once we've set up this generative model were then in a position to invert it.",
                    "label": 0
                },
                {
                    "sent": "In other words, to score the probability of some lexecon given a set of words and objects.",
                    "label": 0
                },
                {
                    "sent": "A set of situations, and we can do that by evaluating the probability of the words given the lexecon objects times prior probability on lexicons, and we define the simplest prior that we can define, which is I. Parsimony prior on lexicons.",
                    "label": 0
                },
                {
                    "sent": "It's just an exponential over the size of the lexecon.",
                    "label": 0
                },
                {
                    "sent": "Once we've set up the scoring function, we can then do Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "We can search around in the space of lexicons to try to find the lexicon which best fits our corpus, and we do this using stochastic search with simulated tempering.",
                    "label": 1
                },
                {
                    "sent": "I'm happy to answer questions about the inference method afterwards, but essentially an any type of search would do so long as it got us a reasonable solution.",
                    "label": 0
                },
                {
                    "sent": "Within this model, we're interested in really is the setup of the model, not the inference method.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the corpus we used is the same one.",
                    "label": 0
                },
                {
                    "sent": "I pulled the video clip from at the beginning, it's 210 minute clips from the child.",
                    "label": 0
                },
                {
                    "sent": "These corpus from the Rollins section.",
                    "label": 0
                },
                {
                    "sent": "This is the same corpus that was used by Chenyuan, his colleagues and the corpus describes an interaction between mom and infant.",
                    "label": 1
                },
                {
                    "sent": "Pairs of moms and infants, the infants around six months old.",
                    "label": 0
                },
                {
                    "sent": "Excuse me and there are about 2500 words total in the corpus, so it's quite small there.",
                    "label": 0
                },
                {
                    "sent": "24 objects which are being played with all of them are toys that are pulled from a chest.",
                    "label": 0
                },
                {
                    "sent": "This nicely sidesteps one of the major problems that young learners have to figure out when they're assigning words to objects in the world.",
                    "label": 0
                },
                {
                    "sent": "That is, is this dog the same as that dog?",
                    "label": 0
                },
                {
                    "sent": "Or they both dogs and they pull take the same label?",
                    "label": 0
                },
                {
                    "sent": "In this case we have a whole bunch of unique toys so we don't have to solve the problem of referential uncertainty.",
                    "label": 0
                },
                {
                    "sent": "We're just going to assume that each toy is a salient object, which can take a label.",
                    "label": 0
                },
                {
                    "sent": "So we ran our model on this corpus and then for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparison we also ran a number of other models we use the Co occurrence frequency model, simple common currency frequency statistic that I described at the beginning.",
                    "label": 0
                },
                {
                    "sent": "We also looked at pointwise mutual information and a number of other simple conditional probability statistics that are similar to these, so I won't talk about them.",
                    "label": 0
                },
                {
                    "sent": "We also implemented a translation model in the style of you in Ballard's translation model.",
                    "label": 0
                },
                {
                    "sent": "This is a simple empty model based on IBM Model 1 basically.",
                    "label": 1
                },
                {
                    "sent": "Estimates the Association probability of objects given words using the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for each of these comparison models, what you end up with is a matrix of Association probabilities, frequencies, mutual informations, or associations from the M model.",
                    "label": 0
                },
                {
                    "sent": "Then you can essentially twiddle a threshold nob to control the permissiveness of the lexecon.",
                    "label": 0
                },
                {
                    "sent": "Do you take everything, or do you cut off the lexecon into certain?",
                    "label": 0
                },
                {
                    "sent": "Frequency or Association threshold in order to include fewer items and our model likewise has this kind of.",
                    "label": 0
                },
                {
                    "sent": "Adjustment parameter that we can control which is our prior parameter.",
                    "label": 0
                },
                {
                    "sent": "How much the model is penalized for adding extra pairings to the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lexicons So what we did to assess our results is to manipulate these parameters across their full range for each model.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit of a non standard graph, so I'll take a second to explain it.",
                    "label": 0
                },
                {
                    "sent": "Basically what we're plotting is for each value along that parameter.",
                    "label": 0
                },
                {
                    "sent": "Recall by the precision of the lexicons that was achieved, so these look sort of like RC curves, but they're not exactly standard RFC curves, because at each point we're evaluating the entire lexecon.",
                    "label": 0
                },
                {
                    "sent": "So if we up the threshold, we may incorrectly remove a good word object pairing from Mexicana and precision and recall may actually go down as you see in some places on the.",
                    "label": 0
                },
                {
                    "sent": "For instance on the green line.",
                    "label": 0
                },
                {
                    "sent": "So what you notice here is that the two simple statistical models Co occurrence frequency and mutual information.",
                    "label": 0
                },
                {
                    "sent": "Which are both in blue on the left.",
                    "label": 0
                },
                {
                    "sent": "Don't do so well.",
                    "label": 0
                },
                {
                    "sent": "They both get relatively low F scores and basically this is because they are extremely permissive.",
                    "label": 0
                },
                {
                    "sent": "You can get every word object pairing that was correct in your lexicon into your lexecon, but only at the cost of including nearly everything that was associated at all.",
                    "label": 0
                },
                {
                    "sent": "On in contrast, both the translation model and the basian model do quite a bit better.",
                    "label": 0
                },
                {
                    "sent": "So what we see for both of those is a fairly reasonable tradeoff between precision and recall here, although for a fairly large regime of values, the Bayesian model does outperform the translation model.",
                    "label": 0
                },
                {
                    "sent": "One thing to note here is that the scores are to some extent a little bit arbitrary.",
                    "label": 0
                },
                {
                    "sent": "This is basically dependent on what kind of gold standard you choose for your lexecon.",
                    "label": 0
                },
                {
                    "sent": "A number of different standards you could choose.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could say the gold standard lexicon is the one learn learned by adult learners who are viewing this corpus.",
                    "label": 0
                },
                {
                    "sent": "So even words that are never used with a particular toy could still be correct in principle.",
                    "label": 0
                },
                {
                    "sent": "There none of these models could learn one, so we went.",
                    "label": 0
                },
                {
                    "sent": "We backed off from that slightly using a gold standard that was.",
                    "label": 0
                },
                {
                    "sent": "All of the correct word object pairings, including plurals, but not including those that were never associated in the corpus.",
                    "label": 0
                },
                {
                    "sent": "So that's why RF scores are relatively low overall, it's because it's basically.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of pairings in here that might be extremely tough for any model to learn.",
                    "label": 0
                },
                {
                    "sent": "We could sort of arbitrarily move those scores, but the basic take home point here is that.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                },
                {
                    "sent": "There's a sort of a substantial increase in performance.",
                    "label": 0
                },
                {
                    "sent": "Especially in precision for from the Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "So just to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You guys a little more of an intuitive feel for how our model did, I'm showing on the left the best lexecon we found by search.",
                    "label": 0
                },
                {
                    "sent": "Basically we're getting about 11 out of 14 correct.",
                    "label": 0
                },
                {
                    "sent": "Another nice feature of this kind of model is that we can also plot out the most likely intention of the speaker in each sentence.",
                    "label": 0
                },
                {
                    "sent": "So in a sentence like look at the Moo cow in the presence of a cow girl anibare, we can say that the speaker was probably trying to intend intending to refer to the cow.",
                    "label": 0
                },
                {
                    "sent": "Also, unlike the baseline models, one really nice feature of our model is how extensible it is.",
                    "label": 1
                },
                {
                    "sent": "Because we've written down an arbitrary generative model, we can then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add an extension to it, and if it doesn't mess up the entire inference scheme, we should be able to add other cues.",
                    "label": 0
                },
                {
                    "sent": "So in the remainder of my talk I'm going to say something about 11 possible extension that is to the task of learning social cues.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we went back through our corpus and coded the social cues for each utterance we quoted where the infants hands were what objects the infants hands were on, what object their eyes were on their mouth, and what object they were touching, but maybe not with their hands.",
                    "label": 0
                },
                {
                    "sent": "Then we did the same for moms, hands, eyes, and touch.",
                    "label": 1
                },
                {
                    "sent": "Mom never put anything in her mouth, so that wasn't a prob.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that gave us a feature value matrix with essentially a zero or one for whether the infant is looking or mom was looking at each object, and we assumed in our model that each of these cues could be caused either by a certain base rate of of that social cue, or they could be generated by the fact that.",
                    "label": 0
                },
                {
                    "sent": "Object was in the intention to refer was was part of the subset of objects that was being referred to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what that gave us was a an extension here where we also observed social cues and then we had a set of unobserved variables, relevance and base rate, which are distributions over the different social features.",
                    "label": 0
                },
                {
                    "sent": "So we can find out now which features were relevant to determining reference and which queues just had high base rates but were not informative as to the intention of the speaker.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here just some preliminary results from that model.",
                    "label": 0
                },
                {
                    "sent": "Basically, we haven't beaten our corpus performance from the previous model using the social model, where we get, you know, approximately equivalent performance.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the model does go through and find the appropriate features from the corpus.",
                    "label": 1
                },
                {
                    "sent": "In particular, it finds that the infants the object of the infants, gays, and the caregivers hand are two extremely frequent and salient cues for referential intent in this corpus.",
                    "label": 0
                },
                {
                    "sent": "Whereas things like the infant's hand or the infants mouth end up having a fairly high base rate but extremely low relevance, and so one nice feature of this model is that it allows us to infer the caregivers intention even when they don't say a noun.",
                    "label": 0
                },
                {
                    "sent": "So the mom says see that, and we can then infer that if the infant is looking and the caregivers hands on a cow toy, it's the cow toy that she's talking about.",
                    "label": 0
                },
                {
                    "sent": "On so this is kind of a nice way of maybe bootstrapping into pronouns or more interesting kinds of words then just.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight nouns.",
                    "label": 0
                },
                {
                    "sent": "So to return to where I started, I mentioned a couple of different experimental results that would be nice to cover in a model.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Word learning.",
                    "label": 0
                },
                {
                    "sent": "The first one was mutual exclusivity.",
                    "label": 1
                },
                {
                    "sent": "The tendency of children to map a novel name to a novel reference, and so we simulated that in this model by training the model on it or giving the model corpus of artificial data and presenting it with a situation in which there was a novel word used in the presence of a novel object and also a familiar object when it was part of the corpus.",
                    "label": 0
                },
                {
                    "sent": "We then proposed four possible lexicons, shown here by these four boxes, which have links indicating the new words word.",
                    "label": 0
                },
                {
                    "sent": "I mean we're in object mappings that were learned in that case, and we scored each one both on the current situation and the old corpus.",
                    "label": 0
                },
                {
                    "sent": "So what we find is that in the case that we learn nothing, we have no way of explaining this current situation.",
                    "label": 0
                },
                {
                    "sent": "We don't know why the word Dax was ordered.",
                    "label": 0
                },
                {
                    "sent": "It must be a non referential word that we've never heard before, and so our situation scores quite low probability.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we score quite well on the old corpus because our lexecon remains parsimonious.",
                    "label": 0
                },
                {
                    "sent": "We score as well as we did before seeing this situation.",
                    "label": 0
                },
                {
                    "sent": "In contrast, if we move on to learning both links, we now have an extremely good explanation for why we heard the word Dax.",
                    "label": 0
                },
                {
                    "sent": "It's because both of these objects actually mean Dax, and so it's clear that whatever the caregiver was trying to intend to, and we don't know what that was, she probably would have said Dax so, so that's a very good explanation.",
                    "label": 0
                },
                {
                    "sent": "The problem is now over our corpus.",
                    "label": 0
                },
                {
                    "sent": "Things get a lot worse, so every time we saw a ball in the past.",
                    "label": 0
                },
                {
                    "sent": "We now could have heard the word Dax instead of hearing the word ball.",
                    "label": 0
                },
                {
                    "sent": "It just so happened and there's a big coincidence that we heard ball every time we never heard Dax before right now.",
                    "label": 0
                },
                {
                    "sent": "So that's the way our generative process, the likelihood of the model actually penalizes this extra mapping that wasn't observed.",
                    "label": 0
                },
                {
                    "sent": "So then we only have to decide between mapping Dax to ball and Dax Dax, and we see the same penalty over the corpus in the case that we map Dax to ball.",
                    "label": 0
                },
                {
                    "sent": "Just minus a little bit for the prior so our model shows a kind of soft mutual exclusivity in its preference for the correct mapping here.",
                    "label": 0
                },
                {
                    "sent": "So it shows the correct experimental phenomenon without the addition of any kind of external external principle or external constraint on the structure of the lexecon.",
                    "label": 0
                },
                {
                    "sent": "This is nice because it seems intuitively implausible and seemed implausible too many researchers to build in constraints on the lexecon, especially when they seem counter productive in the long run.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "If you learn that ball is called ball, it shouldn't be impossible to learn that it's called an object too, or the dog is an animal as well, so the learning of superordinates and subordinate category labels always seemed like a puzzle for accounts and mutual exclusivity, but should be perfectly easy to explain under this type of soft mutual.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Activity.",
                    "label": 0
                },
                {
                    "sent": "Just to go quickly through the example of fast mapping.",
                    "label": 0
                },
                {
                    "sent": "We see here the situation in which there are three non referential words used.",
                    "label": 0
                },
                {
                    "sent": "This is and then a new novel where that's never been heard before Koba, in the presence of a novel object to Koba, and again, what we see is that the over both the current situation and the corpus the model prefers to map the novel object to the novel word.",
                    "label": 0
                },
                {
                    "sent": "Even just from this single instance, and the reason for that is this kind of light syntax that we've essentially built in.",
                    "label": 1
                },
                {
                    "sent": "We can't map cobata any of these other words.",
                    "label": 0
                },
                {
                    "sent": "Because they've been used non referentially throughout the rest of the corpus, and so if we suddenly assume there in our lexecon suddenly assumed that this means Koba exclusively, then every other time that we heard this in our life, we have to wonder why didn't we see akoba there?",
                    "label": 0
                },
                {
                    "sent": "So what's happening here is that our model is actually making use of some of these kinds of explaining away principles that discussed earlier.",
                    "label": 0
                },
                {
                    "sent": "We're seeing under evaluation of the hypothesis in light of the full corpus.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally I mentioned the use of social cues from the work of Kathy Hirsh Pasek and Roberta Golinkoff, among others.",
                    "label": 0
                },
                {
                    "sent": "So the phenomenon here is that in the presence of a novel word and two novel objects, the young learners as young as around 18 months are able to figure out that proper link to make is indicated by the direction of, for instance, of the caregivers, eyes, and so the model will show this as well, basically because of the salient feature being.",
                    "label": 0
                },
                {
                    "sent": "Salient social feature being on the correct object.",
                    "label": 0
                },
                {
                    "sent": "The model infers that the correct you know the correct object.",
                    "label": 0
                },
                {
                    "sent": "Modi is in the intention to refer and thus learns that that mapping preferentially as opposed to the others.",
                    "label": 0
                },
                {
                    "sent": "So the mapping of the model also seems is able to learn word meanings based on social cues alone and not just based on this kind of pure brute force cross situational observation.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude what I've shown here is a Bayesian model of cross situational word learning.",
                    "label": 1
                },
                {
                    "sent": "This is hard computational problem and our model performed best over a small segment of corpus data.",
                    "label": 0
                },
                {
                    "sent": "We hope to extend it to a larger corpus, but this is one that's been discussed in the literature, so it allowed for comparison.",
                    "label": 0
                },
                {
                    "sent": "In addition, one feature of this model that's different is that it allows us to parse sentence is an interpret the speaker's intent, so it allows us to figure out what the speaker is talking about.",
                    "label": 0
                },
                {
                    "sent": "And that may pave the way for figuring out some more complex parts of the language that we're learning.",
                    "label": 0
                },
                {
                    "sent": "Then introduced a social model which can learn which social cues are relevant to reference, sort of.",
                    "label": 1
                },
                {
                    "sent": "Breaking out of this chicken and egg problem by learning jointly both the relevance of particular social cues and the lexecon.",
                    "label": 0
                },
                {
                    "sent": "Finally, I showed an artificial data coverage of certain experiments that have been influential in Word learning, including mutual exclusivity, fast mapping.",
                    "label": 1
                },
                {
                    "sent": "I and learning words by use of social cues.",
                    "label": 0
                }
            ]
        }
    }
}