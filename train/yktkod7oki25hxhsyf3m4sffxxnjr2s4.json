{
    "id": "yktkod7oki25hxhsyf3m4sffxxnjr2s4",
    "title": "Self-taught Clustering",
    "info": {
        "author": [
            "Wenyuan Dai, Shanghai Jiao Tong University"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_dai_stc/",
    "segmentation": [
        [
            "Good afternoon.",
            "I'm going to give a talk about self taught clustering, which is an instance of transfer unsupervised learning.",
            "I'm when you and I this is a joint work with Chung Young, Graham Charan, Young you."
        ],
        [
            "Here is the outline of this talk.",
            "I will first give the introduction to our motivation and then define our problem self talk clustering.",
            "Then I will describe our algorithm in detail.",
            "The first part is the experiments, and finally the conclusion now."
        ],
        [
            "Now let's move to the."
        ],
        [
            "Ovation park.",
            "This is an example for clustering.",
            "This example can.",
            "These in this example, this data can be easily partitioned into three clusters.",
            "Like that?",
            "But the problem is clustering usually relies on large amount of data.",
            "When the data are sparse, it will meet Prob."
        ],
        [
            "For example, let's see when the data are sparse.",
            "We randomly remove some data.",
            "So the data becomes sparse and we cannot cluster them well."
        ],
        [
            "But can sparse data be cluster?",
            "Well, the answer is sometimes it's possible.",
            "For example.",
            "These data can be clustered well.",
            "Like that?",
            "The reason is these data are in a good representation.",
            "That is, the data in different clusters are separated very well, so a good data representation can make the clustering much easier."
        ],
        [
            "Now let's see another example.",
            "These data are sparse and we can.",
            "We are not sure how to cluster these data.",
            "For example, we can cluster in this way.",
            "Or in this way this way.",
            "But we don't know which one is the best.",
            "But if we can transform these data into another feature space to make the data in different clusters separate, well.",
            "The clustering can be much easier.",
            "For example, we can cluster these data in this way or this way.",
            "This way all these ways are OK. And now the problem is how to transform these data to make the data in a good."
        ],
        [
            "Mentation in this work we use transfer learning to help build a good data representation.",
            "For example.",
            "We had target data this I mean the square point in this example.",
            "And we have auxiliary data and large amount of auxiliary data.",
            "These auxiliary data can be easily clustered.",
            "Up that we transform."
        ],
        [
            "These data the auxiliary data separate very well and as a result the target data are separated.",
            "So we can cluster this target data well.",
            "That's the motivation of our."
        ],
        [
            "Work.",
            "Now let's define our problems.",
            "Self taught clustering.",
            "Which which is an instance of transfer unsupervised."
        ],
        [
            "Learning.",
            "We have a small collection of target data.",
            "These data to be clustered.",
            "And we also have a large amount of auxiliary data.",
            "These data are assumed to be irrelevant to the target data in public.",
            "Our object is too.",
            "Help the clustering performance on target data by making use the auxiliary unlabeled data.",
            "We call our problem self talk clustering or transfer unsupervised learning."
        ],
        [
            "We borrowed the concept of self taught learning.",
            "It's a ICM Mail paper last year by Rainer it Italian.",
            "In Self taught learning it make use of large amount of irrelevant unlabeled data to help the target class classification.",
            "For example, in this example the task is to.",
            "Classified elephants and rhinos, and we have a Hunter.",
            "Unlabeled data about sheep or mountain and water.",
            "These data are irrelevant to the target data.",
            "In the self learning paper it shows that these auxiliary data can be used to build high quality basis to help the target class."
        ],
        [
            "Vacation, what we want to do is to.",
            "Is a class clustering version of self taught learning?",
            "How object is to cluster target data and make making use?",
            "The large amount of irrelevant, unlabeled data.",
            "So we call our problem self."
        ],
        [
            "Or clustering.",
            "We can also borrow the concept of transfer learning here.",
            "Only difference between transfer learning and self taught learning.",
            "Here is the auxiliary data are labeled here.",
            "The data in blue frames are labeled data and the other unlabeled data."
        ],
        [
            "What we want to do is.",
            "Unsupervised version transfer learning.",
            "We want to cluster the target data or we can say it's unsupervised learning problem and the auxiliary data are unlabeled.",
            "So we can.",
            "We can only perform unsupervised learning on the auxiliary data."
        ],
        [
            "Now let's describe our argument.",
            "Recall our problem.",
            "We have a small collection of target data.",
            "These data are to be clustered.",
            "We also have a large amount of auxiliary data.",
            "The data is assumed to be irrelevant to the target data.",
            "Here is the scheme, our algorithm.",
            "We have how data auxiliary data they are irrelevant in target, but they share the same feature space.",
            "For example, these data may be or in text or maybe or in image.",
            "Cluster the target data and auxiliary data simultaneous.",
            "And.",
            "Meanwhile we cluster the features based on both target and auxiliary data.",
            "As you can see, there are two cold clusterings here in our algorithm.",
            "And these two Co clustering share a same feature clustering here.",
            "So the feature clustering here can be considered as a new data representation, which is consistent.",
            "With both.",
            "Target and auxiliary.",
            "So the auxiliary data here is used to help build a good feature representation for the target data cluster clustering."
        ],
        [
            "Here is the objective function of our algorithm.",
            "We have X&Y, which is the target and auxiliary data.",
            "We also have Z represent the.",
            "Feature for the proposed target.",
            "An auxiliary data.",
            "The two X~ Y2Z represent the clusterings on XY and Z.",
            "The object function of our algorithm is here.",
            "We can see in this objective function we have two information theoretic code clusterings.",
            "The information theoretical clustering minimizes information loss by Co clustering."
        ],
        [
            "Here is the optimization.",
            "We first convert the information loss to the KL divergences.",
            "Here the represent the KL divergent.",
            "Yeah, we.",
            "Reformulate the KL divergences.",
            "Regarding joint distribution to KL divergences with regard to.",
            "Conditional distributions.",
            "I.",
            "From this formula we can see what we are we need to do is to choose the best clusters.",
            "For each instance or features, to minimize the corresponding care divergent."
        ],
        [
            "So the class clustering function is here.",
            "We need to iteratively optimize these.",
            "Clustering function."
        ],
        [
            "Now let's move."
        ],
        [
            "Two experiments, our experiment.",
            "Focus on image cluster clustering tasks.",
            "We choose hundred categories from the Caltech 256 image image corpus for each clustering task we choose the data from the corresponding category.",
            "As the target unlabeled data.",
            "While the.",
            "The data from the remaining categories are used as the auxiliary unlabeled data.",
            "Our."
        ],
        [
            "Evaluation criteria is entropy.",
            "This."
        ],
        [
            "Experimental results we compare our algorithm with three baseline method.",
            "Platter is 1 dimensional clustering clustering tool.",
            "Feature clustering means we first build a new data representation using feature clustering and then apply 1 dimensional cluster clustering on the.",
            "Data in new feature representation.",
            "Co clustering is the information theoretic Co clustering and STC is our algorithm for each baseline method, we have two versions.",
            "Separate means the algorithm is only applied on the target data and combine mean.",
            "The algorithm is.",
            "Performed on the data combined by combined with both target and auxiliary data.",
            "From the table we can see our algorithm always outperform the baseline method.",
            "These two figures show the parameter setting our experiment.",
            "From this figure we can see we set the number number of feature clustering too.",
            "64 and from this speaker we said the trader parameters for our in our.",
            "Self taught clustering algorithm 21 and this figure showed the.",
            "Convergence property of our algorithm.",
            "From this figure we can see our algorithm converges very fast and very well."
        ],
        [
            "Now let's conclude.",
            "Hope."
        ],
        [
            "Paper.",
            "In this paper we investigate the self taught clustering.",
            "Problem which is an instance of.",
            "Transfer unsupervised learning.",
            "We propose to use.",
            "Irrelevant, unlabeled data.",
            "To help the target clustering.",
            "We develope Co clustering based self taught clustering algorithm in our algorithm to Co clustering our performance simultaneously between target data and features and between the the auxiliary data and the features.",
            "The two Co clustering share.",
            "Feature clustering so the knowledge can be transferred.",
            "By feature clustering, the experimental results show the Encourageing performance of our algorithm."
        ],
        [
            "Thank you very much.",
            "Questions.",
            "Irrelevant.",
            "The relevant data in our.",
            "Our problem is only in topic.",
            "For example, they are in different categories.",
            "Also, you know that they are not in the two categories that you consider for that, yes.",
            "But if if the data are in the.",
            "Elephant or rhino category.",
            "I think it will be better.",
            "And then have you compared your approach with the information bottleneck approach?",
            "Or is this great discussion?",
            "We understand information because that you must find yes I compared."
        ],
        [
            "I compared our algorithm with the Co clustering is the information theoretic code clustering.",
            "Which is derived based on the information bottleneck.",
            "And maybe similar but not the same."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give a talk about self taught clustering, which is an instance of transfer unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "I'm when you and I this is a joint work with Chung Young, Graham Charan, Young you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the outline of this talk.",
                    "label": 0
                },
                {
                    "sent": "I will first give the introduction to our motivation and then define our problem self talk clustering.",
                    "label": 0
                },
                {
                    "sent": "Then I will describe our algorithm in detail.",
                    "label": 0
                },
                {
                    "sent": "The first part is the experiments, and finally the conclusion now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's move to the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ovation park.",
                    "label": 0
                },
                {
                    "sent": "This is an example for clustering.",
                    "label": 0
                },
                {
                    "sent": "This example can.",
                    "label": 0
                },
                {
                    "sent": "These in this example, this data can be easily partitioned into three clusters.",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "But the problem is clustering usually relies on large amount of data.",
                    "label": 1
                },
                {
                    "sent": "When the data are sparse, it will meet Prob.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, let's see when the data are sparse.",
                    "label": 1
                },
                {
                    "sent": "We randomly remove some data.",
                    "label": 0
                },
                {
                    "sent": "So the data becomes sparse and we cannot cluster them well.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But can sparse data be cluster?",
                    "label": 1
                },
                {
                    "sent": "Well, the answer is sometimes it's possible.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 1
                },
                {
                    "sent": "These data can be clustered well.",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "The reason is these data are in a good representation.",
                    "label": 0
                },
                {
                    "sent": "That is, the data in different clusters are separated very well, so a good data representation can make the clustering much easier.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see another example.",
                    "label": 0
                },
                {
                    "sent": "These data are sparse and we can.",
                    "label": 0
                },
                {
                    "sent": "We are not sure how to cluster these data.",
                    "label": 0
                },
                {
                    "sent": "For example, we can cluster in this way.",
                    "label": 0
                },
                {
                    "sent": "Or in this way this way.",
                    "label": 0
                },
                {
                    "sent": "But we don't know which one is the best.",
                    "label": 0
                },
                {
                    "sent": "But if we can transform these data into another feature space to make the data in different clusters separate, well.",
                    "label": 0
                },
                {
                    "sent": "The clustering can be much easier.",
                    "label": 0
                },
                {
                    "sent": "For example, we can cluster these data in this way or this way.",
                    "label": 0
                },
                {
                    "sent": "This way all these ways are OK. And now the problem is how to transform these data to make the data in a good.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mentation in this work we use transfer learning to help build a good data representation.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "We had target data this I mean the square point in this example.",
                    "label": 1
                },
                {
                    "sent": "And we have auxiliary data and large amount of auxiliary data.",
                    "label": 0
                },
                {
                    "sent": "These auxiliary data can be easily clustered.",
                    "label": 1
                },
                {
                    "sent": "Up that we transform.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These data the auxiliary data separate very well and as a result the target data are separated.",
                    "label": 1
                },
                {
                    "sent": "So we can cluster this target data well.",
                    "label": 0
                },
                {
                    "sent": "That's the motivation of our.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "Now let's define our problems.",
                    "label": 0
                },
                {
                    "sent": "Self taught clustering.",
                    "label": 0
                },
                {
                    "sent": "Which which is an instance of transfer unsupervised.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "We have a small collection of target data.",
                    "label": 1
                },
                {
                    "sent": "These data to be clustered.",
                    "label": 0
                },
                {
                    "sent": "And we also have a large amount of auxiliary data.",
                    "label": 1
                },
                {
                    "sent": "These data are assumed to be irrelevant to the target data in public.",
                    "label": 1
                },
                {
                    "sent": "Our object is too.",
                    "label": 0
                },
                {
                    "sent": "Help the clustering performance on target data by making use the auxiliary unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "We call our problem self talk clustering or transfer unsupervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We borrowed the concept of self taught learning.",
                    "label": 0
                },
                {
                    "sent": "It's a ICM Mail paper last year by Rainer it Italian.",
                    "label": 0
                },
                {
                    "sent": "In Self taught learning it make use of large amount of irrelevant unlabeled data to help the target class classification.",
                    "label": 0
                },
                {
                    "sent": "For example, in this example the task is to.",
                    "label": 0
                },
                {
                    "sent": "Classified elephants and rhinos, and we have a Hunter.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data about sheep or mountain and water.",
                    "label": 0
                },
                {
                    "sent": "These data are irrelevant to the target data.",
                    "label": 0
                },
                {
                    "sent": "In the self learning paper it shows that these auxiliary data can be used to build high quality basis to help the target class.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacation, what we want to do is to.",
                    "label": 0
                },
                {
                    "sent": "Is a class clustering version of self taught learning?",
                    "label": 0
                },
                {
                    "sent": "How object is to cluster target data and make making use?",
                    "label": 1
                },
                {
                    "sent": "The large amount of irrelevant, unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So we call our problem self.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or clustering.",
                    "label": 0
                },
                {
                    "sent": "We can also borrow the concept of transfer learning here.",
                    "label": 0
                },
                {
                    "sent": "Only difference between transfer learning and self taught learning.",
                    "label": 1
                },
                {
                    "sent": "Here is the auxiliary data are labeled here.",
                    "label": 1
                },
                {
                    "sent": "The data in blue frames are labeled data and the other unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we want to do is.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised version transfer learning.",
                    "label": 0
                },
                {
                    "sent": "We want to cluster the target data or we can say it's unsupervised learning problem and the auxiliary data are unlabeled.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can only perform unsupervised learning on the auxiliary data.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's describe our argument.",
                    "label": 0
                },
                {
                    "sent": "Recall our problem.",
                    "label": 0
                },
                {
                    "sent": "We have a small collection of target data.",
                    "label": 1
                },
                {
                    "sent": "These data are to be clustered.",
                    "label": 0
                },
                {
                    "sent": "We also have a large amount of auxiliary data.",
                    "label": 1
                },
                {
                    "sent": "The data is assumed to be irrelevant to the target data.",
                    "label": 1
                },
                {
                    "sent": "Here is the scheme, our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have how data auxiliary data they are irrelevant in target, but they share the same feature space.",
                    "label": 0
                },
                {
                    "sent": "For example, these data may be or in text or maybe or in image.",
                    "label": 0
                },
                {
                    "sent": "Cluster the target data and auxiliary data simultaneous.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile we cluster the features based on both target and auxiliary data.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there are two cold clusterings here in our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And these two Co clustering share a same feature clustering here.",
                    "label": 0
                },
                {
                    "sent": "So the feature clustering here can be considered as a new data representation, which is consistent.",
                    "label": 0
                },
                {
                    "sent": "With both.",
                    "label": 0
                },
                {
                    "sent": "Target and auxiliary.",
                    "label": 0
                },
                {
                    "sent": "So the auxiliary data here is used to help build a good feature representation for the target data cluster clustering.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the objective function of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have X&Y, which is the target and auxiliary data.",
                    "label": 1
                },
                {
                    "sent": "We also have Z represent the.",
                    "label": 0
                },
                {
                    "sent": "Feature for the proposed target.",
                    "label": 0
                },
                {
                    "sent": "An auxiliary data.",
                    "label": 1
                },
                {
                    "sent": "The two X~ Y2Z represent the clusterings on XY and Z.",
                    "label": 0
                },
                {
                    "sent": "The object function of our algorithm is here.",
                    "label": 0
                },
                {
                    "sent": "We can see in this objective function we have two information theoretic code clusterings.",
                    "label": 0
                },
                {
                    "sent": "The information theoretical clustering minimizes information loss by Co clustering.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the optimization.",
                    "label": 0
                },
                {
                    "sent": "We first convert the information loss to the KL divergences.",
                    "label": 0
                },
                {
                    "sent": "Here the represent the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we.",
                    "label": 0
                },
                {
                    "sent": "Reformulate the KL divergences.",
                    "label": 0
                },
                {
                    "sent": "Regarding joint distribution to KL divergences with regard to.",
                    "label": 0
                },
                {
                    "sent": "Conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "From this formula we can see what we are we need to do is to choose the best clusters.",
                    "label": 1
                },
                {
                    "sent": "For each instance or features, to minimize the corresponding care divergent.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the class clustering function is here.",
                    "label": 0
                },
                {
                    "sent": "We need to iteratively optimize these.",
                    "label": 0
                },
                {
                    "sent": "Clustering function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's move.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two experiments, our experiment.",
                    "label": 0
                },
                {
                    "sent": "Focus on image cluster clustering tasks.",
                    "label": 0
                },
                {
                    "sent": "We choose hundred categories from the Caltech 256 image image corpus for each clustering task we choose the data from the corresponding category.",
                    "label": 1
                },
                {
                    "sent": "As the target unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "While the.",
                    "label": 0
                },
                {
                    "sent": "The data from the remaining categories are used as the auxiliary unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "Our.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluation criteria is entropy.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experimental results we compare our algorithm with three baseline method.",
                    "label": 0
                },
                {
                    "sent": "Platter is 1 dimensional clustering clustering tool.",
                    "label": 0
                },
                {
                    "sent": "Feature clustering means we first build a new data representation using feature clustering and then apply 1 dimensional cluster clustering on the.",
                    "label": 0
                },
                {
                    "sent": "Data in new feature representation.",
                    "label": 0
                },
                {
                    "sent": "Co clustering is the information theoretic Co clustering and STC is our algorithm for each baseline method, we have two versions.",
                    "label": 0
                },
                {
                    "sent": "Separate means the algorithm is only applied on the target data and combine mean.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Performed on the data combined by combined with both target and auxiliary data.",
                    "label": 0
                },
                {
                    "sent": "From the table we can see our algorithm always outperform the baseline method.",
                    "label": 0
                },
                {
                    "sent": "These two figures show the parameter setting our experiment.",
                    "label": 0
                },
                {
                    "sent": "From this figure we can see we set the number number of feature clustering too.",
                    "label": 0
                },
                {
                    "sent": "64 and from this speaker we said the trader parameters for our in our.",
                    "label": 0
                },
                {
                    "sent": "Self taught clustering algorithm 21 and this figure showed the.",
                    "label": 0
                },
                {
                    "sent": "Convergence property of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "From this figure we can see our algorithm converges very fast and very well.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's conclude.",
                    "label": 0
                },
                {
                    "sent": "Hope.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "In this paper we investigate the self taught clustering.",
                    "label": 0
                },
                {
                    "sent": "Problem which is an instance of.",
                    "label": 0
                },
                {
                    "sent": "Transfer unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "We propose to use.",
                    "label": 0
                },
                {
                    "sent": "Irrelevant, unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "To help the target clustering.",
                    "label": 1
                },
                {
                    "sent": "We develope Co clustering based self taught clustering algorithm in our algorithm to Co clustering our performance simultaneously between target data and features and between the the auxiliary data and the features.",
                    "label": 1
                },
                {
                    "sent": "The two Co clustering share.",
                    "label": 0
                },
                {
                    "sent": "Feature clustering so the knowledge can be transferred.",
                    "label": 0
                },
                {
                    "sent": "By feature clustering, the experimental results show the Encourageing performance of our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Irrelevant.",
                    "label": 0
                },
                {
                    "sent": "The relevant data in our.",
                    "label": 0
                },
                {
                    "sent": "Our problem is only in topic.",
                    "label": 0
                },
                {
                    "sent": "For example, they are in different categories.",
                    "label": 0
                },
                {
                    "sent": "Also, you know that they are not in the two categories that you consider for that, yes.",
                    "label": 0
                },
                {
                    "sent": "But if if the data are in the.",
                    "label": 0
                },
                {
                    "sent": "Elephant or rhino category.",
                    "label": 0
                },
                {
                    "sent": "I think it will be better.",
                    "label": 0
                },
                {
                    "sent": "And then have you compared your approach with the information bottleneck approach?",
                    "label": 0
                },
                {
                    "sent": "Or is this great discussion?",
                    "label": 0
                },
                {
                    "sent": "We understand information because that you must find yes I compared.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I compared our algorithm with the Co clustering is the information theoretic code clustering.",
                    "label": 0
                },
                {
                    "sent": "Which is derived based on the information bottleneck.",
                    "label": 0
                },
                {
                    "sent": "And maybe similar but not the same.",
                    "label": 0
                }
            ]
        }
    }
}