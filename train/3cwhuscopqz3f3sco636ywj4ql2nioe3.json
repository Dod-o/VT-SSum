{
    "id": "3cwhuscopqz3f3sco636ywj4ql2nioe3",
    "title": "Action class detection and recognition",
    "info": {
        "author": [
            "Ivan Laptev, INRIA - The French National Institute for Research in Computer Science and Control"
        ],
        "published": "Feb. 14, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Computer Vision->Video Analysis"
        ]
    },
    "url": "http://videolectures.net/mcvc08_laptev_acdr/",
    "segmentation": [
        [
            "OK, so this talk going to be about video as well, so I hope the videos will run.",
            "My voice.",
            "Will have to."
        ],
        [
            "Right, so this work is about action class detection and recognition in realistic video, and so it's part of the etm.",
            "Visual saliency and DC team is quite large, so if you see there are many different topics here and different actors on the right and it would be difficult to cover all topics and we have chosen just one one of them and it seems to be repres."
        ],
        [
            "Active.",
            "So.",
            "Alright, So what is the problem you're trying to solve here?",
            "And so it is about human actions and the motivation.",
            "Why do we want to look at human actions?",
            "Is there is, as Andrew mentioned, this morning.",
            "So there is a huge amount of video available.",
            "For example, professional archives like BBC Motion Gallery or YouTube and it's growing constantly and it's not indexed well by the keywords, so it's it is a problem.",
            "And in particular, human actions seems to be a major events in TV news, movies and personal videos.",
            "So if you want to index the video, it's obvious that that one, one important component will be human actions.",
            "And OK, so these are for example the the actions which we would like our system to to be able to do in the future.",
            "And so we also look if you think of what are the applications, why do we want to recognize and detect these actions?",
            "So there are obvious applications like content based browsing.",
            "So for example forwarding to the next goal when you browse the.",
            "Football matches or video recycling.",
            "Fighting Bush shaking hands with Putin or some like this, or even we found the papers of human scientists who actually went through the movies and by hand annotated like smoking actions because they were interested in relating the smoking of main actors in movies to their adolescent smoking.",
            "And so there are different, quite quite different and many applications of."
        ],
        [
            "Action recognition, right?",
            "So a bit of look backwards so, or just let's try to post question what are the human actions and.",
            "One type of human actions that people were looking before is.",
            "There may be summarized with these two and so basically so it is a hard problem of people working on it for maybe 1020 years now, and most of the work considered the simple scenarios and actions performed by different individuals, so their their reelections.",
            "But still the setting is quite simplified and so whatever the first definition would be the.",
            "These actions they are defined by the physical body motion.",
            "So I ternative definition, which we think is more relevant for task of multimedia.",
            "Indexing is to look at actions as interactions with the environment on specific purpose.",
            "So if we contrast these and these so we see that here we almost have in all of them have this similar physical motion like turning their hand, but the meaning is completely different because the purpose is different, and so if you will."
        ],
        [
            "More motivating, so here we see even if they have similar objects.",
            "So this scene made may give different contexts and different different actions so.",
            "So it seems that action recognition is much more complex than this type of physical actions that people were looking before, but we really had to approach the real problem in the video indexing or related to human actions we have to."
        ],
        [
            "These type of data.",
            "Right so bit of.",
            "Introduction to the Why is the problem hard like So what are the challenges?",
            "So first of all, we can note that similar problems with that we expect in actions.",
            "They also appear in object recognition.",
            "For example, have you variation lighting, background variation people?",
            "Wear different clothes and so on, but there are additional problems which are variation of individual motion and camera motion that do not that pose additional problems to action recognition compared to object recognition.",
            "So I'm just looking at these two examples so we see this is the difference in objects.",
            "Oh well appearance or the type of.",
            "The coffee container here is quite different.",
            "The also if we look at the motion of these two guys smoking so it is very different so.",
            "But saying all these variations so in contrast to static images we also have motion information in video that we hope to take advantage of in."
        ],
        [
            "You know recognition in the future, right?",
            "So the particular situation actually for this work was that we found excellent movie called Coffee and cigarettes and it is a well.",
            "It is a movie.",
            "1/2 hour movie and but it contains different people different since people are talking and drinking coffee basion smoking so and it appeared to be excellent training set for or test set for 4 four algorithms for action recognition.",
            "So I should say that in the past there were no.",
            "Dunno, realistic action data set, so it is a current problem.",
            "And so this movie in particular contained 159 drinking actions and about the same smoking actions.",
            "And so for the.",
            "In in the scale of 1 movie it was possible to go and to actually annotate by hand.",
            "So we went through the whole movie and annotated the drinking, smoking actions and the notation was partly spatial, so we annotated the rectangle of the head at the moment where the person was actually taking the glass to the mouth or smoking, and also roughly the temporal beginning and the end and the key frame of an action.",
            "So that was the type of annotation and."
        ],
        [
            "Right, so these are the drinking action samples that we consider for training and for testing.",
            "And as you can see.",
            "So there is a large variability within the both sets an between them.",
            "And I should say that there were no.",
            "So these actually cropped actions already.",
            "So the whole scene was more more complex and I should say that there is no subject overlap between different sets and there is no scene overlap, so all.",
            "Actions happen in different scenes."
        ],
        [
            "So.",
            "Right, so now so I was talking about about datasets and now about the methodology which we are trying to adapt here.",
            "So the question we try to ask here, can we consider actions as spacetime objects and the reason for this is to transfer their successful experience that we have in computer vision for object recognition and object detection recently to transfer it to action detection and so?",
            "Many of you might know, for example, Pascal Challenge where these type of objects are more or less successfully detected by the algorithms and.",
            "So you want to try to do the same for actions and we restrict the actions to be atomic.",
            "So atomic means that it has the actions should have a consistent time structure and what I'm saying this.",
            "So for example, for our drinking actions this is an example of such anatomy action.",
            "So if we look at the temporal slice along this.",
            "Line here or different lines.",
            "So we see that the temporal structure of action, which is basically motion of the hand, is quite similar.",
            "For all of them, and this is despite the people are different here and we have different views so well, this is intuitive motivation that tells us that these type of video objects they could be handled similar as spatial objects in images.",
            "So So what we try to do?",
            "They are going to try now is to try to adapt the method that works well for action for object detection in images."
        ],
        [
            "Action detection in movies.",
            "So what is our action representation?",
            "I said in a few slides before that we had a.",
            "We have made manual annotation of actions in the training and test set.",
            "Of course to make them.",
            "To make the.",
            "Ulation later on, so so for the training set, we can imagine that we have a cuboid limited by the time of the 1st frame and the last frame, and we crop and resize as people people usually do for object detection and then we do consider many different features.",
            "Cuboid features in the in this space time volume.",
            "And for each cuboid feature, so we either we compute Hog which is.",
            "As many of you know, so this is history of four inches gradient.",
            "Sift like descriptor or sorry this should be F Hough.",
            "Histograms of optic flow.",
            "And OK, so each such box represents by these or these type of feature and we also in addition subdivide them into blocks, sort of sift motivated to preserve."
        ],
        [
            "Some special arrangement, so bit more about the features so so each of the rectangles here.",
            "So there is a huge number of all possible rectangles, and we actually we just randomly select some of them because they tend to be very redundant.",
            "So for each of these, we compute this type of different gradings and these two types of flow and shape information because we believe that both motion and shape is important for for action recognition.",
            "And OK, so this is just did."
        ],
        [
            "Right, so for now, as we have the features, we have the training set, so the learning is is is pretty similar to the standard Adaboost learning introduced by French appearance, successfully used for.",
            "This detection and many other algorithms and so here, given the large pool of potential features, we tried to, well we do his learning on these features, and the boosting selects the potentially interesting features.",
            "And so this scheme just explains difference.",
            "So in original village on so it was the features for binary.",
            "So it was possible to find the optimal threshold and for histogram features we need something more sophisticated.",
            "So we use Fisher discriminate and but this details you can find.",
            "In the paper."
        ],
        [
            "Right, so I'm skipping some details about training, so we are interested in detection task.",
            "But before going to these, so I'm going to just show some results for classification task which is simpler than detection task and in classification we considered two different scenarios.",
            "So one is for where we took the test samples, cropped test samples for drinking and then just some random motion patterns and so this is.",
            "It will see curve as you see, the precision is very good and so I should mention.",
            "So there are three methods compared here, so one is for optic floor space type optic flow classifier.",
            "So where we where these features are restricted to be on the optic flow.",
            "The Orange One is where the feature service ticket to be optic could be.",
            "The optic flow or hog features to also capture the shape and this one which I didn't talk yet about this.",
            "Actually, this static object classifier trained on the keyframes.",
            "So just remember that keyframe is just the picture in the middle here, and the purpose of trying this was to answer location.",
            "Can we do actually action detection using only the static information in the image?",
            "Without going into this more complex spacetime crossfire?",
            "OK this is our results for drinking random motion recognition and the results are.",
            "Quite good for both methods and so this is much more challenging.",
            "Set test set because we have drinking and smoking and what we see here that the static object classifier fails completely.",
            "So it currently well we know we know that this one works quite well for many object categories in Pascal challenge, so but it doesn't work for discriminating these tools, so this at the moment it seems to be difficult to discriminate this using on the static information, but.",
            "Using the motion information, we can get quite good results and this is quite promising.",
            "So what is it was a bit confusing for us that we expected the shape information which is.",
            "Included in the orange curve, we expected it to boost the performance, so if you add more information so we supposed to get better performance But actually did not, well it performed were equally so.",
            "That was a bit confusing, but we still believe that the shape information is."
        ],
        [
            "Important, so that's why I said here.",
            "We looked at what actually the features are learned by the boosting methods and so just be careful.",
            "Interpretation for this picture.",
            "So, so this is these are all the features, maybe about five or one one 1000 features selected by boosting projected in two hours standard action cuboid and the density corresponds to the number of features in a given part of it.",
            "So as we see so.",
            "The time is here and we see that the most of the features are concentrated in the beginning at the end of the motion and this is about the time when the hand goes up and down.",
            "So so this is quite intuitive and also if we project them just XY.",
            "So we see that the highest density is around the hand here, so it's sort of intuitive.",
            "Whereas if we do the same for the static classifier, we see that the features are in the upper part of the image.",
            "Action space so, and this corresponds to the head rectangle.",
            "So what I'm trying to say this with this slide is that the spacetime classifier and the key key frame classifier seem to capture complementary."
        ],
        [
            "Nation.",
            "So that's why we try to combine these two classifiers and that's what we call here.",
            "Keyframe priming.",
            "So keyframe priming work is for, so we first run the static keyframe classifier on every well every frame in the movie and then we take so in the training we know that these positive examples and all the Reds which are false positives in the object static object keyframe classifier, we take them as a negative examples when training this based amplifier.",
            "So, so by doing this we are rejecting all them.",
            "Other false positives that could be found in the test.",
            "So we do the same.",
            "So so so first round a key frame classifier and then only for the detections.",
            "So we run the spacetime classifier and so we reject lots of.",
            "False positive."
        ],
        [
            "And get good results.",
            "So I guess I should say beat about the test set so we subdivided the coffee and cigarettes movie into training and test sets and the test set was 25 minutes with about 38 drinking actions.",
            "And as I said before, there was an overlap in subjects or seen with the training set and.",
            "So it's important that we also research quite exhaustively over different positions.",
            "Temporal extents of an action, and the spatial size of an action because they were really we observed that in a training set, the drinking was the temporal extent of drinking could be between 40 and 200 frames.",
            "So taking really different temporal scales was quite important and OK, so these results.",
            "So what we can see here that if we don't use the keyframe priming explained before, so we get quite full results.",
            "Shape gives a bit of improvement by the blue curve here compared to optic flow, but it's not compareable to if we combine them staticky frame, detector, and spacetime classifier after."
        ],
        [
            "Training.",
            "Right and so so this is going to show.",
            "Some results, so this is about one or two 2.",
            "Minutes movie so you will see some most of the detections there correct, but you will see also some false positives, and so the always supposed to show that people do behave normally.",
            "So this is just the standard or realistic realistic settings.",
            "So they have scale variations so and so.",
            "Actually people do.",
            "Do different.",
            "Actions, for example like answering phone or other which could be confused with which could be easily confused with the drinking.",
            "So.",
            "Maybe I should not.",
            "So sometimes it appears that OK, so here is he's going to answer the phone.",
            "And so we are quite happy this is not detected as false positive.",
            "So sometimes we got detection here in the mirror and actually it was correct, but it was not annotated by the.",
            "So for example here, it's also seems quite challenging because he was attempted to drink, but he did not and it was not detected as.",
            "He's drinking.",
            "OK, so this is 1 false positive.",
            "OK, and this is another false positive, which we think is quite quite understandable.",
            "So it's a hand moving to the face."
        ],
        [
            "So from there.",
            "20 most confidence detection.",
            "So sorry from the from the detection of these 20 most confidence detection sorted in the confidence order and so the yellow side correct rather incorrect and so some are understandable.",
            "Summer less understandable.",
            "So for example.",
            "So here there are two guys different guys actually checking the witch, so there's some consistency.",
            "There is one guy smoking here and so this we are not clear why is it it always got very high.",
            "Score Confidence Corp but still so most most of detections from these 20 are correct, and so it's obviously there is a space for improvement.",
            "But we."
        ],
        [
            "Happy with this result.",
            "Right, so this is the summary, so this is appears to be this first attempt to address human action in real movies and so action detection recognition seems possible and different challenging conditions.",
            "So this is a positive result, and so something which we are not clear about.",
            "Separate learning of shape in motion appears to be better than joint.",
            "Maybe do it some overfitting so future work there.",
            "So there's lots of interesting things to do.",
            "And one is related to what Andrew was talking in this morning, and so we need so that we were lucky to find this video that had.",
            "Good example, lots of examples for for drinking and smoking, but you know on average, so the human actions will maybe appear five times or 345 times per movie.",
            "So and of course to get enough training and test data.",
            "Emmanuel Isation efforts would be a very exhaustive, so it's it's it's very interesting and we are considering the semi automatic action notation using movie scripts.",
            "So the explicit handling of action under multiple views is interesting and combination of classification.",
            "This text action classification which takes as well and so far how many minutes they left.",
            "5 minutes OK.",
            "So well, if I had five minutes, maybe I could actually, because this this we've done some work in this direction.",
            "Maybe I can just show some of these very briefly.",
            "Right, so well, as I mentioned the currently in action recognition, the data collection is a datasets are our big program and so for example if we want to go to videos, web video search like people do for image.",
            "For collecting image databases, this approach appears to be not very good because we get sometimes examples, but they're not very representative, so example we forget handshake.",
            "We get something like this.",
            "Or will these?",
            "Which are handshakes, but they are quite marginal anyway, so we're getting quite a few examples of this.",
            "And OK so we can go for static image search which works better, but still well it does.",
            "It doesn't contain video, so so much it seems to be potentially much more interesting approaches to look at movies where there are lots of realistic variation of human actions and many lots of classes.",
            "Many examples per class.",
            "But as I said, so there are only few examples per class, sample pair, movie and my notation would be too exhausting so.",
            "The one I price.",
            "Subtitle to script alignments.",
            "Boards from Andrew.",
            "So, so there are lots of movie scripts on the net which you can just download and subtitle information.",
            "It has time information so we can align it to movie script and get there if there are time localization for actions.",
            "So that's the for example DNA in Casablanca movie so we can get that Rick sits down with these which happen at this time.",
            "So this is quite nice.",
            "And so there are some problems.",
            "For example, if people do not speak for a long time, we can get lots of actions happening here, but we don't have a good time localization.",
            "And there are more so so there are many good things with it, so there are realistic creations.",
            "So once we did align these scripts to the movies, so we get there is no extra overhead for new action classes.",
            "We can just run this in.",
            "Same automatic to automatic annotation tool for new action class and get it so there are problems, so no spatial localization or temporal localization and.",
            "And so on.",
            "So OK.",
            "So we did a bit of evaluation here actually to see what what, what happens, what actions described in text, how do they actually correspond with the movie and.",
            "So I should just say that so we did subscript subtitle to script alignment and so this part of.",
            "Action, so this is the number of action samples that we have tried just looking at the text and the movie and so for the actions where we did have perfect alignment, we had only about 6070% recall, and this is because some of the actions that happen that are described in the movie or so in the script actually never happened in the movies.",
            "So there is a potential source for false positives just because the.",
            "This action scripts they don't correspond sometimes, but not correspond to to the movie, and as far as been.",
            "So it's we don't know how to how to deal with this so so it seems to be potentially noisy training set.",
            "So for example.",
            "So this is 1 example of visual false positives, so this script says the big car pulls up to arm, officers get out and actually so the action was there, but the action was outside of the view of the camera views.",
            "So these are just straight some potential problems with them action recognition.",
            "So there are also more problems just without going to vision.",
            "Just if you look at the text.",
            "So get out car action for example could be expressed in very different ways and it seems that we need a text classification here too.",
            "To overcome this variation and also there is a potential false positives like about to sit down, he freezes so it's so there is a part of a text processing processing here which should be addressed.",
            "OK, so I think I will stop here for.",
            "So many questions.",
            "Degrade.",
            "Having YouTube Prince.",
            "OK, so the resolution we work with is.",
            "About 74 for this method.",
            "Yeah, so it's about 70 by 17 in space and about 40 frames in in time.",
            "So I think I think it's well.",
            "70 by 70 it seems to should be should work.",
            "I mean, of course, if you have very small resolution YouTube so you don't get it, but.",
            "For some videos it should work.",
            "Another point you in in your evaluation you showed the degree of certainty or uncertainty associated with your classification.",
            "Have you considered that you could use that also for use active learning ideas for to bootstrap datasets in these for the new types of actions that you were interested in?",
            "Sorry, you are talking about the last part about text or both.",
            "You finished up with this problem that you were talking about having training data.",
            "Like you or no?",
            "In your supplementary piece, OK?",
            "Yeah, so you.",
            "Your problem here is that you don't have enough annotated training samples.",
            "And you're talking about using texts are using this script information to produce training data exactly?",
            "Yes, yes, yes.",
            "Here.",
            "In machine learning is to to to use an active learning strategy, OK to Bootstrap?",
            "To use both vision maybe and text and two to improve.",
            "Learning, but OK. OK right.",
            "OK OK so of course the aim is to try to do full automatic so if we if you.",
            "Yeah, you're right, if you want to.",
            "If we can afford to involve the human, that would be helpful.",
            "Questions.",
            "From the.",
            "Yes, it's post processing.",
            "It's not real time, yeah?",
            "OK, so as I said so this is quite exhaustive, so really we search for all different positions and different size and the different size in the image space and different extents in time.",
            "Well, it's at the moment it's about 7 seconds per frame, so it's not real time, but.",
            "It was possible to run it on on the movie at least.",
            "Look for the questions.",
            "So it's coffee break now.",
            "Right now coffee break for."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this talk going to be about video as well, so I hope the videos will run.",
                    "label": 0
                },
                {
                    "sent": "My voice.",
                    "label": 0
                },
                {
                    "sent": "Will have to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this work is about action class detection and recognition in realistic video, and so it's part of the etm.",
                    "label": 0
                },
                {
                    "sent": "Visual saliency and DC team is quite large, so if you see there are many different topics here and different actors on the right and it would be difficult to cover all topics and we have chosen just one one of them and it seems to be repres.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what is the problem you're trying to solve here?",
                    "label": 0
                },
                {
                    "sent": "And so it is about human actions and the motivation.",
                    "label": 1
                },
                {
                    "sent": "Why do we want to look at human actions?",
                    "label": 1
                },
                {
                    "sent": "Is there is, as Andrew mentioned, this morning.",
                    "label": 0
                },
                {
                    "sent": "So there is a huge amount of video available.",
                    "label": 1
                },
                {
                    "sent": "For example, professional archives like BBC Motion Gallery or YouTube and it's growing constantly and it's not indexed well by the keywords, so it's it is a problem.",
                    "label": 0
                },
                {
                    "sent": "And in particular, human actions seems to be a major events in TV news, movies and personal videos.",
                    "label": 1
                },
                {
                    "sent": "So if you want to index the video, it's obvious that that one, one important component will be human actions.",
                    "label": 0
                },
                {
                    "sent": "And OK, so these are for example the the actions which we would like our system to to be able to do in the future.",
                    "label": 0
                },
                {
                    "sent": "And so we also look if you think of what are the applications, why do we want to recognize and detect these actions?",
                    "label": 1
                },
                {
                    "sent": "So there are obvious applications like content based browsing.",
                    "label": 0
                },
                {
                    "sent": "So for example forwarding to the next goal when you browse the.",
                    "label": 0
                },
                {
                    "sent": "Football matches or video recycling.",
                    "label": 0
                },
                {
                    "sent": "Fighting Bush shaking hands with Putin or some like this, or even we found the papers of human scientists who actually went through the movies and by hand annotated like smoking actions because they were interested in relating the smoking of main actors in movies to their adolescent smoking.",
                    "label": 1
                },
                {
                    "sent": "And so there are different, quite quite different and many applications of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action recognition, right?",
                    "label": 0
                },
                {
                    "sent": "So a bit of look backwards so, or just let's try to post question what are the human actions and.",
                    "label": 1
                },
                {
                    "sent": "One type of human actions that people were looking before is.",
                    "label": 0
                },
                {
                    "sent": "There may be summarized with these two and so basically so it is a hard problem of people working on it for maybe 1020 years now, and most of the work considered the simple scenarios and actions performed by different individuals, so their their reelections.",
                    "label": 0
                },
                {
                    "sent": "But still the setting is quite simplified and so whatever the first definition would be the.",
                    "label": 0
                },
                {
                    "sent": "These actions they are defined by the physical body motion.",
                    "label": 1
                },
                {
                    "sent": "So I ternative definition, which we think is more relevant for task of multimedia.",
                    "label": 0
                },
                {
                    "sent": "Indexing is to look at actions as interactions with the environment on specific purpose.",
                    "label": 1
                },
                {
                    "sent": "So if we contrast these and these so we see that here we almost have in all of them have this similar physical motion like turning their hand, but the meaning is completely different because the purpose is different, and so if you will.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More motivating, so here we see even if they have similar objects.",
                    "label": 0
                },
                {
                    "sent": "So this scene made may give different contexts and different different actions so.",
                    "label": 0
                },
                {
                    "sent": "So it seems that action recognition is much more complex than this type of physical actions that people were looking before, but we really had to approach the real problem in the video indexing or related to human actions we have to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These type of data.",
                    "label": 0
                },
                {
                    "sent": "Right so bit of.",
                    "label": 0
                },
                {
                    "sent": "Introduction to the Why is the problem hard like So what are the challenges?",
                    "label": 0
                },
                {
                    "sent": "So first of all, we can note that similar problems with that we expect in actions.",
                    "label": 1
                },
                {
                    "sent": "They also appear in object recognition.",
                    "label": 0
                },
                {
                    "sent": "For example, have you variation lighting, background variation people?",
                    "label": 1
                },
                {
                    "sent": "Wear different clothes and so on, but there are additional problems which are variation of individual motion and camera motion that do not that pose additional problems to action recognition compared to object recognition.",
                    "label": 1
                },
                {
                    "sent": "So I'm just looking at these two examples so we see this is the difference in objects.",
                    "label": 0
                },
                {
                    "sent": "Oh well appearance or the type of.",
                    "label": 1
                },
                {
                    "sent": "The coffee container here is quite different.",
                    "label": 0
                },
                {
                    "sent": "The also if we look at the motion of these two guys smoking so it is very different so.",
                    "label": 0
                },
                {
                    "sent": "But saying all these variations so in contrast to static images we also have motion information in video that we hope to take advantage of in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know recognition in the future, right?",
                    "label": 0
                },
                {
                    "sent": "So the particular situation actually for this work was that we found excellent movie called Coffee and cigarettes and it is a well.",
                    "label": 1
                },
                {
                    "sent": "It is a movie.",
                    "label": 0
                },
                {
                    "sent": "1/2 hour movie and but it contains different people different since people are talking and drinking coffee basion smoking so and it appeared to be excellent training set for or test set for 4 four algorithms for action recognition.",
                    "label": 0
                },
                {
                    "sent": "So I should say that in the past there were no.",
                    "label": 0
                },
                {
                    "sent": "Dunno, realistic action data set, so it is a current problem.",
                    "label": 0
                },
                {
                    "sent": "And so this movie in particular contained 159 drinking actions and about the same smoking actions.",
                    "label": 0
                },
                {
                    "sent": "And so for the.",
                    "label": 0
                },
                {
                    "sent": "In in the scale of 1 movie it was possible to go and to actually annotate by hand.",
                    "label": 0
                },
                {
                    "sent": "So we went through the whole movie and annotated the drinking, smoking actions and the notation was partly spatial, so we annotated the rectangle of the head at the moment where the person was actually taking the glass to the mouth or smoking, and also roughly the temporal beginning and the end and the key frame of an action.",
                    "label": 0
                },
                {
                    "sent": "So that was the type of annotation and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so these are the drinking action samples that we consider for training and for testing.",
                    "label": 1
                },
                {
                    "sent": "And as you can see.",
                    "label": 0
                },
                {
                    "sent": "So there is a large variability within the both sets an between them.",
                    "label": 0
                },
                {
                    "sent": "And I should say that there were no.",
                    "label": 0
                },
                {
                    "sent": "So these actually cropped actions already.",
                    "label": 0
                },
                {
                    "sent": "So the whole scene was more more complex and I should say that there is no subject overlap between different sets and there is no scene overlap, so all.",
                    "label": 0
                },
                {
                    "sent": "Actions happen in different scenes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, so now so I was talking about about datasets and now about the methodology which we are trying to adapt here.",
                    "label": 0
                },
                {
                    "sent": "So the question we try to ask here, can we consider actions as spacetime objects and the reason for this is to transfer their successful experience that we have in computer vision for object recognition and object detection recently to transfer it to action detection and so?",
                    "label": 0
                },
                {
                    "sent": "Many of you might know, for example, Pascal Challenge where these type of objects are more or less successfully detected by the algorithms and.",
                    "label": 0
                },
                {
                    "sent": "So you want to try to do the same for actions and we restrict the actions to be atomic.",
                    "label": 0
                },
                {
                    "sent": "So atomic means that it has the actions should have a consistent time structure and what I'm saying this.",
                    "label": 0
                },
                {
                    "sent": "So for example, for our drinking actions this is an example of such anatomy action.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the temporal slice along this.",
                    "label": 0
                },
                {
                    "sent": "Line here or different lines.",
                    "label": 0
                },
                {
                    "sent": "So we see that the temporal structure of action, which is basically motion of the hand, is quite similar.",
                    "label": 0
                },
                {
                    "sent": "For all of them, and this is despite the people are different here and we have different views so well, this is intuitive motivation that tells us that these type of video objects they could be handled similar as spatial objects in images.",
                    "label": 0
                },
                {
                    "sent": "So So what we try to do?",
                    "label": 0
                },
                {
                    "sent": "They are going to try now is to try to adapt the method that works well for action for object detection in images.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action detection in movies.",
                    "label": 0
                },
                {
                    "sent": "So what is our action representation?",
                    "label": 0
                },
                {
                    "sent": "I said in a few slides before that we had a.",
                    "label": 0
                },
                {
                    "sent": "We have made manual annotation of actions in the training and test set.",
                    "label": 0
                },
                {
                    "sent": "Of course to make them.",
                    "label": 0
                },
                {
                    "sent": "To make the.",
                    "label": 0
                },
                {
                    "sent": "Ulation later on, so so for the training set, we can imagine that we have a cuboid limited by the time of the 1st frame and the last frame, and we crop and resize as people people usually do for object detection and then we do consider many different features.",
                    "label": 0
                },
                {
                    "sent": "Cuboid features in the in this space time volume.",
                    "label": 0
                },
                {
                    "sent": "And for each cuboid feature, so we either we compute Hog which is.",
                    "label": 0
                },
                {
                    "sent": "As many of you know, so this is history of four inches gradient.",
                    "label": 0
                },
                {
                    "sent": "Sift like descriptor or sorry this should be F Hough.",
                    "label": 0
                },
                {
                    "sent": "Histograms of optic flow.",
                    "label": 0
                },
                {
                    "sent": "And OK, so each such box represents by these or these type of feature and we also in addition subdivide them into blocks, sort of sift motivated to preserve.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some special arrangement, so bit more about the features so so each of the rectangles here.",
                    "label": 0
                },
                {
                    "sent": "So there is a huge number of all possible rectangles, and we actually we just randomly select some of them because they tend to be very redundant.",
                    "label": 0
                },
                {
                    "sent": "So for each of these, we compute this type of different gradings and these two types of flow and shape information because we believe that both motion and shape is important for for action recognition.",
                    "label": 0
                },
                {
                    "sent": "And OK, so this is just did.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so for now, as we have the features, we have the training set, so the learning is is is pretty similar to the standard Adaboost learning introduced by French appearance, successfully used for.",
                    "label": 0
                },
                {
                    "sent": "This detection and many other algorithms and so here, given the large pool of potential features, we tried to, well we do his learning on these features, and the boosting selects the potentially interesting features.",
                    "label": 0
                },
                {
                    "sent": "And so this scheme just explains difference.",
                    "label": 0
                },
                {
                    "sent": "So in original village on so it was the features for binary.",
                    "label": 0
                },
                {
                    "sent": "So it was possible to find the optimal threshold and for histogram features we need something more sophisticated.",
                    "label": 1
                },
                {
                    "sent": "So we use Fisher discriminate and but this details you can find.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so I'm skipping some details about training, so we are interested in detection task.",
                    "label": 0
                },
                {
                    "sent": "But before going to these, so I'm going to just show some results for classification task which is simpler than detection task and in classification we considered two different scenarios.",
                    "label": 0
                },
                {
                    "sent": "So one is for where we took the test samples, cropped test samples for drinking and then just some random motion patterns and so this is.",
                    "label": 1
                },
                {
                    "sent": "It will see curve as you see, the precision is very good and so I should mention.",
                    "label": 0
                },
                {
                    "sent": "So there are three methods compared here, so one is for optic floor space type optic flow classifier.",
                    "label": 0
                },
                {
                    "sent": "So where we where these features are restricted to be on the optic flow.",
                    "label": 0
                },
                {
                    "sent": "The Orange One is where the feature service ticket to be optic could be.",
                    "label": 0
                },
                {
                    "sent": "The optic flow or hog features to also capture the shape and this one which I didn't talk yet about this.",
                    "label": 0
                },
                {
                    "sent": "Actually, this static object classifier trained on the keyframes.",
                    "label": 0
                },
                {
                    "sent": "So just remember that keyframe is just the picture in the middle here, and the purpose of trying this was to answer location.",
                    "label": 0
                },
                {
                    "sent": "Can we do actually action detection using only the static information in the image?",
                    "label": 0
                },
                {
                    "sent": "Without going into this more complex spacetime crossfire?",
                    "label": 0
                },
                {
                    "sent": "OK this is our results for drinking random motion recognition and the results are.",
                    "label": 0
                },
                {
                    "sent": "Quite good for both methods and so this is much more challenging.",
                    "label": 0
                },
                {
                    "sent": "Set test set because we have drinking and smoking and what we see here that the static object classifier fails completely.",
                    "label": 0
                },
                {
                    "sent": "So it currently well we know we know that this one works quite well for many object categories in Pascal challenge, so but it doesn't work for discriminating these tools, so this at the moment it seems to be difficult to discriminate this using on the static information, but.",
                    "label": 0
                },
                {
                    "sent": "Using the motion information, we can get quite good results and this is quite promising.",
                    "label": 0
                },
                {
                    "sent": "So what is it was a bit confusing for us that we expected the shape information which is.",
                    "label": 0
                },
                {
                    "sent": "Included in the orange curve, we expected it to boost the performance, so if you add more information so we supposed to get better performance But actually did not, well it performed were equally so.",
                    "label": 1
                },
                {
                    "sent": "That was a bit confusing, but we still believe that the shape information is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Important, so that's why I said here.",
                    "label": 0
                },
                {
                    "sent": "We looked at what actually the features are learned by the boosting methods and so just be careful.",
                    "label": 0
                },
                {
                    "sent": "Interpretation for this picture.",
                    "label": 0
                },
                {
                    "sent": "So, so this is these are all the features, maybe about five or one one 1000 features selected by boosting projected in two hours standard action cuboid and the density corresponds to the number of features in a given part of it.",
                    "label": 0
                },
                {
                    "sent": "So as we see so.",
                    "label": 0
                },
                {
                    "sent": "The time is here and we see that the most of the features are concentrated in the beginning at the end of the motion and this is about the time when the hand goes up and down.",
                    "label": 0
                },
                {
                    "sent": "So so this is quite intuitive and also if we project them just XY.",
                    "label": 0
                },
                {
                    "sent": "So we see that the highest density is around the hand here, so it's sort of intuitive.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we do the same for the static classifier, we see that the features are in the upper part of the image.",
                    "label": 0
                },
                {
                    "sent": "Action space so, and this corresponds to the head rectangle.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to say this with this slide is that the spacetime classifier and the key key frame classifier seem to capture complementary.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "So that's why we try to combine these two classifiers and that's what we call here.",
                    "label": 0
                },
                {
                    "sent": "Keyframe priming.",
                    "label": 0
                },
                {
                    "sent": "So keyframe priming work is for, so we first run the static keyframe classifier on every well every frame in the movie and then we take so in the training we know that these positive examples and all the Reds which are false positives in the object static object keyframe classifier, we take them as a negative examples when training this based amplifier.",
                    "label": 0
                },
                {
                    "sent": "So, so by doing this we are rejecting all them.",
                    "label": 0
                },
                {
                    "sent": "Other false positives that could be found in the test.",
                    "label": 0
                },
                {
                    "sent": "So we do the same.",
                    "label": 0
                },
                {
                    "sent": "So so so first round a key frame classifier and then only for the detections.",
                    "label": 0
                },
                {
                    "sent": "So we run the spacetime classifier and so we reject lots of.",
                    "label": 0
                },
                {
                    "sent": "False positive.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And get good results.",
                    "label": 0
                },
                {
                    "sent": "So I guess I should say beat about the test set so we subdivided the coffee and cigarettes movie into training and test sets and the test set was 25 minutes with about 38 drinking actions.",
                    "label": 1
                },
                {
                    "sent": "And as I said before, there was an overlap in subjects or seen with the training set and.",
                    "label": 1
                },
                {
                    "sent": "So it's important that we also research quite exhaustively over different positions.",
                    "label": 0
                },
                {
                    "sent": "Temporal extents of an action, and the spatial size of an action because they were really we observed that in a training set, the drinking was the temporal extent of drinking could be between 40 and 200 frames.",
                    "label": 0
                },
                {
                    "sent": "So taking really different temporal scales was quite important and OK, so these results.",
                    "label": 0
                },
                {
                    "sent": "So what we can see here that if we don't use the keyframe priming explained before, so we get quite full results.",
                    "label": 0
                },
                {
                    "sent": "Shape gives a bit of improvement by the blue curve here compared to optic flow, but it's not compareable to if we combine them staticky frame, detector, and spacetime classifier after.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Right and so so this is going to show.",
                    "label": 0
                },
                {
                    "sent": "Some results, so this is about one or two 2.",
                    "label": 0
                },
                {
                    "sent": "Minutes movie so you will see some most of the detections there correct, but you will see also some false positives, and so the always supposed to show that people do behave normally.",
                    "label": 0
                },
                {
                    "sent": "So this is just the standard or realistic realistic settings.",
                    "label": 0
                },
                {
                    "sent": "So they have scale variations so and so.",
                    "label": 0
                },
                {
                    "sent": "Actually people do.",
                    "label": 0
                },
                {
                    "sent": "Do different.",
                    "label": 0
                },
                {
                    "sent": "Actions, for example like answering phone or other which could be confused with which could be easily confused with the drinking.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should not.",
                    "label": 0
                },
                {
                    "sent": "So sometimes it appears that OK, so here is he's going to answer the phone.",
                    "label": 0
                },
                {
                    "sent": "And so we are quite happy this is not detected as false positive.",
                    "label": 0
                },
                {
                    "sent": "So sometimes we got detection here in the mirror and actually it was correct, but it was not annotated by the.",
                    "label": 0
                },
                {
                    "sent": "So for example here, it's also seems quite challenging because he was attempted to drink, but he did not and it was not detected as.",
                    "label": 0
                },
                {
                    "sent": "He's drinking.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 false positive.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is another false positive, which we think is quite quite understandable.",
                    "label": 0
                },
                {
                    "sent": "So it's a hand moving to the face.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from there.",
                    "label": 0
                },
                {
                    "sent": "20 most confidence detection.",
                    "label": 0
                },
                {
                    "sent": "So sorry from the from the detection of these 20 most confidence detection sorted in the confidence order and so the yellow side correct rather incorrect and so some are understandable.",
                    "label": 1
                },
                {
                    "sent": "Summer less understandable.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "So here there are two guys different guys actually checking the witch, so there's some consistency.",
                    "label": 0
                },
                {
                    "sent": "There is one guy smoking here and so this we are not clear why is it it always got very high.",
                    "label": 0
                },
                {
                    "sent": "Score Confidence Corp but still so most most of detections from these 20 are correct, and so it's obviously there is a space for improvement.",
                    "label": 0
                },
                {
                    "sent": "But we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happy with this result.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is the summary, so this is appears to be this first attempt to address human action in real movies and so action detection recognition seems possible and different challenging conditions.",
                    "label": 1
                },
                {
                    "sent": "So this is a positive result, and so something which we are not clear about.",
                    "label": 1
                },
                {
                    "sent": "Separate learning of shape in motion appears to be better than joint.",
                    "label": 0
                },
                {
                    "sent": "Maybe do it some overfitting so future work there.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of interesting things to do.",
                    "label": 0
                },
                {
                    "sent": "And one is related to what Andrew was talking in this morning, and so we need so that we were lucky to find this video that had.",
                    "label": 0
                },
                {
                    "sent": "Good example, lots of examples for for drinking and smoking, but you know on average, so the human actions will maybe appear five times or 345 times per movie.",
                    "label": 0
                },
                {
                    "sent": "So and of course to get enough training and test data.",
                    "label": 0
                },
                {
                    "sent": "Emmanuel Isation efforts would be a very exhaustive, so it's it's it's very interesting and we are considering the semi automatic action notation using movie scripts.",
                    "label": 1
                },
                {
                    "sent": "So the explicit handling of action under multiple views is interesting and combination of classification.",
                    "label": 0
                },
                {
                    "sent": "This text action classification which takes as well and so far how many minutes they left.",
                    "label": 0
                },
                {
                    "sent": "5 minutes OK.",
                    "label": 0
                },
                {
                    "sent": "So well, if I had five minutes, maybe I could actually, because this this we've done some work in this direction.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can just show some of these very briefly.",
                    "label": 0
                },
                {
                    "sent": "Right, so well, as I mentioned the currently in action recognition, the data collection is a datasets are our big program and so for example if we want to go to videos, web video search like people do for image.",
                    "label": 0
                },
                {
                    "sent": "For collecting image databases, this approach appears to be not very good because we get sometimes examples, but they're not very representative, so example we forget handshake.",
                    "label": 0
                },
                {
                    "sent": "We get something like this.",
                    "label": 0
                },
                {
                    "sent": "Or will these?",
                    "label": 0
                },
                {
                    "sent": "Which are handshakes, but they are quite marginal anyway, so we're getting quite a few examples of this.",
                    "label": 0
                },
                {
                    "sent": "And OK so we can go for static image search which works better, but still well it does.",
                    "label": 0
                },
                {
                    "sent": "It doesn't contain video, so so much it seems to be potentially much more interesting approaches to look at movies where there are lots of realistic variation of human actions and many lots of classes.",
                    "label": 0
                },
                {
                    "sent": "Many examples per class.",
                    "label": 0
                },
                {
                    "sent": "But as I said, so there are only few examples per class, sample pair, movie and my notation would be too exhausting so.",
                    "label": 0
                },
                {
                    "sent": "The one I price.",
                    "label": 0
                },
                {
                    "sent": "Subtitle to script alignments.",
                    "label": 0
                },
                {
                    "sent": "Boards from Andrew.",
                    "label": 0
                },
                {
                    "sent": "So, so there are lots of movie scripts on the net which you can just download and subtitle information.",
                    "label": 0
                },
                {
                    "sent": "It has time information so we can align it to movie script and get there if there are time localization for actions.",
                    "label": 0
                },
                {
                    "sent": "So that's the for example DNA in Casablanca movie so we can get that Rick sits down with these which happen at this time.",
                    "label": 0
                },
                {
                    "sent": "So this is quite nice.",
                    "label": 0
                },
                {
                    "sent": "And so there are some problems.",
                    "label": 0
                },
                {
                    "sent": "For example, if people do not speak for a long time, we can get lots of actions happening here, but we don't have a good time localization.",
                    "label": 0
                },
                {
                    "sent": "And there are more so so there are many good things with it, so there are realistic creations.",
                    "label": 0
                },
                {
                    "sent": "So once we did align these scripts to the movies, so we get there is no extra overhead for new action classes.",
                    "label": 0
                },
                {
                    "sent": "We can just run this in.",
                    "label": 0
                },
                {
                    "sent": "Same automatic to automatic annotation tool for new action class and get it so there are problems, so no spatial localization or temporal localization and.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So we did a bit of evaluation here actually to see what what, what happens, what actions described in text, how do they actually correspond with the movie and.",
                    "label": 0
                },
                {
                    "sent": "So I should just say that so we did subscript subtitle to script alignment and so this part of.",
                    "label": 0
                },
                {
                    "sent": "Action, so this is the number of action samples that we have tried just looking at the text and the movie and so for the actions where we did have perfect alignment, we had only about 6070% recall, and this is because some of the actions that happen that are described in the movie or so in the script actually never happened in the movies.",
                    "label": 0
                },
                {
                    "sent": "So there is a potential source for false positives just because the.",
                    "label": 0
                },
                {
                    "sent": "This action scripts they don't correspond sometimes, but not correspond to to the movie, and as far as been.",
                    "label": 0
                },
                {
                    "sent": "So it's we don't know how to how to deal with this so so it seems to be potentially noisy training set.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example of visual false positives, so this script says the big car pulls up to arm, officers get out and actually so the action was there, but the action was outside of the view of the camera views.",
                    "label": 0
                },
                {
                    "sent": "So these are just straight some potential problems with them action recognition.",
                    "label": 0
                },
                {
                    "sent": "So there are also more problems just without going to vision.",
                    "label": 0
                },
                {
                    "sent": "Just if you look at the text.",
                    "label": 0
                },
                {
                    "sent": "So get out car action for example could be expressed in very different ways and it seems that we need a text classification here too.",
                    "label": 0
                },
                {
                    "sent": "To overcome this variation and also there is a potential false positives like about to sit down, he freezes so it's so there is a part of a text processing processing here which should be addressed.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I will stop here for.",
                    "label": 0
                },
                {
                    "sent": "So many questions.",
                    "label": 0
                },
                {
                    "sent": "Degrade.",
                    "label": 0
                },
                {
                    "sent": "Having YouTube Prince.",
                    "label": 0
                },
                {
                    "sent": "OK, so the resolution we work with is.",
                    "label": 0
                },
                {
                    "sent": "About 74 for this method.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's about 70 by 17 in space and about 40 frames in in time.",
                    "label": 0
                },
                {
                    "sent": "So I think I think it's well.",
                    "label": 0
                },
                {
                    "sent": "70 by 70 it seems to should be should work.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course, if you have very small resolution YouTube so you don't get it, but.",
                    "label": 0
                },
                {
                    "sent": "For some videos it should work.",
                    "label": 0
                },
                {
                    "sent": "Another point you in in your evaluation you showed the degree of certainty or uncertainty associated with your classification.",
                    "label": 0
                },
                {
                    "sent": "Have you considered that you could use that also for use active learning ideas for to bootstrap datasets in these for the new types of actions that you were interested in?",
                    "label": 0
                },
                {
                    "sent": "Sorry, you are talking about the last part about text or both.",
                    "label": 0
                },
                {
                    "sent": "You finished up with this problem that you were talking about having training data.",
                    "label": 0
                },
                {
                    "sent": "Like you or no?",
                    "label": 0
                },
                {
                    "sent": "In your supplementary piece, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you.",
                    "label": 0
                },
                {
                    "sent": "Your problem here is that you don't have enough annotated training samples.",
                    "label": 0
                },
                {
                    "sent": "And you're talking about using texts are using this script information to produce training data exactly?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "In machine learning is to to to use an active learning strategy, OK to Bootstrap?",
                    "label": 0
                },
                {
                    "sent": "To use both vision maybe and text and two to improve.",
                    "label": 0
                },
                {
                    "sent": "Learning, but OK. OK right.",
                    "label": 0
                },
                {
                    "sent": "OK OK so of course the aim is to try to do full automatic so if we if you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're right, if you want to.",
                    "label": 0
                },
                {
                    "sent": "If we can afford to involve the human, that would be helpful.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "From the.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's post processing.",
                    "label": 0
                },
                {
                    "sent": "It's not real time, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said so this is quite exhaustive, so really we search for all different positions and different size and the different size in the image space and different extents in time.",
                    "label": 0
                },
                {
                    "sent": "Well, it's at the moment it's about 7 seconds per frame, so it's not real time, but.",
                    "label": 0
                },
                {
                    "sent": "It was possible to run it on on the movie at least.",
                    "label": 0
                },
                {
                    "sent": "Look for the questions.",
                    "label": 0
                },
                {
                    "sent": "So it's coffee break now.",
                    "label": 0
                },
                {
                    "sent": "Right now coffee break for.",
                    "label": 0
                }
            ]
        }
    }
}