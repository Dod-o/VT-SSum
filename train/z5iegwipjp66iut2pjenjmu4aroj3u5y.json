{
    "id": "z5iegwipjp66iut2pjenjmu4aroj3u5y",
    "title": "Phrase-based and factored statistical machine translation",
    "info": {
        "author": [
            "Philipp Koehn, University of Edinburgh"
        ],
        "published": "July 21, 2008",
        "recorded": "June 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/aerfaiss08_koehn_pbfs/",
    "segmentation": [
        [
            "OK, so actually giving yeah the five parts so this is kind of a general introduction to statistical machine translation.",
            "Also the issue of evaluation for machine translation, which is somewhat open issue.",
            "Then about how to you do decoding with phrase based models?",
            "How do you learn phrase based models and then?",
            "I hope we have time for that to more research.",
            "She kind of things of work that we currently do in Edinburgh on factor translation models and on discriminative training."
        ],
        [
            "So the nice thing about machine translation is it's a very easy to you understood problem, so you get things like this and you try to make sense of it.",
            "Try to figure out what does that mean.",
            "I have no idea what that means.",
            "I got that off the Internet at some point and still nobody explained it to me.",
            "So this is one of the oldest problem in artificial intelligence and artificial intelligence research came up as a topic.",
            "How can we build intelligent machines?",
            "Machine translation was considered one of the problems to tackle besides playing chess and to do really well.",
            "The general assumption is that you need to do all the hard things in artificial intelligence.",
            "You need to know better world knowledge.",
            "You need to know about semantics about what things mean, what what.",
            "What does everything mean?",
            "Because there always some translation cases where you really need to know what you're talking about.",
            "We try to do something much more simpler because we."
        ],
        [
            "I know how to do all these hard things, but we try to get as far as you want.",
            "So what we're doing is inspired by the story of the Rosetta Stone that you might have heard about.",
            "So the Egyptian language was for long time a real mystery.",
            "Nobody knew what it meant.",
            "They had all these funny symbols with eyes and camels.",
            "And what do I know and there?",
            "And there probably was some some alphabet, but nobody could encode it until in 1799 someone found in the desert a stone that had one part Egyptian, one Pod Creek Greek, and then another parts third language.",
            "I forgot which one and.",
            "That enabled them people to decode the Egyptian alphabet because you could just look at the Egyptian.",
            "You could look at the corresponding Greek.",
            "So yeah, whenever that Greek word occurs, then that Egyptian word occurs and if you do that for awhile you can really then figure out where all the words in Egyptian are."
        ],
        [
            "So what we are in the separation of right now is that we are in a similar situation where the computer can discover automatically how to translate, so we have now available to us a lot of texts in electronic form thanks to the Internet and all the other advances that computers can analyze and.",
            "So for some language pairs we have in the order of hundreds of millions of words translated.",
            "Even for like Spanish English, it's not hard to get hundreds of millions of words and to get you some idea what that is.",
            "So if you if you just buy a book in a bookstore that has maybe a few 100,000 words in it, so maybe it's a good guess that ordinary educated person reads about 10,000 words a day.",
            "So you read maybe 3 1/2 million words a year and every 3 million words in lifetime.",
            "So the computers have access to translated texts.",
            "In a volume that is comparable, then you are able to read in your lifetime and if you look at monolingual text, computers have access to much, much more data than you'll ever be able to read.",
            "So if you try to read the Internet, it'll keep you busy.",
            "I think I heard that number somewhere 30,000 years.",
            "If you read take Sundays off, that's how long it's going to take, so I mean, it's somewhat stunning that yes, we learn language, and we probably smart about learning language because we learn language in a context where.",
            "Things that pointed out to us, but I said this is, you know, a table and this is a screen.",
            "And then we kind of learn things that way.",
            "But in terms of raw language that we are confronted with, computers actually have a huge advantage of us so they can process much, much, much more text than we'll ever be able to do."
        ],
        [
            "OK, so the statistical machine translation models we're talking about I just give you a quick rundown of the basic principles.",
            "We have two different models.",
            "One is a language model as we also have in speech recognition and one is a translation model.",
            "So the language model is there sort of general story I'm telling here is you translate from a foreign language into English, so the language model is there to figure out what is good English, what is fluent English, what makes sense in English, and the translation model purposes.",
            "The more obvious you have to find out some correspondence between a foreign sentence in English sentence so you have two mathematical models, one tells you.",
            "Are these two sentences?",
            "Do they match up and another model tells you?",
            "Is the English actually good English?",
            "Because you want to have a translation that only not only adequately represents the input, you also want to have a translation that reads fluently and the output language.",
            "So given these two mathematical models, you could take any two sentences and measure is the output good English and two?",
            "That's the input and the output correspond.",
            "And so then the small problem remaining is finding the best sentence for a given input that has the highest probability in terms of translation model and the language model so.",
            "And probability world.",
            "Pretty much everything is possible.",
            "You could take any possible English sentence as a possible translation for, say, Spanish input sentence and all these models should give you some probabilities for it.",
            "So, but you can't list up all Spanish sentences, so you have to be a little bit smarter.",
            "Finding the good Spanish sentence.",
            "So I'll talk a lot about translation model and decoding.",
            "We're not going to talk."
        ],
        [
            "About language model.",
            "OK, this is another slide I want to throw out here so.",
            "So this is the.",
            "Someone's called Okwara triangle.",
            "I like the word pyramid better.",
            "So you have kind of the process, how maybe humans actually do translation.",
            "So when we translate a foreign sentence into English, we read the foreign words we know something about the foreign grandma we have.",
            "We know what the words mean.",
            "You have kind of no concept of of the meaning and ultimately in our head is the meaning of the sentence and give him the meaning of the sentence.",
            "We can then go around and find what is the right English concept.",
            "What are the right words?",
            "What is the right syntax?",
            "I have to put that in and ultimately what are the words I've tried on paper?",
            "All the models that currently exist ignore the entire top part of this pyramid.",
            "So the very simple models take the foreign words, shuffle them around and put produce English words.",
            "Arguably, once he moved to more phrase based, you kind of move up a little bit in the pyramid because you realize that some birds group together and have a meaning.",
            "That is independent of the individual words, and there are a lot of efforts currently in machine translation research, and I'm not going to talk much about it is to move further up and to use syntax and grammar to build better models.",
            "Work in statistical empty is very much driven by what works now.",
            "Can you build me a system and show me that it's better so a lot of the more difficult work is more difficult to pull off when you don't get as good result.",
            "So what most of the people do when building systems are more similar things."
        ],
        [
            "OK, this is actually a graph depicting.",
            "Yeah, an example for the original models that we proposed.",
            "A statistical machine translation.",
            "So statistical machine translation came into being in the late 80s, early 90s an by a group at IBM who said OK. Statistical models work really well for speech recognition.",
            "Let's do the same thing for machine translation and the case that we came up with a model where the input gets transformed by a number of steps into the output.",
            "I'm not going to get too much detail.",
            "You have obviously a translation step where you translate the words important thing to recognize here is that it always meant someone input word to one output word.",
            "So to overcome that problem, sometimes you have to produce the input word and duplicated multiple times.",
            "So the word slap gets multiplied, multiplied.",
            "3 words there be versus get dropped.",
            "If English words like date which don't really have obvious meaning at all.",
            "And then you also have the possibility to insert an Albert.",
            "And another important step at the very end is you have to reorder so not all languages have the same sentence or are of the same order of words in sentences and you have to deal with that.",
            "That's actually one of the hardest problems.",
            "So if you draw the comparison to speech recognition, that's the one problem that makes machine translation much, much harder than than speech recognition and speech recognition.",
            "You have a stream of words and you can write them down the same order and in terms of finding the best.",
            "Best yeah, textual rendering of speech.",
            "It's much easier.",
            "You don't have to kind of jump around 10 words and in machine translation you have to do that a lot.",
            "So Spanish English are as an example for language pay.",
            "You don't have that much reordering, so the most prominent thing is adjective.",
            "Noun skipped inverted most of the time and the but there are many other languages where you have a lot of reordering.",
            "So my personal favorite is German English since I'm native German and.",
            "There's always the verb is out of place and you also have much more nested construction.",
            "We like to put things in middle of other things, so there's much more reordering.",
            "Japanese is also another example.",
            "Germany is everything is wrong and the verb is at the end of the preposition.",
            "So you want to call them that way there after the noun phrase, and so on.",
            "So those things which is a much much harder."
        ],
        [
            "OK, the models I will talk about and I probably have this slide two or three more times.",
            "Our phrase based models and the great thing about them.",
            "They're very simple.",
            "They fit on one slide.",
            "So this is kind of, you know, anything from this talk.",
            "That's the thing I want you to know.",
            "So you take it input sentence, you chop it up into phrases.",
            "Phrases were very liberal over the word phrase, so like any sequences of words, they don't have to be linguistically motivated phrases.",
            "They have to between one and any number of words.",
            "And then you have a one to one mapping from input phrases to output phrases.",
            "And he also allowed to reorder.",
            "So you have actually two types of reordering.",
            "It happened, so there might be some reordering within a phrase, so you have a phrase that contains the actual noun on translated from Spanish into English.",
            "You might have reordering within the phrase, but you also have to order in between phrases and you can reorder them anyway you want."
        ],
        [
            "And that's really it.",
            "So I'm not going to talk much about this, but just definitely want to throw out the notion that maybe especially to deal with languages that have different syntactic structure.",
            "Maybe we want to take syntactic structure into account.",
            "So the examples I gave you with German, where the purpose out of place or you have nested construction or in Japanese for everything is kind of the other way around to know what a verb is and what the clauses.",
            "And what a noun phrases might be really useful things to know.",
            "So, um.",
            "So that's a very convincing story to say we should actually do machine translation on syntactic structures.",
            "The biggest problem is with all problems in natural language processing, the simple things work.",
            "So the simple phrase based models that don't know anything about mountain verbs and so on were pretty convincingly and to build syntax models well.",
            "You need syntactic parsers.",
            "You might even need some generation systems.",
            "You suddenly have to operate on tree structures instead of just shuffling words around, so the whole mechanics of it gets much, much harder.",
            "So the phrase based models make very little assumptions, they only make the assumptions that you know.",
            "Language is made out of string of words and one word follows another, and maybe they get reordered and syntax based models have the assumption that you can build the tree structure that you know when you know what the proper way to represent syntax in a tree.",
            "And people argue about even that.",
            "So there's just a lot of problems with that.",
            "Last two or three years there has been some promising progress, but.",
            "For many language pairs, the state of the art is still the phrase based."
        ],
        [
            "OK. As a good introduction.",
            "Um?",
            "Into the problems with machine translation, I think it's always valuable to look at evaluation.",
            "So if you tackle any task, the first thing you should answer is how do you actually know that you're doing the right thing.",
            "And so how can you figure out that you're giving the right answers?",
            "And then.",
            "So what has been developed in statistical empty over the last 10 years?",
            "Some pretty heavily used or last 10 years is automatic evaluation metrics.",
            "So how can you actually figure out if you're doing the right thing?",
            "So in speech recognition, arguably there's one correct answer.",
            "So if I say something, I don't mumble too much.",
            "There's one I wanted to say.",
            "One particular string of words, and if you figure out that string of words, you're right.",
            "And machine translation, that's not the case.",
            "If I ask you, most of you probably speak Spanish and I gave you a Spanish sentence and ask you to write down English translation.",
            "I wouldn't be too surprised if all of you came up with different translations.",
            "In many cases of corpora where we have multiple translations for the input and all of these are just basically generated by giving the same input to different translation agencies and said give me a translation and sometimes we think it's useful to have, let's say, four different translations of the same input sentence and it form for any sentence of reasonable length, like at least 5 words, it is highly unlikely that two translators come up with the same translation.",
            "So everybody is always a better translator than the next guy.",
            "So whenever someone says that's the translation, I said no really to capture the true meaning.",
            "You actually have to say it this way, so that's a real problem, because you can just say, well, that's the T output and it's a human output with the match.",
            "And if they match, that's correct.",
            "And if they don't match, it's wrong.",
            "Um?",
            "Still, we want to not do the whole thing manually, so the manual evaluation would be to give someone that the input and the output and ask him is that the right translation?",
            "Maybe?",
            "Is it the right translation on a scale from 1 to five, with five being perfect and one being completely terrible?",
            "And then.",
            "Let people judge people don't even agree on that, by the way.",
            "Um?",
            "So the history of automatic metrics is, well, we take what works in speech recognition.",
            "Let's actually general model in machine translation we take what works in speech recognition.",
            "So we look at word error rate.",
            "So we still do the same thing where we have human translation.",
            "We have our machine translation and we just look how many words match.",
            "Word.",
            "Error rate implies how many words match in the same sequence.",
            "But often things get kind of fundamentally reordered, and you don't want to get completely punished for that.",
            "So there are some adaptations for that where you allow some reordering.",
            "The metric that is in use right now, pretty popular, is actually not that old.",
            "Blue was suggested in 2002, and it's basically the same idea.",
            "You have a reference sensation.",
            "You have system output.",
            "The trick is that you not only look at how many words did you get right.",
            "Which would completely ignore reordering, but you also look at how many pairs of words and triples efforts and four grams of words you get right.",
            "And if you get forwards right in comparison with the reference sensation that is deemed to be a good thing."
        ],
        [
            "OK, how does this work?",
            "Let's say this is the reference translation here.",
            "Slightly violent.",
            "The government was shot to death by the police and he, our system translations or human translation.",
            "So whatever they are possible candidate translation.",
            "So you want to judge them by how good they are.",
            "So we use engram overlap with the reference installation.",
            "So I have a little bit color coded so if I get a four gram rider color the whole thing in green and affected by Graham right?",
            "It's blue and and if I get nothing, if it produces wrong word and put it in red.",
            "So you can kind of see that the really terrible translations, like the one that had funny subjects larger than zero symbols of course have a lot of words that are red, so these are judged very lowly.",
            "Well, of course the perfect translation has all matching programs.",
            "This it doesn't work perfectly.",
            "So the last sentence.",
            "Police killed the gunman.",
            "It misses a little bit the information that there was shooting involved so they didn't stab him anything, but it's actually put translation.",
            "But yeah, we have a single word right and then a bigram and the period at the end of that right too.",
            "So that wouldn't be Josh very highly.",
            "So it's very easy with this metric to always come with an example.",
            "We have a perfectly nice translation, but it doesn't match the source matches human reference at all, and it would be judged fairly lowly.",
            "The argument is that you don't do that for one sentence.",
            "You do that for it's 1000 sentences.",
            "And if in in a translation corpus of 1000 sentences you have much more matches with human reference, you probably better than a system that doesn't have that much measures.",
            "OK, so that's the intuitive argument, and you might believe it to believe it or not."
        ],
        [
            "What made this argument convincing that this is a good metric to evaluate machine translation graphs like this where you have automatic scores on the 1 axis and human scores?",
            "So we the human side?",
            "We did what I said earlier.",
            "You give a sentence better to person, say, actually hear you ask different questions.",
            "One is is the meaning preserved, that's called adequacy.",
            "And the other question is, is it fluent output?",
            "And that's called fluency and adequacy in fluency correlate alot.",
            "But the main point is here that also the automatic scores.",
            "So here's the NIST scores.",
            "A slight variant of the blue score.",
            "Also correlate very very well with the with the human judgment.",
            "So the system on the top right that was judged best by the humans is also judged best by the automatic metrics and on the bottom left you have the worst system according both humans and their automatic metric.",
            "So you can draw a nice line here and say you know everything kind of is on that line.",
            "You can also compute the correlation and say well with the correlation.",
            "I think what we have here.",
            "I can read the number somewhere the 80s nineties.",
            "That's a very strong correlation.",
            "And and you say, well, OK, that's one metric.",
            "Maybe I have a different metric and it's a very nice game to play.",
            "So once you have a corpus annotated with human judgments, you can figure out, well, I invent a better metric, and you can invent the better metric.",
            "You can then compute the score for all the systems, and then you can see does it correlate better with human judgments or not.",
            "So it's actually machine translation.",
            "Evaluation is a very well defined task, so we have no several corpora annotated with human judgments of sentences.",
            "How would they are available?",
            "Since they all these competitions and empty and you can download one of these different corpora and you can come up with your own metric and see does it correlate better than than the official metrics that I use."
        ],
        [
            "OK, let me say a few more things about blue.",
            "So the two cases I want to point out where things don't correlate well at all.",
            "So there's this example from 2005.",
            "So you this is the NIST evaluation of Arabic English that is kind of the most popular contest Americans really love to translate Arabic.",
            "They just very fond of Arabic, and they have every year or competition where they want people to translate Arabic and people both statistically machine systems, basically so they're all different groups participating in that.",
            "And you have then the blue scores on one end, and you have human judgment on the other.",
            "Send and you see there is one outlier.",
            "It actually correlates pretty well, except for that one funny outlier.",
            "So that was a that point a greatest student at the University of Edinburgh who read the rules and said, well, it doesn't actually say anywhere in the rules that humans can take the output of the empty system and fix it up.",
            "It was definitely not in the spirit of the game and he was very explicit about it and he admitted to it, although some people called him cheating.",
            "But but what happened with that one system is just humans looking at the output.",
            "There were, you know, English speakers in Edinburgh.",
            "They know nothing about Arabic, but they know what makes sense in English.",
            "So it's not surprisingly that these systems got a very high fluency ranking or not perfect.",
            "'cause as I said, always, someone is a better judge of what is good English and what is it.",
            "Good translation in the next guy.",
            "And it also got a very high adequate hearing because sometimes just you know what the text is about.",
            "You can fill in the missing gaps.",
            "And you can put things until you know things make much more sense.",
            "It's also true that if human serita fluent sentence, they're much more convinced that it has the right meaning, even if it actually has mistakes.",
            "So that's an interesting outlier, so this is.",
            "And system that humans really like a lot, but the Blue Square doesn't like it.",
            "OK, that's a bit of both."
        ],
        [
            "We came across another example that is when we wanted to compare our statistical T systems with the best thing out there, which is commercial software, or at least that's what we thought.",
            "So the most popular system, or commercially successful system in machine translations?",
            "For many language pairs system.",
            "It's a company in in France and then California and we translated texts on the European Parliament.",
            "So we have a huge corpus of European Parliament text so we can learn machine translation systems and we built two different machine translation systems, one good one and one bad one.",
            "It's very easy to build a bad empty system, so the trick here is to just use 3% of the training data.",
            "And of course you're not going to do as well and if you look at the two SMP systems they're kind of on that diagonal that you want, so the systems is judged better.",
            "For humans and by the automatic metric.",
            "But the rule based system here cistern gets judged really harshly worse than our bad anti system by the automatic metric.",
            "But judge pretty highly almost as higher or statistical system by humans.",
            "Um, so that's that's of course a concern.",
            "Um?",
            "So we haven't completely figured out why that is.",
            "We notice one thing that this that rule based systems are strong outliers.",
            "Much more if you test in domain.",
            "If you have a certain corpus and your system learns all the right jargon and knows all the right words while the rule based system might say the same thing, but it uses different words.",
            "And of course that gets punished and by the automatic metrics because you don't match exactly the right thing, so some mistakes that system makes sense on European Parliament are real mistakes.",
            "So when you refer to the enlargement of the European Union, sister always says the widening of the European Union.",
            "That does sound a bit funny, but many other things that perfectly good words and perfectly good translations.",
            "So we think part of the answer is that.",
            "The automatic metrics are little bit too literal.",
            "You have to get the jargon right, not just the word the the meaning, right?",
            "And it's a bit too sensitive to that.",
            "But it's actually still so out of domain.",
            "That problem shows up as well, so that's not all of the answer, but that's an interesting question.",
            "But the nice thing about this is it's a very well defined problem Now, so we have actually several corpora where rule based on statistical systems are judged by humans, and we have all the automatic scores and can come up with a metric that doesn't have these problems.",
            "Yeah.",
            "Yep.",
            "Say, will you need a better language model in your?",
            "Yeah, yeah, that's definitely.",
            "Humans are very sensitive to things being produced.",
            "Things being very fluent and that that does not only impact the fluency judgment, it also biases their adequacy judgment.",
            "More than blue is sort of over sensitive too.",
            "Fluency.",
            "Yeah, not really, and if you actually would ask this people this question to the IBM people that say, well, a different version of the blue scores in some versions are much more biased towards the adequacy.",
            "So arguably if you just look at how many words did you get right?",
            "That's more biased towards adequacy, but if you look for four grams then you're more biased towards fluency because you have to kind of produce fluent N grams.",
            "That results, I mean, it was that with with their multiple reference translations or so.",
            "This is this is a single representation here you had four reference stations, so.",
            "I see so you have multiple reference translations.",
            "I guess for more diversity you know is it is.",
            "Is there some understanding of the?",
            "I guess the statistics do you.",
            "Can you get away with a smaller test corpus if you have more reference translations for, so we often driven by necessity.",
            "So very often we only have one reference station available, so for so I've been involved in organizing European language translation competitions.",
            "And what do you do you take?",
            "The training data and say that's your test set and don't use it for training.",
            "And then you only have one reference translation for the NIST evaluation they always produce for reference translation.",
            "I heard someone from IBM claiming that if you have four reference translations, that's as good as having twice as many sentences.",
            "Um, some people who strongly believe in having multiple reference installations.",
            "I'm not fully convinced that that is so good, because, OK, the argument is if multiple reference translations, there's any variety that can have in translation, it will show up in the reference section, but that's not entirely true either, because there's so much variety in translation that you're not going to get all true 4 grams in all the available reference stations.",
            "They must have some statistics they have, yeah.",
            "It's cheaper to just make more translations, but I mean.",
            "Yeah.",
            "Rather than having more reference traffic.",
            "So yeah, sure.",
            "This result, the difference in kind of the rule based instance to Cisco Systems.",
            "I mean, wouldn't that just tell you you have to be somehow combining the two things that sound?",
            "That's one answer to and then people working on it.",
            "I'm not sure if that's the ultimate argument for it.",
            "They're definitely different strengths and weaknesses for rule based and statistical T. I'm not talking about that.",
            "Also, let me talk about that quickly.",
            "So if you compare system output with statistical empty output, things with statistical T is better is for instance word choice.",
            "If they're ambiguous words, you don't know how to translate it, right?",
            "The statistical systems actually don't have much problems with that, so one of the big problems in natural language processing, word sense, this equation, words have different meanings that translate differently.",
            "Statistical systems are really good at that.",
            "So just look at the context to see which word fits well.",
            "You know maybe the adjective in front of it is already very, very helpful, and so on.",
            "Rule based systems are generally much better in.",
            "Sentence structure maybe, but definitely in the reflection of words.",
            "If you translate into morphologically rich languages.",
            "The statistical systems sometimes just take random guesses.",
            "If something should be in particular case or another case of which verb tenses should be, well, the rule based systems are much more consistent than just look at.",
            "You know what role does this noun phrase playing comes this, not this case, or it preserves the tense of the source working and thanks guys.",
            "But yeah.",
            "OK, let's."
        ],
        [
            "No one.",
            "Still in the introduction.",
            "So, so that definitely evaluation is actually.",
            "It comes and goes in waves.",
            "It's backing away for evaluation is a hot topic.",
            "Someone claimed that maybe there more papers written about machine translation evaluation then about machine translation entirely.",
            "Believe that, but it's a very easy thing to research and their obvious problems.",
            "And ideally you want to have a metric that we can use, maybe even automatically do to optimize our systems that if we get improvement in that automatically metric, we can believe that our system is better.",
            "And that might be also be useful to compare different types of systems, so the metrics we have right now so we are aware of some of their flaws.",
            "But still we trust them enough that when we build a system and we have an improvement in Bleu score, you generally have a real improvement, and then many research papers that I've written that just show improvement in Bleu scores and don't show any improvements in translation.",
            "They don't give a, so there was a famous question.",
            "One of the conferences why you gave this long talk about machine translation.",
            "You didn't show one single example.",
            "So let know prompted some people to put examples in the paper, but you know if you translate 1000 sentences are not going to put in 1000 sentences in your paper.",
            "So if you pick one or two translation examples here, look things improved, what did you really show by that?",
            "Because you always improve few sentences, they got no matter what you do.",
            "So the end of the story is, yeah, we use it in system development.",
            "We kind of believe blue scores.",
            "We believe them too much, but we don't believe them as comparison of different types of systems.",
            "So if I beat sistren with messages go to system on blue, I'm not convinced that I really beat system.",
            "So so so.",
            "The typical situation is that you do your research on daily basis and you measure your progress in blue and then once a year there's official competition and.",
            "Then you actually have the time and money to judge all these sentences.",
            "It's extremely expensive to do this human judgment."
        ],
        [
            "OK, let me quickly go over something that you can do with statistical methods.",
            "So so you have this corpus of the European Parliament.",
            "So the European Parliament is online since 1996, or at least you can the archives go back to 1996 and every two or three years we can crawl news new stuff and they bought 3040 million words translated.",
            "So I told you earlier that in your lifetime you're going to be 300 million words.",
            "If you want to spend 10 * 10% of your lifetime reading reading the European Parliament, well that's about a bigger chunk.",
            "It is so it's actually a lot of text, so you can.",
            "You know, learn a lot from it.",
            "And then, well, it's published in 11 languages by 11 becausw in the mid 90s.",
            "There were eleven official languages.",
            "Now the 23 official languages in the European Union.",
            "But still they don't translate the European Parliament into all the 23 languages.",
            "Only in the 11 original so.",
            "So I'm told that the story with the European translation offices they spent half a billion euros each year on translation.",
            "And they translate what they can.",
            "They could.",
            "What are the translation needs in Europe?",
            "So you could if you give them 2 billion euros they would translate 4 times more less.",
            "There's no limit, so it's very hard to say.",
            "You know how big is the market for?",
            "For translating, well, people like to have all kinds of stuff translated.",
            "So that they kind of save some resources.",
            "They don't translate the Parliament proceedings into all 23 languages.",
            "OK, so you have here said 20 to 30,000,000 words, languages for them.",
            "For some languages we know about, 40,000,000 words have 110 language pairs.",
            "So you can translate built machine translation systems from Danish to English, Spanish, Portuguese and so on.",
            "And with static and T you can do that very nicely.",
            "You can actually.",
            "Take this corpus and expect 110 parallel copper out of it.",
            "Actually, technically 55 parallel corpora and then build 110 anti systems.",
            "And I did that already."
        ],
        [
            "Years ago, and these are the Bleu scores for all the different systems.",
            "Um?",
            "So it's a bit of a gimmick because I built all these systems.",
            "I never use them sitting out still sitting on a hard drive somewhere.",
            "I just recently rebuilt the whole matrix so my scores and I've got two points better, but not much has changed.",
            "It's kind of nice to see what.",
            "What are the easy languages and what I'd be hard languages.",
            "So you see, for instance, translating to finish is really hard.",
            "Also, somewhat surprising.",
            "Translate into German is very hard.",
            "And but it's really easy as French, Spanish, Spanish to Portuguese.",
            "Generally, translating into English is pretty easy.",
            "So, so that's interesting matrix.",
            "So you might have noticed that logo we have a European project for now.",
            "Yeah, year and a half old euro matrix, which is kind of inspired by this.",
            "So why don't we just built in P systems for all European languages?",
            "And we said we're going to do it for all official EU languages and we didn't completely realized at that point.",
            "That means if you do it for 23 languages, you have to build about 500 systems.",
            "But OK, we have a cluster computer in Edinburgh.",
            "It's currently building these systems.",
            "Um?",
            "As part of the project we have this online evaluation, so we have a website which might be down right now because the computer Science Department Edinburgh is moving and all the computers get disconnected.",
            "Help that's coming back up sometime soon.",
            "Where you actually have very can actually look at at least the output of all these empty systems, and for some of the language purfeerst multiple systems participating and and producing output.",
            "And you can kind of see what their performances.",
            "And if you build your own empty system, you can upload it there and it gets automatically scored and ranked across against the other months.",
            "Yeah.",
            "Computed on different corpora.",
            "I don't know, so you have a test set that is also aligned.",
            "Sentence by sentence across the 11 languages.",
            "Search Midget but each.",
            "Test set for the different languages are actually different.",
            "Yeah yeah Oh well.",
            "Well, they're different.",
            "So you have the same sentence.",
            "Translated across 11 languages and you have 2000 of those sentences, so it's it is a parallel across 11 sentences, of course.",
            "So here I'm also not entirely believe the blue scores because you compare against different references.",
            "The part of the reason why finishes harder is finished.",
            "Build gigantic words so the finish verse usually contain things like terminal and preposition, and they also have this German disease of sticking words together.",
            "So getting a birthright and finish is much harder than getting a word in English, so getting it right and finish is almost as good as getting 3 words writing in English.",
            "So getting high Bleu scores and finish this much, much harder because you are judged on how often you get a program right finished.",
            "So.",
            "It's extremely difficult questions, so to answer.",
            "So you basically have to answer the question.",
            "Is this Spanish sentence as bad as this German sentence?",
            "It's very hard to get judgment, so this is this hard to get a handle on.",
            "So yeah, so the all the all the columns.",
            "Well they have the compare against the same output language.",
            "But does it really mean that translating into German is harder than translating to English?",
            "It might not be.",
            "But in the Rose, I mean you always compare against the same output, and we're relatively certain we did them evaluation manually.",
            "If you translate from different source languages, so their blue seems to correlate.",
            "Pretty well with human judgments.",
            "You mentioned earlier.",
            "Yeah.",
            "Yep.",
            "Any?",
            "Yeah, so the big problem is German as well as word order.",
            "So typically the if you don't get the verb right in the sentence that has a lot of knock off effect of everything else we just.",
            "The language model likes to have build a sentence would often hallucinates auxiliary's left and right, so everything falls apart.",
            "If you don't have it in the right place.",
            "So that's one of the reason rich morphology is another reason.",
            "So if you look at the blue scores, finish as the richest morphology of all these languages.",
            "See hardest language translate.",
            "So yeah, those two.",
            "Otherwise German and English are very related languages in terms of vocabulary.",
            "They use a lot of the words look the same, the kind of expressions you can use in English.",
            "You can almost always welcome very often, use the same expression in German.",
            "Um?",
            "Which is not the case in do Chinese, English and so on.",
            "So yeah, I like German, basically German kind of poses.",
            "The challenge of getting syntax, writing empty.",
            "And and morphology otherwise, it's a easy language pair."
        ],
        [
            "OK, so another game make you think I did so you can cluster languages by how well the translate.",
            "So you just take the blue score and say which two systems have the highest goal scorer.",
            "Group them together so this kind of greedy clustering where you first cluster the languages together very close and then you take clusters and see where the cluster flows.",
            "And if you do that well, I think you had a tweak it a little bit.",
            "I mean all comes down to the similarity metric in audio computed, but you get the nice tree of language pairs, so this seems suggestive languages are related, like Portuguese, Spanish, French and Italian are more related than the other ones.",
            "You kind of get these nice cluster of the romantic and the Germanic languages and the two outliers.",
            "Here Greek and Finnish."
        ],
        [
            "Um?",
            "So here's the thing I said about what is what is.",
            "Yeah, I kind of already talked about this.",
            "So you see that some languages are easier to translate into then out of.",
            "So if you translate if you average all the scores of translating from German from English, but English is slightly easier but not much 2020 two 23.8.",
            "But if you look at the scores into German and into English, than German is much much harder.",
            "So 10 point Bluetooth difference.",
            "So there's some languages that if you look at the difference between the average of the system translating into this language in there isn't translating out of these languages, so you see kind of this.",
            "Tracks somewhat the richness of morphology of these languages.",
            "OK, you probably."
        ],
        [
            "I heard this story.",
            "I'm slowly getting tired of telling of it, so there's this story.",
            "Back in the days, I don't know.",
            "2030 years ago, for some reason, Americans didn't care much about Arabic.",
            "They care about Russian also.",
            "Interesting language.",
            "Also, rich morphology and the story is that they wanted to test how would the empty system is and of course being true Americans.",
            "They don't know any foreign languages, so how can they judge it?",
            "So take an English sentence.",
            "The spirit is willing but the flesh is weak.",
            "Translate into Russian and then translated back into English and they get out.",
            "The vodka is good in the meters rotten, so it's a funny story.",
            "It's not true, it's just made up.",
            "Kind of see what happens so you don't know flesh and meat and spirit and what card.",
            "Now it's plausible enough to be believable.",
            "OK, so that's kind of funny, but that's actually what journalists do if you read English language journalists talking bout empty, then only foreign language.",
            "So what they do is the same thing that take English sentence, translate into form and get back to English an if it's much different than, say you know MP is terrible.",
            "So we can actually test if that's actually a good way to assess."
        ],
        [
            "So if you basically take a system with this.",
            "So these are into English and from from English.",
            "So if you take a sentence in English, it translated into into, let's say Danish and translated back into English.",
            "You can then compare how similar to the input.",
            "How do we do this?",
            "We take the blue score like you always do and you kind of see here then which blue scores you get.",
            "Very high Bleu scores.",
            "But they don't correlate it at all with the with the individual system score.",
            "So if you look at Greek actually gets a higher back translation score then then Portuguese.",
            "But the individual systems are much worse.",
            "And you know the ultimate proof that that's a bad thing is if you build a system that does nothing, which takes all the sentences and just producing the output, you would actually get a perfect score on this metric, so."
        ],
        [
            "Don't do that.",
            "OK so but I quickly point out there's a lot of resources available.",
            "A lot of parallel corpora, so we have this Europol corpus.",
            "But you can download this now.",
            "A key community care corpus which actually now in 22 official EU languages has similar size actually.",
            "Um?",
            "So it's a nice car with in terms of volume.",
            "It's a terrible corpus in terms of content.",
            "It's all the legal documents that all the member countries had to serve had to sign, so it has a lot of Article One, Article 2 and the aforementioned, whereas blah blah blah this kind of stuff.",
            "Another corpus, this part of it made available by Rickman, the Canadian hansards.",
            "That's a popular corpus.",
            "So in the Canadian Parliament everything has to be bilingual, so everything is French and English.",
            "For Chinese and Arabic to English, you can get from the LDC over 100 million words.",
            "Um?",
            "There's also a lot of monolingual data available.",
            "If you want to Spanish English, there's also United Nation corpus.",
            "I think it's not made available in any consumable form, but you could actually go to the UN website and download documents and you get a lot of Spanish and the English translations, and that's in the order of 100 million words, so that's.",
            "That's quite a lot.",
            "Um?",
            "So in terms of monolingual data, that is much, much more available, so there's famous Giga Word corpus that is huge and gigantic until people look at the web and there's even more data.",
            "So people Google is famous for.",
            "Taking all the web that they already down on their hard drive.",
            "Simple gigantic language models in the order of trillions of words.",
            "So the latest language models are 7 gram models over 8 trillion words.",
            "Where do they do it?",
            "Because they can."
        ],
        [
            "It does help performance, so I have here a graph when you add more data the performance goes up and that is true no matter how much how much data you have.",
            "So the general experiences if you really want to improve the empty system, one thing is certain.",
            "That's going to help us if you give me more data, especially if you give me data in the domain I care about.",
            "I could draw a different curve also for language modeling data, if you could have more language modeling data can also build better systems."
        ],
        [
            "OK, since we're talking Russo resources to advertise this year, so we have been building over the last two or three years now.",
            "This open source implementation of the phrase based models, exactly what I'm going to talk about today, and it also has a web page which may or may not be up or down right now.",
            "And you can download it.",
            "You get the source code.",
            "It's a full system.",
            "You can just train everything so that got funding by yeah back in the day, T star when it wasn't project currently.",
            "Euro matrix also.",
            "That funding by DARPA basically NSF and so many universes involved.",
            "It's basically the thing kind of.",
            "If you write a paper and you don't compare it against this system, you usually get criticized.",
            "So this means to beat up again so you see a lot of paper on.",
            "This is better than Moses, and that is better than most, so that's the way you have to write a paper."
        ],
        [
            "OK, so there are competitions.",
            "Just want to mention them so the kind of most prominent is the one organized by NIST which is funded on and off by DARPA.",
            "So they do yearly competitions.",
            "They got a little bit behind schedule now and they do it in Arabic, English, Chinese, English.",
            "For that there's a lot of data available.",
            "Problem with that competition is it takes forever to training system because you're given to her at million words.",
            "So everything takes weeks instead of.",
            "Days or hours.",
            "That's the extreme opposite of that is the IW 30, which is motivated by speech translation research so that they have tiny corpora in the order of 40,000 sentences.",
            "Something like this.",
            "And there are always in a travel domain kind of very short sentences like can I pay with credit card and what show me the way to the hotel?",
            "Things like this.",
            "Their training system takes like 15 minutes and well the middle ground.",
            "I don't know is the competition we have been organizing for a number of years now in connection with the ACL on European languages.",
            "I say a little bit."
        ],
        [
            "Actually, my next slide says little, but so the last we just did that earlier this year.",
            "And we also added to the kind of.",
            "Competition a number of rule based systems just off the shelf.",
            "Commercial Systems, 6 different ones, and we ran them on the same test sets.",
            "Typically we do we take which we give people the Europol corpus.",
            "I mean then say OK, how well do you do now I'm translating Europe are so it's in domain and except for English, German, all the statistical systems there beat the rule based systems.",
            "But then we also had a tougher test set and there's many ways you can interpret that.",
            "So that's news stories.",
            "What we did is we.",
            "Literally went out and got new stories in Spanish and English and German and did the same thing.",
            "We've created corpus that is 2000 sentence is aligned across all the six languages that we cared about.",
            "And the same thing we scored, then the systems and these are based on manual judgments.",
            "Which sentence are preferred.",
            "And then we don't do so well with statistical empty French English.",
            "The best statistical system that was trained just on European Parliament text still beat all the rule based systems out there.",
            "But all the other language pairs rule based systems were better.",
            "So from a statistical point of view you can say, well, we don't have training data.",
            "We actually had no training data at all.",
            "We didn't have development data.",
            "We have nothing.",
            "We didn't give people like look here.",
            "This is how the test set might look like.",
            "We didn't even give people that.",
            "Arguably, well, that's probably the point to prove for next year, because we're going to do the same thing next year.",
            "If you have a lot of language model data in the news domain, that should help.",
            "Unfortunately, it's really hard to get parallel texts in the news domain, so there are websites like BBC and even the television show station Euronews that seems to have the same story in multiple languages, but they never translate them, though is completely rewrite it.",
            "That's why it's really annoying.",
            "Serve the audience and tell him the right thing that people care about.",
            "I don't care about us.",
            "So you can't extract just simply sentence aligned parallel corpora.",
            "So there's there's some fishing you can do, and maybe should put some effort into making use text news parallel corpora available.",
            "But there's actually no real training data.",
            "OK, so that's a challenge for next year.",
            "We have to take the right column and make some tea.",
            "Over there, so some languages plus we really do so English, German.",
            "We even on Europol get beaten by rule based system.",
            "That's embarrassing.",
            "It has a lot to do with morphology, so systems are not very good with morphology in Germany if you get the case of noun phrases, that's not something that phrase based systems are very good at because they translate.",
            "They said earlier a little phrases at a time and we don't know where you are."
        ],
        [
            "The sentence anymore.",
            "OK, this introduction took way too long, so let's talk about the real stuff here, phrase based models.",
            "I do that in a bit odd way, so I'm not going to talk much about how you train the model.",
            "How you learn the model, how you parameterized model.",
            "I'll do that later.",
            "First, take a talk about how you use it.",
            "How do you actually use the phrase based model to produce translations?",
            "I think that's more intuitive than the other stuff."
        ],
        [
            "Makes more sense.",
            "So I have this slide so you as I said two models, so we know care about the translation model.",
            "So we have now the phrase based system where we have a language model there and what we talk about right away is."
        ],
        [
            "Decoding algorithm, so this is as a reminder the model we're talking about.",
            "So we take the input sentence, chop it up into phrases when phrases could be anything, and you have some reordering also and."
        ],
        [
            "Yeah, then you pick the best station.",
            "So how does it kind of look like?",
            "In summer, typical translation table, so the German phrase Dean for schlock, you see, the top translations here.",
            "The proposal is the best translation with 62% probability.",
            "Someone's proposal is separated out, the possessive there 10%.",
            "Then you have some variation, Lexicon, idea, proposal ideas.",
            "So nothings suggestions.",
            "You also get it.",
            "So we get this from parallel corpus.",
            "So in German the.",
            "The writer still thought I have to mention the enforce lock again, but the English translator, or maybe it was actually originally in English, said well, I already talked about the proposal.",
            "People know what I'm talking about.",
            "I can use the word it.",
            "So you get things like that, so usually get at the bottom of this table.",
            "A lot of junk.",
            "'cause sometimes people translate very loosely, you make errors and aligning and so on.",
            "But the top looks pretty promising."
        ],
        [
            "So what do we do with this?",
            "So what do I do when I want to translate to Spanish sentence here?",
            "I have no idea if this is a good Spanish sentence.",
            "I took that example from Kevin Knight.",
            "I know it changed a few times, but there might be more may or may not be Spanish and translate that now into English.",
            "Let's terrorize good is now fixed."
        ],
        [
            "So how to do while I start?",
            "Well, pick one of the words that I want to translate 1st and say that's Mary."
        ],
        [
            "OK so and then say OK, I'm done with that.",
            "I'm not going to translate the word again.",
            "And then I take some other words."
        ],
        [
            "Then take some other words."
        ],
        [
            "And translate them and take some know this is kind of a nice little Fraser translate them into one word so you can translate them anyway."
        ],
        [
            "Listen to one.",
            "He also many worsen."
        ],
        [
            "One, and you can also do reordering, so when you do reordering it just take the source material out of out of order so you build the English sentence always in order, but you can take the source words out of order.",
            "That's how you implicitly then do reordering."
        ],
        [
            "And you know you done many translated all the words."
        ],
        [
            "OK, so what's?",
            "What is the computer going to do?",
            "Well, so all these choices.",
            "Um?",
            "And so as each phrase might have multiple translation, you also don't know if you should translate 3 border time or should translate the three words separately.",
            "And you also can take them at any point, so you can reorder so you can start in the middle."
        ],
        [
            "So lot of choices here.",
            "So how do we do this?",
            "We start with.",
            "Yeah, the same starting point at the beginning of the sentence.",
            "I've nothing translated so far.",
            "So that we call these things hypothesis empty hypothesis.",
            "So I indicate here E has nothing translated for F. No foreign words are covered and the probability at the beginning is 1, but haven't done so."
        ],
        [
            "Nothing so far.",
            "And then basically the action of expanding this hypothesis of translating one phrase.",
            "That means you take that phrase, you put the translation at the end of your English.",
            "You have to mark off in your little bit vector there that you have the first word already translated.",
            "Unicorn translated again, and there are all kinds of probabilities that get multiplied in."
        ],
        [
            "Just a quick word to the probabilities we care about, so they obviously phrase translation probability of certain probability that Amari is a translation of Maria.",
            "There also some reordering costs.",
            "I gotta talk about that much more detail, but all these things are.",
            "You know, we also feature how many phrases do they translate so far?",
            "I mean words to translate so far.",
            "Another important thing is you have a language model, so all the thing has to make sense in English, so it has to be fluent English.",
            "So language model is ideally a mathematical model.",
            "You give it a sentence and the language model tells you is it a good sentence.",
            "So has a high language model.",
            "Scores are bad sentence.",
            "The language model score.",
            "The methods we use actually very simple.",
            "OK, I shouldn't say things are simpler types.",
            "OK, so it works fairly straightforward though.",
            "OK, how do you do so?",
            "You basically predicted word at a time.",
            "You say?",
            "How likely does a sentence start with the Mary and then given that the sentence started with the word Mary, how likely is the next word did an you can kind of see how could you possibly know these things?",
            "Well you go over billions of trillions of words of English and see of all the billions of trillions of words and all these sentences.",
            "How many sentences did actually start with Mary?",
            "And how many sentences start with Mary did and how many sentences that have Mary did or not then followed by not so you can come have collect these status."
        ],
        [
            "Thanks.",
            "OK, back to our story here.",
            "So there's no reason why I should have started with Mary and OK, I started with Mary because I knew that was the right thing, but the computer doesn't know that, so the computer could also start with broker and translate that into which, so you have to get some hit for reordering because you jump 8 words.",
            "That's pretty bad usually.",
            "But you know, translation models thinks it's right.",
            "Do more English sentence.",
            "Start with words which then with the word Mary."
        ],
        [
            "Channel OK, so you keep doing this, so the hypothesis you expand that you just expand them."
        ],
        [
            "Ever.",
            "And if you do this all the time, then eventually you gonna reach the end.",
            "You know you reached the end, because if all the foreign words covered.",
            "And then you can kind of go back and read off the sentence that you translate it.",
            "So you're married did not slap the Greenway Trail.",
            "We got that right."
        ],
        [
            "Miracle, of course.",
            "This story is more complicated.",
            "Because at any point you have a lot of choices at the beginning you have, we can take any of the translation options that grew up there.",
            "Typically we have up to 20 cuttitta 20 or so.",
            "Maybe sometimes you gotta 100.",
            "So 2100 translation for each phrase.",
            "Well, we probably have translation for the single words.",
            "Often we have translation many of bigrams trigrams.",
            "We still have a lot of translation, so a lot of choices.",
            "Easily for sentence length of 200 or so choices.",
            "OK, so if you wanna try the beginning then you make that choice and have almost all the same number of choices left.",
            "You can translate the same words again, but almost everything else you can still do, so that explores pretty badly.",
            "So if you just draw it up this way, you actually create an exponential search graph.",
            "An exponential sounds bad."
        ],
        [
            "So most of your computer science students, so you'll know when I say the decoding has been proven to be NP complete.",
            "That means it's just not tractable and you can't can't possibly find, search or list up or possible translations.",
            "So we're going to do two different things here.",
            "So we do first hypothesis recombination.",
            "That's a risk free thing.",
            "So that means if we do this, we're still guaranteed to find the best translation.",
            "Let me realize well it's NP complete, so that's not going to save us, so we're going to do something more risky where we just do pruning.",
            "We just kind of basically throw out kind of the bad hypothesis early on without being entirely sure that they are."
        ],
        [
            "OK, I positive combination.",
            "So the argument is the following.",
            "Let's say you can start the sentence with translating Marianne.",
            "Then you translate 2 words at a time into it did not give.",
            "And that gives you certain probability, but it could also have sort of Amari, but then translate the next word with did not.",
            "And then the third word with give.",
            "And you end up with almost the same thing.",
            "I mean, it's you have this.",
            "You have basically two different hypothesis that have the same foreign words translated.",
            "That produced the same English words.",
            "And the argument now is, well, one has a lower probability than the other, and then one over the lower probability.",
            "I can safely throw away.",
            "And why can't I do that?",
            "Well, the argument is set.",
            "Any continuation I have from there I could also do on the better hypothesis an always going to have the better probability and the better hypothesis.",
            "So the second the lower of the two high positives could not be part of the best path."
        ],
        [
            "So what do we do?",
            "We throw it away.",
            "Sometimes we keep a little bit back pointer.",
            "Important for now."
        ],
        [
            "Why we do that?",
            "OK.",
            "So that should reduce the search space a bit.",
            "OK, now I'm going to make the argument.",
            "If you have a second sentence here.",
            "Here Joe did not give for some reason.",
            "I translated Maria and Joe.",
            "But then I produce gift.",
            "What do I have there?",
            "Well, I also have a hypothesis that has the same first 3 words translated.",
            "The two agree in the last three English words, but don't agree in the first English word.",
            "But the argument is even there, I can throw away that hypothesis.",
            "So you have to keep in mind what happens with scoring when you add more praise translations, you phrase translation probabilities well, they don't care about what you did previously, but you also have a language model.",
            "So language model still cares about the last words you produce, so you can just pick the best translation of the first 3 words, because whatever we produce influences how likely the next words follow.",
            "But if you use the trigram language model or even programming language model in this case, it's not going to care about what what happened at the beginning of the sentence here for words earlier.",
            "So the same argument.",
            "Any continuation of that upper hypothesis.",
            "You can also continue exactly the same way at the lower hypothesis, and you get the same scores in both cases, so the starting point is worse.",
            "The first one couldn't be part of the best."
        ],
        [
            "Translation OK, so then you combine these paths too."
        ],
        [
            "Um?",
            "OK, so I said the whole thing is NP complete, so this helps it actually.",
            "Um cleans up the search space A lot.",
            "It's really useful thing to do, but it's not sufficient, so you still have NP complete problem that goes out of hand.",
            "So we now have to kind of bite the bullet and do some drastic action so we have to look at some of the hypothesis and say so some of them have to go.",
            "So the way we do this, we organize hypothesis into stacks.",
            "Which are technically not stacks.",
            "There's really like bins, and you could organize some different ways.",
            "So typical thing we do it, I've implemented these things is I just put things into stacks that have the same number of foreign words translated.",
            "So these are all the translations that have three foreign words translated so that someone can parable.",
            "You could also organize steps until how have they actually translated the same foreign words.",
            "The only problem with that there is exponential.",
            "A number of possible ways to cover input sentence, so you can't do that without taking some other shortcuts.",
            "And then you do pruning and then we just do two different ways.",
            "So histogram pruning implies, well, you just look at the stack and you only keep the 100 best an threshold pruning means that, well, you know what the best hypothesis, taxes and you don't want to have a positive mistake that are much worse than that.",
            "So you have some scaling factor here maybe 0.01 and say everything that is worst by that factor gets thrown out."
        ],
        [
            "So the nice thing with that stack decoding it kind of makes it very nice.",
            "Then explain the decoding algorithm in different way.",
            "See if all your hypothesis organizing your stacks.",
            "So the first step is all the hypothesis that one word translated in the second step for all the hypothesis have two words translated and decoding kind of means while we go through all the hypothesis in the stack, apply a phrase translation and then well either you translate one foreign words then go to the next step where you translate to form words.",
            "Then you go.",
            "2 steps further down you kind of fill up the steps further down, and once you're done with all the hypothesis in one stack, he can move to the next step.",
            "So it's very kind of nice algorithm.",
            "So should all be able to implement that now."
        ],
        [
            "OK, that's one trick and one problem.",
            "If you just look at hypothesis in their scores.",
            "Look at this example so you could translate.",
            "You could do the right thing and start at the beginning of the sentence and translate Maria not into Mary.",
            "Did not.",
            "The translation model will probably say, yeah, that doesn't look too bad.",
            "Reordering model is very happy because you didn't do any reordering and the language model says, well, Mary is a bit of a advert but did not just kind of OK.",
            "But he could also just jump to somewhere in the middle of the sentence and translate.",
            "Allah is the.",
            "So the gain, same story.",
            "The translation model will say, well, that's a good translation for the language model.",
            "Say that's fantastic.",
            "I like sentences.",
            "Start with the maybe 10% of all English sentence.",
            "Start with that.",
            "That's the best way to start a sentence.",
            "The reorder model is not going to be happy that he jumped 8 words, but it's basically a balance of all these things and typically the probability you incur by jumping Edwards and then producing the word there is going to be much, much better than translating.",
            "Mary did not, so it would come down to those two hypothesis and I would have to throw away the verse hypothesis.",
            "Well, I would throw away the Mary did not hypothesis so.",
            "You could say the second one, just you know it takes the easy part of the sentence first.",
            "And it will at the end of the day, will go back to marry and have to marry and still have to translate Maria.",
            "So we shouldn't only consider the probabilities we factored in so far.",
            "We should also consider well how hard is it.",
            "Translate the rest of the sentence so this is what we call a future cost estimation.",
            "So we basically want to figure out how expensive it is to translate the rest of the symptoms.",
            "Well, it's a bit of a true problem.",
            "Of course, if you knew how expensive it is to translate the rest of the sentence, well, we would just do that so you can perfectly know how expensive it will be to translate the rest of the sentence.",
            "But you could get some.",
            "Good estimate of it maybe?"
        ],
        [
            "What is useful?",
            "So how do we do this for each translation option where you translator input phrase to the output phrase, you can always know well you know the translation costs, so that's in the translation table and you don't know the language model costs, and that's where it gets tricky, so you have to have some estimate how is that language model going to like to the well of course depends on where in the sentence you put together.",
            "So we do something simple here.",
            "We just say, well, how like these were two in English and how like users with the follow the word 2 and that's the two things.",
            "So we have a simpler language model score there.",
            "So here we kind of doing doing some guesswork."
        ],
        [
            "OK, so each input phrase has money based.",
            "Different translations will ultimately we're optimistic we're going to choose the best translation, so we only care about the Hyest were the best cost."
        ],
        [
            "Mission.",
            "And then.",
            "So you can compute them for each input span.",
            "What is the best way to cover that with the translation option?",
            "And it doesn't take much more to then say, well, those three words first 3 words.",
            "Well, there's no translation option that covers the first 2 words, but there are.",
            "I can I know.",
            "What are the best ways to translate the first word in the mobile space way?",
            "Translate the second word and what's the best way to translate the third word?",
            "And also, what's the best way to translate the 2nd and 3rd work together and just based on these costs, I can figure out well what's?",
            "A reasonable estimate, how expensive it will be to cite the first 3 words so you can actually precompute for all contiguous spans.",
            "And they only N squared spans, so it's not too much.",
            "You can compute come some estimate.",
            "How expensive will it be to translate that part of the sentence?",
            "So the nice thing is, yeah, you can do that before you do decoding, so you can actually have a look up table then says OK, this here I still have to translate word 6 to 7.",
            "How expensive will that be?"
        ],
        [
            "You can just look it up.",
            "So what we ultimately do when we we compare hypothesis while we do what we did earlier, we created this hypothesis and have all the probabilities and all the other scores we multiplied up in there.",
            "And then we have also the future cost estimate.",
            "So we here in this case we still have to translate the word no.",
            "And the last four words and we can look up how expensive will there likely be.",
            "Energised combine all these together and this product of the costs of our times, the.",
            "The future cost estimate is the basis of the pruning.",
            "So based on this score we throw out the good or bad hypothesis so that overcomes the problem I had earlier where if you translate the easy part of the sentence first, well the future cost estimator says, well, the rest is still ICS."
        ],
        [
            "OK um.",
            "This is related to technique called a star, which is very popular in AI.",
            "So just wanna reference with certain.",
            "There's actually a version of this decoding that is safe.",
            "Where you actually don't make any search errors.",
            "But again, that means it's ultimately NP complete.",
            "So the trick is that the future cost estimator, if that is either accurate or underestimate you can and you only throw away hypothesis that are worse according to this estimate, then some known solution of the sentence.",
            "So you basically do a best first search, you just try to find one translation.",
            "So you have one score and then you can compare all the other hypothesis against that score.",
            "You can basically then compare for any other hypothesis its probability.",
            "Times future cost estimation is that worse than that known best translation.",
            "Then you can throw it away because the future cost estimation is either accurate or under estimate.",
            "So if that's already higher than a known solution then you can throw it out.",
            "So this is typically not done in decoders nowadays.",
            "Better to have a heuristic that is not guaranteed to be accurate or under some it, but it's an estimate that is realistic in some way.",
            "But of course you make mistakes there, so there is the issue there."
        ],
        [
            "Your search error.",
            "OK, we do a couple of other things to speed up decoding.",
            "So one thing is you can limit reordering.",
            "Reordering is actually the piece that makes the whole thing NP complete.",
            "If you wouldn't have reordering you would have the same situation with speech recognition, make and run Hmm's and you have a polynomial runtime and everything is beautiful.",
            "So you could do the same thing and say, well, there is no reordering, so you just do monotone translation.",
            "That's usually not a good idea.",
            "I usually get worse performance with that, but what we do do is we limit reordering you only say well you can only jump around.",
            "Let's say at most 6 words at most 8 words.",
            "Something like this.",
            "And that already cuts down the NP complete problem into a polynomial problem.",
            "Um?",
            "It's usually also helps with translation quality because the models are not very good at long distance movement anyway.",
            "We could get into detail for that later, so we usually do better if we don't allow too much reordering.",
            "'cause if you allow too much reordering, too much crazy stuff happens.",
            "Um?",
            "Yeah, that's all there is.",
            "If you have a fixed step size and you limit reordering to a certain reordering window, then the decoding algorithm that I explained you is actually linear in the length of the sentence.",
            "You can.",
            "It's kind of intuitive.",
            "You have always, you know, no matter how long the sentence is, hoops during decoding, you always have a stack full of let's say 100 hypothesis.",
            "You have almost always the same number of translation hypothesis that are translation options that apply, and that doesn't change.",
            "Distance gets longer and longer."
        ],
        [
            "OK, few more things.",
            "When do we actually have a break by the way?",
            "OK. OK, then I will finish up 1 two more slides.",
            "So remember when I drew this graph and said, well, we still want to keep these pointers.",
            "Well, the nice thing with that is that we can convert the search graph you produced into phrase lettuce or word lattice of possible translations.",
            "Why do we want to do that?"
        ],
        [
            "Well, we can produce things like this here.",
            "The investors.",
            "We cannot not only produce the best translation, but also the second best third best, 4th best and so on.",
            "Any number.",
            "Abuse at various for various purposes.",
            "One solution, one way you could."
        ],
        [
            "For instance, say that, well, you have.",
            "The translation model improves, then best list and then you have all kinds of additional knowledge you want to throw in.",
            "Maybe you want to Paris all the possible translations and then you use the power scores.",
            "Additional feature to figure out what are the good trans."
        ],
        [
            "Very bad translations.",
            "So this is produces with a made up translation model, but a real language model.",
            "The top translations for a German sentence and the first sentence of the right translation.",
            "I guess this is a small house.",
            "Or is it?",
            "This is a little house.",
            "I don't know the difference between small and little.",
            "Seems to mean the same thing, so these are good translations and you kind of go down the list and the last one is.",
            "This house is a small that sounds a bit funny.",
            "So so this and you see kind of how the scores kind of go down.",
            "See the yeah reordering score so you only have a reordering score.",
            "If there is reordering the first sentence now reordering you see the language model scores.",
            "The language model doesn't like that last sentence either likes it better than it's a little house.",
            "Yeah, translation model and word penalty.",
            "That's just concert honey words."
        ],
        [
            "OK, I think that's a good place.",
            "Good time to stop them."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so actually giving yeah the five parts so this is kind of a general introduction to statistical machine translation.",
                    "label": 1
                },
                {
                    "sent": "Also the issue of evaluation for machine translation, which is somewhat open issue.",
                    "label": 0
                },
                {
                    "sent": "Then about how to you do decoding with phrase based models?",
                    "label": 0
                },
                {
                    "sent": "How do you learn phrase based models and then?",
                    "label": 0
                },
                {
                    "sent": "I hope we have time for that to more research.",
                    "label": 0
                },
                {
                    "sent": "She kind of things of work that we currently do in Edinburgh on factor translation models and on discriminative training.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the nice thing about machine translation is it's a very easy to you understood problem, so you get things like this and you try to make sense of it.",
                    "label": 1
                },
                {
                    "sent": "Try to figure out what does that mean.",
                    "label": 0
                },
                {
                    "sent": "I have no idea what that means.",
                    "label": 0
                },
                {
                    "sent": "I got that off the Internet at some point and still nobody explained it to me.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the oldest problem in artificial intelligence and artificial intelligence research came up as a topic.",
                    "label": 1
                },
                {
                    "sent": "How can we build intelligent machines?",
                    "label": 0
                },
                {
                    "sent": "Machine translation was considered one of the problems to tackle besides playing chess and to do really well.",
                    "label": 0
                },
                {
                    "sent": "The general assumption is that you need to do all the hard things in artificial intelligence.",
                    "label": 1
                },
                {
                    "sent": "You need to know better world knowledge.",
                    "label": 0
                },
                {
                    "sent": "You need to know about semantics about what things mean, what what.",
                    "label": 0
                },
                {
                    "sent": "What does everything mean?",
                    "label": 0
                },
                {
                    "sent": "Because there always some translation cases where you really need to know what you're talking about.",
                    "label": 0
                },
                {
                    "sent": "We try to do something much more simpler because we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know how to do all these hard things, but we try to get as far as you want.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is inspired by the story of the Rosetta Stone that you might have heard about.",
                    "label": 1
                },
                {
                    "sent": "So the Egyptian language was for long time a real mystery.",
                    "label": 1
                },
                {
                    "sent": "Nobody knew what it meant.",
                    "label": 0
                },
                {
                    "sent": "They had all these funny symbols with eyes and camels.",
                    "label": 0
                },
                {
                    "sent": "And what do I know and there?",
                    "label": 0
                },
                {
                    "sent": "And there probably was some some alphabet, but nobody could encode it until in 1799 someone found in the desert a stone that had one part Egyptian, one Pod Creek Greek, and then another parts third language.",
                    "label": 0
                },
                {
                    "sent": "I forgot which one and.",
                    "label": 0
                },
                {
                    "sent": "That enabled them people to decode the Egyptian alphabet because you could just look at the Egyptian.",
                    "label": 0
                },
                {
                    "sent": "You could look at the corresponding Greek.",
                    "label": 0
                },
                {
                    "sent": "So yeah, whenever that Greek word occurs, then that Egyptian word occurs and if you do that for awhile you can really then figure out where all the words in Egyptian are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we are in the separation of right now is that we are in a similar situation where the computer can discover automatically how to translate, so we have now available to us a lot of texts in electronic form thanks to the Internet and all the other advances that computers can analyze and.",
                    "label": 0
                },
                {
                    "sent": "So for some language pairs we have in the order of hundreds of millions of words translated.",
                    "label": 1
                },
                {
                    "sent": "Even for like Spanish English, it's not hard to get hundreds of millions of words and to get you some idea what that is.",
                    "label": 1
                },
                {
                    "sent": "So if you if you just buy a book in a bookstore that has maybe a few 100,000 words in it, so maybe it's a good guess that ordinary educated person reads about 10,000 words a day.",
                    "label": 1
                },
                {
                    "sent": "So you read maybe 3 1/2 million words a year and every 3 million words in lifetime.",
                    "label": 0
                },
                {
                    "sent": "So the computers have access to translated texts.",
                    "label": 0
                },
                {
                    "sent": "In a volume that is comparable, then you are able to read in your lifetime and if you look at monolingual text, computers have access to much, much more data than you'll ever be able to read.",
                    "label": 0
                },
                {
                    "sent": "So if you try to read the Internet, it'll keep you busy.",
                    "label": 0
                },
                {
                    "sent": "I think I heard that number somewhere 30,000 years.",
                    "label": 0
                },
                {
                    "sent": "If you read take Sundays off, that's how long it's going to take, so I mean, it's somewhat stunning that yes, we learn language, and we probably smart about learning language because we learn language in a context where.",
                    "label": 0
                },
                {
                    "sent": "Things that pointed out to us, but I said this is, you know, a table and this is a screen.",
                    "label": 0
                },
                {
                    "sent": "And then we kind of learn things that way.",
                    "label": 0
                },
                {
                    "sent": "But in terms of raw language that we are confronted with, computers actually have a huge advantage of us so they can process much, much, much more text than we'll ever be able to do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the statistical machine translation models we're talking about I just give you a quick rundown of the basic principles.",
                    "label": 1
                },
                {
                    "sent": "We have two different models.",
                    "label": 0
                },
                {
                    "sent": "One is a language model as we also have in speech recognition and one is a translation model.",
                    "label": 1
                },
                {
                    "sent": "So the language model is there sort of general story I'm telling here is you translate from a foreign language into English, so the language model is there to figure out what is good English, what is fluent English, what makes sense in English, and the translation model purposes.",
                    "label": 0
                },
                {
                    "sent": "The more obvious you have to find out some correspondence between a foreign sentence in English sentence so you have two mathematical models, one tells you.",
                    "label": 0
                },
                {
                    "sent": "Are these two sentences?",
                    "label": 0
                },
                {
                    "sent": "Do they match up and another model tells you?",
                    "label": 0
                },
                {
                    "sent": "Is the English actually good English?",
                    "label": 0
                },
                {
                    "sent": "Because you want to have a translation that only not only adequately represents the input, you also want to have a translation that reads fluently and the output language.",
                    "label": 0
                },
                {
                    "sent": "So given these two mathematical models, you could take any two sentences and measure is the output good English and two?",
                    "label": 0
                },
                {
                    "sent": "That's the input and the output correspond.",
                    "label": 0
                },
                {
                    "sent": "And so then the small problem remaining is finding the best sentence for a given input that has the highest probability in terms of translation model and the language model so.",
                    "label": 0
                },
                {
                    "sent": "And probability world.",
                    "label": 0
                },
                {
                    "sent": "Pretty much everything is possible.",
                    "label": 0
                },
                {
                    "sent": "You could take any possible English sentence as a possible translation for, say, Spanish input sentence and all these models should give you some probabilities for it.",
                    "label": 0
                },
                {
                    "sent": "So, but you can't list up all Spanish sentences, so you have to be a little bit smarter.",
                    "label": 1
                },
                {
                    "sent": "Finding the good Spanish sentence.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk a lot about translation model and decoding.",
                    "label": 0
                },
                {
                    "sent": "We're not going to talk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About language model.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another slide I want to throw out here so.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Someone's called Okwara triangle.",
                    "label": 0
                },
                {
                    "sent": "I like the word pyramid better.",
                    "label": 0
                },
                {
                    "sent": "So you have kind of the process, how maybe humans actually do translation.",
                    "label": 0
                },
                {
                    "sent": "So when we translate a foreign sentence into English, we read the foreign words we know something about the foreign grandma we have.",
                    "label": 0
                },
                {
                    "sent": "We know what the words mean.",
                    "label": 0
                },
                {
                    "sent": "You have kind of no concept of of the meaning and ultimately in our head is the meaning of the sentence and give him the meaning of the sentence.",
                    "label": 0
                },
                {
                    "sent": "We can then go around and find what is the right English concept.",
                    "label": 0
                },
                {
                    "sent": "What are the right words?",
                    "label": 0
                },
                {
                    "sent": "What is the right syntax?",
                    "label": 0
                },
                {
                    "sent": "I have to put that in and ultimately what are the words I've tried on paper?",
                    "label": 0
                },
                {
                    "sent": "All the models that currently exist ignore the entire top part of this pyramid.",
                    "label": 0
                },
                {
                    "sent": "So the very simple models take the foreign words, shuffle them around and put produce English words.",
                    "label": 1
                },
                {
                    "sent": "Arguably, once he moved to more phrase based, you kind of move up a little bit in the pyramid because you realize that some birds group together and have a meaning.",
                    "label": 0
                },
                {
                    "sent": "That is independent of the individual words, and there are a lot of efforts currently in machine translation research, and I'm not going to talk much about it is to move further up and to use syntax and grammar to build better models.",
                    "label": 0
                },
                {
                    "sent": "Work in statistical empty is very much driven by what works now.",
                    "label": 0
                },
                {
                    "sent": "Can you build me a system and show me that it's better so a lot of the more difficult work is more difficult to pull off when you don't get as good result.",
                    "label": 0
                },
                {
                    "sent": "So what most of the people do when building systems are more similar things.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is actually a graph depicting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, an example for the original models that we proposed.",
                    "label": 0
                },
                {
                    "sent": "A statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "So statistical machine translation came into being in the late 80s, early 90s an by a group at IBM who said OK. Statistical models work really well for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "Let's do the same thing for machine translation and the case that we came up with a model where the input gets transformed by a number of steps into the output.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get too much detail.",
                    "label": 0
                },
                {
                    "sent": "You have obviously a translation step where you translate the words important thing to recognize here is that it always meant someone input word to one output word.",
                    "label": 0
                },
                {
                    "sent": "So to overcome that problem, sometimes you have to produce the input word and duplicated multiple times.",
                    "label": 0
                },
                {
                    "sent": "So the word slap gets multiplied, multiplied.",
                    "label": 0
                },
                {
                    "sent": "3 words there be versus get dropped.",
                    "label": 0
                },
                {
                    "sent": "If English words like date which don't really have obvious meaning at all.",
                    "label": 0
                },
                {
                    "sent": "And then you also have the possibility to insert an Albert.",
                    "label": 0
                },
                {
                    "sent": "And another important step at the very end is you have to reorder so not all languages have the same sentence or are of the same order of words in sentences and you have to deal with that.",
                    "label": 0
                },
                {
                    "sent": "That's actually one of the hardest problems.",
                    "label": 0
                },
                {
                    "sent": "So if you draw the comparison to speech recognition, that's the one problem that makes machine translation much, much harder than than speech recognition and speech recognition.",
                    "label": 0
                },
                {
                    "sent": "You have a stream of words and you can write them down the same order and in terms of finding the best.",
                    "label": 0
                },
                {
                    "sent": "Best yeah, textual rendering of speech.",
                    "label": 0
                },
                {
                    "sent": "It's much easier.",
                    "label": 0
                },
                {
                    "sent": "You don't have to kind of jump around 10 words and in machine translation you have to do that a lot.",
                    "label": 0
                },
                {
                    "sent": "So Spanish English are as an example for language pay.",
                    "label": 0
                },
                {
                    "sent": "You don't have that much reordering, so the most prominent thing is adjective.",
                    "label": 0
                },
                {
                    "sent": "Noun skipped inverted most of the time and the but there are many other languages where you have a lot of reordering.",
                    "label": 0
                },
                {
                    "sent": "So my personal favorite is German English since I'm native German and.",
                    "label": 0
                },
                {
                    "sent": "There's always the verb is out of place and you also have much more nested construction.",
                    "label": 0
                },
                {
                    "sent": "We like to put things in middle of other things, so there's much more reordering.",
                    "label": 0
                },
                {
                    "sent": "Japanese is also another example.",
                    "label": 0
                },
                {
                    "sent": "Germany is everything is wrong and the verb is at the end of the preposition.",
                    "label": 0
                },
                {
                    "sent": "So you want to call them that way there after the noun phrase, and so on.",
                    "label": 0
                },
                {
                    "sent": "So those things which is a much much harder.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the models I will talk about and I probably have this slide two or three more times.",
                    "label": 1
                },
                {
                    "sent": "Our phrase based models and the great thing about them.",
                    "label": 0
                },
                {
                    "sent": "They're very simple.",
                    "label": 0
                },
                {
                    "sent": "They fit on one slide.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of, you know, anything from this talk.",
                    "label": 0
                },
                {
                    "sent": "That's the thing I want you to know.",
                    "label": 0
                },
                {
                    "sent": "So you take it input sentence, you chop it up into phrases.",
                    "label": 0
                },
                {
                    "sent": "Phrases were very liberal over the word phrase, so like any sequences of words, they don't have to be linguistically motivated phrases.",
                    "label": 1
                },
                {
                    "sent": "They have to between one and any number of words.",
                    "label": 0
                },
                {
                    "sent": "And then you have a one to one mapping from input phrases to output phrases.",
                    "label": 0
                },
                {
                    "sent": "And he also allowed to reorder.",
                    "label": 0
                },
                {
                    "sent": "So you have actually two types of reordering.",
                    "label": 0
                },
                {
                    "sent": "It happened, so there might be some reordering within a phrase, so you have a phrase that contains the actual noun on translated from Spanish into English.",
                    "label": 0
                },
                {
                    "sent": "You might have reordering within the phrase, but you also have to order in between phrases and you can reorder them anyway you want.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's really it.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to talk much about this, but just definitely want to throw out the notion that maybe especially to deal with languages that have different syntactic structure.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to take syntactic structure into account.",
                    "label": 0
                },
                {
                    "sent": "So the examples I gave you with German, where the purpose out of place or you have nested construction or in Japanese for everything is kind of the other way around to know what a verb is and what the clauses.",
                    "label": 0
                },
                {
                    "sent": "And what a noun phrases might be really useful things to know.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So that's a very convincing story to say we should actually do machine translation on syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "The biggest problem is with all problems in natural language processing, the simple things work.",
                    "label": 0
                },
                {
                    "sent": "So the simple phrase based models that don't know anything about mountain verbs and so on were pretty convincingly and to build syntax models well.",
                    "label": 0
                },
                {
                    "sent": "You need syntactic parsers.",
                    "label": 0
                },
                {
                    "sent": "You might even need some generation systems.",
                    "label": 0
                },
                {
                    "sent": "You suddenly have to operate on tree structures instead of just shuffling words around, so the whole mechanics of it gets much, much harder.",
                    "label": 0
                },
                {
                    "sent": "So the phrase based models make very little assumptions, they only make the assumptions that you know.",
                    "label": 0
                },
                {
                    "sent": "Language is made out of string of words and one word follows another, and maybe they get reordered and syntax based models have the assumption that you can build the tree structure that you know when you know what the proper way to represent syntax in a tree.",
                    "label": 0
                },
                {
                    "sent": "And people argue about even that.",
                    "label": 0
                },
                {
                    "sent": "So there's just a lot of problems with that.",
                    "label": 0
                },
                {
                    "sent": "Last two or three years there has been some promising progress, but.",
                    "label": 0
                },
                {
                    "sent": "For many language pairs, the state of the art is still the phrase based.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. As a good introduction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Into the problems with machine translation, I think it's always valuable to look at evaluation.",
                    "label": 0
                },
                {
                    "sent": "So if you tackle any task, the first thing you should answer is how do you actually know that you're doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "And so how can you figure out that you're giving the right answers?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So what has been developed in statistical empty over the last 10 years?",
                    "label": 0
                },
                {
                    "sent": "Some pretty heavily used or last 10 years is automatic evaluation metrics.",
                    "label": 1
                },
                {
                    "sent": "So how can you actually figure out if you're doing the right thing?",
                    "label": 0
                },
                {
                    "sent": "So in speech recognition, arguably there's one correct answer.",
                    "label": 0
                },
                {
                    "sent": "So if I say something, I don't mumble too much.",
                    "label": 0
                },
                {
                    "sent": "There's one I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "One particular string of words, and if you figure out that string of words, you're right.",
                    "label": 1
                },
                {
                    "sent": "And machine translation, that's not the case.",
                    "label": 0
                },
                {
                    "sent": "If I ask you, most of you probably speak Spanish and I gave you a Spanish sentence and ask you to write down English translation.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't be too surprised if all of you came up with different translations.",
                    "label": 0
                },
                {
                    "sent": "In many cases of corpora where we have multiple translations for the input and all of these are just basically generated by giving the same input to different translation agencies and said give me a translation and sometimes we think it's useful to have, let's say, four different translations of the same input sentence and it form for any sentence of reasonable length, like at least 5 words, it is highly unlikely that two translators come up with the same translation.",
                    "label": 0
                },
                {
                    "sent": "So everybody is always a better translator than the next guy.",
                    "label": 0
                },
                {
                    "sent": "So whenever someone says that's the translation, I said no really to capture the true meaning.",
                    "label": 0
                },
                {
                    "sent": "You actually have to say it this way, so that's a real problem, because you can just say, well, that's the T output and it's a human output with the match.",
                    "label": 0
                },
                {
                    "sent": "And if they match, that's correct.",
                    "label": 0
                },
                {
                    "sent": "And if they don't match, it's wrong.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Still, we want to not do the whole thing manually, so the manual evaluation would be to give someone that the input and the output and ask him is that the right translation?",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Is it the right translation on a scale from 1 to five, with five being perfect and one being completely terrible?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Let people judge people don't even agree on that, by the way.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the history of automatic metrics is, well, we take what works in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "Let's actually general model in machine translation we take what works in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So we look at word error rate.",
                    "label": 1
                },
                {
                    "sent": "So we still do the same thing where we have human translation.",
                    "label": 0
                },
                {
                    "sent": "We have our machine translation and we just look how many words match.",
                    "label": 0
                },
                {
                    "sent": "Word.",
                    "label": 0
                },
                {
                    "sent": "Error rate implies how many words match in the same sequence.",
                    "label": 0
                },
                {
                    "sent": "But often things get kind of fundamentally reordered, and you don't want to get completely punished for that.",
                    "label": 0
                },
                {
                    "sent": "So there are some adaptations for that where you allow some reordering.",
                    "label": 0
                },
                {
                    "sent": "The metric that is in use right now, pretty popular, is actually not that old.",
                    "label": 0
                },
                {
                    "sent": "Blue was suggested in 2002, and it's basically the same idea.",
                    "label": 0
                },
                {
                    "sent": "You have a reference sensation.",
                    "label": 0
                },
                {
                    "sent": "You have system output.",
                    "label": 0
                },
                {
                    "sent": "The trick is that you not only look at how many words did you get right.",
                    "label": 0
                },
                {
                    "sent": "Which would completely ignore reordering, but you also look at how many pairs of words and triples efforts and four grams of words you get right.",
                    "label": 0
                },
                {
                    "sent": "And if you get forwards right in comparison with the reference sensation that is deemed to be a good thing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, how does this work?",
                    "label": 0
                },
                {
                    "sent": "Let's say this is the reference translation here.",
                    "label": 1
                },
                {
                    "sent": "Slightly violent.",
                    "label": 0
                },
                {
                    "sent": "The government was shot to death by the police and he, our system translations or human translation.",
                    "label": 1
                },
                {
                    "sent": "So whatever they are possible candidate translation.",
                    "label": 0
                },
                {
                    "sent": "So you want to judge them by how good they are.",
                    "label": 0
                },
                {
                    "sent": "So we use engram overlap with the reference installation.",
                    "label": 0
                },
                {
                    "sent": "So I have a little bit color coded so if I get a four gram rider color the whole thing in green and affected by Graham right?",
                    "label": 0
                },
                {
                    "sent": "It's blue and and if I get nothing, if it produces wrong word and put it in red.",
                    "label": 0
                },
                {
                    "sent": "So you can kind of see that the really terrible translations, like the one that had funny subjects larger than zero symbols of course have a lot of words that are red, so these are judged very lowly.",
                    "label": 0
                },
                {
                    "sent": "Well, of course the perfect translation has all matching programs.",
                    "label": 0
                },
                {
                    "sent": "This it doesn't work perfectly.",
                    "label": 0
                },
                {
                    "sent": "So the last sentence.",
                    "label": 0
                },
                {
                    "sent": "Police killed the gunman.",
                    "label": 0
                },
                {
                    "sent": "It misses a little bit the information that there was shooting involved so they didn't stab him anything, but it's actually put translation.",
                    "label": 0
                },
                {
                    "sent": "But yeah, we have a single word right and then a bigram and the period at the end of that right too.",
                    "label": 0
                },
                {
                    "sent": "So that wouldn't be Josh very highly.",
                    "label": 0
                },
                {
                    "sent": "So it's very easy with this metric to always come with an example.",
                    "label": 0
                },
                {
                    "sent": "We have a perfectly nice translation, but it doesn't match the source matches human reference at all, and it would be judged fairly lowly.",
                    "label": 0
                },
                {
                    "sent": "The argument is that you don't do that for one sentence.",
                    "label": 0
                },
                {
                    "sent": "You do that for it's 1000 sentences.",
                    "label": 0
                },
                {
                    "sent": "And if in in a translation corpus of 1000 sentences you have much more matches with human reference, you probably better than a system that doesn't have that much measures.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the intuitive argument, and you might believe it to believe it or not.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What made this argument convincing that this is a good metric to evaluate machine translation graphs like this where you have automatic scores on the 1 axis and human scores?",
                    "label": 0
                },
                {
                    "sent": "So we the human side?",
                    "label": 0
                },
                {
                    "sent": "We did what I said earlier.",
                    "label": 0
                },
                {
                    "sent": "You give a sentence better to person, say, actually hear you ask different questions.",
                    "label": 0
                },
                {
                    "sent": "One is is the meaning preserved, that's called adequacy.",
                    "label": 0
                },
                {
                    "sent": "And the other question is, is it fluent output?",
                    "label": 0
                },
                {
                    "sent": "And that's called fluency and adequacy in fluency correlate alot.",
                    "label": 0
                },
                {
                    "sent": "But the main point is here that also the automatic scores.",
                    "label": 0
                },
                {
                    "sent": "So here's the NIST scores.",
                    "label": 0
                },
                {
                    "sent": "A slight variant of the blue score.",
                    "label": 0
                },
                {
                    "sent": "Also correlate very very well with the with the human judgment.",
                    "label": 0
                },
                {
                    "sent": "So the system on the top right that was judged best by the humans is also judged best by the automatic metrics and on the bottom left you have the worst system according both humans and their automatic metric.",
                    "label": 0
                },
                {
                    "sent": "So you can draw a nice line here and say you know everything kind of is on that line.",
                    "label": 0
                },
                {
                    "sent": "You can also compute the correlation and say well with the correlation.",
                    "label": 0
                },
                {
                    "sent": "I think what we have here.",
                    "label": 0
                },
                {
                    "sent": "I can read the number somewhere the 80s nineties.",
                    "label": 0
                },
                {
                    "sent": "That's a very strong correlation.",
                    "label": 0
                },
                {
                    "sent": "And and you say, well, OK, that's one metric.",
                    "label": 0
                },
                {
                    "sent": "Maybe I have a different metric and it's a very nice game to play.",
                    "label": 0
                },
                {
                    "sent": "So once you have a corpus annotated with human judgments, you can figure out, well, I invent a better metric, and you can invent the better metric.",
                    "label": 0
                },
                {
                    "sent": "You can then compute the score for all the systems, and then you can see does it correlate better with human judgments or not.",
                    "label": 0
                },
                {
                    "sent": "So it's actually machine translation.",
                    "label": 0
                },
                {
                    "sent": "Evaluation is a very well defined task, so we have no several corpora annotated with human judgments of sentences.",
                    "label": 0
                },
                {
                    "sent": "How would they are available?",
                    "label": 0
                },
                {
                    "sent": "Since they all these competitions and empty and you can download one of these different corpora and you can come up with your own metric and see does it correlate better than than the official metrics that I use.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me say a few more things about blue.",
                    "label": 0
                },
                {
                    "sent": "So the two cases I want to point out where things don't correlate well at all.",
                    "label": 0
                },
                {
                    "sent": "So there's this example from 2005.",
                    "label": 0
                },
                {
                    "sent": "So you this is the NIST evaluation of Arabic English that is kind of the most popular contest Americans really love to translate Arabic.",
                    "label": 0
                },
                {
                    "sent": "They just very fond of Arabic, and they have every year or competition where they want people to translate Arabic and people both statistically machine systems, basically so they're all different groups participating in that.",
                    "label": 0
                },
                {
                    "sent": "And you have then the blue scores on one end, and you have human judgment on the other.",
                    "label": 0
                },
                {
                    "sent": "Send and you see there is one outlier.",
                    "label": 0
                },
                {
                    "sent": "It actually correlates pretty well, except for that one funny outlier.",
                    "label": 0
                },
                {
                    "sent": "So that was a that point a greatest student at the University of Edinburgh who read the rules and said, well, it doesn't actually say anywhere in the rules that humans can take the output of the empty system and fix it up.",
                    "label": 0
                },
                {
                    "sent": "It was definitely not in the spirit of the game and he was very explicit about it and he admitted to it, although some people called him cheating.",
                    "label": 0
                },
                {
                    "sent": "But but what happened with that one system is just humans looking at the output.",
                    "label": 0
                },
                {
                    "sent": "There were, you know, English speakers in Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "They know nothing about Arabic, but they know what makes sense in English.",
                    "label": 0
                },
                {
                    "sent": "So it's not surprisingly that these systems got a very high fluency ranking or not perfect.",
                    "label": 0
                },
                {
                    "sent": "'cause as I said, always, someone is a better judge of what is good English and what is it.",
                    "label": 0
                },
                {
                    "sent": "Good translation in the next guy.",
                    "label": 0
                },
                {
                    "sent": "And it also got a very high adequate hearing because sometimes just you know what the text is about.",
                    "label": 0
                },
                {
                    "sent": "You can fill in the missing gaps.",
                    "label": 0
                },
                {
                    "sent": "And you can put things until you know things make much more sense.",
                    "label": 0
                },
                {
                    "sent": "It's also true that if human serita fluent sentence, they're much more convinced that it has the right meaning, even if it actually has mistakes.",
                    "label": 0
                },
                {
                    "sent": "So that's an interesting outlier, so this is.",
                    "label": 0
                },
                {
                    "sent": "And system that humans really like a lot, but the Blue Square doesn't like it.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a bit of both.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We came across another example that is when we wanted to compare our statistical T systems with the best thing out there, which is commercial software, or at least that's what we thought.",
                    "label": 0
                },
                {
                    "sent": "So the most popular system, or commercially successful system in machine translations?",
                    "label": 0
                },
                {
                    "sent": "For many language pairs system.",
                    "label": 0
                },
                {
                    "sent": "It's a company in in France and then California and we translated texts on the European Parliament.",
                    "label": 0
                },
                {
                    "sent": "So we have a huge corpus of European Parliament text so we can learn machine translation systems and we built two different machine translation systems, one good one and one bad one.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to build a bad empty system, so the trick here is to just use 3% of the training data.",
                    "label": 0
                },
                {
                    "sent": "And of course you're not going to do as well and if you look at the two SMP systems they're kind of on that diagonal that you want, so the systems is judged better.",
                    "label": 0
                },
                {
                    "sent": "For humans and by the automatic metric.",
                    "label": 0
                },
                {
                    "sent": "But the rule based system here cistern gets judged really harshly worse than our bad anti system by the automatic metric.",
                    "label": 0
                },
                {
                    "sent": "But judge pretty highly almost as higher or statistical system by humans.",
                    "label": 0
                },
                {
                    "sent": "Um, so that's that's of course a concern.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we haven't completely figured out why that is.",
                    "label": 0
                },
                {
                    "sent": "We notice one thing that this that rule based systems are strong outliers.",
                    "label": 0
                },
                {
                    "sent": "Much more if you test in domain.",
                    "label": 0
                },
                {
                    "sent": "If you have a certain corpus and your system learns all the right jargon and knows all the right words while the rule based system might say the same thing, but it uses different words.",
                    "label": 0
                },
                {
                    "sent": "And of course that gets punished and by the automatic metrics because you don't match exactly the right thing, so some mistakes that system makes sense on European Parliament are real mistakes.",
                    "label": 0
                },
                {
                    "sent": "So when you refer to the enlargement of the European Union, sister always says the widening of the European Union.",
                    "label": 0
                },
                {
                    "sent": "That does sound a bit funny, but many other things that perfectly good words and perfectly good translations.",
                    "label": 0
                },
                {
                    "sent": "So we think part of the answer is that.",
                    "label": 0
                },
                {
                    "sent": "The automatic metrics are little bit too literal.",
                    "label": 0
                },
                {
                    "sent": "You have to get the jargon right, not just the word the the meaning, right?",
                    "label": 0
                },
                {
                    "sent": "And it's a bit too sensitive to that.",
                    "label": 0
                },
                {
                    "sent": "But it's actually still so out of domain.",
                    "label": 0
                },
                {
                    "sent": "That problem shows up as well, so that's not all of the answer, but that's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about this is it's a very well defined problem Now, so we have actually several corpora where rule based on statistical systems are judged by humans, and we have all the automatic scores and can come up with a metric that doesn't have these problems.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Say, will you need a better language model in your?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's definitely.",
                    "label": 0
                },
                {
                    "sent": "Humans are very sensitive to things being produced.",
                    "label": 0
                },
                {
                    "sent": "Things being very fluent and that that does not only impact the fluency judgment, it also biases their adequacy judgment.",
                    "label": 0
                },
                {
                    "sent": "More than blue is sort of over sensitive too.",
                    "label": 0
                },
                {
                    "sent": "Fluency.",
                    "label": 0
                },
                {
                    "sent": "Yeah, not really, and if you actually would ask this people this question to the IBM people that say, well, a different version of the blue scores in some versions are much more biased towards the adequacy.",
                    "label": 0
                },
                {
                    "sent": "So arguably if you just look at how many words did you get right?",
                    "label": 0
                },
                {
                    "sent": "That's more biased towards adequacy, but if you look for four grams then you're more biased towards fluency because you have to kind of produce fluent N grams.",
                    "label": 0
                },
                {
                    "sent": "That results, I mean, it was that with with their multiple reference translations or so.",
                    "label": 0
                },
                {
                    "sent": "This is this is a single representation here you had four reference stations, so.",
                    "label": 0
                },
                {
                    "sent": "I see so you have multiple reference translations.",
                    "label": 0
                },
                {
                    "sent": "I guess for more diversity you know is it is.",
                    "label": 0
                },
                {
                    "sent": "Is there some understanding of the?",
                    "label": 0
                },
                {
                    "sent": "I guess the statistics do you.",
                    "label": 0
                },
                {
                    "sent": "Can you get away with a smaller test corpus if you have more reference translations for, so we often driven by necessity.",
                    "label": 0
                },
                {
                    "sent": "So very often we only have one reference station available, so for so I've been involved in organizing European language translation competitions.",
                    "label": 0
                },
                {
                    "sent": "And what do you do you take?",
                    "label": 0
                },
                {
                    "sent": "The training data and say that's your test set and don't use it for training.",
                    "label": 0
                },
                {
                    "sent": "And then you only have one reference translation for the NIST evaluation they always produce for reference translation.",
                    "label": 0
                },
                {
                    "sent": "I heard someone from IBM claiming that if you have four reference translations, that's as good as having twice as many sentences.",
                    "label": 0
                },
                {
                    "sent": "Um, some people who strongly believe in having multiple reference installations.",
                    "label": 0
                },
                {
                    "sent": "I'm not fully convinced that that is so good, because, OK, the argument is if multiple reference translations, there's any variety that can have in translation, it will show up in the reference section, but that's not entirely true either, because there's so much variety in translation that you're not going to get all true 4 grams in all the available reference stations.",
                    "label": 0
                },
                {
                    "sent": "They must have some statistics they have, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's cheaper to just make more translations, but I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Rather than having more reference traffic.",
                    "label": 0
                },
                {
                    "sent": "So yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "This result, the difference in kind of the rule based instance to Cisco Systems.",
                    "label": 0
                },
                {
                    "sent": "I mean, wouldn't that just tell you you have to be somehow combining the two things that sound?",
                    "label": 0
                },
                {
                    "sent": "That's one answer to and then people working on it.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if that's the ultimate argument for it.",
                    "label": 0
                },
                {
                    "sent": "They're definitely different strengths and weaknesses for rule based and statistical T. I'm not talking about that.",
                    "label": 0
                },
                {
                    "sent": "Also, let me talk about that quickly.",
                    "label": 0
                },
                {
                    "sent": "So if you compare system output with statistical empty output, things with statistical T is better is for instance word choice.",
                    "label": 0
                },
                {
                    "sent": "If they're ambiguous words, you don't know how to translate it, right?",
                    "label": 0
                },
                {
                    "sent": "The statistical systems actually don't have much problems with that, so one of the big problems in natural language processing, word sense, this equation, words have different meanings that translate differently.",
                    "label": 0
                },
                {
                    "sent": "Statistical systems are really good at that.",
                    "label": 0
                },
                {
                    "sent": "So just look at the context to see which word fits well.",
                    "label": 0
                },
                {
                    "sent": "You know maybe the adjective in front of it is already very, very helpful, and so on.",
                    "label": 0
                },
                {
                    "sent": "Rule based systems are generally much better in.",
                    "label": 0
                },
                {
                    "sent": "Sentence structure maybe, but definitely in the reflection of words.",
                    "label": 0
                },
                {
                    "sent": "If you translate into morphologically rich languages.",
                    "label": 0
                },
                {
                    "sent": "The statistical systems sometimes just take random guesses.",
                    "label": 0
                },
                {
                    "sent": "If something should be in particular case or another case of which verb tenses should be, well, the rule based systems are much more consistent than just look at.",
                    "label": 0
                },
                {
                    "sent": "You know what role does this noun phrase playing comes this, not this case, or it preserves the tense of the source working and thanks guys.",
                    "label": 0
                },
                {
                    "sent": "But yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, let's.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No one.",
                    "label": 0
                },
                {
                    "sent": "Still in the introduction.",
                    "label": 0
                },
                {
                    "sent": "So, so that definitely evaluation is actually.",
                    "label": 0
                },
                {
                    "sent": "It comes and goes in waves.",
                    "label": 0
                },
                {
                    "sent": "It's backing away for evaluation is a hot topic.",
                    "label": 0
                },
                {
                    "sent": "Someone claimed that maybe there more papers written about machine translation evaluation then about machine translation entirely.",
                    "label": 0
                },
                {
                    "sent": "Believe that, but it's a very easy thing to research and their obvious problems.",
                    "label": 0
                },
                {
                    "sent": "And ideally you want to have a metric that we can use, maybe even automatically do to optimize our systems that if we get improvement in that automatically metric, we can believe that our system is better.",
                    "label": 0
                },
                {
                    "sent": "And that might be also be useful to compare different types of systems, so the metrics we have right now so we are aware of some of their flaws.",
                    "label": 0
                },
                {
                    "sent": "But still we trust them enough that when we build a system and we have an improvement in Bleu score, you generally have a real improvement, and then many research papers that I've written that just show improvement in Bleu scores and don't show any improvements in translation.",
                    "label": 0
                },
                {
                    "sent": "They don't give a, so there was a famous question.",
                    "label": 0
                },
                {
                    "sent": "One of the conferences why you gave this long talk about machine translation.",
                    "label": 0
                },
                {
                    "sent": "You didn't show one single example.",
                    "label": 0
                },
                {
                    "sent": "So let know prompted some people to put examples in the paper, but you know if you translate 1000 sentences are not going to put in 1000 sentences in your paper.",
                    "label": 0
                },
                {
                    "sent": "So if you pick one or two translation examples here, look things improved, what did you really show by that?",
                    "label": 0
                },
                {
                    "sent": "Because you always improve few sentences, they got no matter what you do.",
                    "label": 0
                },
                {
                    "sent": "So the end of the story is, yeah, we use it in system development.",
                    "label": 0
                },
                {
                    "sent": "We kind of believe blue scores.",
                    "label": 0
                },
                {
                    "sent": "We believe them too much, but we don't believe them as comparison of different types of systems.",
                    "label": 0
                },
                {
                    "sent": "So if I beat sistren with messages go to system on blue, I'm not convinced that I really beat system.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "The typical situation is that you do your research on daily basis and you measure your progress in blue and then once a year there's official competition and.",
                    "label": 0
                },
                {
                    "sent": "Then you actually have the time and money to judge all these sentences.",
                    "label": 0
                },
                {
                    "sent": "It's extremely expensive to do this human judgment.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me quickly go over something that you can do with statistical methods.",
                    "label": 0
                },
                {
                    "sent": "So so you have this corpus of the European Parliament.",
                    "label": 1
                },
                {
                    "sent": "So the European Parliament is online since 1996, or at least you can the archives go back to 1996 and every two or three years we can crawl news new stuff and they bought 3040 million words translated.",
                    "label": 0
                },
                {
                    "sent": "So I told you earlier that in your lifetime you're going to be 300 million words.",
                    "label": 0
                },
                {
                    "sent": "If you want to spend 10 * 10% of your lifetime reading reading the European Parliament, well that's about a bigger chunk.",
                    "label": 0
                },
                {
                    "sent": "It is so it's actually a lot of text, so you can.",
                    "label": 0
                },
                {
                    "sent": "You know, learn a lot from it.",
                    "label": 0
                },
                {
                    "sent": "And then, well, it's published in 11 languages by 11 becausw in the mid 90s.",
                    "label": 0
                },
                {
                    "sent": "There were eleven official languages.",
                    "label": 0
                },
                {
                    "sent": "Now the 23 official languages in the European Union.",
                    "label": 0
                },
                {
                    "sent": "But still they don't translate the European Parliament into all the 23 languages.",
                    "label": 0
                },
                {
                    "sent": "Only in the 11 original so.",
                    "label": 0
                },
                {
                    "sent": "So I'm told that the story with the European translation offices they spent half a billion euros each year on translation.",
                    "label": 0
                },
                {
                    "sent": "And they translate what they can.",
                    "label": 0
                },
                {
                    "sent": "They could.",
                    "label": 0
                },
                {
                    "sent": "What are the translation needs in Europe?",
                    "label": 0
                },
                {
                    "sent": "So you could if you give them 2 billion euros they would translate 4 times more less.",
                    "label": 0
                },
                {
                    "sent": "There's no limit, so it's very hard to say.",
                    "label": 0
                },
                {
                    "sent": "You know how big is the market for?",
                    "label": 0
                },
                {
                    "sent": "For translating, well, people like to have all kinds of stuff translated.",
                    "label": 0
                },
                {
                    "sent": "So that they kind of save some resources.",
                    "label": 0
                },
                {
                    "sent": "They don't translate the Parliament proceedings into all 23 languages.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have here said 20 to 30,000,000 words, languages for them.",
                    "label": 0
                },
                {
                    "sent": "For some languages we know about, 40,000,000 words have 110 language pairs.",
                    "label": 0
                },
                {
                    "sent": "So you can translate built machine translation systems from Danish to English, Spanish, Portuguese and so on.",
                    "label": 0
                },
                {
                    "sent": "And with static and T you can do that very nicely.",
                    "label": 0
                },
                {
                    "sent": "You can actually.",
                    "label": 0
                },
                {
                    "sent": "Take this corpus and expect 110 parallel copper out of it.",
                    "label": 0
                },
                {
                    "sent": "Actually, technically 55 parallel corpora and then build 110 anti systems.",
                    "label": 0
                },
                {
                    "sent": "And I did that already.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Years ago, and these are the Bleu scores for all the different systems.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it's a bit of a gimmick because I built all these systems.",
                    "label": 0
                },
                {
                    "sent": "I never use them sitting out still sitting on a hard drive somewhere.",
                    "label": 0
                },
                {
                    "sent": "I just recently rebuilt the whole matrix so my scores and I've got two points better, but not much has changed.",
                    "label": 0
                },
                {
                    "sent": "It's kind of nice to see what.",
                    "label": 0
                },
                {
                    "sent": "What are the easy languages and what I'd be hard languages.",
                    "label": 0
                },
                {
                    "sent": "So you see, for instance, translating to finish is really hard.",
                    "label": 0
                },
                {
                    "sent": "Also, somewhat surprising.",
                    "label": 0
                },
                {
                    "sent": "Translate into German is very hard.",
                    "label": 0
                },
                {
                    "sent": "And but it's really easy as French, Spanish, Spanish to Portuguese.",
                    "label": 0
                },
                {
                    "sent": "Generally, translating into English is pretty easy.",
                    "label": 0
                },
                {
                    "sent": "So, so that's interesting matrix.",
                    "label": 0
                },
                {
                    "sent": "So you might have noticed that logo we have a European project for now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, year and a half old euro matrix, which is kind of inspired by this.",
                    "label": 0
                },
                {
                    "sent": "So why don't we just built in P systems for all European languages?",
                    "label": 0
                },
                {
                    "sent": "And we said we're going to do it for all official EU languages and we didn't completely realized at that point.",
                    "label": 0
                },
                {
                    "sent": "That means if you do it for 23 languages, you have to build about 500 systems.",
                    "label": 0
                },
                {
                    "sent": "But OK, we have a cluster computer in Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "It's currently building these systems.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As part of the project we have this online evaluation, so we have a website which might be down right now because the computer Science Department Edinburgh is moving and all the computers get disconnected.",
                    "label": 0
                },
                {
                    "sent": "Help that's coming back up sometime soon.",
                    "label": 0
                },
                {
                    "sent": "Where you actually have very can actually look at at least the output of all these empty systems, and for some of the language purfeerst multiple systems participating and and producing output.",
                    "label": 0
                },
                {
                    "sent": "And you can kind of see what their performances.",
                    "label": 0
                },
                {
                    "sent": "And if you build your own empty system, you can upload it there and it gets automatically scored and ranked across against the other months.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Computed on different corpora.",
                    "label": 0
                },
                {
                    "sent": "I don't know, so you have a test set that is also aligned.",
                    "label": 0
                },
                {
                    "sent": "Sentence by sentence across the 11 languages.",
                    "label": 0
                },
                {
                    "sent": "Search Midget but each.",
                    "label": 0
                },
                {
                    "sent": "Test set for the different languages are actually different.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah Oh well.",
                    "label": 0
                },
                {
                    "sent": "Well, they're different.",
                    "label": 0
                },
                {
                    "sent": "So you have the same sentence.",
                    "label": 0
                },
                {
                    "sent": "Translated across 11 languages and you have 2000 of those sentences, so it's it is a parallel across 11 sentences, of course.",
                    "label": 0
                },
                {
                    "sent": "So here I'm also not entirely believe the blue scores because you compare against different references.",
                    "label": 0
                },
                {
                    "sent": "The part of the reason why finishes harder is finished.",
                    "label": 0
                },
                {
                    "sent": "Build gigantic words so the finish verse usually contain things like terminal and preposition, and they also have this German disease of sticking words together.",
                    "label": 0
                },
                {
                    "sent": "So getting a birthright and finish is much harder than getting a word in English, so getting it right and finish is almost as good as getting 3 words writing in English.",
                    "label": 0
                },
                {
                    "sent": "So getting high Bleu scores and finish this much, much harder because you are judged on how often you get a program right finished.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's extremely difficult questions, so to answer.",
                    "label": 0
                },
                {
                    "sent": "So you basically have to answer the question.",
                    "label": 0
                },
                {
                    "sent": "Is this Spanish sentence as bad as this German sentence?",
                    "label": 0
                },
                {
                    "sent": "It's very hard to get judgment, so this is this hard to get a handle on.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so the all the all the columns.",
                    "label": 0
                },
                {
                    "sent": "Well they have the compare against the same output language.",
                    "label": 0
                },
                {
                    "sent": "But does it really mean that translating into German is harder than translating to English?",
                    "label": 0
                },
                {
                    "sent": "It might not be.",
                    "label": 0
                },
                {
                    "sent": "But in the Rose, I mean you always compare against the same output, and we're relatively certain we did them evaluation manually.",
                    "label": 0
                },
                {
                    "sent": "If you translate from different source languages, so their blue seems to correlate.",
                    "label": 0
                },
                {
                    "sent": "Pretty well with human judgments.",
                    "label": 0
                },
                {
                    "sent": "You mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the big problem is German as well as word order.",
                    "label": 0
                },
                {
                    "sent": "So typically the if you don't get the verb right in the sentence that has a lot of knock off effect of everything else we just.",
                    "label": 0
                },
                {
                    "sent": "The language model likes to have build a sentence would often hallucinates auxiliary's left and right, so everything falls apart.",
                    "label": 0
                },
                {
                    "sent": "If you don't have it in the right place.",
                    "label": 0
                },
                {
                    "sent": "So that's one of the reason rich morphology is another reason.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the blue scores, finish as the richest morphology of all these languages.",
                    "label": 0
                },
                {
                    "sent": "See hardest language translate.",
                    "label": 0
                },
                {
                    "sent": "So yeah, those two.",
                    "label": 0
                },
                {
                    "sent": "Otherwise German and English are very related languages in terms of vocabulary.",
                    "label": 0
                },
                {
                    "sent": "They use a lot of the words look the same, the kind of expressions you can use in English.",
                    "label": 0
                },
                {
                    "sent": "You can almost always welcome very often, use the same expression in German.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which is not the case in do Chinese, English and so on.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I like German, basically German kind of poses.",
                    "label": 0
                },
                {
                    "sent": "The challenge of getting syntax, writing empty.",
                    "label": 0
                },
                {
                    "sent": "And and morphology otherwise, it's a easy language pair.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another game make you think I did so you can cluster languages by how well the translate.",
                    "label": 0
                },
                {
                    "sent": "So you just take the blue score and say which two systems have the highest goal scorer.",
                    "label": 0
                },
                {
                    "sent": "Group them together so this kind of greedy clustering where you first cluster the languages together very close and then you take clusters and see where the cluster flows.",
                    "label": 0
                },
                {
                    "sent": "And if you do that well, I think you had a tweak it a little bit.",
                    "label": 0
                },
                {
                    "sent": "I mean all comes down to the similarity metric in audio computed, but you get the nice tree of language pairs, so this seems suggestive languages are related, like Portuguese, Spanish, French and Italian are more related than the other ones.",
                    "label": 0
                },
                {
                    "sent": "You kind of get these nice cluster of the romantic and the Germanic languages and the two outliers.",
                    "label": 0
                },
                {
                    "sent": "Here Greek and Finnish.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here's the thing I said about what is what is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I kind of already talked about this.",
                    "label": 0
                },
                {
                    "sent": "So you see that some languages are easier to translate into then out of.",
                    "label": 1
                },
                {
                    "sent": "So if you translate if you average all the scores of translating from German from English, but English is slightly easier but not much 2020 two 23.8.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the scores into German and into English, than German is much much harder.",
                    "label": 0
                },
                {
                    "sent": "So 10 point Bluetooth difference.",
                    "label": 0
                },
                {
                    "sent": "So there's some languages that if you look at the difference between the average of the system translating into this language in there isn't translating out of these languages, so you see kind of this.",
                    "label": 0
                },
                {
                    "sent": "Tracks somewhat the richness of morphology of these languages.",
                    "label": 0
                },
                {
                    "sent": "OK, you probably.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I heard this story.",
                    "label": 0
                },
                {
                    "sent": "I'm slowly getting tired of telling of it, so there's this story.",
                    "label": 0
                },
                {
                    "sent": "Back in the days, I don't know.",
                    "label": 0
                },
                {
                    "sent": "2030 years ago, for some reason, Americans didn't care much about Arabic.",
                    "label": 0
                },
                {
                    "sent": "They care about Russian also.",
                    "label": 0
                },
                {
                    "sent": "Interesting language.",
                    "label": 0
                },
                {
                    "sent": "Also, rich morphology and the story is that they wanted to test how would the empty system is and of course being true Americans.",
                    "label": 0
                },
                {
                    "sent": "They don't know any foreign languages, so how can they judge it?",
                    "label": 0
                },
                {
                    "sent": "So take an English sentence.",
                    "label": 0
                },
                {
                    "sent": "The spirit is willing but the flesh is weak.",
                    "label": 1
                },
                {
                    "sent": "Translate into Russian and then translated back into English and they get out.",
                    "label": 0
                },
                {
                    "sent": "The vodka is good in the meters rotten, so it's a funny story.",
                    "label": 0
                },
                {
                    "sent": "It's not true, it's just made up.",
                    "label": 0
                },
                {
                    "sent": "Kind of see what happens so you don't know flesh and meat and spirit and what card.",
                    "label": 0
                },
                {
                    "sent": "Now it's plausible enough to be believable.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of funny, but that's actually what journalists do if you read English language journalists talking bout empty, then only foreign language.",
                    "label": 0
                },
                {
                    "sent": "So what they do is the same thing that take English sentence, translate into form and get back to English an if it's much different than, say you know MP is terrible.",
                    "label": 0
                },
                {
                    "sent": "So we can actually test if that's actually a good way to assess.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you basically take a system with this.",
                    "label": 0
                },
                {
                    "sent": "So these are into English and from from English.",
                    "label": 0
                },
                {
                    "sent": "So if you take a sentence in English, it translated into into, let's say Danish and translated back into English.",
                    "label": 0
                },
                {
                    "sent": "You can then compare how similar to the input.",
                    "label": 0
                },
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "We take the blue score like you always do and you kind of see here then which blue scores you get.",
                    "label": 0
                },
                {
                    "sent": "Very high Bleu scores.",
                    "label": 0
                },
                {
                    "sent": "But they don't correlate it at all with the with the individual system score.",
                    "label": 0
                },
                {
                    "sent": "So if you look at Greek actually gets a higher back translation score then then Portuguese.",
                    "label": 0
                },
                {
                    "sent": "But the individual systems are much worse.",
                    "label": 0
                },
                {
                    "sent": "And you know the ultimate proof that that's a bad thing is if you build a system that does nothing, which takes all the sentences and just producing the output, you would actually get a perfect score on this metric, so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Don't do that.",
                    "label": 0
                },
                {
                    "sent": "OK so but I quickly point out there's a lot of resources available.",
                    "label": 0
                },
                {
                    "sent": "A lot of parallel corpora, so we have this Europol corpus.",
                    "label": 0
                },
                {
                    "sent": "But you can download this now.",
                    "label": 0
                },
                {
                    "sent": "A key community care corpus which actually now in 22 official EU languages has similar size actually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it's a nice car with in terms of volume.",
                    "label": 0
                },
                {
                    "sent": "It's a terrible corpus in terms of content.",
                    "label": 0
                },
                {
                    "sent": "It's all the legal documents that all the member countries had to serve had to sign, so it has a lot of Article One, Article 2 and the aforementioned, whereas blah blah blah this kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "Another corpus, this part of it made available by Rickman, the Canadian hansards.",
                    "label": 0
                },
                {
                    "sent": "That's a popular corpus.",
                    "label": 0
                },
                {
                    "sent": "So in the Canadian Parliament everything has to be bilingual, so everything is French and English.",
                    "label": 0
                },
                {
                    "sent": "For Chinese and Arabic to English, you can get from the LDC over 100 million words.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There's also a lot of monolingual data available.",
                    "label": 0
                },
                {
                    "sent": "If you want to Spanish English, there's also United Nation corpus.",
                    "label": 0
                },
                {
                    "sent": "I think it's not made available in any consumable form, but you could actually go to the UN website and download documents and you get a lot of Spanish and the English translations, and that's in the order of 100 million words, so that's.",
                    "label": 0
                },
                {
                    "sent": "That's quite a lot.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in terms of monolingual data, that is much, much more available, so there's famous Giga Word corpus that is huge and gigantic until people look at the web and there's even more data.",
                    "label": 0
                },
                {
                    "sent": "So people Google is famous for.",
                    "label": 0
                },
                {
                    "sent": "Taking all the web that they already down on their hard drive.",
                    "label": 0
                },
                {
                    "sent": "Simple gigantic language models in the order of trillions of words.",
                    "label": 0
                },
                {
                    "sent": "So the latest language models are 7 gram models over 8 trillion words.",
                    "label": 0
                },
                {
                    "sent": "Where do they do it?",
                    "label": 0
                },
                {
                    "sent": "Because they can.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It does help performance, so I have here a graph when you add more data the performance goes up and that is true no matter how much how much data you have.",
                    "label": 0
                },
                {
                    "sent": "So the general experiences if you really want to improve the empty system, one thing is certain.",
                    "label": 0
                },
                {
                    "sent": "That's going to help us if you give me more data, especially if you give me data in the domain I care about.",
                    "label": 0
                },
                {
                    "sent": "I could draw a different curve also for language modeling data, if you could have more language modeling data can also build better systems.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, since we're talking Russo resources to advertise this year, so we have been building over the last two or three years now.",
                    "label": 0
                },
                {
                    "sent": "This open source implementation of the phrase based models, exactly what I'm going to talk about today, and it also has a web page which may or may not be up or down right now.",
                    "label": 0
                },
                {
                    "sent": "And you can download it.",
                    "label": 0
                },
                {
                    "sent": "You get the source code.",
                    "label": 0
                },
                {
                    "sent": "It's a full system.",
                    "label": 0
                },
                {
                    "sent": "You can just train everything so that got funding by yeah back in the day, T star when it wasn't project currently.",
                    "label": 0
                },
                {
                    "sent": "Euro matrix also.",
                    "label": 0
                },
                {
                    "sent": "That funding by DARPA basically NSF and so many universes involved.",
                    "label": 0
                },
                {
                    "sent": "It's basically the thing kind of.",
                    "label": 0
                },
                {
                    "sent": "If you write a paper and you don't compare it against this system, you usually get criticized.",
                    "label": 0
                },
                {
                    "sent": "So this means to beat up again so you see a lot of paper on.",
                    "label": 0
                },
                {
                    "sent": "This is better than Moses, and that is better than most, so that's the way you have to write a paper.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there are competitions.",
                    "label": 0
                },
                {
                    "sent": "Just want to mention them so the kind of most prominent is the one organized by NIST which is funded on and off by DARPA.",
                    "label": 0
                },
                {
                    "sent": "So they do yearly competitions.",
                    "label": 0
                },
                {
                    "sent": "They got a little bit behind schedule now and they do it in Arabic, English, Chinese, English.",
                    "label": 0
                },
                {
                    "sent": "For that there's a lot of data available.",
                    "label": 0
                },
                {
                    "sent": "Problem with that competition is it takes forever to training system because you're given to her at million words.",
                    "label": 0
                },
                {
                    "sent": "So everything takes weeks instead of.",
                    "label": 0
                },
                {
                    "sent": "Days or hours.",
                    "label": 0
                },
                {
                    "sent": "That's the extreme opposite of that is the IW 30, which is motivated by speech translation research so that they have tiny corpora in the order of 40,000 sentences.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "And there are always in a travel domain kind of very short sentences like can I pay with credit card and what show me the way to the hotel?",
                    "label": 1
                },
                {
                    "sent": "Things like this.",
                    "label": 0
                },
                {
                    "sent": "Their training system takes like 15 minutes and well the middle ground.",
                    "label": 0
                },
                {
                    "sent": "I don't know is the competition we have been organizing for a number of years now in connection with the ACL on European languages.",
                    "label": 1
                },
                {
                    "sent": "I say a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, my next slide says little, but so the last we just did that earlier this year.",
                    "label": 0
                },
                {
                    "sent": "And we also added to the kind of.",
                    "label": 0
                },
                {
                    "sent": "Competition a number of rule based systems just off the shelf.",
                    "label": 0
                },
                {
                    "sent": "Commercial Systems, 6 different ones, and we ran them on the same test sets.",
                    "label": 0
                },
                {
                    "sent": "Typically we do we take which we give people the Europol corpus.",
                    "label": 0
                },
                {
                    "sent": "I mean then say OK, how well do you do now I'm translating Europe are so it's in domain and except for English, German, all the statistical systems there beat the rule based systems.",
                    "label": 0
                },
                {
                    "sent": "But then we also had a tougher test set and there's many ways you can interpret that.",
                    "label": 0
                },
                {
                    "sent": "So that's news stories.",
                    "label": 0
                },
                {
                    "sent": "What we did is we.",
                    "label": 0
                },
                {
                    "sent": "Literally went out and got new stories in Spanish and English and German and did the same thing.",
                    "label": 0
                },
                {
                    "sent": "We've created corpus that is 2000 sentence is aligned across all the six languages that we cared about.",
                    "label": 0
                },
                {
                    "sent": "And the same thing we scored, then the systems and these are based on manual judgments.",
                    "label": 0
                },
                {
                    "sent": "Which sentence are preferred.",
                    "label": 0
                },
                {
                    "sent": "And then we don't do so well with statistical empty French English.",
                    "label": 0
                },
                {
                    "sent": "The best statistical system that was trained just on European Parliament text still beat all the rule based systems out there.",
                    "label": 0
                },
                {
                    "sent": "But all the other language pairs rule based systems were better.",
                    "label": 0
                },
                {
                    "sent": "So from a statistical point of view you can say, well, we don't have training data.",
                    "label": 0
                },
                {
                    "sent": "We actually had no training data at all.",
                    "label": 0
                },
                {
                    "sent": "We didn't have development data.",
                    "label": 0
                },
                {
                    "sent": "We have nothing.",
                    "label": 0
                },
                {
                    "sent": "We didn't give people like look here.",
                    "label": 0
                },
                {
                    "sent": "This is how the test set might look like.",
                    "label": 0
                },
                {
                    "sent": "We didn't even give people that.",
                    "label": 0
                },
                {
                    "sent": "Arguably, well, that's probably the point to prove for next year, because we're going to do the same thing next year.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of language model data in the news domain, that should help.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's really hard to get parallel texts in the news domain, so there are websites like BBC and even the television show station Euronews that seems to have the same story in multiple languages, but they never translate them, though is completely rewrite it.",
                    "label": 0
                },
                {
                    "sent": "That's why it's really annoying.",
                    "label": 0
                },
                {
                    "sent": "Serve the audience and tell him the right thing that people care about.",
                    "label": 0
                },
                {
                    "sent": "I don't care about us.",
                    "label": 0
                },
                {
                    "sent": "So you can't extract just simply sentence aligned parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "So there's there's some fishing you can do, and maybe should put some effort into making use text news parallel corpora available.",
                    "label": 0
                },
                {
                    "sent": "But there's actually no real training data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a challenge for next year.",
                    "label": 0
                },
                {
                    "sent": "We have to take the right column and make some tea.",
                    "label": 0
                },
                {
                    "sent": "Over there, so some languages plus we really do so English, German.",
                    "label": 0
                },
                {
                    "sent": "We even on Europol get beaten by rule based system.",
                    "label": 0
                },
                {
                    "sent": "That's embarrassing.",
                    "label": 0
                },
                {
                    "sent": "It has a lot to do with morphology, so systems are not very good with morphology in Germany if you get the case of noun phrases, that's not something that phrase based systems are very good at because they translate.",
                    "label": 0
                },
                {
                    "sent": "They said earlier a little phrases at a time and we don't know where you are.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sentence anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, this introduction took way too long, so let's talk about the real stuff here, phrase based models.",
                    "label": 0
                },
                {
                    "sent": "I do that in a bit odd way, so I'm not going to talk much about how you train the model.",
                    "label": 0
                },
                {
                    "sent": "How you learn the model, how you parameterized model.",
                    "label": 0
                },
                {
                    "sent": "I'll do that later.",
                    "label": 0
                },
                {
                    "sent": "First, take a talk about how you use it.",
                    "label": 0
                },
                {
                    "sent": "How do you actually use the phrase based model to produce translations?",
                    "label": 0
                },
                {
                    "sent": "I think that's more intuitive than the other stuff.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Makes more sense.",
                    "label": 0
                },
                {
                    "sent": "So I have this slide so you as I said two models, so we know care about the translation model.",
                    "label": 1
                },
                {
                    "sent": "So we have now the phrase based system where we have a language model there and what we talk about right away is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Decoding algorithm, so this is as a reminder the model we're talking about.",
                    "label": 0
                },
                {
                    "sent": "So we take the input sentence, chop it up into phrases when phrases could be anything, and you have some reordering also and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, then you pick the best station.",
                    "label": 0
                },
                {
                    "sent": "So how does it kind of look like?",
                    "label": 0
                },
                {
                    "sent": "In summer, typical translation table, so the German phrase Dean for schlock, you see, the top translations here.",
                    "label": 0
                },
                {
                    "sent": "The proposal is the best translation with 62% probability.",
                    "label": 0
                },
                {
                    "sent": "Someone's proposal is separated out, the possessive there 10%.",
                    "label": 0
                },
                {
                    "sent": "Then you have some variation, Lexicon, idea, proposal ideas.",
                    "label": 0
                },
                {
                    "sent": "So nothings suggestions.",
                    "label": 0
                },
                {
                    "sent": "You also get it.",
                    "label": 0
                },
                {
                    "sent": "So we get this from parallel corpus.",
                    "label": 0
                },
                {
                    "sent": "So in German the.",
                    "label": 0
                },
                {
                    "sent": "The writer still thought I have to mention the enforce lock again, but the English translator, or maybe it was actually originally in English, said well, I already talked about the proposal.",
                    "label": 0
                },
                {
                    "sent": "People know what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "I can use the word it.",
                    "label": 0
                },
                {
                    "sent": "So you get things like that, so usually get at the bottom of this table.",
                    "label": 0
                },
                {
                    "sent": "A lot of junk.",
                    "label": 0
                },
                {
                    "sent": "'cause sometimes people translate very loosely, you make errors and aligning and so on.",
                    "label": 0
                },
                {
                    "sent": "But the top looks pretty promising.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we do with this?",
                    "label": 0
                },
                {
                    "sent": "So what do I do when I want to translate to Spanish sentence here?",
                    "label": 0
                },
                {
                    "sent": "I have no idea if this is a good Spanish sentence.",
                    "label": 0
                },
                {
                    "sent": "I took that example from Kevin Knight.",
                    "label": 0
                },
                {
                    "sent": "I know it changed a few times, but there might be more may or may not be Spanish and translate that now into English.",
                    "label": 0
                },
                {
                    "sent": "Let's terrorize good is now fixed.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to do while I start?",
                    "label": 0
                },
                {
                    "sent": "Well, pick one of the words that I want to translate 1st and say that's Mary.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so and then say OK, I'm done with that.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to translate the word again.",
                    "label": 0
                },
                {
                    "sent": "And then I take some other words.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then take some other words.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And translate them and take some know this is kind of a nice little Fraser translate them into one word so you can translate them anyway.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listen to one.",
                    "label": 0
                },
                {
                    "sent": "He also many worsen.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, and you can also do reordering, so when you do reordering it just take the source material out of out of order so you build the English sentence always in order, but you can take the source words out of order.",
                    "label": 0
                },
                {
                    "sent": "That's how you implicitly then do reordering.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know you done many translated all the words.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's?",
                    "label": 0
                },
                {
                    "sent": "What is the computer going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, so all these choices.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so as each phrase might have multiple translation, you also don't know if you should translate 3 border time or should translate the three words separately.",
                    "label": 0
                },
                {
                    "sent": "And you also can take them at any point, so you can reorder so you can start in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So lot of choices here.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "We start with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the same starting point at the beginning of the sentence.",
                    "label": 0
                },
                {
                    "sent": "I've nothing translated so far.",
                    "label": 0
                },
                {
                    "sent": "So that we call these things hypothesis empty hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So I indicate here E has nothing translated for F. No foreign words are covered and the probability at the beginning is 1, but haven't done so.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing so far.",
                    "label": 0
                },
                {
                    "sent": "And then basically the action of expanding this hypothesis of translating one phrase.",
                    "label": 0
                },
                {
                    "sent": "That means you take that phrase, you put the translation at the end of your English.",
                    "label": 0
                },
                {
                    "sent": "You have to mark off in your little bit vector there that you have the first word already translated.",
                    "label": 0
                },
                {
                    "sent": "Unicorn translated again, and there are all kinds of probabilities that get multiplied in.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a quick word to the probabilities we care about, so they obviously phrase translation probability of certain probability that Amari is a translation of Maria.",
                    "label": 1
                },
                {
                    "sent": "There also some reordering costs.",
                    "label": 0
                },
                {
                    "sent": "I gotta talk about that much more detail, but all these things are.",
                    "label": 0
                },
                {
                    "sent": "You know, we also feature how many phrases do they translate so far?",
                    "label": 0
                },
                {
                    "sent": "I mean words to translate so far.",
                    "label": 0
                },
                {
                    "sent": "Another important thing is you have a language model, so all the thing has to make sense in English, so it has to be fluent English.",
                    "label": 0
                },
                {
                    "sent": "So language model is ideally a mathematical model.",
                    "label": 1
                },
                {
                    "sent": "You give it a sentence and the language model tells you is it a good sentence.",
                    "label": 0
                },
                {
                    "sent": "So has a high language model.",
                    "label": 0
                },
                {
                    "sent": "Scores are bad sentence.",
                    "label": 0
                },
                {
                    "sent": "The language model score.",
                    "label": 0
                },
                {
                    "sent": "The methods we use actually very simple.",
                    "label": 0
                },
                {
                    "sent": "OK, I shouldn't say things are simpler types.",
                    "label": 0
                },
                {
                    "sent": "OK, so it works fairly straightforward though.",
                    "label": 0
                },
                {
                    "sent": "OK, how do you do so?",
                    "label": 0
                },
                {
                    "sent": "You basically predicted word at a time.",
                    "label": 0
                },
                {
                    "sent": "You say?",
                    "label": 0
                },
                {
                    "sent": "How likely does a sentence start with the Mary and then given that the sentence started with the word Mary, how likely is the next word did an you can kind of see how could you possibly know these things?",
                    "label": 0
                },
                {
                    "sent": "Well you go over billions of trillions of words of English and see of all the billions of trillions of words and all these sentences.",
                    "label": 0
                },
                {
                    "sent": "How many sentences did actually start with Mary?",
                    "label": 0
                },
                {
                    "sent": "And how many sentences start with Mary did and how many sentences that have Mary did or not then followed by not so you can come have collect these status.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, back to our story here.",
                    "label": 0
                },
                {
                    "sent": "So there's no reason why I should have started with Mary and OK, I started with Mary because I knew that was the right thing, but the computer doesn't know that, so the computer could also start with broker and translate that into which, so you have to get some hit for reordering because you jump 8 words.",
                    "label": 0
                },
                {
                    "sent": "That's pretty bad usually.",
                    "label": 0
                },
                {
                    "sent": "But you know, translation models thinks it's right.",
                    "label": 0
                },
                {
                    "sent": "Do more English sentence.",
                    "label": 0
                },
                {
                    "sent": "Start with words which then with the word Mary.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Channel OK, so you keep doing this, so the hypothesis you expand that you just expand them.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ever.",
                    "label": 0
                },
                {
                    "sent": "And if you do this all the time, then eventually you gonna reach the end.",
                    "label": 0
                },
                {
                    "sent": "You know you reached the end, because if all the foreign words covered.",
                    "label": 1
                },
                {
                    "sent": "And then you can kind of go back and read off the sentence that you translate it.",
                    "label": 0
                },
                {
                    "sent": "So you're married did not slap the Greenway Trail.",
                    "label": 1
                },
                {
                    "sent": "We got that right.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Miracle, of course.",
                    "label": 0
                },
                {
                    "sent": "This story is more complicated.",
                    "label": 0
                },
                {
                    "sent": "Because at any point you have a lot of choices at the beginning you have, we can take any of the translation options that grew up there.",
                    "label": 0
                },
                {
                    "sent": "Typically we have up to 20 cuttitta 20 or so.",
                    "label": 0
                },
                {
                    "sent": "Maybe sometimes you gotta 100.",
                    "label": 0
                },
                {
                    "sent": "So 2100 translation for each phrase.",
                    "label": 0
                },
                {
                    "sent": "Well, we probably have translation for the single words.",
                    "label": 0
                },
                {
                    "sent": "Often we have translation many of bigrams trigrams.",
                    "label": 0
                },
                {
                    "sent": "We still have a lot of translation, so a lot of choices.",
                    "label": 0
                },
                {
                    "sent": "Easily for sentence length of 200 or so choices.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you wanna try the beginning then you make that choice and have almost all the same number of choices left.",
                    "label": 0
                },
                {
                    "sent": "You can translate the same words again, but almost everything else you can still do, so that explores pretty badly.",
                    "label": 0
                },
                {
                    "sent": "So if you just draw it up this way, you actually create an exponential search graph.",
                    "label": 0
                },
                {
                    "sent": "An exponential sounds bad.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So most of your computer science students, so you'll know when I say the decoding has been proven to be NP complete.",
                    "label": 0
                },
                {
                    "sent": "That means it's just not tractable and you can't can't possibly find, search or list up or possible translations.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do two different things here.",
                    "label": 0
                },
                {
                    "sent": "So we do first hypothesis recombination.",
                    "label": 1
                },
                {
                    "sent": "That's a risk free thing.",
                    "label": 1
                },
                {
                    "sent": "So that means if we do this, we're still guaranteed to find the best translation.",
                    "label": 0
                },
                {
                    "sent": "Let me realize well it's NP complete, so that's not going to save us, so we're going to do something more risky where we just do pruning.",
                    "label": 0
                },
                {
                    "sent": "We just kind of basically throw out kind of the bad hypothesis early on without being entirely sure that they are.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I positive combination.",
                    "label": 0
                },
                {
                    "sent": "So the argument is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's say you can start the sentence with translating Marianne.",
                    "label": 0
                },
                {
                    "sent": "Then you translate 2 words at a time into it did not give.",
                    "label": 1
                },
                {
                    "sent": "And that gives you certain probability, but it could also have sort of Amari, but then translate the next word with did not.",
                    "label": 0
                },
                {
                    "sent": "And then the third word with give.",
                    "label": 0
                },
                {
                    "sent": "And you end up with almost the same thing.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's you have this.",
                    "label": 0
                },
                {
                    "sent": "You have basically two different hypothesis that have the same foreign words translated.",
                    "label": 1
                },
                {
                    "sent": "That produced the same English words.",
                    "label": 0
                },
                {
                    "sent": "And the argument now is, well, one has a lower probability than the other, and then one over the lower probability.",
                    "label": 0
                },
                {
                    "sent": "I can safely throw away.",
                    "label": 0
                },
                {
                    "sent": "And why can't I do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the argument is set.",
                    "label": 0
                },
                {
                    "sent": "Any continuation I have from there I could also do on the better hypothesis an always going to have the better probability and the better hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So the second the lower of the two high positives could not be part of the best path.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We throw it away.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we keep a little bit back pointer.",
                    "label": 0
                },
                {
                    "sent": "Important for now.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why we do that?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that should reduce the search space a bit.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'm going to make the argument.",
                    "label": 0
                },
                {
                    "sent": "If you have a second sentence here.",
                    "label": 0
                },
                {
                    "sent": "Here Joe did not give for some reason.",
                    "label": 1
                },
                {
                    "sent": "I translated Maria and Joe.",
                    "label": 0
                },
                {
                    "sent": "But then I produce gift.",
                    "label": 0
                },
                {
                    "sent": "What do I have there?",
                    "label": 0
                },
                {
                    "sent": "Well, I also have a hypothesis that has the same first 3 words translated.",
                    "label": 0
                },
                {
                    "sent": "The two agree in the last three English words, but don't agree in the first English word.",
                    "label": 0
                },
                {
                    "sent": "But the argument is even there, I can throw away that hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So you have to keep in mind what happens with scoring when you add more praise translations, you phrase translation probabilities well, they don't care about what you did previously, but you also have a language model.",
                    "label": 0
                },
                {
                    "sent": "So language model still cares about the last words you produce, so you can just pick the best translation of the first 3 words, because whatever we produce influences how likely the next words follow.",
                    "label": 0
                },
                {
                    "sent": "But if you use the trigram language model or even programming language model in this case, it's not going to care about what what happened at the beginning of the sentence here for words earlier.",
                    "label": 0
                },
                {
                    "sent": "So the same argument.",
                    "label": 0
                },
                {
                    "sent": "Any continuation of that upper hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You can also continue exactly the same way at the lower hypothesis, and you get the same scores in both cases, so the starting point is worse.",
                    "label": 0
                },
                {
                    "sent": "The first one couldn't be part of the best.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Translation OK, so then you combine these paths too.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so I said the whole thing is NP complete, so this helps it actually.",
                    "label": 0
                },
                {
                    "sent": "Um cleans up the search space A lot.",
                    "label": 0
                },
                {
                    "sent": "It's really useful thing to do, but it's not sufficient, so you still have NP complete problem that goes out of hand.",
                    "label": 0
                },
                {
                    "sent": "So we now have to kind of bite the bullet and do some drastic action so we have to look at some of the hypothesis and say so some of them have to go.",
                    "label": 0
                },
                {
                    "sent": "So the way we do this, we organize hypothesis into stacks.",
                    "label": 1
                },
                {
                    "sent": "Which are technically not stacks.",
                    "label": 0
                },
                {
                    "sent": "There's really like bins, and you could organize some different ways.",
                    "label": 0
                },
                {
                    "sent": "So typical thing we do it, I've implemented these things is I just put things into stacks that have the same number of foreign words translated.",
                    "label": 1
                },
                {
                    "sent": "So these are all the translations that have three foreign words translated so that someone can parable.",
                    "label": 1
                },
                {
                    "sent": "You could also organize steps until how have they actually translated the same foreign words.",
                    "label": 0
                },
                {
                    "sent": "The only problem with that there is exponential.",
                    "label": 0
                },
                {
                    "sent": "A number of possible ways to cover input sentence, so you can't do that without taking some other shortcuts.",
                    "label": 0
                },
                {
                    "sent": "And then you do pruning and then we just do two different ways.",
                    "label": 1
                },
                {
                    "sent": "So histogram pruning implies, well, you just look at the stack and you only keep the 100 best an threshold pruning means that, well, you know what the best hypothesis, taxes and you don't want to have a positive mistake that are much worse than that.",
                    "label": 0
                },
                {
                    "sent": "So you have some scaling factor here maybe 0.01 and say everything that is worst by that factor gets thrown out.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the nice thing with that stack decoding it kind of makes it very nice.",
                    "label": 0
                },
                {
                    "sent": "Then explain the decoding algorithm in different way.",
                    "label": 0
                },
                {
                    "sent": "See if all your hypothesis organizing your stacks.",
                    "label": 0
                },
                {
                    "sent": "So the first step is all the hypothesis that one word translated in the second step for all the hypothesis have two words translated and decoding kind of means while we go through all the hypothesis in the stack, apply a phrase translation and then well either you translate one foreign words then go to the next step where you translate to form words.",
                    "label": 0
                },
                {
                    "sent": "Then you go.",
                    "label": 0
                },
                {
                    "sent": "2 steps further down you kind of fill up the steps further down, and once you're done with all the hypothesis in one stack, he can move to the next step.",
                    "label": 0
                },
                {
                    "sent": "So it's very kind of nice algorithm.",
                    "label": 0
                },
                {
                    "sent": "So should all be able to implement that now.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's one trick and one problem.",
                    "label": 0
                },
                {
                    "sent": "If you just look at hypothesis in their scores.",
                    "label": 0
                },
                {
                    "sent": "Look at this example so you could translate.",
                    "label": 0
                },
                {
                    "sent": "You could do the right thing and start at the beginning of the sentence and translate Maria not into Mary.",
                    "label": 0
                },
                {
                    "sent": "Did not.",
                    "label": 0
                },
                {
                    "sent": "The translation model will probably say, yeah, that doesn't look too bad.",
                    "label": 0
                },
                {
                    "sent": "Reordering model is very happy because you didn't do any reordering and the language model says, well, Mary is a bit of a advert but did not just kind of OK.",
                    "label": 0
                },
                {
                    "sent": "But he could also just jump to somewhere in the middle of the sentence and translate.",
                    "label": 0
                },
                {
                    "sent": "Allah is the.",
                    "label": 0
                },
                {
                    "sent": "So the gain, same story.",
                    "label": 0
                },
                {
                    "sent": "The translation model will say, well, that's a good translation for the language model.",
                    "label": 0
                },
                {
                    "sent": "Say that's fantastic.",
                    "label": 0
                },
                {
                    "sent": "I like sentences.",
                    "label": 0
                },
                {
                    "sent": "Start with the maybe 10% of all English sentence.",
                    "label": 0
                },
                {
                    "sent": "Start with that.",
                    "label": 0
                },
                {
                    "sent": "That's the best way to start a sentence.",
                    "label": 0
                },
                {
                    "sent": "The reorder model is not going to be happy that he jumped 8 words, but it's basically a balance of all these things and typically the probability you incur by jumping Edwards and then producing the word there is going to be much, much better than translating.",
                    "label": 0
                },
                {
                    "sent": "Mary did not, so it would come down to those two hypothesis and I would have to throw away the verse hypothesis.",
                    "label": 1
                },
                {
                    "sent": "Well, I would throw away the Mary did not hypothesis so.",
                    "label": 1
                },
                {
                    "sent": "You could say the second one, just you know it takes the easy part of the sentence first.",
                    "label": 0
                },
                {
                    "sent": "And it will at the end of the day, will go back to marry and have to marry and still have to translate Maria.",
                    "label": 0
                },
                {
                    "sent": "So we shouldn't only consider the probabilities we factored in so far.",
                    "label": 1
                },
                {
                    "sent": "We should also consider well how hard is it.",
                    "label": 0
                },
                {
                    "sent": "Translate the rest of the sentence so this is what we call a future cost estimation.",
                    "label": 0
                },
                {
                    "sent": "So we basically want to figure out how expensive it is to translate the rest of the symptoms.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a bit of a true problem.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you knew how expensive it is to translate the rest of the sentence, well, we would just do that so you can perfectly know how expensive it will be to translate the rest of the sentence.",
                    "label": 0
                },
                {
                    "sent": "But you could get some.",
                    "label": 0
                },
                {
                    "sent": "Good estimate of it maybe?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is useful?",
                    "label": 0
                },
                {
                    "sent": "So how do we do this for each translation option where you translator input phrase to the output phrase, you can always know well you know the translation costs, so that's in the translation table and you don't know the language model costs, and that's where it gets tricky, so you have to have some estimate how is that language model going to like to the well of course depends on where in the sentence you put together.",
                    "label": 1
                },
                {
                    "sent": "So we do something simple here.",
                    "label": 0
                },
                {
                    "sent": "We just say, well, how like these were two in English and how like users with the follow the word 2 and that's the two things.",
                    "label": 0
                },
                {
                    "sent": "So we have a simpler language model score there.",
                    "label": 0
                },
                {
                    "sent": "So here we kind of doing doing some guesswork.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so each input phrase has money based.",
                    "label": 0
                },
                {
                    "sent": "Different translations will ultimately we're optimistic we're going to choose the best translation, so we only care about the Hyest were the best cost.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So you can compute them for each input span.",
                    "label": 0
                },
                {
                    "sent": "What is the best way to cover that with the translation option?",
                    "label": 0
                },
                {
                    "sent": "And it doesn't take much more to then say, well, those three words first 3 words.",
                    "label": 0
                },
                {
                    "sent": "Well, there's no translation option that covers the first 2 words, but there are.",
                    "label": 0
                },
                {
                    "sent": "I can I know.",
                    "label": 0
                },
                {
                    "sent": "What are the best ways to translate the first word in the mobile space way?",
                    "label": 0
                },
                {
                    "sent": "Translate the second word and what's the best way to translate the third word?",
                    "label": 0
                },
                {
                    "sent": "And also, what's the best way to translate the 2nd and 3rd work together and just based on these costs, I can figure out well what's?",
                    "label": 0
                },
                {
                    "sent": "A reasonable estimate, how expensive it will be to cite the first 3 words so you can actually precompute for all contiguous spans.",
                    "label": 0
                },
                {
                    "sent": "And they only N squared spans, so it's not too much.",
                    "label": 0
                },
                {
                    "sent": "You can compute come some estimate.",
                    "label": 0
                },
                {
                    "sent": "How expensive will it be to translate that part of the sentence?",
                    "label": 0
                },
                {
                    "sent": "So the nice thing is, yeah, you can do that before you do decoding, so you can actually have a look up table then says OK, this here I still have to translate word 6 to 7.",
                    "label": 0
                },
                {
                    "sent": "How expensive will that be?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can just look it up.",
                    "label": 0
                },
                {
                    "sent": "So what we ultimately do when we we compare hypothesis while we do what we did earlier, we created this hypothesis and have all the probabilities and all the other scores we multiplied up in there.",
                    "label": 0
                },
                {
                    "sent": "And then we have also the future cost estimate.",
                    "label": 1
                },
                {
                    "sent": "So we here in this case we still have to translate the word no.",
                    "label": 1
                },
                {
                    "sent": "And the last four words and we can look up how expensive will there likely be.",
                    "label": 0
                },
                {
                    "sent": "Energised combine all these together and this product of the costs of our times, the.",
                    "label": 1
                },
                {
                    "sent": "The future cost estimate is the basis of the pruning.",
                    "label": 0
                },
                {
                    "sent": "So based on this score we throw out the good or bad hypothesis so that overcomes the problem I had earlier where if you translate the easy part of the sentence first, well the future cost estimator says, well, the rest is still ICS.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "This is related to technique called a star, which is very popular in AI.",
                    "label": 0
                },
                {
                    "sent": "So just wanna reference with certain.",
                    "label": 0
                },
                {
                    "sent": "There's actually a version of this decoding that is safe.",
                    "label": 0
                },
                {
                    "sent": "Where you actually don't make any search errors.",
                    "label": 0
                },
                {
                    "sent": "But again, that means it's ultimately NP complete.",
                    "label": 0
                },
                {
                    "sent": "So the trick is that the future cost estimator, if that is either accurate or underestimate you can and you only throw away hypothesis that are worse according to this estimate, then some known solution of the sentence.",
                    "label": 1
                },
                {
                    "sent": "So you basically do a best first search, you just try to find one translation.",
                    "label": 0
                },
                {
                    "sent": "So you have one score and then you can compare all the other hypothesis against that score.",
                    "label": 0
                },
                {
                    "sent": "You can basically then compare for any other hypothesis its probability.",
                    "label": 1
                },
                {
                    "sent": "Times future cost estimation is that worse than that known best translation.",
                    "label": 0
                },
                {
                    "sent": "Then you can throw it away because the future cost estimation is either accurate or under estimate.",
                    "label": 0
                },
                {
                    "sent": "So if that's already higher than a known solution then you can throw it out.",
                    "label": 1
                },
                {
                    "sent": "So this is typically not done in decoders nowadays.",
                    "label": 0
                },
                {
                    "sent": "Better to have a heuristic that is not guaranteed to be accurate or under some it, but it's an estimate that is realistic in some way.",
                    "label": 0
                },
                {
                    "sent": "But of course you make mistakes there, so there is the issue there.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your search error.",
                    "label": 0
                },
                {
                    "sent": "OK, we do a couple of other things to speed up decoding.",
                    "label": 1
                },
                {
                    "sent": "So one thing is you can limit reordering.",
                    "label": 0
                },
                {
                    "sent": "Reordering is actually the piece that makes the whole thing NP complete.",
                    "label": 0
                },
                {
                    "sent": "If you wouldn't have reordering you would have the same situation with speech recognition, make and run Hmm's and you have a polynomial runtime and everything is beautiful.",
                    "label": 0
                },
                {
                    "sent": "So you could do the same thing and say, well, there is no reordering, so you just do monotone translation.",
                    "label": 1
                },
                {
                    "sent": "That's usually not a good idea.",
                    "label": 0
                },
                {
                    "sent": "I usually get worse performance with that, but what we do do is we limit reordering you only say well you can only jump around.",
                    "label": 1
                },
                {
                    "sent": "Let's say at most 6 words at most 8 words.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "And that already cuts down the NP complete problem into a polynomial problem.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's usually also helps with translation quality because the models are not very good at long distance movement anyway.",
                    "label": 1
                },
                {
                    "sent": "We could get into detail for that later, so we usually do better if we don't allow too much reordering.",
                    "label": 0
                },
                {
                    "sent": "'cause if you allow too much reordering, too much crazy stuff happens.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's all there is.",
                    "label": 0
                },
                {
                    "sent": "If you have a fixed step size and you limit reordering to a certain reordering window, then the decoding algorithm that I explained you is actually linear in the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "It's kind of intuitive.",
                    "label": 0
                },
                {
                    "sent": "You have always, you know, no matter how long the sentence is, hoops during decoding, you always have a stack full of let's say 100 hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You have almost always the same number of translation hypothesis that are translation options that apply, and that doesn't change.",
                    "label": 0
                },
                {
                    "sent": "Distance gets longer and longer.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, few more things.",
                    "label": 0
                },
                {
                    "sent": "When do we actually have a break by the way?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, then I will finish up 1 two more slides.",
                    "label": 0
                },
                {
                    "sent": "So remember when I drew this graph and said, well, we still want to keep these pointers.",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing with that is that we can convert the search graph you produced into phrase lettuce or word lattice of possible translations.",
                    "label": 0
                },
                {
                    "sent": "Why do we want to do that?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we can produce things like this here.",
                    "label": 0
                },
                {
                    "sent": "The investors.",
                    "label": 0
                },
                {
                    "sent": "We cannot not only produce the best translation, but also the second best third best, 4th best and so on.",
                    "label": 0
                },
                {
                    "sent": "Any number.",
                    "label": 0
                },
                {
                    "sent": "Abuse at various for various purposes.",
                    "label": 0
                },
                {
                    "sent": "One solution, one way you could.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance, say that, well, you have.",
                    "label": 0
                },
                {
                    "sent": "The translation model improves, then best list and then you have all kinds of additional knowledge you want to throw in.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to Paris all the possible translations and then you use the power scores.",
                    "label": 0
                },
                {
                    "sent": "Additional feature to figure out what are the good trans.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very bad translations.",
                    "label": 0
                },
                {
                    "sent": "So this is produces with a made up translation model, but a real language model.",
                    "label": 1
                },
                {
                    "sent": "The top translations for a German sentence and the first sentence of the right translation.",
                    "label": 0
                },
                {
                    "sent": "I guess this is a small house.",
                    "label": 1
                },
                {
                    "sent": "Or is it?",
                    "label": 0
                },
                {
                    "sent": "This is a little house.",
                    "label": 1
                },
                {
                    "sent": "I don't know the difference between small and little.",
                    "label": 1
                },
                {
                    "sent": "Seems to mean the same thing, so these are good translations and you kind of go down the list and the last one is.",
                    "label": 0
                },
                {
                    "sent": "This house is a small that sounds a bit funny.",
                    "label": 0
                },
                {
                    "sent": "So so this and you see kind of how the scores kind of go down.",
                    "label": 1
                },
                {
                    "sent": "See the yeah reordering score so you only have a reordering score.",
                    "label": 0
                },
                {
                    "sent": "If there is reordering the first sentence now reordering you see the language model scores.",
                    "label": 0
                },
                {
                    "sent": "The language model doesn't like that last sentence either likes it better than it's a little house.",
                    "label": 0
                },
                {
                    "sent": "Yeah, translation model and word penalty.",
                    "label": 0
                },
                {
                    "sent": "That's just concert honey words.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think that's a good place.",
                    "label": 0
                },
                {
                    "sent": "Good time to stop them.",
                    "label": 0
                }
            ]
        }
    }
}