{
    "id": "ncipym6uihuyor344lhybkwetkf4ehj7",
    "title": "Variational Autoencoder and Extensions",
    "info": {
        "author": [
            "Aaron Courville, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_courville_autoencoder_extension/",
    "segmentation": [
        [
            "Type alright so.",
            "I guess I spoke to you guys about a week ago.",
            "In fact actually.",
            "Just over a week ago about undirected graphical models in our BMS and since then I believe you've had some.",
            "I guess Russ talked about deep Boltzmann machines, that's happened, and so there you're going to see basically the what I would view as the sort of the.",
            "I guess the sort of high watermark I guess, of undirected graphical models, at least where they stand right now.",
            "And I guess hung like also talked to you about undirected graphical models, or at least hybrid graphical models and sense of these stacked RBM's.",
            "So I'm going to talk to you about a different line of research.",
            "That's something that's been around for a little over a year now, and I think it's very exciting.",
            "And we've done a little bit of work in this.",
            "I'll talk about a little bit of the stuff we've done, but mainly what?"
        ],
        [
            "I want to do is sort of go over the basic introduction of the variational autoencoder, something it's actually comes by a few different names, but we've sort of well.",
            "I guess I like the term variational autoencoder for this so, so that's what I'm going to use.",
            "I'm going to talk about how it's you, how it can be used in a semi supervised setting.",
            "That's just so each one of these bullet points is essentially a paper that's been out in the last year or so.",
            "So we have a semi supervised learning with the VA and we then we have a sequential application of the VE.",
            "This is our model that's there's an archive paper out right now we call the variational recurrent neural net.",
            "Then we have the draw model which is very exciting.",
            "This one actually was published before this one, but for pedagogical reasons I'm actually going to be presenting it.",
            "Second, at least after this one and then we have two papers here that go into sort of improving how you do inference in these kinds of models.",
            "And they were both in the most recent ICML, so if we get if we have time to get to them, I think we will will cover these so.",
            "This is a lot of material and I'm not going to go over any of it in detail.",
            "My my my ambition is just to give you guys sort of.",
            "What are the key ideas in each one of these and sort of give you a sort of a state of where this field is right now.",
            "Alright, so let's get star."
        ],
        [
            "So a variational autoencoder, it's sort of in the family of deep directed graphical models.",
            "So we talked about undirected graphical models last time.",
            "This is like a week ago, and now we're talking about directed graphical models.",
            "So the idea this this.",
            "The.",
            "This is second.",
            "Right, so the variational autoencoder model is essentially simultaneously discovered by two groups.",
            "By Kingman dwelling here and then a group from the deep mind actually.",
            "And it's essentially I think they published later, but they both had archive papers out, essentially around the same time.",
            "So unlike our BMS and DBMS, here are interesting deep directed graphical models.",
            "So the question is why deep directed graph malls versus?"
        ],
        [
            "Undirected graphical models well to sort of motivate this, I'd like to talk a bit about what do we want.",
            "A latent variable model to do.",
            "So last time after after my talk I had a few students up at the front asking questions and.",
            "We're talking about our BMS and sort of, you know, stacking them and and one of the things I mentioned to them was that one of the reasons why I'm a little less enthusiastic about GBM models is actually comes from my own experience trying to train stacks of these models and it comes down to a question of what do we want our latent variables to do.",
            "We, I think what we want our latent variables to do is actually model natural variations that occur in the world, right?",
            "So if we want to model an image, for example, we want to have our latent variables actually track variations you see in the image when you do something like stacking RBM's together, what you have at the low layer is a sequence of latent variables like say for example the bottom layer model is an RBM.",
            "You have a set of random variables there that are very very close to the data.",
            "Variations in that layer.",
            "Often gets you what I'll say.",
            "I'll use this term informally, puts you off the data manifold right?",
            "Because you have to in order to sort of stay on the data manifold, you have to.",
            "Perturb these variables in a very structured way, so just sort of normal variations that you might see in the in that top layer.",
            "Push you off the manifold so in this sense you're not necessarily what I would say is.",
            "You're not necessarily modeling.",
            "With you're not using those latent variables to actually track the variations along that manifold, which is actually what you want them to do.",
            "So one question is, well, how do you actually manage to do that?",
            "Yeah.",
            "Right, and so to the extent that it is focused like to the extent that it's a deterministic mapping, then yes, absolutely this will workout and and to the extent that you have noise at that level, yes, that will work.",
            "But what I'm arguing is that.",
            "And I think you know we're starting to collect some evidence in this direction that says that you are better off doing something else, not necessarily modern modeling.",
            "Stochastic city at that low level, but more likely modeling stochastic that had a higher level, which is what something like the Villa allows you to do right?",
            "So the VA, the idea behind the VA is to actually.",
            "Propagate up.",
            "So right so we'll just back to the slides here.",
            "So so we want our latent variables to live in some space.",
            "That's relatively simple an encode a meaningful things.",
            "And then this is sort of the directed graphical model view, right?",
            "So we're sort of imagining causes out in the world, and we're going to project those down into our data manifold.",
            "And now we're thinking about things like images and highly structured things, right?",
            "So, so this is going to be a pretty nonlinear manifold, and so in order for this for you to manage that projection, this actually has to be a pretty complicated nonlinear transformation.",
            "So that's what we're going to be talking about.",
            "Here is we're going to be focusing on models where this is actually something fairly complicated, unlike what you see in GBM and the problem with an argument you can say, well, you can stack our BMS.",
            "And that's fine.",
            "Yes, you can do that, but training this thing involves training those lower layer models.",
            "And and doing that is actually gets complicated and we can go into reasons if you like afterwards in first time for questions.",
            "But basically it comes up to you have trouble with the mixing when you're trying to train these models 'cause you have to learn these things with a generative model going on in the background.",
            "If you're using PCD persistent contrastive divergent and so we end up having sampling issues there, it's just a much more difficult thing, much more natural if we can just train a directed graphical model to do this.",
            "So what we're hoping is that we actually can learn latent variables that can be coupled.",
            "The true factors of variation.",
            "This is a bit of a pipe dream, right?",
            "Like this, we want to actually discover the true underlying factors of variation in a given data set.",
            "This is a very ambitious task, but we're going to see how far we can get along that."
        ],
        [
            "Action right so so the way we're going to do that is in the context of a variational encoder.",
            "We have our deep graphical model here and we're going to start with some latent variables here and then some complicated transformation and then the data here.",
            "That's our G here, right?",
            "So, so this is the basic structure we're going to use, and we want to basically the trick behind the variational autoencoders that we're going to leverage in neural network structure to map between our latent variables and our data.",
            "That's it, this is our generative model.",
            "And then we have a complicated.",
            "It's complicated as we like relationship between our latent variables and our data in the generative sense.",
            "Yeah.",
            "It's deterministic, it's this this graph.",
            "Yes, this part of the model is deterministic, right?",
            "So that's kind of hidden in here.",
            "I'm being a bit sloppy in my notation.",
            "X is a random variable, zed is a random variable, and what's in between here is a deterministic mapping.",
            "That's pretty important, right?",
            "Because what I was arguing is that variations say in a place like here, which is close to the data will actually in something in highly structured data like like images, will actually push you off that manifold.",
            "I mean, it's possible if you have like.",
            "Noisy images where variations you know you have kind of grainy images, for example.",
            "Then you could imagine that noise at this low level.",
            "Israel is important and relevant most of the time my experiences been, it ends up trying to model it that way ends up hurting you more than it helps.",
            "Right, so first of all, let's just see what we can do when you when you add this generative model here too.",
            "As a encoding a neural net, having your latent variables be separated from your data."
        ],
        [
            "What can this kind of?",
            "What kind of this kind of model do?",
            "Well, what I'm plotting here is a 2 dimensional latent space, and then the data.",
            "So for example, if I were to sample appoint here in Z1Z2, this is my zed space here and then push that through my neural net I get a zero like looking thing right here, and if I were to do the same thing here I would get this face here, right?",
            "So what this is giving you is kind of a chart of how you map from the latent space to the input.",
            "And what's interesting is, well, first of all, and emnace that you're actually able to.",
            "To model all of the digits and you see this kind of these smooth transitions because zed I haven't told you this, but that is a real valued quantity.",
            "It's a 2 dimensional real valued quantity.",
            "So as I'm moving from one to another, you sort of transitioning through them, but one of the things that's interesting here is that if you look at this right, this plot right here, this is actually a model that was trained with the phrase faces, which is essentially a bunch of images of Brendan Frey here.",
            "So what you can see is that the latent variables here are actually doing something like.",
            "Discovering these latent factors right 'cause what you what you have is X zed one here encoding something like pose.",
            "Sorry Zed, one encoding something like pose and Z2 encoding.",
            "Something like expression.",
            "So we're actually seeing this, and actually I don't.",
            "Unfortunately, I'm not going to be able to show you this, but a student of mine, Vincent Millay, has actually done some other experiments with this kind of thing.",
            "Training on a bigger face data set.",
            "TFD and this kind of effect holds up.",
            "You've got latent variables that actually can change the mouth per person.",
            "You can sample persons that face and then change one latent variable and watch them smile and then have the smile go away when you move it in the other direction.",
            "It's pretty exciting now the question is OK, so this is what it can do.",
            "Fairly promising, but."
        ],
        [
            "Does it do it?",
            "So the question really is because in graphical models we've always known that you can.",
            "You can generate from them pretty easily, right?",
            "It's just ancestral sampling you sample from Zed.",
            "This is how we did this.",
            "We have some simple prior over zed and we sample in this case we're just pushing it through our MLP and then we sample from X given the parameters given these values here the difficulty with graphical models and why we as a field turn to undirected graphical models is because of inference inference in these models.",
            "Is is difficult, right?",
            "So that's the whole problem is, once you, how do you even train these models right?",
            "Because you need some sort of correspondence between X&Z and the typical way to get that correspondence when training is to do inference.",
            "So I give you an X.",
            "You recover some zed and then you train the model to optimize to match those better.",
            "But now the question is we can't recover this posterior very easily.",
            "In general, it's complicated, and one of the advantages of something like deep Boltzmann machines is that the variational inference in that context is actually a fairly decent approximation.",
            "In this context, with the directed graphical model, even variational inference is difficult in general, but at least that way of applying variational inference.",
            "So what we're going to do with the variational."
        ],
        [
            "Autoencoder is something else.",
            "We're still going to use a variational approach, but the way we're going to do this is we're actually so yeah.",
            "So first, we're just we're actually going to use a variational approach, so we're going to find some posterior Q here, which is just Q of zed given X and we're going to define our bound so I don't know to what extent you've seen this kind of thing.",
            "The elbow.",
            "Sometimes it's called the variational lower bound, so we can define using some approximating distribution.",
            "And this is true for any distribution Q here.",
            "That we can find a lower bound here, and the trick is just to make sure to have the best Q such that this lower bound is as good as it can be, as close to P of X as it can be.",
            "So this turns out to be.",
            "Right, so you can express this as our lower bound.",
            "This is always lower bound no matter what Q we use and what the variational autoencoder does and what we the way we are going to look at the variational encoder.",
            "This we're going to rephrase this a little bit into a form that looks like this.",
            "And what is this?",
            "So we haven't told you how we're getting our Q yet, but if we just look at what this for what this looks like here.",
            "What we see is we have a term that looks an awful lot like a reconstruction term in a typical auto encoder.",
            "And we also have a term which we're going to call the regularization term here.",
            "Which is just this, KL divergences between our posterior.",
            "And the prior here P of Theta.",
            "So again, we have this term.",
            "This is actually why we're calling this the variational autoencoders, because it's got this flavor that looks very much like an auto encoder, right?",
            "So we have it's, and in this case it's a nonlinear auditing coder, because or like deep autoencoder, if you like, because this model here can be as deep as you like, and then we're going to.",
            "We're just taking expectations with respect to Q, which again we don't know what it is.",
            "And we have this other term which is the KL divergent between this Q&R prior, which is something simple, right?",
            "We know this we're going to specify this is just something.",
            "Simple, like a Gaussian distribution, is actually what's often used for prior onset here?",
            "Alright, so now what is Q?",
            "Well, one of the things that's one of the things that's really innovative about the variational autoencoder, is it actually treats.",
            "Variational inference in a very different way than typical right?",
            "So the typical way you do variational inference is kind of a non parametric way, meaning that what you're doing is you actually sort of do a little optimization for each X that you have.",
            "You tried to fit Q and so for that you need some sort of tractable family of Q and you fit the best you typically like mean field.",
            "So queues the zeds would be independent under mean field assumption and you fit the best independent distribution you can for that X."
        ],
        [
            "Variational unquote is something very different.",
            "It uses a second neural net structure, second only to actually predict the parameters of your queue, so it's important to note that this will the assumption we're still making here is that it's actually going to be mean field, or the Zeds are going to factorize, but it's still the conditional distribution here is interesting, because now we're just going to input X and we're going to come up with the parameters of a distribution over Q.",
            "Here.",
            "So we've got a neural net encoding, sort of in the decoding model here, and a neural net in the encoding model here in Q.",
            "And we have our optimization function, so this is the basic structure of the VE.",
            "There's one more trick that you need to know about to make this thing work."
        ],
        [
            "And that's what we call the reparameterization trick.",
            "So how are we going to train this thing, right?",
            "We know how to do inference now, assuming we can train this model, we know how to do inference in this thing, and we can generate from this model.",
            "So the question is, how do we train it?",
            "Well, what we're using is, we're going to use this memorization trick, which says that we have this Q of said given X is just, we're going to find this thing to be a normal distribution with some mean in some variance, which is a function of X.",
            "But it's also these things.",
            "Are parameterized by this neural net here our inference model, and we're going to parameterise zed as being in this kind of a fashion here, right?",
            "So this is just a way to parameterise this normal distribution, so you have some mean plus some variance times epsilon, which is just a random draw from the standard normal distribution, so why aren't parameterized things this way?",
            "Well, the advantages is that now we have a way of back cropping, so here this is just a neural net.",
            "We can back propagate error all the way through here.",
            "And by the way, we can do the same thing here.",
            "This is actually optional.",
            "You don't need to do this.",
            "You can have any kind of standard distribution on X.",
            "Here can be discrete continuous.",
            "That's not a problem, but we can now back prop through here.",
            "Usually the problem is is that we get to zed and we can't back prop through.",
            "But now with this kind of parameterisation we actually can, we can back propagate through these parameters just fine.",
            "We don't propagate back propagate through epsilon, but that's fine.",
            "It's a standard Apple, and there's no parameters to learn there, so we back propagate through mu and Sigma and then we can learn the parameters of the.",
            "Encoder model so this gives us a way to learn the decoder model and the encoder model, and we can optionally also if we want to parameterise the input this way.",
            "Are there any questions up to this point, yeah.",
            "Yes, yeah yeah.",
            "So the good points OSI has to be continuous right?",
            "So what I've given you is the option of it being Gaussian.",
            "That's one option, but it certainly has to be is continuous in order to do this back propagation.",
            "If you want it to be discrete, you'd actually have to use some other technique like the reinforce algorithm or something like that, But that's kind of typically not what to use in the VE context.",
            "Any other questions, yeah?",
            "Yeah, this is just a.",
            "This is just a deterministic mapping from zed 2X.",
            "And if you like it's it's, it's just a neural net, right?",
            "You've got you literally take zed as your input.",
            "And you propagate that through and then you have parameters if you will, from which you can samples at which define a distribution that samples X. Szeto said the directed model is said to X.",
            "That's it, that's the directed model, so you're right.",
            "So this is.",
            "This is kind of.",
            "This is, I guess, confusing in the sense that it's showing a directed model through these layers, but it's actually the directed graphical model.",
            "Here is Zed arrow X.",
            "And that's it, yeah.",
            "Exactly.",
            "That's exactly right.",
            "Yeah.",
            "In this case, yes, yeah, But that's going to be true always for I mean, this part is the decoder network generating sufficient statistics.",
            "You can, yeah, I mean, you're just parameterising some low level distribution here.",
            "And this is generating the parameters for that.",
            "Yeah, functions for that and the same is true over here, right?",
            "So you start with some X and you generate conditionally the parameters of a distribution.",
            "Now in this case it actually has to be something like a continuous distribution.",
            "In this case we're talking about a Gaussian, because that we need to be able to back prop through.",
            "Makes sense.",
            "Yeah.",
            "But the pre parameterisation trick is just expressing this.",
            "Distribution in this form.",
            "Right, so so that we can actually back so we have, uh, where this is a random variable and we're writing it as a function of a deterministic component and then the sum of the deterministic component times something purely stochastic.",
            "So the stochastic element we can't backdrop through, but both of the deterministic parts we can.",
            "And that's yeah.",
            "Yeah.",
            "That's right, that's right.",
            "Yeah, that's an important point, so if you."
        ],
        [
            "Get the if you go back and you look at the this term here right, you see that this term is going to want this decoder to essentially be able to reconstruct the individual example as best it can.",
            "So so in this term is essentially acting to kind of make the set as unique as possible in order that it can reconstruct the X as accurately as possible.",
            "On the other hand, this term here you see it's being regularizer, it wants this posterior distribution to be as close to P as possible.",
            "Sorry, the prior on Zed.",
            "As possible, So what that's going to do is essentially do the opposite.",
            "It's going to sort of push them together and make them as noisy as possible.",
            "You essentially this term kind of is happiest when when the output of Q here is independent of X and it's this interplay between the two that actually gives you the posteriors matching.",
            "The prior this P of zed here.",
            "That's typically something simple like a Gaussian distribution on set, an independent Gaussian distribution that we can sample from easily.",
            "Yeah, that's that's a good point so.",
            "Yeah.",
            "Yeah, that's right.",
            "I think that's a good way to see it.",
            "And just like that, so there's a sort of a temptation here.",
            "When working with this to sort of look at this term and think that.",
            "You want this thing to actually kind of match.",
            "You actually don't write, it's just like it's just like any regularization term.",
            "You don't actually want it to go to the optimum of that term, right, so?",
            "Same thing here, right?",
            "This is just the other part of the term and the actually the other thing that's nice is that this this sort of division into this reconstruction term plus a regularizer just comes from the variational lower bound.",
            "We've added nothing to it.",
            "These two pieces are just a reformulation of that same variational lower bound that everyone uses when they do variational methods.",
            "So so connect this to the the contractive automotive.",
            "You guys have seen the connector contractive, autoencoder, have you.",
            "Yeah, OK, so so it's similar.",
            "Yeah?",
            "I mean it's.",
            "I guess one difference would be that I mean you could apply.",
            "Typically that's applied at every layer right here.",
            "It's only being applied at one point here, and the effect tends to be that it's a kind of a choke point.",
            "We're going to see in a minute the effect of this regularization is actually pretty dramatic, so in some sense it's may be used in a different regime then contractive auto encoders.",
            "Yeah.",
            "OK yeah, sorry."
        ],
        [
            "So here.",
            "This term yeah, OK.",
            "So.",
            "So the denoising auto encoder will.",
            "If you fix this to one, then what you have is a.",
            "You still have a regularization effect.",
            "That's happening on the mean here.",
            "This is actually still being pushed towards the zero basically, or the mean of you're still getting the this."
        ],
        [
            "Sorry, this term is still acting on the mean parametrization.",
            "Right, right, but I'm right exactly, but what I'm saying is it isn't isn't strictly the same thing as a denoising autoencoder.",
            "If you fix that term.",
            "Yeah.",
            "Right, so I think the main points where this crucially differs is that this is a way to use all of the leverage.",
            "All of the optimization methods, all the things that we've learned about neural Nets in the Latin, what you've learned about neural Nets in the last week and a half to build directed graphical models.",
            "Then that you don't get, at least simply from denoising autoencoders.",
            "Yoshua, I don't know if you're going to talk about this, the your generative at.",
            "Sorry your GSN's.",
            "OK, so you'll learn about how you can actually get other generative models out of things like denoising autoencoders, so that's actually a bridge between these kinds of methods.",
            "But what we're really getting out of this technique is actually maybe more innovative if you think about from the generative model POV.",
            "That's right, that's right.",
            "So in a denoising autoencoder, you don't usually have these probabilistic semantics.",
            "Sort of given to you, at least immediately.",
            "So you can't necessarily think about Zed as being in that context is being a random variable, whereas here we can.",
            "And that's actually pretty powerful.",
            "And the other thing is at least we haven't tended to see too much of the kind of distance.",
            "Kind of extracting these factors of variation or disentangle in these factors of variation, whereas in this case it's actually pretty extreme.",
            "You see it, and it's it can be fairly dramatic, will get into a second of the kind of thing we're seeing in that just in terms of the kind of the effect that that prior has on zed.",
            "Oh yeah.",
            "Yeah, yeah, that's right yeah, so in that context that is.",
            "Yeah, so that is similar.",
            "I've not seen anyone actually compare those like this from the distant angling that this kind of model gives you versus the disentanglement.",
            "Something like that would give you I've not seen that compared, so that would be interesting, I agree.",
            "Well, yeah.",
            "Noise.",
            "Big difference.",
            "Yes, that's exactly right, yeah, so thank you good good yeah, that's a very good way to say, yeah, so that's that's why I'm saying the big loss function is different and that's what's giving rise to a different kind of regularization here.",
            "So specifically in the example he gave, you have the you have the.",
            "The variance term was fixed, but this term still acts on the main component, right to regularize that.",
            "Yeah.",
            "Yeah.",
            "The pixel values are multiple.",
            "Values.",
            "Yeah OK I sounds reasonable.",
            "OK, yeah.",
            "But but that's.",
            "Yeah.",
            "Right, so so just to be clear, if you if you have say for example just reconstruction error like least squares reconstruction error, right that content you can see that you can interpret that as just having a Gaussian distribution over X&X being random variables.",
            "And that's actually that is the easy part, right?",
            "What's what's difficult in?",
            "This is actually getting, well, difficult.",
            "The technical trick was that was used as is having some distribution over said that we can back propagate through in order to train our inference net.",
            "'cause that's what's without the inference that you can't train your generative, not your decoder.",
            "Yeah.",
            "Yeah.",
            "Right?",
            "Right?",
            "Yeah, that's that's true.",
            "Yeah yeah, it's it's in the statistics community.",
            "It's it's a very old trick.",
            "Yeah, in fact, I mean in almost every community, right?",
            "I've seen in in the.",
            "I mean, it's a standard way of writing, driving things like the common distribution to actually.",
            "So it's.",
            "It's a very standard thing to do.",
            "It's just for whatever reason we did.",
            "Not until these guys, these two groups thought of doing this.",
            "We have not thought of doing it in this context."
        ],
        [
            "Right?"
        ],
        [
            "OK, so now we're getting to how to train this model and we basically already discussed this because we have this reparameterization trick we can simelton train this, simultaneously train both the generative model and the inference model just by standard back propagation.",
            "So we forward propagate through the entire model going from X to X had some or some we can sample from this, or we can just actually look at the reconstruction error under our distribution that we generate from zed, so we actually have to do is sample from zed here.",
            "So we generate sample here and then generate full and then forward propagate.",
            "We derive our error and then we back propagate our error and we train it just like we would in a standard neural net.",
            "So the only real difference is that we're sort of injecting noise here.",
            "That's one way to think about what we're doing.",
            "And we have our objective function and we just do standard grading, descent or any kind of optimization methods you want.",
            "In this case, actually.",
            "So one of the one of the points, I think is worth pointing out is.",
            "Is you think about that.",
            "If you want a generative model, it's fairly deep and you're going to want an inference model that's fairly deep as well.",
            "So the the training this thing is essentially adopt training like sort of a twice that length.",
            "That depth, right?",
            "So actually you can have issues training this model if you want a very deep generative model or inference model.",
            "So tricks like I guess in fact, Vincent has found things like batch normalization turned out to help quite a bit for this.",
            "For training this kind of model so it is a bit of a challenge to train these.",
            "Another reason why inference.",
            "Training is challenging.",
            "Is that your your?",
            "Your regularization term comes in at that point, right?",
            "So it's acting purely on the this term right here.",
            "Your kid divergences acting at this point, so it's acting just on the inference network.",
            "So typically the kind of learning dynamics you see when you're training this is is this the inference model just sort of learns to match the prior on zed early on, so the distribution that you predict for no matter any X here comes to match this, and then from there it has to sort of.",
            "Dude, it's best at reconstructing.",
            "So essentially it learns to forget information about X and then it has to sort of recover that information.",
            "So that's sort of a pretty common early state in training, and then it sort of comes out of that and starts to do a decent job reconstructing X.",
            "That's a typical learning dynamic you'd see."
        ],
        [
            "Alright, so when you apply this kind of model you can compare it to other things, so this is a case where it's applied I think to relatively simple task.",
            "This actually comes from there.",
            "There one of the original papers and this is just a point saying that they've compared to wake sleep and MCM.",
            "So this is a Monte Carlo OEM using hybrid Monte Carlo here.",
            "And what we basically see is it's basically competitive with MCM which is a much more expensive algorithm 'cause you're doing many samplings from the posterior distribution here.",
            "And then you get you see you're not quite as good as, but in the context where you are doing it on full, this is in the case of MNIST when you can't do it on the full data set, you see that it gets much more because the M is basically too expensive to run in a practical way, so so it ends up doing much better, so it's a scalable method that ends up performing quite well and you can see the performance is much better than standard wake sleep.",
            "Alright."
        ],
        [
            "So if you look at what's going on, sort of under the hood of this myth of this variational autoencoder, you can see the effect of the Cal term here.",
            "So what's being plotted here across the bottom in each one of these graphs is the number of dimensions of zed that we have an in.",
            "This axis is just the KL divergent's right for that component and what you see is that for small values of zed it says relatively constant, but as soon as you have a sort of critical mass of number of components and said some of these, just start to go to 0.",
            "The KL divergent starts to go to zero, yeah?",
            "You're but each.",
            "In this case you're training a 5 dimensional zed.",
            "Yeah, so you're across the graphs.",
            "The dimensionality of zed is increasing.",
            "From here to here to hear, to hear.",
            "So what we're seeing here is that we have some components of zed, and here it's it's even more dramatic.",
            "The KL divergent goes to zero.",
            "What this means is that the output for that dimension of zed is essentially matches the prior right?",
            "So it's essentially not using that dimension to reconstruct the example.",
            "So this is a pretty common characteristic of the variational autoencoder and what it's basically saying is that you're basically it's picking a suitable dimensionality to redo the reconstruction.",
            "And then it takes the rest of those, those dimensions that you've given it and just sets them to zero.",
            "It doesn't use them, yeah?",
            "Well, so my experience is, or at least I think I think, yeah, it's possible.",
            "It's just a question of you.",
            "Don't know what that is apriori, right?",
            "You kind of have to do a few experiments and then find out what it is, but I think in general it is slightly easier to give it a few more dimensions to optimize.",
            "That's I don't know Van side.",
            "Do you have an opinion on that which you've done?",
            "Most of these experiments?",
            "Dimensions of Z being recruited for the reconstruction part of the term, and.",
            "Progress is you have more and more of these dimensions.",
            "Optimization technique.",
            "Yeah, yeah.",
            "Well, the variance is a function of the model itself, right?",
            "So it's it's difficult to.",
            "Control that essentially, I mean, at least if you want to, the way it's typically specified, you can sort of artificially impose a given variance, But then actually so.",
            "Oh, I'm sorry.",
            "The variance of the gradient.",
            "Yeah, I don't know that.",
            "Actually I don't know.",
            "I don't know if Vincent has an idea.",
            "It's possible.",
            "Yeah, you have other issues too.",
            "I don't think variance is actually the dominant effect.",
            "I think that the real challenge in training is what I mentioned before that part of your objective function comes in at an earlier point.",
            "So when training the encoder model you have a decent signal there early on in training, whereas the other part is given to you in a very distal way, like after the decoder.",
            "So by the time it reaches your encoder, there's probably not nothing very sensible to learn from that, at least initially.",
            "So I think that's what makes the the learning problem a bit more challenging here than you typically see, but I should emphasize that this actually contrained fairly fairly easily compared to other sort of methods, which which this thing is in competition with.",
            "Let's say, where you have Sir, for example, discrete variable discrete said you have to use something like reinforce in that context, variances is a real issue, but I think that the parameter representation reparameterization trick here really helps for dealing with that variance issue.",
            "Actually, so I think we had a question over here.",
            "Or yes I can.",
            "Yes."
        ],
        [
            "Right, so this is what happens if you actually look at the weights corresponding to where you see that collapse.",
            "So what I'm plotting here is the in red.",
            "Here is the KL divergent's in blue.",
            "Here it's the.",
            "It's the weight norms associated with that unit projecting out of it, right?",
            "So what you see is that where you have 0 Cal.",
            "Divergance the weight norms essentially go to 0, so they basically aren't used, they collapse and it's basically like, OK, those dimensions.",
            "We're just going to let them to cover the.",
            "Regularization term the KL divergent's and I'm going to use the rest for reconstruction so it doesn't inject anymore noise than necessary into the reconstruction.",
            "Yeah.",
            "Exactly that's and that's I think that's a really important difference.",
            "Actually, the support doesn't change in this case, so really what we're talking about your.",
            "It's more like you know, like a nonlinear principal components analysis, where you're kind of put going through this bottleneck and the bottleneck is actually chosen by the model itself, so you can almost think of it like a kind of a its own regularization where it's picking its own internal dimensionality.",
            "Now I mean, so this is kind of you can see this is a good thing, right?",
            "Oh, it's picking it's doing its own regularization in terms of dimensionality.",
            "That's wonderful, right?",
            "But on the other hand, you can also see this as maybe a negative right.",
            "Maybe you actually want something that looks more like sparse coding, where where it's sort of dynamic, right?",
            "'cause you think you probably have a bit more capacity if you were to do that, not force it through some some static bottleneck like you have here.",
            "Thanks yeah.",
            "So so certainly we have seen that that's actually how training progress is.",
            "I don't know if you have an opinion on whether that's still the case that we find with batch normalization, but I I know that we've seen in the past where that's essentially how it how it goes.",
            "It collapses a bunch of terms and then slowly adds them, but from a computational point of view, there's there's no game 'cause you're running on all of them.",
            "Yeah.",
            "Right, but you're.",
            "Yes, so I don't years.",
            "There's going to be much of a competition that benefit 'cause if you think about this is on emnace, right?",
            "So we're using something like, you know 500 hidden units in the in the MLP and then it goes down to something like 20 Z.",
            "So there's a pretty big order of magnitude difference there so I don't see there being much of a computational benefit to reducing zed so dramatically there because you're still going through a multi layer MLP with 500 hidden units each, so it's not clear to me that there's a benefit there.",
            "Yeah?",
            "Yep.",
            "Prior from.",
            "Well, what it's doing is it's ignore it.",
            "That's actually I disagree with that statement.",
            "You what you just said is, I think what you're saying is that if you now just try to think of this thing as appear generative model where you sample from the prior and see what kind of reconstruction it gives you, it wouldn't do very well.",
            "That's not true, because what it's doing is it's essentially picking a few dimensions that it's going to use for to determine the what digit you're going to generate, and it just ignores the rest.",
            "It's not like you don't train on the rest of the, it's just simply ignoring them.",
            "Well, I mean in the all the other components it's pretty much matching exactly the prior.",
            "It's ignoring those components, yes, so you could think that there's more capacity that it could use to do the reconstruction.",
            "That I agree with.",
            "So in that sense, I think there it is.",
            "It is a weird property of the model.",
            "Yeah, yeah.",
            "So I I mean I have not studied this question explicitly.",
            "Maybe I should.",
            "'cause I think about it a lot, but my perspective is that it does seem to over regularize compared to what you would hope it would do, so I don't know what that I mean.",
            "What that means?",
            "I mean you're free, I mean this is just with a particular choice of prior right independent Zeds.",
            "If you explore different.",
            "Choices of that prior you might find you get different behavior, so we've actually done some of that exploration.",
            "We haven't that direction.",
            "We haven't found very much that's better.",
            "Yeah.",
            "Right, so we when we were first doing these kinds of experiments, we thought there was what we found was that the deeper the models, the more sparsity you tended to see in zed.",
            "And we thought that was a real effect and we had this story that said something like Oh well, you know it's higher capacity so we can use this non linearity to cram it into into, you know a smaller dimension and these other thing is said is is continuous valued right?",
            "So so in some sense all it needs is one dimension right?",
            "It can lay out all of the data in one dimension and just have very little noise and it can reconstruct the input.",
            "Now that's not going to generalize very well but but in principle it doesn't need multiple dimensions.",
            "To encode the input because of because of its continuous.",
            "So I don't think that that turned out to be probably more of an optimization issue, and that if you sort of initialize it and use kind of better optimization techniques, you don't tend to see this correlation between the depth or capacity of the encoder decoder and the zed that's chosen the dimensionality of said that's used.",
            "Yeah, yeah.",
            "Which sleep algorithm.",
            "OK so plants are good 'cause I'm not really an expert on week sleep but."
        ],
        [
            "The essentially the wake sleep uses the variational bound, the sort of that we're using here uses it in a different order, and it's using.",
            "You're basically using.",
            "It's more like if you had one graphical model and you have a different set of weights projecting between them, and every layer is considered to be stochastic, so it's in some sense the graphical model is closer to something like a deep Boltzmann machine, and you just have two sets of weights projecting one set.",
            "Projecting up one set.",
            "Projecting down.",
            "So formally, that's the don't you want something about that.",
            "OK."
        ],
        [
            "What?",
            "So.",
            "And.",
            "Right, so just just on that point.",
            "To be fair though, like this, 5 dimensions were modeling like MNIST or something similar, right?",
            "And so the kind of dimensionality you see for something like I missed something like between 20 and 30 dimensions is kind of where we kind of usually sit.",
            "So whether Amnesty needs more than that number of dimensions, I'm not sure, yeah.",
            "OK. Slightly though, right?",
            "Yeah alright so so.",
            "But you still getting dead units right?",
            "Right so.",
            "Yeah, I'm not sure it's definitely not using the capacity that it potentially could use, right?",
            "So the objective function is in some sense bottlenecking the process.",
            "What you're saying is basically the more capacity you give it, the more capacity it chooses to use, which is almost that sounds almost more like an optimization issue in itself.",
            "Yeah anyway, yeah.",
            "Yes.",
            "Right, but in this case right, the true posterior is also going to ignore these dimensions, right?",
            "Because the.",
            "Actually wait is that is that true?",
            "Yeah, it it will ignore these dimensions because there's they is X along these dimensions.",
            "X is independent of Z.",
            "So the posterior is just going to be the prior along those dimensions.",
            "Yeah.",
            "Well, it's just it's it's it's X, Zedd and has no.",
            "Zedd provides no information about X, right?",
            "So.",
            "So, but I mean certainly there's a yeah, this is an issue there.",
            "Are and yeah and independent, yeah?",
            "Yes, that's right.",
            "Yeah, it's a good question.",
            "I would imagine it would impact it, right?",
            "I mean, you could well in a trivial sense you can make it impact it by just saying I'm going to set like one of my dimensions to have significant variance and all the rest to have essentially zero variance.",
            "And then you're forcing it to go in that one dimension, right?",
            "So if you made that soft, yeah, you would be definitely impacting that.",
            "I don't have a good way to prescribe that prior for you, though I would almost rather learn it, but learning it actually doesn't really work that well either because it's more like I think you're better off actually keeping it as a kind of a constant constraint, because it's a challenging optimization problem as it is, and the effect that that prior has on the resulting encoder is such that if you, if it were to change dramatically early in training, you could really hurt performance.",
            "Overall, OK, let's one last question here and then we're going to move on.",
            "For the prior, do you mean so yeah.",
            "So one thing we have done is experimented with.",
            "It like mixture models for priors, things like that so you can do that.",
            "It's you have to make an approximation to the objective function to make that work.",
            "Yeah, I don't remember us having a lot of success with that kind of structure.",
            "What actually, what?",
            "In fact the optimization problem.",
            "There tends to be turns out to be difficult, right?",
            "'cause again because of this dependency between the encoder and prior.",
            "So typically we typically have the best results when we use just a fairly simple prior and what actually ends up happening, and this is again, this was some time ago, so maybe now if we tried this with better optimization methods, we'd see a different result.",
            "But what we tend to found what we found before.",
            "Was that when we tried something like that?",
            "Is that the model would just pick one of the mixture components that we use Gaussian mixtures?",
            "It would just pick one of the mixture components and say I'll just use that 'cause the encoder decoder were powerful enough to be able to use that to model the data so it didn't have much incentive to use the other.",
            "The other capacity in the prior that we gave it.",
            "OK, so let's."
        ],
        [
            "Move on to right, so that was that's basically it for the VA. Now we're just going to talk about other work that's been done to be."
        ],
        [
            "Hold up on this and what we're going to talk about is this was published in NIPS 2004.",
            "This is a semi supervised learning or essentially building on the V model, so they basically introduced the study two basic approaches to using the VE type architecture in the context of semi supervised learning.",
            "The first is basically the same architecture that we would use sort of kind of very common in the deep learning literature pre 2012 right?",
            "So that is we first train.",
            "AV in this case of EAE model but basically just any kind of unsupervised model, A single layer unsupervised model.",
            "Now in this case remember this.",
            "So this is the graphical model associated with the VE.",
            "But in this case there's a.",
            "There's a whole non linearity here.",
            "There's a whole MLP map in this map, but any case you train this kind of model and then you would then train a supervised model going from using the posterior to recover zed from X and then training zed to the label.",
            "So that's one way to do it.",
            "The other approach they took was something else, so that's what they, the other approach was.",
            "Essentially that they would combine zed and the labels through an MLP too into X itself.",
            "So this is a kind of an interesting model because basically what you're asking the zed to do here is different than here, right?",
            "So in this case said is just modeling everything, it's incorporating information about the label and everything else.",
            "In this case, presumably the label information is taken care of here by by putting the label directly in here, so for Ennis for example you would say OK, Now I'm modeling a one, so this will be a one.",
            "And Zeds role here is ideally to model everything else.",
            "That's all these other factors of variation that are associated with that particular one.",
            "That would then be generated, right?",
            "So it's kind of modeling everything else other than the label.",
            "And they."
        ],
        [
            "They also explored one other kind of model configuration, which is just sort of a combination of the two, so they have this latent, so they have this typical model VS.",
            "Structured model here and then they would just stack there M2 model.",
            "On top of that.",
            "So now they're just kind of making a bit of a deeper connection between the Fusion of the latent variables that have nothing to do with the label plus the label into into X.",
            "Here through this other layer here.",
            "Yes yeah yeah.",
            "Well, they didn't explore that.",
            "But yeah, I mean you could imagine all kinds of different ways of incorporating this info."
        ],
        [
            "So if you had, if you had the model where you have something like, I guess this model here, right?",
            "Then what you're basically saying is it's.",
            "I mean it's in some sense it's a lot like this, right?",
            "Zed is incorporating information about the label."
        ],
        [
            "Right so."
        ],
        [
            "So in inference, I'm going to go over this pretty quickly.",
            "The details are obviously in their paper, but the labels are typically categorical labels, so this is a discrete random variable inference.",
            "In this case, this is your standard via E model.",
            "In this case, what you have for inference is basically a model that goes from X to Y and then then the inference process on zed is actually conditional on X&Y itself.",
            "Yeah?",
            "Paper.",
            "They get right?",
            "Yeah, that's that's right.",
            "Which is actually, I mean.",
            "Right, so that's that's actually will get.",
            "Will get to that in a second but but I mean of course they do right?",
            "Those are just literally injecting noise into your process, so it would make sense that they would throw those out.",
            "That's a very good point.",
            "Yeah, yeah, we'll touch on that point again in a second.",
            "But yeah, that's a very good point.",
            "Alright, so this is our how we were doing inference.",
            "And again, these are just always this sort of.",
            "You see this process here where we have this arrow here in this area.",
            "Here these are just being concatenated, then passed to MLP which is predicting the parameters of Z and then again that is going feats.",
            "Then you sample from that zedan then you.",
            "Project that into your decoder net to predict X again, right?",
            "So it's just the standard kind of way you would imagine the VE being incorporated into a ski."
        ],
        [
            "Like this?",
            "Right, so just a note here.",
            "They need actually in this case if they're going to train with labeled and unlabeled data, they need a way of being able to train this structure when you don't have.",
            "When you don't have Y, right?",
            "So they the way they do that is that they basically just use their joint model and do essentially marginalized out why stochastically in the case, actually that you can do it stochastically.",
            "But I actually think because it's categorical here, you don't actually have to do it statically.",
            "You can actually marginalized out explicitly because you can just.",
            "And you can somehow all possible values of why there?",
            "So that's just how they deal with the win.",
            "Why is missing so in the case where you're modeling your training of this on purely unlabeled data?",
            "And and just another just another last point when they're actually using this for classification.",
            "They also train with what would be essentially a traditional classification algorithm here, yeah.",
            "OK so so OK. Yeah, I forgot about so they they they prescribe sampling.",
            "Right?",
            "OK. Yeah.",
            "OK. OK, that's good."
        ],
        [
            "OK, so results right?",
            "So this is getting back to the point that Kyle was raising, which is there when you do so M1 and so this is an M nest, right?",
            "So what they're doing here is they're actually looking at the case where they've limited the number of training examples, so this is a semi supervised case.",
            "They've trained in an unsupervised way on all of the data and given, for example 1000 labels here.",
            "And what they find is if they take just the M1.",
            "This is sort of the classical way of using this, just M1 and plus train put that through a transductive SVM.",
            "You get something like 11% error if they go with their M1 plus M2.",
            "You get very good performance here and this is essentially the kinds of performance they see in their algorithm.",
            "And just to give you some comparison, this is the contractive autoencoder here.",
            "These are the kind of performance you see, so significantly better, especially in the context where you have very few labeled examples, right?",
            "So this is where you're seeing this effect of this bottleneck, as Kyle was mentioning right of coming through is that with very few examples you can't afford a very.",
            "High dimensional space and the VA is giving you the ability to essentially contract and possibly, you know, decoupler.",
            "Factors of variation, yeah.",
            "You just, it would be more like this setting.",
            "I think this is where you basically you pre train a model.",
            "Yeah, and then fine tune right?",
            "So or you can.",
            "I mean this is not pretend fine too, but you could stick an SVM on top of it, yeah?",
            "One point 1% here with that.",
            "Yeah, yeah, so that's that's really really interesting work.",
            "Yeah, right?",
            "Yeah, that's a good point.",
            "So right actually in fact, that is the algorithm that gets so on full amnesty they get a very impressive result, which is .96%.",
            "This is kind of I think at this point this was a couple.",
            "Things are moving quickly.",
            "This is about a year ago and.",
            "No, it wasn't.",
            "It wasn't said the art, but it was close to state of the art then.",
            "Now it's not so very close to state of the art, yeah.",
            "Oh well, the arrangement.",
            "They're not the way their combined is in the."
        ],
        [
            "Kind of a structure, so the way they train this thing is they first train this lower layer model.",
            "They basically stack pre training they do.",
            "You know stacked, pretraining, greedy pretraining, right.",
            "So the first train, this model and then they trained this model as well as those samples from here where the data or they might even use the mean.",
            "I forget if they forget how they treat this child you remember, do they when they when they stack this on top and their training this Top Model?",
            "Are they sampling here or are they getting the means?",
            "For example they're taking the means so they reduce the variance and by doing that yeah that makes sense."
        ],
        [
            "Alright, so this is kind of a set of results you can see, so this is kind of playing with this.",
            "Structure here where you've got zed and why being separate things so you can kind of clamp why the label and then sample from zed?",
            "So this is kind of showing you the kind of map I was showing you earlier where you're moving around the Dead Space and you're getting a bunch of different tools 'cause you've clamped up to two here.",
            "We have 3, four and then we have this kind of analogy.",
            "So in this case what they are doing is they are.",
            "For a given row, here Zed is fixed and now they're just twiddling the Y, so for different.",
            "So given this guy here, they do inference on this for so they figure out what is the zed associated with that for.",
            "Then they fix that said, and then they move the Y and they can generate other digits that have that same zed, right?",
            "So how that same sort of style?",
            "So this is, you know, you're sort of doing this separation of style and content with this Y and zed, and they're doing it for.",
            "The SVN here too.",
            "So these are house numbers where you're seeing a very similar sort of thing, right?",
            "So they've got this two in this context, and then there's sort of and these are just pure samples from the model, right?",
            "So these do not necessarily exist in the training or test set there.",
            "Just here's a here's a.",
            "You're given this example.",
            "Then they take the set associated with that and change the Y, and they generate these.",
            "So these are pretty impressive results.",
            "Actually, in terms of separating style and content."
        ],
        [
            "OK, so let's move on.",
            "Alright, so now I."
        ],
        [
            "I'm going to go over this pretty quickly.",
            "This is this is our recurrent model that we're basically making a recurrent version of the VA because what we're interested in is modeling sequential data.",
            "Things like speech synthesis is actually really what our target is in this task, and right now that kind of space historically has been dominating things like Hmm's.",
            "More recently, things like our like recurrent neural Nets have been very popular for that kind of thing, and our motivation here is that, well, we're seeing something pretty interesting with these vehicles, right?",
            "With the fact that these latent variables are encoding some sort of interesting variation in the data, and we thought that maybe by building a sequential model that incorporates these kind of latent variables, we're going to see more natural variations in the data than we do with something like a standard RNN, where the only sort of variability or stochastic city in the standard RNN is right at the level of the data, right?",
            "You're using like a mixture, Gaussian models typically, so that doesn't seem like a very natural place.",
            "If you have this idea of this kind of data manifold, we want to be.",
            "Variation is in the space.",
            "It's quite far away from the data, so in this context we're sort of taking that same insight and applying it to sequential data.",
            "So here we have our traditional via E model and we're just sort of adding recurrence here and then we're going to do something that's a little bit different than your standard V model is.",
            "We're going to sort of make have a single recurrent neural net here.",
            "That's going to do double duty.",
            "It's essentially going to be used both as our encoder model and as our decoder model, where we still have sort of MLP's that connect.",
            "X to Z that are that are sort of encoding what normally encoder and decoder functions is, but the information about the history is encoded into a sort of compressed into a single RNN.",
            "Any questions about the structure we're going to go through the pieces."
        ],
        [
            "But now, so here we have our variational RNN.",
            "We can sort of decompose this into a few different pieces.",
            "So we have our prior structure.",
            "We want to generate.",
            "This is these are the components involved in that we have our recurrent structure here and we have our infrastructure.",
            "We're going to take these essentially into."
        ],
        [
            "Turn 0 first our prior so the typical via E just has a prior on zed which would be like just zed here right?",
            "In independent components on Zed our prior on Zed is a function of our recurrent hitting unit of the previous state here right?",
            "So this is how we're incorporating information from the history were doing it, remediating that through our recurrent neural net hidden units.",
            "And again, it's standard Gaussian distribution on zed, so so the only sort of innovation here is that we're conditioning on this.",
            "This previous hidden units here, so that's what's new about this from the case of the prior."
        ],
        [
            "For generation Now what we're going to do is we're going to.",
            "The typical value would be you take this latent variable to current time step and generate X with that.",
            "We're also going to generate where it's going to use our previous context.",
            "Given mediated again by our last hidden recurrent hidden state.",
            "And we're going to generate from X.",
            "Given both of these two pieces of information, yeah?",
            "Contact picture another right.",
            "Right, so so one way to the other piece that's important about this is we're actually going to be exploiting the fact that what we have is a per timestep via E model.",
            "That's conditional, and we're basically going to use that when it time."
        ],
        [
            "When it comes time for learning just a real quick our our recurrence information now we're basically taking information from everywhere from X, from zed and from our previous or current state is coming in to define the current context."
        ],
        [
            "Now for inference.",
            "What we're doing again, we're taking this previous time step, and we're incorporating information about the current X into a model about the current zed.",
            "And again, we're exploiting this fact that we're using a factor posterior where it's factored given X and the.",
            "Yeah, an implicitly the previous H is previous, Zeds are given through."
        ],
        [
            "Each year zero training again.",
            "We're exploiting the fact that we can decompose this our objective function essentially into a series of V models in sequence, and then we back propagate through this whole thing through the encoder decoder structure as in a standard V, but also across time.",
            "Doing backpropagation through time as in the standard RNN, and you just basically do that following this objective function, right?",
            "So every just following the functional form that we.",
            "We provide here.",
            "You basically just do your back propagation and train as you would, essentially as you would have standard RNN.",
            "But we inject the are stochastic city here in latent variable rather than at X.",
            "We also have a simple latent like X is still latent in the sense that sorry X is still random in the sense that we have a probability distribution over expert.",
            "Typically we use something simpler than when we have to use for an RNN, yeah?",
            "Yeah.",
            "Yeah.",
            "Right, right?",
            "It's I mean the fact that we're sampling at every time step.",
            "It's essentially the same approximate, it's a. Yeah, it's a similar approximation to use.",
            "Typically make Innovia context, but you're right, I think you're right about this."
        ],
        [
            "Alright, so how does this model perform?",
            "And again, what we're interested in doing is comparing it to a standard RNN model, and so which is given by, for example, here is an RNN model with a Gaussian distribution on X.",
            "The data here is an RNN model with a Gaussian mixture model on X.",
            "This is sort of the standard way of injecting a lot of diversity in your in X, and what we have for speech modeling is.",
            "We've got four different speech modeling datasets, so here what we're doing is we get we have a.",
            "And this is on the raw input, so we have a raw input.",
            "We're training this model and then we have it.",
            "We're we're now comparing how likely a test set sequences under our model versus something like the standard recurrent neural net in all cases.",
            "Here we use a thinking STM model in this case for the recurrent neural net, as well as our reusing LCM also in the in the context of our model.",
            "So here we're seeing actually, so I guess what we typically see is pretty good performance.",
            "I guess our model is performing fairly favorably compared to your standard RNN in both cases, so this is speech modeling and handwriting."
        ],
        [
            "We go to some examples.",
            "This is sort of a wide view of what speech looks like, and this is sort of a zoomed in view.",
            "This is the this is our best standard RNN model that we could train.",
            "You can see that we're capturing capturing a little bit more of the global structure here, as well as the local structure is a little bit cleaner.",
            "You're seeing quite quite a bit more noise evident in the case of the just the normal RNN with a Gaussian mixture model, and you might think that this is sort of the kind of thing we'd expect to see.",
            "If we're sort of being successful at modeling variations at a higher level, right?",
            "Because the RNN, all it can do to model variations is move the data itself.",
            "That's the only way you can generate something something different.",
            "Whereas in our case when we do model, when we sample in our latent variable, weaken the model itself, concerns smooth that stochastic city out, and so that's why we're sort of seeing this kind of smoother pattern here and sort of matching a little bit closer to the ground truth.",
            "Then we see with the RNG MMM."
        ],
        [
            "Interesting if you actually look at the KL divergent's across time here in the in the model.",
            "So this is the waveform we get in as input.",
            "The what we can see here is that the KL divergent actually seems to peak when it's when you're seeing sort of state transitions.",
            "If you like between one pattern of behavior and another pattern of behavior.",
            "It's kind of."
        ],
        [
            "Interesting observe."
        ],
        [
            "And here we have handwriting recognition.",
            "So in this case both models actually perform reasonably well.",
            "It's sort of the same type of thing that you see with kind of typical models of this kind of locally, it looks pretty compelling, But if you actually try to read these sentences on either case, it's pretty hard to make out any words.",
            "This is the ground truth data.",
            "These are two examples of RNN models, and this is our our variational RNN.",
            "This time just we're just showing you with another Gaussian mixture.",
            "All of the input.",
            "Yeah, we see a qualitative difference or quantitative difference between these in terms of the likelihood.",
            "I don't see a big difference in terms of the quality of, but maybe there's more variation here.",
            "I don't know more consistency like this is sort of.",
            "You see more consistency across the whole trajectory.",
            "See maybe here.",
            "No no.",
            "It's not dramatic, but then again it's hard to really assess this OK, right?"
        ],
        [
            "So now I want to talk to you about a different model.",
            "I don't think I'll get to the more more recent methods, which is too bad, but that's OK. Well, stopping here will probably be interesting enough.",
            "But now I'd like to talk to you about the draw."
        ],
        [
            "So the draw stands for deep, recurrent, attentive writer.",
            "So the basic idea here is actually similar to what we saw with the previous work, in that we're going to.",
            "Incorporate a recurrent neural net structure into the inference model and generation model of a standard VE, but it's slightly different.",
            "They're actually doing this in the context of non sequential data.",
            "You can also apply to sequential data and they have, but the way I think is a very interesting aspect of this is they're basically using a recurrent neural net in the context and a VE together to do inference in, say for example in image.",
            "And the other thing that they do that I think is very interesting is they actually augment the model with an attention mechanism which allows them to essentially define this non sequential data as being being generated as a result of a sequential process.",
            "As well, so these two ideas sort of work pretty well together, although you can and will show you results of doing of using the draw structure without actually using their attention."
        ],
        [
            "Kinism so this is what would be a standard via E from the perspective of this model.",
            "So here we have X.",
            "Actually this read process doesn't really exist in the context of a standard VSO X is going to map to this encoder network.",
            "You could take you out you sample from that distribution.",
            "You get RZ and then you have a decoder MLP and you get that gives you the distribution over X and you can sample from that.",
            "That's a standard variational autoencoder.",
            "Now the way draw change is this."
        ],
        [
            "Structure is like this, so X and again X can be the whole data and you see X is now we have a sequential structure so we have a recurrent neural net here and X is being fed in at every time step.",
            "Note that X can be a subset if you use attention that X can be changing overtime.",
            "It depends on this the attention mechanism.",
            "Then you have an encoder RNN.",
            "Going in that's giving you your distribution, your Q distribution, and then you take a sample from that and then you have your zed.",
            "That's again a sequence of Zeds here and then.",
            "You have a decoder RNN and the decoder RNN.",
            "Interestingly enough, feeds back into both the encoder and and and the read structure at the next time step.",
            "Also, at the end point you have the decoder and projecting to a right mechanism, which then generates this what they call a canvas, which is essentially sort of a.",
            "Essentially something that you're writing to that Maps to your probability distribution over X, right?",
            "So at this end point here, this is essentially just a X is now conditionally independent given zed, but also given essentially given the C here.",
            "So you're sort of filling in this canvas, which is your input, and then at the end you just you just sample from that in a fairly simple distribution.",
            "But what's interesting is you're sort of building up this canvas over the overtime here, so that's the structure of draw.",
            "It's a pretty complicated model.",
            "It's actually like surprisingly complicated, I would say, but it seems to work fairly well.",
            "Are there any questions about that structure?",
            "OK, we can move on, so this is just.",
            "So yeah, I mean, I'm not going to go into anymore details than this.",
            "The details are actually all pretty straightforward, yeah?",
            "Big.",
            "Big T is fixed here.",
            "I think big T is always fixed.",
            "They do experiments with a very big T, but for any given experiment it's fixed.",
            "Yeah.",
            "Oh right, it's just it's just an operation that basically Maps from the output of the decoder into CT. Well, it's just you can think of it as just a parametric model that when the parameters you're going to train.",
            "So it's just and you can think of it as another PC or like.",
            "Think of it as an MLP for example."
        ],
        [
            "Right, so actually in this case, if you have it, it's actually it's mostly used in the context of the attention mechanism, which you don't have attention.",
            "It's essentially a pretty simple operation here.",
            "For example, the read operation it just uses is essentially just giving you.",
            "XNXT so."
        ],
        [
            "Coming back here, this Reed operation here is just feeding in X.",
            "And and the previous X hat that comes from the write operation of the previous time step basically, which is kind of at this point in the model."
        ],
        [
            "So that's essentially all it's doing, so it's not doing anything in the case of the without attention.",
            "In this case, the right operation is similar, just a linear mapping from.",
            "In oops."
        ],
        [
            "The right operation right here, right?"
        ],
        [
            "So.",
            "It's just actually taking in the case of.",
            "Of I guess it's just taking the the current hidden state and actually not using.",
            "It's interesting it's not using the previous right information as well, so this is apparently not being used in that context.",
            "That's interesting, OK, sorry, and if you were to apply this, this is the kind of generation process you get, right?",
            "So time here is just generation time.",
            "So you start sampling here.",
            "This is for amnesty once again and you get so it starts out fairly blurry an as you continue the RNN generation through time it sort of refines the model into something that's clear."
        ],
        [
            "Nur.",
            "And more detailed, and it's a sharper image.",
            "Over here there.",
            "Now they the attention mechanism that they use is actually something pretty interesting, so I'm not going to go into the details of it.",
            "Basically, the idea is that they use it's essentially a differentiable model.",
            "What they what they have is the output coming from the decoder model is essentially extracts a few parameters that include the location, for example of the attention.",
            "So you're attending a given image Patch.",
            "And the output is the location of that Patch.",
            "The size of that Patch, and the blur of that Patch.",
            "So these are essentially the three parameters that or that, well, you have location in two dimensions and size in two dimensions and then blur scalar blur.",
            "So that's 5 dimensions that your base that's output from the decoder model to give you the attention, and then then what you do in this case.",
            "For example, you know on this five you have.",
            "In this case, a small Patch centered about this point here, and it's fairly sharp.",
            "Yeah, reasonably, moderately sharp, let's say, and you get this kind of an output and which which then feeds into the input for the next sequence of them of the recurrent structure.",
            "Here we have a large sharp image, and here we have a large blurred image, and this is the result that you get.",
            "So it's interesting that they choose to separate the scale in the blur.",
            "This is something that I think I would have naturally just fused together.",
            "I'm not sure I I don't.",
            "I've not studied that.",
            "I don't know if that's an important distinction to that.",
            "Right, yeah, that's right.",
            "Yeah, I'm sorry, yeah, that's that's a good point there.",
            "There is the scale.",
            "Yeah, right?",
            "Right, yeah?"
        ],
        [
            "Yeah, so right.",
            "So I mean this is just kind of an example of how this would work.",
            "These are examples of that for a given Patch.",
            "This is sort of an example of the kind of Gaussian blur kernel you would use.",
            "It's generating a single a blur, so there's an example of how it would blur over the input to generate a given pixel.",
            "Then the read operation you generate, so it's it's down.",
            "It's using a significant Gaussian blur here, and the scale is quite large, so it downsamples.",
            "So if the blur is large, it's going to downsample into something quite small like this, and then regenerate into something that's large here.",
            "So that's the basically structure of the of the attention mechanism."
        ],
        [
            "Now how you can use this attention mechanism in interesting ways for for the classification is shown here, right?",
            "So this is a data set that's called clutter damnest.",
            "In this context we have a single digit in a background of things that are kind of digit like but actually aren't digits.",
            "And the goal here is just to do classification.",
            "So this is kind of a sequence showing how the sequential operation of the attention plus draw structure.",
            "Sorta zooms in on the digit.",
            "So we have sort of structure here so it sees a big image early on.",
            "It actually learns to sort of have a blurry large image of the whole thing, and then it zooms in once it end into a sharper, sharper image as it goes along.",
            "And this is the kind of structure we see, and we get reasonable performance.",
            "I mean, this isn't necessarily a very competitive task, so it's hard to say how well this does compared to other methods.",
            "I think this was state of the art on this task when it was applied, but there's only been like one or two other people applying it on this so.",
            "It's it's not sure how competitive it is at this test, but at least it's it's clear that the there.",
            "Attention mechanism is doing something interesting, and another thing that's interesting about their attention mechanism versus other attention mechanisms that have been proposed is that it's fully differentiable, so they don't have to use something like reinforce or something like that to train it, which is very nice.",
            "So here's a."
        ],
        [
            "Lee, this is a this was.",
            "Basically the attention mechanism being used to generate MNIST, and this is a result by actually somebody in our lab York who was actually able to repeat this performance an actually get match their performance.",
            "So this is essentially been reproduced.",
            "This is his result that I'm showing you here and the important thing, and This is why I think draw is probably the most exciting of the models that have come out recently and why we're going to be spending the rest of my time on it rather than on the other methods is that it's able to.",
            "This is what's being measured.",
            "Here is the negative log likelihood on Msom list.",
            "It's a simple distribution, but it happens to be the only one that people are routinely testing on for.",
            "For the capacity of a generative model.",
            "So what we're measuring here is the negative log likelihood on test examples, and for years we've been stuck in it about this range right here.",
            "8584 we've never really been able.",
            "I mean, we don't know how good one can do.",
            "This model with attention has significantly lowered that that component, right?",
            "So this is a very exciting innovation.",
            "Now there's some questions to ask.",
            "Where does where does this come from?",
            "And by the way, this this number has also been reproduced, so it's a real result, and so there's a few things that might go into this right?",
            "One thing that's clear is that there the attention mechanism there use of an attention mechanism is implicitly incorporating a topology, so you might expect to get some gain from that, right?",
            "'cause all of a sudden you're?",
            "You're now more in the regime of something like a convolutional model, as opposed to a fully connected model, so that could be a reason why you're getting a significantly lower value here.",
            "In fact, that probably accounts for quite a bit of this.",
            "The other thing is that they're sequential inference mechanism.",
            "For example, if you compare this model to standard via E. Which isn't here actually.",
            "This is one of the other mechanism models that have recently been proposed, so so Standard V gets you gets about 90, right?",
            "So you see a full.",
            "10 not improvement from Standard VEC and so one possibility is that apology and other possibilities.",
            "It's a sequential inference scheme that's helping here.",
            "Seems like right now.",
            "Both are contributing pretty significantly to getting that kind of result.",
            "And so if we just look."
        ],
        [
            "At.",
            "So this is sort of using attention in reading that was just the reading process.",
            "This is how this is generating.",
            "These are again from the original authors, so the ones that proposed draw.",
            "So this is generating two sets of images.",
            "We can generate SCHN.",
            "It's just looping back, so this possible you're seeing now is how attention works in reading.",
            "And now what we're going to see is the writing output.",
            "So these things are happening in parallel, of course, but you're viewing sort of one or the other of them.",
            "And this is for SSV HN, so these are Street View house numbers.",
            "And of course there.",
            "For some reason it's just decided to sort of pick on one side and generate kind of sweeping across.",
            "It's the training policy.",
            "It essentially learns, yeah.",
            "Well, that would be essential if they take the whole image and stay at the whole image.",
            "They're essentially then using the standard draw model without attention, right?",
            "The point of the attention is they're able to zoom in.",
            "If they they learn it, it's the the parameters are like, for example, the location and scale of that Patch are the output of the hidden units of the decoding model.",
            "So it's just trained via the global objective function 228 for a particular policy, and it's perfect.",
            "It's 100% back probable, so you just back prop through that and train those parameters.",
            "Yeah.",
            "Yeah, so so I think that's actually a significant component, and the evidence for that.",
            "I mean, this is purely qualitative, but I'm not showing their results on Cifar generation.",
            "So C far these tiny.",
            "You've probably seen these this week.",
            "They're tiny little images.",
            "There are like 32 by 32 images of cats, dogs, planes, cars, and they've done this generation, and it's not nearly as impressive as these results.",
            "Alright, so it would be an interesting thing.",
            "The problem is nobody's really established baselines for NLL on things like cifar yet, so we're still kind of, you know, something I'm interested in doing.",
            "What's that?",
            "Baseline in terms of NLL numbers and not negative log likelihood.",
            "So typically way they go back to MNIST.",
            "Which is kind of unfortunate at this point, but because I think we're actually getting to the point where it's almost a solved problem even in the generative context.",
            "Alright, so I think.",
            "I think I'm going to.",
            "Stop, I just want men."
        ],
        [
            "And really quickly these other two papers that were in the most recent ICML I just want to go over them super quick, just give you a real quick flavor of what they."
        ],
        [
            "Or after so the basic idea here is they're interested.",
            "They're taking kind of going back to the original via E model and saying, well, this one has this one weakness, which is this this the encoder model still has this factorial prior, right?",
            "It's still essentially kind of a mean field assumption on the inputs.",
            "Sorry in the posterior and the question is well, can we do something about that?",
            "Can we improve that kind of model?",
            "So there's been these two?",
            "Works recently that are essentially attacking directly that problem."
        ],
        [
            "This is one of them, so variational inference with normalizing flows."
        ],
        [
            "And it's actually very similar to the nice small.",
            "I don't know if you have you talked about nicer talk about now, so it's actually in some sense it builds on on this nice model, but they use it as as a way.",
            "So basically they're using this insight that you can do these sort of chain together sequences of transformations of variables in order to relate random variables in one space where you can put in a simple distribution into a random variable in another space where it can be a little bit more complicated.",
            "So what that allows you to do is take a simple.",
            "Sort of inject a simple distribution somewhere in your model and then project through this chaining of random variables into this complicated space and that you can consider to be your posterior, and because if you're mapping is invertible, you can actually do this right?",
            "So so it's essentially the objective function still allows you."
        ],
        [
            "Train these models.",
            "So this is essentially the lower bound you get when you do that chaining together with these transformations F here."
        ],
        [
            "You can see that if you do that in the context and they pick for example what they do is they pick a particular form of F that looks like this, which is essentially just kind of a linear like you've got a linear component and a nonlinear component.",
            "I think this they used a soft plus here and what they find is they're actually able to this."
        ],
        [
            "Sort of your standard distribution.",
            "This is a unit Gaussian.",
            "This would be essentially what your prior would be.",
            "Let's see what your posterior would be under the traditional view, and if you go through one sequence of this two sequences of this 10 sequences of this, you can get 2 pretty rich distributions.",
            "And what they do with that is of course, just as I mentioned, they sort of stacked this on top of a standard MLP to get you from this place, where you would output a fairly simple prior to where you're sort of interfacing the objective function here, which would be a fairly complicated.",
            "Simple posterior distribution here, but now you can actually express that as a fairly complicated and rich posterior.",
            "So that's that."
        ],
        [
            "Now if you compare it analytically, what you end up getting is reasonable and performance improvement.",
            "So what we're looking at here is chaining this.",
            "For example, is chaining 80 of these functions together through this normalizing flow, and you're getting a decrease from about 90.",
            "This is what you can get in a standard case with VE down to about 85.",
            "OK, this is again in the context.",
            "There's no topology information now, so it's kind of incomparable to compare this to what you saw with draw, because there was this implicit.",
            "Topological information coming up through the attention mechanism, but if you were to compare it to, for example, what you get withdraw without an attention mechanism that was around 87, so we're actually seeing a benefit here.",
            "So it does seem to be doing something interesting in this context."
        ],
        [
            "Alright, so finally, if we just talk about one last."
        ],
        [
            "Idea?"
        ],
        [
            "Right, so this one, I'll just go over what they're doing.",
            "Something really cool.",
            "They're integrating MCMC with a variational method and the way they're doing is they're kind of envisioning the hybrid Monte Carlo, the MCMC chain that you get out as doing variational inference in an augmented space.",
            "That includes this whole chain, right?",
            "And they end up being able to."
        ],
        [
            "Late this."
        ],
        [
            "That's just a little."
        ],
        [
            "Still on the type of MCMC they use is Hamiltonian Monte Carlo, which is useful because they actually can do got it with gradient information.",
            "But the insight here is that they are able to relate it to doing this augmented space to a lower variance to a variational lower bound.",
            "It just happens to be lower than the normal variational lower bound."
        ],
        [
            "And they end up with this trainable parameters for a."
        ],
        [
            "Model, but they can learn to sort of augment that.",
            "So anyway, I guess I'll stop there anyway, actually."
        ],
        [
            "If they, if they do that.",
            "They end up getting in and they incorporate the topological information.",
            "They actually end up getting very close to draw.",
            "OK, now I'll stop."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type alright so.",
                    "label": 0
                },
                {
                    "sent": "I guess I spoke to you guys about a week ago.",
                    "label": 0
                },
                {
                    "sent": "In fact actually.",
                    "label": 0
                },
                {
                    "sent": "Just over a week ago about undirected graphical models in our BMS and since then I believe you've had some.",
                    "label": 0
                },
                {
                    "sent": "I guess Russ talked about deep Boltzmann machines, that's happened, and so there you're going to see basically the what I would view as the sort of the.",
                    "label": 0
                },
                {
                    "sent": "I guess the sort of high watermark I guess, of undirected graphical models, at least where they stand right now.",
                    "label": 0
                },
                {
                    "sent": "And I guess hung like also talked to you about undirected graphical models, or at least hybrid graphical models and sense of these stacked RBM's.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk to you about a different line of research.",
                    "label": 0
                },
                {
                    "sent": "That's something that's been around for a little over a year now, and I think it's very exciting.",
                    "label": 0
                },
                {
                    "sent": "And we've done a little bit of work in this.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about a little bit of the stuff we've done, but mainly what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to do is sort of go over the basic introduction of the variational autoencoder, something it's actually comes by a few different names, but we've sort of well.",
                    "label": 0
                },
                {
                    "sent": "I guess I like the term variational autoencoder for this so, so that's what I'm going to use.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about how it's you, how it can be used in a semi supervised setting.",
                    "label": 0
                },
                {
                    "sent": "That's just so each one of these bullet points is essentially a paper that's been out in the last year or so.",
                    "label": 0
                },
                {
                    "sent": "So we have a semi supervised learning with the VA and we then we have a sequential application of the VE.",
                    "label": 1
                },
                {
                    "sent": "This is our model that's there's an archive paper out right now we call the variational recurrent neural net.",
                    "label": 1
                },
                {
                    "sent": "Then we have the draw model which is very exciting.",
                    "label": 0
                },
                {
                    "sent": "This one actually was published before this one, but for pedagogical reasons I'm actually going to be presenting it.",
                    "label": 0
                },
                {
                    "sent": "Second, at least after this one and then we have two papers here that go into sort of improving how you do inference in these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "And they were both in the most recent ICML, so if we get if we have time to get to them, I think we will will cover these so.",
                    "label": 0
                },
                {
                    "sent": "This is a lot of material and I'm not going to go over any of it in detail.",
                    "label": 0
                },
                {
                    "sent": "My my my ambition is just to give you guys sort of.",
                    "label": 0
                },
                {
                    "sent": "What are the key ideas in each one of these and sort of give you a sort of a state of where this field is right now.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's get star.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a variational autoencoder, it's sort of in the family of deep directed graphical models.",
                    "label": 1
                },
                {
                    "sent": "So we talked about undirected graphical models last time.",
                    "label": 0
                },
                {
                    "sent": "This is like a week ago, and now we're talking about directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "So the idea this this.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "This is second.",
                    "label": 1
                },
                {
                    "sent": "Right, so the variational autoencoder model is essentially simultaneously discovered by two groups.",
                    "label": 0
                },
                {
                    "sent": "By Kingman dwelling here and then a group from the deep mind actually.",
                    "label": 0
                },
                {
                    "sent": "And it's essentially I think they published later, but they both had archive papers out, essentially around the same time.",
                    "label": 1
                },
                {
                    "sent": "So unlike our BMS and DBMS, here are interesting deep directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "So the question is why deep directed graph malls versus?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Undirected graphical models well to sort of motivate this, I'd like to talk a bit about what do we want.",
                    "label": 0
                },
                {
                    "sent": "A latent variable model to do.",
                    "label": 1
                },
                {
                    "sent": "So last time after after my talk I had a few students up at the front asking questions and.",
                    "label": 0
                },
                {
                    "sent": "We're talking about our BMS and sort of, you know, stacking them and and one of the things I mentioned to them was that one of the reasons why I'm a little less enthusiastic about GBM models is actually comes from my own experience trying to train stacks of these models and it comes down to a question of what do we want our latent variables to do.",
                    "label": 0
                },
                {
                    "sent": "We, I think what we want our latent variables to do is actually model natural variations that occur in the world, right?",
                    "label": 0
                },
                {
                    "sent": "So if we want to model an image, for example, we want to have our latent variables actually track variations you see in the image when you do something like stacking RBM's together, what you have at the low layer is a sequence of latent variables like say for example the bottom layer model is an RBM.",
                    "label": 0
                },
                {
                    "sent": "You have a set of random variables there that are very very close to the data.",
                    "label": 0
                },
                {
                    "sent": "Variations in that layer.",
                    "label": 0
                },
                {
                    "sent": "Often gets you what I'll say.",
                    "label": 0
                },
                {
                    "sent": "I'll use this term informally, puts you off the data manifold right?",
                    "label": 0
                },
                {
                    "sent": "Because you have to in order to sort of stay on the data manifold, you have to.",
                    "label": 0
                },
                {
                    "sent": "Perturb these variables in a very structured way, so just sort of normal variations that you might see in the in that top layer.",
                    "label": 0
                },
                {
                    "sent": "Push you off the manifold so in this sense you're not necessarily what I would say is.",
                    "label": 0
                },
                {
                    "sent": "You're not necessarily modeling.",
                    "label": 0
                },
                {
                    "sent": "With you're not using those latent variables to actually track the variations along that manifold, which is actually what you want them to do.",
                    "label": 0
                },
                {
                    "sent": "So one question is, well, how do you actually manage to do that?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, and so to the extent that it is focused like to the extent that it's a deterministic mapping, then yes, absolutely this will workout and and to the extent that you have noise at that level, yes, that will work.",
                    "label": 0
                },
                {
                    "sent": "But what I'm arguing is that.",
                    "label": 0
                },
                {
                    "sent": "And I think you know we're starting to collect some evidence in this direction that says that you are better off doing something else, not necessarily modern modeling.",
                    "label": 0
                },
                {
                    "sent": "Stochastic city at that low level, but more likely modeling stochastic that had a higher level, which is what something like the Villa allows you to do right?",
                    "label": 0
                },
                {
                    "sent": "So the VA, the idea behind the VA is to actually.",
                    "label": 0
                },
                {
                    "sent": "Propagate up.",
                    "label": 0
                },
                {
                    "sent": "So right so we'll just back to the slides here.",
                    "label": 0
                },
                {
                    "sent": "So so we want our latent variables to live in some space.",
                    "label": 0
                },
                {
                    "sent": "That's relatively simple an encode a meaningful things.",
                    "label": 0
                },
                {
                    "sent": "And then this is sort of the directed graphical model view, right?",
                    "label": 0
                },
                {
                    "sent": "So we're sort of imagining causes out in the world, and we're going to project those down into our data manifold.",
                    "label": 0
                },
                {
                    "sent": "And now we're thinking about things like images and highly structured things, right?",
                    "label": 0
                },
                {
                    "sent": "So, so this is going to be a pretty nonlinear manifold, and so in order for this for you to manage that projection, this actually has to be a pretty complicated nonlinear transformation.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "Here is we're going to be focusing on models where this is actually something fairly complicated, unlike what you see in GBM and the problem with an argument you can say, well, you can stack our BMS.",
                    "label": 0
                },
                {
                    "sent": "And that's fine.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can do that, but training this thing involves training those lower layer models.",
                    "label": 0
                },
                {
                    "sent": "And and doing that is actually gets complicated and we can go into reasons if you like afterwards in first time for questions.",
                    "label": 0
                },
                {
                    "sent": "But basically it comes up to you have trouble with the mixing when you're trying to train these models 'cause you have to learn these things with a generative model going on in the background.",
                    "label": 0
                },
                {
                    "sent": "If you're using PCD persistent contrastive divergent and so we end up having sampling issues there, it's just a much more difficult thing, much more natural if we can just train a directed graphical model to do this.",
                    "label": 0
                },
                {
                    "sent": "So what we're hoping is that we actually can learn latent variables that can be coupled.",
                    "label": 1
                },
                {
                    "sent": "The true factors of variation.",
                    "label": 0
                },
                {
                    "sent": "This is a bit of a pipe dream, right?",
                    "label": 0
                },
                {
                    "sent": "Like this, we want to actually discover the true underlying factors of variation in a given data set.",
                    "label": 0
                },
                {
                    "sent": "This is a very ambitious task, but we're going to see how far we can get along that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action right so so the way we're going to do that is in the context of a variational encoder.",
                    "label": 0
                },
                {
                    "sent": "We have our deep graphical model here and we're going to start with some latent variables here and then some complicated transformation and then the data here.",
                    "label": 0
                },
                {
                    "sent": "That's our G here, right?",
                    "label": 0
                },
                {
                    "sent": "So, so this is the basic structure we're going to use, and we want to basically the trick behind the variational autoencoders that we're going to leverage in neural network structure to map between our latent variables and our data.",
                    "label": 0
                },
                {
                    "sent": "That's it, this is our generative model.",
                    "label": 0
                },
                {
                    "sent": "And then we have a complicated.",
                    "label": 0
                },
                {
                    "sent": "It's complicated as we like relationship between our latent variables and our data in the generative sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's deterministic, it's this this graph.",
                    "label": 0
                },
                {
                    "sent": "Yes, this part of the model is deterministic, right?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of hidden in here.",
                    "label": 0
                },
                {
                    "sent": "I'm being a bit sloppy in my notation.",
                    "label": 0
                },
                {
                    "sent": "X is a random variable, zed is a random variable, and what's in between here is a deterministic mapping.",
                    "label": 0
                },
                {
                    "sent": "That's pretty important, right?",
                    "label": 0
                },
                {
                    "sent": "Because what I was arguing is that variations say in a place like here, which is close to the data will actually in something in highly structured data like like images, will actually push you off that manifold.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's possible if you have like.",
                    "label": 0
                },
                {
                    "sent": "Noisy images where variations you know you have kind of grainy images, for example.",
                    "label": 0
                },
                {
                    "sent": "Then you could imagine that noise at this low level.",
                    "label": 0
                },
                {
                    "sent": "Israel is important and relevant most of the time my experiences been, it ends up trying to model it that way ends up hurting you more than it helps.",
                    "label": 0
                },
                {
                    "sent": "Right, so first of all, let's just see what we can do when you when you add this generative model here too.",
                    "label": 0
                },
                {
                    "sent": "As a encoding a neural net, having your latent variables be separated from your data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What can this kind of?",
                    "label": 0
                },
                {
                    "sent": "What kind of this kind of model do?",
                    "label": 0
                },
                {
                    "sent": "Well, what I'm plotting here is a 2 dimensional latent space, and then the data.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I were to sample appoint here in Z1Z2, this is my zed space here and then push that through my neural net I get a zero like looking thing right here, and if I were to do the same thing here I would get this face here, right?",
                    "label": 0
                },
                {
                    "sent": "So what this is giving you is kind of a chart of how you map from the latent space to the input.",
                    "label": 1
                },
                {
                    "sent": "And what's interesting is, well, first of all, and emnace that you're actually able to.",
                    "label": 1
                },
                {
                    "sent": "To model all of the digits and you see this kind of these smooth transitions because zed I haven't told you this, but that is a real valued quantity.",
                    "label": 0
                },
                {
                    "sent": "It's a 2 dimensional real valued quantity.",
                    "label": 0
                },
                {
                    "sent": "So as I'm moving from one to another, you sort of transitioning through them, but one of the things that's interesting here is that if you look at this right, this plot right here, this is actually a model that was trained with the phrase faces, which is essentially a bunch of images of Brendan Frey here.",
                    "label": 0
                },
                {
                    "sent": "So what you can see is that the latent variables here are actually doing something like.",
                    "label": 0
                },
                {
                    "sent": "Discovering these latent factors right 'cause what you what you have is X zed one here encoding something like pose.",
                    "label": 0
                },
                {
                    "sent": "Sorry Zed, one encoding something like pose and Z2 encoding.",
                    "label": 0
                },
                {
                    "sent": "Something like expression.",
                    "label": 0
                },
                {
                    "sent": "So we're actually seeing this, and actually I don't.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I'm not going to be able to show you this, but a student of mine, Vincent Millay, has actually done some other experiments with this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Training on a bigger face data set.",
                    "label": 0
                },
                {
                    "sent": "TFD and this kind of effect holds up.",
                    "label": 0
                },
                {
                    "sent": "You've got latent variables that actually can change the mouth per person.",
                    "label": 1
                },
                {
                    "sent": "You can sample persons that face and then change one latent variable and watch them smile and then have the smile go away when you move it in the other direction.",
                    "label": 0
                },
                {
                    "sent": "It's pretty exciting now the question is OK, so this is what it can do.",
                    "label": 0
                },
                {
                    "sent": "Fairly promising, but.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does it do it?",
                    "label": 0
                },
                {
                    "sent": "So the question really is because in graphical models we've always known that you can.",
                    "label": 0
                },
                {
                    "sent": "You can generate from them pretty easily, right?",
                    "label": 0
                },
                {
                    "sent": "It's just ancestral sampling you sample from Zed.",
                    "label": 0
                },
                {
                    "sent": "This is how we did this.",
                    "label": 0
                },
                {
                    "sent": "We have some simple prior over zed and we sample in this case we're just pushing it through our MLP and then we sample from X given the parameters given these values here the difficulty with graphical models and why we as a field turn to undirected graphical models is because of inference inference in these models.",
                    "label": 0
                },
                {
                    "sent": "Is is difficult, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the whole problem is, once you, how do you even train these models right?",
                    "label": 0
                },
                {
                    "sent": "Because you need some sort of correspondence between X&Z and the typical way to get that correspondence when training is to do inference.",
                    "label": 0
                },
                {
                    "sent": "So I give you an X.",
                    "label": 0
                },
                {
                    "sent": "You recover some zed and then you train the model to optimize to match those better.",
                    "label": 0
                },
                {
                    "sent": "But now the question is we can't recover this posterior very easily.",
                    "label": 0
                },
                {
                    "sent": "In general, it's complicated, and one of the advantages of something like deep Boltzmann machines is that the variational inference in that context is actually a fairly decent approximation.",
                    "label": 0
                },
                {
                    "sent": "In this context, with the directed graphical model, even variational inference is difficult in general, but at least that way of applying variational inference.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do with the variational.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Autoencoder is something else.",
                    "label": 0
                },
                {
                    "sent": "We're still going to use a variational approach, but the way we're going to do this is we're actually so yeah.",
                    "label": 1
                },
                {
                    "sent": "So first, we're just we're actually going to use a variational approach, so we're going to find some posterior Q here, which is just Q of zed given X and we're going to define our bound so I don't know to what extent you've seen this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "The elbow.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's called the variational lower bound, so we can define using some approximating distribution.",
                    "label": 1
                },
                {
                    "sent": "And this is true for any distribution Q here.",
                    "label": 0
                },
                {
                    "sent": "That we can find a lower bound here, and the trick is just to make sure to have the best Q such that this lower bound is as good as it can be, as close to P of X as it can be.",
                    "label": 0
                },
                {
                    "sent": "So this turns out to be.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can express this as our lower bound.",
                    "label": 0
                },
                {
                    "sent": "This is always lower bound no matter what Q we use and what the variational autoencoder does and what we the way we are going to look at the variational encoder.",
                    "label": 0
                },
                {
                    "sent": "This we're going to rephrase this a little bit into a form that looks like this.",
                    "label": 0
                },
                {
                    "sent": "And what is this?",
                    "label": 1
                },
                {
                    "sent": "So we haven't told you how we're getting our Q yet, but if we just look at what this for what this looks like here.",
                    "label": 0
                },
                {
                    "sent": "What we see is we have a term that looks an awful lot like a reconstruction term in a typical auto encoder.",
                    "label": 1
                },
                {
                    "sent": "And we also have a term which we're going to call the regularization term here.",
                    "label": 0
                },
                {
                    "sent": "Which is just this, KL divergences between our posterior.",
                    "label": 0
                },
                {
                    "sent": "And the prior here P of Theta.",
                    "label": 0
                },
                {
                    "sent": "So again, we have this term.",
                    "label": 0
                },
                {
                    "sent": "This is actually why we're calling this the variational autoencoders, because it's got this flavor that looks very much like an auto encoder, right?",
                    "label": 0
                },
                {
                    "sent": "So we have it's, and in this case it's a nonlinear auditing coder, because or like deep autoencoder, if you like, because this model here can be as deep as you like, and then we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're just taking expectations with respect to Q, which again we don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "And we have this other term which is the KL divergent between this Q&R prior, which is something simple, right?",
                    "label": 1
                },
                {
                    "sent": "We know this we're going to specify this is just something.",
                    "label": 0
                },
                {
                    "sent": "Simple, like a Gaussian distribution, is actually what's often used for prior onset here?",
                    "label": 1
                },
                {
                    "sent": "Alright, so now what is Q?",
                    "label": 0
                },
                {
                    "sent": "Well, one of the things that's one of the things that's really innovative about the variational autoencoder, is it actually treats.",
                    "label": 0
                },
                {
                    "sent": "Variational inference in a very different way than typical right?",
                    "label": 0
                },
                {
                    "sent": "So the typical way you do variational inference is kind of a non parametric way, meaning that what you're doing is you actually sort of do a little optimization for each X that you have.",
                    "label": 0
                },
                {
                    "sent": "You tried to fit Q and so for that you need some sort of tractable family of Q and you fit the best you typically like mean field.",
                    "label": 0
                },
                {
                    "sent": "So queues the zeds would be independent under mean field assumption and you fit the best independent distribution you can for that X.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variational unquote is something very different.",
                    "label": 0
                },
                {
                    "sent": "It uses a second neural net structure, second only to actually predict the parameters of your queue, so it's important to note that this will the assumption we're still making here is that it's actually going to be mean field, or the Zeds are going to factorize, but it's still the conditional distribution here is interesting, because now we're just going to input X and we're going to come up with the parameters of a distribution over Q.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So we've got a neural net encoding, sort of in the decoding model here, and a neural net in the encoding model here in Q.",
                    "label": 0
                },
                {
                    "sent": "And we have our optimization function, so this is the basic structure of the VE.",
                    "label": 0
                },
                {
                    "sent": "There's one more trick that you need to know about to make this thing work.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what we call the reparameterization trick.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to train this thing, right?",
                    "label": 0
                },
                {
                    "sent": "We know how to do inference now, assuming we can train this model, we know how to do inference in this thing, and we can generate from this model.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do we train it?",
                    "label": 0
                },
                {
                    "sent": "Well, what we're using is, we're going to use this memorization trick, which says that we have this Q of said given X is just, we're going to find this thing to be a normal distribution with some mean in some variance, which is a function of X.",
                    "label": 0
                },
                {
                    "sent": "But it's also these things.",
                    "label": 0
                },
                {
                    "sent": "Are parameterized by this neural net here our inference model, and we're going to parameterise zed as being in this kind of a fashion here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is just a way to parameterise this normal distribution, so you have some mean plus some variance times epsilon, which is just a random draw from the standard normal distribution, so why aren't parameterized things this way?",
                    "label": 0
                },
                {
                    "sent": "Well, the advantages is that now we have a way of back cropping, so here this is just a neural net.",
                    "label": 0
                },
                {
                    "sent": "We can back propagate error all the way through here.",
                    "label": 0
                },
                {
                    "sent": "And by the way, we can do the same thing here.",
                    "label": 0
                },
                {
                    "sent": "This is actually optional.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do this.",
                    "label": 0
                },
                {
                    "sent": "You can have any kind of standard distribution on X.",
                    "label": 0
                },
                {
                    "sent": "Here can be discrete continuous.",
                    "label": 0
                },
                {
                    "sent": "That's not a problem, but we can now back prop through here.",
                    "label": 0
                },
                {
                    "sent": "Usually the problem is is that we get to zed and we can't back prop through.",
                    "label": 0
                },
                {
                    "sent": "But now with this kind of parameterisation we actually can, we can back propagate through these parameters just fine.",
                    "label": 0
                },
                {
                    "sent": "We don't propagate back propagate through epsilon, but that's fine.",
                    "label": 0
                },
                {
                    "sent": "It's a standard Apple, and there's no parameters to learn there, so we back propagate through mu and Sigma and then we can learn the parameters of the.",
                    "label": 0
                },
                {
                    "sent": "Encoder model so this gives us a way to learn the decoder model and the encoder model, and we can optionally also if we want to parameterise the input this way.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions up to this point, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So the good points OSI has to be continuous right?",
                    "label": 0
                },
                {
                    "sent": "So what I've given you is the option of it being Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's one option, but it certainly has to be is continuous in order to do this back propagation.",
                    "label": 0
                },
                {
                    "sent": "If you want it to be discrete, you'd actually have to use some other technique like the reinforce algorithm or something like that, But that's kind of typically not what to use in the VE context.",
                    "label": 0
                },
                {
                    "sent": "Any other questions, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is just a.",
                    "label": 0
                },
                {
                    "sent": "This is just a deterministic mapping from zed 2X.",
                    "label": 0
                },
                {
                    "sent": "And if you like it's it's, it's just a neural net, right?",
                    "label": 0
                },
                {
                    "sent": "You've got you literally take zed as your input.",
                    "label": 0
                },
                {
                    "sent": "And you propagate that through and then you have parameters if you will, from which you can samples at which define a distribution that samples X. Szeto said the directed model is said to X.",
                    "label": 0
                },
                {
                    "sent": "That's it, that's the directed model, so you're right.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of.",
                    "label": 0
                },
                {
                    "sent": "This is, I guess, confusing in the sense that it's showing a directed model through these layers, but it's actually the directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "Here is Zed arrow X.",
                    "label": 0
                },
                {
                    "sent": "And that's it, yeah.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "That's exactly right.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "In this case, yes, yeah, But that's going to be true always for I mean, this part is the decoder network generating sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "You can, yeah, I mean, you're just parameterising some low level distribution here.",
                    "label": 0
                },
                {
                    "sent": "And this is generating the parameters for that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, functions for that and the same is true over here, right?",
                    "label": 0
                },
                {
                    "sent": "So you start with some X and you generate conditionally the parameters of a distribution.",
                    "label": 0
                },
                {
                    "sent": "Now in this case it actually has to be something like a continuous distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case we're talking about a Gaussian, because that we need to be able to back prop through.",
                    "label": 0
                },
                {
                    "sent": "Makes sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But the pre parameterisation trick is just expressing this.",
                    "label": 0
                },
                {
                    "sent": "Distribution in this form.",
                    "label": 0
                },
                {
                    "sent": "Right, so so that we can actually back so we have, uh, where this is a random variable and we're writing it as a function of a deterministic component and then the sum of the deterministic component times something purely stochastic.",
                    "label": 0
                },
                {
                    "sent": "So the stochastic element we can't backdrop through, but both of the deterministic parts we can.",
                    "label": 0
                },
                {
                    "sent": "And that's yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's an important point, so if you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get the if you go back and you look at the this term here right, you see that this term is going to want this decoder to essentially be able to reconstruct the individual example as best it can.",
                    "label": 0
                },
                {
                    "sent": "So so in this term is essentially acting to kind of make the set as unique as possible in order that it can reconstruct the X as accurately as possible.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this term here you see it's being regularizer, it wants this posterior distribution to be as close to P as possible.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the prior on Zed.",
                    "label": 0
                },
                {
                    "sent": "As possible, So what that's going to do is essentially do the opposite.",
                    "label": 0
                },
                {
                    "sent": "It's going to sort of push them together and make them as noisy as possible.",
                    "label": 0
                },
                {
                    "sent": "You essentially this term kind of is happiest when when the output of Q here is independent of X and it's this interplay between the two that actually gives you the posteriors matching.",
                    "label": 0
                },
                {
                    "sent": "The prior this P of zed here.",
                    "label": 0
                },
                {
                    "sent": "That's typically something simple like a Gaussian distribution on set, an independent Gaussian distribution that we can sample from easily.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good point so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "I think that's a good way to see it.",
                    "label": 0
                },
                {
                    "sent": "And just like that, so there's a sort of a temptation here.",
                    "label": 0
                },
                {
                    "sent": "When working with this to sort of look at this term and think that.",
                    "label": 0
                },
                {
                    "sent": "You want this thing to actually kind of match.",
                    "label": 0
                },
                {
                    "sent": "You actually don't write, it's just like it's just like any regularization term.",
                    "label": 0
                },
                {
                    "sent": "You don't actually want it to go to the optimum of that term, right, so?",
                    "label": 0
                },
                {
                    "sent": "Same thing here, right?",
                    "label": 0
                },
                {
                    "sent": "This is just the other part of the term and the actually the other thing that's nice is that this this sort of division into this reconstruction term plus a regularizer just comes from the variational lower bound.",
                    "label": 1
                },
                {
                    "sent": "We've added nothing to it.",
                    "label": 0
                },
                {
                    "sent": "These two pieces are just a reformulation of that same variational lower bound that everyone uses when they do variational methods.",
                    "label": 0
                },
                {
                    "sent": "So so connect this to the the contractive automotive.",
                    "label": 0
                },
                {
                    "sent": "You guys have seen the connector contractive, autoencoder, have you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so so it's similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "I mean it's.",
                    "label": 0
                },
                {
                    "sent": "I guess one difference would be that I mean you could apply.",
                    "label": 0
                },
                {
                    "sent": "Typically that's applied at every layer right here.",
                    "label": 0
                },
                {
                    "sent": "It's only being applied at one point here, and the effect tends to be that it's a kind of a choke point.",
                    "label": 0
                },
                {
                    "sent": "We're going to see in a minute the effect of this regularization is actually pretty dramatic, so in some sense it's may be used in a different regime then contractive auto encoders.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "This term yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the denoising auto encoder will.",
                    "label": 0
                },
                {
                    "sent": "If you fix this to one, then what you have is a.",
                    "label": 0
                },
                {
                    "sent": "You still have a regularization effect.",
                    "label": 0
                },
                {
                    "sent": "That's happening on the mean here.",
                    "label": 0
                },
                {
                    "sent": "This is actually still being pushed towards the zero basically, or the mean of you're still getting the this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, this term is still acting on the mean parametrization.",
                    "label": 0
                },
                {
                    "sent": "Right, right, but I'm right exactly, but what I'm saying is it isn't isn't strictly the same thing as a denoising autoencoder.",
                    "label": 0
                },
                {
                    "sent": "If you fix that term.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so I think the main points where this crucially differs is that this is a way to use all of the leverage.",
                    "label": 0
                },
                {
                    "sent": "All of the optimization methods, all the things that we've learned about neural Nets in the Latin, what you've learned about neural Nets in the last week and a half to build directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "Then that you don't get, at least simply from denoising autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Yoshua, I don't know if you're going to talk about this, the your generative at.",
                    "label": 0
                },
                {
                    "sent": "Sorry your GSN's.",
                    "label": 0
                },
                {
                    "sent": "OK, so you'll learn about how you can actually get other generative models out of things like denoising autoencoders, so that's actually a bridge between these kinds of methods.",
                    "label": 0
                },
                {
                    "sent": "But what we're really getting out of this technique is actually maybe more innovative if you think about from the generative model POV.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "So in a denoising autoencoder, you don't usually have these probabilistic semantics.",
                    "label": 0
                },
                {
                    "sent": "Sort of given to you, at least immediately.",
                    "label": 0
                },
                {
                    "sent": "So you can't necessarily think about Zed as being in that context is being a random variable, whereas here we can.",
                    "label": 0
                },
                {
                    "sent": "And that's actually pretty powerful.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is at least we haven't tended to see too much of the kind of distance.",
                    "label": 0
                },
                {
                    "sent": "Kind of extracting these factors of variation or disentangle in these factors of variation, whereas in this case it's actually pretty extreme.",
                    "label": 0
                },
                {
                    "sent": "You see it, and it's it can be fairly dramatic, will get into a second of the kind of thing we're seeing in that just in terms of the kind of the effect that that prior has on zed.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's right yeah, so in that context that is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that is similar.",
                    "label": 0
                },
                {
                    "sent": "I've not seen anyone actually compare those like this from the distant angling that this kind of model gives you versus the disentanglement.",
                    "label": 0
                },
                {
                    "sent": "Something like that would give you I've not seen that compared, so that would be interesting, I agree.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "Noise.",
                    "label": 0
                },
                {
                    "sent": "Big difference.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's exactly right, yeah, so thank you good good yeah, that's a very good way to say, yeah, so that's that's why I'm saying the big loss function is different and that's what's giving rise to a different kind of regularization here.",
                    "label": 0
                },
                {
                    "sent": "So specifically in the example he gave, you have the you have the.",
                    "label": 0
                },
                {
                    "sent": "The variance term was fixed, but this term still acts on the main component, right to regularize that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The pixel values are multiple.",
                    "label": 0
                },
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK I sounds reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "But but that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so so just to be clear, if you if you have say for example just reconstruction error like least squares reconstruction error, right that content you can see that you can interpret that as just having a Gaussian distribution over X&X being random variables.",
                    "label": 0
                },
                {
                    "sent": "And that's actually that is the easy part, right?",
                    "label": 0
                },
                {
                    "sent": "What's what's difficult in?",
                    "label": 0
                },
                {
                    "sent": "This is actually getting, well, difficult.",
                    "label": 0
                },
                {
                    "sent": "The technical trick was that was used as is having some distribution over said that we can back propagate through in order to train our inference net.",
                    "label": 0
                },
                {
                    "sent": "'cause that's what's without the inference that you can't train your generative, not your decoder.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, it's it's in the statistics community.",
                    "label": 0
                },
                {
                    "sent": "It's it's a very old trick.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact, I mean in almost every community, right?",
                    "label": 0
                },
                {
                    "sent": "I've seen in in the.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a standard way of writing, driving things like the common distribution to actually.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "It's a very standard thing to do.",
                    "label": 0
                },
                {
                    "sent": "It's just for whatever reason we did.",
                    "label": 0
                },
                {
                    "sent": "Not until these guys, these two groups thought of doing this.",
                    "label": 0
                },
                {
                    "sent": "We have not thought of doing it in this context.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we're getting to how to train this model and we basically already discussed this because we have this reparameterization trick we can simelton train this, simultaneously train both the generative model and the inference model just by standard back propagation.",
                    "label": 1
                },
                {
                    "sent": "So we forward propagate through the entire model going from X to X had some or some we can sample from this, or we can just actually look at the reconstruction error under our distribution that we generate from zed, so we actually have to do is sample from zed here.",
                    "label": 0
                },
                {
                    "sent": "So we generate sample here and then generate full and then forward propagate.",
                    "label": 0
                },
                {
                    "sent": "We derive our error and then we back propagate our error and we train it just like we would in a standard neural net.",
                    "label": 0
                },
                {
                    "sent": "So the only real difference is that we're sort of injecting noise here.",
                    "label": 0
                },
                {
                    "sent": "That's one way to think about what we're doing.",
                    "label": 0
                },
                {
                    "sent": "And we have our objective function and we just do standard grading, descent or any kind of optimization methods you want.",
                    "label": 0
                },
                {
                    "sent": "In this case, actually.",
                    "label": 0
                },
                {
                    "sent": "So one of the one of the points, I think is worth pointing out is.",
                    "label": 0
                },
                {
                    "sent": "Is you think about that.",
                    "label": 0
                },
                {
                    "sent": "If you want a generative model, it's fairly deep and you're going to want an inference model that's fairly deep as well.",
                    "label": 0
                },
                {
                    "sent": "So the the training this thing is essentially adopt training like sort of a twice that length.",
                    "label": 0
                },
                {
                    "sent": "That depth, right?",
                    "label": 0
                },
                {
                    "sent": "So actually you can have issues training this model if you want a very deep generative model or inference model.",
                    "label": 0
                },
                {
                    "sent": "So tricks like I guess in fact, Vincent has found things like batch normalization turned out to help quite a bit for this.",
                    "label": 0
                },
                {
                    "sent": "For training this kind of model so it is a bit of a challenge to train these.",
                    "label": 0
                },
                {
                    "sent": "Another reason why inference.",
                    "label": 0
                },
                {
                    "sent": "Training is challenging.",
                    "label": 0
                },
                {
                    "sent": "Is that your your?",
                    "label": 0
                },
                {
                    "sent": "Your regularization term comes in at that point, right?",
                    "label": 0
                },
                {
                    "sent": "So it's acting purely on the this term right here.",
                    "label": 0
                },
                {
                    "sent": "Your kid divergences acting at this point, so it's acting just on the inference network.",
                    "label": 0
                },
                {
                    "sent": "So typically the kind of learning dynamics you see when you're training this is is this the inference model just sort of learns to match the prior on zed early on, so the distribution that you predict for no matter any X here comes to match this, and then from there it has to sort of.",
                    "label": 0
                },
                {
                    "sent": "Dude, it's best at reconstructing.",
                    "label": 0
                },
                {
                    "sent": "So essentially it learns to forget information about X and then it has to sort of recover that information.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a pretty common early state in training, and then it sort of comes out of that and starts to do a decent job reconstructing X.",
                    "label": 0
                },
                {
                    "sent": "That's a typical learning dynamic you'd see.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so when you apply this kind of model you can compare it to other things, so this is a case where it's applied I think to relatively simple task.",
                    "label": 0
                },
                {
                    "sent": "This actually comes from there.",
                    "label": 0
                },
                {
                    "sent": "There one of the original papers and this is just a point saying that they've compared to wake sleep and MCM.",
                    "label": 0
                },
                {
                    "sent": "So this is a Monte Carlo OEM using hybrid Monte Carlo here.",
                    "label": 0
                },
                {
                    "sent": "And what we basically see is it's basically competitive with MCM which is a much more expensive algorithm 'cause you're doing many samplings from the posterior distribution here.",
                    "label": 0
                },
                {
                    "sent": "And then you get you see you're not quite as good as, but in the context where you are doing it on full, this is in the case of MNIST when you can't do it on the full data set, you see that it gets much more because the M is basically too expensive to run in a practical way, so so it ends up doing much better, so it's a scalable method that ends up performing quite well and you can see the performance is much better than standard wake sleep.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at what's going on, sort of under the hood of this myth of this variational autoencoder, you can see the effect of the Cal term here.",
                    "label": 0
                },
                {
                    "sent": "So what's being plotted here across the bottom in each one of these graphs is the number of dimensions of zed that we have an in.",
                    "label": 0
                },
                {
                    "sent": "This axis is just the KL divergent's right for that component and what you see is that for small values of zed it says relatively constant, but as soon as you have a sort of critical mass of number of components and said some of these, just start to go to 0.",
                    "label": 0
                },
                {
                    "sent": "The KL divergent starts to go to zero, yeah?",
                    "label": 0
                },
                {
                    "sent": "You're but each.",
                    "label": 0
                },
                {
                    "sent": "In this case you're training a 5 dimensional zed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you're across the graphs.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality of zed is increasing.",
                    "label": 0
                },
                {
                    "sent": "From here to here to hear, to hear.",
                    "label": 0
                },
                {
                    "sent": "So what we're seeing here is that we have some components of zed, and here it's it's even more dramatic.",
                    "label": 0
                },
                {
                    "sent": "The KL divergent goes to zero.",
                    "label": 0
                },
                {
                    "sent": "What this means is that the output for that dimension of zed is essentially matches the prior right?",
                    "label": 0
                },
                {
                    "sent": "So it's essentially not using that dimension to reconstruct the example.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty common characteristic of the variational autoencoder and what it's basically saying is that you're basically it's picking a suitable dimensionality to redo the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "And then it takes the rest of those, those dimensions that you've given it and just sets them to zero.",
                    "label": 0
                },
                {
                    "sent": "It doesn't use them, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well, so my experience is, or at least I think I think, yeah, it's possible.",
                    "label": 0
                },
                {
                    "sent": "It's just a question of you.",
                    "label": 0
                },
                {
                    "sent": "Don't know what that is apriori, right?",
                    "label": 0
                },
                {
                    "sent": "You kind of have to do a few experiments and then find out what it is, but I think in general it is slightly easier to give it a few more dimensions to optimize.",
                    "label": 0
                },
                {
                    "sent": "That's I don't know Van side.",
                    "label": 0
                },
                {
                    "sent": "Do you have an opinion on that which you've done?",
                    "label": 0
                },
                {
                    "sent": "Most of these experiments?",
                    "label": 0
                },
                {
                    "sent": "Dimensions of Z being recruited for the reconstruction part of the term, and.",
                    "label": 0
                },
                {
                    "sent": "Progress is you have more and more of these dimensions.",
                    "label": 0
                },
                {
                    "sent": "Optimization technique.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, the variance is a function of the model itself, right?",
                    "label": 0
                },
                {
                    "sent": "So it's it's difficult to.",
                    "label": 0
                },
                {
                    "sent": "Control that essentially, I mean, at least if you want to, the way it's typically specified, you can sort of artificially impose a given variance, But then actually so.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "The variance of the gradient.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know that.",
                    "label": 0
                },
                {
                    "sent": "Actually I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know if Vincent has an idea.",
                    "label": 0
                },
                {
                    "sent": "It's possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have other issues too.",
                    "label": 0
                },
                {
                    "sent": "I don't think variance is actually the dominant effect.",
                    "label": 0
                },
                {
                    "sent": "I think that the real challenge in training is what I mentioned before that part of your objective function comes in at an earlier point.",
                    "label": 0
                },
                {
                    "sent": "So when training the encoder model you have a decent signal there early on in training, whereas the other part is given to you in a very distal way, like after the decoder.",
                    "label": 0
                },
                {
                    "sent": "So by the time it reaches your encoder, there's probably not nothing very sensible to learn from that, at least initially.",
                    "label": 0
                },
                {
                    "sent": "So I think that's what makes the the learning problem a bit more challenging here than you typically see, but I should emphasize that this actually contrained fairly fairly easily compared to other sort of methods, which which this thing is in competition with.",
                    "label": 0
                },
                {
                    "sent": "Let's say, where you have Sir, for example, discrete variable discrete said you have to use something like reinforce in that context, variances is a real issue, but I think that the parameter representation reparameterization trick here really helps for dealing with that variance issue.",
                    "label": 0
                },
                {
                    "sent": "Actually, so I think we had a question over here.",
                    "label": 0
                },
                {
                    "sent": "Or yes I can.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is what happens if you actually look at the weights corresponding to where you see that collapse.",
                    "label": 0
                },
                {
                    "sent": "So what I'm plotting here is the in red.",
                    "label": 0
                },
                {
                    "sent": "Here is the KL divergent's in blue.",
                    "label": 0
                },
                {
                    "sent": "Here it's the.",
                    "label": 0
                },
                {
                    "sent": "It's the weight norms associated with that unit projecting out of it, right?",
                    "label": 1
                },
                {
                    "sent": "So what you see is that where you have 0 Cal.",
                    "label": 0
                },
                {
                    "sent": "Divergance the weight norms essentially go to 0, so they basically aren't used, they collapse and it's basically like, OK, those dimensions.",
                    "label": 0
                },
                {
                    "sent": "We're just going to let them to cover the.",
                    "label": 0
                },
                {
                    "sent": "Regularization term the KL divergent's and I'm going to use the rest for reconstruction so it doesn't inject anymore noise than necessary into the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Exactly that's and that's I think that's a really important difference.",
                    "label": 0
                },
                {
                    "sent": "Actually, the support doesn't change in this case, so really what we're talking about your.",
                    "label": 0
                },
                {
                    "sent": "It's more like you know, like a nonlinear principal components analysis, where you're kind of put going through this bottleneck and the bottleneck is actually chosen by the model itself, so you can almost think of it like a kind of a its own regularization where it's picking its own internal dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Now I mean, so this is kind of you can see this is a good thing, right?",
                    "label": 0
                },
                {
                    "sent": "Oh, it's picking it's doing its own regularization in terms of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "That's wonderful, right?",
                    "label": 1
                },
                {
                    "sent": "But on the other hand, you can also see this as maybe a negative right.",
                    "label": 0
                },
                {
                    "sent": "Maybe you actually want something that looks more like sparse coding, where where it's sort of dynamic, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you think you probably have a bit more capacity if you were to do that, not force it through some some static bottleneck like you have here.",
                    "label": 0
                },
                {
                    "sent": "Thanks yeah.",
                    "label": 0
                },
                {
                    "sent": "So so certainly we have seen that that's actually how training progress is.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have an opinion on whether that's still the case that we find with batch normalization, but I I know that we've seen in the past where that's essentially how it how it goes.",
                    "label": 0
                },
                {
                    "sent": "It collapses a bunch of terms and then slowly adds them, but from a computational point of view, there's there's no game 'cause you're running on all of them.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, but you're.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I don't years.",
                    "label": 0
                },
                {
                    "sent": "There's going to be much of a competition that benefit 'cause if you think about this is on emnace, right?",
                    "label": 0
                },
                {
                    "sent": "So we're using something like, you know 500 hidden units in the in the MLP and then it goes down to something like 20 Z.",
                    "label": 0
                },
                {
                    "sent": "So there's a pretty big order of magnitude difference there so I don't see there being much of a computational benefit to reducing zed so dramatically there because you're still going through a multi layer MLP with 500 hidden units each, so it's not clear to me that there's a benefit there.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Prior from.",
                    "label": 0
                },
                {
                    "sent": "Well, what it's doing is it's ignore it.",
                    "label": 0
                },
                {
                    "sent": "That's actually I disagree with that statement.",
                    "label": 0
                },
                {
                    "sent": "You what you just said is, I think what you're saying is that if you now just try to think of this thing as appear generative model where you sample from the prior and see what kind of reconstruction it gives you, it wouldn't do very well.",
                    "label": 0
                },
                {
                    "sent": "That's not true, because what it's doing is it's essentially picking a few dimensions that it's going to use for to determine the what digit you're going to generate, and it just ignores the rest.",
                    "label": 0
                },
                {
                    "sent": "It's not like you don't train on the rest of the, it's just simply ignoring them.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean in the all the other components it's pretty much matching exactly the prior.",
                    "label": 0
                },
                {
                    "sent": "It's ignoring those components, yes, so you could think that there's more capacity that it could use to do the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "That I agree with.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, I think there it is.",
                    "label": 0
                },
                {
                    "sent": "It is a weird property of the model.",
                    "label": 1
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I I mean I have not studied this question explicitly.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should.",
                    "label": 0
                },
                {
                    "sent": "'cause I think about it a lot, but my perspective is that it does seem to over regularize compared to what you would hope it would do, so I don't know what that I mean.",
                    "label": 0
                },
                {
                    "sent": "What that means?",
                    "label": 0
                },
                {
                    "sent": "I mean you're free, I mean this is just with a particular choice of prior right independent Zeds.",
                    "label": 0
                },
                {
                    "sent": "If you explore different.",
                    "label": 0
                },
                {
                    "sent": "Choices of that prior you might find you get different behavior, so we've actually done some of that exploration.",
                    "label": 0
                },
                {
                    "sent": "We haven't that direction.",
                    "label": 0
                },
                {
                    "sent": "We haven't found very much that's better.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so we when we were first doing these kinds of experiments, we thought there was what we found was that the deeper the models, the more sparsity you tended to see in zed.",
                    "label": 0
                },
                {
                    "sent": "And we thought that was a real effect and we had this story that said something like Oh well, you know it's higher capacity so we can use this non linearity to cram it into into, you know a smaller dimension and these other thing is said is is continuous valued right?",
                    "label": 0
                },
                {
                    "sent": "So so in some sense all it needs is one dimension right?",
                    "label": 0
                },
                {
                    "sent": "It can lay out all of the data in one dimension and just have very little noise and it can reconstruct the input.",
                    "label": 0
                },
                {
                    "sent": "Now that's not going to generalize very well but but in principle it doesn't need multiple dimensions.",
                    "label": 0
                },
                {
                    "sent": "To encode the input because of because of its continuous.",
                    "label": 0
                },
                {
                    "sent": "So I don't think that that turned out to be probably more of an optimization issue, and that if you sort of initialize it and use kind of better optimization techniques, you don't tend to see this correlation between the depth or capacity of the encoder decoder and the zed that's chosen the dimensionality of said that's used.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Which sleep algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK so plants are good 'cause I'm not really an expert on week sleep but.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The essentially the wake sleep uses the variational bound, the sort of that we're using here uses it in a different order, and it's using.",
                    "label": 0
                },
                {
                    "sent": "You're basically using.",
                    "label": 0
                },
                {
                    "sent": "It's more like if you had one graphical model and you have a different set of weights projecting between them, and every layer is considered to be stochastic, so it's in some sense the graphical model is closer to something like a deep Boltzmann machine, and you just have two sets of weights projecting one set.",
                    "label": 0
                },
                {
                    "sent": "Projecting up one set.",
                    "label": 0
                },
                {
                    "sent": "Projecting down.",
                    "label": 0
                },
                {
                    "sent": "So formally, that's the don't you want something about that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Right, so just just on that point.",
                    "label": 0
                },
                {
                    "sent": "To be fair though, like this, 5 dimensions were modeling like MNIST or something similar, right?",
                    "label": 0
                },
                {
                    "sent": "And so the kind of dimensionality you see for something like I missed something like between 20 and 30 dimensions is kind of where we kind of usually sit.",
                    "label": 0
                },
                {
                    "sent": "So whether Amnesty needs more than that number of dimensions, I'm not sure, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. Slightly though, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah alright so so.",
                    "label": 0
                },
                {
                    "sent": "But you still getting dead units right?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure it's definitely not using the capacity that it potentially could use, right?",
                    "label": 0
                },
                {
                    "sent": "So the objective function is in some sense bottlenecking the process.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is basically the more capacity you give it, the more capacity it chooses to use, which is almost that sounds almost more like an optimization issue in itself.",
                    "label": 0
                },
                {
                    "sent": "Yeah anyway, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, but in this case right, the true posterior is also going to ignore these dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "Because the.",
                    "label": 0
                },
                {
                    "sent": "Actually wait is that is that true?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it will ignore these dimensions because there's they is X along these dimensions.",
                    "label": 0
                },
                {
                    "sent": "X is independent of Z.",
                    "label": 0
                },
                {
                    "sent": "So the posterior is just going to be the prior along those dimensions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, it's just it's it's it's X, Zedd and has no.",
                    "label": 0
                },
                {
                    "sent": "Zedd provides no information about X, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So, but I mean certainly there's a yeah, this is an issue there.",
                    "label": 0
                },
                {
                    "sent": "Are and yeah and independent, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I would imagine it would impact it, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, you could well in a trivial sense you can make it impact it by just saying I'm going to set like one of my dimensions to have significant variance and all the rest to have essentially zero variance.",
                    "label": 0
                },
                {
                    "sent": "And then you're forcing it to go in that one dimension, right?",
                    "label": 0
                },
                {
                    "sent": "So if you made that soft, yeah, you would be definitely impacting that.",
                    "label": 0
                },
                {
                    "sent": "I don't have a good way to prescribe that prior for you, though I would almost rather learn it, but learning it actually doesn't really work that well either because it's more like I think you're better off actually keeping it as a kind of a constant constraint, because it's a challenging optimization problem as it is, and the effect that that prior has on the resulting encoder is such that if you, if it were to change dramatically early in training, you could really hurt performance.",
                    "label": 0
                },
                {
                    "sent": "Overall, OK, let's one last question here and then we're going to move on.",
                    "label": 0
                },
                {
                    "sent": "For the prior, do you mean so yeah.",
                    "label": 0
                },
                {
                    "sent": "So one thing we have done is experimented with.",
                    "label": 0
                },
                {
                    "sent": "It like mixture models for priors, things like that so you can do that.",
                    "label": 0
                },
                {
                    "sent": "It's you have to make an approximation to the objective function to make that work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't remember us having a lot of success with that kind of structure.",
                    "label": 0
                },
                {
                    "sent": "What actually, what?",
                    "label": 0
                },
                {
                    "sent": "In fact the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There tends to be turns out to be difficult, right?",
                    "label": 0
                },
                {
                    "sent": "'cause again because of this dependency between the encoder and prior.",
                    "label": 0
                },
                {
                    "sent": "So typically we typically have the best results when we use just a fairly simple prior and what actually ends up happening, and this is again, this was some time ago, so maybe now if we tried this with better optimization methods, we'd see a different result.",
                    "label": 0
                },
                {
                    "sent": "But what we tend to found what we found before.",
                    "label": 0
                },
                {
                    "sent": "Was that when we tried something like that?",
                    "label": 0
                },
                {
                    "sent": "Is that the model would just pick one of the mixture components that we use Gaussian mixtures?",
                    "label": 0
                },
                {
                    "sent": "It would just pick one of the mixture components and say I'll just use that 'cause the encoder decoder were powerful enough to be able to use that to model the data so it didn't have much incentive to use the other.",
                    "label": 0
                },
                {
                    "sent": "The other capacity in the prior that we gave it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move on to right, so that was that's basically it for the VA. Now we're just going to talk about other work that's been done to be.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hold up on this and what we're going to talk about is this was published in NIPS 2004.",
                    "label": 0
                },
                {
                    "sent": "This is a semi supervised learning or essentially building on the V model, so they basically introduced the study two basic approaches to using the VE type architecture in the context of semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The first is basically the same architecture that we would use sort of kind of very common in the deep learning literature pre 2012 right?",
                    "label": 0
                },
                {
                    "sent": "So that is we first train.",
                    "label": 0
                },
                {
                    "sent": "AV in this case of EAE model but basically just any kind of unsupervised model, A single layer unsupervised model.",
                    "label": 0
                },
                {
                    "sent": "Now in this case remember this.",
                    "label": 0
                },
                {
                    "sent": "So this is the graphical model associated with the VE.",
                    "label": 0
                },
                {
                    "sent": "But in this case there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a whole non linearity here.",
                    "label": 0
                },
                {
                    "sent": "There's a whole MLP map in this map, but any case you train this kind of model and then you would then train a supervised model going from using the posterior to recover zed from X and then training zed to the label.",
                    "label": 0
                },
                {
                    "sent": "So that's one way to do it.",
                    "label": 0
                },
                {
                    "sent": "The other approach they took was something else, so that's what they, the other approach was.",
                    "label": 0
                },
                {
                    "sent": "Essentially that they would combine zed and the labels through an MLP too into X itself.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of an interesting model because basically what you're asking the zed to do here is different than here, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case said is just modeling everything, it's incorporating information about the label and everything else.",
                    "label": 0
                },
                {
                    "sent": "In this case, presumably the label information is taken care of here by by putting the label directly in here, so for Ennis for example you would say OK, Now I'm modeling a one, so this will be a one.",
                    "label": 0
                },
                {
                    "sent": "And Zeds role here is ideally to model everything else.",
                    "label": 0
                },
                {
                    "sent": "That's all these other factors of variation that are associated with that particular one.",
                    "label": 0
                },
                {
                    "sent": "That would then be generated, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of modeling everything else other than the label.",
                    "label": 0
                },
                {
                    "sent": "And they.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They also explored one other kind of model configuration, which is just sort of a combination of the two, so they have this latent, so they have this typical model VS.",
                    "label": 0
                },
                {
                    "sent": "Structured model here and then they would just stack there M2 model.",
                    "label": 0
                },
                {
                    "sent": "On top of that.",
                    "label": 0
                },
                {
                    "sent": "So now they're just kind of making a bit of a deeper connection between the Fusion of the latent variables that have nothing to do with the label plus the label into into X.",
                    "label": 0
                },
                {
                    "sent": "Here through this other layer here.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, they didn't explore that.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I mean you could imagine all kinds of different ways of incorporating this info.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you had, if you had the model where you have something like, I guess this model here, right?",
                    "label": 0
                },
                {
                    "sent": "Then what you're basically saying is it's.",
                    "label": 0
                },
                {
                    "sent": "I mean it's in some sense it's a lot like this, right?",
                    "label": 0
                },
                {
                    "sent": "Zed is incorporating information about the label.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in inference, I'm going to go over this pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "The details are obviously in their paper, but the labels are typically categorical labels, so this is a discrete random variable inference.",
                    "label": 0
                },
                {
                    "sent": "In this case, this is your standard via E model.",
                    "label": 0
                },
                {
                    "sent": "In this case, what you have for inference is basically a model that goes from X to Y and then then the inference process on zed is actually conditional on X&Y itself.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "They get right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's right.",
                    "label": 0
                },
                {
                    "sent": "Which is actually, I mean.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's actually will get.",
                    "label": 0
                },
                {
                    "sent": "Will get to that in a second but but I mean of course they do right?",
                    "label": 0
                },
                {
                    "sent": "Those are just literally injecting noise into your process, so it would make sense that they would throw those out.",
                    "label": 0
                },
                {
                    "sent": "That's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, we'll touch on that point again in a second.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is our how we were doing inference.",
                    "label": 0
                },
                {
                    "sent": "And again, these are just always this sort of.",
                    "label": 0
                },
                {
                    "sent": "You see this process here where we have this arrow here in this area.",
                    "label": 0
                },
                {
                    "sent": "Here these are just being concatenated, then passed to MLP which is predicting the parameters of Z and then again that is going feats.",
                    "label": 0
                },
                {
                    "sent": "Then you sample from that zedan then you.",
                    "label": 0
                },
                {
                    "sent": "Project that into your decoder net to predict X again, right?",
                    "label": 0
                },
                {
                    "sent": "So it's just the standard kind of way you would imagine the VE being incorporated into a ski.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Right, so just a note here.",
                    "label": 0
                },
                {
                    "sent": "They need actually in this case if they're going to train with labeled and unlabeled data, they need a way of being able to train this structure when you don't have.",
                    "label": 0
                },
                {
                    "sent": "When you don't have Y, right?",
                    "label": 0
                },
                {
                    "sent": "So they the way they do that is that they basically just use their joint model and do essentially marginalized out why stochastically in the case, actually that you can do it stochastically.",
                    "label": 0
                },
                {
                    "sent": "But I actually think because it's categorical here, you don't actually have to do it statically.",
                    "label": 0
                },
                {
                    "sent": "You can actually marginalized out explicitly because you can just.",
                    "label": 0
                },
                {
                    "sent": "And you can somehow all possible values of why there?",
                    "label": 0
                },
                {
                    "sent": "So that's just how they deal with the win.",
                    "label": 0
                },
                {
                    "sent": "Why is missing so in the case where you're modeling your training of this on purely unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "And and just another just another last point when they're actually using this for classification.",
                    "label": 0
                },
                {
                    "sent": "They also train with what would be essentially a traditional classification algorithm here, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK so so OK. Yeah, I forgot about so they they they prescribe sampling.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, that's good.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so results right?",
                    "label": 0
                },
                {
                    "sent": "So this is getting back to the point that Kyle was raising, which is there when you do so M1 and so this is an M nest, right?",
                    "label": 0
                },
                {
                    "sent": "So what they're doing here is they're actually looking at the case where they've limited the number of training examples, so this is a semi supervised case.",
                    "label": 0
                },
                {
                    "sent": "They've trained in an unsupervised way on all of the data and given, for example 1000 labels here.",
                    "label": 0
                },
                {
                    "sent": "And what they find is if they take just the M1.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the classical way of using this, just M1 and plus train put that through a transductive SVM.",
                    "label": 0
                },
                {
                    "sent": "You get something like 11% error if they go with their M1 plus M2.",
                    "label": 0
                },
                {
                    "sent": "You get very good performance here and this is essentially the kinds of performance they see in their algorithm.",
                    "label": 0
                },
                {
                    "sent": "And just to give you some comparison, this is the contractive autoencoder here.",
                    "label": 0
                },
                {
                    "sent": "These are the kind of performance you see, so significantly better, especially in the context where you have very few labeled examples, right?",
                    "label": 0
                },
                {
                    "sent": "So this is where you're seeing this effect of this bottleneck, as Kyle was mentioning right of coming through is that with very few examples you can't afford a very.",
                    "label": 0
                },
                {
                    "sent": "High dimensional space and the VA is giving you the ability to essentially contract and possibly, you know, decoupler.",
                    "label": 0
                },
                {
                    "sent": "Factors of variation, yeah.",
                    "label": 0
                },
                {
                    "sent": "You just, it would be more like this setting.",
                    "label": 0
                },
                {
                    "sent": "I think this is where you basically you pre train a model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then fine tune right?",
                    "label": 0
                },
                {
                    "sent": "So or you can.",
                    "label": 0
                },
                {
                    "sent": "I mean this is not pretend fine too, but you could stick an SVM on top of it, yeah?",
                    "label": 0
                },
                {
                    "sent": "One point 1% here with that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so that's that's really really interesting work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "So right actually in fact, that is the algorithm that gets so on full amnesty they get a very impressive result, which is .96%.",
                    "label": 0
                },
                {
                    "sent": "This is kind of I think at this point this was a couple.",
                    "label": 0
                },
                {
                    "sent": "Things are moving quickly.",
                    "label": 0
                },
                {
                    "sent": "This is about a year ago and.",
                    "label": 0
                },
                {
                    "sent": "No, it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It wasn't said the art, but it was close to state of the art then.",
                    "label": 0
                },
                {
                    "sent": "Now it's not so very close to state of the art, yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh well, the arrangement.",
                    "label": 0
                },
                {
                    "sent": "They're not the way their combined is in the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of a structure, so the way they train this thing is they first train this lower layer model.",
                    "label": 0
                },
                {
                    "sent": "They basically stack pre training they do.",
                    "label": 0
                },
                {
                    "sent": "You know stacked, pretraining, greedy pretraining, right.",
                    "label": 0
                },
                {
                    "sent": "So the first train, this model and then they trained this model as well as those samples from here where the data or they might even use the mean.",
                    "label": 0
                },
                {
                    "sent": "I forget if they forget how they treat this child you remember, do they when they when they stack this on top and their training this Top Model?",
                    "label": 0
                },
                {
                    "sent": "Are they sampling here or are they getting the means?",
                    "label": 0
                },
                {
                    "sent": "For example they're taking the means so they reduce the variance and by doing that yeah that makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is kind of a set of results you can see, so this is kind of playing with this.",
                    "label": 0
                },
                {
                    "sent": "Structure here where you've got zed and why being separate things so you can kind of clamp why the label and then sample from zed?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of showing you the kind of map I was showing you earlier where you're moving around the Dead Space and you're getting a bunch of different tools 'cause you've clamped up to two here.",
                    "label": 0
                },
                {
                    "sent": "We have 3, four and then we have this kind of analogy.",
                    "label": 0
                },
                {
                    "sent": "So in this case what they are doing is they are.",
                    "label": 0
                },
                {
                    "sent": "For a given row, here Zed is fixed and now they're just twiddling the Y, so for different.",
                    "label": 0
                },
                {
                    "sent": "So given this guy here, they do inference on this for so they figure out what is the zed associated with that for.",
                    "label": 0
                },
                {
                    "sent": "Then they fix that said, and then they move the Y and they can generate other digits that have that same zed, right?",
                    "label": 0
                },
                {
                    "sent": "So how that same sort of style?",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, you're sort of doing this separation of style and content with this Y and zed, and they're doing it for.",
                    "label": 0
                },
                {
                    "sent": "The SVN here too.",
                    "label": 0
                },
                {
                    "sent": "So these are house numbers where you're seeing a very similar sort of thing, right?",
                    "label": 0
                },
                {
                    "sent": "So they've got this two in this context, and then there's sort of and these are just pure samples from the model, right?",
                    "label": 0
                },
                {
                    "sent": "So these do not necessarily exist in the training or test set there.",
                    "label": 0
                },
                {
                    "sent": "Just here's a here's a.",
                    "label": 0
                },
                {
                    "sent": "You're given this example.",
                    "label": 0
                },
                {
                    "sent": "Then they take the set associated with that and change the Y, and they generate these.",
                    "label": 0
                },
                {
                    "sent": "So these are pretty impressive results.",
                    "label": 0
                },
                {
                    "sent": "Actually, in terms of separating style and content.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's move on.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now I.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to go over this pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "This is this is our recurrent model that we're basically making a recurrent version of the VA because what we're interested in is modeling sequential data.",
                    "label": 1
                },
                {
                    "sent": "Things like speech synthesis is actually really what our target is in this task, and right now that kind of space historically has been dominating things like Hmm's.",
                    "label": 0
                },
                {
                    "sent": "More recently, things like our like recurrent neural Nets have been very popular for that kind of thing, and our motivation here is that, well, we're seeing something pretty interesting with these vehicles, right?",
                    "label": 0
                },
                {
                    "sent": "With the fact that these latent variables are encoding some sort of interesting variation in the data, and we thought that maybe by building a sequential model that incorporates these kind of latent variables, we're going to see more natural variations in the data than we do with something like a standard RNN, where the only sort of variability or stochastic city in the standard RNN is right at the level of the data, right?",
                    "label": 1
                },
                {
                    "sent": "You're using like a mixture, Gaussian models typically, so that doesn't seem like a very natural place.",
                    "label": 0
                },
                {
                    "sent": "If you have this idea of this kind of data manifold, we want to be.",
                    "label": 0
                },
                {
                    "sent": "Variation is in the space.",
                    "label": 0
                },
                {
                    "sent": "It's quite far away from the data, so in this context we're sort of taking that same insight and applying it to sequential data.",
                    "label": 0
                },
                {
                    "sent": "So here we have our traditional via E model and we're just sort of adding recurrence here and then we're going to do something that's a little bit different than your standard V model is.",
                    "label": 1
                },
                {
                    "sent": "We're going to sort of make have a single recurrent neural net here.",
                    "label": 0
                },
                {
                    "sent": "That's going to do double duty.",
                    "label": 0
                },
                {
                    "sent": "It's essentially going to be used both as our encoder model and as our decoder model, where we still have sort of MLP's that connect.",
                    "label": 0
                },
                {
                    "sent": "X to Z that are that are sort of encoding what normally encoder and decoder functions is, but the information about the history is encoded into a sort of compressed into a single RNN.",
                    "label": 1
                },
                {
                    "sent": "Any questions about the structure we're going to go through the pieces.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now, so here we have our variational RNN.",
                    "label": 0
                },
                {
                    "sent": "We can sort of decompose this into a few different pieces.",
                    "label": 0
                },
                {
                    "sent": "So we have our prior structure.",
                    "label": 0
                },
                {
                    "sent": "We want to generate.",
                    "label": 0
                },
                {
                    "sent": "This is these are the components involved in that we have our recurrent structure here and we have our infrastructure.",
                    "label": 0
                },
                {
                    "sent": "We're going to take these essentially into.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turn 0 first our prior so the typical via E just has a prior on zed which would be like just zed here right?",
                    "label": 0
                },
                {
                    "sent": "In independent components on Zed our prior on Zed is a function of our recurrent hitting unit of the previous state here right?",
                    "label": 1
                },
                {
                    "sent": "So this is how we're incorporating information from the history were doing it, remediating that through our recurrent neural net hidden units.",
                    "label": 0
                },
                {
                    "sent": "And again, it's standard Gaussian distribution on zed, so so the only sort of innovation here is that we're conditioning on this.",
                    "label": 0
                },
                {
                    "sent": "This previous hidden units here, so that's what's new about this from the case of the prior.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For generation Now what we're going to do is we're going to.",
                    "label": 0
                },
                {
                    "sent": "The typical value would be you take this latent variable to current time step and generate X with that.",
                    "label": 0
                },
                {
                    "sent": "We're also going to generate where it's going to use our previous context.",
                    "label": 0
                },
                {
                    "sent": "Given mediated again by our last hidden recurrent hidden state.",
                    "label": 0
                },
                {
                    "sent": "And we're going to generate from X.",
                    "label": 0
                },
                {
                    "sent": "Given both of these two pieces of information, yeah?",
                    "label": 0
                },
                {
                    "sent": "Contact picture another right.",
                    "label": 0
                },
                {
                    "sent": "Right, so so one way to the other piece that's important about this is we're actually going to be exploiting the fact that what we have is a per timestep via E model.",
                    "label": 0
                },
                {
                    "sent": "That's conditional, and we're basically going to use that when it time.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When it comes time for learning just a real quick our our recurrence information now we're basically taking information from everywhere from X, from zed and from our previous or current state is coming in to define the current context.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for inference.",
                    "label": 0
                },
                {
                    "sent": "What we're doing again, we're taking this previous time step, and we're incorporating information about the current X into a model about the current zed.",
                    "label": 0
                },
                {
                    "sent": "And again, we're exploiting this fact that we're using a factor posterior where it's factored given X and the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, an implicitly the previous H is previous, Zeds are given through.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each year zero training again.",
                    "label": 0
                },
                {
                    "sent": "We're exploiting the fact that we can decompose this our objective function essentially into a series of V models in sequence, and then we back propagate through this whole thing through the encoder decoder structure as in a standard V, but also across time.",
                    "label": 1
                },
                {
                    "sent": "Doing backpropagation through time as in the standard RNN, and you just basically do that following this objective function, right?",
                    "label": 1
                },
                {
                    "sent": "So every just following the functional form that we.",
                    "label": 0
                },
                {
                    "sent": "We provide here.",
                    "label": 0
                },
                {
                    "sent": "You basically just do your back propagation and train as you would, essentially as you would have standard RNN.",
                    "label": 1
                },
                {
                    "sent": "But we inject the are stochastic city here in latent variable rather than at X.",
                    "label": 0
                },
                {
                    "sent": "We also have a simple latent like X is still latent in the sense that sorry X is still random in the sense that we have a probability distribution over expert.",
                    "label": 0
                },
                {
                    "sent": "Typically we use something simpler than when we have to use for an RNN, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "It's I mean the fact that we're sampling at every time step.",
                    "label": 0
                },
                {
                    "sent": "It's essentially the same approximate, it's a. Yeah, it's a similar approximation to use.",
                    "label": 0
                },
                {
                    "sent": "Typically make Innovia context, but you're right, I think you're right about this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so how does this model perform?",
                    "label": 0
                },
                {
                    "sent": "And again, what we're interested in doing is comparing it to a standard RNN model, and so which is given by, for example, here is an RNN model with a Gaussian distribution on X.",
                    "label": 0
                },
                {
                    "sent": "The data here is an RNN model with a Gaussian mixture model on X.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the standard way of injecting a lot of diversity in your in X, and what we have for speech modeling is.",
                    "label": 0
                },
                {
                    "sent": "We've got four different speech modeling datasets, so here what we're doing is we get we have a.",
                    "label": 0
                },
                {
                    "sent": "And this is on the raw input, so we have a raw input.",
                    "label": 0
                },
                {
                    "sent": "We're training this model and then we have it.",
                    "label": 0
                },
                {
                    "sent": "We're we're now comparing how likely a test set sequences under our model versus something like the standard recurrent neural net in all cases.",
                    "label": 0
                },
                {
                    "sent": "Here we use a thinking STM model in this case for the recurrent neural net, as well as our reusing LCM also in the in the context of our model.",
                    "label": 0
                },
                {
                    "sent": "So here we're seeing actually, so I guess what we typically see is pretty good performance.",
                    "label": 0
                },
                {
                    "sent": "I guess our model is performing fairly favorably compared to your standard RNN in both cases, so this is speech modeling and handwriting.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go to some examples.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a wide view of what speech looks like, and this is sort of a zoomed in view.",
                    "label": 0
                },
                {
                    "sent": "This is the this is our best standard RNN model that we could train.",
                    "label": 0
                },
                {
                    "sent": "You can see that we're capturing capturing a little bit more of the global structure here, as well as the local structure is a little bit cleaner.",
                    "label": 0
                },
                {
                    "sent": "You're seeing quite quite a bit more noise evident in the case of the just the normal RNN with a Gaussian mixture model, and you might think that this is sort of the kind of thing we'd expect to see.",
                    "label": 0
                },
                {
                    "sent": "If we're sort of being successful at modeling variations at a higher level, right?",
                    "label": 0
                },
                {
                    "sent": "Because the RNN, all it can do to model variations is move the data itself.",
                    "label": 0
                },
                {
                    "sent": "That's the only way you can generate something something different.",
                    "label": 0
                },
                {
                    "sent": "Whereas in our case when we do model, when we sample in our latent variable, weaken the model itself, concerns smooth that stochastic city out, and so that's why we're sort of seeing this kind of smoother pattern here and sort of matching a little bit closer to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Then we see with the RNG MMM.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting if you actually look at the KL divergent's across time here in the in the model.",
                    "label": 0
                },
                {
                    "sent": "So this is the waveform we get in as input.",
                    "label": 0
                },
                {
                    "sent": "The what we can see here is that the KL divergent actually seems to peak when it's when you're seeing sort of state transitions.",
                    "label": 0
                },
                {
                    "sent": "If you like between one pattern of behavior and another pattern of behavior.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting observe.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we have handwriting recognition.",
                    "label": 0
                },
                {
                    "sent": "So in this case both models actually perform reasonably well.",
                    "label": 0
                },
                {
                    "sent": "It's sort of the same type of thing that you see with kind of typical models of this kind of locally, it looks pretty compelling, But if you actually try to read these sentences on either case, it's pretty hard to make out any words.",
                    "label": 0
                },
                {
                    "sent": "This is the ground truth data.",
                    "label": 0
                },
                {
                    "sent": "These are two examples of RNN models, and this is our our variational RNN.",
                    "label": 0
                },
                {
                    "sent": "This time just we're just showing you with another Gaussian mixture.",
                    "label": 0
                },
                {
                    "sent": "All of the input.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we see a qualitative difference or quantitative difference between these in terms of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "I don't see a big difference in terms of the quality of, but maybe there's more variation here.",
                    "label": 0
                },
                {
                    "sent": "I don't know more consistency like this is sort of.",
                    "label": 0
                },
                {
                    "sent": "You see more consistency across the whole trajectory.",
                    "label": 0
                },
                {
                    "sent": "See maybe here.",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                },
                {
                    "sent": "It's not dramatic, but then again it's hard to really assess this OK, right?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I want to talk to you about a different model.",
                    "label": 0
                },
                {
                    "sent": "I don't think I'll get to the more more recent methods, which is too bad, but that's OK. Well, stopping here will probably be interesting enough.",
                    "label": 0
                },
                {
                    "sent": "But now I'd like to talk to you about the draw.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the draw stands for deep, recurrent, attentive writer.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea here is actually similar to what we saw with the previous work, in that we're going to.",
                    "label": 1
                },
                {
                    "sent": "Incorporate a recurrent neural net structure into the inference model and generation model of a standard VE, but it's slightly different.",
                    "label": 0
                },
                {
                    "sent": "They're actually doing this in the context of non sequential data.",
                    "label": 0
                },
                {
                    "sent": "You can also apply to sequential data and they have, but the way I think is a very interesting aspect of this is they're basically using a recurrent neural net in the context and a VE together to do inference in, say for example in image.",
                    "label": 1
                },
                {
                    "sent": "And the other thing that they do that I think is very interesting is they actually augment the model with an attention mechanism which allows them to essentially define this non sequential data as being being generated as a result of a sequential process.",
                    "label": 0
                },
                {
                    "sent": "As well, so these two ideas sort of work pretty well together, although you can and will show you results of doing of using the draw structure without actually using their attention.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kinism so this is what would be a standard via E from the perspective of this model.",
                    "label": 0
                },
                {
                    "sent": "So here we have X.",
                    "label": 0
                },
                {
                    "sent": "Actually this read process doesn't really exist in the context of a standard VSO X is going to map to this encoder network.",
                    "label": 0
                },
                {
                    "sent": "You could take you out you sample from that distribution.",
                    "label": 0
                },
                {
                    "sent": "You get RZ and then you have a decoder MLP and you get that gives you the distribution over X and you can sample from that.",
                    "label": 1
                },
                {
                    "sent": "That's a standard variational autoencoder.",
                    "label": 1
                },
                {
                    "sent": "Now the way draw change is this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure is like this, so X and again X can be the whole data and you see X is now we have a sequential structure so we have a recurrent neural net here and X is being fed in at every time step.",
                    "label": 0
                },
                {
                    "sent": "Note that X can be a subset if you use attention that X can be changing overtime.",
                    "label": 0
                },
                {
                    "sent": "It depends on this the attention mechanism.",
                    "label": 0
                },
                {
                    "sent": "Then you have an encoder RNN.",
                    "label": 0
                },
                {
                    "sent": "Going in that's giving you your distribution, your Q distribution, and then you take a sample from that and then you have your zed.",
                    "label": 0
                },
                {
                    "sent": "That's again a sequence of Zeds here and then.",
                    "label": 0
                },
                {
                    "sent": "You have a decoder RNN and the decoder RNN.",
                    "label": 1
                },
                {
                    "sent": "Interestingly enough, feeds back into both the encoder and and and the read structure at the next time step.",
                    "label": 0
                },
                {
                    "sent": "Also, at the end point you have the decoder and projecting to a right mechanism, which then generates this what they call a canvas, which is essentially sort of a.",
                    "label": 0
                },
                {
                    "sent": "Essentially something that you're writing to that Maps to your probability distribution over X, right?",
                    "label": 0
                },
                {
                    "sent": "So at this end point here, this is essentially just a X is now conditionally independent given zed, but also given essentially given the C here.",
                    "label": 0
                },
                {
                    "sent": "So you're sort of filling in this canvas, which is your input, and then at the end you just you just sample from that in a fairly simple distribution.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting is you're sort of building up this canvas over the overtime here, so that's the structure of draw.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty complicated model.",
                    "label": 0
                },
                {
                    "sent": "It's actually like surprisingly complicated, I would say, but it seems to work fairly well.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about that structure?",
                    "label": 0
                },
                {
                    "sent": "OK, we can move on, so this is just.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I mean, I'm not going to go into anymore details than this.",
                    "label": 0
                },
                {
                    "sent": "The details are actually all pretty straightforward, yeah?",
                    "label": 0
                },
                {
                    "sent": "Big.",
                    "label": 0
                },
                {
                    "sent": "Big T is fixed here.",
                    "label": 0
                },
                {
                    "sent": "I think big T is always fixed.",
                    "label": 0
                },
                {
                    "sent": "They do experiments with a very big T, but for any given experiment it's fixed.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh right, it's just it's just an operation that basically Maps from the output of the decoder into CT. Well, it's just you can think of it as just a parametric model that when the parameters you're going to train.",
                    "label": 0
                },
                {
                    "sent": "So it's just and you can think of it as another PC or like.",
                    "label": 0
                },
                {
                    "sent": "Think of it as an MLP for example.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so actually in this case, if you have it, it's actually it's mostly used in the context of the attention mechanism, which you don't have attention.",
                    "label": 0
                },
                {
                    "sent": "It's essentially a pretty simple operation here.",
                    "label": 0
                },
                {
                    "sent": "For example, the read operation it just uses is essentially just giving you.",
                    "label": 0
                },
                {
                    "sent": "XNXT so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming back here, this Reed operation here is just feeding in X.",
                    "label": 0
                },
                {
                    "sent": "And and the previous X hat that comes from the write operation of the previous time step basically, which is kind of at this point in the model.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's essentially all it's doing, so it's not doing anything in the case of the without attention.",
                    "label": 0
                },
                {
                    "sent": "In this case, the right operation is similar, just a linear mapping from.",
                    "label": 0
                },
                {
                    "sent": "In oops.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The right operation right here, right?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's just actually taking in the case of.",
                    "label": 0
                },
                {
                    "sent": "Of I guess it's just taking the the current hidden state and actually not using.",
                    "label": 0
                },
                {
                    "sent": "It's interesting it's not using the previous right information as well, so this is apparently not being used in that context.",
                    "label": 0
                },
                {
                    "sent": "That's interesting, OK, sorry, and if you were to apply this, this is the kind of generation process you get, right?",
                    "label": 0
                },
                {
                    "sent": "So time here is just generation time.",
                    "label": 0
                },
                {
                    "sent": "So you start sampling here.",
                    "label": 0
                },
                {
                    "sent": "This is for amnesty once again and you get so it starts out fairly blurry an as you continue the RNN generation through time it sort of refines the model into something that's clear.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nur.",
                    "label": 0
                },
                {
                    "sent": "And more detailed, and it's a sharper image.",
                    "label": 0
                },
                {
                    "sent": "Over here there.",
                    "label": 0
                },
                {
                    "sent": "Now they the attention mechanism that they use is actually something pretty interesting, so I'm not going to go into the details of it.",
                    "label": 0
                },
                {
                    "sent": "Basically, the idea is that they use it's essentially a differentiable model.",
                    "label": 0
                },
                {
                    "sent": "What they what they have is the output coming from the decoder model is essentially extracts a few parameters that include the location, for example of the attention.",
                    "label": 0
                },
                {
                    "sent": "So you're attending a given image Patch.",
                    "label": 0
                },
                {
                    "sent": "And the output is the location of that Patch.",
                    "label": 0
                },
                {
                    "sent": "The size of that Patch, and the blur of that Patch.",
                    "label": 0
                },
                {
                    "sent": "So these are essentially the three parameters that or that, well, you have location in two dimensions and size in two dimensions and then blur scalar blur.",
                    "label": 0
                },
                {
                    "sent": "So that's 5 dimensions that your base that's output from the decoder model to give you the attention, and then then what you do in this case.",
                    "label": 0
                },
                {
                    "sent": "For example, you know on this five you have.",
                    "label": 0
                },
                {
                    "sent": "In this case, a small Patch centered about this point here, and it's fairly sharp.",
                    "label": 0
                },
                {
                    "sent": "Yeah, reasonably, moderately sharp, let's say, and you get this kind of an output and which which then feeds into the input for the next sequence of them of the recurrent structure.",
                    "label": 0
                },
                {
                    "sent": "Here we have a large sharp image, and here we have a large blurred image, and this is the result that you get.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting that they choose to separate the scale in the blur.",
                    "label": 0
                },
                {
                    "sent": "This is something that I think I would have naturally just fused together.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I I don't.",
                    "label": 0
                },
                {
                    "sent": "I've not studied that.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's an important distinction to that.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm sorry, yeah, that's that's a good point there.",
                    "label": 0
                },
                {
                    "sent": "There is the scale.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so right.",
                    "label": 0
                },
                {
                    "sent": "So I mean this is just kind of an example of how this would work.",
                    "label": 0
                },
                {
                    "sent": "These are examples of that for a given Patch.",
                    "label": 0
                },
                {
                    "sent": "This is sort of an example of the kind of Gaussian blur kernel you would use.",
                    "label": 0
                },
                {
                    "sent": "It's generating a single a blur, so there's an example of how it would blur over the input to generate a given pixel.",
                    "label": 0
                },
                {
                    "sent": "Then the read operation you generate, so it's it's down.",
                    "label": 0
                },
                {
                    "sent": "It's using a significant Gaussian blur here, and the scale is quite large, so it downsamples.",
                    "label": 0
                },
                {
                    "sent": "So if the blur is large, it's going to downsample into something quite small like this, and then regenerate into something that's large here.",
                    "label": 0
                },
                {
                    "sent": "So that's the basically structure of the of the attention mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now how you can use this attention mechanism in interesting ways for for the classification is shown here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a data set that's called clutter damnest.",
                    "label": 0
                },
                {
                    "sent": "In this context we have a single digit in a background of things that are kind of digit like but actually aren't digits.",
                    "label": 1
                },
                {
                    "sent": "And the goal here is just to do classification.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a sequence showing how the sequential operation of the attention plus draw structure.",
                    "label": 1
                },
                {
                    "sent": "Sorta zooms in on the digit.",
                    "label": 1
                },
                {
                    "sent": "So we have sort of structure here so it sees a big image early on.",
                    "label": 0
                },
                {
                    "sent": "It actually learns to sort of have a blurry large image of the whole thing, and then it zooms in once it end into a sharper, sharper image as it goes along.",
                    "label": 0
                },
                {
                    "sent": "And this is the kind of structure we see, and we get reasonable performance.",
                    "label": 0
                },
                {
                    "sent": "I mean, this isn't necessarily a very competitive task, so it's hard to say how well this does compared to other methods.",
                    "label": 0
                },
                {
                    "sent": "I think this was state of the art on this task when it was applied, but there's only been like one or two other people applying it on this so.",
                    "label": 0
                },
                {
                    "sent": "It's it's not sure how competitive it is at this test, but at least it's it's clear that the there.",
                    "label": 0
                },
                {
                    "sent": "Attention mechanism is doing something interesting, and another thing that's interesting about their attention mechanism versus other attention mechanisms that have been proposed is that it's fully differentiable, so they don't have to use something like reinforce or something like that to train it, which is very nice.",
                    "label": 0
                },
                {
                    "sent": "So here's a.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee, this is a this was.",
                    "label": 0
                },
                {
                    "sent": "Basically the attention mechanism being used to generate MNIST, and this is a result by actually somebody in our lab York who was actually able to repeat this performance an actually get match their performance.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially been reproduced.",
                    "label": 0
                },
                {
                    "sent": "This is his result that I'm showing you here and the important thing, and This is why I think draw is probably the most exciting of the models that have come out recently and why we're going to be spending the rest of my time on it rather than on the other methods is that it's able to.",
                    "label": 0
                },
                {
                    "sent": "This is what's being measured.",
                    "label": 0
                },
                {
                    "sent": "Here is the negative log likelihood on Msom list.",
                    "label": 0
                },
                {
                    "sent": "It's a simple distribution, but it happens to be the only one that people are routinely testing on for.",
                    "label": 0
                },
                {
                    "sent": "For the capacity of a generative model.",
                    "label": 0
                },
                {
                    "sent": "So what we're measuring here is the negative log likelihood on test examples, and for years we've been stuck in it about this range right here.",
                    "label": 0
                },
                {
                    "sent": "8584 we've never really been able.",
                    "label": 0
                },
                {
                    "sent": "I mean, we don't know how good one can do.",
                    "label": 0
                },
                {
                    "sent": "This model with attention has significantly lowered that that component, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a very exciting innovation.",
                    "label": 0
                },
                {
                    "sent": "Now there's some questions to ask.",
                    "label": 0
                },
                {
                    "sent": "Where does where does this come from?",
                    "label": 0
                },
                {
                    "sent": "And by the way, this this number has also been reproduced, so it's a real result, and so there's a few things that might go into this right?",
                    "label": 0
                },
                {
                    "sent": "One thing that's clear is that there the attention mechanism there use of an attention mechanism is implicitly incorporating a topology, so you might expect to get some gain from that, right?",
                    "label": 0
                },
                {
                    "sent": "'cause all of a sudden you're?",
                    "label": 0
                },
                {
                    "sent": "You're now more in the regime of something like a convolutional model, as opposed to a fully connected model, so that could be a reason why you're getting a significantly lower value here.",
                    "label": 0
                },
                {
                    "sent": "In fact, that probably accounts for quite a bit of this.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that they're sequential inference mechanism.",
                    "label": 0
                },
                {
                    "sent": "For example, if you compare this model to standard via E. Which isn't here actually.",
                    "label": 0
                },
                {
                    "sent": "This is one of the other mechanism models that have recently been proposed, so so Standard V gets you gets about 90, right?",
                    "label": 0
                },
                {
                    "sent": "So you see a full.",
                    "label": 0
                },
                {
                    "sent": "10 not improvement from Standard VEC and so one possibility is that apology and other possibilities.",
                    "label": 0
                },
                {
                    "sent": "It's a sequential inference scheme that's helping here.",
                    "label": 0
                },
                {
                    "sent": "Seems like right now.",
                    "label": 0
                },
                {
                    "sent": "Both are contributing pretty significantly to getting that kind of result.",
                    "label": 0
                },
                {
                    "sent": "And so if we just look.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of using attention in reading that was just the reading process.",
                    "label": 0
                },
                {
                    "sent": "This is how this is generating.",
                    "label": 0
                },
                {
                    "sent": "These are again from the original authors, so the ones that proposed draw.",
                    "label": 0
                },
                {
                    "sent": "So this is generating two sets of images.",
                    "label": 0
                },
                {
                    "sent": "We can generate SCHN.",
                    "label": 0
                },
                {
                    "sent": "It's just looping back, so this possible you're seeing now is how attention works in reading.",
                    "label": 0
                },
                {
                    "sent": "And now what we're going to see is the writing output.",
                    "label": 0
                },
                {
                    "sent": "So these things are happening in parallel, of course, but you're viewing sort of one or the other of them.",
                    "label": 0
                },
                {
                    "sent": "And this is for SSV HN, so these are Street View house numbers.",
                    "label": 0
                },
                {
                    "sent": "And of course there.",
                    "label": 0
                },
                {
                    "sent": "For some reason it's just decided to sort of pick on one side and generate kind of sweeping across.",
                    "label": 0
                },
                {
                    "sent": "It's the training policy.",
                    "label": 0
                },
                {
                    "sent": "It essentially learns, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, that would be essential if they take the whole image and stay at the whole image.",
                    "label": 0
                },
                {
                    "sent": "They're essentially then using the standard draw model without attention, right?",
                    "label": 0
                },
                {
                    "sent": "The point of the attention is they're able to zoom in.",
                    "label": 0
                },
                {
                    "sent": "If they they learn it, it's the the parameters are like, for example, the location and scale of that Patch are the output of the hidden units of the decoding model.",
                    "label": 0
                },
                {
                    "sent": "So it's just trained via the global objective function 228 for a particular policy, and it's perfect.",
                    "label": 0
                },
                {
                    "sent": "It's 100% back probable, so you just back prop through that and train those parameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so I think that's actually a significant component, and the evidence for that.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is purely qualitative, but I'm not showing their results on Cifar generation.",
                    "label": 0
                },
                {
                    "sent": "So C far these tiny.",
                    "label": 0
                },
                {
                    "sent": "You've probably seen these this week.",
                    "label": 0
                },
                {
                    "sent": "They're tiny little images.",
                    "label": 0
                },
                {
                    "sent": "There are like 32 by 32 images of cats, dogs, planes, cars, and they've done this generation, and it's not nearly as impressive as these results.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it would be an interesting thing.",
                    "label": 0
                },
                {
                    "sent": "The problem is nobody's really established baselines for NLL on things like cifar yet, so we're still kind of, you know, something I'm interested in doing.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Baseline in terms of NLL numbers and not negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So typically way they go back to MNIST.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of unfortunate at this point, but because I think we're actually getting to the point where it's almost a solved problem even in the generative context.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I think.",
                    "label": 0
                },
                {
                    "sent": "I think I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Stop, I just want men.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And really quickly these other two papers that were in the most recent ICML I just want to go over them super quick, just give you a real quick flavor of what they.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or after so the basic idea here is they're interested.",
                    "label": 0
                },
                {
                    "sent": "They're taking kind of going back to the original via E model and saying, well, this one has this one weakness, which is this this the encoder model still has this factorial prior, right?",
                    "label": 0
                },
                {
                    "sent": "It's still essentially kind of a mean field assumption on the inputs.",
                    "label": 0
                },
                {
                    "sent": "Sorry in the posterior and the question is well, can we do something about that?",
                    "label": 1
                },
                {
                    "sent": "Can we improve that kind of model?",
                    "label": 0
                },
                {
                    "sent": "So there's been these two?",
                    "label": 0
                },
                {
                    "sent": "Works recently that are essentially attacking directly that problem.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is one of them, so variational inference with normalizing flows.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's actually very similar to the nice small.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have you talked about nicer talk about now, so it's actually in some sense it builds on on this nice model, but they use it as as a way.",
                    "label": 0
                },
                {
                    "sent": "So basically they're using this insight that you can do these sort of chain together sequences of transformations of variables in order to relate random variables in one space where you can put in a simple distribution into a random variable in another space where it can be a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "So what that allows you to do is take a simple.",
                    "label": 0
                },
                {
                    "sent": "Sort of inject a simple distribution somewhere in your model and then project through this chaining of random variables into this complicated space and that you can consider to be your posterior, and because if you're mapping is invertible, you can actually do this right?",
                    "label": 0
                },
                {
                    "sent": "So so it's essentially the objective function still allows you.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Train these models.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the lower bound you get when you do that chaining together with these transformations F here.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that if you do that in the context and they pick for example what they do is they pick a particular form of F that looks like this, which is essentially just kind of a linear like you've got a linear component and a nonlinear component.",
                    "label": 0
                },
                {
                    "sent": "I think this they used a soft plus here and what they find is they're actually able to this.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of your standard distribution.",
                    "label": 0
                },
                {
                    "sent": "This is a unit Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This would be essentially what your prior would be.",
                    "label": 0
                },
                {
                    "sent": "Let's see what your posterior would be under the traditional view, and if you go through one sequence of this two sequences of this 10 sequences of this, you can get 2 pretty rich distributions.",
                    "label": 0
                },
                {
                    "sent": "And what they do with that is of course, just as I mentioned, they sort of stacked this on top of a standard MLP to get you from this place, where you would output a fairly simple prior to where you're sort of interfacing the objective function here, which would be a fairly complicated.",
                    "label": 0
                },
                {
                    "sent": "Simple posterior distribution here, but now you can actually express that as a fairly complicated and rich posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's that.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you compare it analytically, what you end up getting is reasonable and performance improvement.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking at here is chaining this.",
                    "label": 0
                },
                {
                    "sent": "For example, is chaining 80 of these functions together through this normalizing flow, and you're getting a decrease from about 90.",
                    "label": 0
                },
                {
                    "sent": "This is what you can get in a standard case with VE down to about 85.",
                    "label": 0
                },
                {
                    "sent": "OK, this is again in the context.",
                    "label": 0
                },
                {
                    "sent": "There's no topology information now, so it's kind of incomparable to compare this to what you saw with draw, because there was this implicit.",
                    "label": 0
                },
                {
                    "sent": "Topological information coming up through the attention mechanism, but if you were to compare it to, for example, what you get withdraw without an attention mechanism that was around 87, so we're actually seeing a benefit here.",
                    "label": 0
                },
                {
                    "sent": "So it does seem to be doing something interesting in this context.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so finally, if we just talk about one last.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this one, I'll just go over what they're doing.",
                    "label": 0
                },
                {
                    "sent": "Something really cool.",
                    "label": 0
                },
                {
                    "sent": "They're integrating MCMC with a variational method and the way they're doing is they're kind of envisioning the hybrid Monte Carlo, the MCMC chain that you get out as doing variational inference in an augmented space.",
                    "label": 1
                },
                {
                    "sent": "That includes this whole chain, right?",
                    "label": 0
                },
                {
                    "sent": "And they end up being able to.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Late this.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's just a little.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still on the type of MCMC they use is Hamiltonian Monte Carlo, which is useful because they actually can do got it with gradient information.",
                    "label": 0
                },
                {
                    "sent": "But the insight here is that they are able to relate it to doing this augmented space to a lower variance to a variational lower bound.",
                    "label": 0
                },
                {
                    "sent": "It just happens to be lower than the normal variational lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they end up with this trainable parameters for a.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model, but they can learn to sort of augment that.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I guess I'll stop there anyway, actually.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If they, if they do that.",
                    "label": 0
                },
                {
                    "sent": "They end up getting in and they incorporate the topological information.",
                    "label": 0
                },
                {
                    "sent": "They actually end up getting very close to draw.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'll stop.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}