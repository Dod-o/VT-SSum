{
    "id": "sopzh47kzzxkxldxk45t23px2kuxlrcs",
    "title": "Unsupervised Learning by Discriminating Data from Artificial Noise",
    "info": {
        "author": [
            "Michael Gutmann, Helsinki Institute for Information Technology"
        ],
        "published": "March 26, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_gutmann_ulddan/",
    "segmentation": [
        [
            "Today's workshop.",
            "Before the panel.",
            "So Michael good map from the University of Helsinki will present his work and this committee data from artificial noise.",
            "Thanks for the introduction.",
            "So this is joint work with our poor hearing.",
            "And.",
            "Well.",
            "The main focus in our research group is actually unsupervised learning.",
            "But a central message will actually teach central message of this talk is that although you may be interested in unsupervised learning only, you can do unsupervised learning by supervised learning.",
            "So even if you're only interested in unsupervised learning, you should automatically be interested in supervised learning.",
            "So let me start the talk with."
        ],
        [
            "They.",
            "Simple, while a standard method for discriminative learning.",
            "So for supervised learning name logistic regression.",
            "So we have two datasets.",
            "X&Y and we want to discriminate between them, and as you well may all know that you can use logistic regression to train a classifier to solve that task, and well, logistic regression uses the model that.",
            "Well, actually the model in equation one and two, so it assigns a probability P to input U.",
            "So assigns probability P that you belongs to class X as given in this equation and well it assigns probably 1 -- P that input you belongs to class Y.",
            "And.",
            "This function she here can be seen anything and well here we include some parameters Theta which power model parameterized this function.",
            "And well, you will see if she is, say, Infinity.",
            "Then like P = 1.",
            "So so if P is large, if she is large then P is larger than 1/2 and you wouldn't classify the input you belong to Class X.",
            "And well, the really simplest example is here.",
            "This linear classifier with where you take that in a product with this little bias term WO and cause she larger than SEO.",
            "Is well like the decision boundary, this wce oh acts like a threshold.",
            "Well, this is maybe well known to most of you, but what actually what happens in the learning of this?",
            "Well of this classifier?",
            "Well, I."
        ],
        [
            "Would say.",
            "For the learning to be successful, the classifier must find somehow differences between the two datasets.",
            "And now we can leverage on this simple observation.",
            "Namely, assume you know properties of data, data set Y and cause you learn the differences between X&Y.",
            "By doing this classification task, you can then infer from these two things the properties of the data set X. OK, so a simple example.",
            "Well as actually example which we are doing in using in our research group.",
            "So we want to learn properties of natural images.",
            "Well, a way of doing that is to generate artificial noise.",
            "Why we share some properties with the with the natural images?",
            "For instance, the same 2nd order information.",
            "And then, well, we controlled train the classifier to distinguish between these two datasets.",
            "And because we know the well, the properties of the noise, we can then infer properties of the natural images.",
            "OK, so this is the basic principle or the intuition behind the whole thing, and we formalize that now by estimation theory so."
        ],
        [
            "So.",
            "You observe some data X, for instance the natural images with some PDF which you don't know, but you want to model it.",
            "Until what you do then is to generate some noise Y with some known PDF.",
            "And then you, well, as always, you set up a model which we denote by F, which should like model the log PDF of the unknown of the data.",
            "Well then you do logistic regression, but with a model.",
            "Well, maybe nonstandard nonlinearity given by Inequation 3.",
            "And while you do, you can set up then an objective function.",
            "Wearwell in.",
            "Well, the objective function equation for here where like the H is basically the probability that you belongs to the set X.",
            "And value defined in your estimator as the parameter value Theta, which maximizes this objective function.",
            "So basically you do like logistic regression with a highly nonstandard nonlinearity given by equations.",
            "Well, let's see what kind of properties this estimator has."
        ],
        [
            "First of all.",
            "We'll assume that your model is rich enough so that you can basically model anything, and then we could show that the maximum of the objective function is actually attained if our model equals the log PDF of the data.",
            "So by maximizing the objective function we can actually learn the true probability distribution.",
            "Well, of course, like in real world, we cannot really permit.",
            "Well, we cannot really have a function F which can approximate any function, so that's why people then normally look like what happens if I would date or follows the model or within able to get back the true parameter value.",
            "Well, this is kind of a standard approach which is not limit well which not special to our mess, so that's what just statisticians are doing.",
            "And, well, let's assume that the data is generated according to the model.",
            "So there is a specific parameter value of data star and well, we can then show that this method kind of finds back the correct data store.",
            "In other words, like the estimator is statically consistent.",
            "And here is a bit of technical point which is rather important for our research group.",
            "It is, namely that this maximization.",
            "You can do it without any constraints.",
            "So let's compile the actual likelihood.",
            "As you may know, like in likelihood your model must be valid density, so you cannot set up any model like the model must be valid density, which integrates for one, for instance, else you cannot lose likelihood.",
            "So in a certain sense, like likelihood is something like constraint optimization.",
            "So you can only you're only looking for functions which satisfy this normalization constraint.",
            "And if the model is highly complex, for instance like in for Markov random fields like, you are not able to satisfy this normalization constraint.",
            "So this is like the problem of the partition function for those who are familiar with this.",
            "With this well with the problem of the partition function.",
            "And the well cause in for in our case, the maximization is unconstrained.",
            "We can actually learn like Unnormalized models, which is for instance not possible in likelihood.",
            "Well, I hope this was clear, else please free.",
            "Feel free to ask questions.",
            "Right?",
            "Nation.",
            "Garcia Republicans finding out that.",
            "Connect inspection.",
            "To be in likelihood.",
            "Well, actually likelihood is actually likelihood is something like, well, it's actually equivalent to minimize the minimization of the KL distance between your model and the data.",
            "Well, actually this for the KL dense distance there.",
            "Did the two functions must be true PDF well, but likelihood is actually equivalent.",
            "Maximization of likelihood is equivalent to minimization of KL distance.",
            "The building is still coming off each other, so we can.",
            "If you only do the likelihood with respect to the populations that is, but you don't need to be right.",
            "The general definition of language.",
            "I cannot really follow him so well.",
            "We can discuss it offline, but for likelihood you model must be a valid PDF.",
            "So we can discuss it right, but yeah.",
            "Estimation involving a model where the partition function is a summer roll.",
            "Yeah yeah, yeah yeah yeah I did norm like for like for instance you had to know the partition function but here I am.",
            "Well baby proof that actually you don't need to know the partition function in fact you.",
            "Well.",
            "Well, you can make some high level connections to sampling like to for instance important sampling, but we will see later in the experiments that actually this method performs much better.",
            "So for this.",
            "The consistent we don't make any assumption on the noise distribution very well.",
            "I skipped over that.",
            "Assumptions are really mild, for instance.",
            "This support must be well, doesn't need to be the same, but at least it should cover where the two data PDF is supported, so you cannot kind of if your data is just on the left hand like just all minus and you take noise which is all positive, then you cannot do anything.",
            "But that's actually it.",
            "OK so but maybe for this workshop the descent well.",
            "The most."
        ],
        [
            "Portant thing is that, well, you can do unsupervised learning by supervised learning.",
            "So this is maybe the central message of this talk for this virtual.",
            "So.",
            "Well, let's see how well how well that is dismissed that actually performs in the estimation."
        ],
        [
            "And often I see a model.",
            "So I see a model is like you have data X which is like, well, you start with some sources.",
            "South Wichita mixed together by a mixing matrix A and then you observe this mixture and well, you don't know A and you just know the distribution of the sources here, like the ideal application and the only thing you assume is that the mixing matrix A is invertible.",
            "So the goal is then of to find back from the data.",
            "The mixing matrix A.",
            "Well, so the true PDF of data is like, well, it's given an equation 8 here.",
            "And so you see, it has two parts like one part here, which is well which contains the data samples and well, the data X and then the second part here, which is for the correct normalization of the PDF.",
            "So if you integrate over all X is and you don't have this term here, then the thing won't integrate over.",
            "So we set up then the model like an equation line where we keep this thing.",
            "So we are assuming that we know actually the distribution of the sources.",
            "So we want to estimate this beta star here.",
            "So well we are using like a powder be and as I said we don't need to know the normalization constant.",
            "That's why we just have here another parameter C which should then more well estimate the normalization.",
            "Well, and so for the contrastive noise.",
            "We use state of its own covariances.",
            "As well, we use data with the same Koreans as X. OK, so here's the results."
        ],
        [
            "There are many colors and many curves, so I will walk you through the plot.",
            "So each circle here is basically on one, like the average result of many simulations, which is certain sample size.",
            "And I plot then like the error we got versus the time which was needed to get the result.",
            "So towards the left here like we get a better, better estimation result, and here up here like it took longer to get a result, so we won't actually be here on this corner.",
            "And so I compared here several methods, so the red curve is the newly proposed method.",
            "The black here is maximum likelihood and this here like auto methods which are used and you see like maximum likelihood is well it's the best but.",
            "Well, as I pointed out before, for maximum likelihood we need to know the correct partition function, so that's why it's well, I indicated here just for reference, so it's like it's a complete other state.",
            "Well, it's another kind of estimation method and then the others, because for the others we don't need to know the partition function.",
            "So you see that, well, the next best estimation I thought is then like the newly proposed, especially for a given squared error, well, we don't need to wait as long As for instance, for contrastive divergent.",
            "Yeah.",
            "Will actually have time to quickly explain the auto methods like.",
            "The pink one maximum likelihood.",
            "This important sampling is.",
            "Where in each step I estimated the correct while I estimated the partition function.",
            "We are important sampling and then use this estimate to estimate the other parameters and we see well as the answer to your question that well, the new method actually performs better than, for instance, important something.",
            "Yeah, well at one reason.",
            "Why score matching, which is another method too.",
            "Get to learn Unnormalized models.",
            "Didn't perform that well?",
            "Is that we?"
        ],
        [
            "Just here.",
            "Like we get here this absolute value.",
            "And so this is like a model which is not smooth enough for score matching.",
            "But this is like a minor comment."
        ],
        [
            "Yeah, so this is the last slide."
        ],
        [
            "So I'm in time for the summary.",
            "Maybe the well on the high level.",
            "What this talk was about, just like something I would call learning by comparison.",
            "So we want to learn something about some data, which properties we don't know and what we do is we take like reference data, the purposes of which we know.",
            "And then we compare.",
            "And well, this intuitive principle principle actually provides a consistent estimator.",
            "And well, which is important for a source of possibly unnormalized models?",
            "And then we checked.",
            "Well, the performance for an ICA model model and we have seen that validate it gives the best tradeoff between computational and statistical efficiency.",
            "Very important for this workshop.",
            "It shows that unsupervised learning can be performed by supervised learning.",
            "And while I hope for the in a discussion, we can think about what this implications are.",
            "For instance, like the pre training things we heard before another well, relations for deep through deep learning.",
            "Well, thanks for your attention.",
            "Have plenty of questions."
        ],
        [
            "Does it work better for closer?",
            "You are close enough.",
            "Tonight is provision is to go through distribution.",
            "Yeah well actually a big question is what is the optimal noise distribution?",
            "And it's a it's a it's a difficult question to answer, but when you when you take the extreme case where the noise distribution is the same as the data distribution actually performed pretty nicely.",
            "So the closer the better.",
            "But it's not the optimal.",
            "Email in your visa.",
            "Converting"
        ],
        [
            "Convert.",
            "Well, you should."
        ],
        [
            "Victor.",
            "Mobile.",
            "And comprises two and in your case you have like this, knowing distribution and progressive.",
            "Respect you can provide it used to.",
            "Please this return samples from your mother.",
            "So I was wondering.",
            "What happened like if the story distribution seems like a version of your motorhome for installation or something about this make you make for some sticking points.",
            "Yeah, actually I think there.",
            "Well, in the in the if you go to the details of course differences as always, but on the high level they are surely like many like parallels, especially if you look CCD's like comparison between like the imaginary data and the true data and.",
            "Yeah, I mean we are looking into what happens in well, what are the related in detail?",
            "The connections and well maybe an important point is that you see for I mean this.",
            "And here and here we have the same sample size.",
            "So this time samples of of noise we are using and you see that CD actually performs better.",
            "So we are we are more to the left here.",
            "So it performs better than this method, but well, it's just.",
            "It also takes longer, so if you look at the tradeoff.",
            "Nude looking at the photo addition function is not a constant if you change the model area."
        ],
        [
            "It's only customers when you fix them exactly, so if you if you replace the partition function via custom, which will actually be definitely.",
            "Inconsistent estimator so.",
            "Do you get a particular classifier by moving from this way?",
            "But it's not going to become well, I. I'm sorry I don't agree with your last statement because we proved that actually the estimator is consistent.",
            "So of course the C depends on your parameters.",
            "In becausw like here you see that I mean you see here that the normalization.",
            "Is part is a function of the parameters, but it seems like you can actually couple the two things.",
            "Basically it's like healthier parameters and then you have a function which Maps it to the normalization constant, right?",
            "And basically we have like data in the estimation we have like data map to the parameters and then map to the partition function.",
            "So what we're actually doing is just the map directly from the data to the partition function.",
            "I hope this helps.",
            "Thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today's workshop.",
                    "label": 0
                },
                {
                    "sent": "Before the panel.",
                    "label": 0
                },
                {
                    "sent": "So Michael good map from the University of Helsinki will present his work and this committee data from artificial noise.",
                    "label": 1
                },
                {
                    "sent": "Thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with our poor hearing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The main focus in our research group is actually unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "But a central message will actually teach central message of this talk is that although you may be interested in unsupervised learning only, you can do unsupervised learning by supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So even if you're only interested in unsupervised learning, you should automatically be interested in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So let me start the talk with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They.",
                    "label": 0
                },
                {
                    "sent": "Simple, while a standard method for discriminative learning.",
                    "label": 1
                },
                {
                    "sent": "So for supervised learning name logistic regression.",
                    "label": 1
                },
                {
                    "sent": "So we have two datasets.",
                    "label": 0
                },
                {
                    "sent": "X&Y and we want to discriminate between them, and as you well may all know that you can use logistic regression to train a classifier to solve that task, and well, logistic regression uses the model that.",
                    "label": 1
                },
                {
                    "sent": "Well, actually the model in equation one and two, so it assigns a probability P to input U.",
                    "label": 0
                },
                {
                    "sent": "So assigns probability P that you belongs to class X as given in this equation and well it assigns probably 1 -- P that input you belongs to class Y.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This function she here can be seen anything and well here we include some parameters Theta which power model parameterized this function.",
                    "label": 0
                },
                {
                    "sent": "And well, you will see if she is, say, Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then like P = 1.",
                    "label": 1
                },
                {
                    "sent": "So so if P is large, if she is large then P is larger than 1/2 and you wouldn't classify the input you belong to Class X.",
                    "label": 0
                },
                {
                    "sent": "And well, the really simplest example is here.",
                    "label": 0
                },
                {
                    "sent": "This linear classifier with where you take that in a product with this little bias term WO and cause she larger than SEO.",
                    "label": 0
                },
                {
                    "sent": "Is well like the decision boundary, this wce oh acts like a threshold.",
                    "label": 0
                },
                {
                    "sent": "Well, this is maybe well known to most of you, but what actually what happens in the learning of this?",
                    "label": 0
                },
                {
                    "sent": "Well of this classifier?",
                    "label": 0
                },
                {
                    "sent": "Well, I.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would say.",
                    "label": 0
                },
                {
                    "sent": "For the learning to be successful, the classifier must find somehow differences between the two datasets.",
                    "label": 1
                },
                {
                    "sent": "And now we can leverage on this simple observation.",
                    "label": 0
                },
                {
                    "sent": "Namely, assume you know properties of data, data set Y and cause you learn the differences between X&Y.",
                    "label": 0
                },
                {
                    "sent": "By doing this classification task, you can then infer from these two things the properties of the data set X. OK, so a simple example.",
                    "label": 0
                },
                {
                    "sent": "Well as actually example which we are doing in using in our research group.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn properties of natural images.",
                    "label": 0
                },
                {
                    "sent": "Well, a way of doing that is to generate artificial noise.",
                    "label": 0
                },
                {
                    "sent": "Why we share some properties with the with the natural images?",
                    "label": 0
                },
                {
                    "sent": "For instance, the same 2nd order information.",
                    "label": 0
                },
                {
                    "sent": "And then, well, we controlled train the classifier to distinguish between these two datasets.",
                    "label": 0
                },
                {
                    "sent": "And because we know the well, the properties of the noise, we can then infer properties of the natural images.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the basic principle or the intuition behind the whole thing, and we formalize that now by estimation theory so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You observe some data X, for instance the natural images with some PDF which you don't know, but you want to model it.",
                    "label": 0
                },
                {
                    "sent": "Until what you do then is to generate some noise Y with some known PDF.",
                    "label": 1
                },
                {
                    "sent": "And then you, well, as always, you set up a model which we denote by F, which should like model the log PDF of the unknown of the data.",
                    "label": 0
                },
                {
                    "sent": "Well then you do logistic regression, but with a model.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe nonstandard nonlinearity given by Inequation 3.",
                    "label": 0
                },
                {
                    "sent": "And while you do, you can set up then an objective function.",
                    "label": 0
                },
                {
                    "sent": "Wearwell in.",
                    "label": 0
                },
                {
                    "sent": "Well, the objective function equation for here where like the H is basically the probability that you belongs to the set X.",
                    "label": 1
                },
                {
                    "sent": "And value defined in your estimator as the parameter value Theta, which maximizes this objective function.",
                    "label": 0
                },
                {
                    "sent": "So basically you do like logistic regression with a highly nonstandard nonlinearity given by equations.",
                    "label": 0
                },
                {
                    "sent": "Well, let's see what kind of properties this estimator has.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "We'll assume that your model is rich enough so that you can basically model anything, and then we could show that the maximum of the objective function is actually attained if our model equals the log PDF of the data.",
                    "label": 1
                },
                {
                    "sent": "So by maximizing the objective function we can actually learn the true probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, of course, like in real world, we cannot really permit.",
                    "label": 0
                },
                {
                    "sent": "Well, we cannot really have a function F which can approximate any function, so that's why people then normally look like what happens if I would date or follows the model or within able to get back the true parameter value.",
                    "label": 0
                },
                {
                    "sent": "Well, this is kind of a standard approach which is not limit well which not special to our mess, so that's what just statisticians are doing.",
                    "label": 1
                },
                {
                    "sent": "And, well, let's assume that the data is generated according to the model.",
                    "label": 0
                },
                {
                    "sent": "So there is a specific parameter value of data star and well, we can then show that this method kind of finds back the correct data store.",
                    "label": 0
                },
                {
                    "sent": "In other words, like the estimator is statically consistent.",
                    "label": 0
                },
                {
                    "sent": "And here is a bit of technical point which is rather important for our research group.",
                    "label": 0
                },
                {
                    "sent": "It is, namely that this maximization.",
                    "label": 0
                },
                {
                    "sent": "You can do it without any constraints.",
                    "label": 0
                },
                {
                    "sent": "So let's compile the actual likelihood.",
                    "label": 0
                },
                {
                    "sent": "As you may know, like in likelihood your model must be valid density, so you cannot set up any model like the model must be valid density, which integrates for one, for instance, else you cannot lose likelihood.",
                    "label": 0
                },
                {
                    "sent": "So in a certain sense, like likelihood is something like constraint optimization.",
                    "label": 0
                },
                {
                    "sent": "So you can only you're only looking for functions which satisfy this normalization constraint.",
                    "label": 0
                },
                {
                    "sent": "And if the model is highly complex, for instance like in for Markov random fields like, you are not able to satisfy this normalization constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is like the problem of the partition function for those who are familiar with this.",
                    "label": 0
                },
                {
                    "sent": "With this well with the problem of the partition function.",
                    "label": 1
                },
                {
                    "sent": "And the well cause in for in our case, the maximization is unconstrained.",
                    "label": 0
                },
                {
                    "sent": "We can actually learn like Unnormalized models, which is for instance not possible in likelihood.",
                    "label": 0
                },
                {
                    "sent": "Well, I hope this was clear, else please free.",
                    "label": 0
                },
                {
                    "sent": "Feel free to ask questions.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "Garcia Republicans finding out that.",
                    "label": 0
                },
                {
                    "sent": "Connect inspection.",
                    "label": 0
                },
                {
                    "sent": "To be in likelihood.",
                    "label": 0
                },
                {
                    "sent": "Well, actually likelihood is actually likelihood is something like, well, it's actually equivalent to minimize the minimization of the KL distance between your model and the data.",
                    "label": 0
                },
                {
                    "sent": "Well, actually this for the KL dense distance there.",
                    "label": 0
                },
                {
                    "sent": "Did the two functions must be true PDF well, but likelihood is actually equivalent.",
                    "label": 0
                },
                {
                    "sent": "Maximization of likelihood is equivalent to minimization of KL distance.",
                    "label": 0
                },
                {
                    "sent": "The building is still coming off each other, so we can.",
                    "label": 0
                },
                {
                    "sent": "If you only do the likelihood with respect to the populations that is, but you don't need to be right.",
                    "label": 0
                },
                {
                    "sent": "The general definition of language.",
                    "label": 0
                },
                {
                    "sent": "I cannot really follow him so well.",
                    "label": 0
                },
                {
                    "sent": "We can discuss it offline, but for likelihood you model must be a valid PDF.",
                    "label": 0
                },
                {
                    "sent": "So we can discuss it right, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Estimation involving a model where the partition function is a summer roll.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah yeah yeah I did norm like for like for instance you had to know the partition function but here I am.",
                    "label": 1
                },
                {
                    "sent": "Well baby proof that actually you don't need to know the partition function in fact you.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Well, you can make some high level connections to sampling like to for instance important sampling, but we will see later in the experiments that actually this method performs much better.",
                    "label": 0
                },
                {
                    "sent": "So for this.",
                    "label": 0
                },
                {
                    "sent": "The consistent we don't make any assumption on the noise distribution very well.",
                    "label": 0
                },
                {
                    "sent": "I skipped over that.",
                    "label": 0
                },
                {
                    "sent": "Assumptions are really mild, for instance.",
                    "label": 0
                },
                {
                    "sent": "This support must be well, doesn't need to be the same, but at least it should cover where the two data PDF is supported, so you cannot kind of if your data is just on the left hand like just all minus and you take noise which is all positive, then you cannot do anything.",
                    "label": 0
                },
                {
                    "sent": "But that's actually it.",
                    "label": 0
                },
                {
                    "sent": "OK so but maybe for this workshop the descent well.",
                    "label": 0
                },
                {
                    "sent": "The most.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Portant thing is that, well, you can do unsupervised learning by supervised learning.",
                    "label": 1
                },
                {
                    "sent": "So this is maybe the central message of this talk for this virtual.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, let's see how well how well that is dismissed that actually performs in the estimation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And often I see a model.",
                    "label": 0
                },
                {
                    "sent": "So I see a model is like you have data X which is like, well, you start with some sources.",
                    "label": 0
                },
                {
                    "sent": "South Wichita mixed together by a mixing matrix A and then you observe this mixture and well, you don't know A and you just know the distribution of the sources here, like the ideal application and the only thing you assume is that the mixing matrix A is invertible.",
                    "label": 1
                },
                {
                    "sent": "So the goal is then of to find back from the data.",
                    "label": 0
                },
                {
                    "sent": "The mixing matrix A.",
                    "label": 0
                },
                {
                    "sent": "Well, so the true PDF of data is like, well, it's given an equation 8 here.",
                    "label": 1
                },
                {
                    "sent": "And so you see, it has two parts like one part here, which is well which contains the data samples and well, the data X and then the second part here, which is for the correct normalization of the PDF.",
                    "label": 0
                },
                {
                    "sent": "So if you integrate over all X is and you don't have this term here, then the thing won't integrate over.",
                    "label": 0
                },
                {
                    "sent": "So we set up then the model like an equation line where we keep this thing.",
                    "label": 0
                },
                {
                    "sent": "So we are assuming that we know actually the distribution of the sources.",
                    "label": 0
                },
                {
                    "sent": "So we want to estimate this beta star here.",
                    "label": 0
                },
                {
                    "sent": "So well we are using like a powder be and as I said we don't need to know the normalization constant.",
                    "label": 0
                },
                {
                    "sent": "That's why we just have here another parameter C which should then more well estimate the normalization.",
                    "label": 0
                },
                {
                    "sent": "Well, and so for the contrastive noise.",
                    "label": 0
                },
                {
                    "sent": "We use state of its own covariances.",
                    "label": 1
                },
                {
                    "sent": "As well, we use data with the same Koreans as X. OK, so here's the results.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many colors and many curves, so I will walk you through the plot.",
                    "label": 0
                },
                {
                    "sent": "So each circle here is basically on one, like the average result of many simulations, which is certain sample size.",
                    "label": 0
                },
                {
                    "sent": "And I plot then like the error we got versus the time which was needed to get the result.",
                    "label": 0
                },
                {
                    "sent": "So towards the left here like we get a better, better estimation result, and here up here like it took longer to get a result, so we won't actually be here on this corner.",
                    "label": 0
                },
                {
                    "sent": "And so I compared here several methods, so the red curve is the newly proposed method.",
                    "label": 0
                },
                {
                    "sent": "The black here is maximum likelihood and this here like auto methods which are used and you see like maximum likelihood is well it's the best but.",
                    "label": 0
                },
                {
                    "sent": "Well, as I pointed out before, for maximum likelihood we need to know the correct partition function, so that's why it's well, I indicated here just for reference, so it's like it's a complete other state.",
                    "label": 0
                },
                {
                    "sent": "Well, it's another kind of estimation method and then the others, because for the others we don't need to know the partition function.",
                    "label": 0
                },
                {
                    "sent": "So you see that, well, the next best estimation I thought is then like the newly proposed, especially for a given squared error, well, we don't need to wait as long As for instance, for contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Will actually have time to quickly explain the auto methods like.",
                    "label": 0
                },
                {
                    "sent": "The pink one maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "This important sampling is.",
                    "label": 0
                },
                {
                    "sent": "Where in each step I estimated the correct while I estimated the partition function.",
                    "label": 0
                },
                {
                    "sent": "We are important sampling and then use this estimate to estimate the other parameters and we see well as the answer to your question that well, the new method actually performs better than, for instance, important something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well at one reason.",
                    "label": 1
                },
                {
                    "sent": "Why score matching, which is another method too.",
                    "label": 0
                },
                {
                    "sent": "Get to learn Unnormalized models.",
                    "label": 0
                },
                {
                    "sent": "Didn't perform that well?",
                    "label": 0
                },
                {
                    "sent": "Is that we?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just here.",
                    "label": 0
                },
                {
                    "sent": "Like we get here this absolute value.",
                    "label": 0
                },
                {
                    "sent": "And so this is like a model which is not smooth enough for score matching.",
                    "label": 0
                },
                {
                    "sent": "But this is like a minor comment.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so this is the last slide.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm in time for the summary.",
                    "label": 0
                },
                {
                    "sent": "Maybe the well on the high level.",
                    "label": 0
                },
                {
                    "sent": "What this talk was about, just like something I would call learning by comparison.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn something about some data, which properties we don't know and what we do is we take like reference data, the purposes of which we know.",
                    "label": 0
                },
                {
                    "sent": "And then we compare.",
                    "label": 0
                },
                {
                    "sent": "And well, this intuitive principle principle actually provides a consistent estimator.",
                    "label": 0
                },
                {
                    "sent": "And well, which is important for a source of possibly unnormalized models?",
                    "label": 0
                },
                {
                    "sent": "And then we checked.",
                    "label": 0
                },
                {
                    "sent": "Well, the performance for an ICA model model and we have seen that validate it gives the best tradeoff between computational and statistical efficiency.",
                    "label": 1
                },
                {
                    "sent": "Very important for this workshop.",
                    "label": 0
                },
                {
                    "sent": "It shows that unsupervised learning can be performed by supervised learning.",
                    "label": 1
                },
                {
                    "sent": "And while I hope for the in a discussion, we can think about what this implications are.",
                    "label": 0
                },
                {
                    "sent": "For instance, like the pre training things we heard before another well, relations for deep through deep learning.",
                    "label": 0
                },
                {
                    "sent": "Well, thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Have plenty of questions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does it work better for closer?",
                    "label": 0
                },
                {
                    "sent": "You are close enough.",
                    "label": 0
                },
                {
                    "sent": "Tonight is provision is to go through distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah well actually a big question is what is the optimal noise distribution?",
                    "label": 0
                },
                {
                    "sent": "And it's a it's a it's a difficult question to answer, but when you when you take the extreme case where the noise distribution is the same as the data distribution actually performed pretty nicely.",
                    "label": 0
                },
                {
                    "sent": "So the closer the better.",
                    "label": 0
                },
                {
                    "sent": "But it's not the optimal.",
                    "label": 0
                },
                {
                    "sent": "Email in your visa.",
                    "label": 0
                },
                {
                    "sent": "Converting",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convert.",
                    "label": 0
                },
                {
                    "sent": "Well, you should.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Victor.",
                    "label": 0
                },
                {
                    "sent": "Mobile.",
                    "label": 0
                },
                {
                    "sent": "And comprises two and in your case you have like this, knowing distribution and progressive.",
                    "label": 0
                },
                {
                    "sent": "Respect you can provide it used to.",
                    "label": 0
                },
                {
                    "sent": "Please this return samples from your mother.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering.",
                    "label": 0
                },
                {
                    "sent": "What happened like if the story distribution seems like a version of your motorhome for installation or something about this make you make for some sticking points.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually I think there.",
                    "label": 0
                },
                {
                    "sent": "Well, in the in the if you go to the details of course differences as always, but on the high level they are surely like many like parallels, especially if you look CCD's like comparison between like the imaginary data and the true data and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean we are looking into what happens in well, what are the related in detail?",
                    "label": 0
                },
                {
                    "sent": "The connections and well maybe an important point is that you see for I mean this.",
                    "label": 0
                },
                {
                    "sent": "And here and here we have the same sample size.",
                    "label": 0
                },
                {
                    "sent": "So this time samples of of noise we are using and you see that CD actually performs better.",
                    "label": 0
                },
                {
                    "sent": "So we are we are more to the left here.",
                    "label": 0
                },
                {
                    "sent": "So it performs better than this method, but well, it's just.",
                    "label": 0
                },
                {
                    "sent": "It also takes longer, so if you look at the tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Nude looking at the photo addition function is not a constant if you change the model area.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's only customers when you fix them exactly, so if you if you replace the partition function via custom, which will actually be definitely.",
                    "label": 0
                },
                {
                    "sent": "Inconsistent estimator so.",
                    "label": 0
                },
                {
                    "sent": "Do you get a particular classifier by moving from this way?",
                    "label": 0
                },
                {
                    "sent": "But it's not going to become well, I. I'm sorry I don't agree with your last statement because we proved that actually the estimator is consistent.",
                    "label": 0
                },
                {
                    "sent": "So of course the C depends on your parameters.",
                    "label": 0
                },
                {
                    "sent": "In becausw like here you see that I mean you see here that the normalization.",
                    "label": 0
                },
                {
                    "sent": "Is part is a function of the parameters, but it seems like you can actually couple the two things.",
                    "label": 0
                },
                {
                    "sent": "Basically it's like healthier parameters and then you have a function which Maps it to the normalization constant, right?",
                    "label": 0
                },
                {
                    "sent": "And basically we have like data in the estimation we have like data map to the parameters and then map to the partition function.",
                    "label": 0
                },
                {
                    "sent": "So what we're actually doing is just the map directly from the data to the partition function.",
                    "label": 0
                },
                {
                    "sent": "I hope this helps.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                }
            ]
        }
    }
}