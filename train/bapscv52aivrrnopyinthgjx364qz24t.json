{
    "id": "bapscv52aivrrnopyinthgjx364qz24t",
    "title": "Kernel Bayes Rule",
    "info": {
        "author": [
            "Kenji Fukumizu, Institute of Statistical Mathematics"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_fukumizu_bayes/",
    "segmentation": [
        [
            "And we're going to talk about recently, Devo."
        ],
        [
            "Opment of kernel method which is our nonparametric inference with possibly different kernels.",
            "Well, here's here's a very brief overview about service teologi in this methodology, kernel means are used in representing and manipulating the probability distribution of variables.",
            "And there well.",
            "Main focus of this talk with our non parametric way of making vision inference.",
            "Which is also possible with possibly different kernels and this kernel Bayesian approaches completing more nonparametric and the computation is basically done by linear algebra with grammar is not as usual kernel method, but well, this is quite different from.",
            "Well, this is a kind of nonparametric Bayesian approach, but this is quite different from Beijing nonparametrics, such as do supplier and so on.",
            "And this well, the kernel based approach is a main topic of my talk."
        ],
        [
            "OK, here's the outline.",
            "My talk.",
            "I first introduced kernel mean as a method for nonparametric inference.",
            "And there are.",
            "I will discuss how the conditional probabilities can be handled by these operators and the based on these building blocks.",
            "I will introduce kernel based rule and discuss some applications."
        ],
        [
            "OK.",
            "So of course we have are many well, classical nonparametric approach to for representing probabilities.",
            "For example if we have data, kernel density estimation can be applied and this is the kind of representing probability distribution and lying probability distribution and also in probability theory characteristic function is are very famous tool to represent the probability.",
            "This is basically the Fourier transform or probability density function.",
            "And then pickers mention is straightforward.",
            "Well, I would like to introduce knew alternative way of representing probabilities.",
            "That is kernel mean.",
            "Well, here's a home formal definition.",
            "Suppose we have random variable taking values in some space.",
            "Omega and K is the positive definite function positive definite kernel Omega?",
            "Colonel mean of the variable X on this arc HS it's simply their expectation or mean of the feature vector so that simple.",
            "That's again the vector in RHS and there's a functional form.",
            "This is an integral of the kernel function with respect to this probability distribution, so that this is very simple.",
            "Imperious mission is quite straightforward because this is our integral with probability.",
            "But so this kernel mean."
        ],
        [
            "Very nice properties.",
            "For example, we have reproducing property of expectation.",
            "Well, because of the expectation with are reproducing property of the kernel, we can.",
            "Well.",
            "Heather this relation, namely their inner product between the kernel mean and the function, is equal to the expectation of F of X.",
            "And also this is an important part of my talk.",
            "Kernel mean has information of higher order moment of the origonal random variable.",
            "Suppose we have well kernel which is defined by the product of you and X and well.",
            "For example, suppose we have this kernel.",
            "This kernel has a Taylor series expansion in this form with no negative coefficient.",
            "And then the kernel mean also allows this type of Taylor series expansion as a function of you.",
            "We observe that older, higher order moments of X appear.",
            "The question of this series.",
            "So.",
            "Well, this tells that kernel mean of some kernel can work as a moment generating function.",
            "So with this view in mind.",
            "It is natural to introduce this death."
        ],
        [
            "Mission.",
            "Well, Colonel is called characteristic if equality over the kernel means implies equality of the probability distributions.",
            "Well, in other words, kernel mean with characteristic kernel.",
            "Uniquely determines the probability.",
            "So this is a necessary condition for kernel mean to characterize our probability uniquely.",
            "And some examples are Gaussian kernel and Laplacian kernels.",
            "But our polynomial, is not characteristic cause kernel mean with polynomial kernel contains only a finite number of moments given by the degree of the kernel.",
            "A degree of the polynomial.",
            "And also the kernel mean of characteristic kernel is very analagous to the characteristic function.",
            "It is well known that the characteristic function uniquely determines the probability distribution.",
            "So characteristic function defining our one to one transform from the spatial probability to some other function space.",
            "And positive definite kernel can do the similar thing.",
            "But we have advantage by using the positive definite kernel becausw.",
            "We don't need to care about the integral appeared in the captives function, so their efficient computation is best possible by kernel trick.",
            "And also the kernel method can be applied to nonbacterial data.",
            "Well, in contrast, the characteristic function can be defined only for Euclidean domain."
        ],
        [
            "OK, so with characteristic kernel, because of this one to one correspondence of their probability to their their kernel means we can cast their problem of inference problem on the distribution to the inference problem of their kernel means.",
            "This is our principle.",
            "For example, 2 sample tests can be interpreted as a comparison between two kernel means and independence tests can be regarded as a comparison between the kernel kernel meaning of the joint distribution and the kernel mean of the product of the marginals.",
            "And these ideas, having already used in applied and there this one called the MD.",
            "And this result in sick and proposal BIOS, aggression and so on.",
            "OK, here my talk.",
            "Focus on Bayesian inference in the Bayesian inference.",
            "In this framework, our goal is to estimate the kernel meaning of the posterior.",
            "Given kernel representation of prior and conditional probability and I'm going to explain the more precise meaning of this sentence well later."
        ],
        [
            "OK, of course.",
            "Well, there are many were classical approaches to nonparametric inference.",
            "Well, the most famous one is smoothing kernel such as kernel density estimation and local polynomial fitting and the characteristic function can be also used for nonparametric inference such as two sample test or independence tests.",
            "However, it is well known that these classical nonparametric approach are very weak to high dimensional data.",
            "Well, in this case high demand doesn't mean 100 or 1000.",
            "Well, just five or six dimension are already very difficult to handle by this classical method.",
            "So with our new framework with kernels, the questions are what can we do with this new operators and also how robust to high dimensionality?",
            "And because this talk is very short, I will mainly focus on the 1st question.",
            "What can we do with this methodology?"
        ],
        [
            "OK, let's move onto their conditional probability.",
            "Obviously."
        ],
        [
            "It is very important to constructor method that we handle conditional probability and statistical inference.",
            "For example, graphical models can be defined by is defined conditional independence of variables and the Bayesian inference is also, well, some sort of conditional distribution.",
            "And kernel meaning of conditional probability will in population is simply defined in this way.",
            "This is just the kernel mean of with respect to this conditional probability.",
            "However well it is well known that accurate estimation of conditional density function is very difficult, so we need to find well another way to estimate this kernel conditional mean.",
            "Without explicit estimation of conditional density and we use regression approach to solve this problem."
        ],
        [
            "And to introduce regression, I first well define covariance on our cages.",
            "Well, this is against very simple.",
            "Suppose we have two random variables X&Y.",
            "In discussing the relation between X&YI, use their covariance of feature vectors by introducing respective feature kernels on each variables.",
            "And this is, well, simply the covariance of feature vectors, so there is no difficulty in considering this one.",
            "And this is operator from HX to HY.",
            "And so this is a simple, straightforward extension of the covariance matrix for ordinary random vectors.",
            "Which, again, by virtue of the reproducing property of the kernel we have reproducing property of covariance.",
            "Well, in this talk I only talk about uncentered covariance.",
            "So we can we have repositioned property of the kernel by inner product.",
            "Another important remark about the covariance operator is that covariance can be identified with their kernel mean on the product space, where this is again, well the same as our finite dimensional case.",
            "So the covariance can be well build in two spaces.",
            "One is linear operator and the one is.",
            "Mean on the product space?",
            "I'm going to use this fact in constructing the kernel based rules later."
        ],
        [
            "OK, let's come back to the conditional kernel mean.",
            "But before considering kernel, let's review the conditional expectation of Gaussian random variables.",
            "Well, this is just the least square.",
            "Estimation is the square well least square.",
            "Problem for random vectors and well it is easy to see that the solution is given by this matrix given by their covariance operators.",
            "And therefore Gaussian random variable conditional expectation of Y given X is given by this matrix multiplied by vector X. OK, this is a very basic fact and we can extend this fact to the kernel case by considering the feature vectors.",
            "So let's consider this little square problem estimating predicting fee of Y by a linear mapping of XF is just a linear mapping.",
            "And there in the similar way we can show that their solution is given by the operator which are defined by covariance operators.",
            "And.",
            "If we use characters, the kernel 4X.",
            "We can show that the conditional expectation of fear of Y given X is equal to there.",
            "This operator multiplied by this feature vector.",
            "This is completely analogous to the Gaussian case.",
            "And we can see that the left hand side is exactly what we want.",
            "Color, mean, conditional kernel mean of Y given X.",
            "So this equation tells that there are two estimates.",
            "Are there conditional kernel mean?",
            "We can estimate the covariance operators.",
            "And this can be done with gram matrices.",
            "So."
        ],
        [
            "Empirical estimation is given in this way, so the implication of this one from the previous equation simply given by there.",
            "Rich Kernel Ridge regression.",
            "But different from the standard kernel Ridge regression instead of Y we use kernel because we are considering the regression with the feature vector.",
            "And this one is given by the kernel for the feature vector evaluated at the sample point.",
            "And epsilon is a regularization question in the kernel Ledger question.",
            "So this is our empirical estimator for the conditional kernel mean.",
            "What's one important notices that that we're using joint sample two gives covariance between X&Y.",
            "And then there are population.",
            "We can use our conditional density function.",
            "But our sample from the conditional density given X is impossible for continuous domain.",
            "Instead we assume that we have a joint sample from some joint distribution that gives a conditional distribution which we are discussing."
        ],
        [
            "OK, let's move onto their kernel based rule.",
            "Here is a very famous phrase rule on T shirts and we want to."
        ],
        [
            "Implement this by using kernels.",
            "Um?",
            "So in.",
            "Statical inference with graphical models.",
            "We often use these rules of making inference.",
            "Well, this is some rule.",
            "This makes this computer marginal of Y and this is chain rule.",
            "This gives the product joint distribution of X&Y by this product and combination needs.",
            "Reduce it to the base rule.",
            "And we want to kernelized these rules.",
            "So by kernelization I mean that we wish to express their all the probability by kernel means and express the status quo relation among the variable in terms of covariance operators or gram matrices and realize these rules with gram matrix computation that's our goal."
        ],
        [
            "OK, let's look at some rule first.",
            "Welsummer always.",
            "Given in this way and well, we already have the expression of conditional kernel mean and let's consider this expression which I previously showed.",
            "And by taking the integral with respect to prior.",
            "The left hand side, sorry the right hand side is.",
            "Exactly the same as this right hand side and the left hand side.",
            "You can see that this is the kernel meaning of Q.",
            "Right?",
            "So from the previous conditional kernel mean result that we can easily give this expression.",
            "This is a counter aggression.",
            "Of a condominium, the prior to the color mean of Y.",
            "And empirically there our input is our estimator over the prior and the joint sample to represent conditional probability and the joint sample is a linear combination of some feature vector with extruder and in this case extruder and this sample X can be different.",
            "It could be the same, but there can be different.",
            "And the solution there empirically, this can be again given by Colonel Ritchie regression.",
            "And because we have empire here, we use the grammar tricks of X&X tutor.",
            "But basically bet the question to the Y is given by the Kernel Ridge regression."
        ],
        [
            "OK. Let's consider kernel chain rule, kernel chain rule that gives a disjoint probability by the product of the conditional and prior.",
            "And actually this is a special case of previous kernel, some rule.",
            "We don't have an integral here, but by introducing this slightly fancy covariance operator, we can do that.",
            "This fancy covariance operator is a operator from the space of X to the space product space of X&Y.",
            "So this account will start over that answer.",
            "But the probability is given by the product of the conditional probability and the Delta function between X&X prime and by taking the integral with respect to X prime.",
            "We can raise this date function and we can raise integral and we get this.",
            "Product and their empirical estimation is almost the same as the previous kernel sumeru this is our gain their kernel Ridge regression, the computational kernel recognition and the questions are the same, but the difference is here.",
            "The feature vector is the is given by the product.",
            "This is the only difference from the previous kernel, some rule.",
            "And this is our there.",
            "Estimator for the kernel meaning of this product probability.",
            "OK, once we have this kernel chain rule."
        ],
        [
            "Bayes rule is actually very straightforward.",
            "Well, we have already know the expression of this joint distribution.",
            "Once we know this joint distribution, the Bayes rule is simply the regression from Y to X or the conditional probability of X given Y.",
            "So we have already developed there's a way of expressing conditional probability if we know the covariance of this operator of this distribution, right?",
            "So the Kernel Bayes rule is given by this kernel mean.",
            "An embedding our conditional cardamine.",
            "Using the covariance operator over disjoint probability.",
            "And this covariance?",
            "Well, we should recall that the covariance operator is equivalent to the mean on the product space and in the previous slide we have all."
        ],
        [
            "Redeveloped how to compute the kernel meaning of this?",
            "Conditional probability on the product space.",
            "So we have done."
        ],
        [
            "So actually, in the first step we compute the covariance operator or kernel mean on the product space by using their kernel chain rule and in the second step we solve this regression.",
            "By Kernel Ridge regression.",
            "So there are two steps to regression steps and we can compute the question which depend on the observation Y.",
            "And this is our feature.",
            "This is a vector in the feature space of X.",
            "And this works as our estimator of the kernel mean of the posterior.",
            "This is our kernel based rule.",
            "OK."
        ],
        [
            "So we have developed Sir estimator of the kernel meaning of posterior, but that's not exactly the the posterior, so we need to consider how to use it for Beijing interference.",
            "Well, I have two examples, so if our purpose is to compute the expectation for some function with respect to the posterior, simply takes the inner product.",
            "Then this empirical expression is guaranteed to converge to the true expectation with posterior.",
            "And if our purpose is the point estimation of variable X.",
            "Let's consider this preimage problem.",
            "This is the optimal point, so that's a feature vector is closest to choose our kernel mean of the posterior.",
            "So this is the kind of primitive problem, and there are lots of algorithms to solve this problem."
        ],
        [
            "OK, so the Kernel Bayes rule is gives a completely nonparametric way of computing Bayes rule.",
            "And this is well.",
            "We haven't tested it with no parametric model here.",
            "But data or sample are used to express the probability probabilistic relational among variable.",
            "So this is completely nonparametric, more unknown parameter example.",
            "So called Bayesian nonparametrics, right?",
            "So everything is nonparametric.",
            "And here are a couple of examples where such approaches approach is useful.",
            "The first one is nonparametric hidden Markov model, but I'm going to explain a little bit in more detail in the next slide.",
            "And the second example is our situation where explicit form of liquid function is not available, but sampling is possible.",
            "Well, in this situation a typical method is approximate Bayesian computation, called ABC.",
            "And we have we can use our Kernel Bayes rule by taking the sample by sampling from this distribution and prepare the training sample.",
            "And we have actually proposed a kernel version of ABC called Colonel ABC and we have applied the kernel ABC approach to population genetics problem as we observe that kernel ABC gives better results than standard ABC approach.",
            "And in the second example there we have also developed the Kernel Congregational Bellman equation, implemented presetting.",
            "I'm not going into the details, but we can construct their reinforcement learning algorithm in nonparametric way for continuous domain.",
            "And this paper has been appeared in this year's UI."
        ],
        [
            "OK.",
            "So.",
            "Because because the time is very short, I will explain only one example of nonparametric killer Markov model.",
            "So this is a standard hidden Markov model, but we assume that the probability distribution of transition or end or observation not to know.",
            "But instead, we assume that the data is available in training phase, including the hidden state.",
            "Having data of hidden state might sound a little bit strange, but are we have?",
            "Example, we have situations where this is possible.",
            "For example, if the measurement of the hidden states very expensive, we have only small number of training samples for hidden states, but and we want to make our inference based on these small number of hit of training sample.",
            "That's the one example and in another case there hidden states may be observed with big time dilay.",
            "In that case the previous well there all the sample may be used for training data that we wish to make.",
            "Inference for the current or future state hidden state.",
            "And in testing phase we observe a new series of observation and well, we can make an inference for the hidden state using kernel based role.",
            "Well, in their numerical example, which I'm going to show from now on, I use a point estimator given by the preimage.",
            "And there are big cause sequential estimation rule of hidden Markov model in general.",
            "It's written by Bayes rule, only bends role.",
            "We can straightforwardly implement our kernel version.",
            "Using their standard.",
            "No inference rule of hidden Markov model."
        ],
        [
            "OK, here is 1 example.",
            "This is a smoothing problem and there the hidden state is generated along this red line an with time with the shift of angle.",
            "And our observation is just a noisy observation.",
            "And I compare our kernel based approach with their standard nonlinear Kalman filter extender common filter and unscented common filter and the notice that our Kernel Bayes rule doesn't know this dynamics that just use a sample.",
            "But this this nonlinear common filters.",
            "Know this dynamics and use this, but if the training sample size is over 200, we observed that our kernel based approach outperforms significantly to the standard nonlinear Kalman filters.",
            "By this strong nonlinearity."
        ],
        [
            "OK, second example is a little bit more realistic.",
            "In this problem we want to estimate there's a rotational angular camera located at the corner of our room and their observation is.",
            "Movie frame taken by the camera.",
            "Like this?",
            "And after collecting some training data, we want to estimate the camera and the current camera angle from the Taken movie frame and this problem in this problem it is very difficult to introduce some reasonable parameter Komodo from the camera angle to the movie frame.",
            "So permit nonparametric approaches very preferable.",
            "Actually bye.",
            "Comparison with our approach to the standard common filter, assuming their transition and observation, linear Gaussian, we can see that the counter based approach significantly outperforms there, stands the kernel Kalman filter.",
            "OK."
        ],
        [
            "This isn't my talk, so there I introduced their number.",
            "Very general nonparametric approach with positive definite kernels.",
            "And there are everything can be and also introduce the Bayesian inference.",
            "Which is completely nonparametric and we don't need to assume any parametric model about the sample is used to describe the relation.",
            "And we don't need to any integral or we don't need sample sampling.",
            "And."
        ],
        [
            "Here are some ongoing and future works.",
            "So there.",
            "Our kernel basically was completely nonparametric, but still in some problems.",
            "Some parts may be modeled nicely by parametric model, then the others may not.",
            "In that case it is attractive to combine parametric approach with kernel nonparametric approach and actually we have preliminary results about the combination of.",
            "Known dynamics and unknown nonparametric part in hidden Markov model exactly integration plus kernel nonparametrics or particle filter approach with kernel nonparametric.",
            "Nonparametrics, but I'm not going into the details about this.",
            "And also I skipped all the theoretical analysis in my talk about the theoretical analysis, especially for high dimensional data, is very important in comparison with other conventional approach nonparametric approach.",
            "And also, well, I'm very interested into.",
            "Interested to understand the link with other recent nonparametric approach such as Gaussian process or vision nonparametrics with our kernel vision approach."
        ],
        [
            "OK, just this talk is based on the collaboration with these guys and here's my last message.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're going to talk about recently, Devo.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opment of kernel method which is our nonparametric inference with possibly different kernels.",
                    "label": 0
                },
                {
                    "sent": "Well, here's here's a very brief overview about service teologi in this methodology, kernel means are used in representing and manipulating the probability distribution of variables.",
                    "label": 1
                },
                {
                    "sent": "And there well.",
                    "label": 0
                },
                {
                    "sent": "Main focus of this talk with our non parametric way of making vision inference.",
                    "label": 1
                },
                {
                    "sent": "Which is also possible with possibly different kernels and this kernel Bayesian approaches completing more nonparametric and the computation is basically done by linear algebra with grammar is not as usual kernel method, but well, this is quite different from.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a kind of nonparametric Bayesian approach, but this is quite different from Beijing nonparametrics, such as do supplier and so on.",
                    "label": 0
                },
                {
                    "sent": "And this well, the kernel based approach is a main topic of my talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's the outline.",
                    "label": 0
                },
                {
                    "sent": "My talk.",
                    "label": 0
                },
                {
                    "sent": "I first introduced kernel mean as a method for nonparametric inference.",
                    "label": 1
                },
                {
                    "sent": "And there are.",
                    "label": 0
                },
                {
                    "sent": "I will discuss how the conditional probabilities can be handled by these operators and the based on these building blocks.",
                    "label": 0
                },
                {
                    "sent": "I will introduce kernel based rule and discuss some applications.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So of course we have are many well, classical nonparametric approach to for representing probabilities.",
                    "label": 1
                },
                {
                    "sent": "For example if we have data, kernel density estimation can be applied and this is the kind of representing probability distribution and lying probability distribution and also in probability theory characteristic function is are very famous tool to represent the probability.",
                    "label": 0
                },
                {
                    "sent": "This is basically the Fourier transform or probability density function.",
                    "label": 0
                },
                {
                    "sent": "And then pickers mention is straightforward.",
                    "label": 1
                },
                {
                    "sent": "Well, I would like to introduce knew alternative way of representing probabilities.",
                    "label": 0
                },
                {
                    "sent": "That is kernel mean.",
                    "label": 0
                },
                {
                    "sent": "Well, here's a home formal definition.",
                    "label": 1
                },
                {
                    "sent": "Suppose we have random variable taking values in some space.",
                    "label": 1
                },
                {
                    "sent": "Omega and K is the positive definite function positive definite kernel Omega?",
                    "label": 0
                },
                {
                    "sent": "Colonel mean of the variable X on this arc HS it's simply their expectation or mean of the feature vector so that simple.",
                    "label": 0
                },
                {
                    "sent": "That's again the vector in RHS and there's a functional form.",
                    "label": 1
                },
                {
                    "sent": "This is an integral of the kernel function with respect to this probability distribution, so that this is very simple.",
                    "label": 0
                },
                {
                    "sent": "Imperious mission is quite straightforward because this is our integral with probability.",
                    "label": 0
                },
                {
                    "sent": "But so this kernel mean.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very nice properties.",
                    "label": 0
                },
                {
                    "sent": "For example, we have reproducing property of expectation.",
                    "label": 0
                },
                {
                    "sent": "Well, because of the expectation with are reproducing property of the kernel, we can.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Heather this relation, namely their inner product between the kernel mean and the function, is equal to the expectation of F of X.",
                    "label": 0
                },
                {
                    "sent": "And also this is an important part of my talk.",
                    "label": 0
                },
                {
                    "sent": "Kernel mean has information of higher order moment of the origonal random variable.",
                    "label": 1
                },
                {
                    "sent": "Suppose we have well kernel which is defined by the product of you and X and well.",
                    "label": 0
                },
                {
                    "sent": "For example, suppose we have this kernel.",
                    "label": 0
                },
                {
                    "sent": "This kernel has a Taylor series expansion in this form with no negative coefficient.",
                    "label": 0
                },
                {
                    "sent": "And then the kernel mean also allows this type of Taylor series expansion as a function of you.",
                    "label": 1
                },
                {
                    "sent": "We observe that older, higher order moments of X appear.",
                    "label": 0
                },
                {
                    "sent": "The question of this series.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Well, this tells that kernel mean of some kernel can work as a moment generating function.",
                    "label": 0
                },
                {
                    "sent": "So with this view in mind.",
                    "label": 0
                },
                {
                    "sent": "It is natural to introduce this death.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "Well, Colonel is called characteristic if equality over the kernel means implies equality of the probability distributions.",
                    "label": 1
                },
                {
                    "sent": "Well, in other words, kernel mean with characteristic kernel.",
                    "label": 1
                },
                {
                    "sent": "Uniquely determines the probability.",
                    "label": 0
                },
                {
                    "sent": "So this is a necessary condition for kernel mean to characterize our probability uniquely.",
                    "label": 0
                },
                {
                    "sent": "And some examples are Gaussian kernel and Laplacian kernels.",
                    "label": 0
                },
                {
                    "sent": "But our polynomial, is not characteristic cause kernel mean with polynomial kernel contains only a finite number of moments given by the degree of the kernel.",
                    "label": 0
                },
                {
                    "sent": "A degree of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "And also the kernel mean of characteristic kernel is very analagous to the characteristic function.",
                    "label": 1
                },
                {
                    "sent": "It is well known that the characteristic function uniquely determines the probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So characteristic function defining our one to one transform from the spatial probability to some other function space.",
                    "label": 0
                },
                {
                    "sent": "And positive definite kernel can do the similar thing.",
                    "label": 0
                },
                {
                    "sent": "But we have advantage by using the positive definite kernel becausw.",
                    "label": 0
                },
                {
                    "sent": "We don't need to care about the integral appeared in the captives function, so their efficient computation is best possible by kernel trick.",
                    "label": 0
                },
                {
                    "sent": "And also the kernel method can be applied to nonbacterial data.",
                    "label": 0
                },
                {
                    "sent": "Well, in contrast, the characteristic function can be defined only for Euclidean domain.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so with characteristic kernel, because of this one to one correspondence of their probability to their their kernel means we can cast their problem of inference problem on the distribution to the inference problem of their kernel means.",
                    "label": 0
                },
                {
                    "sent": "This is our principle.",
                    "label": 0
                },
                {
                    "sent": "For example, 2 sample tests can be interpreted as a comparison between two kernel means and independence tests can be regarded as a comparison between the kernel kernel meaning of the joint distribution and the kernel mean of the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "And these ideas, having already used in applied and there this one called the MD.",
                    "label": 0
                },
                {
                    "sent": "And this result in sick and proposal BIOS, aggression and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, here my talk.",
                    "label": 0
                },
                {
                    "sent": "Focus on Bayesian inference in the Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "In this framework, our goal is to estimate the kernel meaning of the posterior.",
                    "label": 0
                },
                {
                    "sent": "Given kernel representation of prior and conditional probability and I'm going to explain the more precise meaning of this sentence well later.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, of course.",
                    "label": 0
                },
                {
                    "sent": "Well, there are many were classical approaches to nonparametric inference.",
                    "label": 1
                },
                {
                    "sent": "Well, the most famous one is smoothing kernel such as kernel density estimation and local polynomial fitting and the characteristic function can be also used for nonparametric inference such as two sample test or independence tests.",
                    "label": 1
                },
                {
                    "sent": "However, it is well known that these classical nonparametric approach are very weak to high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "Well, in this case high demand doesn't mean 100 or 1000.",
                    "label": 0
                },
                {
                    "sent": "Well, just five or six dimension are already very difficult to handle by this classical method.",
                    "label": 0
                },
                {
                    "sent": "So with our new framework with kernels, the questions are what can we do with this new operators and also how robust to high dimensionality?",
                    "label": 0
                },
                {
                    "sent": "And because this talk is very short, I will mainly focus on the 1st question.",
                    "label": 1
                },
                {
                    "sent": "What can we do with this methodology?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's move onto their conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Obviously.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is very important to constructor method that we handle conditional probability and statistical inference.",
                    "label": 0
                },
                {
                    "sent": "For example, graphical models can be defined by is defined conditional independence of variables and the Bayesian inference is also, well, some sort of conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And kernel meaning of conditional probability will in population is simply defined in this way.",
                    "label": 0
                },
                {
                    "sent": "This is just the kernel mean of with respect to this conditional probability.",
                    "label": 1
                },
                {
                    "sent": "However well it is well known that accurate estimation of conditional density function is very difficult, so we need to find well another way to estimate this kernel conditional mean.",
                    "label": 1
                },
                {
                    "sent": "Without explicit estimation of conditional density and we use regression approach to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to introduce regression, I first well define covariance on our cages.",
                    "label": 0
                },
                {
                    "sent": "Well, this is against very simple.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have two random variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "In discussing the relation between X&YI, use their covariance of feature vectors by introducing respective feature kernels on each variables.",
                    "label": 0
                },
                {
                    "sent": "And this is, well, simply the covariance of feature vectors, so there is no difficulty in considering this one.",
                    "label": 0
                },
                {
                    "sent": "And this is operator from HX to HY.",
                    "label": 0
                },
                {
                    "sent": "And so this is a simple, straightforward extension of the covariance matrix for ordinary random vectors.",
                    "label": 1
                },
                {
                    "sent": "Which, again, by virtue of the reproducing property of the kernel we have reproducing property of covariance.",
                    "label": 1
                },
                {
                    "sent": "Well, in this talk I only talk about uncentered covariance.",
                    "label": 0
                },
                {
                    "sent": "So we can we have repositioned property of the kernel by inner product.",
                    "label": 0
                },
                {
                    "sent": "Another important remark about the covariance operator is that covariance can be identified with their kernel mean on the product space, where this is again, well the same as our finite dimensional case.",
                    "label": 1
                },
                {
                    "sent": "So the covariance can be well build in two spaces.",
                    "label": 0
                },
                {
                    "sent": "One is linear operator and the one is.",
                    "label": 0
                },
                {
                    "sent": "Mean on the product space?",
                    "label": 0
                },
                {
                    "sent": "I'm going to use this fact in constructing the kernel based rules later.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's come back to the conditional kernel mean.",
                    "label": 1
                },
                {
                    "sent": "But before considering kernel, let's review the conditional expectation of Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "Well, this is just the least square.",
                    "label": 0
                },
                {
                    "sent": "Estimation is the square well least square.",
                    "label": 0
                },
                {
                    "sent": "Problem for random vectors and well it is easy to see that the solution is given by this matrix given by their covariance operators.",
                    "label": 0
                },
                {
                    "sent": "And therefore Gaussian random variable conditional expectation of Y given X is given by this matrix multiplied by vector X. OK, this is a very basic fact and we can extend this fact to the kernel case by considering the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So let's consider this little square problem estimating predicting fee of Y by a linear mapping of XF is just a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "And there in the similar way we can show that their solution is given by the operator which are defined by covariance operators.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If we use characters, the kernel 4X.",
                    "label": 0
                },
                {
                    "sent": "We can show that the conditional expectation of fear of Y given X is equal to there.",
                    "label": 0
                },
                {
                    "sent": "This operator multiplied by this feature vector.",
                    "label": 0
                },
                {
                    "sent": "This is completely analogous to the Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "And we can see that the left hand side is exactly what we want.",
                    "label": 0
                },
                {
                    "sent": "Color, mean, conditional kernel mean of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So this equation tells that there are two estimates.",
                    "label": 0
                },
                {
                    "sent": "Are there conditional kernel mean?",
                    "label": 1
                },
                {
                    "sent": "We can estimate the covariance operators.",
                    "label": 0
                },
                {
                    "sent": "And this can be done with gram matrices.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Empirical estimation is given in this way, so the implication of this one from the previous equation simply given by there.",
                    "label": 0
                },
                {
                    "sent": "Rich Kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "But different from the standard kernel Ridge regression instead of Y we use kernel because we are considering the regression with the feature vector.",
                    "label": 1
                },
                {
                    "sent": "And this one is given by the kernel for the feature vector evaluated at the sample point.",
                    "label": 0
                },
                {
                    "sent": "And epsilon is a regularization question in the kernel Ledger question.",
                    "label": 0
                },
                {
                    "sent": "So this is our empirical estimator for the conditional kernel mean.",
                    "label": 1
                },
                {
                    "sent": "What's one important notices that that we're using joint sample two gives covariance between X&Y.",
                    "label": 0
                },
                {
                    "sent": "And then there are population.",
                    "label": 0
                },
                {
                    "sent": "We can use our conditional density function.",
                    "label": 0
                },
                {
                    "sent": "But our sample from the conditional density given X is impossible for continuous domain.",
                    "label": 0
                },
                {
                    "sent": "Instead we assume that we have a joint sample from some joint distribution that gives a conditional distribution which we are discussing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's move onto their kernel based rule.",
                    "label": 0
                },
                {
                    "sent": "Here is a very famous phrase rule on T shirts and we want to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implement this by using kernels.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                },
                {
                    "sent": "Statical inference with graphical models.",
                    "label": 1
                },
                {
                    "sent": "We often use these rules of making inference.",
                    "label": 0
                },
                {
                    "sent": "Well, this is some rule.",
                    "label": 1
                },
                {
                    "sent": "This makes this computer marginal of Y and this is chain rule.",
                    "label": 0
                },
                {
                    "sent": "This gives the product joint distribution of X&Y by this product and combination needs.",
                    "label": 0
                },
                {
                    "sent": "Reduce it to the base rule.",
                    "label": 0
                },
                {
                    "sent": "And we want to kernelized these rules.",
                    "label": 0
                },
                {
                    "sent": "So by kernelization I mean that we wish to express their all the probability by kernel means and express the status quo relation among the variable in terms of covariance operators or gram matrices and realize these rules with gram matrix computation that's our goal.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's look at some rule first.",
                    "label": 0
                },
                {
                    "sent": "Welsummer always.",
                    "label": 0
                },
                {
                    "sent": "Given in this way and well, we already have the expression of conditional kernel mean and let's consider this expression which I previously showed.",
                    "label": 0
                },
                {
                    "sent": "And by taking the integral with respect to prior.",
                    "label": 0
                },
                {
                    "sent": "The left hand side, sorry the right hand side is.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same as this right hand side and the left hand side.",
                    "label": 0
                },
                {
                    "sent": "You can see that this is the kernel meaning of Q.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So from the previous conditional kernel mean result that we can easily give this expression.",
                    "label": 0
                },
                {
                    "sent": "This is a counter aggression.",
                    "label": 0
                },
                {
                    "sent": "Of a condominium, the prior to the color mean of Y.",
                    "label": 0
                },
                {
                    "sent": "And empirically there our input is our estimator over the prior and the joint sample to represent conditional probability and the joint sample is a linear combination of some feature vector with extruder and in this case extruder and this sample X can be different.",
                    "label": 0
                },
                {
                    "sent": "It could be the same, but there can be different.",
                    "label": 0
                },
                {
                    "sent": "And the solution there empirically, this can be again given by Colonel Ritchie regression.",
                    "label": 0
                },
                {
                    "sent": "And because we have empire here, we use the grammar tricks of X&X tutor.",
                    "label": 0
                },
                {
                    "sent": "But basically bet the question to the Y is given by the Kernel Ridge regression.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Let's consider kernel chain rule, kernel chain rule that gives a disjoint probability by the product of the conditional and prior.",
                    "label": 1
                },
                {
                    "sent": "And actually this is a special case of previous kernel, some rule.",
                    "label": 0
                },
                {
                    "sent": "We don't have an integral here, but by introducing this slightly fancy covariance operator, we can do that.",
                    "label": 0
                },
                {
                    "sent": "This fancy covariance operator is a operator from the space of X to the space product space of X&Y.",
                    "label": 0
                },
                {
                    "sent": "So this account will start over that answer.",
                    "label": 0
                },
                {
                    "sent": "But the probability is given by the product of the conditional probability and the Delta function between X&X prime and by taking the integral with respect to X prime.",
                    "label": 0
                },
                {
                    "sent": "We can raise this date function and we can raise integral and we get this.",
                    "label": 0
                },
                {
                    "sent": "Product and their empirical estimation is almost the same as the previous kernel sumeru this is our gain their kernel Ridge regression, the computational kernel recognition and the questions are the same, but the difference is here.",
                    "label": 0
                },
                {
                    "sent": "The feature vector is the is given by the product.",
                    "label": 0
                },
                {
                    "sent": "This is the only difference from the previous kernel, some rule.",
                    "label": 0
                },
                {
                    "sent": "And this is our there.",
                    "label": 0
                },
                {
                    "sent": "Estimator for the kernel meaning of this product probability.",
                    "label": 0
                },
                {
                    "sent": "OK, once we have this kernel chain rule.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bayes rule is actually very straightforward.",
                    "label": 0
                },
                {
                    "sent": "Well, we have already know the expression of this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Once we know this joint distribution, the Bayes rule is simply the regression from Y to X or the conditional probability of X given Y.",
                    "label": 0
                },
                {
                    "sent": "So we have already developed there's a way of expressing conditional probability if we know the covariance of this operator of this distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So the Kernel Bayes rule is given by this kernel mean.",
                    "label": 1
                },
                {
                    "sent": "An embedding our conditional cardamine.",
                    "label": 0
                },
                {
                    "sent": "Using the covariance operator over disjoint probability.",
                    "label": 0
                },
                {
                    "sent": "And this covariance?",
                    "label": 0
                },
                {
                    "sent": "Well, we should recall that the covariance operator is equivalent to the mean on the product space and in the previous slide we have all.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Redeveloped how to compute the kernel meaning of this?",
                    "label": 0
                },
                {
                    "sent": "Conditional probability on the product space.",
                    "label": 0
                },
                {
                    "sent": "So we have done.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually, in the first step we compute the covariance operator or kernel mean on the product space by using their kernel chain rule and in the second step we solve this regression.",
                    "label": 1
                },
                {
                    "sent": "By Kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So there are two steps to regression steps and we can compute the question which depend on the observation Y.",
                    "label": 0
                },
                {
                    "sent": "And this is our feature.",
                    "label": 0
                },
                {
                    "sent": "This is a vector in the feature space of X.",
                    "label": 0
                },
                {
                    "sent": "And this works as our estimator of the kernel mean of the posterior.",
                    "label": 0
                },
                {
                    "sent": "This is our kernel based rule.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have developed Sir estimator of the kernel meaning of posterior, but that's not exactly the the posterior, so we need to consider how to use it for Beijing interference.",
                    "label": 1
                },
                {
                    "sent": "Well, I have two examples, so if our purpose is to compute the expectation for some function with respect to the posterior, simply takes the inner product.",
                    "label": 0
                },
                {
                    "sent": "Then this empirical expression is guaranteed to converge to the true expectation with posterior.",
                    "label": 1
                },
                {
                    "sent": "And if our purpose is the point estimation of variable X.",
                    "label": 1
                },
                {
                    "sent": "Let's consider this preimage problem.",
                    "label": 1
                },
                {
                    "sent": "This is the optimal point, so that's a feature vector is closest to choose our kernel mean of the posterior.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of primitive problem, and there are lots of algorithms to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the Kernel Bayes rule is gives a completely nonparametric way of computing Bayes rule.",
                    "label": 1
                },
                {
                    "sent": "And this is well.",
                    "label": 0
                },
                {
                    "sent": "We haven't tested it with no parametric model here.",
                    "label": 0
                },
                {
                    "sent": "But data or sample are used to express the probability probabilistic relational among variable.",
                    "label": 1
                },
                {
                    "sent": "So this is completely nonparametric, more unknown parameter example.",
                    "label": 0
                },
                {
                    "sent": "So called Bayesian nonparametrics, right?",
                    "label": 0
                },
                {
                    "sent": "So everything is nonparametric.",
                    "label": 0
                },
                {
                    "sent": "And here are a couple of examples where such approaches approach is useful.",
                    "label": 1
                },
                {
                    "sent": "The first one is nonparametric hidden Markov model, but I'm going to explain a little bit in more detail in the next slide.",
                    "label": 0
                },
                {
                    "sent": "And the second example is our situation where explicit form of liquid function is not available, but sampling is possible.",
                    "label": 0
                },
                {
                    "sent": "Well, in this situation a typical method is approximate Bayesian computation, called ABC.",
                    "label": 0
                },
                {
                    "sent": "And we have we can use our Kernel Bayes rule by taking the sample by sampling from this distribution and prepare the training sample.",
                    "label": 0
                },
                {
                    "sent": "And we have actually proposed a kernel version of ABC called Colonel ABC and we have applied the kernel ABC approach to population genetics problem as we observe that kernel ABC gives better results than standard ABC approach.",
                    "label": 0
                },
                {
                    "sent": "And in the second example there we have also developed the Kernel Congregational Bellman equation, implemented presetting.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into the details, but we can construct their reinforcement learning algorithm in nonparametric way for continuous domain.",
                    "label": 0
                },
                {
                    "sent": "And this paper has been appeared in this year's UI.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because because the time is very short, I will explain only one example of nonparametric killer Markov model.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard hidden Markov model, but we assume that the probability distribution of transition or end or observation not to know.",
                    "label": 0
                },
                {
                    "sent": "But instead, we assume that the data is available in training phase, including the hidden state.",
                    "label": 1
                },
                {
                    "sent": "Having data of hidden state might sound a little bit strange, but are we have?",
                    "label": 0
                },
                {
                    "sent": "Example, we have situations where this is possible.",
                    "label": 1
                },
                {
                    "sent": "For example, if the measurement of the hidden states very expensive, we have only small number of training samples for hidden states, but and we want to make our inference based on these small number of hit of training sample.",
                    "label": 0
                },
                {
                    "sent": "That's the one example and in another case there hidden states may be observed with big time dilay.",
                    "label": 0
                },
                {
                    "sent": "In that case the previous well there all the sample may be used for training data that we wish to make.",
                    "label": 0
                },
                {
                    "sent": "Inference for the current or future state hidden state.",
                    "label": 0
                },
                {
                    "sent": "And in testing phase we observe a new series of observation and well, we can make an inference for the hidden state using kernel based role.",
                    "label": 0
                },
                {
                    "sent": "Well, in their numerical example, which I'm going to show from now on, I use a point estimator given by the preimage.",
                    "label": 1
                },
                {
                    "sent": "And there are big cause sequential estimation rule of hidden Markov model in general.",
                    "label": 0
                },
                {
                    "sent": "It's written by Bayes rule, only bends role.",
                    "label": 0
                },
                {
                    "sent": "We can straightforwardly implement our kernel version.",
                    "label": 1
                },
                {
                    "sent": "Using their standard.",
                    "label": 0
                },
                {
                    "sent": "No inference rule of hidden Markov model.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here is 1 example.",
                    "label": 0
                },
                {
                    "sent": "This is a smoothing problem and there the hidden state is generated along this red line an with time with the shift of angle.",
                    "label": 0
                },
                {
                    "sent": "And our observation is just a noisy observation.",
                    "label": 0
                },
                {
                    "sent": "And I compare our kernel based approach with their standard nonlinear Kalman filter extender common filter and unscented common filter and the notice that our Kernel Bayes rule doesn't know this dynamics that just use a sample.",
                    "label": 0
                },
                {
                    "sent": "But this this nonlinear common filters.",
                    "label": 0
                },
                {
                    "sent": "Know this dynamics and use this, but if the training sample size is over 200, we observed that our kernel based approach outperforms significantly to the standard nonlinear Kalman filters.",
                    "label": 0
                },
                {
                    "sent": "By this strong nonlinearity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, second example is a little bit more realistic.",
                    "label": 0
                },
                {
                    "sent": "In this problem we want to estimate there's a rotational angular camera located at the corner of our room and their observation is.",
                    "label": 1
                },
                {
                    "sent": "Movie frame taken by the camera.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And after collecting some training data, we want to estimate the camera and the current camera angle from the Taken movie frame and this problem in this problem it is very difficult to introduce some reasonable parameter Komodo from the camera angle to the movie frame.",
                    "label": 0
                },
                {
                    "sent": "So permit nonparametric approaches very preferable.",
                    "label": 0
                },
                {
                    "sent": "Actually bye.",
                    "label": 0
                },
                {
                    "sent": "Comparison with our approach to the standard common filter, assuming their transition and observation, linear Gaussian, we can see that the counter based approach significantly outperforms there, stands the kernel Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This isn't my talk, so there I introduced their number.",
                    "label": 0
                },
                {
                    "sent": "Very general nonparametric approach with positive definite kernels.",
                    "label": 0
                },
                {
                    "sent": "And there are everything can be and also introduce the Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Which is completely nonparametric and we don't need to assume any parametric model about the sample is used to describe the relation.",
                    "label": 0
                },
                {
                    "sent": "And we don't need to any integral or we don't need sample sampling.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some ongoing and future works.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                },
                {
                    "sent": "Our kernel basically was completely nonparametric, but still in some problems.",
                    "label": 0
                },
                {
                    "sent": "Some parts may be modeled nicely by parametric model, then the others may not.",
                    "label": 0
                },
                {
                    "sent": "In that case it is attractive to combine parametric approach with kernel nonparametric approach and actually we have preliminary results about the combination of.",
                    "label": 0
                },
                {
                    "sent": "Known dynamics and unknown nonparametric part in hidden Markov model exactly integration plus kernel nonparametrics or particle filter approach with kernel nonparametric.",
                    "label": 1
                },
                {
                    "sent": "Nonparametrics, but I'm not going into the details about this.",
                    "label": 0
                },
                {
                    "sent": "And also I skipped all the theoretical analysis in my talk about the theoretical analysis, especially for high dimensional data, is very important in comparison with other conventional approach nonparametric approach.",
                    "label": 0
                },
                {
                    "sent": "And also, well, I'm very interested into.",
                    "label": 1
                },
                {
                    "sent": "Interested to understand the link with other recent nonparametric approach such as Gaussian process or vision nonparametrics with our kernel vision approach.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just this talk is based on the collaboration with these guys and here's my last message.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}