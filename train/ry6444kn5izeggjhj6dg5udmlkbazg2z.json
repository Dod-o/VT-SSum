{
    "id": "ry6444kn5izeggjhj6dg5udmlkbazg2z",
    "title": "PAC-Bayesian Bounds and Aggregation",
    "info": {
        "author": [
            "Jean Yves Audibert, Center for Education and Research in Computer Science of the \u00c9cole des ponts, \u00c9cole des Ponts ParisTech, MINES ParisTech"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_audibert_pbba/",
    "segmentation": [
        [
            "I will talk about the user fact."
        ],
        [
            "Asian analysis to address the free statistical aggregation problems introduced by numerous key in his lecture notes inside in terms of prediction as.",
            "Suggest linear combination of these functions and we will see that dealing with these problems could be a way to address estimation in high dimension, which is a very popular topic nowadays."
        ],
        [
            "OK, so let me start with the context.",
            "It is usual one, so this is light switch just to set the notation essentially.",
            "So we observe an input output pairs.",
            "XIYIZ axis as input, YC output and a new input comes and the goal is to predict the corresponding output.",
            "Why OK and probably stick assumption is that the N + 1 pairs of input output pairs are independent and identically distributed according to some unknown distribution.",
            "OK, so this is completely standard."
        ],
        [
            "This is a usual statistical learning setting and we measure the loss of predicting Y prime instead of of Y by the function L applied to yny prime.",
            "And you know the typical losses are the square law.",
            "This is fairly standard.",
            "OK, let me mention."
        ],
        [
            "Briefly, as basic probability properties of the kullback.",
            "Leiber divergent.",
            "So first definition it just the expectation with respect to the first argument of the logarithm of the density of the first argument with respect to the second one when it exists.",
            "If there is no such density.",
            "Then so could back labor.",
            "Divergent is equal to plus Infinity.",
            "This callback library diversions can also be written as the expectation with respect to the second argument of some function of the density.",
            "And this thing function would be in this one, which has the nice property of being non negative and equal to 0 just for you equal 1.",
            "This implies that the kullback Leiber divergent is always non negative and it is equal to 0 if and only if.",
            "So two distribution identical.",
            "And they never property, which can be useful when we are interested in probabilities on Phoenix set is that circle back library of any distribution with respect to the uniform distribution and the finished set is always smaller than the logarithm of the Cardinal of this finished set.",
            "I believe this is quite."
        ],
        [
            "Well known so here is maybe the first piece of notation which is not fully standard.",
            "I will when we have a distribution on some set of prediction function distribution pipe.",
            "We define Pi indexed by H as a distribution which has a density with respect to this distribution which is proportional to exponential of H. So typical typical H here will be like minus the empirical risk.",
            "Timer constant for instance.",
            "And if you compute circle back, if you expand the definition of the callback library divergents with respect to this distribution in terms of circle back library versions with respect to pipe, then you just have this formula very easily just expanding the definition.",
            "And since we have seen that this quantity is non negative and equal to 0 for regal equal PIH, it means that this term.",
            "Is always non negative and is equal to 0 for a particular choice of Rome, meaning that we have that this supremum is equal to the local plus of H when when the distribution is just going to argument of the library.",
            "Could back library versions and the other properties at this supremum is achieved for role equal to this age distribution?",
            "OK.",
            "Please do not hesitate to stop me if there is some piece of notation which is unclear.",
            "OK, and this last property is just to say that indeed the kullback Leiber divergent system kind of measuring the distance between distribution.",
            "While it's an intuitive property.",
            "By H is is a distribution which tends to concentrate on the F for which each of F is large.",
            "So if you consider here.",
            "By Lambda each.",
            "When Lambda grows, you are concentrating more and more on the F4, which are close to these, so maximizing ones for each and and you have that this function as a function of Lambda is decreasing is increasing and decreasing."
        ],
        [
            "OK, John Dunford was asking yesterday for you with our back patient down to approach for me to it's away.",
            "It's it's about bounding.",
            "So risk of randomized estimators, so it will provide bounds on this type of quantity basically.",
            "Which is the expectation when F is drawn according to role of the risk of F. And.",
            "It is also abound, which will come from the use at some point of of this equality, which is the same as in the previous slide, except that I have replaced each of F, But what is usually used, that is distance, some kind of dissimilarity between the empirical risk and that risk.",
            "So typically here this D will be the absolute value of the difference.",
            "Also square of the difference, or so goodbye in case.",
            "In the case of classification, it will be there.",
            "Direct labor divergent between the Bernoulli distribution of parameter and smaller.",
            "So to compare with the traditional statistical learning theory, which is based on supremum of empirical processes.",
            "Well, the difference is that instead of having this kind of bound for any distribution, the standard approach will give you this kind of bounds which hold for any function F. So if you want to go from this to this, it's simple, just takes the expectation with respect to row on both side, and you have this kind of bounds, and if you want to go from this to this.",
            "What you would like to do is take rule as a direct distribution at some point and you will obtain these bounds.",
            "So in principle it is very similar, but it is not at all the case because both ways don't don't lead to the same thing if you take the expectation with respect to rule here.",
            "You won't get this kind of pack, Bashan bounce, because you will have a term here which is linear in row, whereas circuit back library versions which appears in background is not at all dinner, and reciprocally.",
            "Inversely, if you want to go from this to this, taking row equal to the direct distribution won't work because well, at least when you have some kind of well spread prior distribution, because this callback library.",
            "Tim will diverge for Royco direct distribution at the point and the prior distribution, which is quite well spread, which does not put mass on any particular function."
        ],
        [
            "So it is disseminated this these bounds essentially dissimilar because of the comeback Libra term.",
            "And the interest of.",
            "Of pack Bayesian approach is to have a measure of complexity which is quite different from the usual viewpoint, which is expressed in terms of the kullback.",
            "Leiber divergent with respect to some prior and this prior as has been already said yesterday, is not at all representing some kind of belief on the way the data has been generated, so it's not at all.",
            "Coming from some kind of valuation approach.",
            "And the bounds also holds for any prior and any posterior distribution, so this is also a difference.",
            "OK, let me.",
            "C as basic, different present basic different bounds and emphasizing on the similarity on this bounds.",
            "So for this part of the talk will consider that the losses are between zero and one.",
            "Some of the bounds holds for any setting such that we have this condition and some of our bounds will hold only for the classification setting.",
            "But the basic bound from the David Blacklisters Holdin for any are bounded loss, and it states that for any prior distribution, we probably for any confidence level with probability at least one minus epsilon.",
            "We have that for any distribution, roll the absolute value of the difference between the average risk and the average empirical risk is bounded by 1 / sqrt N terms when the multiplicative factor.",
            "Makes it appear so cool back library versions.",
            "And equivalently to this property which hold for any row.",
            "We can say that for any data dependent distribution rule hat, so any posterior distribution, if you prefer we have with probability at least one minus epsilon, this this property.",
            "So note the differences.",
            "Is that we are here considering data dependent distribution and here we just say that it's holds simultaneously for any distribution.",
            "But both statements are equivalent because if you want to prove the second one from the first one, what you say is that the difference between the left hand side and the right hand side.",
            "Is smaller than the Super mom of a rule of the difference between system and system and the first statement says that this supremum of the difference is smaller than zero, so you get the second one this way and to get the first one from the second one, it suffices to take if you put the measurability program, decides to take the hat rule which realized the supremum of the difference between system and system."
        ],
        [
            "So both statements are equivalent.",
            "So in the pioneering work of Macalester is the proof was rather lengthy and Fortunately Matthias Seeger provides a shorter proof, and I will review it.",
            "Well, with my own words, so I will emphasize on on this lemma, which is which says that if you want to have a background so not only pack Bayesian bounds, just you want to have the background.",
            "So basically you want that for some random variable with probability at least one minus epsilon.",
            "You are below.",
            "You know that this random variable is below log epsilon minus one.",
            "And to prove this kind of bounds, the only thing you have to check is to prove that Laplace transform of your valuable is smaller than one.",
            "And so this is a trivial limit is just Markov's inequality written in the way we want.",
            "So if you want to prove.",
            "Invasion bounder I would present into first parts is the only thing you have to do is show what is a random variable V. So if we want to prove this we want to isolate the log epsilon minus one terms.",
            "So you just take the square multiply by two N -- 1.",
            "Put this kickback Libra turn on the left hand side.",
            "Put the log 4 and terms in the left hand side and you have your random variable.",
            "So explicitly your random variable is this.",
            "This and you want that hold for any rules and so your random variable is the supermom of a role, so distribution.",
            "Of this quantity.",
            "So since we want to prove this, the only thing we have to prove is at the expense as expectation of exponential V is below 1.",
            "So let's first.",
            "Notes that.",
            "OK, we want to make appear the legend transform of the comeback library versions.",
            "So we want to have a term which has the form expectation with respect to rule of some quantity minus a callback library term.",
            "So to do this, we see that we want to put the square inside where we want to put stories expectation outside, and this is done by Jensen's inequality.",
            "So you have that V is trivially from Jensen Inequality's models and system, and then you can compute exactly while you have a closed form formula for this problem, which is.",
            "Which is this quantity minus minus system which is a constant?",
            "So in fact, you can view this presentation of the proof as completely similar to.",
            "To the work of Francois and Vascular Gemma.",
            "But it's OK, it's just a way of stating it a bit differently because I'm putting the emphasis on this lemma.",
            "So for now the we have used this Markov property and Jensen inequality."
        ],
        [
            "Kim.",
            "And then so OK."
        ],
        [
            "No, I think it is the expenditure of system and the expectation when you take the exponential, you will have one of the four N. Times the expectation with respect to the distribution having generated the data.",
            "So you're training data of this expectation here.",
            "So this is what I."
        ],
        [
            "It's written in this slide.",
            "And then what you do is just use should be need to interpret inverted.",
            "So to sign some signs.",
            "And you add 1 here and subtract 1 here just to make appear non negative random variable while you want to have a non negative random variable.",
            "It's just because it's the expectation of a non negative random variable is equal to the integral.",
            "That's the probability of this random variable exceeds some threshold.",
            "So it's not that these are qualities and then you can rewrite this event as just being the absolute value of the difference between the true risk and the empirical risk is above this threshold.",
            "This is just rewriting differently, so same event.",
            "So it's still an inequality, and at this point this is 1/3 way place where we have an inequality.",
            "We use everything inequality.",
            "So because here we are in in known grounds this is sum of IID random variables.",
            "So we can apply Deans inequality.",
            "And obtain this quantity and then we just compute this integral which is we have a closed form for it.",
            "We end up with form four N -- 1 / 4 N which we say it's below 1.",
            "So we have proved that the random variable V has Laplace transform which is below 1.",
            "So we have proved the PAC Bayesian bounds.",
            "So this is a."
        ],
        [
            "So this was the proof of Mcallister's inequality and.",
            "At the beginning of the patient approach, but was stand up to do is take your empirical bound on your average risk and minimize it.",
            "It seems that since then we have a bit depart from this approach to the extent that we are more and more trying to not to minimize it, but to find.",
            "Prior and posterior distribution for which we have a closed form expression from for the callback library versions.",
            "So if you want to, if you try to minimize this, you realize that the distribution which minimizes the empirical bound has this form, so it's Gibbs distribution with energy as an empirical risk and invest temperature parameter Lambda, where this Lambda you can see it, which is essentially between square root of N&N.",
            "Lambda satisfies this fixed point equation.",
            "Or you can also see it as visualizing the minimum of this quantity.",
            "So as you see Lambda.",
            "He's depending under an end, so confidence level and also some kind of geometry of the problem.",
            "'cause this is some kind of quite complex quantity.",
            "But this is just a real parameter to tune."
        ],
        [
            "OK, now I will present a serious bone which holds into classification setting.",
            "So I will denote this way the callback Leiber divergent between the Bernoulli distribution of parameter P and then re distribution of parameter Q and the Seagulls bound can be stated.",
            "This way it's just upper bounding circle back library vergence between the boundary distribution of parameter is the average empirical risk and parameters have rich true risk by quantity which has the same flavor as in the previous slide.",
            "That is a callback library vergence divided by N. And to prove this, it is exactly the same if we want to prove this, we've isolates your log epsilon minus one term, so we multiply by N, puts a callback library divergent, and the left hand side puts the log 2 sqrt N on the left hand side so that we have a random variable."
        ],
        [
            "Revealed so this is how under variable V we want to do.",
            "To have effect bound.",
            "So we want to control this by one.",
            "So we just write what it is and what it is to be noted here is that.",
            "OK, since the kullback Leiber divergent is convex, you can use Jensen inequality once more, as in the previous proof, and as in the general proof of Pascal and Francois encoder.",
            "You use Jensen inequality pushes the expectation outside and then you are in known ground because you know that this Supreme could you have across form for the expression for this supremum.",
            "So you make appear this on TT once more.",
            "You use the beanie to interpret the two and you compute explicitly so it is a simple computation to compute explicitly.",
            "This expectation of this exponential of circle back library versions.",
            "So you end up with this formula.",
            "And then you have to use stealing the approximation to have a quite tight bound on this quantity.",
            "And This is why you have the 2 sqrt N. So."
        ],
        [
            "This is the proof of signal spell.",
            "Set up.",
            "So now if we want to compare both bounds, the first thing is that if you think of Pinsker inequality, you see right away that they really have a well, at least from.",
            "From this one you can deduce abound, which is really similar to the first one.",
            "Just taking the square root and both sides and the things I said square root is above this quantity, but it would miss the advantage of.",
            "Of Seagulls bound to do this too applies in this Pinsker inequality.",
            "What is better to do is to understand that when the kullback Leiber divergent between 2 numbers well.",
            "I know you have parameters.",
            "These two numbers is below some threshold then.",
            "You can deduce from this this property that we have here, so it needs some computation.",
            "And this property is really of the same form as this one, because here you have a term which is at most 1/4 because it is X * 1 -- X.",
            "So and this is so cool back library term.",
            "So you exactly recover the same form as here plus an additional term.",
            "But this term is of order one of and so this is some kind of 2nd order term which will jump in only when.",
            "You have very low empirical risk, so this is maybe a more explicit way of seeing the stickers bound.",
            "OK, I will mention here.",
            "So first back by Asian bounds of Olivia Katani which which is intercept."
        ],
        [
            "Election notes, so this is not the one that was mentioned yesterday into talks, so it the difference between the previous bound is that you have a free parameter Lambda.",
            "80 and it says that we find probability you have the average risk which is bounded by the empirical risk.",
            "Which has a factor which is.",
            "1 /, 1 minus Lambda over.",
            "In this side function.",
            "So say function is justice.",
            "When reading this bound you can.",
            "Assume that Lambda is small with respect to N and this this term is just one half essentially.",
            "And you have the comeback Liebert also, so it's not clear how to compare this bound to previous bound, but a way to do this is to remember that the well not to remember.",
            "You can see a posteriori.",
            "That's the typical value of Lambda between square root of N&N, so that here the 1 / 1 minus Lambda over to enter you can see it as one plus Lambda over 2 N. So this term becomes system.",
            "Approximately by taking a.",
            "Just a Taylor expansion and Zephyr terms is of order this quantity, and if you optimize your choice of Lambda.",
            "You will see exactly the term.",
            "Well, not exactly, but similar term appearing here, which is the callback library versions divided by N. With some factor here, but this factor was also in the previous slide.",
            "Up to this additional factor, but if you think of low empirical risk and distribution, row concentrating and low empirical risk, this is close to one this.",
            "So the expression here is really similar.",
            "OK, yeah.",
            "Kelso, because you can't convex right, so you can know about it again by maximizing your functions.",
            "Somehow it's the slope or something.",
            "Lower bounding it again, putting in this parimeter.",
            "Linearize this.",
            "And if you shift the lumber where it's the right place on it, but it's at the tourist small, then you be at the right point.",
            "If it's kind of 1/2 and you had nothing point.",
            "I don't, I don't know, certainly.",
            "Yesterday.",
            "We consider here.",
            "I think.",
            "As the empirical risk goes to 0, this quantity appears to go to zero.",
            "OK, yeah, so This is why here you have this and I don't have room on my side, but you always have the 2nd order term, which is a term which was appearing here."
        ],
        [
            "So indeed, this is not rigorous bound to."
        ],
        [
            "Have it rigorous.",
            "You have to do first.",
            "You have to optimize over Lambda so you have to take agreed over Lambda and user Union bound.",
            "This is doable.",
            "And indeed, when you do properly some minimization, you will have a term which is of order callback library version divided by N which will appear so."
        ],
        [
            "Cricket.",
            "So the bound, which is straightforward extension of the previous one when you are interested not in classification but in more general losses, is this one.",
            "So it has a similar form, except that except that you have these variants which appears, so this term is just variance with respect to the pair XY of the loss.",
            "And then you take the expectation with respect to rule of this variance.",
            "The link to the previous one is just to say that for classification, the variance of loss is bounded by the risk the true risk.",
            "So the term which appears you can put it with the left hand side, because this is a similar term and This is why this found.",
            "It's exactly the same reasoning which gives this one."
        ],
        [
            "In which this quantity was appearing, and this."
        ],
        [
            "One in which the quantity was appearing.",
            "Because this time you put it on the left hand side.",
            "OK."
        ],
        [
            "And finally bound, which is not, I think, not very popular, but which is also of interest is a songs bound.",
            "Which is in a form which is maybe not so nice, because here what is control it is not the average risk.",
            "It's some kind of average with respect to rule of some complex.",
            "City.",
            "But it's right inside the easy as usual, well has the usual form I would say, but we understand that we recover, stormed out of bounds or something similar.",
            "We can do a Taylor expansion of this term when Lambda over any small and precisely we have this thing.",
            "And we note that here what was in system was two things.",
            "The average of the risk with respect to roll.",
            "And civilian stem, which was appearing in the previous slide.",
            "So once more.",
            "But it appears it's a sexpectations with respect to the posterior.",
            "Of the variance of the loss.",
            "Up to 2nd order term.",
            "So you really have a similar bound.",
            "Anne."
        ],
        [
            "Is about which was cited yesterday.",
            "Was this the bound?",
            "Which can be derived from songs bound by instead of using this formula, you can use the expressive value of the log path for binary distribution.",
            "So you can compute it exactly and then it makes appears this this function and if you do a Taylor expansion of this function, you will see that says some kind of variance term hide hidden in it.",
            "So this is essentially so basically."
        ],
        [
            "The point here is that we recover a similar bound, so if you take things bound and apply.",
            "Exactly low class formula for the binary distribution in the classification setting you recover Catanese bound to bound.",
            "I used during my PhD was essentially of the same form.",
            "After optimization of the parameter Lambda with constant bit less less good because we were not taking advantage of the classification setting of the explicit formula of the.",
            "Lab tests are of solace.",
            "And and it is similar to the also to the first bound of Olivia category in the classification setting, because once more since we are interested in function, we flow through risk.",
            "This is not a big difference between system and system.",
            "And it is not also a big difference between this term.",
            "System where the difference is mainly in the position of this this quantity.",
            "OK, one of the advantage of cigarettes Brown is that it is really expressed like this, whereas these ones.",
            "We have done some kind of tax pension, so if we wanted to write it properly what will happen?",
            "What will appear?",
            "Is this kind of terms so going to order term and here you will have a login instead of log epsilon minus one.",
            "You will have log of log in minus epsilon minus one.",
            "So at the end it will be really similar if we do properly the computation.",
            "So the bounds could be considered very similar for this.",
            "For this reason, not the same, exactly the same constantly appears in the square root of N terms."
        ],
        [
            "OK, so now I will switch to something which is less toilets, more research talk about, least square regression and how to aggregate production function in this setting.",
            "So when can see a motivation for this program can be if you are given some estimators.",
            "It's a G1G D and it comes from data that you you don't have and you want to predict as well as the best of these D estimators.",
            "You can use.",
            "And the whole story is to predict as well as suggest of this D functions.",
            "Auto predict as well as the best convex combination of this function or to predict as well as the best dinner combination of this function.",
            "We are in the OK.",
            "I should specify that I will consider, for simplicity abounding nice setting so they'll put are in minus one one.",
            "An trivially target, which is to some more difficult would be is to perform as well as this one, so the risk of this target function is smaller than the risk of this target function, which is smaller than the risk of this target function.",
            "But in terms of estimation, it should be simpler.",
            "Well, it would be simpler to predict as well as this one bit more difficult to predict as well as this one and a bit more difficult to predict as well as this one."
        ],
        [
            "What is known for from different works?",
            "Is that is the expected excess there exist algorithms for each of the three tasks?",
            "Suggest he expected excess risk is smaller essentially says unlock Dover N for get about a minimum.",
            "This is for model selection type of aggregation for the convex aggregation this is not.",
            "We cannot reach this estimation rate.",
            "What is rich is.",
            "Rate, which is essentially square root of log D / N when D is much larger than square root of N. Yeah, and when D is much smaller than square root of N, we can go a bit faster than this 1 / sqrt N terms and reach the 1 / N rate, but with a factor here, which is D&D instead of log D for model selection.",
            "So naturally the speed's learning rate is a bit worse, whereas for linear.",
            "Interlinear aggregation setting.",
            "So convergence rate is of the D of N. If you choose, well, your algorithm.",
            "Unfortunately for the slice last point.",
            "The estimator needs to know the input distribution, so this is something we will address in the last part of the talk.",
            "So I will go over the free problems and talk about the algorithms which allows to perform this kind of rates.",
            "So first let me be OK. Is the previous slide was saying there exist algorithms such that we reach this learning rates and we know that these are tight to the extent that we have exactly the corresponding lower bounds?",
            "Thanks to see back off.",
            "So in a mini Max."
        ],
        [
            "So if you consider.",
            "A set of probability distribution like this.",
            "So you you consider that so why are generated this way with Goshen noise?",
            "Then you know that whatever estimator you take, you know that the Supreme of the expected excess risk will be for the model section.",
            "Task would be always greater than this quantity, so logged over N is the learning rate for this problem.",
            "Similarly, square root of log D / N is essentially so learning rate for this task when D is much much bigger than square root of N. And if D is much smaller than square root of N, you can reach 1 / N rate.",
            "And for dinner, aggregation is a mini.",
            "Max rate is really the over and so these are the lower bounds corresponding to previous upper bounds.",
            "Yeah, see.",
            "Oceans on the input space and are where the VR component is normal.",
            "Sorry this is.",
            "Set up for Verity on XY, so consider.",
            "What is the why?",
            "The Why is given by some fixed function?",
            "Yes yes yeah yeah.",
            "I mean for me you can see.",
            "Also pair XY, which are generated by this model when G has this property and this is a set of probability distribution.",
            "So I will first consider the model section and detail what Happ."
        ],
        [
            "Intermodal selection elevation.",
            "So the target is to predict as well as this.",
            "Function is a very surprising phenomenon for this problem, because to be optimal you need to choose your estimator cannot stay cannot output just one of these function.",
            "If it does this think of empirical risk minimization.",
            "For instance, it will output a function which is always in this set.",
            "Think of cross validation.",
            "Same thing.",
            "Will output a function which is always in this set.",
            "But all this is.",
            "Estimator pool to the extent that they are learning rate is of order 1 / sqrt N. Whereas we know from the previous slides that's a correct rate.",
            "Is 1 / N times log D. But if you look at the dependencies in N you have, you will lose a lot by staying in your model.",
            "The second thing which is surprising is that up to recently is the estimator, which was performing so logged over N rate.",
            "With some kind of tricky algorithm called the progressive Nature rule.",
            "Which does not come from the usual statistical running for re up also packed well.",
            "It has a language pack version approach but not in the way we use it.",
            "But and finally, scissor proof was not at all based on the pack Bayesian analysis or Supermom of America."
        ],
        [
            "Processes so this and this algorithm.",
            "So let me describe this algorithm.",
            "So this is a progressive nature rule.",
            "You start with the uniform distribution on this fine, it's finite set of function.",
            "You have a parameter, Lambda positive and you consider the cumulative loss on the 1st I points.",
            "So if it was in here it will be end times empirical risk this quantity.",
            "But here you just consider the cumulative last.",
            "And the first high points of your training data.",
            "And you consider the average of the function G when G is drawn according to the Gibbs distribution.",
            "Link to this cumulative loss so.",
            "This distribution is the distribution which has, with respect to the uniform distribution density, which is proportional to exponential minus Lambda Sigma I.",
            "So it concentrates on functions which has low low cumulative loss up to time.",
            "I.",
            "If you want well, I should not say this because we know we have all the data right right away.",
            "It's not online learning.",
            "But we consider this kind of distribution for I going from zero to N. So for I = 0, this is just the prior distribution.",
            "This is just a uniform distribution and we take a seasonal mean.",
            "Of this bridge.",
            "With respect to I so this is quite surprising, because when would they expect that uneasy last one is important only for I equal N. We have some kind of good estimation because this is concentrating on low empirical risk function.",
            "This distribution for I equal to 2 N, so we would expect that just just this quantity for I equal any sufficient.",
            "But we don't know any result for this.",
            "This estimator, so when we know is for this progressive material which is, this is Armin, so another way of presenting this estimator is this one.",
            "And the theoretical grantee of this estimator is that the expected excess risk is below.",
            "Sorry, so.",
            "Yeah, for Lambda, equal for Lambda equals some particular value.",
            "In fact, 1/8, so for Lambda equal 1 eighth we have this property.",
            "So we recover this log deveren rate.",
            "Yeah, sorry.",
            "And this is kind of the Gaussian likelihood.",
            "Yeah.",
            "Would you have the syndrome in if you use the Bashan estimate, I don't think.",
            "Expectational posterior.",
            "I don't know.",
            "I would need to do the computations.",
            "Las Vegas wouldn't give, I mean, it sort of gives more emphasis to the early points.",
            "Is an arbitrary or yeah.",
            "Yeah, that's true, yeah?",
            "Also, you write.",
            "Previous points, you can always write it by changing with Lester.",
            "So if you have it, you end up with something that doesn't make any difference between your.",
            "Yeah, this is not at all exchangeable.",
            "Got any plans it should give you the same thing.",
            "You know this does not.",
            "Yeah OK yeah.",
            "Need to figure out is in is another trigger.",
            "Because our eyes thing.",
            "Expect.",
            "Yeah, yeah, yeah, the idea comes from Andrew Barron, but this estimator was in your work and so one of young young.",
            "Paper paper Andrew Barron said this is for the technical trick of.",
            "You think it's icy 1987 or something like this?"
        ],
        [
            "So one of them.",
            "OK. Let me present another algorithm which is a bit more sophisticated, but for which we have a tighter tighter bound and which comes from a sequential analysis from the work of both and vomit.",
            "Kevin and I think I'm forgetting 104.",
            "And in which state that we can?",
            "So we still have a Lambda parameter.",
            "And for any I we consider prediction function which satisfies this property.",
            "So it's not clear what it is doing, so for a moment, let's, let's consider that we are allowed to put this expectation inside here.",
            "If we are allowed to do this, it would mean that the log exponential answer minus Lambda would collapse.",
            "So so this prediction function, which essentially some prediction function which predict better.",
            "Then this one for any XY.",
            "Naturally it's not exactly this, but this is just to emphasis that this inequality that we ask for is something guaranteeing that this quantity is more.",
            "And we use this H hat function by using a scissor mean.",
            "So it's a bit similar to the previous estimator, an in fact by using some kind of X concavity argument.",
            "You can check that this edge edge hat.",
            "You can take it equal to the the average according to \u03a0 minus Lambda Sigma I.",
            "If you consider Lambda which is smaller than one over 8th.",
            "So this function does exist for Lambda smaller than one over 8th, but it also exists when you take Lambda equal to one of the two, but then it has not simple explicit form.",
            "But it has the advantage that for this particular value, the resulting estimator will satisfy this property, which is 4 times better than the previous one.",
            "No."
        ],
        [
            "Zeze drawback that drawback to this to both of these estimator, which is the following.",
            "The excess risk.",
            "Well, we know that it is expected accessories is of order 1 / N, and we would expect that with high probability.",
            "We have this success rates which is of order 1 / N This does not hold.",
            "And we have a very simple example.",
            "Even with D equal two and two constant functions, you can prove that for some confidence level you will have a lower bound on this excess risk, meaning that you do not have this 1 / N deviation and you have to suffer some kind of one over square root of indication.",
            "So you don't have exponential concentration here.",
            "The latter.",
            "That are very very small.",
            "Lot of event.",
            "Yeah, and you say that you don't have with high, probably yeah, in your family there's a lot for which it's almost zero.",
            "Yeah, it's a way of stating, yeah, but.",
            "Yeah.",
            "Camping go outside from the original model.",
            "Concentrate.",
            "And so this is my non paid version side we."
        ],
        [
            "Present.",
            "An algorithm which has the 1 / N division.",
            "So the algorithm is very simple.",
            "So if the whiteboard is my set.",
            "So prediction function set space plus right and if you have this is your your function, let's say.",
            "So this is my prediction function.",
            "So the estimator first.",
            "Look for the empirical risk minimizer among these D functions.",
            "So assume that this is the empirical risk minimizer, and then it consider all the functions that are convex combination of symmetrical rich minimizer and the other functions.",
            "So we consider all the functions which are.",
            "In this three segments.",
            "For the full four and we we minimize empirical risk on this set with his some kind of star shaped set function and just by doing this.",
            "So the nice thing here is there is no parameter.",
            "We have that with probability at least one minus epsilon.",
            "The risk of the excess risk of this estimator is below.",
            "Well, some constant times logged over in with her.",
            "Well, where is the confidence of appears in the logarithmic terms?",
            "So this means that the two algorithm which we have seen progressive mature rule and progressive interact mature rule have indeed suboptimal deviation to the extent that there does exist an algorithm which for which we have the one of deviations.",
            "Yep.",
            "Do this mixture, yeah.",
            "Crazy lesson.",
            "So we have way of doing the mixture.",
            "He will have his feeling that if you average and even ovarian.",
            "They didn't actually officer.",
            "Yeah, to some extent.",
            "Look at I there is a different kind of regularization here because we are just considering this kind of functions.",
            "Number.",
            "So if you are in the middle.",
            "Yeah.",
            "I love seeing this is that if you look at the output of your progressive material it will always be in the in the convex of your function.",
            "So you have to pay for being maybe in a larger model then the algorithm here at the end it will you know before before running it that it will be on one of these segments.",
            "One of the segments which delimits the convex Hull because we are just considering this kind of segment.",
            "So we should not pay.",
            "This is some kind of regularising.",
            "We rest fixes this set of possible output of the algorithms, so definitely it should do some regularization.",
            "Soon.",
            "And.",
            "You have two functions with.",
            "Almost nothing with lost, and when the difference between the two losses of order 1 / sqrt N. Patience is the empirical risk for the two functions and then did the greater the usual gifts estimated may concentrate on those under the wrong one.",
            "All right here you you draw the line between the two.",
            "To do pairwise.",
            "And you have longer to prepare like yeah, yeah.",
            "Try to go by triples.",
            "No, because I mean so target for me was rich.",
            "That is managing this but.",
            "At.",
            "The smallest extension of the model in which you can work.",
            "Number two simplex.",
            "It might be better to do the liar.",
            "Then you should not compare yourself to this one.",
            "You should compare to some kind of other target, because you will pay for going in a more complex set of function so.",
            "Yeah.",
            "Original function.",
            "As well as the best.",
            "Yeah.",
            "OK."
        ],
        [
            "Let me go.",
            "I think I'm not having much time left, so let me talk briefly of the.",
            "So convex aggregation program.",
            "So now we want to predict as well as specs the best convex combination of the initial function.",
            "And we consider the case when the dimension is much larger than square root of N, because 4D which is smaller than square root of N as the rate is D / N which is similar to the linear aggregation setting.",
            "So if we know how to solve the linear aggregation setting, then we solve the convex segregation problem under same time, which minimax optimal rate.",
            "So in this setting, what we expect is an algorithm which has a rate which is square root of log D / N and the algorithm which managed to do this.",
            "There are several possibilities.",
            "The first one is to apply the Progressive Nature rule on an appropriate grid.",
            "Also previous algorithm.",
            "The second one is to use the exponential shifted gradient algorithm.",
            "Oh, and for the first one is to use some kind of stochastic version of the mirror dissent algorithm.",
            "But all these algorithm results in expectation.",
            "So we control the expected excess risk and at the deviation, and they're all based on sequential procedure.",
            "And what have?"
        ],
        [
            "Friends here is very nice because it is the setting in which bag beige and furry applies perfectly.",
            "Because contrary to the classifications setting in which you always have this problem between the risk of the average.",
            "As a risk of the average, sorry.",
            "Risk of this quantity.",
            "I don't remember the notation of the Gibbs risk you OK this gives risks is.",
            "What's?",
            "What was used is that this gives.",
            "This was below twice the risk of the, so this is the risk of the majority vote is below twice it keeps Gibbs risk, so we can use subversion analysis to control this bridge.",
            "This average some kind of risk in the product space and by using directly the pack Bayesian bounds we have seen in the first part of the talk.",
            "And by using this time I really do so.",
            "So Union bound of Lambda.",
            "You you get this kind of bounds.",
            "Which is which is essentially empirical.",
            "Up to this term, but this term you can lower bound this empirical risk by is infima of the risk on your convex set.",
            "By minimizing the bound."
        ],
        [
            "What you will obtain is an algorithm whose posterior distribution is diluted will had see and for which the excess risk is controlled by this term.",
            "So you recover the log the square root of log over N terms, and you have logged over enter here.",
            "And the nice thing is that in the worst case, this is of order square root of log D / N. So this is a minimax optimal rate and in the case where the best convex combinations happen to be one of the initial function, this this variance is then equal to 0.",
            "So this query term disappears and recovers logged over an rate of the.",
            "Model selection type aggregation so we have some kind of adaptivity of the natural pack version estimator."
        ],
        [
            "So I think I won't have time to detail this and Olivia Keitany will talk about aggregation in Salina aggregation setting, but what can be said is that in this setting.",
            "There is no simple deal for unbound which exists.",
            "Phase one from Boeotian Mesa, which has.",
            "Look in terms, but if we care about Logan says no simple down.",
            "And we provide in this work."
        ],
        [
            "Found which has this deal for N rate and the algorithm is simple, so we are considering linear aggregation when we know we know we sorry we know that the coefficient of linear aggregation is in some bounded sets.",
            "We took the prior distribution which is uniform and is bounded set and we consider the Gibbs estimators.",
            "This is a.",
            "And this algorithm has the van down as we wanted to to obtain.",
            "So this is and here there is clearly a shrinking effect of this distribution, because if you think of if my space of.",
            "Coefficient like this if the minimizer of the empirical risk is here using this kind of estimator, will.",
            "Will put you more on the inside of your bounded set of coefficient, so there is a shrinking effect due to the Gibbs distribution.",
            "So."
        ],
        [
            "I don't know.",
            "Maybe let me just quickly talk about this, because this this is linked to the talk of PayPal key.",
            "It's what happens in very high dimension, meaning D much greater than.",
            "In in this case we have seen from the learning rates that we can predict as well as the best convex combination, but we cannot predict as well as the best linear combination because the over N would be larger than one, so we have no guarantee and natural target is to predict as well as the best linear combinations when the number of non zero coefficient into linear combination is below some threshold.",
            "So it is now very well known that too.",
            "Predict as well as just a when way is to use lasso.",
            "That is L1 regularization, but the bound success rates bound which has.",
            "So as of the end times, log rate happens occurs only under strong assumption under correlation of the of the function we need this function to be quite uncredited."
        ],
        [
            "And what is interesting is that if you use the model selection approach, you will get rid of this assumption.",
            "And the model section approaches just the following algorithm.",
            "You have your training set.",
            "You could take cut it into two parts.",
            "On the 1st part you train.",
            "We'll see estimators corresponding to the different set of nonzero pattern.",
            "So you consider all this subset of size S. And you use your so keep the estimator.",
            "I have presented in the previous slide.",
            "Two linear agree gate only this function.",
            "So the basis function corresponding to the subset I.",
            "So this is done on the first part of your training set and on the second part of your training set you use the empirical star estimator to select the best of these ones and you can very easily from the previous result just by combining them by Union bound you prove that the excess risk of the resulting estimator.",
            "Have this property which is you have running rate which is S the size of the non zero pattern over N times.",
            "Log log essentially.",
            "And this is without strong assumption and very easily.",
            "So there is an open problem here, which for me is of great interest is, is there a way to achieve this kind of bound with a computationally efficient estimator?",
            "Because this estimator you don't want to implement it because you would need to consider all the subset of size S of your.",
            "Set of basis function.",
            "And I think your talk will partly address this problem and OK, I will end up here.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will talk about the user fact.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian analysis to address the free statistical aggregation problems introduced by numerous key in his lecture notes inside in terms of prediction as.",
                    "label": 0
                },
                {
                    "sent": "Suggest linear combination of these functions and we will see that dealing with these problems could be a way to address estimation in high dimension, which is a very popular topic nowadays.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me start with the context.",
                    "label": 0
                },
                {
                    "sent": "It is usual one, so this is light switch just to set the notation essentially.",
                    "label": 0
                },
                {
                    "sent": "So we observe an input output pairs.",
                    "label": 0
                },
                {
                    "sent": "XIYIZ axis as input, YC output and a new input comes and the goal is to predict the corresponding output.",
                    "label": 1
                },
                {
                    "sent": "Why OK and probably stick assumption is that the N + 1 pairs of input output pairs are independent and identically distributed according to some unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is completely standard.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a usual statistical learning setting and we measure the loss of predicting Y prime instead of of Y by the function L applied to yny prime.",
                    "label": 0
                },
                {
                    "sent": "And you know the typical losses are the square law.",
                    "label": 1
                },
                {
                    "sent": "This is fairly standard.",
                    "label": 0
                },
                {
                    "sent": "OK, let me mention.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Briefly, as basic probability properties of the kullback.",
                    "label": 0
                },
                {
                    "sent": "Leiber divergent.",
                    "label": 0
                },
                {
                    "sent": "So first definition it just the expectation with respect to the first argument of the logarithm of the density of the first argument with respect to the second one when it exists.",
                    "label": 0
                },
                {
                    "sent": "If there is no such density.",
                    "label": 0
                },
                {
                    "sent": "Then so could back labor.",
                    "label": 0
                },
                {
                    "sent": "Divergent is equal to plus Infinity.",
                    "label": 0
                },
                {
                    "sent": "This callback library diversions can also be written as the expectation with respect to the second argument of some function of the density.",
                    "label": 0
                },
                {
                    "sent": "And this thing function would be in this one, which has the nice property of being non negative and equal to 0 just for you equal 1.",
                    "label": 0
                },
                {
                    "sent": "This implies that the kullback Leiber divergent is always non negative and it is equal to 0 if and only if.",
                    "label": 0
                },
                {
                    "sent": "So two distribution identical.",
                    "label": 0
                },
                {
                    "sent": "And they never property, which can be useful when we are interested in probabilities on Phoenix set is that circle back library of any distribution with respect to the uniform distribution and the finished set is always smaller than the logarithm of the Cardinal of this finished set.",
                    "label": 0
                },
                {
                    "sent": "I believe this is quite.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well known so here is maybe the first piece of notation which is not fully standard.",
                    "label": 0
                },
                {
                    "sent": "I will when we have a distribution on some set of prediction function distribution pipe.",
                    "label": 0
                },
                {
                    "sent": "We define Pi indexed by H as a distribution which has a density with respect to this distribution which is proportional to exponential of H. So typical typical H here will be like minus the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Timer constant for instance.",
                    "label": 0
                },
                {
                    "sent": "And if you compute circle back, if you expand the definition of the callback library divergents with respect to this distribution in terms of circle back library versions with respect to pipe, then you just have this formula very easily just expanding the definition.",
                    "label": 0
                },
                {
                    "sent": "And since we have seen that this quantity is non negative and equal to 0 for regal equal PIH, it means that this term.",
                    "label": 0
                },
                {
                    "sent": "Is always non negative and is equal to 0 for a particular choice of Rome, meaning that we have that this supremum is equal to the local plus of H when when the distribution is just going to argument of the library.",
                    "label": 0
                },
                {
                    "sent": "Could back library versions and the other properties at this supremum is achieved for role equal to this age distribution?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Please do not hesitate to stop me if there is some piece of notation which is unclear.",
                    "label": 0
                },
                {
                    "sent": "OK, and this last property is just to say that indeed the kullback Leiber divergent system kind of measuring the distance between distribution.",
                    "label": 0
                },
                {
                    "sent": "While it's an intuitive property.",
                    "label": 0
                },
                {
                    "sent": "By H is is a distribution which tends to concentrate on the F for which each of F is large.",
                    "label": 0
                },
                {
                    "sent": "So if you consider here.",
                    "label": 0
                },
                {
                    "sent": "By Lambda each.",
                    "label": 0
                },
                {
                    "sent": "When Lambda grows, you are concentrating more and more on the F4, which are close to these, so maximizing ones for each and and you have that this function as a function of Lambda is decreasing is increasing and decreasing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, John Dunford was asking yesterday for you with our back patient down to approach for me to it's away.",
                    "label": 0
                },
                {
                    "sent": "It's it's about bounding.",
                    "label": 0
                },
                {
                    "sent": "So risk of randomized estimators, so it will provide bounds on this type of quantity basically.",
                    "label": 0
                },
                {
                    "sent": "Which is the expectation when F is drawn according to role of the risk of F. And.",
                    "label": 0
                },
                {
                    "sent": "It is also abound, which will come from the use at some point of of this equality, which is the same as in the previous slide, except that I have replaced each of F, But what is usually used, that is distance, some kind of dissimilarity between the empirical risk and that risk.",
                    "label": 1
                },
                {
                    "sent": "So typically here this D will be the absolute value of the difference.",
                    "label": 1
                },
                {
                    "sent": "Also square of the difference, or so goodbye in case.",
                    "label": 0
                },
                {
                    "sent": "In the case of classification, it will be there.",
                    "label": 0
                },
                {
                    "sent": "Direct labor divergent between the Bernoulli distribution of parameter and smaller.",
                    "label": 0
                },
                {
                    "sent": "So to compare with the traditional statistical learning theory, which is based on supremum of empirical processes.",
                    "label": 0
                },
                {
                    "sent": "Well, the difference is that instead of having this kind of bound for any distribution, the standard approach will give you this kind of bounds which hold for any function F. So if you want to go from this to this, it's simple, just takes the expectation with respect to row on both side, and you have this kind of bounds, and if you want to go from this to this.",
                    "label": 0
                },
                {
                    "sent": "What you would like to do is take rule as a direct distribution at some point and you will obtain these bounds.",
                    "label": 0
                },
                {
                    "sent": "So in principle it is very similar, but it is not at all the case because both ways don't don't lead to the same thing if you take the expectation with respect to rule here.",
                    "label": 0
                },
                {
                    "sent": "You won't get this kind of pack, Bashan bounce, because you will have a term here which is linear in row, whereas circuit back library versions which appears in background is not at all dinner, and reciprocally.",
                    "label": 0
                },
                {
                    "sent": "Inversely, if you want to go from this to this, taking row equal to the direct distribution won't work because well, at least when you have some kind of well spread prior distribution, because this callback library.",
                    "label": 0
                },
                {
                    "sent": "Tim will diverge for Royco direct distribution at the point and the prior distribution, which is quite well spread, which does not put mass on any particular function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is disseminated this these bounds essentially dissimilar because of the comeback Libra term.",
                    "label": 0
                },
                {
                    "sent": "And the interest of.",
                    "label": 0
                },
                {
                    "sent": "Of pack Bayesian approach is to have a measure of complexity which is quite different from the usual viewpoint, which is expressed in terms of the kullback.",
                    "label": 0
                },
                {
                    "sent": "Leiber divergent with respect to some prior and this prior as has been already said yesterday, is not at all representing some kind of belief on the way the data has been generated, so it's not at all.",
                    "label": 0
                },
                {
                    "sent": "Coming from some kind of valuation approach.",
                    "label": 0
                },
                {
                    "sent": "And the bounds also holds for any prior and any posterior distribution, so this is also a difference.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                },
                {
                    "sent": "C as basic, different present basic different bounds and emphasizing on the similarity on this bounds.",
                    "label": 0
                },
                {
                    "sent": "So for this part of the talk will consider that the losses are between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Some of the bounds holds for any setting such that we have this condition and some of our bounds will hold only for the classification setting.",
                    "label": 0
                },
                {
                    "sent": "But the basic bound from the David Blacklisters Holdin for any are bounded loss, and it states that for any prior distribution, we probably for any confidence level with probability at least one minus epsilon.",
                    "label": 1
                },
                {
                    "sent": "We have that for any distribution, roll the absolute value of the difference between the average risk and the average empirical risk is bounded by 1 / sqrt N terms when the multiplicative factor.",
                    "label": 0
                },
                {
                    "sent": "Makes it appear so cool back library versions.",
                    "label": 0
                },
                {
                    "sent": "And equivalently to this property which hold for any row.",
                    "label": 1
                },
                {
                    "sent": "We can say that for any data dependent distribution rule hat, so any posterior distribution, if you prefer we have with probability at least one minus epsilon, this this property.",
                    "label": 0
                },
                {
                    "sent": "So note the differences.",
                    "label": 0
                },
                {
                    "sent": "Is that we are here considering data dependent distribution and here we just say that it's holds simultaneously for any distribution.",
                    "label": 0
                },
                {
                    "sent": "But both statements are equivalent because if you want to prove the second one from the first one, what you say is that the difference between the left hand side and the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Is smaller than the Super mom of a rule of the difference between system and system and the first statement says that this supremum of the difference is smaller than zero, so you get the second one this way and to get the first one from the second one, it suffices to take if you put the measurability program, decides to take the hat rule which realized the supremum of the difference between system and system.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So both statements are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So in the pioneering work of Macalester is the proof was rather lengthy and Fortunately Matthias Seeger provides a shorter proof, and I will review it.",
                    "label": 0
                },
                {
                    "sent": "Well, with my own words, so I will emphasize on on this lemma, which is which says that if you want to have a background so not only pack Bayesian bounds, just you want to have the background.",
                    "label": 0
                },
                {
                    "sent": "So basically you want that for some random variable with probability at least one minus epsilon.",
                    "label": 1
                },
                {
                    "sent": "You are below.",
                    "label": 0
                },
                {
                    "sent": "You know that this random variable is below log epsilon minus one.",
                    "label": 1
                },
                {
                    "sent": "And to prove this kind of bounds, the only thing you have to check is to prove that Laplace transform of your valuable is smaller than one.",
                    "label": 0
                },
                {
                    "sent": "And so this is a trivial limit is just Markov's inequality written in the way we want.",
                    "label": 0
                },
                {
                    "sent": "So if you want to prove.",
                    "label": 0
                },
                {
                    "sent": "Invasion bounder I would present into first parts is the only thing you have to do is show what is a random variable V. So if we want to prove this we want to isolate the log epsilon minus one terms.",
                    "label": 0
                },
                {
                    "sent": "So you just take the square multiply by two N -- 1.",
                    "label": 0
                },
                {
                    "sent": "Put this kickback Libra turn on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "Put the log 4 and terms in the left hand side and you have your random variable.",
                    "label": 0
                },
                {
                    "sent": "So explicitly your random variable is this.",
                    "label": 0
                },
                {
                    "sent": "This and you want that hold for any rules and so your random variable is the supermom of a role, so distribution.",
                    "label": 0
                },
                {
                    "sent": "Of this quantity.",
                    "label": 0
                },
                {
                    "sent": "So since we want to prove this, the only thing we have to prove is at the expense as expectation of exponential V is below 1.",
                    "label": 0
                },
                {
                    "sent": "So let's first.",
                    "label": 0
                },
                {
                    "sent": "Notes that.",
                    "label": 0
                },
                {
                    "sent": "OK, we want to make appear the legend transform of the comeback library versions.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a term which has the form expectation with respect to rule of some quantity minus a callback library term.",
                    "label": 0
                },
                {
                    "sent": "So to do this, we see that we want to put the square inside where we want to put stories expectation outside, and this is done by Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "So you have that V is trivially from Jensen Inequality's models and system, and then you can compute exactly while you have a closed form formula for this problem, which is.",
                    "label": 0
                },
                {
                    "sent": "Which is this quantity minus minus system which is a constant?",
                    "label": 0
                },
                {
                    "sent": "So in fact, you can view this presentation of the proof as completely similar to.",
                    "label": 0
                },
                {
                    "sent": "To the work of Francois and Vascular Gemma.",
                    "label": 1
                },
                {
                    "sent": "But it's OK, it's just a way of stating it a bit differently because I'm putting the emphasis on this lemma.",
                    "label": 0
                },
                {
                    "sent": "So for now the we have used this Markov property and Jensen inequality.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kim.",
                    "label": 0
                },
                {
                    "sent": "And then so OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, I think it is the expenditure of system and the expectation when you take the exponential, you will have one of the four N. Times the expectation with respect to the distribution having generated the data.",
                    "label": 0
                },
                {
                    "sent": "So you're training data of this expectation here.",
                    "label": 0
                },
                {
                    "sent": "So this is what I.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's written in this slide.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is just use should be need to interpret inverted.",
                    "label": 0
                },
                {
                    "sent": "So to sign some signs.",
                    "label": 0
                },
                {
                    "sent": "And you add 1 here and subtract 1 here just to make appear non negative random variable while you want to have a non negative random variable.",
                    "label": 0
                },
                {
                    "sent": "It's just because it's the expectation of a non negative random variable is equal to the integral.",
                    "label": 0
                },
                {
                    "sent": "That's the probability of this random variable exceeds some threshold.",
                    "label": 0
                },
                {
                    "sent": "So it's not that these are qualities and then you can rewrite this event as just being the absolute value of the difference between the true risk and the empirical risk is above this threshold.",
                    "label": 0
                },
                {
                    "sent": "This is just rewriting differently, so same event.",
                    "label": 0
                },
                {
                    "sent": "So it's still an inequality, and at this point this is 1/3 way place where we have an inequality.",
                    "label": 0
                },
                {
                    "sent": "We use everything inequality.",
                    "label": 0
                },
                {
                    "sent": "So because here we are in in known grounds this is sum of IID random variables.",
                    "label": 0
                },
                {
                    "sent": "So we can apply Deans inequality.",
                    "label": 0
                },
                {
                    "sent": "And obtain this quantity and then we just compute this integral which is we have a closed form for it.",
                    "label": 0
                },
                {
                    "sent": "We end up with form four N -- 1 / 4 N which we say it's below 1.",
                    "label": 0
                },
                {
                    "sent": "So we have proved that the random variable V has Laplace transform which is below 1.",
                    "label": 0
                },
                {
                    "sent": "So we have proved the PAC Bayesian bounds.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the proof of Mcallister's inequality and.",
                    "label": 0
                },
                {
                    "sent": "At the beginning of the patient approach, but was stand up to do is take your empirical bound on your average risk and minimize it.",
                    "label": 0
                },
                {
                    "sent": "It seems that since then we have a bit depart from this approach to the extent that we are more and more trying to not to minimize it, but to find.",
                    "label": 0
                },
                {
                    "sent": "Prior and posterior distribution for which we have a closed form expression from for the callback library versions.",
                    "label": 0
                },
                {
                    "sent": "So if you want to, if you try to minimize this, you realize that the distribution which minimizes the empirical bound has this form, so it's Gibbs distribution with energy as an empirical risk and invest temperature parameter Lambda, where this Lambda you can see it, which is essentially between square root of N&N.",
                    "label": 0
                },
                {
                    "sent": "Lambda satisfies this fixed point equation.",
                    "label": 0
                },
                {
                    "sent": "Or you can also see it as visualizing the minimum of this quantity.",
                    "label": 0
                },
                {
                    "sent": "So as you see Lambda.",
                    "label": 0
                },
                {
                    "sent": "He's depending under an end, so confidence level and also some kind of geometry of the problem.",
                    "label": 0
                },
                {
                    "sent": "'cause this is some kind of quite complex quantity.",
                    "label": 0
                },
                {
                    "sent": "But this is just a real parameter to tune.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I will present a serious bone which holds into classification setting.",
                    "label": 0
                },
                {
                    "sent": "So I will denote this way the callback Leiber divergent between the Bernoulli distribution of parameter P and then re distribution of parameter Q and the Seagulls bound can be stated.",
                    "label": 0
                },
                {
                    "sent": "This way it's just upper bounding circle back library vergence between the boundary distribution of parameter is the average empirical risk and parameters have rich true risk by quantity which has the same flavor as in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "That is a callback library vergence divided by N. And to prove this, it is exactly the same if we want to prove this, we've isolates your log epsilon minus one term, so we multiply by N, puts a callback library divergent, and the left hand side puts the log 2 sqrt N on the left hand side so that we have a random variable.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Revealed so this is how under variable V we want to do.",
                    "label": 0
                },
                {
                    "sent": "To have effect bound.",
                    "label": 0
                },
                {
                    "sent": "So we want to control this by one.",
                    "label": 0
                },
                {
                    "sent": "So we just write what it is and what it is to be noted here is that.",
                    "label": 0
                },
                {
                    "sent": "OK, since the kullback Leiber divergent is convex, you can use Jensen inequality once more, as in the previous proof, and as in the general proof of Pascal and Francois encoder.",
                    "label": 0
                },
                {
                    "sent": "You use Jensen inequality pushes the expectation outside and then you are in known ground because you know that this Supreme could you have across form for the expression for this supremum.",
                    "label": 0
                },
                {
                    "sent": "So you make appear this on TT once more.",
                    "label": 0
                },
                {
                    "sent": "You use the beanie to interpret the two and you compute explicitly so it is a simple computation to compute explicitly.",
                    "label": 0
                },
                {
                    "sent": "This expectation of this exponential of circle back library versions.",
                    "label": 0
                },
                {
                    "sent": "So you end up with this formula.",
                    "label": 0
                },
                {
                    "sent": "And then you have to use stealing the approximation to have a quite tight bound on this quantity.",
                    "label": 0
                },
                {
                    "sent": "And This is why you have the 2 sqrt N. So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the proof of signal spell.",
                    "label": 0
                },
                {
                    "sent": "Set up.",
                    "label": 0
                },
                {
                    "sent": "So now if we want to compare both bounds, the first thing is that if you think of Pinsker inequality, you see right away that they really have a well, at least from.",
                    "label": 0
                },
                {
                    "sent": "From this one you can deduce abound, which is really similar to the first one.",
                    "label": 0
                },
                {
                    "sent": "Just taking the square root and both sides and the things I said square root is above this quantity, but it would miss the advantage of.",
                    "label": 0
                },
                {
                    "sent": "Of Seagulls bound to do this too applies in this Pinsker inequality.",
                    "label": 0
                },
                {
                    "sent": "What is better to do is to understand that when the kullback Leiber divergent between 2 numbers well.",
                    "label": 0
                },
                {
                    "sent": "I know you have parameters.",
                    "label": 0
                },
                {
                    "sent": "These two numbers is below some threshold then.",
                    "label": 0
                },
                {
                    "sent": "You can deduce from this this property that we have here, so it needs some computation.",
                    "label": 0
                },
                {
                    "sent": "And this property is really of the same form as this one, because here you have a term which is at most 1/4 because it is X * 1 -- X.",
                    "label": 0
                },
                {
                    "sent": "So and this is so cool back library term.",
                    "label": 0
                },
                {
                    "sent": "So you exactly recover the same form as here plus an additional term.",
                    "label": 0
                },
                {
                    "sent": "But this term is of order one of and so this is some kind of 2nd order term which will jump in only when.",
                    "label": 0
                },
                {
                    "sent": "You have very low empirical risk, so this is maybe a more explicit way of seeing the stickers bound.",
                    "label": 0
                },
                {
                    "sent": "OK, I will mention here.",
                    "label": 0
                },
                {
                    "sent": "So first back by Asian bounds of Olivia Katani which which is intercept.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Election notes, so this is not the one that was mentioned yesterday into talks, so it the difference between the previous bound is that you have a free parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "80 and it says that we find probability you have the average risk which is bounded by the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Which has a factor which is.",
                    "label": 0
                },
                {
                    "sent": "1 /, 1 minus Lambda over.",
                    "label": 0
                },
                {
                    "sent": "In this side function.",
                    "label": 0
                },
                {
                    "sent": "So say function is justice.",
                    "label": 0
                },
                {
                    "sent": "When reading this bound you can.",
                    "label": 0
                },
                {
                    "sent": "Assume that Lambda is small with respect to N and this this term is just one half essentially.",
                    "label": 0
                },
                {
                    "sent": "And you have the comeback Liebert also, so it's not clear how to compare this bound to previous bound, but a way to do this is to remember that the well not to remember.",
                    "label": 0
                },
                {
                    "sent": "You can see a posteriori.",
                    "label": 0
                },
                {
                    "sent": "That's the typical value of Lambda between square root of N&N, so that here the 1 / 1 minus Lambda over to enter you can see it as one plus Lambda over 2 N. So this term becomes system.",
                    "label": 0
                },
                {
                    "sent": "Approximately by taking a.",
                    "label": 0
                },
                {
                    "sent": "Just a Taylor expansion and Zephyr terms is of order this quantity, and if you optimize your choice of Lambda.",
                    "label": 0
                },
                {
                    "sent": "You will see exactly the term.",
                    "label": 0
                },
                {
                    "sent": "Well, not exactly, but similar term appearing here, which is the callback library versions divided by N. With some factor here, but this factor was also in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Up to this additional factor, but if you think of low empirical risk and distribution, row concentrating and low empirical risk, this is close to one this.",
                    "label": 0
                },
                {
                    "sent": "So the expression here is really similar.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Kelso, because you can't convex right, so you can know about it again by maximizing your functions.",
                    "label": 0
                },
                {
                    "sent": "Somehow it's the slope or something.",
                    "label": 0
                },
                {
                    "sent": "Lower bounding it again, putting in this parimeter.",
                    "label": 0
                },
                {
                    "sent": "Linearize this.",
                    "label": 0
                },
                {
                    "sent": "And if you shift the lumber where it's the right place on it, but it's at the tourist small, then you be at the right point.",
                    "label": 0
                },
                {
                    "sent": "If it's kind of 1/2 and you had nothing point.",
                    "label": 0
                },
                {
                    "sent": "I don't, I don't know, certainly.",
                    "label": 0
                },
                {
                    "sent": "Yesterday.",
                    "label": 0
                },
                {
                    "sent": "We consider here.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "As the empirical risk goes to 0, this quantity appears to go to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, so This is why here you have this and I don't have room on my side, but you always have the 2nd order term, which is a term which was appearing here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So indeed, this is not rigorous bound to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have it rigorous.",
                    "label": 0
                },
                {
                    "sent": "You have to do first.",
                    "label": 0
                },
                {
                    "sent": "You have to optimize over Lambda so you have to take agreed over Lambda and user Union bound.",
                    "label": 0
                },
                {
                    "sent": "This is doable.",
                    "label": 0
                },
                {
                    "sent": "And indeed, when you do properly some minimization, you will have a term which is of order callback library version divided by N which will appear so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cricket.",
                    "label": 0
                },
                {
                    "sent": "So the bound, which is straightforward extension of the previous one when you are interested not in classification but in more general losses, is this one.",
                    "label": 0
                },
                {
                    "sent": "So it has a similar form, except that except that you have these variants which appears, so this term is just variance with respect to the pair XY of the loss.",
                    "label": 0
                },
                {
                    "sent": "And then you take the expectation with respect to rule of this variance.",
                    "label": 0
                },
                {
                    "sent": "The link to the previous one is just to say that for classification, the variance of loss is bounded by the risk the true risk.",
                    "label": 0
                },
                {
                    "sent": "So the term which appears you can put it with the left hand side, because this is a similar term and This is why this found.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same reasoning which gives this one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which this quantity was appearing, and this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One in which the quantity was appearing.",
                    "label": 0
                },
                {
                    "sent": "Because this time you put it on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally bound, which is not, I think, not very popular, but which is also of interest is a songs bound.",
                    "label": 0
                },
                {
                    "sent": "Which is in a form which is maybe not so nice, because here what is control it is not the average risk.",
                    "label": 0
                },
                {
                    "sent": "It's some kind of average with respect to rule of some complex.",
                    "label": 0
                },
                {
                    "sent": "City.",
                    "label": 0
                },
                {
                    "sent": "But it's right inside the easy as usual, well has the usual form I would say, but we understand that we recover, stormed out of bounds or something similar.",
                    "label": 0
                },
                {
                    "sent": "We can do a Taylor expansion of this term when Lambda over any small and precisely we have this thing.",
                    "label": 0
                },
                {
                    "sent": "And we note that here what was in system was two things.",
                    "label": 0
                },
                {
                    "sent": "The average of the risk with respect to roll.",
                    "label": 0
                },
                {
                    "sent": "And civilian stem, which was appearing in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So once more.",
                    "label": 0
                },
                {
                    "sent": "But it appears it's a sexpectations with respect to the posterior.",
                    "label": 0
                },
                {
                    "sent": "Of the variance of the loss.",
                    "label": 0
                },
                {
                    "sent": "Up to 2nd order term.",
                    "label": 0
                },
                {
                    "sent": "So you really have a similar bound.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is about which was cited yesterday.",
                    "label": 0
                },
                {
                    "sent": "Was this the bound?",
                    "label": 0
                },
                {
                    "sent": "Which can be derived from songs bound by instead of using this formula, you can use the expressive value of the log path for binary distribution.",
                    "label": 0
                },
                {
                    "sent": "So you can compute it exactly and then it makes appears this this function and if you do a Taylor expansion of this function, you will see that says some kind of variance term hide hidden in it.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially so basically.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The point here is that we recover a similar bound, so if you take things bound and apply.",
                    "label": 0
                },
                {
                    "sent": "Exactly low class formula for the binary distribution in the classification setting you recover Catanese bound to bound.",
                    "label": 0
                },
                {
                    "sent": "I used during my PhD was essentially of the same form.",
                    "label": 1
                },
                {
                    "sent": "After optimization of the parameter Lambda with constant bit less less good because we were not taking advantage of the classification setting of the explicit formula of the.",
                    "label": 0
                },
                {
                    "sent": "Lab tests are of solace.",
                    "label": 0
                },
                {
                    "sent": "And and it is similar to the also to the first bound of Olivia category in the classification setting, because once more since we are interested in function, we flow through risk.",
                    "label": 0
                },
                {
                    "sent": "This is not a big difference between system and system.",
                    "label": 0
                },
                {
                    "sent": "And it is not also a big difference between this term.",
                    "label": 0
                },
                {
                    "sent": "System where the difference is mainly in the position of this this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, one of the advantage of cigarettes Brown is that it is really expressed like this, whereas these ones.",
                    "label": 1
                },
                {
                    "sent": "We have done some kind of tax pension, so if we wanted to write it properly what will happen?",
                    "label": 0
                },
                {
                    "sent": "What will appear?",
                    "label": 0
                },
                {
                    "sent": "Is this kind of terms so going to order term and here you will have a login instead of log epsilon minus one.",
                    "label": 0
                },
                {
                    "sent": "You will have log of log in minus epsilon minus one.",
                    "label": 0
                },
                {
                    "sent": "So at the end it will be really similar if we do properly the computation.",
                    "label": 0
                },
                {
                    "sent": "So the bounds could be considered very similar for this.",
                    "label": 1
                },
                {
                    "sent": "For this reason, not the same, exactly the same constantly appears in the square root of N terms.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I will switch to something which is less toilets, more research talk about, least square regression and how to aggregate production function in this setting.",
                    "label": 1
                },
                {
                    "sent": "So when can see a motivation for this program can be if you are given some estimators.",
                    "label": 0
                },
                {
                    "sent": "It's a G1G D and it comes from data that you you don't have and you want to predict as well as the best of these D estimators.",
                    "label": 0
                },
                {
                    "sent": "You can use.",
                    "label": 0
                },
                {
                    "sent": "And the whole story is to predict as well as suggest of this D functions.",
                    "label": 0
                },
                {
                    "sent": "Auto predict as well as the best convex combination of this function or to predict as well as the best dinner combination of this function.",
                    "label": 0
                },
                {
                    "sent": "We are in the OK.",
                    "label": 0
                },
                {
                    "sent": "I should specify that I will consider, for simplicity abounding nice setting so they'll put are in minus one one.",
                    "label": 0
                },
                {
                    "sent": "An trivially target, which is to some more difficult would be is to perform as well as this one, so the risk of this target function is smaller than the risk of this target function, which is smaller than the risk of this target function.",
                    "label": 0
                },
                {
                    "sent": "But in terms of estimation, it should be simpler.",
                    "label": 0
                },
                {
                    "sent": "Well, it would be simpler to predict as well as this one bit more difficult to predict as well as this one and a bit more difficult to predict as well as this one.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is known for from different works?",
                    "label": 0
                },
                {
                    "sent": "Is that is the expected excess there exist algorithms for each of the three tasks?",
                    "label": 1
                },
                {
                    "sent": "Suggest he expected excess risk is smaller essentially says unlock Dover N for get about a minimum.",
                    "label": 1
                },
                {
                    "sent": "This is for model selection type of aggregation for the convex aggregation this is not.",
                    "label": 0
                },
                {
                    "sent": "We cannot reach this estimation rate.",
                    "label": 0
                },
                {
                    "sent": "What is rich is.",
                    "label": 0
                },
                {
                    "sent": "Rate, which is essentially square root of log D / N when D is much larger than square root of N. Yeah, and when D is much smaller than square root of N, we can go a bit faster than this 1 / sqrt N terms and reach the 1 / N rate, but with a factor here, which is D&D instead of log D for model selection.",
                    "label": 0
                },
                {
                    "sent": "So naturally the speed's learning rate is a bit worse, whereas for linear.",
                    "label": 0
                },
                {
                    "sent": "Interlinear aggregation setting.",
                    "label": 0
                },
                {
                    "sent": "So convergence rate is of the D of N. If you choose, well, your algorithm.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately for the slice last point.",
                    "label": 1
                },
                {
                    "sent": "The estimator needs to know the input distribution, so this is something we will address in the last part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So I will go over the free problems and talk about the algorithms which allows to perform this kind of rates.",
                    "label": 0
                },
                {
                    "sent": "So first let me be OK. Is the previous slide was saying there exist algorithms such that we reach this learning rates and we know that these are tight to the extent that we have exactly the corresponding lower bounds?",
                    "label": 0
                },
                {
                    "sent": "Thanks to see back off.",
                    "label": 0
                },
                {
                    "sent": "So in a mini Max.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you consider.",
                    "label": 0
                },
                {
                    "sent": "A set of probability distribution like this.",
                    "label": 0
                },
                {
                    "sent": "So you you consider that so why are generated this way with Goshen noise?",
                    "label": 0
                },
                {
                    "sent": "Then you know that whatever estimator you take, you know that the Supreme of the expected excess risk will be for the model section.",
                    "label": 0
                },
                {
                    "sent": "Task would be always greater than this quantity, so logged over N is the learning rate for this problem.",
                    "label": 0
                },
                {
                    "sent": "Similarly, square root of log D / N is essentially so learning rate for this task when D is much much bigger than square root of N. And if D is much smaller than square root of N, you can reach 1 / N rate.",
                    "label": 0
                },
                {
                    "sent": "And for dinner, aggregation is a mini.",
                    "label": 0
                },
                {
                    "sent": "Max rate is really the over and so these are the lower bounds corresponding to previous upper bounds.",
                    "label": 0
                },
                {
                    "sent": "Yeah, see.",
                    "label": 0
                },
                {
                    "sent": "Oceans on the input space and are where the VR component is normal.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is.",
                    "label": 0
                },
                {
                    "sent": "Set up for Verity on XY, so consider.",
                    "label": 0
                },
                {
                    "sent": "What is the why?",
                    "label": 0
                },
                {
                    "sent": "The Why is given by some fixed function?",
                    "label": 0
                },
                {
                    "sent": "Yes yes yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean for me you can see.",
                    "label": 0
                },
                {
                    "sent": "Also pair XY, which are generated by this model when G has this property and this is a set of probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So I will first consider the model section and detail what Happ.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intermodal selection elevation.",
                    "label": 0
                },
                {
                    "sent": "So the target is to predict as well as this.",
                    "label": 0
                },
                {
                    "sent": "Function is a very surprising phenomenon for this problem, because to be optimal you need to choose your estimator cannot stay cannot output just one of these function.",
                    "label": 1
                },
                {
                    "sent": "If it does this think of empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "For instance, it will output a function which is always in this set.",
                    "label": 0
                },
                {
                    "sent": "Think of cross validation.",
                    "label": 0
                },
                {
                    "sent": "Same thing.",
                    "label": 0
                },
                {
                    "sent": "Will output a function which is always in this set.",
                    "label": 0
                },
                {
                    "sent": "But all this is.",
                    "label": 0
                },
                {
                    "sent": "Estimator pool to the extent that they are learning rate is of order 1 / sqrt N. Whereas we know from the previous slides that's a correct rate.",
                    "label": 1
                },
                {
                    "sent": "Is 1 / N times log D. But if you look at the dependencies in N you have, you will lose a lot by staying in your model.",
                    "label": 1
                },
                {
                    "sent": "The second thing which is surprising is that up to recently is the estimator, which was performing so logged over N rate.",
                    "label": 0
                },
                {
                    "sent": "With some kind of tricky algorithm called the progressive Nature rule.",
                    "label": 0
                },
                {
                    "sent": "Which does not come from the usual statistical running for re up also packed well.",
                    "label": 1
                },
                {
                    "sent": "It has a language pack version approach but not in the way we use it.",
                    "label": 0
                },
                {
                    "sent": "But and finally, scissor proof was not at all based on the pack Bayesian analysis or Supermom of America.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Processes so this and this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So let me describe this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a progressive nature rule.",
                    "label": 0
                },
                {
                    "sent": "You start with the uniform distribution on this fine, it's finite set of function.",
                    "label": 1
                },
                {
                    "sent": "You have a parameter, Lambda positive and you consider the cumulative loss on the 1st I points.",
                    "label": 0
                },
                {
                    "sent": "So if it was in here it will be end times empirical risk this quantity.",
                    "label": 1
                },
                {
                    "sent": "But here you just consider the cumulative last.",
                    "label": 0
                },
                {
                    "sent": "And the first high points of your training data.",
                    "label": 0
                },
                {
                    "sent": "And you consider the average of the function G when G is drawn according to the Gibbs distribution.",
                    "label": 0
                },
                {
                    "sent": "Link to this cumulative loss so.",
                    "label": 0
                },
                {
                    "sent": "This distribution is the distribution which has, with respect to the uniform distribution density, which is proportional to exponential minus Lambda Sigma I.",
                    "label": 0
                },
                {
                    "sent": "So it concentrates on functions which has low low cumulative loss up to time.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "If you want well, I should not say this because we know we have all the data right right away.",
                    "label": 0
                },
                {
                    "sent": "It's not online learning.",
                    "label": 0
                },
                {
                    "sent": "But we consider this kind of distribution for I going from zero to N. So for I = 0, this is just the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "This is just a uniform distribution and we take a seasonal mean.",
                    "label": 0
                },
                {
                    "sent": "Of this bridge.",
                    "label": 0
                },
                {
                    "sent": "With respect to I so this is quite surprising, because when would they expect that uneasy last one is important only for I equal N. We have some kind of good estimation because this is concentrating on low empirical risk function.",
                    "label": 0
                },
                {
                    "sent": "This distribution for I equal to 2 N, so we would expect that just just this quantity for I equal any sufficient.",
                    "label": 0
                },
                {
                    "sent": "But we don't know any result for this.",
                    "label": 0
                },
                {
                    "sent": "This estimator, so when we know is for this progressive material which is, this is Armin, so another way of presenting this estimator is this one.",
                    "label": 0
                },
                {
                    "sent": "And the theoretical grantee of this estimator is that the expected excess risk is below.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for Lambda, equal for Lambda equals some particular value.",
                    "label": 0
                },
                {
                    "sent": "In fact, 1/8, so for Lambda equal 1 eighth we have this property.",
                    "label": 0
                },
                {
                    "sent": "So we recover this log deveren rate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of the Gaussian likelihood.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Would you have the syndrome in if you use the Bashan estimate, I don't think.",
                    "label": 0
                },
                {
                    "sent": "Expectational posterior.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I would need to do the computations.",
                    "label": 0
                },
                {
                    "sent": "Las Vegas wouldn't give, I mean, it sort of gives more emphasis to the early points.",
                    "label": 0
                },
                {
                    "sent": "Is an arbitrary or yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true, yeah?",
                    "label": 0
                },
                {
                    "sent": "Also, you write.",
                    "label": 0
                },
                {
                    "sent": "Previous points, you can always write it by changing with Lester.",
                    "label": 0
                },
                {
                    "sent": "So if you have it, you end up with something that doesn't make any difference between your.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is not at all exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Got any plans it should give you the same thing.",
                    "label": 0
                },
                {
                    "sent": "You know this does not.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK yeah.",
                    "label": 0
                },
                {
                    "sent": "Need to figure out is in is another trigger.",
                    "label": 0
                },
                {
                    "sent": "Because our eyes thing.",
                    "label": 0
                },
                {
                    "sent": "Expect.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, the idea comes from Andrew Barron, but this estimator was in your work and so one of young young.",
                    "label": 0
                },
                {
                    "sent": "Paper paper Andrew Barron said this is for the technical trick of.",
                    "label": 0
                },
                {
                    "sent": "You think it's icy 1987 or something like this?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of them.",
                    "label": 0
                },
                {
                    "sent": "OK. Let me present another algorithm which is a bit more sophisticated, but for which we have a tighter tighter bound and which comes from a sequential analysis from the work of both and vomit.",
                    "label": 0
                },
                {
                    "sent": "Kevin and I think I'm forgetting 104.",
                    "label": 0
                },
                {
                    "sent": "And in which state that we can?",
                    "label": 0
                },
                {
                    "sent": "So we still have a Lambda parameter.",
                    "label": 0
                },
                {
                    "sent": "And for any I we consider prediction function which satisfies this property.",
                    "label": 1
                },
                {
                    "sent": "So it's not clear what it is doing, so for a moment, let's, let's consider that we are allowed to put this expectation inside here.",
                    "label": 0
                },
                {
                    "sent": "If we are allowed to do this, it would mean that the log exponential answer minus Lambda would collapse.",
                    "label": 0
                },
                {
                    "sent": "So so this prediction function, which essentially some prediction function which predict better.",
                    "label": 0
                },
                {
                    "sent": "Then this one for any XY.",
                    "label": 0
                },
                {
                    "sent": "Naturally it's not exactly this, but this is just to emphasis that this inequality that we ask for is something guaranteeing that this quantity is more.",
                    "label": 0
                },
                {
                    "sent": "And we use this H hat function by using a scissor mean.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit similar to the previous estimator, an in fact by using some kind of X concavity argument.",
                    "label": 0
                },
                {
                    "sent": "You can check that this edge edge hat.",
                    "label": 0
                },
                {
                    "sent": "You can take it equal to the the average according to \u03a0 minus Lambda Sigma I.",
                    "label": 0
                },
                {
                    "sent": "If you consider Lambda which is smaller than one over 8th.",
                    "label": 0
                },
                {
                    "sent": "So this function does exist for Lambda smaller than one over 8th, but it also exists when you take Lambda equal to one of the two, but then it has not simple explicit form.",
                    "label": 0
                },
                {
                    "sent": "But it has the advantage that for this particular value, the resulting estimator will satisfy this property, which is 4 times better than the previous one.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zeze drawback that drawback to this to both of these estimator, which is the following.",
                    "label": 0
                },
                {
                    "sent": "The excess risk.",
                    "label": 0
                },
                {
                    "sent": "Well, we know that it is expected accessories is of order 1 / N, and we would expect that with high probability.",
                    "label": 0
                },
                {
                    "sent": "We have this success rates which is of order 1 / N This does not hold.",
                    "label": 0
                },
                {
                    "sent": "And we have a very simple example.",
                    "label": 0
                },
                {
                    "sent": "Even with D equal two and two constant functions, you can prove that for some confidence level you will have a lower bound on this excess risk, meaning that you do not have this 1 / N deviation and you have to suffer some kind of one over square root of indication.",
                    "label": 0
                },
                {
                    "sent": "So you don't have exponential concentration here.",
                    "label": 0
                },
                {
                    "sent": "The latter.",
                    "label": 0
                },
                {
                    "sent": "That are very very small.",
                    "label": 0
                },
                {
                    "sent": "Lot of event.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and you say that you don't have with high, probably yeah, in your family there's a lot for which it's almost zero.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a way of stating, yeah, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Camping go outside from the original model.",
                    "label": 0
                },
                {
                    "sent": "Concentrate.",
                    "label": 0
                },
                {
                    "sent": "And so this is my non paid version side we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Present.",
                    "label": 0
                },
                {
                    "sent": "An algorithm which has the 1 / N division.",
                    "label": 1
                },
                {
                    "sent": "So the algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "So if the whiteboard is my set.",
                    "label": 0
                },
                {
                    "sent": "So prediction function set space plus right and if you have this is your your function, let's say.",
                    "label": 0
                },
                {
                    "sent": "So this is my prediction function.",
                    "label": 0
                },
                {
                    "sent": "So the estimator first.",
                    "label": 0
                },
                {
                    "sent": "Look for the empirical risk minimizer among these D functions.",
                    "label": 0
                },
                {
                    "sent": "So assume that this is the empirical risk minimizer, and then it consider all the functions that are convex combination of symmetrical rich minimizer and the other functions.",
                    "label": 0
                },
                {
                    "sent": "So we consider all the functions which are.",
                    "label": 0
                },
                {
                    "sent": "In this three segments.",
                    "label": 0
                },
                {
                    "sent": "For the full four and we we minimize empirical risk on this set with his some kind of star shaped set function and just by doing this.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing here is there is no parameter.",
                    "label": 0
                },
                {
                    "sent": "We have that with probability at least one minus epsilon.",
                    "label": 1
                },
                {
                    "sent": "The risk of the excess risk of this estimator is below.",
                    "label": 0
                },
                {
                    "sent": "Well, some constant times logged over in with her.",
                    "label": 0
                },
                {
                    "sent": "Well, where is the confidence of appears in the logarithmic terms?",
                    "label": 0
                },
                {
                    "sent": "So this means that the two algorithm which we have seen progressive mature rule and progressive interact mature rule have indeed suboptimal deviation to the extent that there does exist an algorithm which for which we have the one of deviations.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Do this mixture, yeah.",
                    "label": 0
                },
                {
                    "sent": "Crazy lesson.",
                    "label": 0
                },
                {
                    "sent": "So we have way of doing the mixture.",
                    "label": 0
                },
                {
                    "sent": "He will have his feeling that if you average and even ovarian.",
                    "label": 0
                },
                {
                    "sent": "They didn't actually officer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, to some extent.",
                    "label": 0
                },
                {
                    "sent": "Look at I there is a different kind of regularization here because we are just considering this kind of functions.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "So if you are in the middle.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I love seeing this is that if you look at the output of your progressive material it will always be in the in the convex of your function.",
                    "label": 0
                },
                {
                    "sent": "So you have to pay for being maybe in a larger model then the algorithm here at the end it will you know before before running it that it will be on one of these segments.",
                    "label": 0
                },
                {
                    "sent": "One of the segments which delimits the convex Hull because we are just considering this kind of segment.",
                    "label": 0
                },
                {
                    "sent": "So we should not pay.",
                    "label": 0
                },
                {
                    "sent": "This is some kind of regularising.",
                    "label": 0
                },
                {
                    "sent": "We rest fixes this set of possible output of the algorithms, so definitely it should do some regularization.",
                    "label": 0
                },
                {
                    "sent": "Soon.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You have two functions with.",
                    "label": 0
                },
                {
                    "sent": "Almost nothing with lost, and when the difference between the two losses of order 1 / sqrt N. Patience is the empirical risk for the two functions and then did the greater the usual gifts estimated may concentrate on those under the wrong one.",
                    "label": 0
                },
                {
                    "sent": "All right here you you draw the line between the two.",
                    "label": 0
                },
                {
                    "sent": "To do pairwise.",
                    "label": 0
                },
                {
                    "sent": "And you have longer to prepare like yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Try to go by triples.",
                    "label": 0
                },
                {
                    "sent": "No, because I mean so target for me was rich.",
                    "label": 0
                },
                {
                    "sent": "That is managing this but.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "The smallest extension of the model in which you can work.",
                    "label": 0
                },
                {
                    "sent": "Number two simplex.",
                    "label": 0
                },
                {
                    "sent": "It might be better to do the liar.",
                    "label": 0
                },
                {
                    "sent": "Then you should not compare yourself to this one.",
                    "label": 0
                },
                {
                    "sent": "You should compare to some kind of other target, because you will pay for going in a more complex set of function so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Original function.",
                    "label": 0
                },
                {
                    "sent": "As well as the best.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me go.",
                    "label": 0
                },
                {
                    "sent": "I think I'm not having much time left, so let me talk briefly of the.",
                    "label": 0
                },
                {
                    "sent": "So convex aggregation program.",
                    "label": 0
                },
                {
                    "sent": "So now we want to predict as well as specs the best convex combination of the initial function.",
                    "label": 0
                },
                {
                    "sent": "And we consider the case when the dimension is much larger than square root of N, because 4D which is smaller than square root of N as the rate is D / N which is similar to the linear aggregation setting.",
                    "label": 0
                },
                {
                    "sent": "So if we know how to solve the linear aggregation setting, then we solve the convex segregation problem under same time, which minimax optimal rate.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, what we expect is an algorithm which has a rate which is square root of log D / N and the algorithm which managed to do this.",
                    "label": 0
                },
                {
                    "sent": "There are several possibilities.",
                    "label": 0
                },
                {
                    "sent": "The first one is to apply the Progressive Nature rule on an appropriate grid.",
                    "label": 1
                },
                {
                    "sent": "Also previous algorithm.",
                    "label": 1
                },
                {
                    "sent": "The second one is to use the exponential shifted gradient algorithm.",
                    "label": 1
                },
                {
                    "sent": "Oh, and for the first one is to use some kind of stochastic version of the mirror dissent algorithm.",
                    "label": 0
                },
                {
                    "sent": "But all these algorithm results in expectation.",
                    "label": 0
                },
                {
                    "sent": "So we control the expected excess risk and at the deviation, and they're all based on sequential procedure.",
                    "label": 0
                },
                {
                    "sent": "And what have?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Friends here is very nice because it is the setting in which bag beige and furry applies perfectly.",
                    "label": 0
                },
                {
                    "sent": "Because contrary to the classifications setting in which you always have this problem between the risk of the average.",
                    "label": 0
                },
                {
                    "sent": "As a risk of the average, sorry.",
                    "label": 0
                },
                {
                    "sent": "Risk of this quantity.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the notation of the Gibbs risk you OK this gives risks is.",
                    "label": 0
                },
                {
                    "sent": "What's?",
                    "label": 0
                },
                {
                    "sent": "What was used is that this gives.",
                    "label": 0
                },
                {
                    "sent": "This was below twice the risk of the, so this is the risk of the majority vote is below twice it keeps Gibbs risk, so we can use subversion analysis to control this bridge.",
                    "label": 0
                },
                {
                    "sent": "This average some kind of risk in the product space and by using directly the pack Bayesian bounds we have seen in the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "And by using this time I really do so.",
                    "label": 0
                },
                {
                    "sent": "So Union bound of Lambda.",
                    "label": 0
                },
                {
                    "sent": "You you get this kind of bounds.",
                    "label": 0
                },
                {
                    "sent": "Which is which is essentially empirical.",
                    "label": 0
                },
                {
                    "sent": "Up to this term, but this term you can lower bound this empirical risk by is infima of the risk on your convex set.",
                    "label": 0
                },
                {
                    "sent": "By minimizing the bound.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you will obtain is an algorithm whose posterior distribution is diluted will had see and for which the excess risk is controlled by this term.",
                    "label": 1
                },
                {
                    "sent": "So you recover the log the square root of log over N terms, and you have logged over enter here.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that in the worst case, this is of order square root of log D / N. So this is a minimax optimal rate and in the case where the best convex combinations happen to be one of the initial function, this this variance is then equal to 0.",
                    "label": 1
                },
                {
                    "sent": "So this query term disappears and recovers logged over an rate of the.",
                    "label": 0
                },
                {
                    "sent": "Model selection type aggregation so we have some kind of adaptivity of the natural pack version estimator.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think I won't have time to detail this and Olivia Keitany will talk about aggregation in Salina aggregation setting, but what can be said is that in this setting.",
                    "label": 0
                },
                {
                    "sent": "There is no simple deal for unbound which exists.",
                    "label": 1
                },
                {
                    "sent": "Phase one from Boeotian Mesa, which has.",
                    "label": 0
                },
                {
                    "sent": "Look in terms, but if we care about Logan says no simple down.",
                    "label": 1
                },
                {
                    "sent": "And we provide in this work.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Found which has this deal for N rate and the algorithm is simple, so we are considering linear aggregation when we know we know we sorry we know that the coefficient of linear aggregation is in some bounded sets.",
                    "label": 0
                },
                {
                    "sent": "We took the prior distribution which is uniform and is bounded set and we consider the Gibbs estimators.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm has the van down as we wanted to to obtain.",
                    "label": 0
                },
                {
                    "sent": "So this is and here there is clearly a shrinking effect of this distribution, because if you think of if my space of.",
                    "label": 1
                },
                {
                    "sent": "Coefficient like this if the minimizer of the empirical risk is here using this kind of estimator, will.",
                    "label": 0
                },
                {
                    "sent": "Will put you more on the inside of your bounded set of coefficient, so there is a shrinking effect due to the Gibbs distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe let me just quickly talk about this, because this this is linked to the talk of PayPal key.",
                    "label": 0
                },
                {
                    "sent": "It's what happens in very high dimension, meaning D much greater than.",
                    "label": 0
                },
                {
                    "sent": "In in this case we have seen from the learning rates that we can predict as well as the best convex combination, but we cannot predict as well as the best linear combination because the over N would be larger than one, so we have no guarantee and natural target is to predict as well as the best linear combinations when the number of non zero coefficient into linear combination is below some threshold.",
                    "label": 0
                },
                {
                    "sent": "So it is now very well known that too.",
                    "label": 0
                },
                {
                    "sent": "Predict as well as just a when way is to use lasso.",
                    "label": 0
                },
                {
                    "sent": "That is L1 regularization, but the bound success rates bound which has.",
                    "label": 0
                },
                {
                    "sent": "So as of the end times, log rate happens occurs only under strong assumption under correlation of the of the function we need this function to be quite uncredited.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what is interesting is that if you use the model selection approach, you will get rid of this assumption.",
                    "label": 1
                },
                {
                    "sent": "And the model section approaches just the following algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have your training set.",
                    "label": 0
                },
                {
                    "sent": "You could take cut it into two parts.",
                    "label": 0
                },
                {
                    "sent": "On the 1st part you train.",
                    "label": 1
                },
                {
                    "sent": "We'll see estimators corresponding to the different set of nonzero pattern.",
                    "label": 0
                },
                {
                    "sent": "So you consider all this subset of size S. And you use your so keep the estimator.",
                    "label": 1
                },
                {
                    "sent": "I have presented in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Two linear agree gate only this function.",
                    "label": 1
                },
                {
                    "sent": "So the basis function corresponding to the subset I.",
                    "label": 0
                },
                {
                    "sent": "So this is done on the first part of your training set and on the second part of your training set you use the empirical star estimator to select the best of these ones and you can very easily from the previous result just by combining them by Union bound you prove that the excess risk of the resulting estimator.",
                    "label": 0
                },
                {
                    "sent": "Have this property which is you have running rate which is S the size of the non zero pattern over N times.",
                    "label": 1
                },
                {
                    "sent": "Log log essentially.",
                    "label": 0
                },
                {
                    "sent": "And this is without strong assumption and very easily.",
                    "label": 0
                },
                {
                    "sent": "So there is an open problem here, which for me is of great interest is, is there a way to achieve this kind of bound with a computationally efficient estimator?",
                    "label": 0
                },
                {
                    "sent": "Because this estimator you don't want to implement it because you would need to consider all the subset of size S of your.",
                    "label": 0
                },
                {
                    "sent": "Set of basis function.",
                    "label": 0
                },
                {
                    "sent": "And I think your talk will partly address this problem and OK, I will end up here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}