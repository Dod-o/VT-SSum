{
    "id": "3ehhcqbzzaamabksezbezewl6gvbxxe5",
    "title": "Tree-Structured Stick Breaking for Hierarchical Data",
    "info": {
        "author": [
            "Ryan Prescott Adams, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nips2010_adams_tss/",
    "segmentation": [
        [
            "OK, so the big picture here is essentially going to be one of hierarchical clustering where we're going to allow for data to live at internal nodes on the tree.",
            "So essentially we're going to imagine that our data have some underlying hierarchical structure.",
            "That's an observed an what we're going to do is try to recover this."
        ],
        [
            "Tree you can motivate this in a couple different ways by thinking about for example images where we might imagine that images that live at higher levels of the tree might in some ways be exemplars for those lower in the trees.",
            "Maybe kind of Canonical examples of large classes, or in the case of something like document modeling, say scientific papers, we might imagine that the papers that live higher in the tree, maybe there are similar papers.",
            "Maybe they gave rise to whole subfields or a lot of sort of other specialized papers.",
            "That would then appear farther down in the tree as descendants of these Seminole Papers, so that's kind of the general motivation here."
        ],
        [
            "The approach we're going to take is going to be a nonparametric Bayesian one, and we're going to root it essentially in mixture modeling, with the idea that we're going to come up with a way to partition data to come up with an infinite probability measure that has a tree structured topology an what's going to be about this is that the trees are going to be able to have unbounded width, an unbounded depth, and as I mentioned before there, it's going to allow for data limit internal nodes in these trees, and and then on top of that, if we imagine the resulting distribution over data, it's going to be infinitely exchangeable, which is.",
            "A nice property to have that I won't go into detail about an at the end will be able to use Markov chain Monte Carlo for inference, and I'll show you some examples of applying this to image and text data."
        ],
        [
            "OK, so our starting point here is going to be the Dirichlet process mixture model, something that I think most of you are probably already familiar with.",
            "It's just like the standard kind of finite Dirichlet prior mixture model, except that it has an infinite number of components and kind of the main.",
            "The main trick to making this work is coming up with the mixture weights essentially so."
        ],
        [
            "And this this infinite vector pie here that is there all non negative they sum to one, but it's infinitely long and one really nice and intuitive way to construct this thing for the original process is via something called the stick breaking process and the typical stick breaking process.",
            "We begin with a with a stick of unit length with which I'm showing here in this with this Gray bar."
        ],
        [
            "And what we're going to do is draw a random beta variant and break the stick at that location.",
            "OK, and when we do this, we keep the piece on the left.",
            "That's here in red, and we're going to call that the weight of the first mixture component.",
            "Then after we do that, we're going to."
        ],
        [
            "We take the piece on the right and we're going to break that again again with another beta variant from the same distribution, and we're going to keep the peace that results on the left.",
            "Here again in red, and we're going to call that the weight of the second mixture component, and we recurse again into."
        ],
        [
            "Right and we do this over."
        ],
        [
            "Over break and then keep the piece on the left.",
            "Recurse into the piece on the right."
        ],
        [
            "And what we wind up with."
        ],
        [
            "After we do this is an infinite partition of the unit interval that we can treat as a as a random discrete distribution over over an infinite number of outcomes, and this is a very nice idea, and it allows for a lot of flexible, interesting nonparametric Bayesian models.",
            "But in some ways, it's kind of it's kind of unsatisfying because it's very flat.",
            "Typically when we use this in a nonparametric Bayesian mixture model, what we do is we draw the parameters that are associated with each of these mixture components independently from some underlying base measure and.",
            "And that's nice, but this is.",
            "Pretty strong assumption saying that all of these things are independent.",
            "So what?"
        ],
        [
            "Work on tree structured stick breaking processes is about is introducing additional structure into these components, and so and as I said, we do this with stick breaking by adding some new features essentially to the vanilla Dursley Process style stick breaking process, so as before we're going to start with the stick of unit length and."
        ],
        [
            "Going to break it with a draw from a beta variant.",
            "Just like with the battery, it just like we would before in the vanilla kind of case.",
            "But now instead of just recursing directly into the right hand side."
        ],
        [
            "We're actually going to do is break that piece several times according to say a symmetric jewishly distribution or something.",
            "In this case I'm breaking into 3 blue pieces here on the right.",
            "Now we."
        ],
        [
            "Curse into each of these pieces separately and break sticks within each one of them."
        ],
        [
            "Then we do the same thing again, recursing further into the each of these three pieces on the right, breaking them into.",
            "In this case, 3 little blue pieces."
        ],
        [
            "And we continue this on and so what's going to happen now?"
        ],
        [
            "Is we're going to still?"
        ],
        [
            "Wind up with an infinite partition of the unit interval, but it's going to have the nice property that we can.",
            "We can think about it as having a tree structure topology.",
            "OK, so this is now still an infinite discrete distribution, but with this additional interesting semantics.",
            "Now an in fact one thing we could do that to kind of extend this even further and make this more interesting is actually rather than breaking it, just say three times.",
            "We could actually interleave in here another full GM style stick breaking process, so the kind of breaking process we use for the Dursley distributed directly process, and now we wind up with trees that are both infinitely wide and infinitely deep.",
            "OK, so this is very flexible way to specify a bunch of a bunch of mixture weights in an infinite model that also has this additional interesting structure."
        ],
        [
            "Now to give you an idea of what trees you would get out of out of this if you applied at the data, we can talk about drawing data actually from this from this random discrete tree structure distribution, and in fact we can construct an urn scheme.",
            "The kind of the black one with Queen style thing that you often think of it as being the Chinese restaurant process.",
            "But we could do something very similar in this case, and so here are some pictures, six of 'em of 50 data represented by these darkened squares here that have been drawn essentially from.",
            "A random distribution drawn from this tree structured stick breaking process and you can see that we can turn the different knobs of the stick breaking process and wind up with different essentially inductive biases for this problem.",
            "So in the bottom left we have this long kind of dangly tree, and the top right we have something where all of the data live right near the top essentially, and there's only there's only a couple of represented branches, so the point is we can very, very these knobs and get different kinds of behavior."
        ],
        [
            "Now I talked about the Dursley process being kind of providing you with essentially kind of a flat prior for the mixture components, and so far I've just described a way to come up with this infinite random measure.",
            "But of course we're going to need to tie the parameters together in a useful way in this mixture model of is going to do anything interesting for us, and so the really intuitive thing to do is just to treat this tree topology that we've constructed as a big directed graphical model.",
            "Alright, so the idea is that all of each of the each datum that we're trying to model is going to live at some node in the tree.",
            "And the parameters at that node are going to model the data, but they're also going to provide a prior for the children that sent immediately beneath it, and so as a result will get this kind of diffusion down the tree where the children are noisy versions of the parents, and we hope to express the intuition that I mentioned earlier that we get more specialization as we go further down the tree."
        ],
        [
            "So was kind of all these different pieces.",
            "We now have enough to actually go and think about data, and we do inference in this with Markov chain Monte Carlo.",
            "And it turns out that actually most of the things that we need to do a relatively straightforward extensions of the kind of Markov chain Monte Carlo that we do for standard nonparametric Bayesian models.",
            "The only thing that's a little bit fancy is that of course we need to assign the data to the different nodes in the tree and sample from the posterior over that an we have an auxiliary variable slice sampling scheme that allows us to do that without having to specify intermediate tuning parameters.",
            "And I'm not going to detail about this, but so if you'd like to chat about this later, come find me or at the poster tonight.",
            "Or of course, at the."
        ],
        [
            "Instead, let's look at data because the data is what really makes this fun.",
            "So we applied it to some image data.",
            "In this case 50,032 by 32 color images.",
            "These data are a subset of the 80,000,000 tiny images data set.",
            "And they were labeled by some folks at Toronto in two 100 classes.",
            "And what makes it nice for this problem is that these hundred classes aggregate into 20 super classes, and we don't use the labels in the unsupervised hierarchical clustering type stuff that we do here.",
            "But the idea was to use the data set that we think probably has some built-in hierarchy.",
            "OK, so."
        ],
        [
            "So one thing I should note is that we actually we actually didn't throw the pixels of these of these images at this nonparametric Bayesian model, but instead boiled off the pixels into 256 dimensional binary vectors, and these binary vectors came from some independent work by Alex Cruz Zewski, who is a grad student of Geoff Hinton, and he built a deep autoencoder to extract to extract features for some work he's doing on image retrieval, and this is really fun stuff, and it's these features are fantastic.",
            "We found them really useful for this, and so I just want to plug to that and you can, I think, grab the codes from his.",
            "From his website or or read his tech report on this.",
            "In any case, what we did is we modeled these binary features using a product of Bernoulli distributions at each of the each of the nodes in the tree.",
            "And then at that point we."
        ],
        [
            "Essentially just turn the crank of RMC and then we get a tree of 50,000 images.",
            "That isn't going to appear on a slide here, but what we can do is zoom in and kind of look at how the tree reflects variations."
        ],
        [
            "In"
        ],
        [
            "In say color and texture and different things so we can zoom in on some."
        ],
        [
            "Images that are, say, kind of things about."
        ],
        [
            "With C and Sky in them and we can zoom."
        ],
        [
            "Further and look at different kinds of."
        ],
        [
            "Life, for example, there's some.",
            "I think this image.",
            "If people can see it, it has some images of baby turtles in one of the clusters, an sharks and another cluster."
        ],
        [
            "And we can see."
        ],
        [
            "Around and."
        ],
        [
            "Different parts of the tree."
        ],
        [
            "For example, this has a bunch of different fruit, so some nodes have things like apples and oranges and pears, and then there's also a bunch of dinner plates because their color colorful and round I guess, and kind of look like fruit."
        ],
        [
            "Hadn't really occurred before we."
        ],
        [
            "Also, zoom in on."
        ],
        [
            "A place."
        ],
        [
            "It has some fun little spindly legged things, so things like picnic tables and chairs but also camels which I guess have long spindly legs.",
            "If you scale them up or down.",
            "So on the left there in the Dinosaur."
        ],
        [
            "And."
        ],
        [
            "And we can also."
        ],
        [
            "Zoom around and find."
        ],
        [
            "An area that has some flower."
        ],
        [
            "Ursanne, pretty sunflowers."
        ],
        [
            "Some things, so it's discovering some interesting structure and taking advantage of this.",
            "This feature representation too.",
            "I mean, it seems to mostly do color, but obviously it's capturing some interesting shape and texture as well.",
            "Now at this point, I know what you're wondering.",
            "You're wondering what is at the top of this tree."
        ],
        [
            "English here."
        ],
        [
            "We have this big tree and I'll talk about exemplars and and so if we.",
            "The idea that at the top of the tree that we have some sort of Canonical may."
        ],
        [
            "The very deeply philosophical image that represents all images in the world somehow, and this is super important.",
            "And we're just going to learn about life from this unfortu"
        ],
        [
            "Only it's lawn mowers.",
            "So I mean, maybe it turns out that lawnmowers are are really deeply philosophical.",
            "I don't know, but it's not the most satisfying thing in the world, and I should say, by the way, that this tree you can come by the poster I printed out a bigger version of this that you can come and check out, and you can see the lawnmowers right at the top if you want."
        ],
        [
            "So let's move on.",
            "Another thing we did.",
            "We looked at.",
            "We looked at text modeling, so we modeled the NIPS corpus using LDA style topic model and a lot of you worked in this kind of stuff before, so I won't go into too much details.",
            "But but essentially we have a."
        ],
        [
            "Topic is a distribution of words that we imagine the document is described by a distribution over over these topics and that the words are all exchangeable.",
            "And like I said, we applied it to the NIPS corpus with the idea that every node in the tree now is going to have a distribution over topics.",
            "So the cartoon here is we have the thing on the bottom right, which is.",
            "So we have three topics are red, green and blue and we have 6 words and each topic is providing distribution and then every node has a distribution over these topics and so it owns some documents.",
            "And then also it's providing a prior for it's for its children and so you get this diffusion down the tree, essentially of what we think of as chained multi directly multinomial distributions.",
            "And just like with all this image data we could apply this."
        ],
        [
            "To this corpus and 1700 or so documents, and it's a little less satisfying 'cause it's really hard to visualize what a bunch of what a bunch of documents cluster as but."
        ],
        [
            "We can zoom in on different bits of this and."
        ],
        [
            "And look at the statistics for example so we can look at the most common words at each node and we can look at the most common authors.",
            "And then I've also plotted a histogram of the proceedings in which those documents appeared and what you hope to see is some interesting temporal coherency that reflects the kind of our intuitive notions about what this kind of tree structure is hoping to discover and."
        ],
        [
            "Again, you can look at this at the poster."
        ],
        [
            "And a larger version."
        ],
        [
            "And if you know more about the sort of scientific, cultural history of nips than me, then I'm sure you can maybe identify interesting aspects.",
            "Maybe here's some one of these nodes I think has some interesting increase.",
            "Maybe reflects increasing popularity.",
            "Bayesian methods.",
            "We can also."
        ],
        [
            "Zoom over to."
        ],
        [
            "The."
        ],
        [
            "To the reinforcement learning node and see how it became more popular.",
            "So this is just the first 12 years.",
            "OK."
        ],
        [
            "So wrapping up basically what we've done is constructed a nonparametric Bayesian way to construct two construct latent trees over our data.",
            "Kind of phrasing it as a mixture model.",
            "That's going to allow us to talk about trees that are infinitely wide and infinitely deep, and where data can live it at internal nodes, and it results in nice, exchangeable, infinitely exchangeable distribution over data, and we're able to use MCMC to perform inference, and we've had some success applying this to image and text data.",
            "Let me just take a quick second to think some folks.",
            "So I mentioned Alex providing his image retrieval codes.",
            "That and that was very helpful.",
            "Also like to thank Kurt Miller, Ann Hanna Wallach and Sinead Williamson for really valuable discussions and also Ian Mareanie YT for good advice on the Markov chain Monte Carlo.",
            "So thank you all for listening.",
            "Well so.",
            "So the question is, can we compare this quantitatively to other hierarchical clustering approaches, and so to do that would of course need to layer an interesting objective function over it, and a lot of times and unsupervised learning, unless you invent an interesting, say discriminate task or something, then it can be a little hard to do and the answers.",
            "So the short answer is no, we haven't.",
            "What we have done is examined the predicted performance of the topic model as compared to sort of like an LDA baseline.",
            "Ann it's we can talk about this.",
            "More of the poster, but it's.",
            "It seems to get reasonable predicted performance Max.",
            "Ryan come hello, coming back to the lawnmower is at the top of the so is it necessary that all the data is, you know, is also occupying the internal nodes?",
            "Or can you put all the data at the leaves and let it sort of come up with abstractions in the at the higher levels?",
            "OK, so so can we have data at at kind of the bottom?",
            "Well, the idea is that this tree actually goes infinitely deep, and so as you break these sticks, the things just get really really small, and so in a sense there aren't really leaves in this tree.",
            "Precisely everything is an internal node, and the data always lives at one of those.",
            "Now you can change that.",
            "You can turn the knobs of the stick breaking process to change the what you how you would like to specify essentially the prior over the depth so that any given data that you.",
            "Data that you draw, how far down the tree does it want to appear?",
            "But there's not really an ocean of leaves.",
            "So this is maybe just the same question, but I'm really unclear about what the semantics means of parent child relationship here when I have when we're talking about objects.",
            "I'm sorry when we're talking about images or text.",
            "You know what?",
            "What can you give an intuition about the semantics of a parent versus child relationship of two documents?",
            "Well, I mean, I think the idea really is that the parent is in some way A is providing an exemplar or the data that live at the parent are providing exemplars for data that live at the children.",
            "So this is something we sometimes try to do in clustering is, rather than represent state centers of clusters as abstract as abstract places in the space we actually try to use examples from our data set.",
            "To provide exemplars, and this is trying to trying to get that idea right, so there a little bit like say meteoroids or somethings cluster centers.",
            "Yeah, so that's one way to view it.",
            "So we could imagine that this is discovering meteoroids that allows these things to be to be sort of arbitrary, arbitrarily complex in terms of meteoroids and meteorites and things.",
            "Well, thank you all very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the big picture here is essentially going to be one of hierarchical clustering where we're going to allow for data to live at internal nodes on the tree.",
                    "label": 1
                },
                {
                    "sent": "So essentially we're going to imagine that our data have some underlying hierarchical structure.",
                    "label": 0
                },
                {
                    "sent": "That's an observed an what we're going to do is try to recover this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree you can motivate this in a couple different ways by thinking about for example images where we might imagine that images that live at higher levels of the tree might in some ways be exemplars for those lower in the trees.",
                    "label": 0
                },
                {
                    "sent": "Maybe kind of Canonical examples of large classes, or in the case of something like document modeling, say scientific papers, we might imagine that the papers that live higher in the tree, maybe there are similar papers.",
                    "label": 0
                },
                {
                    "sent": "Maybe they gave rise to whole subfields or a lot of sort of other specialized papers.",
                    "label": 0
                },
                {
                    "sent": "That would then appear farther down in the tree as descendants of these Seminole Papers, so that's kind of the general motivation here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The approach we're going to take is going to be a nonparametric Bayesian one, and we're going to root it essentially in mixture modeling, with the idea that we're going to come up with a way to partition data to come up with an infinite probability measure that has a tree structured topology an what's going to be about this is that the trees are going to be able to have unbounded width, an unbounded depth, and as I mentioned before there, it's going to allow for data limit internal nodes in these trees, and and then on top of that, if we imagine the resulting distribution over data, it's going to be infinitely exchangeable, which is.",
                    "label": 0
                },
                {
                    "sent": "A nice property to have that I won't go into detail about an at the end will be able to use Markov chain Monte Carlo for inference, and I'll show you some examples of applying this to image and text data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our starting point here is going to be the Dirichlet process mixture model, something that I think most of you are probably already familiar with.",
                    "label": 1
                },
                {
                    "sent": "It's just like the standard kind of finite Dirichlet prior mixture model, except that it has an infinite number of components and kind of the main.",
                    "label": 0
                },
                {
                    "sent": "The main trick to making this work is coming up with the mixture weights essentially so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this this infinite vector pie here that is there all non negative they sum to one, but it's infinitely long and one really nice and intuitive way to construct this thing for the original process is via something called the stick breaking process and the typical stick breaking process.",
                    "label": 0
                },
                {
                    "sent": "We begin with a with a stick of unit length with which I'm showing here in this with this Gray bar.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we're going to do is draw a random beta variant and break the stick at that location.",
                    "label": 0
                },
                {
                    "sent": "OK, and when we do this, we keep the piece on the left.",
                    "label": 0
                },
                {
                    "sent": "That's here in red, and we're going to call that the weight of the first mixture component.",
                    "label": 0
                },
                {
                    "sent": "Then after we do that, we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take the piece on the right and we're going to break that again again with another beta variant from the same distribution, and we're going to keep the peace that results on the left.",
                    "label": 0
                },
                {
                    "sent": "Here again in red, and we're going to call that the weight of the second mixture component, and we recurse again into.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right and we do this over.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over break and then keep the piece on the left.",
                    "label": 0
                },
                {
                    "sent": "Recurse into the piece on the right.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we wind up with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we do this is an infinite partition of the unit interval that we can treat as a as a random discrete distribution over over an infinite number of outcomes, and this is a very nice idea, and it allows for a lot of flexible, interesting nonparametric Bayesian models.",
                    "label": 0
                },
                {
                    "sent": "But in some ways, it's kind of it's kind of unsatisfying because it's very flat.",
                    "label": 0
                },
                {
                    "sent": "Typically when we use this in a nonparametric Bayesian mixture model, what we do is we draw the parameters that are associated with each of these mixture components independently from some underlying base measure and.",
                    "label": 0
                },
                {
                    "sent": "And that's nice, but this is.",
                    "label": 0
                },
                {
                    "sent": "Pretty strong assumption saying that all of these things are independent.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work on tree structured stick breaking processes is about is introducing additional structure into these components, and so and as I said, we do this with stick breaking by adding some new features essentially to the vanilla Dursley Process style stick breaking process, so as before we're going to start with the stick of unit length and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to break it with a draw from a beta variant.",
                    "label": 0
                },
                {
                    "sent": "Just like with the battery, it just like we would before in the vanilla kind of case.",
                    "label": 0
                },
                {
                    "sent": "But now instead of just recursing directly into the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're actually going to do is break that piece several times according to say a symmetric jewishly distribution or something.",
                    "label": 0
                },
                {
                    "sent": "In this case I'm breaking into 3 blue pieces here on the right.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Curse into each of these pieces separately and break sticks within each one of them.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we do the same thing again, recursing further into the each of these three pieces on the right, breaking them into.",
                    "label": 0
                },
                {
                    "sent": "In this case, 3 little blue pieces.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we continue this on and so what's going to happen now?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we're going to still?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wind up with an infinite partition of the unit interval, but it's going to have the nice property that we can.",
                    "label": 0
                },
                {
                    "sent": "We can think about it as having a tree structure topology.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is now still an infinite discrete distribution, but with this additional interesting semantics.",
                    "label": 0
                },
                {
                    "sent": "Now an in fact one thing we could do that to kind of extend this even further and make this more interesting is actually rather than breaking it, just say three times.",
                    "label": 0
                },
                {
                    "sent": "We could actually interleave in here another full GM style stick breaking process, so the kind of breaking process we use for the Dursley distributed directly process, and now we wind up with trees that are both infinitely wide and infinitely deep.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very flexible way to specify a bunch of a bunch of mixture weights in an infinite model that also has this additional interesting structure.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to give you an idea of what trees you would get out of out of this if you applied at the data, we can talk about drawing data actually from this from this random discrete tree structure distribution, and in fact we can construct an urn scheme.",
                    "label": 0
                },
                {
                    "sent": "The kind of the black one with Queen style thing that you often think of it as being the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "But we could do something very similar in this case, and so here are some pictures, six of 'em of 50 data represented by these darkened squares here that have been drawn essentially from.",
                    "label": 0
                },
                {
                    "sent": "A random distribution drawn from this tree structured stick breaking process and you can see that we can turn the different knobs of the stick breaking process and wind up with different essentially inductive biases for this problem.",
                    "label": 0
                },
                {
                    "sent": "So in the bottom left we have this long kind of dangly tree, and the top right we have something where all of the data live right near the top essentially, and there's only there's only a couple of represented branches, so the point is we can very, very these knobs and get different kinds of behavior.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I talked about the Dursley process being kind of providing you with essentially kind of a flat prior for the mixture components, and so far I've just described a way to come up with this infinite random measure.",
                    "label": 1
                },
                {
                    "sent": "But of course we're going to need to tie the parameters together in a useful way in this mixture model of is going to do anything interesting for us, and so the really intuitive thing to do is just to treat this tree topology that we've constructed as a big directed graphical model.",
                    "label": 1
                },
                {
                    "sent": "Alright, so the idea is that all of each of the each datum that we're trying to model is going to live at some node in the tree.",
                    "label": 0
                },
                {
                    "sent": "And the parameters at that node are going to model the data, but they're also going to provide a prior for the children that sent immediately beneath it, and so as a result will get this kind of diffusion down the tree where the children are noisy versions of the parents, and we hope to express the intuition that I mentioned earlier that we get more specialization as we go further down the tree.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So was kind of all these different pieces.",
                    "label": 0
                },
                {
                    "sent": "We now have enough to actually go and think about data, and we do inference in this with Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that actually most of the things that we need to do a relatively straightforward extensions of the kind of Markov chain Monte Carlo that we do for standard nonparametric Bayesian models.",
                    "label": 0
                },
                {
                    "sent": "The only thing that's a little bit fancy is that of course we need to assign the data to the different nodes in the tree and sample from the posterior over that an we have an auxiliary variable slice sampling scheme that allows us to do that without having to specify intermediate tuning parameters.",
                    "label": 1
                },
                {
                    "sent": "And I'm not going to detail about this, but so if you'd like to chat about this later, come find me or at the poster tonight.",
                    "label": 0
                },
                {
                    "sent": "Or of course, at the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead, let's look at data because the data is what really makes this fun.",
                    "label": 0
                },
                {
                    "sent": "So we applied it to some image data.",
                    "label": 1
                },
                {
                    "sent": "In this case 50,032 by 32 color images.",
                    "label": 0
                },
                {
                    "sent": "These data are a subset of the 80,000,000 tiny images data set.",
                    "label": 1
                },
                {
                    "sent": "And they were labeled by some folks at Toronto in two 100 classes.",
                    "label": 1
                },
                {
                    "sent": "And what makes it nice for this problem is that these hundred classes aggregate into 20 super classes, and we don't use the labels in the unsupervised hierarchical clustering type stuff that we do here.",
                    "label": 0
                },
                {
                    "sent": "But the idea was to use the data set that we think probably has some built-in hierarchy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing I should note is that we actually we actually didn't throw the pixels of these of these images at this nonparametric Bayesian model, but instead boiled off the pixels into 256 dimensional binary vectors, and these binary vectors came from some independent work by Alex Cruz Zewski, who is a grad student of Geoff Hinton, and he built a deep autoencoder to extract to extract features for some work he's doing on image retrieval, and this is really fun stuff, and it's these features are fantastic.",
                    "label": 0
                },
                {
                    "sent": "We found them really useful for this, and so I just want to plug to that and you can, I think, grab the codes from his.",
                    "label": 0
                },
                {
                    "sent": "From his website or or read his tech report on this.",
                    "label": 0
                },
                {
                    "sent": "In any case, what we did is we modeled these binary features using a product of Bernoulli distributions at each of the each of the nodes in the tree.",
                    "label": 1
                },
                {
                    "sent": "And then at that point we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially just turn the crank of RMC and then we get a tree of 50,000 images.",
                    "label": 0
                },
                {
                    "sent": "That isn't going to appear on a slide here, but what we can do is zoom in and kind of look at how the tree reflects variations.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In say color and texture and different things so we can zoom in on some.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images that are, say, kind of things about.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With C and Sky in them and we can zoom.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further and look at different kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Life, for example, there's some.",
                    "label": 0
                },
                {
                    "sent": "I think this image.",
                    "label": 0
                },
                {
                    "sent": "If people can see it, it has some images of baby turtles in one of the clusters, an sharks and another cluster.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different parts of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, this has a bunch of different fruit, so some nodes have things like apples and oranges and pears, and then there's also a bunch of dinner plates because their color colorful and round I guess, and kind of look like fruit.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hadn't really occurred before we.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, zoom in on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A place.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has some fun little spindly legged things, so things like picnic tables and chairs but also camels which I guess have long spindly legs.",
                    "label": 0
                },
                {
                    "sent": "If you scale them up or down.",
                    "label": 0
                },
                {
                    "sent": "So on the left there in the Dinosaur.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zoom around and find.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An area that has some flower.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ursanne, pretty sunflowers.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some things, so it's discovering some interesting structure and taking advantage of this.",
                    "label": 0
                },
                {
                    "sent": "This feature representation too.",
                    "label": 0
                },
                {
                    "sent": "I mean, it seems to mostly do color, but obviously it's capturing some interesting shape and texture as well.",
                    "label": 0
                },
                {
                    "sent": "Now at this point, I know what you're wondering.",
                    "label": 0
                },
                {
                    "sent": "You're wondering what is at the top of this tree.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "English here.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this big tree and I'll talk about exemplars and and so if we.",
                    "label": 0
                },
                {
                    "sent": "The idea that at the top of the tree that we have some sort of Canonical may.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The very deeply philosophical image that represents all images in the world somehow, and this is super important.",
                    "label": 0
                },
                {
                    "sent": "And we're just going to learn about life from this unfortu",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only it's lawn mowers.",
                    "label": 0
                },
                {
                    "sent": "So I mean, maybe it turns out that lawnmowers are are really deeply philosophical.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but it's not the most satisfying thing in the world, and I should say, by the way, that this tree you can come by the poster I printed out a bigger version of this that you can come and check out, and you can see the lawnmowers right at the top if you want.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's move on.",
                    "label": 0
                },
                {
                    "sent": "Another thing we did.",
                    "label": 0
                },
                {
                    "sent": "We looked at.",
                    "label": 0
                },
                {
                    "sent": "We looked at text modeling, so we modeled the NIPS corpus using LDA style topic model and a lot of you worked in this kind of stuff before, so I won't go into too much details.",
                    "label": 1
                },
                {
                    "sent": "But but essentially we have a.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic is a distribution of words that we imagine the document is described by a distribution over over these topics and that the words are all exchangeable.",
                    "label": 0
                },
                {
                    "sent": "And like I said, we applied it to the NIPS corpus with the idea that every node in the tree now is going to have a distribution over topics.",
                    "label": 0
                },
                {
                    "sent": "So the cartoon here is we have the thing on the bottom right, which is.",
                    "label": 0
                },
                {
                    "sent": "So we have three topics are red, green and blue and we have 6 words and each topic is providing distribution and then every node has a distribution over these topics and so it owns some documents.",
                    "label": 1
                },
                {
                    "sent": "And then also it's providing a prior for it's for its children and so you get this diffusion down the tree, essentially of what we think of as chained multi directly multinomial distributions.",
                    "label": 0
                },
                {
                    "sent": "And just like with all this image data we could apply this.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To this corpus and 1700 or so documents, and it's a little less satisfying 'cause it's really hard to visualize what a bunch of what a bunch of documents cluster as but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can zoom in on different bits of this and.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And look at the statistics for example so we can look at the most common words at each node and we can look at the most common authors.",
                    "label": 0
                },
                {
                    "sent": "And then I've also plotted a histogram of the proceedings in which those documents appeared and what you hope to see is some interesting temporal coherency that reflects the kind of our intuitive notions about what this kind of tree structure is hoping to discover and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, you can look at this at the poster.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a larger version.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you know more about the sort of scientific, cultural history of nips than me, then I'm sure you can maybe identify interesting aspects.",
                    "label": 0
                },
                {
                    "sent": "Maybe here's some one of these nodes I think has some interesting increase.",
                    "label": 0
                },
                {
                    "sent": "Maybe reflects increasing popularity.",
                    "label": 0
                },
                {
                    "sent": "Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "We can also.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zoom over to.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the reinforcement learning node and see how it became more popular.",
                    "label": 0
                },
                {
                    "sent": "So this is just the first 12 years.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So wrapping up basically what we've done is constructed a nonparametric Bayesian way to construct two construct latent trees over our data.",
                    "label": 0
                },
                {
                    "sent": "Kind of phrasing it as a mixture model.",
                    "label": 0
                },
                {
                    "sent": "That's going to allow us to talk about trees that are infinitely wide and infinitely deep, and where data can live it at internal nodes, and it results in nice, exchangeable, infinitely exchangeable distribution over data, and we're able to use MCMC to perform inference, and we've had some success applying this to image and text data.",
                    "label": 1
                },
                {
                    "sent": "Let me just take a quick second to think some folks.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned Alex providing his image retrieval codes.",
                    "label": 0
                },
                {
                    "sent": "That and that was very helpful.",
                    "label": 1
                },
                {
                    "sent": "Also like to thank Kurt Miller, Ann Hanna Wallach and Sinead Williamson for really valuable discussions and also Ian Mareanie YT for good advice on the Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "So thank you all for listening.",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we compare this quantitatively to other hierarchical clustering approaches, and so to do that would of course need to layer an interesting objective function over it, and a lot of times and unsupervised learning, unless you invent an interesting, say discriminate task or something, then it can be a little hard to do and the answers.",
                    "label": 0
                },
                {
                    "sent": "So the short answer is no, we haven't.",
                    "label": 0
                },
                {
                    "sent": "What we have done is examined the predicted performance of the topic model as compared to sort of like an LDA baseline.",
                    "label": 0
                },
                {
                    "sent": "Ann it's we can talk about this.",
                    "label": 0
                },
                {
                    "sent": "More of the poster, but it's.",
                    "label": 0
                },
                {
                    "sent": "It seems to get reasonable predicted performance Max.",
                    "label": 0
                },
                {
                    "sent": "Ryan come hello, coming back to the lawnmower is at the top of the so is it necessary that all the data is, you know, is also occupying the internal nodes?",
                    "label": 0
                },
                {
                    "sent": "Or can you put all the data at the leaves and let it sort of come up with abstractions in the at the higher levels?",
                    "label": 0
                },
                {
                    "sent": "OK, so so can we have data at at kind of the bottom?",
                    "label": 0
                },
                {
                    "sent": "Well, the idea is that this tree actually goes infinitely deep, and so as you break these sticks, the things just get really really small, and so in a sense there aren't really leaves in this tree.",
                    "label": 0
                },
                {
                    "sent": "Precisely everything is an internal node, and the data always lives at one of those.",
                    "label": 0
                },
                {
                    "sent": "Now you can change that.",
                    "label": 0
                },
                {
                    "sent": "You can turn the knobs of the stick breaking process to change the what you how you would like to specify essentially the prior over the depth so that any given data that you.",
                    "label": 0
                },
                {
                    "sent": "Data that you draw, how far down the tree does it want to appear?",
                    "label": 0
                },
                {
                    "sent": "But there's not really an ocean of leaves.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe just the same question, but I'm really unclear about what the semantics means of parent child relationship here when I have when we're talking about objects.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry when we're talking about images or text.",
                    "label": 0
                },
                {
                    "sent": "You know what?",
                    "label": 0
                },
                {
                    "sent": "What can you give an intuition about the semantics of a parent versus child relationship of two documents?",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, I think the idea really is that the parent is in some way A is providing an exemplar or the data that live at the parent are providing exemplars for data that live at the children.",
                    "label": 0
                },
                {
                    "sent": "So this is something we sometimes try to do in clustering is, rather than represent state centers of clusters as abstract as abstract places in the space we actually try to use examples from our data set.",
                    "label": 0
                },
                {
                    "sent": "To provide exemplars, and this is trying to trying to get that idea right, so there a little bit like say meteoroids or somethings cluster centers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's one way to view it.",
                    "label": 0
                },
                {
                    "sent": "So we could imagine that this is discovering meteoroids that allows these things to be to be sort of arbitrary, arbitrarily complex in terms of meteoroids and meteorites and things.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you all very much.",
                    "label": 0
                }
            ]
        }
    }
}