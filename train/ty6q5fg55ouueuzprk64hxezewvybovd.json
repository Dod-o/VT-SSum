{
    "id": "ty6q5fg55ouueuzprk64hxezewvybovd",
    "title": "TD Learning",
    "info": {
        "author": [
            "Richard S. Sutton, Department of Computing Science, University of Alberta"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_sutton_td_learning/",
    "segmentation": [
        [
            "Thank you everybody.",
            "Thank you jewel.",
            "It's true.",
            "I'm kind of the I have the most Gray hair here.",
            "I've been around for a long time and and I want to kind of talk about the Longview a bit actually, so this is the challenge.",
            "First, let me say though, it's really exciting just to talk to all you young people learning about the field, learning about, want to get into AI and reinforcement learning.",
            "And it's just like it's a special opportunity.",
            "I really appreciate it and.",
            "So when I do this, I have to think of what can I try to communicate to you because.",
            "You know we have really a brief amount of time and.",
            "The field is immense, written a whole book on the subject, and So what we what can I try to do now?",
            "My topic normally today is temporal difference learning, and certainly I'm going to talk about that.",
            "But I also want to speak to you as young people entering the field and just tell you what I think about it from a sort of a longer view and.",
            "Well so."
        ],
        [
            "I think the first thing to be said along those lines is that this is a really special time an we can look forward now to doing an amazing thing which is maybe understanding the way the mind works.",
            "Way intelligence works.",
            "This is like a monumental event, not just you know this century.",
            "You know, for thousands of years, maybe in the history of the Earth when intelligent beings, animals.",
            "Things that can replicate themselves finally come to understand the way they work well enough to buy, design, create intelligence, and so the big thing that's going to happen is that we're going to come to understand how the mind works, how intelligence can work, how it does work, and just the fact of our understanding.",
            "It is going to change.",
            "The world is going to change.",
            "Obviously there will be lots of applications, but just it'll change ourselves.",
            "Our view of ourselves.",
            "What we do, what we play with what we work at.",
            "Everything, it's a big event.",
            "It's a big event and we have to.",
            "We should keep that in mind.",
            "And as I say you know I'm not saying it's going to happen, you know?",
            "Tomorrow it's not.",
            "But you know, 10 years it could happen 30 years it could happen, and in the great sweep of history, that's a small amount of time.",
            "It's going to happen within your lives with high probability.",
            "So.",
            "So first thing we have to ask is why is this happening and I think it's really comes down to like Moore's Law.",
            "OK, so I took a slide from Kurz.",
            "While this is just standard slide of the increasing computation per dollar and it's just reaching an appoint where it starts to compete with the computation that we can do with our own brains with natural systems.",
            "OK, that's happening now and it's just going to continue increasing.",
            "Dramatically, I'm sure you've seen these kind of slides here.",
            "We have time years along the bottom and on the other.",
            "We have computations per second per dollar.",
            "OK, and you see, it's a log scale, so of course a straight line would be exponential increase and curves while argues that it's actually slightly super exponential, but the point is, the computations become available to us is become enormous, and that changes things.",
            "It's the rise of deep learning and all that is purely because the GPU's and computation getting cheaper.",
            "And that will only continue and it will become more extreme as a vast increase computation, so that alters everything and it really is profound, and I'm just going to try to slide to say what I conclude from this, as I think it's an answer to like a question for AI.",
            "This had for 6070 years.",
            "OK, so the answer I'm not going to.",
            "I'm just going to do a couple of slides like this, but I guess I should one last bit of prefaces that my my talk today is going to be like half.",
            "Examine these big things.",
            "OK and like I'm starting to do now in half.",
            "Like really small things, we go back to the foundations, real stuff.",
            "That's basic that we need to understand.",
            "OK, I think the details.",
            "The big pictures is important, but the details also really matter.",
            "And like we heard a lot of specific algorithms early today, exactly how they work is really important and the details matter.",
            "But the big picture matters.",
            "Sometimes we lose sight of the big picture.",
            "Sometimes something so obvious that we overlook it, I think.",
            "The computation is as long that lines and its implication implication in a phrase is that methods.",
            "That scale with computation.",
            "Are the future of."
        ],
        [
            "I. OK, and that's"
        ],
        [
            "That's by scale computation.",
            "I mean, as we get more computer power, those methods become more powerful, and this has not always been true.",
            "True, but it certainly is true of learning and search methods.",
            "These are what we call general purpose methods, and it's the answer to this.",
            "One of the oldest questions in AI for 60 years old.",
            "Do we want weak methods?",
            "That's what they call them.",
            "In the old days they call general purpose Methods Week 'cause they just.",
            "They just use the data or use computation.",
            "They can leave their their general purpose in general there week.",
            "OK, that's what they call them in those days, the strong ones were the ones that we lose human knowledge and human expertise in human insight to make their systems so much better.",
            "OK, now probably you guys are thinking that's crazy.",
            "The general purpose ones.",
            "Are you thinking that with me general purpose?",
            "How many grateful human insight?",
            "Sum of all.",
            "OK, yeah, that's that's that's nice.",
            "Compromising sort of position.",
            "And so all today I'm going to try it all on this big part of the talk.",
            "I'm going to try to talk about.",
            "I'm going to present strong views.",
            "OK, really, maybe you should do compromise and nuances, but it's good to talk about strong views because they give you a working hypothesis to give you a point of view and you know you can say this is a strong but you don't have to believe it.",
            "You have to say, well, that's the strong point of view that I should be thinking about.",
            "And then maybe you have several of them anyway, so I'm going to present the strongly this all this question has been answered.",
            "It's been imagined in favor of the week methods.",
            "And.",
            "Yeah, Ann.",
            "So I don't want you to.",
            "I could talk about this all day, but I'll refrain from it.",
            "I'll just note the next thing to say is that you make your thinking you're good you're thinking you're into deep learning or reinforcement learning.",
            "So you're on the right side of history.",
            "But I'm not sure that's correct, OK?",
            "'cause we take things like any kind of supervised learning, even model free reinforcement thing.",
            "I love more than anything, right?",
            "It's only weekly scalable.",
            "If I had, you know, a million times this computation, I'm still limited in my model for reinforcement, how fast I can gather data and I'm only learning value function may be a policy.",
            "What's the big deal?",
            "That's a tiny object in some map from states to what to do.",
            "OK, and that's not a big thing and we can help.",
            "How many features you have?",
            "It's and super certainly for supervised learning is only weakly scalable because it requires people to provide datasets and they become the bottleneck.",
            "If you have, you know you get people to label things on the web, things scale.",
            "But it only weakly only as fast as you can gather the data and eventually that becomes a bottleneck.",
            "You really want to be able to learn from raw data.",
            "You want to be scale."
        ],
        [
            "Oh OK, so.",
            "If these things that we love are not scalable, what is scalable would be fully scalable.",
            "Well, my answer is simple.",
            "It's what I call prediction.",
            "Learning prediction.",
            "Learning means learning to predict what will happen OK, and so it's the unsupervised supervised learning because we have targets where supervised learning we have we.",
            "We just wait and see what does happen and that's our target, OK?",
            "And so you don't need to have human labeling or you have a target, but you don't need a human to provide it, just get it from the data so you see unsupervised supervised learning.",
            "But anyway, it's definitely the scalable model, free learning.",
            "And maybe it's the scalable model free learning.",
            "And prediction learning is at the heart of all of our control methods.",
            "We learn value functions.",
            "I think maybe that sort of argument.",
            "I want the big argument about temporal difference learning is that it's the scalable model free method.",
            "And of course I haven't given you even a step towards that yet.",
            "I'm just saying that the idea of predicting.",
            "Predicting of having learning that predicts.",
            "Is the key problem.",
            "It's what we should be thinking about an.",
            "You may say, well, deep learning, supervised learning all about prediction learning, but that's not true.",
            "It's all it's about predicting what the other with the label will be, but it's not about predicting overtime.",
            "It's not about predicting where you have to wait and see what happens, and that makes all the difference.",
            "OK, so.",
            "So with all this words, I want to ground us a little bit, remind us with the data what the data looks like, so I put in the slide of.",
            "This is what?"
        ],
        [
            "Real life is like real life is a temporal stream.",
            "We have things like playing soccer and we have to make actions at every moment.",
            "Where maybe a hyena being chased by lion and trying to predict whether it's going to live or die.",
            "Or maybe a baseball player and we are our eyes are watching this tiny little ball flash bias really fast.",
            "I have to swing at just the right moment to hit the ball.",
            "Or maybe you're talking to someone you're trying to predict what they will do.",
            "So this is the data streams that AI should be learning to deal with and so we should always keep this in mind when I say learning to predict well, I think I think that like a hyena trying to protect you know fear.",
            "Fear is your prediction of are you going to die?",
            "OK, so he's trying to predict it several times.",
            "It looks good and bad.",
            "And.",
            "And at the end here it's not looking so good.",
            "OK, so with those prefaces, let me start talking."
        ],
        [
            "Temporal difference learning.",
            "Temporal difference learning.",
            "Temperance learning.",
            "It's a method for learning to predict it's while using reinforcement learning to predict future award value functions, and it's basically the center of the core of many methods that you know about Q learning and Sarsa TD Lambda deep Q networks.",
            "TD Gammon the world Champion backgammon player using deep reinforcement learning from.",
            "15 know 25 years ago.",
            "My God 25 years ago.",
            "Deep reinforcement learning.",
            "1992 but not all reinforcement methods so Alpha go.",
            "It happens not to use TD learning using other kinds of reinforcement learning the helicopter thing that you might have heard that doesn't use it.",
            "And then there's sort of pure like Peter's talk was interesting because he talked about policy based methods, and some of those don't use temporal difference learning, but eventually he would get to it and put it in to make things better.",
            "So it's sort of.",
            "Yeah.",
            "So it's sort of optional.",
            "It feels like it's optional.",
            "Many people say it's optional, but I do want to argue that you do want to use it.",
            "That really you should always be using it, it's ubiquitous.",
            "And OK, it seems how the brain works.",
            "What else?",
            "Oh, now it can be used to predict anything.",
            "It's for general method for prediction learning, not just for rewards.",
            "OK, so don't be fooled by reinforcing.",
            "Read all the TV.",
            "It's all about predicting value functions reward.",
            "But we can use for anything.",
            "And since my talk is just about TD, I want to be sure to think about this general use question.",
            "Not saying that it's key to Alpha.",
            "Need to train value functions that they use to evaluate the states.",
            "Research.",
            "Yeah, I've forgotten exactly the details.",
            "The initial learning system was learned from actual outcomes, actual games and they would go all the way to the final outcome.",
            "They make a prediction, look at position, make a prediction of how my gonna win or lose and they would go ahead and see who actually won the game at the end and they would use who actually won.",
            "Instead of a temporal difference learning, so that's really want to work for.",
            "Do they wait and see who actually won.",
            "They see the outcome or the return.",
            "Or do they do the update it guess from a guess?",
            "OK, let me make that clear, that's mine."
        ],
        [
            "Next slide is a TD learning what is TD learning.",
            "Basically it's learning a prediction from another later prediction.",
            "OK, so if Alpha go was licking position, making prediction and zooming all the games he won and then it's not TD, it's not learning that prediction from a prediction.",
            "On the other hand, if you make a prediction from a position and then you make one move and you see your next prediction.",
            "And you use that next prediction as to form a target.",
            "Then you're doing TD learning.",
            "OK, so the quick word, quick phrases.",
            "We're learning a guest from a guess.",
            "OK, sounds a bit dangerous, doesn't it?",
            "How would constrain it would tie it down OK, but that is the idea we want to learn an estimate from an estimate.",
            "And we have to talk about whether this is good or not OK, the TD error, the TV error is the difference between 2 predictions, two temporally successive predictions, right?",
            "So if you're playing your game, you say I think I'm winning, then you take another move.",
            "You know now I think I'm losing you.",
            "Try to learn from that.",
            "You don't know who actually is going to end yet when you try to learn from the temporal difference in your predictions.",
            "OK, now after that it's the same.",
            "As which are all used to, you just have an error and you send it back proper through whatever you're doing, and so it's just really where does the error come from?",
            "Where does the target come from?",
            "Is the target come from the end of the game, or does the target come from the next prediction?"
        ],
        [
            "OK, so here's the example of TD Gammon, originally 1992, and he had a chest, a back end position.",
            "And he would send that into a neural network which would filter through actually just a single well had many versions stand version was a single hidden layer and you end up with this probability of winning and the error then was the probably winning in one position minus the probably winning the next position.",
            "So we look at the change in the estimated probability of winning and that was used as the error.",
            "That would be backpropagated through it.",
            "And this would learn.",
            "Just stick it in the corner playing against itself, learning from itself from its own.",
            "Trial and error and it came out to be.",
            "Competitive with the world's best players.",
            "Really the best in the world.",
            "OK, so that's familiar.",
            "Now we I'm trying to get to this question.",
            "These questions.",
            "Do you need to use TV?"
        ],
        [
            "This is all this is is to motivate motivation is the most important.",
            "Do I need to use TD learning or can I get away with it because you go to the field now, maybe even in reinforcement learning you'll find a good fraction of the people don't believe in TD LTE and they think they can get away without it.",
            "OK, and so it's a real question.",
            "We should all be asking do we need it?",
            "I want you to understand.",
            "I want you even as as a people learning about the field to be able to engage with this question and know the basic facts pertinent to whether we should.",
            "We need to use TD learning.",
            "OK."
        ],
        [
            "So I will skip over."
        ],
        [
            "Over Atari.",
            "And you've seen that?"
        ],
        [
            "Already so TD learning when do when do we need it?",
            "OK, well it's only relevant and what I multi step prediction problems.",
            "That's the first thing only when the thing predicted is multiple steps in the future.",
            "So obviously if you're predicting the outcome of game, that's multiple steps in the future.",
            "But if you were predicting.",
            "A label or or in Alpha go where the data was the initial data at least was.",
            "Here's a guess and then I'll just see who won the game.",
            "And if you just use who won the game, then it's essentially a one step prediction, OK?",
            "OK, so now I want to say that it's really broadly applicable.",
            "You should.",
            "I said it's only applicable when you have multi step predictions, but really everything you want to do is going to be multi step prediction.",
            "If you want to predict, I have some examples."
        ],
        [
            "Multi step prediction if you want to protect them again if you want to stock market index will be well really you could just predict what's going to be but you get more data on every day after that annual meeting like new predictions.",
            "So you make a mold like repeated predictions about a long term outcome.",
            "If you want to predict will be the next president, you can predict that you know every every day is a new event happens.",
            "If you want to predict who US would go to war against next.",
            "So all these are long-term predictions.",
            "They don't.",
            "They don't jump to the end.",
            "Now, even even if you want to predict a sensory observation, if you want to purchase the very next sensory observation, that would not be multi step prediction.",
            "But but if you were to predict, you know even 10 steps ahead.",
            "Or a discounted measure of the future.",
            "Those are long-term or multi step predictions and we think about go back to the real world, the hyena and Lions or the conversation or messy playing soccer.",
            "He's got to make long-term predictions.",
            "It's not, you know what's the next thing.",
            "It's going to make this goal.",
            "Will I get around that fellow?",
            "Where's the law going to be in a few milliseconds from now?",
            "All these things are multi step predictions now can."
        ],
        [
            "I mean.",
            "Can we treat it?",
            "Can we just use our one step methods like you know?",
            "Sure things happen bit by bit but ignore that.",
            "Just wait and see what happens.",
            "You know like see who won the game and use our one step methods and or can you learn one step model and then compose your model.",
            "OK, and the answer is that we really can't do these things and I want to try to give some sense of that today.",
            "I'm really going to talk mainly about this first question.",
            "Can we think of the multi step case as one big step or do we have to deal with it bit by bit?",
            "And I wanted one slide in the second bit.",
            "The second bit is can we learn one step predictions of the model and then iterate them to get a multi step prediction when you need to?"
        ],
        [
            "Ann I just.",
            "I just want to say that I think it's a trap.",
            "OK, I don't know, really, properly explained this to you, but I think it's a trap.",
            "I think it's enough to model the world to make it like a low level simulation of the world to make like a think trait rule is a Markov decision process and model the transition probabilities or treat the world as a as a engineering model where we just have to learn the velocities in the accelerating effect of the accelerations of reactions and then integrate this low level differential equation.",
            "This is all a trap.",
            "The short term models and then iteration is.",
            "It feels good because we know we know that it can be done perfectly on the one step.",
            "Then it can be done perfectly for however we want to look at the future.",
            "But there's two problems first, so we can't do it perfectly.",
            "And when we do it imperfectly with, we're always going to have an approximation.",
            "Then when we try to iterate them, we get a propagation of errors and com pounding of our errors and we get a useless long term prediction.",
            "And Secondly, of course it's exponentially complex.",
            "Because as we look ahead each step, there will be many possibilities.",
            "The world is stochastic and also our actions.",
            "Maybe we have different choices.",
            "We might look at their actions so it quickly becomes computationally intractable and it will always be computationally intractable to try to look ahead many small steps into the future, like to try to try to iterate your model of physics to get to you, know how much fun will I have going to Montreal and taking the summer school?",
            "OK, it's crazy.",
            "And it just doesn't.",
            "There's no future in that that way of thinking.",
            "It's a trap, and lots of people are, in my opinion, are falling into it, OK?",
            "But let's go back to the other side.",
            "The other side, remember things going backwards here?",
            "We have these two things.",
            "Two ways to get away from TD.",
            "If we don't get away.",
            "We don't like TD.",
            "Can we learn a model and iterate it?",
            "That's the second one or the first one?",
            "Can we think of this one step thing and justice use?",
            "Do the one step thing OK so the ones?"
        ],
        [
            "Up thing.",
            "I'll do it.",
            "I'm going to do it in.",
            "I'm about to transition to my low level part of the talk, but I don't want to try to answer it here, just at the high level.",
            "And then maybe we can even take questions after this.",
            "This high level part of the talk.",
            "Can't we just use our familiar one step supervised learning method and reinforcing?",
            "These are known as Monte Carlo methods.",
            "I mean, just roll it out or whatever, see what happens.",
            "An use that what happens is a target.",
            "OK, so this has costs.",
            "Number one causes is you have to make your making this prediction.",
            "Then you're rolling it out to the end and really going to make a prediction every moment in time.",
            "So you've got to remember all whole mess of predictions as you as you go out to the end to see the outcome.",
            "Or really, if you have returns, you get the outcomes of different different steps, different outcomes you have to relate them back to the to the earlier situations.",
            "It's horribly complex, it's nasty.",
            "It's first of all you need to remember all the things you did.",
            "Think about yourself.",
            "Maybe there is a lie in there you're trying to make a good prediction.",
            "OK, and what do you have?",
            "You have all this stuff swirling around you.",
            "You have the hyenas running away.",
            "You have glimpse of him, you're feeling of your feet.",
            "Have all this stuff that you can sense, and then you want to relate.",
            "That to how well it's going?",
            "How?",
            "How good you should feel about this.",
            "Chase.",
            "OK, and So what is it sensible to think?",
            "Or just be much better if you can do it now?",
            "If right now when all this stuff is in your mind and in your sensors, learn if it's going well or going poorly and learn now as opposed to wait.",
            "Wait 5 seconds later when you have a whole number large number of frames.",
            "Of different different sensations, different patterns of sensation you've forgotten.",
            "You know if you have to wait 5 seconds till the end of it, it's too late.",
            "You can't remember that.",
            "Whatever you remembered will be a tiny shadow of the real vivid representation you had at the time it happened.",
            "OK. And of course, the computation is poorly distributed, you can't.",
            "Learn now.",
            "So what are you doing now?",
            "Later you find out coming after all the learning then just a poor temporal distribution.",
            "And you can avoid these problems with special methods, and that's what really what TD is about.",
            "Specialized methods for the multi step case.",
            "Another reason that you don't want to wait is that sometimes you never know the target.",
            "Like I don't know.",
            "Let's say you're playing your chest gay man.",
            "There's a fire alarm and you never finish your game, so you never see a final outcome.",
            "But if your TV you know maybe thought you were winning and then you know it was going really poorly and you say you did something bad.",
            "You can learn without waiting till you're checkmated.",
            "Maybe the fire alarm, just for like 1 move away from the checkpoint.",
            "So technically the game never ended and you can learn a lot from your experience so we could try to ignore all these things.",
            "Think of them as nuisances, but I think of them as clues.",
            "These are hints from nature, but how we should proceed?",
            "OK, OK so now."
        ],
        [
            "Now I'm going to get down to it more technically.",
            "But I hope you're starting to see the view I'm trying to present.",
            "We really need to learn from new predictions so that we can do it as we go along.",
            "And I think it's really ubiquitous in kind and all the different kinds of learning we're looking at in AI.",
            "OK so I'm gonna use notations a little bit different than what we've heard earlier today.",
            "That's what I call my new notation used in the second edition of the Reinforcement Learning textbook.",
            "Big thing is that we're trying to use the status Sticks Statisticians convention that random variables are capital letters and instances are lower case letters, so all of the things that happen that make up life are capital letters, because there are the random events that actually happened or not possibilities.",
            "There would ever happen, so S 0 is the first state, a zero is the first action taken in that state are one is the first reward that depends on that state and taking that action in that state and then at the same time as we get the reward we get the new state.",
            "So I I'd like to give them the same temporal index R1 S one.",
            "They occur together.",
            "They are jointly determined in fact.",
            "OK, and then life goes on and that's the data.",
            "That's all we have in terms of data.",
            "We have a trajectory's and maybe a single trajectory and we're interested in classically in the return and the return.",
            "Is the sum of rewards.",
            "And I'm not going to use Capital R for the return because capital R is actually the actual rewards the sequence of rewards, and so I'm going to find the return.",
            "I need a new letter.",
            "I'm calling a G Capital G 'cause it's a random event the whatever some of future rewards after time T actually was going to call that G of TG of T is the return.",
            "And.",
            "As we note here, if I can use this thing, this dot just means that this is a definition is not a.",
            "Statement of something that's true because it follows from other things that I've said.",
            "It's the definition, so G of T the return is the sum of the feature awards with the discount rate is the most common way of dealing with it.",
            "And if that of course can be written now, we're using quality is not a definition that equals the first reward plus gamma times the sum of all the later rewards.",
            "We're just taking one factor gamma out of all of these guys, so this is no.",
            "Now, no gamma is this one is 1 game right now with this guys two guys.",
            "We just taking one out and then the rest of this is a Psalm of future rewards from from one time step later.",
            "OK so it's a bit like what we started with just like the same thing, it's GMT plus one.",
            "OK, this is just a definite this is this is.",
            "This is this is a true equality here there any return to be written as the first reward plus gamma times the next return OK?",
            "And that is going to be the basis for our temporal difference learning, because we're going to risk going to use this is.",
            "We're going to do this as a target where you need the reward, next reward plus gamma times the next return.",
            "Essentially as a target, I guess that's going to be explained right, right?",
            "Next we look at a state value function OK?",
            "So for using capital letters for the random variables, I can't use capital V for my for the true value function.",
            "OK, it's gotta be lower case.",
            "It's a function of the policy.",
            "So V Pi of X, where X is any particular state.",
            "It's an instance.",
            "Any state lower case S, its value is the expectation of the random variable to return if we started in stat status.",
            "OK, so we can expect the return could be.",
            "Under Pie pie is the policy, so in some way of picking the actions.",
            "Of course the value of state depends what you do if you dance at the top of.",
            "The Grand Canyon.",
            "It might be bad, but if you can sedate Lee, walk up to the railings, it's good.",
            "So Policy's value fund values depend on policies.",
            "OK?",
            "So then this is the return we can just use the above equation.",
            "The return can be written as the first reward plus gamma times the rest of the return.",
            "And then since we're taking the expectation of this, this is the expected next reward and expected value of the next state.",
            "OK, so that naturally leads to the notion of an error we can use.",
            "We can compare the estimate estimated value of a state at some time to the reward and the estimated value of the next date.",
            "OK, so that's going to be RTD error.",
            "This is what we're going to use to replace.",
            "The normal conventional error.",
            "Now this V. It is a random variable.",
            "This is our estimate.",
            "Their estimate will depend on what happens and so that is random.",
            "The estimated values are a random value or random function, and so it's capital and that's our tier.",
            "You got it.",
            "The TD error is that clear.",
            "Good, OK, so now let's talk about our methods.",
            "I want to contrast supervised learning and remember I said supervised learning is called Monte Carlo in this context.",
            "So what exactly is that?",
            "That's we take our estimated value function for the state that we run into V of S of T. OK, we're going to update it based upon some experience.",
            "OK, here's the experience here.",
            "We are aggressive T and this is the tree of the things that might happen.",
            "Like we might pick either of these two actions, and if we did pick this action.",
            "Either of these two states by to rise.",
            "So yeah, black dots are actions to open.",
            "Dots are states.",
            "So basically this is this is the tree of all the features that might happen.",
            "In this case, we're imagining that there are terminal states.",
            "We're basically adding things up until we reach a terminal state.",
            "So here is a particular trajectory that might happen.",
            "Let's say it did happen from that state.",
            "We went this, this and then we terminated OK, so we now once we've terminated, we know what G is.",
            "We know what the return is and we can do this update rule.",
            "We can compare our estimate.",
            "For the state at time T up here to the actual return, and we make that error and then we do an increment.",
            "I should.",
            "This is the step size Alpha.",
            "There's a number like .1, so we increment org."
        ],
        [
            "Towards this target?",
            "OK, that's that's a standard.",
            "Would be a Monte Carlo learning rule.",
            "Supervised learning rule an.",
            "That's the competition for the TD method.",
            "The simplest TV method looks instead like this."
        ],
        [
            "We only look ahead one step were at FT. We look ahead at one.",
            "We see the reward that happens and the next state that happens and based on those we formed this TD air which is against comparing comparing, we're updating the estimate of this for the state at time T. So we're going there, but we're guessing for that state.",
            "And this new target reward plus gamma times the estimated value of the next date.",
            "OK, now you've probably also heard about dynamic programming."
        ],
        [
            "And you can think you can put dynamic programming in the same figure, the same kind of figure.",
            "And if the dynamic querying version looks like this, 'cause it's not considering a single line through the possible tree, it's considering all possibilities.",
            "It's considering both actions.",
            "And both possible next states, so this way you need the model of the world, because although you know you're probably picking each action that probably the world will give you possible next states will be known only to the world.",
            "But in dynamic programming you assume you know all that.",
            "So in dynamic programming the equation is that the value the estimated value for a state is moved towards the expectation of the first reward and the expectation of gamma times the value of the next state.",
            "OK, so there's this expectation and that's what makes it dynamic.",
            "Programming 'cause it's you see me know all the probabilities you can figure out.",
            "That expectation doesn't give you the answer because your value will still be.",
            "You're still learning a guest from a guess.",
            "You're learning your new estimate.",
            "Still, from your old estimate.",
            "Well, that's a number for him, so so really.",
            "We can say the following.",
            "Not special."
        ],
        [
            "About T methods to say bootstrap and sample.",
            "So bootstrapping is this idea that your target involves a guess, an existing prediction.",
            "OK, so Monte Carlo on Monte Carlo.",
            "The whole point is that it doesn't bootstrap, it's just looking all the way to the end and seeing what the return is.",
            "There's no, there's no estimates playing a role in the return.",
            "Dynamic programming also bootstraps and therefore he says, look ahead one step and look at the expected value of the next state and back it up.",
            "So you're only you're using your estimates.",
            "And your estimates gradually get better.",
            "TD of course also is using your estimate.",
            "Yeah, it's like Monte Carlo and TDR learning methods.",
            "I guess that's my next point.",
            "Learning methods Monte Carlo and TD.",
            "They sample they sample what happens because you don't know how the world works and dynamic programming does not sample.",
            "Just use the expectation.",
            "It assumes you know what will happen, what could happen.",
            "So those are the two basic dimensions, whether you're sampling and therefore learning.",
            "And whether you are bootstrapping, you're using your bootstrapping your estimate from other estimates.",
            "You're learning guesses from guesses.",
            "And so T."
        ],
        [
            "The prediction.",
            "Basically, I'm just saying this is this is the update you saw before rusting.",
            "The Monte Carlo is here and the TV is there, so just.",
            "The contrast is that one, the target is the actual return and the other is the target.",
            "Is this sort of?",
            "One step estimate of what their return will be.",
            "OK, now."
        ],
        [
            "Let's think let's do an example, so here I am.",
            "I'm coming home after working a hard day at the office and I'm trying to guess how long it will take me to get home.",
            "OK, so I'm leaving my office.",
            "It's Friday at 6:00 o'clock.",
            "I have some other features and I make a guess of how long it will take, so I will.",
            "I'm going to guess it'll take 30 minutes to get home.",
            "OK, so and that's my prediction of my total time 'cause I haven't gone.",
            "I'm just starting now so my elapsed time is 0.",
            "Now as I come out of the, I come out of my building, go to the parking lot an I see it's raining OK and it's raining you know it's going to take me longer because everyone drives slower in the rain.",
            "So I think.",
            "Well, first of all, it's already.",
            "I've already spent 5 minutes just getting down from my office into the parking line.",
            "And then I also think it's going to take me longer.",
            "I think it took me 35 minutes from now for a total of 40 minutes.",
            "OK, So what I want you to see.",
            "The first thing you want to see is that.",
            "My guess about how long it's going to take me and my guess is that the total time to go home.",
            "It's constantly changing as I get more information.",
            "I revised my estimates.",
            "OK, so to carry the example through, I get, I get, I start getting my car.",
            "I drive on the highway.",
            "It turns out it didn't take it takes as long as I thought.",
            "I've spent 20 minutes total now and I think it'll only take me 15 more to go home.",
            "It wasn't so bad with in the rain and so that's 35 minutes total.",
            "Now this this car.",
            "This column is my total estimate as it goes up and down, and then I get stuck behind a truck on a secondary road and I think it's gonna take me longer and then I reached my home screen.",
            "I think will take 43 minutes and it does take me 43 minutes.",
            "OK so that's a possible thing that might happen.",
            "A possible trajectory an what I want you to ask is what you might learn from that, OK?"
        ],
        [
            "So if you're doing a Monte Carlo methods, you just say, well, it took me 43 minutes to get home.",
            "That's the answer.",
            "So all my estimates.",
            "My first initial estimate of 30 minutes.",
            "That's going to move towards 43 minutes.",
            "That's the error.",
            "In fact, all of these will be moved up towards 43 minutes, because whatever guess I made at each point in time.",
            "Could be moved towards what actually happened or whatever.",
            "Whatever is remaining in the future at that point.",
            "OK, now if you're using a TD method.",
            "If you're using your guests, going to learn a guest from a guest and then something very different happens.",
            "Even some of the signs change.",
            "So your first prediction will move up because you start out 30 and then after we found out it's raining, so you move up.",
            "But this one, for example, will move down.",
            "And the actual that all the errors are different.",
            "All the errors are different.",
            "The long, long long runs law wash out.",
            "But for all, actual learning is a law is very different.",
            "OK, now I also want you to think about the computational consequences.",
            "OK, if you're doing TDD, then you know when you're here then you go to the next stage.",
            "You get an error and you can update right away.",
            "You can say well, why did I make that prediction?",
            "What are my features there?",
            "How should I change those?",
            "What are the contents of my deep network that led me to make that prediction?",
            "I need to change those.",
            "That's true at each step, and so when you go from here to here to here, you can update this guy and then you can forget about it, but you're never going to update him again.",
            "Where is Monte Carlo.",
            "You have to remember why you made each one of these predictions until you get to the end.",
            "Then you have to go back and say, well, OK, why did I make that one and then and then adjust its weights with knowledge of its feature vector and the contents of your network and so on.",
            "Yeah, and it's terrible.",
            "It's distribution because you all this time you're doing nothing, you're driving home, you can't do any learning.",
            "OK, you can only wait till the end.",
            "You know the answer and then you can go back and look at all the earlier things and learn them.",
            "So the distribution of computation is poor, the memory is poor.",
            "It's it's just kind of inconvenient.",
            "It's much more convenient if you just go along and you think about it, you're in your car.",
            "You're trying to drive home.",
            "You get stuck behind a truck, do you say?",
            "You say you say this is bad, you know I say it's going to take me longer than I thought.",
            "I was too optimistic before.",
            "You don't say, well, you know.",
            "Maybe this truck will disappear and you don't say, hold, hold judgment.",
            "You could hold judgment until you get home, but you know my feeling is I'm learning as I go along and I'm responding to what I see and we actually do learn as we go along, OK?"
        ],
        [
            "So I think I've said these things and TD with Monte Carlo you can be fully incremental learning as you go along.",
            "You can learn before you know the final outcome.",
            "This means you need less memory and less peak computation.",
            "You don't do it all at the end.",
            "You can even learn if you don't.",
            "If you ever find out how long it takes you to actually go home.",
            "Maybe you get a phone call and you're called away for something important and you never find out, but you can learn without knowing the final final outcome.",
            "Now when you do the math, both of these methods will converge and.",
            "But the only question is which is faster?",
            "OK, this is the only question, but it's a big question.",
            "OK so I don't know.",
            "Let's just do a simple experiment and find find out OK, so here's a trivial."
        ],
        [
            "Experiment.",
            "Famous famous I know I did this along time ago.",
            "Just a random walk and it's meant to test the idea of which one of these is better.",
            "So we're going to have 5 States and we're just going to estimate for each state of what the outcome will be OK, this random walk, it takes 50 step right and left and you start in the middle and you go back and forth back and forth, back and forth until you end at one side.",
            "OK, if you ended this side, you get a zero and you do get zeros all along the way for your reward.",
            "But if you know you get a non zeros if you end on the right side you could get a reward of one.",
            "OK so are you with me?",
            "What's the correct prediction?",
            "So the correct prediction?",
            "There's no discounting here, so we're just trying to predict the sum of the rewards up until the end.",
            "What's the correct prediction for the start State C?",
            "You're in C. What's the correct prediction for the expected value of your return?",
            "Squared gamma squared gamma is 1.",
            "So the expected expected return.",
            "So if you if you end if you go blah blah blah blah blah and you end on this side, the return is.",
            "What if you on the other side?",
            "They return SP0, then you start in the middle.",
            "Where do we expect the return to be?",
            "You know, by symmetry it's going to be like .5.",
            "OK and.",
            "State B. I don't know.",
            "It's going to be less than .5.",
            "And stay still less.",
            "Anyone want to guess what they are?",
            "The true values of all the states.",
            "See is definitely 1/2.",
            "What do you think B is?",
            "Guess just gas.",
            "Third, yeah, I think that is 1/3 an in the next one is 1/6.",
            "Yeah, this just go by 61626364656 and those are plotted here.",
            "This'll in line is supposed to be the true values, so a state a true that has a true value.",
            "That's one 6th and State B is the true values 1/3.",
            "This is the true value of 1/2.",
            "And so forth.",
            "And these other lines are the estimated values from applying TD to it.",
            "OK, so TD you do have to care about the initial conditions, 'cause it's making a guess my guess right?",
            "So your guess is you know affect things.",
            "They either pollute things or or brilliantly provide good guesses and the value.",
            "OK, so the initial guess is 0.",
            "So at time at episode 0.",
            "All of the estimated values or extra?",
            "Excuse me.",
            "Yes, all the estimated values are half because since there could be zeros and ones for possible returns, it seems reasonable to start the estimated values all over half of that happens to be right for the middle state, but it's quite a bit long for the other States and then we're going to do.",
            "Episodes were going to learn on every time step, and we're going to update the states according to the D rule.",
            "And after one episode we have this this darker line.",
            "This is the episode number after one episode.",
            "We have these values for estimated values of the five states, right?",
            "So what do you know happened on the 1st episode?",
            "You ended on this side because.",
            "Well, what's going to?",
            "What does this TV rule going to do?",
            "What is what is the TD error going to do?",
            "Let's say if we start in the middle and we could go either way, we move around.",
            "Well, what's the TD error going to be?",
            "It's going to be the reward.",
            "The reward beginnings can be 0.",
            "And then it's gammas winds.",
            "We forget about gamma and then we just basically the change in the value.",
            "OK, and if you went from, say, this state to this state, what's the change in value?",
            "Zero is there all.",
            "Their estimated values are all 1/2.",
            "And so we went from one state from estimate of after another state's estimate of the House.",
            "So as we go all this balancing all around, nothing is going to happen really until we run into one of the ends.",
            "Well, we ran, we can run into this end, or we can run to that end, if we ran into this end, will go from a state that was a half.",
            "To the terminal, say Terminal State, always by definition has a value of 0.",
            "OK.",
            "So so over here, if you if you did this transition, you get a reward of 1.",
            "A reward of 1 the starting state starting state here would be.",
            "1/2 and you get this thing that has zero.",
            "OK, so it'll be one.",
            "Minus 1/2 it'll be positive half anyway.",
            "The estimate of state E would go up.",
            "And that didn't happen.",
            "'cause here's the scientist 80.",
            "It's still at 1/2.",
            "Instead what happened is we ended on this side we went from here that had estimated value half to this thing, which has estimated value 0.",
            "Terminal State and so we went from half.",
            "To zero and RTD, air is minus 1/2.",
            "So we moved down from a half Tord Zero, and you can actually see how far we move.",
            "We actually moved by.",
            "From a half, 2.45 and so our step size Alpha was ten was 110th.",
            "OK. We can understand this algorithm very simple.",
            "And then as you get more episodes, you get closer and closer to the true value after 10 episodes.",
            "If the blue line after 1000 episodes, you get close to the true values, you never get exactly to the true values, 'cause there's always randomness in the individual episode.",
            "An Alpha is non zero.",
            "It's a 10th and so you keep bumping around bubbling around the true values.",
            "So that's.",
            "That's an example.",
            "Now let's compare now."
        ],
        [
            "Monte Carlo versus TD.",
            "On this problem."
        ],
        [
            "OK, and we actually draw whole learning curves now and we have to worry about what's the value of the step size.",
            "OK, So what I'm showing you in this is a learning curve, meaning the X axis is time.",
            "Or episode number.",
            "And the Y axis is some measure of error.",
            "It's actually the root mean square error averaged over the five States and many.",
            "Many iterations of the whole experiment, I guess 100 iterations of the whole experiment.",
            "OK, so.",
            "As I said, they're going down in.",
            "Everything is getting better overtime, but things will not go to zero because we have the step size 110th and or whatever step sizes.",
            "It's always going to be there and we're always going to have some residual error I."
        ],
        [
            "So which one should we look at first?",
            "Maybe Monte Carlo.",
            "It's simpler.",
            "We just Monte Carlo.",
            "You just wait until you know the final return is and then you do your update for all the states that were visited.",
            "So we take a nice slow learning rate, 100th.",
            "We just gradually move down towards 0 error and it's actually this Alpha equals 100.",
            "Very slow actually get the closest in the long long long run.",
            "20 error, but it's very slow so you might want to go faster if we take Alpha is 150th we go down faster, but we're not.",
            "We're going to start to bubble and we tried tried going as fast as.",
            ".04 an we do go.",
            "The initial part is fastest, but now we're definitely bubbling and you can't really do better.",
            "There's no step size which will do better than the ones that are shown here if you try to go faster, you're going to.",
            "You may be a little bit faster, very beginning, but you're going to leverage level out at a higher level.",
            "OK, and for TD we see a similar pattern, but all the numbers are lower.",
            "OK, so here's our one of the slowest one I'm showing here is .05 and it goes it's lowest, but it gets slower and then other ones are faster.",
            "And of course they bubble more and they.",
            "They don't get as low in the long run.",
            "OK, now.",
            "We have long someone may ask you, some of you may be wondering what's going on with that TD stuff, because it seems like they go down and they start to come up again.",
            "Anybody wondering that anybody wondering that?",
            "Yeah, it's it's weird, isn't it?",
            "And.",
            "It's real, it's not a bug in my program.",
            "That's the first thing.",
            "To be sure of.",
            "Yeah it has to do with the fact that actually starting out the estimates at 1/2 is not not stupid.",
            "It's actually a reasonable guess.",
            "If you started all the estimates out at something really bad then you wouldn't see that bounce off the balance as we go down.",
            "Then we seem to balance.",
            "We come up a little bit higher.",
            "And that balance is really interesting.",
            "It has to do with the fact that we do have some some effect of the initial estimates in TD, and whereas we don't really, at least not as much for Monte Carlo.",
            "OK, so so.",
            "This is just a random walk and I've sort of been systematic about the random walk.",
            "And I don't know the big picture is that TD is faster.",
            "OK, there's a balance.",
            "OK, whatever, but it's still much faster.",
            "But this is just one problem.",
            "This is just a random walk.",
            "OK, maybe it's something special about the random walk.",
            "Or maybe if I did on, you know Atari games.",
            "I would get a more fundamental result.",
            "I like to do simple things question.",
            "Doesn't converge.",
            "Oh no, they they all converge.",
            "Even with the non well with a non 0 if you don't reduce the step size then you don't expect anything converge right?",
            "They will converge in the mean OK and all of them will converge to a mean that depends on the step size and higher step size would be higher.",
            "Lower step size be lower convergence point.",
            "Yeah, convergence properties are roughly the same in both cases.",
            "I'm."
        ],
        [
            "I want I want to.",
            "I want to ask now, can I say anything about the general case, not for the random walk?",
            "But can I say general case?",
            "Actually no, no.",
            "I'm going to.",
            "I'm going to do that in a minute.",
            "But first I'm going to the random walk again under this setting called batch updating.",
            "OK, batch updating means we take some training set like 100 episodes or 10 episodes or whatever, and we presented over and over again until it does converge.",
            "So even for finite stepsize, we'll get complete convergence if we repeatedly present the same training set.",
            "OK. Because there is no randomness in random samples are just spinning the same data over and over again and you will converge.",
            "The two methods TD in mind will converge to two different things.",
            "And.",
            "This is for constant Alpha.",
            "These problems converge to different things as long as your step size is small enough, it won't depend on your step size.",
            "Yeah, all step sizes, as long as they're small enough so that you don't diverge, will converge the same thing OK, and they converge to different things.",
            "The two algorithms converge to different things, so we can ask.",
            "On this problem, which one converges to a better thing?",
            "If we present the data over and over again to the algorithm?",
            "OK, Ann."
        ],
        [
            "Here's the results on the random walk.",
            "Again we have.",
            "We have different numbers of different sizes.",
            "Are training sets were increasing that along here, but for each case they say with 50 training set of 50 episodes, we present those 50 over and over and over again until we converge and we measure the asymptotic error.",
            "That is independent of the steps so I can eliminate this effect of the step size just to measure which algorithm is getting me a better result on this problem.",
            "OK. Anti D is faster by this measure.",
            "I mean we're doing a lot more computation.",
            "We have to go to convergence.",
            "We have to repeatedly present things, none of which I like.",
            "But is getting us insight into what the real difference between the two algorithms, so it's like TD is moving towards a better place even on a single example, as suggested by the initial results, an if you go over and over again, you can get that OK. Now this again is all random walk and you have to ask if this is happens on all problems.",
            "So one approach would be to do all problems and.",
            "That's obviously not satisfactory.",
            "So So what can you do instead?",
            "You can try to prove a theorem OK, and you can.",
            "You can also try to get insight.",
            "I guess I'm going to try to get insight first.",
            "And then we'll do the formal result.",
            "So let's try to get insight."
        ],
        [
            "Glasses, people.",
            "I want you to.",
            "You'd be the predictor.",
            "Imagine you were having some experience, so I imagine you are experiencing a training set of these eight 8 episodes.",
            "These are all very short episodes, right?",
            "So most of them are episodes like B0 means I'm in state B and then I get a reward of zero and the end of the episode at the end of the episode.",
            "OK, or I see state BI get a reward of 1 and that's the end of the episode.",
            "The only nontrivial episode is this first line running state A. I get a zero and then I go to State B and from BI get roller to 0 and that's the end of the episode.",
            "OK, so that's the data you see just these eight episodes and I want you to tell me what.",
            "What prediction would you make?",
            "OK, first question is what prediction would you make for state B?",
            "If you found yourself in State B.",
            "What would you guess for the expected return ahead of you?",
            "From state B.",
            "Say again.",
            "3/4 I agree because what would we do that we said?",
            "Well, I was in state B all eight times and six of them ended up with one and two of them ended up with a 0.",
            "So you're going to get 3/4 OK. OK, that was easy one.",
            "What about stayte?",
            "Stay, it's really much more uncertain if only been, say, once.",
            "Anne, what are you going to ask for?",
            "State agents?",
            "They take a moment and think about it.",
            "What are you going to get to return if you find yourself against A tag?",
            "When is the estimated value?",
            "Well, how would you estimate the value of state as OK?",
            "Now let me just say right away this is a question.",
            "There's multiple good answers to OK, so I'd like someone to raise your hand and give me one.",
            "Give me one answer and why it's a good answer.",
            "OK, how about you?",
            "You always go from A to B and the only time you've seen a went from A to B&B has value 75.",
            "Percent 3/4 then Asia also.",
            "That sort of makes sense.",
            "What's another good answer?",
            "Zero because every time at 8 degrees awarded 0.",
            "Yeah, I don't know if everyone heard that you said you've seen a once every time you saw it the return was zero, so you know might not predict zero.",
            "OK, now those two answers.",
            "Those two answers are the two are modular Monte Carlo's answers and TDs answers.",
            "OK, so we could say 0.",
            "That's what Monte Carlo would say.",
            "Monte Carlo just looks at what happened.",
            "I was in a once and the outcome was zero, so I should predict 0.",
            "Monte Carlo.",
            "Now the other one, the other one is what TD predicts.",
            "It's also what you would predict, and this is the gentleman explained what was going on in his head.",
            "He was saying, well, I'd seen a go to be an with an all that sort of stuff he was building in his head.",
            "This model."
        ],
        [
            "Using I've seen the only time I seen a it went to state B.",
            "By the way, the reward was zero on that transition, so let's guess that happens every time.",
            "And then in BBI saw eight times and six out of eight in one way to one and then stopped an two out of the.",
            "And I saw 8 * 6 out of the 8 winter went to one and two out of the eight went to a 0.",
            "So I'm building this in my head.",
            "OK this is like and this has a name.",
            "This is called the.",
            "The maximum likelihood model of the MDP.",
            "And just just just means what you would get by counting.",
            "Say how often do you go from here?",
            "Turn those into into probabilities.",
            "OK, this is the maximum likelihood model of the underlying Markov process, and then if you take this model and you solve it.",
            "If this is, this is the true world, then the true value is 3/4.",
            "OK. And so this is the general phenomenon of what TD does.",
            "If you present a training set over and over again to it it it.",
            "It gets the answer that you would get if you collected all the data and made a maximum likelihood model of the world and then solve that model with dynamic programming or with any method the true the true solution if that model was this was the reality.",
            "OK, so that's.",
            "Well.",
            "That's.",
            "That's why.",
            "TD is can be faster.",
            "Can be better 'cause it's using this Markov property that.",
            "Saying I've gotta be, I know B is a Markov state.",
            "And whereas Monte Carlo just says I don't care about what happened between, I ended up with a.",
            "Getting a 0."
        ],
        [
            "OK, so to summarize that.",
            "The prediction that best matches the training data is the Monte Carlo estimate.",
            "Best matches the training data.",
            "Remember the training data.",
            "OK, and if you saw you saw a once and it ended up with a 0 so you want to match the training data, the right prediction is the value of a 0.",
            "That is the prediction that will best match the data.",
            "OK, now of course I want to say we don't want to match the data.",
            "If we don't want to, we don't want to minimize the mean square error on the training set.",
            "Weird, huh?",
            "And it seems like we should want to minimize the mean square on the training set.",
            "And that's why I've gone through at some length this example with you guys.",
            "So I want to have some intuition of why we don't want to minimize the mean square error on the training set.",
            "So what can I offer you if I can't offer you minimizing mean square error on the training set, it's going to be minimizing the mean square error on future experience.",
            "'cause we don't really care about the training set.",
            "Past experience.",
            "We care about the future and so we think if we believe we have some real states here, we would think that the estimate value of a is 3/4 will actually be a better match to future data.",
            "OK.",
            "If we get a new experience, will say it's probably going to be end in a one 3/4 chance of being in a one.",
            "OK, so this interesting.",
            "Now we have to really distinguish between minimizing error on the training set, minimizing error on the future.",
            "This is different things and TD can be faster because it can take advantage of the state property.",
            "And match future experience better.",
            "Now, even as I said that you may be able to get immediately a sense of possible limitation of TD methods.",
            "As I said, they're going to take advantage of the state property that I know when I get to be, it doesn't matter how I got to be.",
            "But in real life, you don't normally have complete state knowledge, have incomplete state knowledge.",
            "If anytime using function approximation here we're just using discrete states at anytime using function approximation, you're going to have imperfect knowledge, imperfect state information, and so.",
            "So in the end, it's going to be a mix.",
            "It's going to be a question which is going to win in practice.",
            "But in the end, it's going to be TD that wins.",
            "In practice, I'm thinking OK in the end end.",
            "OK, OK, so.",
            "Yeah.",
            "Good good time for questions.",
            "When you introduce.",
            "Problem.",
            "U's B2 particular return, yes.",
            "No, V is still predicting the sum of the other awards from that from the state to the end.",
            "And so you remember, the example was a is followed by zeros, followed by bees, followed by another zero followed by termination.",
            "So right we're still trying to predict cumulative reward until the end.",
            "OK.",
            "Thank you for clarifying that question.",
            "Yes, thank you for for for this.",
            "A possible disadvantage that you are sold Philip the micro model.",
            "Go back and I'm coming back to the initial comparison of essential idea that to be scalable to do really have this computer to be nonlocal.",
            "Yes.",
            "I was thinking that I mean if you have an explicit representation of the state, yes.",
            "The memory overhead of MC is linear because you have the space there.",
            "You can put just a number of things I don't remember which who they are pretty slim an_Say the competition is not localized, that's OK, but then you have any pieces representation you are pointing when you set up a fortune for having a function approximation then of course in this case PD is this not working horribly subliminal.",
            "Yes, depending on the size of the representation of the function, but then then they updating this function.",
            "This company presentation can be expensive, so these are there is something there will be space and the locality there is like.",
            "It's hard to have both of them so so let's think that through.",
            "So let's assume that instead of having a table like this, all table look up.",
            "But instead of that we have a complicated neural network.",
            "Right and so then when we get a new error, we have to back propagate through the network.",
            "We have to do somewhat expensive computation, but it's not.",
            "Should we consider that expensive?",
            "I'm going to say no, because even the back propagation of 1 error backpropagation through the network, that complexity is the same.",
            "It's the same order as a forward pass to the network, so we already spent before we had to make a forward pass in order to get the prediction.",
            "And so there's an equivalent complexity to do the update.",
            "So even though it's a bunch of weights, it's it's we should consider that cheap.",
            "OK. OK.",
            "It's linear in the size of the network.",
            "Good.",
            "Good, any other questions?",
            "Good, OK so."
        ],
        [
            "So I've just done one step methods, tabular methods, model free methods.",
            "All these qualifiers can be generalized, but even here in this simplest case, one step method meeting we're looking from one step to the next step rather than one step to five steps ahead, like in AC 3.",
            "But there we can see the basic ideas and it's tabular tablet is easy to think about, but it all all the ideas really do generalize to the to the network case.",
            "Complicated function approximator.",
            "We've seen the basic things is that we're going to bootstrap and sample combine aspect of dynamic programming.",
            "Much Carol Carlo.",
            "These methods are computationally congenial, just a little bit of work on each step you have to wait until the end, and then do a whole bunch of work.",
            "And if the world is truly Markov, then TV methods are faster.",
            "That's what we see.",
            "And it has to do with the past data versus the future data.",
            "Now before I go into a summit, new thing I like to also try to summarize where we are in terms of pictorially OK, what today we've talked about?"
        ],
        [
            "Contrast ING TD method One Step TD method, which is like this is what I.",
            "This is a little picture is what I used to summarize the method that mean the thing at the top.",
            "Something updating it and this says I go ahead 1 action and one next state and I use my estimate here to improve this guy.",
            "So this is like a picture of the algorithm and the same kind of picture for Monte Carlo is you want to estimate, improve the estimate of this guys value, you go ahead one state, an action state action states that overall the end and you see the final outcome and then you back all that up.",
            "OK so and this is like a dimension you can occupy intermediate methods.",
            "You can do two step methods, three step methods, four step methods.",
            "Five step is like an infinite step method where you go all the way to the end of the episode and then.",
            "And then there is the parameter Lambda.",
            "You might have heard about in TD Lambda the eligibility trace parameter is really a bootstrapping pattern that determines it's not the number of steps, but it's.",
            "It's analogous to the number of steps.",
            "And so this is really a dimension and we can occupy any point along this dimension.",
            "OK, and that's now there's a second dimension, which is are we going to use planning OK?",
            "Are we going to use knowledge of the model of the world?",
            "OK, dynamic programming?",
            "Dynamic programming is this corner.",
            "It means we're still going to one step method dynamically add one step and use your estimates at that one step.",
            "Look ahead into the future.",
            "OK, so that's moving along the top here.",
            "He says keep keep these short backups one step backups, but instead of being a sample, do all the possibilities.",
            "And that's dynamic programming.",
            "And then there's a fourth corner where analogous to Monte Carlo, but with planning is like exhaustive search.",
            "We consider all the possibilities all the way to the end.",
            "And so we can get these four corners is classic methods.",
            "And then we can occupy the area in between them and.",
            "That's kind of a big space of reinforcement learning methods, although it's certainly not the whole space.",
            "OK. Now.",
            "Where should we go next?",
            "I don't have time to present all that I wanted to say.",
            "But let me just sort of.",
            "We've done a good group here.",
            "I can almost sort of wrap up and talk about the future from here, but let me just tell you some of the things that we had more time we might talk about.",
            "OK, first we would talk about estimating instead of state values.",
            "We talked about estimating action values, 'cause you know, really, for control you want to action values and it's not."
        ],
        [
            "That different you would just estimate coopi instead of the pie, and it's going to be lower case because that's this is the true value function coopi and you would estimate it with an action value estimate since it's just tabular, I can say big Q of essay for the actual state match encountered in this update.",
            "This update is essentially the TD update.",
            "It's just state action pairs rather than I'm states, right?",
            "This is still a TD error.",
            "This is my original estimate.",
            "This is the estimate for the next state.",
            "This is the Sarsa algorithm.",
            "That's that's quite straightforward, and so that's our salgo rhythm ends up being that rule that update done over and over again.",
            "Anne."
        ],
        [
            "And.",
            "And some examples keuler."
        ],
        [
            "Ling Q Learning is almost the same.",
            "Rule is you were doing updating state state action value.",
            "The new one is the old 1 + a step size times of TD error with the TD error is a little bit different.",
            "The TD error.",
            "We were comparing our original estimate to next reward plus something of the next state.",
            "But it's the Max of our possible actions at the next date.",
            "That's Q learning.",
            "I like to draw this picture.",
            "This is it's picture.",
            "The picture says you know I'm updating a state action.",
            "Put all the possible estimated action values.",
            "Take the maximum.",
            "That's this part of the rule, right Max or all possible a.",
            "And then I back up the Max to improve the estimate of this guy at the top.",
            "That's Q learning.",
            "It's a TD algorithm with that particular target.",
            "And."
        ],
        [
            "This is a nice example.",
            "Cliff Walk is a nice example comparing source in Q learning sources and on policy method.",
            "Q Learning is an off policy method and.",
            "We see that actually here the Y axis or the wax is reward for episode.",
            "This will actually workout better.",
            "I don't have time to explain this example."
        ],
        [
            "I wouldn't.",
            "I didn't want to do something it.",
            "You guys didn't really understand.",
            "Better to skip over it.",
            "OK, here's one sort of new algorithm, expected Sarsa.",
            "Expected size, so if you look at the picture right in Q, learning is taking all these possible things you might do and you take the best of them.",
            "Take the Max from the arc means Max.",
            "And if you have, if you don't have an arc, then it means expectation.",
            "OK, so expected so you don't take the best of the things you might do.",
            "You take the the expectation based on how you would actually do them according to your policy.",
            "So here we are, we're summing over all the things we might do.",
            "How likely are we to do it under our policy, which we know?",
            "We know our policy.",
            "We know how likely we are like.",
            "Maybe it's epsilon greedy.",
            "And we take the expectation.",
            "The action value times our likelihood of doing that action and we back up that OK, and that's that's arguably listen, improve.",
            "Our version of Sarsa.",
            "And it can also be made an off policy version of Sarsa.",
            "And there's some other."
        ],
        [
            "Novel Tees you can.",
            "You can do an off policy version of expected services.",
            "I've used the word off of."
        ],
        [
            "See a couple of times that explaining it.",
            "I'm sorry about that, but off policy means that you're learning about a policy that's different than the one you're following.",
            "OK, and on policy means you're learning about the same policy is that when you're following the same one that's generating the data.",
            "So the way to remember is that on policy is almost one policy, and in on policy methods there is only one policy.",
            "Some policy you're doing its policy learning about, but very often these want to be different, like you want to do something that is more exploratory and you might want to learn the optimal policy.",
            "OK, so if you're going to learn the optimal policy, but you're going to actually get your data and exploratory way, we're just not going to be optimal.",
            "Then you have two policies.",
            "OK, and then you're in the realm of off policy learning.",
            "Q Learning does this, but off policy learning is is theoretically more difficult and more challenging for our methods.",
            "OK. That's that's all."
        ],
        [
            "Policy OK so.",
            "So I basically just extended these things to control OK Now.",
            "We've seen some.",
            "Some methods that can do the on policy case in the off policy case.",
            "We didn't talk about double Q learning.",
            "OK."
        ],
        [
            "So I've talked a lot about about do we want to use Monte Carlo or do we want to use TD OK and it says there's a sense in which we don't have to choose 'cause if you use an algorithm like if you use TD, you can parametrically very Lambda or vary the height of your backups to get to give you any intermediate point between one step TD in Monte Carlo you can get both and a kenite way of doing this is with the parameter.",
            "Lambda the bootstrapping parameter which isn't really talked about, but it is a way to very parametrically between TD and Monte Carlo.",
            "So if land equals 0, which is the left side of all these graphs, that's pure TD pure bootstrapping.",
            "OK, if land is one, that means you're not doing any bootstrapping, it's Monte Carlo.",
            "OK, OK, so now all these graphs have Lambda can cross the bottom, so it's basically like this to pure to know bootstrapping and they all have a measure performance on the top where in all cases lower is better.",
            "OK so it's like it's like Mountain Car an you want to have a few steps to get to the top of the Hill.",
            "OK So what you see looking at this is that you know performance depends on Lambda this is this is.",
            "Random walk and it's actually not best at land equals zero.",
            "Pure TD is not the best you can do better if you do some amount of TD intermediate between Pure TD and Monte Carlo.",
            "But if you go all the way to land equals equals one, then things get really bad.",
            "That's like the worst case in general, and that's the pattern.",
            "Land equals zero is Monte Carlo, and Monte Carlo has really high variance and it has to be ruled out.",
            "It's not very happy if you are committed to Monte Carlo now you can do TDD and say, oh, I can pick any step in between.",
            "That's what you want to have this facility of doing some bootstrapping.",
            "And that's sort of some evidence for that, even though this is old data, I think you know, like Peter would agree that Monte Carlo is is not really an efficient strategy to do it in a pure way.",
            "OK, now another I want to give it 1 slide also for taking questions on the linear case case with the real function approximation.",
            "I'm not going to go to nonlinear networks, but I want to go to something which five talks someone theoretically about which is the linear case so."
        ],
        [
            "Close reading linear function approximation, linear function means that our estimate estimated value is formed as an inner product between a parameter.",
            "A weight vector and a feature vector.",
            "OK, so feature vectors are.",
            "Fee fee for feature fee of T is our feature vector from the state at time T and the parameter vectors Theta, so that might think about the weights of your network.",
            "This is a linear network.",
            "OK, so do we take the inner product and so this transpose thing means the inner product so Theta inner product with fee?",
            "Is our estimated value of state T use?",
            "It's just made value of the state at time T because this is the feature vector for the state at time T. OK, so this is our estimate.",
            "This is estimated value of time T. This is our estimated value at time T + 1.",
            "So this really is a TD error.",
            "And this is a TD rule.",
            "The TD rule is that the parameters are the old parameters plus step size times RTD error and this need the gradient and the general in general, nonlinear cases would be a gradient of the.",
            "Of the prediction with respect to the parameters in the linear case is just the feature vector Phi Phi.",
            "Excuse me.",
            "OK, so that rule should be fairly familiar to you now.",
            "It's just a TV rule using a stochastic.",
            "Gradient descent.",
            "Um, it's.",
            "Lots could be said about that, but that's that's the standard team linear TV T0.",
            "And if you look at this you can of course write it like this.",
            "You can take the file the fee and carried inside here it's there and you carried inside here with little some transpose the stuff you can write it like that and this is a vector if you think the expectation we're going to take the expectation.",
            "OK, so in expectation the new feature vector is the old 1 + a step size and this thing this thing in expectation.",
            "What is it?",
            "OK?",
            "Well I'm just going to make some names for it.",
            "This thing is a vector and this thing is a matrix times Theta.",
            "OK so B is going to call that vector so that vector B is just the expected value of this thing.",
            "It is a well defined vector.",
            "You don't know it, but it's there and this thing is a matrix 'cause it's an outer product of feature vector with the change in the feature vectors.",
            "So the expectation of that matrix is what I'm going to call a.",
            "So let me let me write the whole expectation like this, and I'm interested in the and what happens at the fixed point?",
            "Where will this settle?",
            "An expectation will converge.",
            "Where will the expected update be zero?",
            "Well, the expected update.",
            "Is basically this part, so I want to win.",
            "Is that zero?",
            "Well, that's going to zero in B = a Theta.",
            "Or when B -- 8 Theta is zero OK and that that Theta?",
            "That's a special Theta for which this is true for B -- a Theta equals zero.",
            "I'm going to call it Theta TD 'cause it's the fixed point that TD converges to linear TV converges to OK, and then you can just compute it will be is.",
            "Is a type Theta and so you have to take the inverse of A and you get.",
            "The TV fixed point is the A inverse B and which by the way is a key to another algorithm.",
            "Least squares algorithm says estimate a directly even though it's a matrix can take its inverse.",
            "An also estimate be directly and then multiply them together to get least squares.",
            "TV works OK.",
            "But this way of computing what what?",
            "What the algorithm converges to, and then you can say something theoretical about it.",
            "This is your guarantee that we get that the mean square value error measure how the values are is bounded by an expansion times the mean square value of the best data.",
            "So this means that we don't find the best Theta.",
            "OK, but we do get an expansion of it anyway, so that's what the theory would look like if we had more time to talk about it.",
            "Unless you want to."
        ],
        [
            "Mention quickly some parts of the frontiers, some things that people are working on now, so off policy prediction is a big area people are working on trying to generalize to the off policy case.",
            "Also, we'd like to talk about non theory for the case of non linear function approximation.",
            "There's just a little bit of that.",
            "There's also very little convergence theory for control methods.",
            "And I think chavaux.",
            "Maybe we'll talk about that tomorrow.",
            "And we also like to say things beyond convergence.",
            "Would like to know how well can you do in a finite amount of time, how fast you converge.",
            "Now when you combine TV with with deep learning, a lot of different issues come up and I think there's just a lot of uncertainty.",
            "Do we really need a replay buffer so that one of the folk theorems it always?",
            "Especially you have instability in correlation quote correlation.",
            "So we need this thing called the replay buffer, but I think it's really.",
            "There's lots of questions about what happens when we combine TD with deep learning and finally, the idea of predicting things other than reward.",
            "Remember, I started with that.",
            "We might want this is tedious.",
            "A general prediction method.",
            "Multi step prediction methods.",
            "We want to use it to predict other things and in particular we want to learn it to use learn a model of the world.",
            "So in conclusion I guess."
        ],
        [
            "I want to say is something like this that E learning is a uniquely important kind of learning.",
            "Anyway, maybe it's ubiquitous.",
            "We're always going to be using it, and I think this may be true.",
            "It's a hypothesis.",
            "So anyway, it's learning to predict, which is perhaps the only scalable kind of learning.",
            "It's A kind of running this specialized for general multi step prediction, which may be the key to perception modeling the world.",
            "The meaning of our knowledge, it's key ideas to take advantage of the state property.",
            "Which can make it fast and efficient, but can also make it asymptotically biased and its other key claim to fame is that is computationally cheap congenial, and we're only beginning to use to explore different ways we use it for things other than reward.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you everybody.",
                    "label": 0
                },
                {
                    "sent": "Thank you jewel.",
                    "label": 0
                },
                {
                    "sent": "It's true.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of the I have the most Gray hair here.",
                    "label": 0
                },
                {
                    "sent": "I've been around for a long time and and I want to kind of talk about the Longview a bit actually, so this is the challenge.",
                    "label": 0
                },
                {
                    "sent": "First, let me say though, it's really exciting just to talk to all you young people learning about the field, learning about, want to get into AI and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And it's just like it's a special opportunity.",
                    "label": 0
                },
                {
                    "sent": "I really appreciate it and.",
                    "label": 0
                },
                {
                    "sent": "So when I do this, I have to think of what can I try to communicate to you because.",
                    "label": 0
                },
                {
                    "sent": "You know we have really a brief amount of time and.",
                    "label": 0
                },
                {
                    "sent": "The field is immense, written a whole book on the subject, and So what we what can I try to do now?",
                    "label": 0
                },
                {
                    "sent": "My topic normally today is temporal difference learning, and certainly I'm going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "But I also want to speak to you as young people entering the field and just tell you what I think about it from a sort of a longer view and.",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think the first thing to be said along those lines is that this is a really special time an we can look forward now to doing an amazing thing which is maybe understanding the way the mind works.",
                    "label": 0
                },
                {
                    "sent": "Way intelligence works.",
                    "label": 0
                },
                {
                    "sent": "This is like a monumental event, not just you know this century.",
                    "label": 0
                },
                {
                    "sent": "You know, for thousands of years, maybe in the history of the Earth when intelligent beings, animals.",
                    "label": 0
                },
                {
                    "sent": "Things that can replicate themselves finally come to understand the way they work well enough to buy, design, create intelligence, and so the big thing that's going to happen is that we're going to come to understand how the mind works, how intelligence can work, how it does work, and just the fact of our understanding.",
                    "label": 0
                },
                {
                    "sent": "It is going to change.",
                    "label": 0
                },
                {
                    "sent": "The world is going to change.",
                    "label": 0
                },
                {
                    "sent": "Obviously there will be lots of applications, but just it'll change ourselves.",
                    "label": 0
                },
                {
                    "sent": "Our view of ourselves.",
                    "label": 0
                },
                {
                    "sent": "What we do, what we play with what we work at.",
                    "label": 0
                },
                {
                    "sent": "Everything, it's a big event.",
                    "label": 0
                },
                {
                    "sent": "It's a big event and we have to.",
                    "label": 0
                },
                {
                    "sent": "We should keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "And as I say you know I'm not saying it's going to happen, you know?",
                    "label": 0
                },
                {
                    "sent": "Tomorrow it's not.",
                    "label": 0
                },
                {
                    "sent": "But you know, 10 years it could happen 30 years it could happen, and in the great sweep of history, that's a small amount of time.",
                    "label": 0
                },
                {
                    "sent": "It's going to happen within your lives with high probability.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So first thing we have to ask is why is this happening and I think it's really comes down to like Moore's Law.",
                    "label": 0
                },
                {
                    "sent": "OK, so I took a slide from Kurz.",
                    "label": 1
                },
                {
                    "sent": "While this is just standard slide of the increasing computation per dollar and it's just reaching an appoint where it starts to compete with the computation that we can do with our own brains with natural systems.",
                    "label": 0
                },
                {
                    "sent": "OK, that's happening now and it's just going to continue increasing.",
                    "label": 0
                },
                {
                    "sent": "Dramatically, I'm sure you've seen these kind of slides here.",
                    "label": 0
                },
                {
                    "sent": "We have time years along the bottom and on the other.",
                    "label": 0
                },
                {
                    "sent": "We have computations per second per dollar.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see, it's a log scale, so of course a straight line would be exponential increase and curves while argues that it's actually slightly super exponential, but the point is, the computations become available to us is become enormous, and that changes things.",
                    "label": 0
                },
                {
                    "sent": "It's the rise of deep learning and all that is purely because the GPU's and computation getting cheaper.",
                    "label": 0
                },
                {
                    "sent": "And that will only continue and it will become more extreme as a vast increase computation, so that alters everything and it really is profound, and I'm just going to try to slide to say what I conclude from this, as I think it's an answer to like a question for AI.",
                    "label": 0
                },
                {
                    "sent": "This had for 6070 years.",
                    "label": 0
                },
                {
                    "sent": "OK, so the answer I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to do a couple of slides like this, but I guess I should one last bit of prefaces that my my talk today is going to be like half.",
                    "label": 0
                },
                {
                    "sent": "Examine these big things.",
                    "label": 0
                },
                {
                    "sent": "OK and like I'm starting to do now in half.",
                    "label": 0
                },
                {
                    "sent": "Like really small things, we go back to the foundations, real stuff.",
                    "label": 0
                },
                {
                    "sent": "That's basic that we need to understand.",
                    "label": 0
                },
                {
                    "sent": "OK, I think the details.",
                    "label": 0
                },
                {
                    "sent": "The big pictures is important, but the details also really matter.",
                    "label": 0
                },
                {
                    "sent": "And like we heard a lot of specific algorithms early today, exactly how they work is really important and the details matter.",
                    "label": 0
                },
                {
                    "sent": "But the big picture matters.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we lose sight of the big picture.",
                    "label": 0
                },
                {
                    "sent": "Sometimes something so obvious that we overlook it, I think.",
                    "label": 0
                },
                {
                    "sent": "The computation is as long that lines and its implication implication in a phrase is that methods.",
                    "label": 0
                },
                {
                    "sent": "That scale with computation.",
                    "label": 0
                },
                {
                    "sent": "Are the future of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I. OK, and that's",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's by scale computation.",
                    "label": 0
                },
                {
                    "sent": "I mean, as we get more computer power, those methods become more powerful, and this has not always been true.",
                    "label": 0
                },
                {
                    "sent": "True, but it certainly is true of learning and search methods.",
                    "label": 1
                },
                {
                    "sent": "These are what we call general purpose methods, and it's the answer to this.",
                    "label": 0
                },
                {
                    "sent": "One of the oldest questions in AI for 60 years old.",
                    "label": 1
                },
                {
                    "sent": "Do we want weak methods?",
                    "label": 0
                },
                {
                    "sent": "That's what they call them.",
                    "label": 0
                },
                {
                    "sent": "In the old days they call general purpose Methods Week 'cause they just.",
                    "label": 0
                },
                {
                    "sent": "They just use the data or use computation.",
                    "label": 0
                },
                {
                    "sent": "They can leave their their general purpose in general there week.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what they call them in those days, the strong ones were the ones that we lose human knowledge and human expertise in human insight to make their systems so much better.",
                    "label": 0
                },
                {
                    "sent": "OK, now probably you guys are thinking that's crazy.",
                    "label": 0
                },
                {
                    "sent": "The general purpose ones.",
                    "label": 1
                },
                {
                    "sent": "Are you thinking that with me general purpose?",
                    "label": 0
                },
                {
                    "sent": "How many grateful human insight?",
                    "label": 0
                },
                {
                    "sent": "Sum of all.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, that's that's that's nice.",
                    "label": 0
                },
                {
                    "sent": "Compromising sort of position.",
                    "label": 0
                },
                {
                    "sent": "And so all today I'm going to try it all on this big part of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to talk about.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present strong views.",
                    "label": 0
                },
                {
                    "sent": "OK, really, maybe you should do compromise and nuances, but it's good to talk about strong views because they give you a working hypothesis to give you a point of view and you know you can say this is a strong but you don't have to believe it.",
                    "label": 0
                },
                {
                    "sent": "You have to say, well, that's the strong point of view that I should be thinking about.",
                    "label": 0
                },
                {
                    "sent": "And then maybe you have several of them anyway, so I'm going to present the strongly this all this question has been answered.",
                    "label": 0
                },
                {
                    "sent": "It's been imagined in favor of the week methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, Ann.",
                    "label": 0
                },
                {
                    "sent": "So I don't want you to.",
                    "label": 0
                },
                {
                    "sent": "I could talk about this all day, but I'll refrain from it.",
                    "label": 0
                },
                {
                    "sent": "I'll just note the next thing to say is that you make your thinking you're good you're thinking you're into deep learning or reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So you're on the right side of history.",
                    "label": 0
                },
                {
                    "sent": "But I'm not sure that's correct, OK?",
                    "label": 0
                },
                {
                    "sent": "'cause we take things like any kind of supervised learning, even model free reinforcement thing.",
                    "label": 0
                },
                {
                    "sent": "I love more than anything, right?",
                    "label": 0
                },
                {
                    "sent": "It's only weekly scalable.",
                    "label": 0
                },
                {
                    "sent": "If I had, you know, a million times this computation, I'm still limited in my model for reinforcement, how fast I can gather data and I'm only learning value function may be a policy.",
                    "label": 0
                },
                {
                    "sent": "What's the big deal?",
                    "label": 0
                },
                {
                    "sent": "That's a tiny object in some map from states to what to do.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's not a big thing and we can help.",
                    "label": 0
                },
                {
                    "sent": "How many features you have?",
                    "label": 1
                },
                {
                    "sent": "It's and super certainly for supervised learning is only weakly scalable because it requires people to provide datasets and they become the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "If you have, you know you get people to label things on the web, things scale.",
                    "label": 0
                },
                {
                    "sent": "But it only weakly only as fast as you can gather the data and eventually that becomes a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "You really want to be able to learn from raw data.",
                    "label": 0
                },
                {
                    "sent": "You want to be scale.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh OK, so.",
                    "label": 0
                },
                {
                    "sent": "If these things that we love are not scalable, what is scalable would be fully scalable.",
                    "label": 0
                },
                {
                    "sent": "Well, my answer is simple.",
                    "label": 0
                },
                {
                    "sent": "It's what I call prediction.",
                    "label": 0
                },
                {
                    "sent": "Learning prediction.",
                    "label": 0
                },
                {
                    "sent": "Learning means learning to predict what will happen OK, and so it's the unsupervised supervised learning because we have targets where supervised learning we have we.",
                    "label": 1
                },
                {
                    "sent": "We just wait and see what does happen and that's our target, OK?",
                    "label": 0
                },
                {
                    "sent": "And so you don't need to have human labeling or you have a target, but you don't need a human to provide it, just get it from the data so you see unsupervised supervised learning.",
                    "label": 0
                },
                {
                    "sent": "But anyway, it's definitely the scalable model, free learning.",
                    "label": 1
                },
                {
                    "sent": "And maybe it's the scalable model free learning.",
                    "label": 0
                },
                {
                    "sent": "And prediction learning is at the heart of all of our control methods.",
                    "label": 0
                },
                {
                    "sent": "We learn value functions.",
                    "label": 0
                },
                {
                    "sent": "I think maybe that sort of argument.",
                    "label": 0
                },
                {
                    "sent": "I want the big argument about temporal difference learning is that it's the scalable model free method.",
                    "label": 0
                },
                {
                    "sent": "And of course I haven't given you even a step towards that yet.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that the idea of predicting.",
                    "label": 0
                },
                {
                    "sent": "Predicting of having learning that predicts.",
                    "label": 0
                },
                {
                    "sent": "Is the key problem.",
                    "label": 0
                },
                {
                    "sent": "It's what we should be thinking about an.",
                    "label": 0
                },
                {
                    "sent": "You may say, well, deep learning, supervised learning all about prediction learning, but that's not true.",
                    "label": 0
                },
                {
                    "sent": "It's all it's about predicting what the other with the label will be, but it's not about predicting overtime.",
                    "label": 0
                },
                {
                    "sent": "It's not about predicting where you have to wait and see what happens, and that makes all the difference.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So with all this words, I want to ground us a little bit, remind us with the data what the data looks like, so I put in the slide of.",
                    "label": 0
                },
                {
                    "sent": "This is what?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Real life is like real life is a temporal stream.",
                    "label": 0
                },
                {
                    "sent": "We have things like playing soccer and we have to make actions at every moment.",
                    "label": 0
                },
                {
                    "sent": "Where maybe a hyena being chased by lion and trying to predict whether it's going to live or die.",
                    "label": 0
                },
                {
                    "sent": "Or maybe a baseball player and we are our eyes are watching this tiny little ball flash bias really fast.",
                    "label": 0
                },
                {
                    "sent": "I have to swing at just the right moment to hit the ball.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you're talking to someone you're trying to predict what they will do.",
                    "label": 0
                },
                {
                    "sent": "So this is the data streams that AI should be learning to deal with and so we should always keep this in mind when I say learning to predict well, I think I think that like a hyena trying to protect you know fear.",
                    "label": 0
                },
                {
                    "sent": "Fear is your prediction of are you going to die?",
                    "label": 0
                },
                {
                    "sent": "OK, so he's trying to predict it several times.",
                    "label": 0
                },
                {
                    "sent": "It looks good and bad.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And at the end here it's not looking so good.",
                    "label": 0
                },
                {
                    "sent": "OK, so with those prefaces, let me start talking.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "Temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "Temperance learning.",
                    "label": 0
                },
                {
                    "sent": "It's a method for learning to predict it's while using reinforcement learning to predict future award value functions, and it's basically the center of the core of many methods that you know about Q learning and Sarsa TD Lambda deep Q networks.",
                    "label": 1
                },
                {
                    "sent": "TD Gammon the world Champion backgammon player using deep reinforcement learning from.",
                    "label": 0
                },
                {
                    "sent": "15 know 25 years ago.",
                    "label": 0
                },
                {
                    "sent": "My God 25 years ago.",
                    "label": 0
                },
                {
                    "sent": "Deep reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "1992 but not all reinforcement methods so Alpha go.",
                    "label": 0
                },
                {
                    "sent": "It happens not to use TD learning using other kinds of reinforcement learning the helicopter thing that you might have heard that doesn't use it.",
                    "label": 0
                },
                {
                    "sent": "And then there's sort of pure like Peter's talk was interesting because he talked about policy based methods, and some of those don't use temporal difference learning, but eventually he would get to it and put it in to make things better.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of optional.",
                    "label": 0
                },
                {
                    "sent": "It feels like it's optional.",
                    "label": 0
                },
                {
                    "sent": "Many people say it's optional, but I do want to argue that you do want to use it.",
                    "label": 0
                },
                {
                    "sent": "That really you should always be using it, it's ubiquitous.",
                    "label": 0
                },
                {
                    "sent": "And OK, it seems how the brain works.",
                    "label": 0
                },
                {
                    "sent": "What else?",
                    "label": 1
                },
                {
                    "sent": "Oh, now it can be used to predict anything.",
                    "label": 0
                },
                {
                    "sent": "It's for general method for prediction learning, not just for rewards.",
                    "label": 0
                },
                {
                    "sent": "OK, so don't be fooled by reinforcing.",
                    "label": 0
                },
                {
                    "sent": "Read all the TV.",
                    "label": 1
                },
                {
                    "sent": "It's all about predicting value functions reward.",
                    "label": 0
                },
                {
                    "sent": "But we can use for anything.",
                    "label": 0
                },
                {
                    "sent": "And since my talk is just about TD, I want to be sure to think about this general use question.",
                    "label": 0
                },
                {
                    "sent": "Not saying that it's key to Alpha.",
                    "label": 0
                },
                {
                    "sent": "Need to train value functions that they use to evaluate the states.",
                    "label": 0
                },
                {
                    "sent": "Research.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I've forgotten exactly the details.",
                    "label": 0
                },
                {
                    "sent": "The initial learning system was learned from actual outcomes, actual games and they would go all the way to the final outcome.",
                    "label": 0
                },
                {
                    "sent": "They make a prediction, look at position, make a prediction of how my gonna win or lose and they would go ahead and see who actually won the game at the end and they would use who actually won.",
                    "label": 0
                },
                {
                    "sent": "Instead of a temporal difference learning, so that's really want to work for.",
                    "label": 0
                },
                {
                    "sent": "Do they wait and see who actually won.",
                    "label": 0
                },
                {
                    "sent": "They see the outcome or the return.",
                    "label": 0
                },
                {
                    "sent": "Or do they do the update it guess from a guess?",
                    "label": 0
                },
                {
                    "sent": "OK, let me make that clear, that's mine.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next slide is a TD learning what is TD learning.",
                    "label": 1
                },
                {
                    "sent": "Basically it's learning a prediction from another later prediction.",
                    "label": 1
                },
                {
                    "sent": "OK, so if Alpha go was licking position, making prediction and zooming all the games he won and then it's not TD, it's not learning that prediction from a prediction.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you make a prediction from a position and then you make one move and you see your next prediction.",
                    "label": 0
                },
                {
                    "sent": "And you use that next prediction as to form a target.",
                    "label": 0
                },
                {
                    "sent": "Then you're doing TD learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so the quick word, quick phrases.",
                    "label": 1
                },
                {
                    "sent": "We're learning a guest from a guess.",
                    "label": 0
                },
                {
                    "sent": "OK, sounds a bit dangerous, doesn't it?",
                    "label": 0
                },
                {
                    "sent": "How would constrain it would tie it down OK, but that is the idea we want to learn an estimate from an estimate.",
                    "label": 0
                },
                {
                    "sent": "And we have to talk about whether this is good or not OK, the TD error, the TV error is the difference between 2 predictions, two temporally successive predictions, right?",
                    "label": 1
                },
                {
                    "sent": "So if you're playing your game, you say I think I'm winning, then you take another move.",
                    "label": 0
                },
                {
                    "sent": "You know now I think I'm losing you.",
                    "label": 0
                },
                {
                    "sent": "Try to learn from that.",
                    "label": 0
                },
                {
                    "sent": "You don't know who actually is going to end yet when you try to learn from the temporal difference in your predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, now after that it's the same.",
                    "label": 0
                },
                {
                    "sent": "As which are all used to, you just have an error and you send it back proper through whatever you're doing, and so it's just really where does the error come from?",
                    "label": 0
                },
                {
                    "sent": "Where does the target come from?",
                    "label": 0
                },
                {
                    "sent": "Is the target come from the end of the game, or does the target come from the next prediction?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the example of TD Gammon, originally 1992, and he had a chest, a back end position.",
                    "label": 1
                },
                {
                    "sent": "And he would send that into a neural network which would filter through actually just a single well had many versions stand version was a single hidden layer and you end up with this probability of winning and the error then was the probably winning in one position minus the probably winning the next position.",
                    "label": 1
                },
                {
                    "sent": "So we look at the change in the estimated probability of winning and that was used as the error.",
                    "label": 0
                },
                {
                    "sent": "That would be backpropagated through it.",
                    "label": 0
                },
                {
                    "sent": "And this would learn.",
                    "label": 1
                },
                {
                    "sent": "Just stick it in the corner playing against itself, learning from itself from its own.",
                    "label": 0
                },
                {
                    "sent": "Trial and error and it came out to be.",
                    "label": 0
                },
                {
                    "sent": "Competitive with the world's best players.",
                    "label": 0
                },
                {
                    "sent": "Really the best in the world.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's familiar.",
                    "label": 0
                },
                {
                    "sent": "Now we I'm trying to get to this question.",
                    "label": 0
                },
                {
                    "sent": "These questions.",
                    "label": 0
                },
                {
                    "sent": "Do you need to use TV?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is all this is is to motivate motivation is the most important.",
                    "label": 0
                },
                {
                    "sent": "Do I need to use TD learning or can I get away with it because you go to the field now, maybe even in reinforcement learning you'll find a good fraction of the people don't believe in TD LTE and they think they can get away without it.",
                    "label": 1
                },
                {
                    "sent": "OK, and so it's a real question.",
                    "label": 0
                },
                {
                    "sent": "We should all be asking do we need it?",
                    "label": 0
                },
                {
                    "sent": "I want you to understand.",
                    "label": 0
                },
                {
                    "sent": "I want you even as as a people learning about the field to be able to engage with this question and know the basic facts pertinent to whether we should.",
                    "label": 0
                },
                {
                    "sent": "We need to use TD learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will skip over.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over Atari.",
                    "label": 0
                },
                {
                    "sent": "And you've seen that?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already so TD learning when do when do we need it?",
                    "label": 0
                },
                {
                    "sent": "OK, well it's only relevant and what I multi step prediction problems.",
                    "label": 0
                },
                {
                    "sent": "That's the first thing only when the thing predicted is multiple steps in the future.",
                    "label": 1
                },
                {
                    "sent": "So obviously if you're predicting the outcome of game, that's multiple steps in the future.",
                    "label": 0
                },
                {
                    "sent": "But if you were predicting.",
                    "label": 0
                },
                {
                    "sent": "A label or or in Alpha go where the data was the initial data at least was.",
                    "label": 0
                },
                {
                    "sent": "Here's a guess and then I'll just see who won the game.",
                    "label": 0
                },
                {
                    "sent": "And if you just use who won the game, then it's essentially a one step prediction, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so now I want to say that it's really broadly applicable.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "I said it's only applicable when you have multi step predictions, but really everything you want to do is going to be multi step prediction.",
                    "label": 0
                },
                {
                    "sent": "If you want to predict, I have some examples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multi step prediction if you want to protect them again if you want to stock market index will be well really you could just predict what's going to be but you get more data on every day after that annual meeting like new predictions.",
                    "label": 0
                },
                {
                    "sent": "So you make a mold like repeated predictions about a long term outcome.",
                    "label": 0
                },
                {
                    "sent": "If you want to predict will be the next president, you can predict that you know every every day is a new event happens.",
                    "label": 1
                },
                {
                    "sent": "If you want to predict who US would go to war against next.",
                    "label": 1
                },
                {
                    "sent": "So all these are long-term predictions.",
                    "label": 1
                },
                {
                    "sent": "They don't.",
                    "label": 0
                },
                {
                    "sent": "They don't jump to the end.",
                    "label": 1
                },
                {
                    "sent": "Now, even even if you want to predict a sensory observation, if you want to purchase the very next sensory observation, that would not be multi step prediction.",
                    "label": 1
                },
                {
                    "sent": "But but if you were to predict, you know even 10 steps ahead.",
                    "label": 0
                },
                {
                    "sent": "Or a discounted measure of the future.",
                    "label": 0
                },
                {
                    "sent": "Those are long-term or multi step predictions and we think about go back to the real world, the hyena and Lions or the conversation or messy playing soccer.",
                    "label": 0
                },
                {
                    "sent": "He's got to make long-term predictions.",
                    "label": 0
                },
                {
                    "sent": "It's not, you know what's the next thing.",
                    "label": 0
                },
                {
                    "sent": "It's going to make this goal.",
                    "label": 0
                },
                {
                    "sent": "Will I get around that fellow?",
                    "label": 0
                },
                {
                    "sent": "Where's the law going to be in a few milliseconds from now?",
                    "label": 0
                },
                {
                    "sent": "All these things are multi step predictions now can.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Can we treat it?",
                    "label": 0
                },
                {
                    "sent": "Can we just use our one step methods like you know?",
                    "label": 1
                },
                {
                    "sent": "Sure things happen bit by bit but ignore that.",
                    "label": 0
                },
                {
                    "sent": "Just wait and see what happens.",
                    "label": 0
                },
                {
                    "sent": "You know like see who won the game and use our one step methods and or can you learn one step model and then compose your model.",
                    "label": 0
                },
                {
                    "sent": "OK, and the answer is that we really can't do these things and I want to try to give some sense of that today.",
                    "label": 1
                },
                {
                    "sent": "I'm really going to talk mainly about this first question.",
                    "label": 0
                },
                {
                    "sent": "Can we think of the multi step case as one big step or do we have to deal with it bit by bit?",
                    "label": 1
                },
                {
                    "sent": "And I wanted one slide in the second bit.",
                    "label": 0
                },
                {
                    "sent": "The second bit is can we learn one step predictions of the model and then iterate them to get a multi step prediction when you need to?",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ann I just.",
                    "label": 0
                },
                {
                    "sent": "I just want to say that I think it's a trap.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't know, really, properly explained this to you, but I think it's a trap.",
                    "label": 1
                },
                {
                    "sent": "I think it's enough to model the world to make it like a low level simulation of the world to make like a think trait rule is a Markov decision process and model the transition probabilities or treat the world as a as a engineering model where we just have to learn the velocities in the accelerating effect of the accelerations of reactions and then integrate this low level differential equation.",
                    "label": 0
                },
                {
                    "sent": "This is all a trap.",
                    "label": 0
                },
                {
                    "sent": "The short term models and then iteration is.",
                    "label": 0
                },
                {
                    "sent": "It feels good because we know we know that it can be done perfectly on the one step.",
                    "label": 1
                },
                {
                    "sent": "Then it can be done perfectly for however we want to look at the future.",
                    "label": 0
                },
                {
                    "sent": "But there's two problems first, so we can't do it perfectly.",
                    "label": 0
                },
                {
                    "sent": "And when we do it imperfectly with, we're always going to have an approximation.",
                    "label": 0
                },
                {
                    "sent": "Then when we try to iterate them, we get a propagation of errors and com pounding of our errors and we get a useless long term prediction.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, of course it's exponentially complex.",
                    "label": 1
                },
                {
                    "sent": "Because as we look ahead each step, there will be many possibilities.",
                    "label": 0
                },
                {
                    "sent": "The world is stochastic and also our actions.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have different choices.",
                    "label": 0
                },
                {
                    "sent": "We might look at their actions so it quickly becomes computationally intractable and it will always be computationally intractable to try to look ahead many small steps into the future, like to try to try to iterate your model of physics to get to you, know how much fun will I have going to Montreal and taking the summer school?",
                    "label": 0
                },
                {
                    "sent": "OK, it's crazy.",
                    "label": 1
                },
                {
                    "sent": "And it just doesn't.",
                    "label": 0
                },
                {
                    "sent": "There's no future in that that way of thinking.",
                    "label": 0
                },
                {
                    "sent": "It's a trap, and lots of people are, in my opinion, are falling into it, OK?",
                    "label": 0
                },
                {
                    "sent": "But let's go back to the other side.",
                    "label": 0
                },
                {
                    "sent": "The other side, remember things going backwards here?",
                    "label": 0
                },
                {
                    "sent": "We have these two things.",
                    "label": 0
                },
                {
                    "sent": "Two ways to get away from TD.",
                    "label": 0
                },
                {
                    "sent": "If we don't get away.",
                    "label": 0
                },
                {
                    "sent": "We don't like TD.",
                    "label": 0
                },
                {
                    "sent": "Can we learn a model and iterate it?",
                    "label": 0
                },
                {
                    "sent": "That's the second one or the first one?",
                    "label": 0
                },
                {
                    "sent": "Can we think of this one step thing and justice use?",
                    "label": 0
                },
                {
                    "sent": "Do the one step thing OK so the ones?",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up thing.",
                    "label": 0
                },
                {
                    "sent": "I'll do it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do it in.",
                    "label": 0
                },
                {
                    "sent": "I'm about to transition to my low level part of the talk, but I don't want to try to answer it here, just at the high level.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we can even take questions after this.",
                    "label": 0
                },
                {
                    "sent": "This high level part of the talk.",
                    "label": 1
                },
                {
                    "sent": "Can't we just use our familiar one step supervised learning method and reinforcing?",
                    "label": 1
                },
                {
                    "sent": "These are known as Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "I mean, just roll it out or whatever, see what happens.",
                    "label": 0
                },
                {
                    "sent": "An use that what happens is a target.",
                    "label": 0
                },
                {
                    "sent": "OK, so this has costs.",
                    "label": 0
                },
                {
                    "sent": "Number one causes is you have to make your making this prediction.",
                    "label": 1
                },
                {
                    "sent": "Then you're rolling it out to the end and really going to make a prediction every moment in time.",
                    "label": 0
                },
                {
                    "sent": "So you've got to remember all whole mess of predictions as you as you go out to the end to see the outcome.",
                    "label": 0
                },
                {
                    "sent": "Or really, if you have returns, you get the outcomes of different different steps, different outcomes you have to relate them back to the to the earlier situations.",
                    "label": 0
                },
                {
                    "sent": "It's horribly complex, it's nasty.",
                    "label": 0
                },
                {
                    "sent": "It's first of all you need to remember all the things you did.",
                    "label": 0
                },
                {
                    "sent": "Think about yourself.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a lie in there you're trying to make a good prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, and what do you have?",
                    "label": 0
                },
                {
                    "sent": "You have all this stuff swirling around you.",
                    "label": 0
                },
                {
                    "sent": "You have the hyenas running away.",
                    "label": 0
                },
                {
                    "sent": "You have glimpse of him, you're feeling of your feet.",
                    "label": 0
                },
                {
                    "sent": "Have all this stuff that you can sense, and then you want to relate.",
                    "label": 0
                },
                {
                    "sent": "That to how well it's going?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How good you should feel about this.",
                    "label": 0
                },
                {
                    "sent": "Chase.",
                    "label": 0
                },
                {
                    "sent": "OK, and So what is it sensible to think?",
                    "label": 0
                },
                {
                    "sent": "Or just be much better if you can do it now?",
                    "label": 0
                },
                {
                    "sent": "If right now when all this stuff is in your mind and in your sensors, learn if it's going well or going poorly and learn now as opposed to wait.",
                    "label": 0
                },
                {
                    "sent": "Wait 5 seconds later when you have a whole number large number of frames.",
                    "label": 0
                },
                {
                    "sent": "Of different different sensations, different patterns of sensation you've forgotten.",
                    "label": 0
                },
                {
                    "sent": "You know if you have to wait 5 seconds till the end of it, it's too late.",
                    "label": 0
                },
                {
                    "sent": "You can't remember that.",
                    "label": 1
                },
                {
                    "sent": "Whatever you remembered will be a tiny shadow of the real vivid representation you had at the time it happened.",
                    "label": 0
                },
                {
                    "sent": "OK. And of course, the computation is poorly distributed, you can't.",
                    "label": 0
                },
                {
                    "sent": "Learn now.",
                    "label": 0
                },
                {
                    "sent": "So what are you doing now?",
                    "label": 0
                },
                {
                    "sent": "Later you find out coming after all the learning then just a poor temporal distribution.",
                    "label": 0
                },
                {
                    "sent": "And you can avoid these problems with special methods, and that's what really what TD is about.",
                    "label": 0
                },
                {
                    "sent": "Specialized methods for the multi step case.",
                    "label": 0
                },
                {
                    "sent": "Another reason that you don't want to wait is that sometimes you never know the target.",
                    "label": 0
                },
                {
                    "sent": "Like I don't know.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're playing your chest gay man.",
                    "label": 0
                },
                {
                    "sent": "There's a fire alarm and you never finish your game, so you never see a final outcome.",
                    "label": 0
                },
                {
                    "sent": "But if your TV you know maybe thought you were winning and then you know it was going really poorly and you say you did something bad.",
                    "label": 0
                },
                {
                    "sent": "You can learn without waiting till you're checkmated.",
                    "label": 1
                },
                {
                    "sent": "Maybe the fire alarm, just for like 1 move away from the checkpoint.",
                    "label": 0
                },
                {
                    "sent": "So technically the game never ended and you can learn a lot from your experience so we could try to ignore all these things.",
                    "label": 1
                },
                {
                    "sent": "Think of them as nuisances, but I think of them as clues.",
                    "label": 0
                },
                {
                    "sent": "These are hints from nature, but how we should proceed?",
                    "label": 0
                },
                {
                    "sent": "OK, OK so now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to get down to it more technically.",
                    "label": 0
                },
                {
                    "sent": "But I hope you're starting to see the view I'm trying to present.",
                    "label": 0
                },
                {
                    "sent": "We really need to learn from new predictions so that we can do it as we go along.",
                    "label": 0
                },
                {
                    "sent": "And I think it's really ubiquitous in kind and all the different kinds of learning we're looking at in AI.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm gonna use notations a little bit different than what we've heard earlier today.",
                    "label": 0
                },
                {
                    "sent": "That's what I call my new notation used in the second edition of the Reinforcement Learning textbook.",
                    "label": 1
                },
                {
                    "sent": "Big thing is that we're trying to use the status Sticks Statisticians convention that random variables are capital letters and instances are lower case letters, so all of the things that happen that make up life are capital letters, because there are the random events that actually happened or not possibilities.",
                    "label": 0
                },
                {
                    "sent": "There would ever happen, so S 0 is the first state, a zero is the first action taken in that state are one is the first reward that depends on that state and taking that action in that state and then at the same time as we get the reward we get the new state.",
                    "label": 0
                },
                {
                    "sent": "So I I'd like to give them the same temporal index R1 S one.",
                    "label": 1
                },
                {
                    "sent": "They occur together.",
                    "label": 0
                },
                {
                    "sent": "They are jointly determined in fact.",
                    "label": 0
                },
                {
                    "sent": "OK, and then life goes on and that's the data.",
                    "label": 0
                },
                {
                    "sent": "That's all we have in terms of data.",
                    "label": 0
                },
                {
                    "sent": "We have a trajectory's and maybe a single trajectory and we're interested in classically in the return and the return.",
                    "label": 0
                },
                {
                    "sent": "Is the sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to use Capital R for the return because capital R is actually the actual rewards the sequence of rewards, and so I'm going to find the return.",
                    "label": 1
                },
                {
                    "sent": "I need a new letter.",
                    "label": 0
                },
                {
                    "sent": "I'm calling a G Capital G 'cause it's a random event the whatever some of future rewards after time T actually was going to call that G of TG of T is the return.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As we note here, if I can use this thing, this dot just means that this is a definition is not a.",
                    "label": 1
                },
                {
                    "sent": "Statement of something that's true because it follows from other things that I've said.",
                    "label": 0
                },
                {
                    "sent": "It's the definition, so G of T the return is the sum of the feature awards with the discount rate is the most common way of dealing with it.",
                    "label": 0
                },
                {
                    "sent": "And if that of course can be written now, we're using quality is not a definition that equals the first reward plus gamma times the sum of all the later rewards.",
                    "label": 0
                },
                {
                    "sent": "We're just taking one factor gamma out of all of these guys, so this is no.",
                    "label": 1
                },
                {
                    "sent": "Now, no gamma is this one is 1 game right now with this guys two guys.",
                    "label": 0
                },
                {
                    "sent": "We just taking one out and then the rest of this is a Psalm of future rewards from from one time step later.",
                    "label": 0
                },
                {
                    "sent": "OK so it's a bit like what we started with just like the same thing, it's GMT plus one.",
                    "label": 1
                },
                {
                    "sent": "OK, this is just a definite this is this is.",
                    "label": 1
                },
                {
                    "sent": "This is this is a true equality here there any return to be written as the first reward plus gamma times the next return OK?",
                    "label": 0
                },
                {
                    "sent": "And that is going to be the basis for our temporal difference learning, because we're going to risk going to use this is.",
                    "label": 1
                },
                {
                    "sent": "We're going to do this as a target where you need the reward, next reward plus gamma times the next return.",
                    "label": 1
                },
                {
                    "sent": "Essentially as a target, I guess that's going to be explained right, right?",
                    "label": 0
                },
                {
                    "sent": "Next we look at a state value function OK?",
                    "label": 0
                },
                {
                    "sent": "So for using capital letters for the random variables, I can't use capital V for my for the true value function.",
                    "label": 1
                },
                {
                    "sent": "OK, it's gotta be lower case.",
                    "label": 0
                },
                {
                    "sent": "It's a function of the policy.",
                    "label": 0
                },
                {
                    "sent": "So V Pi of X, where X is any particular state.",
                    "label": 0
                },
                {
                    "sent": "It's an instance.",
                    "label": 0
                },
                {
                    "sent": "Any state lower case S, its value is the expectation of the random variable to return if we started in stat status.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can expect the return could be.",
                    "label": 0
                },
                {
                    "sent": "Under Pie pie is the policy, so in some way of picking the actions.",
                    "label": 0
                },
                {
                    "sent": "Of course the value of state depends what you do if you dance at the top of.",
                    "label": 1
                },
                {
                    "sent": "The Grand Canyon.",
                    "label": 0
                },
                {
                    "sent": "It might be bad, but if you can sedate Lee, walk up to the railings, it's good.",
                    "label": 0
                },
                {
                    "sent": "So Policy's value fund values depend on policies.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So then this is the return we can just use the above equation.",
                    "label": 1
                },
                {
                    "sent": "The return can be written as the first reward plus gamma times the rest of the return.",
                    "label": 0
                },
                {
                    "sent": "And then since we're taking the expectation of this, this is the expected next reward and expected value of the next state.",
                    "label": 0
                },
                {
                    "sent": "OK, so that naturally leads to the notion of an error we can use.",
                    "label": 0
                },
                {
                    "sent": "We can compare the estimate estimated value of a state at some time to the reward and the estimated value of the next date.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's going to be RTD error.",
                    "label": 1
                },
                {
                    "sent": "This is what we're going to use to replace.",
                    "label": 0
                },
                {
                    "sent": "The normal conventional error.",
                    "label": 0
                },
                {
                    "sent": "Now this V. It is a random variable.",
                    "label": 0
                },
                {
                    "sent": "This is our estimate.",
                    "label": 0
                },
                {
                    "sent": "Their estimate will depend on what happens and so that is random.",
                    "label": 1
                },
                {
                    "sent": "The estimated values are a random value or random function, and so it's capital and that's our tier.",
                    "label": 0
                },
                {
                    "sent": "You got it.",
                    "label": 0
                },
                {
                    "sent": "The TD error is that clear.",
                    "label": 0
                },
                {
                    "sent": "Good, OK, so now let's talk about our methods.",
                    "label": 0
                },
                {
                    "sent": "I want to contrast supervised learning and remember I said supervised learning is called Monte Carlo in this context.",
                    "label": 0
                },
                {
                    "sent": "So what exactly is that?",
                    "label": 0
                },
                {
                    "sent": "That's we take our estimated value function for the state that we run into V of S of T. OK, we're going to update it based upon some experience.",
                    "label": 0
                },
                {
                    "sent": "OK, here's the experience here.",
                    "label": 0
                },
                {
                    "sent": "We are aggressive T and this is the tree of the things that might happen.",
                    "label": 1
                },
                {
                    "sent": "Like we might pick either of these two actions, and if we did pick this action.",
                    "label": 0
                },
                {
                    "sent": "Either of these two states by to rise.",
                    "label": 0
                },
                {
                    "sent": "So yeah, black dots are actions to open.",
                    "label": 0
                },
                {
                    "sent": "Dots are states.",
                    "label": 1
                },
                {
                    "sent": "So basically this is this is the tree of all the features that might happen.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're imagining that there are terminal states.",
                    "label": 0
                },
                {
                    "sent": "We're basically adding things up until we reach a terminal state.",
                    "label": 0
                },
                {
                    "sent": "So here is a particular trajectory that might happen.",
                    "label": 0
                },
                {
                    "sent": "Let's say it did happen from that state.",
                    "label": 0
                },
                {
                    "sent": "We went this, this and then we terminated OK, so we now once we've terminated, we know what G is.",
                    "label": 0
                },
                {
                    "sent": "We know what the return is and we can do this update rule.",
                    "label": 0
                },
                {
                    "sent": "We can compare our estimate.",
                    "label": 0
                },
                {
                    "sent": "For the state at time T up here to the actual return, and we make that error and then we do an increment.",
                    "label": 0
                },
                {
                    "sent": "I should.",
                    "label": 0
                },
                {
                    "sent": "This is the step size Alpha.",
                    "label": 0
                },
                {
                    "sent": "There's a number like .1, so we increment org.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Towards this target?",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's a standard.",
                    "label": 0
                },
                {
                    "sent": "Would be a Monte Carlo learning rule.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning rule an.",
                    "label": 0
                },
                {
                    "sent": "That's the competition for the TD method.",
                    "label": 0
                },
                {
                    "sent": "The simplest TV method looks instead like this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We only look ahead one step were at FT. We look ahead at one.",
                    "label": 0
                },
                {
                    "sent": "We see the reward that happens and the next state that happens and based on those we formed this TD air which is against comparing comparing, we're updating the estimate of this for the state at time T. So we're going there, but we're guessing for that state.",
                    "label": 0
                },
                {
                    "sent": "And this new target reward plus gamma times the estimated value of the next date.",
                    "label": 0
                },
                {
                    "sent": "OK, now you've probably also heard about dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can think you can put dynamic programming in the same figure, the same kind of figure.",
                    "label": 0
                },
                {
                    "sent": "And if the dynamic querying version looks like this, 'cause it's not considering a single line through the possible tree, it's considering all possibilities.",
                    "label": 0
                },
                {
                    "sent": "It's considering both actions.",
                    "label": 0
                },
                {
                    "sent": "And both possible next states, so this way you need the model of the world, because although you know you're probably picking each action that probably the world will give you possible next states will be known only to the world.",
                    "label": 0
                },
                {
                    "sent": "But in dynamic programming you assume you know all that.",
                    "label": 0
                },
                {
                    "sent": "So in dynamic programming the equation is that the value the estimated value for a state is moved towards the expectation of the first reward and the expectation of gamma times the value of the next state.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's this expectation and that's what makes it dynamic.",
                    "label": 0
                },
                {
                    "sent": "Programming 'cause it's you see me know all the probabilities you can figure out.",
                    "label": 0
                },
                {
                    "sent": "That expectation doesn't give you the answer because your value will still be.",
                    "label": 0
                },
                {
                    "sent": "You're still learning a guest from a guess.",
                    "label": 0
                },
                {
                    "sent": "You're learning your new estimate.",
                    "label": 0
                },
                {
                    "sent": "Still, from your old estimate.",
                    "label": 0
                },
                {
                    "sent": "Well, that's a number for him, so so really.",
                    "label": 0
                },
                {
                    "sent": "We can say the following.",
                    "label": 0
                },
                {
                    "sent": "Not special.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About T methods to say bootstrap and sample.",
                    "label": 1
                },
                {
                    "sent": "So bootstrapping is this idea that your target involves a guess, an existing prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so Monte Carlo on Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "The whole point is that it doesn't bootstrap, it's just looking all the way to the end and seeing what the return is.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no estimates playing a role in the return.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming also bootstraps and therefore he says, look ahead one step and look at the expected value of the next state and back it up.",
                    "label": 0
                },
                {
                    "sent": "So you're only you're using your estimates.",
                    "label": 0
                },
                {
                    "sent": "And your estimates gradually get better.",
                    "label": 0
                },
                {
                    "sent": "TD of course also is using your estimate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's like Monte Carlo and TDR learning methods.",
                    "label": 0
                },
                {
                    "sent": "I guess that's my next point.",
                    "label": 0
                },
                {
                    "sent": "Learning methods Monte Carlo and TD.",
                    "label": 1
                },
                {
                    "sent": "They sample they sample what happens because you don't know how the world works and dynamic programming does not sample.",
                    "label": 0
                },
                {
                    "sent": "Just use the expectation.",
                    "label": 0
                },
                {
                    "sent": "It assumes you know what will happen, what could happen.",
                    "label": 1
                },
                {
                    "sent": "So those are the two basic dimensions, whether you're sampling and therefore learning.",
                    "label": 0
                },
                {
                    "sent": "And whether you are bootstrapping, you're using your bootstrapping your estimate from other estimates.",
                    "label": 0
                },
                {
                    "sent": "You're learning guesses from guesses.",
                    "label": 0
                },
                {
                    "sent": "And so T.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The prediction.",
                    "label": 0
                },
                {
                    "sent": "Basically, I'm just saying this is this is the update you saw before rusting.",
                    "label": 0
                },
                {
                    "sent": "The Monte Carlo is here and the TV is there, so just.",
                    "label": 1
                },
                {
                    "sent": "The contrast is that one, the target is the actual return and the other is the target.",
                    "label": 1
                },
                {
                    "sent": "Is this sort of?",
                    "label": 0
                },
                {
                    "sent": "One step estimate of what their return will be.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's think let's do an example, so here I am.",
                    "label": 0
                },
                {
                    "sent": "I'm coming home after working a hard day at the office and I'm trying to guess how long it will take me to get home.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm leaving my office.",
                    "label": 0
                },
                {
                    "sent": "It's Friday at 6:00 o'clock.",
                    "label": 1
                },
                {
                    "sent": "I have some other features and I make a guess of how long it will take, so I will.",
                    "label": 0
                },
                {
                    "sent": "I'm going to guess it'll take 30 minutes to get home.",
                    "label": 0
                },
                {
                    "sent": "OK, so and that's my prediction of my total time 'cause I haven't gone.",
                    "label": 1
                },
                {
                    "sent": "I'm just starting now so my elapsed time is 0.",
                    "label": 0
                },
                {
                    "sent": "Now as I come out of the, I come out of my building, go to the parking lot an I see it's raining OK and it's raining you know it's going to take me longer because everyone drives slower in the rain.",
                    "label": 0
                },
                {
                    "sent": "So I think.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, it's already.",
                    "label": 0
                },
                {
                    "sent": "I've already spent 5 minutes just getting down from my office into the parking line.",
                    "label": 1
                },
                {
                    "sent": "And then I also think it's going to take me longer.",
                    "label": 0
                },
                {
                    "sent": "I think it took me 35 minutes from now for a total of 40 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want you to see.",
                    "label": 1
                },
                {
                    "sent": "The first thing you want to see is that.",
                    "label": 0
                },
                {
                    "sent": "My guess about how long it's going to take me and my guess is that the total time to go home.",
                    "label": 0
                },
                {
                    "sent": "It's constantly changing as I get more information.",
                    "label": 0
                },
                {
                    "sent": "I revised my estimates.",
                    "label": 0
                },
                {
                    "sent": "OK, so to carry the example through, I get, I get, I start getting my car.",
                    "label": 0
                },
                {
                    "sent": "I drive on the highway.",
                    "label": 0
                },
                {
                    "sent": "It turns out it didn't take it takes as long as I thought.",
                    "label": 0
                },
                {
                    "sent": "I've spent 20 minutes total now and I think it'll only take me 15 more to go home.",
                    "label": 0
                },
                {
                    "sent": "It wasn't so bad with in the rain and so that's 35 minutes total.",
                    "label": 1
                },
                {
                    "sent": "Now this this car.",
                    "label": 0
                },
                {
                    "sent": "This column is my total estimate as it goes up and down, and then I get stuck behind a truck on a secondary road and I think it's gonna take me longer and then I reached my home screen.",
                    "label": 1
                },
                {
                    "sent": "I think will take 43 minutes and it does take me 43 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK so that's a possible thing that might happen.",
                    "label": 0
                },
                {
                    "sent": "A possible trajectory an what I want you to ask is what you might learn from that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you're doing a Monte Carlo methods, you just say, well, it took me 43 minutes to get home.",
                    "label": 1
                },
                {
                    "sent": "That's the answer.",
                    "label": 0
                },
                {
                    "sent": "So all my estimates.",
                    "label": 0
                },
                {
                    "sent": "My first initial estimate of 30 minutes.",
                    "label": 0
                },
                {
                    "sent": "That's going to move towards 43 minutes.",
                    "label": 0
                },
                {
                    "sent": "That's the error.",
                    "label": 0
                },
                {
                    "sent": "In fact, all of these will be moved up towards 43 minutes, because whatever guess I made at each point in time.",
                    "label": 0
                },
                {
                    "sent": "Could be moved towards what actually happened or whatever.",
                    "label": 0
                },
                {
                    "sent": "Whatever is remaining in the future at that point.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you're using a TD method.",
                    "label": 0
                },
                {
                    "sent": "If you're using your guests, going to learn a guest from a guest and then something very different happens.",
                    "label": 0
                },
                {
                    "sent": "Even some of the signs change.",
                    "label": 0
                },
                {
                    "sent": "So your first prediction will move up because you start out 30 and then after we found out it's raining, so you move up.",
                    "label": 0
                },
                {
                    "sent": "But this one, for example, will move down.",
                    "label": 0
                },
                {
                    "sent": "And the actual that all the errors are different.",
                    "label": 0
                },
                {
                    "sent": "All the errors are different.",
                    "label": 0
                },
                {
                    "sent": "The long, long long runs law wash out.",
                    "label": 0
                },
                {
                    "sent": "But for all, actual learning is a law is very different.",
                    "label": 0
                },
                {
                    "sent": "OK, now I also want you to think about the computational consequences.",
                    "label": 0
                },
                {
                    "sent": "OK, if you're doing TDD, then you know when you're here then you go to the next stage.",
                    "label": 0
                },
                {
                    "sent": "You get an error and you can update right away.",
                    "label": 0
                },
                {
                    "sent": "You can say well, why did I make that prediction?",
                    "label": 0
                },
                {
                    "sent": "What are my features there?",
                    "label": 0
                },
                {
                    "sent": "How should I change those?",
                    "label": 0
                },
                {
                    "sent": "What are the contents of my deep network that led me to make that prediction?",
                    "label": 0
                },
                {
                    "sent": "I need to change those.",
                    "label": 0
                },
                {
                    "sent": "That's true at each step, and so when you go from here to here to here, you can update this guy and then you can forget about it, but you're never going to update him again.",
                    "label": 0
                },
                {
                    "sent": "Where is Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "You have to remember why you made each one of these predictions until you get to the end.",
                    "label": 0
                },
                {
                    "sent": "Then you have to go back and say, well, OK, why did I make that one and then and then adjust its weights with knowledge of its feature vector and the contents of your network and so on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and it's terrible.",
                    "label": 0
                },
                {
                    "sent": "It's distribution because you all this time you're doing nothing, you're driving home, you can't do any learning.",
                    "label": 0
                },
                {
                    "sent": "OK, you can only wait till the end.",
                    "label": 0
                },
                {
                    "sent": "You know the answer and then you can go back and look at all the earlier things and learn them.",
                    "label": 0
                },
                {
                    "sent": "So the distribution of computation is poor, the memory is poor.",
                    "label": 0
                },
                {
                    "sent": "It's it's just kind of inconvenient.",
                    "label": 0
                },
                {
                    "sent": "It's much more convenient if you just go along and you think about it, you're in your car.",
                    "label": 0
                },
                {
                    "sent": "You're trying to drive home.",
                    "label": 0
                },
                {
                    "sent": "You get stuck behind a truck, do you say?",
                    "label": 0
                },
                {
                    "sent": "You say you say this is bad, you know I say it's going to take me longer than I thought.",
                    "label": 0
                },
                {
                    "sent": "I was too optimistic before.",
                    "label": 0
                },
                {
                    "sent": "You don't say, well, you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe this truck will disappear and you don't say, hold, hold judgment.",
                    "label": 0
                },
                {
                    "sent": "You could hold judgment until you get home, but you know my feeling is I'm learning as I go along and I'm responding to what I see and we actually do learn as we go along, OK?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think I've said these things and TD with Monte Carlo you can be fully incremental learning as you go along.",
                    "label": 1
                },
                {
                    "sent": "You can learn before you know the final outcome.",
                    "label": 0
                },
                {
                    "sent": "This means you need less memory and less peak computation.",
                    "label": 0
                },
                {
                    "sent": "You don't do it all at the end.",
                    "label": 1
                },
                {
                    "sent": "You can even learn if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you ever find out how long it takes you to actually go home.",
                    "label": 0
                },
                {
                    "sent": "Maybe you get a phone call and you're called away for something important and you never find out, but you can learn without knowing the final final outcome.",
                    "label": 1
                },
                {
                    "sent": "Now when you do the math, both of these methods will converge and.",
                    "label": 0
                },
                {
                    "sent": "But the only question is which is faster?",
                    "label": 0
                },
                {
                    "sent": "OK, this is the only question, but it's a big question.",
                    "label": 0
                },
                {
                    "sent": "OK so I don't know.",
                    "label": 0
                },
                {
                    "sent": "Let's just do a simple experiment and find find out OK, so here's a trivial.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment.",
                    "label": 0
                },
                {
                    "sent": "Famous famous I know I did this along time ago.",
                    "label": 0
                },
                {
                    "sent": "Just a random walk and it's meant to test the idea of which one of these is better.",
                    "label": 1
                },
                {
                    "sent": "So we're going to have 5 States and we're just going to estimate for each state of what the outcome will be OK, this random walk, it takes 50 step right and left and you start in the middle and you go back and forth back and forth, back and forth until you end at one side.",
                    "label": 0
                },
                {
                    "sent": "OK, if you ended this side, you get a zero and you do get zeros all along the way for your reward.",
                    "label": 0
                },
                {
                    "sent": "But if you know you get a non zeros if you end on the right side you could get a reward of one.",
                    "label": 0
                },
                {
                    "sent": "OK so are you with me?",
                    "label": 0
                },
                {
                    "sent": "What's the correct prediction?",
                    "label": 0
                },
                {
                    "sent": "So the correct prediction?",
                    "label": 1
                },
                {
                    "sent": "There's no discounting here, so we're just trying to predict the sum of the rewards up until the end.",
                    "label": 0
                },
                {
                    "sent": "What's the correct prediction for the start State C?",
                    "label": 0
                },
                {
                    "sent": "You're in C. What's the correct prediction for the expected value of your return?",
                    "label": 0
                },
                {
                    "sent": "Squared gamma squared gamma is 1.",
                    "label": 0
                },
                {
                    "sent": "So the expected expected return.",
                    "label": 0
                },
                {
                    "sent": "So if you if you end if you go blah blah blah blah blah and you end on this side, the return is.",
                    "label": 0
                },
                {
                    "sent": "What if you on the other side?",
                    "label": 0
                },
                {
                    "sent": "They return SP0, then you start in the middle.",
                    "label": 0
                },
                {
                    "sent": "Where do we expect the return to be?",
                    "label": 0
                },
                {
                    "sent": "You know, by symmetry it's going to be like .5.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 0
                },
                {
                    "sent": "State B. I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's going to be less than .5.",
                    "label": 0
                },
                {
                    "sent": "And stay still less.",
                    "label": 0
                },
                {
                    "sent": "Anyone want to guess what they are?",
                    "label": 0
                },
                {
                    "sent": "The true values of all the states.",
                    "label": 0
                },
                {
                    "sent": "See is definitely 1/2.",
                    "label": 0
                },
                {
                    "sent": "What do you think B is?",
                    "label": 0
                },
                {
                    "sent": "Guess just gas.",
                    "label": 1
                },
                {
                    "sent": "Third, yeah, I think that is 1/3 an in the next one is 1/6.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this just go by 61626364656 and those are plotted here.",
                    "label": 0
                },
                {
                    "sent": "This'll in line is supposed to be the true values, so a state a true that has a true value.",
                    "label": 0
                },
                {
                    "sent": "That's one 6th and State B is the true values 1/3.",
                    "label": 0
                },
                {
                    "sent": "This is the true value of 1/2.",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "And these other lines are the estimated values from applying TD to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so TD you do have to care about the initial conditions, 'cause it's making a guess my guess right?",
                    "label": 0
                },
                {
                    "sent": "So your guess is you know affect things.",
                    "label": 0
                },
                {
                    "sent": "They either pollute things or or brilliantly provide good guesses and the value.",
                    "label": 1
                },
                {
                    "sent": "OK, so the initial guess is 0.",
                    "label": 0
                },
                {
                    "sent": "So at time at episode 0.",
                    "label": 0
                },
                {
                    "sent": "All of the estimated values or extra?",
                    "label": 1
                },
                {
                    "sent": "Excuse me.",
                    "label": 0
                },
                {
                    "sent": "Yes, all the estimated values are half because since there could be zeros and ones for possible returns, it seems reasonable to start the estimated values all over half of that happens to be right for the middle state, but it's quite a bit long for the other States and then we're going to do.",
                    "label": 0
                },
                {
                    "sent": "Episodes were going to learn on every time step, and we're going to update the states according to the D rule.",
                    "label": 0
                },
                {
                    "sent": "And after one episode we have this this darker line.",
                    "label": 0
                },
                {
                    "sent": "This is the episode number after one episode.",
                    "label": 0
                },
                {
                    "sent": "We have these values for estimated values of the five states, right?",
                    "label": 0
                },
                {
                    "sent": "So what do you know happened on the 1st episode?",
                    "label": 0
                },
                {
                    "sent": "You ended on this side because.",
                    "label": 0
                },
                {
                    "sent": "Well, what's going to?",
                    "label": 0
                },
                {
                    "sent": "What does this TV rule going to do?",
                    "label": 0
                },
                {
                    "sent": "What is what is the TD error going to do?",
                    "label": 0
                },
                {
                    "sent": "Let's say if we start in the middle and we could go either way, we move around.",
                    "label": 0
                },
                {
                    "sent": "Well, what's the TD error going to be?",
                    "label": 0
                },
                {
                    "sent": "It's going to be the reward.",
                    "label": 0
                },
                {
                    "sent": "The reward beginnings can be 0.",
                    "label": 0
                },
                {
                    "sent": "And then it's gammas winds.",
                    "label": 0
                },
                {
                    "sent": "We forget about gamma and then we just basically the change in the value.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you went from, say, this state to this state, what's the change in value?",
                    "label": 0
                },
                {
                    "sent": "Zero is there all.",
                    "label": 0
                },
                {
                    "sent": "Their estimated values are all 1/2.",
                    "label": 0
                },
                {
                    "sent": "And so we went from one state from estimate of after another state's estimate of the House.",
                    "label": 0
                },
                {
                    "sent": "So as we go all this balancing all around, nothing is going to happen really until we run into one of the ends.",
                    "label": 0
                },
                {
                    "sent": "Well, we ran, we can run into this end, or we can run to that end, if we ran into this end, will go from a state that was a half.",
                    "label": 0
                },
                {
                    "sent": "To the terminal, say Terminal State, always by definition has a value of 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so over here, if you if you did this transition, you get a reward of 1.",
                    "label": 0
                },
                {
                    "sent": "A reward of 1 the starting state starting state here would be.",
                    "label": 0
                },
                {
                    "sent": "1/2 and you get this thing that has zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so it'll be one.",
                    "label": 0
                },
                {
                    "sent": "Minus 1/2 it'll be positive half anyway.",
                    "label": 0
                },
                {
                    "sent": "The estimate of state E would go up.",
                    "label": 1
                },
                {
                    "sent": "And that didn't happen.",
                    "label": 0
                },
                {
                    "sent": "'cause here's the scientist 80.",
                    "label": 0
                },
                {
                    "sent": "It's still at 1/2.",
                    "label": 0
                },
                {
                    "sent": "Instead what happened is we ended on this side we went from here that had estimated value half to this thing, which has estimated value 0.",
                    "label": 0
                },
                {
                    "sent": "Terminal State and so we went from half.",
                    "label": 0
                },
                {
                    "sent": "To zero and RTD, air is minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "So we moved down from a half Tord Zero, and you can actually see how far we move.",
                    "label": 0
                },
                {
                    "sent": "We actually moved by.",
                    "label": 0
                },
                {
                    "sent": "From a half, 2.45 and so our step size Alpha was ten was 110th.",
                    "label": 0
                },
                {
                    "sent": "OK. We can understand this algorithm very simple.",
                    "label": 0
                },
                {
                    "sent": "And then as you get more episodes, you get closer and closer to the true value after 10 episodes.",
                    "label": 0
                },
                {
                    "sent": "If the blue line after 1000 episodes, you get close to the true values, you never get exactly to the true values, 'cause there's always randomness in the individual episode.",
                    "label": 0
                },
                {
                    "sent": "An Alpha is non zero.",
                    "label": 0
                },
                {
                    "sent": "It's a 10th and so you keep bumping around bubbling around the true values.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "That's an example.",
                    "label": 0
                },
                {
                    "sent": "Now let's compare now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Monte Carlo versus TD.",
                    "label": 0
                },
                {
                    "sent": "On this problem.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and we actually draw whole learning curves now and we have to worry about what's the value of the step size.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm showing you in this is a learning curve, meaning the X axis is time.",
                    "label": 0
                },
                {
                    "sent": "Or episode number.",
                    "label": 0
                },
                {
                    "sent": "And the Y axis is some measure of error.",
                    "label": 0
                },
                {
                    "sent": "It's actually the root mean square error averaged over the five States and many.",
                    "label": 1
                },
                {
                    "sent": "Many iterations of the whole experiment, I guess 100 iterations of the whole experiment.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "As I said, they're going down in.",
                    "label": 0
                },
                {
                    "sent": "Everything is getting better overtime, but things will not go to zero because we have the step size 110th and or whatever step sizes.",
                    "label": 0
                },
                {
                    "sent": "It's always going to be there and we're always going to have some residual error I.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So which one should we look at first?",
                    "label": 0
                },
                {
                    "sent": "Maybe Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "It's simpler.",
                    "label": 0
                },
                {
                    "sent": "We just Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "You just wait until you know the final return is and then you do your update for all the states that were visited.",
                    "label": 0
                },
                {
                    "sent": "So we take a nice slow learning rate, 100th.",
                    "label": 0
                },
                {
                    "sent": "We just gradually move down towards 0 error and it's actually this Alpha equals 100.",
                    "label": 0
                },
                {
                    "sent": "Very slow actually get the closest in the long long long run.",
                    "label": 0
                },
                {
                    "sent": "20 error, but it's very slow so you might want to go faster if we take Alpha is 150th we go down faster, but we're not.",
                    "label": 0
                },
                {
                    "sent": "We're going to start to bubble and we tried tried going as fast as.",
                    "label": 0
                },
                {
                    "sent": ".04 an we do go.",
                    "label": 0
                },
                {
                    "sent": "The initial part is fastest, but now we're definitely bubbling and you can't really do better.",
                    "label": 0
                },
                {
                    "sent": "There's no step size which will do better than the ones that are shown here if you try to go faster, you're going to.",
                    "label": 0
                },
                {
                    "sent": "You may be a little bit faster, very beginning, but you're going to leverage level out at a higher level.",
                    "label": 0
                },
                {
                    "sent": "OK, and for TD we see a similar pattern, but all the numbers are lower.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's our one of the slowest one I'm showing here is .05 and it goes it's lowest, but it gets slower and then other ones are faster.",
                    "label": 0
                },
                {
                    "sent": "And of course they bubble more and they.",
                    "label": 0
                },
                {
                    "sent": "They don't get as low in the long run.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "We have long someone may ask you, some of you may be wondering what's going on with that TD stuff, because it seems like they go down and they start to come up again.",
                    "label": 0
                },
                {
                    "sent": "Anybody wondering that anybody wondering that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's weird, isn't it?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's real, it's not a bug in my program.",
                    "label": 0
                },
                {
                    "sent": "That's the first thing.",
                    "label": 0
                },
                {
                    "sent": "To be sure of.",
                    "label": 0
                },
                {
                    "sent": "Yeah it has to do with the fact that actually starting out the estimates at 1/2 is not not stupid.",
                    "label": 0
                },
                {
                    "sent": "It's actually a reasonable guess.",
                    "label": 0
                },
                {
                    "sent": "If you started all the estimates out at something really bad then you wouldn't see that bounce off the balance as we go down.",
                    "label": 0
                },
                {
                    "sent": "Then we seem to balance.",
                    "label": 0
                },
                {
                    "sent": "We come up a little bit higher.",
                    "label": 0
                },
                {
                    "sent": "And that balance is really interesting.",
                    "label": 0
                },
                {
                    "sent": "It has to do with the fact that we do have some some effect of the initial estimates in TD, and whereas we don't really, at least not as much for Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "This is just a random walk and I've sort of been systematic about the random walk.",
                    "label": 0
                },
                {
                    "sent": "And I don't know the big picture is that TD is faster.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a balance.",
                    "label": 0
                },
                {
                    "sent": "OK, whatever, but it's still much faster.",
                    "label": 0
                },
                {
                    "sent": "But this is just one problem.",
                    "label": 0
                },
                {
                    "sent": "This is just a random walk.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe it's something special about the random walk.",
                    "label": 0
                },
                {
                    "sent": "Or maybe if I did on, you know Atari games.",
                    "label": 0
                },
                {
                    "sent": "I would get a more fundamental result.",
                    "label": 0
                },
                {
                    "sent": "I like to do simple things question.",
                    "label": 0
                },
                {
                    "sent": "Doesn't converge.",
                    "label": 0
                },
                {
                    "sent": "Oh no, they they all converge.",
                    "label": 0
                },
                {
                    "sent": "Even with the non well with a non 0 if you don't reduce the step size then you don't expect anything converge right?",
                    "label": 0
                },
                {
                    "sent": "They will converge in the mean OK and all of them will converge to a mean that depends on the step size and higher step size would be higher.",
                    "label": 0
                },
                {
                    "sent": "Lower step size be lower convergence point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, convergence properties are roughly the same in both cases.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want I want to.",
                    "label": 0
                },
                {
                    "sent": "I want to ask now, can I say anything about the general case, not for the random walk?",
                    "label": 0
                },
                {
                    "sent": "But can I say general case?",
                    "label": 0
                },
                {
                    "sent": "Actually no, no.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do that in a minute.",
                    "label": 0
                },
                {
                    "sent": "But first I'm going to the random walk again under this setting called batch updating.",
                    "label": 1
                },
                {
                    "sent": "OK, batch updating means we take some training set like 100 episodes or 10 episodes or whatever, and we presented over and over again until it does converge.",
                    "label": 1
                },
                {
                    "sent": "So even for finite stepsize, we'll get complete convergence if we repeatedly present the same training set.",
                    "label": 0
                },
                {
                    "sent": "OK. Because there is no randomness in random samples are just spinning the same data over and over again and you will converge.",
                    "label": 0
                },
                {
                    "sent": "The two methods TD in mind will converge to two different things.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is for constant Alpha.",
                    "label": 0
                },
                {
                    "sent": "These problems converge to different things as long as your step size is small enough, it won't depend on your step size.",
                    "label": 0
                },
                {
                    "sent": "Yeah, all step sizes, as long as they're small enough so that you don't diverge, will converge the same thing OK, and they converge to different things.",
                    "label": 1
                },
                {
                    "sent": "The two algorithms converge to different things, so we can ask.",
                    "label": 1
                },
                {
                    "sent": "On this problem, which one converges to a better thing?",
                    "label": 0
                },
                {
                    "sent": "If we present the data over and over again to the algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the results on the random walk.",
                    "label": 1
                },
                {
                    "sent": "Again we have.",
                    "label": 0
                },
                {
                    "sent": "We have different numbers of different sizes.",
                    "label": 0
                },
                {
                    "sent": "Are training sets were increasing that along here, but for each case they say with 50 training set of 50 episodes, we present those 50 over and over and over again until we converge and we measure the asymptotic error.",
                    "label": 0
                },
                {
                    "sent": "That is independent of the steps so I can eliminate this effect of the step size just to measure which algorithm is getting me a better result on this problem.",
                    "label": 0
                },
                {
                    "sent": "OK. Anti D is faster by this measure.",
                    "label": 0
                },
                {
                    "sent": "I mean we're doing a lot more computation.",
                    "label": 0
                },
                {
                    "sent": "We have to go to convergence.",
                    "label": 0
                },
                {
                    "sent": "We have to repeatedly present things, none of which I like.",
                    "label": 0
                },
                {
                    "sent": "But is getting us insight into what the real difference between the two algorithms, so it's like TD is moving towards a better place even on a single example, as suggested by the initial results, an if you go over and over again, you can get that OK. Now this again is all random walk and you have to ask if this is happens on all problems.",
                    "label": 0
                },
                {
                    "sent": "So one approach would be to do all problems and.",
                    "label": 0
                },
                {
                    "sent": "That's obviously not satisfactory.",
                    "label": 0
                },
                {
                    "sent": "So So what can you do instead?",
                    "label": 1
                },
                {
                    "sent": "You can try to prove a theorem OK, and you can.",
                    "label": 0
                },
                {
                    "sent": "You can also try to get insight.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm going to try to get insight first.",
                    "label": 0
                },
                {
                    "sent": "And then we'll do the formal result.",
                    "label": 0
                },
                {
                    "sent": "So let's try to get insight.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Glasses, people.",
                    "label": 0
                },
                {
                    "sent": "I want you to.",
                    "label": 0
                },
                {
                    "sent": "You'd be the predictor.",
                    "label": 0
                },
                {
                    "sent": "Imagine you were having some experience, so I imagine you are experiencing a training set of these eight 8 episodes.",
                    "label": 1
                },
                {
                    "sent": "These are all very short episodes, right?",
                    "label": 0
                },
                {
                    "sent": "So most of them are episodes like B0 means I'm in state B and then I get a reward of zero and the end of the episode at the end of the episode.",
                    "label": 0
                },
                {
                    "sent": "OK, or I see state BI get a reward of 1 and that's the end of the episode.",
                    "label": 0
                },
                {
                    "sent": "The only nontrivial episode is this first line running state A. I get a zero and then I go to State B and from BI get roller to 0 and that's the end of the episode.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the data you see just these eight episodes and I want you to tell me what.",
                    "label": 0
                },
                {
                    "sent": "What prediction would you make?",
                    "label": 0
                },
                {
                    "sent": "OK, first question is what prediction would you make for state B?",
                    "label": 0
                },
                {
                    "sent": "If you found yourself in State B.",
                    "label": 0
                },
                {
                    "sent": "What would you guess for the expected return ahead of you?",
                    "label": 0
                },
                {
                    "sent": "From state B.",
                    "label": 0
                },
                {
                    "sent": "Say again.",
                    "label": 0
                },
                {
                    "sent": "3/4 I agree because what would we do that we said?",
                    "label": 0
                },
                {
                    "sent": "Well, I was in state B all eight times and six of them ended up with one and two of them ended up with a 0.",
                    "label": 0
                },
                {
                    "sent": "So you're going to get 3/4 OK. OK, that was easy one.",
                    "label": 0
                },
                {
                    "sent": "What about stayte?",
                    "label": 0
                },
                {
                    "sent": "Stay, it's really much more uncertain if only been, say, once.",
                    "label": 0
                },
                {
                    "sent": "Anne, what are you going to ask for?",
                    "label": 0
                },
                {
                    "sent": "State agents?",
                    "label": 0
                },
                {
                    "sent": "They take a moment and think about it.",
                    "label": 0
                },
                {
                    "sent": "What are you going to get to return if you find yourself against A tag?",
                    "label": 0
                },
                {
                    "sent": "When is the estimated value?",
                    "label": 0
                },
                {
                    "sent": "Well, how would you estimate the value of state as OK?",
                    "label": 0
                },
                {
                    "sent": "Now let me just say right away this is a question.",
                    "label": 0
                },
                {
                    "sent": "There's multiple good answers to OK, so I'd like someone to raise your hand and give me one.",
                    "label": 0
                },
                {
                    "sent": "Give me one answer and why it's a good answer.",
                    "label": 0
                },
                {
                    "sent": "OK, how about you?",
                    "label": 0
                },
                {
                    "sent": "You always go from A to B and the only time you've seen a went from A to B&B has value 75.",
                    "label": 0
                },
                {
                    "sent": "Percent 3/4 then Asia also.",
                    "label": 0
                },
                {
                    "sent": "That sort of makes sense.",
                    "label": 0
                },
                {
                    "sent": "What's another good answer?",
                    "label": 0
                },
                {
                    "sent": "Zero because every time at 8 degrees awarded 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know if everyone heard that you said you've seen a once every time you saw it the return was zero, so you know might not predict zero.",
                    "label": 0
                },
                {
                    "sent": "OK, now those two answers.",
                    "label": 1
                },
                {
                    "sent": "Those two answers are the two are modular Monte Carlo's answers and TDs answers.",
                    "label": 0
                },
                {
                    "sent": "OK, so we could say 0.",
                    "label": 0
                },
                {
                    "sent": "That's what Monte Carlo would say.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo just looks at what happened.",
                    "label": 0
                },
                {
                    "sent": "I was in a once and the outcome was zero, so I should predict 0.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Now the other one, the other one is what TD predicts.",
                    "label": 0
                },
                {
                    "sent": "It's also what you would predict, and this is the gentleman explained what was going on in his head.",
                    "label": 0
                },
                {
                    "sent": "He was saying, well, I'd seen a go to be an with an all that sort of stuff he was building in his head.",
                    "label": 0
                },
                {
                    "sent": "This model.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using I've seen the only time I seen a it went to state B.",
                    "label": 0
                },
                {
                    "sent": "By the way, the reward was zero on that transition, so let's guess that happens every time.",
                    "label": 0
                },
                {
                    "sent": "And then in BBI saw eight times and six out of eight in one way to one and then stopped an two out of the.",
                    "label": 0
                },
                {
                    "sent": "And I saw 8 * 6 out of the 8 winter went to one and two out of the eight went to a 0.",
                    "label": 0
                },
                {
                    "sent": "So I'm building this in my head.",
                    "label": 0
                },
                {
                    "sent": "OK this is like and this has a name.",
                    "label": 0
                },
                {
                    "sent": "This is called the.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood model of the MDP.",
                    "label": 0
                },
                {
                    "sent": "And just just just means what you would get by counting.",
                    "label": 0
                },
                {
                    "sent": "Say how often do you go from here?",
                    "label": 0
                },
                {
                    "sent": "Turn those into into probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the maximum likelihood model of the underlying Markov process, and then if you take this model and you solve it.",
                    "label": 0
                },
                {
                    "sent": "If this is, this is the true world, then the true value is 3/4.",
                    "label": 0
                },
                {
                    "sent": "OK. And so this is the general phenomenon of what TD does.",
                    "label": 0
                },
                {
                    "sent": "If you present a training set over and over again to it it it.",
                    "label": 0
                },
                {
                    "sent": "It gets the answer that you would get if you collected all the data and made a maximum likelihood model of the world and then solve that model with dynamic programming or with any method the true the true solution if that model was this was the reality.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "That's why.",
                    "label": 0
                },
                {
                    "sent": "TD is can be faster.",
                    "label": 0
                },
                {
                    "sent": "Can be better 'cause it's using this Markov property that.",
                    "label": 0
                },
                {
                    "sent": "Saying I've gotta be, I know B is a Markov state.",
                    "label": 0
                },
                {
                    "sent": "And whereas Monte Carlo just says I don't care about what happened between, I ended up with a.",
                    "label": 0
                },
                {
                    "sent": "Getting a 0.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize that.",
                    "label": 0
                },
                {
                    "sent": "The prediction that best matches the training data is the Monte Carlo estimate.",
                    "label": 1
                },
                {
                    "sent": "Best matches the training data.",
                    "label": 0
                },
                {
                    "sent": "Remember the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you saw you saw a once and it ended up with a 0 so you want to match the training data, the right prediction is the value of a 0.",
                    "label": 0
                },
                {
                    "sent": "That is the prediction that will best match the data.",
                    "label": 0
                },
                {
                    "sent": "OK, now of course I want to say we don't want to match the data.",
                    "label": 1
                },
                {
                    "sent": "If we don't want to, we don't want to minimize the mean square error on the training set.",
                    "label": 0
                },
                {
                    "sent": "Weird, huh?",
                    "label": 0
                },
                {
                    "sent": "And it seems like we should want to minimize the mean square on the training set.",
                    "label": 0
                },
                {
                    "sent": "And that's why I've gone through at some length this example with you guys.",
                    "label": 0
                },
                {
                    "sent": "So I want to have some intuition of why we don't want to minimize the mean square error on the training set.",
                    "label": 0
                },
                {
                    "sent": "So what can I offer you if I can't offer you minimizing mean square error on the training set, it's going to be minimizing the mean square error on future experience.",
                    "label": 0
                },
                {
                    "sent": "'cause we don't really care about the training set.",
                    "label": 0
                },
                {
                    "sent": "Past experience.",
                    "label": 0
                },
                {
                    "sent": "We care about the future and so we think if we believe we have some real states here, we would think that the estimate value of a is 3/4 will actually be a better match to future data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If we get a new experience, will say it's probably going to be end in a one 3/4 chance of being in a one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this interesting.",
                    "label": 1
                },
                {
                    "sent": "Now we have to really distinguish between minimizing error on the training set, minimizing error on the future.",
                    "label": 0
                },
                {
                    "sent": "This is different things and TD can be faster because it can take advantage of the state property.",
                    "label": 0
                },
                {
                    "sent": "And match future experience better.",
                    "label": 0
                },
                {
                    "sent": "Now, even as I said that you may be able to get immediately a sense of possible limitation of TD methods.",
                    "label": 0
                },
                {
                    "sent": "As I said, they're going to take advantage of the state property that I know when I get to be, it doesn't matter how I got to be.",
                    "label": 0
                },
                {
                    "sent": "But in real life, you don't normally have complete state knowledge, have incomplete state knowledge.",
                    "label": 0
                },
                {
                    "sent": "If anytime using function approximation here we're just using discrete states at anytime using function approximation, you're going to have imperfect knowledge, imperfect state information, and so.",
                    "label": 0
                },
                {
                    "sent": "So in the end, it's going to be a mix.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a question which is going to win in practice.",
                    "label": 0
                },
                {
                    "sent": "But in the end, it's going to be TD that wins.",
                    "label": 0
                },
                {
                    "sent": "In practice, I'm thinking OK in the end end.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Good good time for questions.",
                    "label": 0
                },
                {
                    "sent": "When you introduce.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "U's B2 particular return, yes.",
                    "label": 0
                },
                {
                    "sent": "No, V is still predicting the sum of the other awards from that from the state to the end.",
                    "label": 0
                },
                {
                    "sent": "And so you remember, the example was a is followed by zeros, followed by bees, followed by another zero followed by termination.",
                    "label": 0
                },
                {
                    "sent": "So right we're still trying to predict cumulative reward until the end.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you for clarifying that question.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you for for for this.",
                    "label": 0
                },
                {
                    "sent": "A possible disadvantage that you are sold Philip the micro model.",
                    "label": 0
                },
                {
                    "sent": "Go back and I'm coming back to the initial comparison of essential idea that to be scalable to do really have this computer to be nonlocal.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I was thinking that I mean if you have an explicit representation of the state, yes.",
                    "label": 0
                },
                {
                    "sent": "The memory overhead of MC is linear because you have the space there.",
                    "label": 0
                },
                {
                    "sent": "You can put just a number of things I don't remember which who they are pretty slim an_Say the competition is not localized, that's OK, but then you have any pieces representation you are pointing when you set up a fortune for having a function approximation then of course in this case PD is this not working horribly subliminal.",
                    "label": 0
                },
                {
                    "sent": "Yes, depending on the size of the representation of the function, but then then they updating this function.",
                    "label": 0
                },
                {
                    "sent": "This company presentation can be expensive, so these are there is something there will be space and the locality there is like.",
                    "label": 0
                },
                {
                    "sent": "It's hard to have both of them so so let's think that through.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that instead of having a table like this, all table look up.",
                    "label": 0
                },
                {
                    "sent": "But instead of that we have a complicated neural network.",
                    "label": 1
                },
                {
                    "sent": "Right and so then when we get a new error, we have to back propagate through the network.",
                    "label": 0
                },
                {
                    "sent": "We have to do somewhat expensive computation, but it's not.",
                    "label": 0
                },
                {
                    "sent": "Should we consider that expensive?",
                    "label": 0
                },
                {
                    "sent": "I'm going to say no, because even the back propagation of 1 error backpropagation through the network, that complexity is the same.",
                    "label": 0
                },
                {
                    "sent": "It's the same order as a forward pass to the network, so we already spent before we had to make a forward pass in order to get the prediction.",
                    "label": 0
                },
                {
                    "sent": "And so there's an equivalent complexity to do the update.",
                    "label": 0
                },
                {
                    "sent": "So even though it's a bunch of weights, it's it's we should consider that cheap.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "It's linear in the size of the network.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Good, any other questions?",
                    "label": 0
                },
                {
                    "sent": "Good, OK so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've just done one step methods, tabular methods, model free methods.",
                    "label": 0
                },
                {
                    "sent": "All these qualifiers can be generalized, but even here in this simplest case, one step method meeting we're looking from one step to the next step rather than one step to five steps ahead, like in AC 3.",
                    "label": 0
                },
                {
                    "sent": "But there we can see the basic ideas and it's tabular tablet is easy to think about, but it all all the ideas really do generalize to the to the network case.",
                    "label": 0
                },
                {
                    "sent": "Complicated function approximator.",
                    "label": 0
                },
                {
                    "sent": "We've seen the basic things is that we're going to bootstrap and sample combine aspect of dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "Much Carol Carlo.",
                    "label": 1
                },
                {
                    "sent": "These methods are computationally congenial, just a little bit of work on each step you have to wait until the end, and then do a whole bunch of work.",
                    "label": 0
                },
                {
                    "sent": "And if the world is truly Markov, then TV methods are faster.",
                    "label": 1
                },
                {
                    "sent": "That's what we see.",
                    "label": 0
                },
                {
                    "sent": "And it has to do with the past data versus the future data.",
                    "label": 0
                },
                {
                    "sent": "Now before I go into a summit, new thing I like to also try to summarize where we are in terms of pictorially OK, what today we've talked about?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contrast ING TD method One Step TD method, which is like this is what I.",
                    "label": 0
                },
                {
                    "sent": "This is a little picture is what I used to summarize the method that mean the thing at the top.",
                    "label": 0
                },
                {
                    "sent": "Something updating it and this says I go ahead 1 action and one next state and I use my estimate here to improve this guy.",
                    "label": 0
                },
                {
                    "sent": "So this is like a picture of the algorithm and the same kind of picture for Monte Carlo is you want to estimate, improve the estimate of this guys value, you go ahead one state, an action state action states that overall the end and you see the final outcome and then you back all that up.",
                    "label": 0
                },
                {
                    "sent": "OK so and this is like a dimension you can occupy intermediate methods.",
                    "label": 0
                },
                {
                    "sent": "You can do two step methods, three step methods, four step methods.",
                    "label": 0
                },
                {
                    "sent": "Five step is like an infinite step method where you go all the way to the end of the episode and then.",
                    "label": 0
                },
                {
                    "sent": "And then there is the parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "You might have heard about in TD Lambda the eligibility trace parameter is really a bootstrapping pattern that determines it's not the number of steps, but it's.",
                    "label": 0
                },
                {
                    "sent": "It's analogous to the number of steps.",
                    "label": 0
                },
                {
                    "sent": "And so this is really a dimension and we can occupy any point along this dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's now there's a second dimension, which is are we going to use planning OK?",
                    "label": 0
                },
                {
                    "sent": "Are we going to use knowledge of the model of the world?",
                    "label": 0
                },
                {
                    "sent": "OK, dynamic programming?",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming is this corner.",
                    "label": 1
                },
                {
                    "sent": "It means we're still going to one step method dynamically add one step and use your estimates at that one step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead into the future.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's moving along the top here.",
                    "label": 0
                },
                {
                    "sent": "He says keep keep these short backups one step backups, but instead of being a sample, do all the possibilities.",
                    "label": 0
                },
                {
                    "sent": "And that's dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "And then there's a fourth corner where analogous to Monte Carlo, but with planning is like exhaustive search.",
                    "label": 1
                },
                {
                    "sent": "We consider all the possibilities all the way to the end.",
                    "label": 0
                },
                {
                    "sent": "And so we can get these four corners is classic methods.",
                    "label": 0
                },
                {
                    "sent": "And then we can occupy the area in between them and.",
                    "label": 0
                },
                {
                    "sent": "That's kind of a big space of reinforcement learning methods, although it's certainly not the whole space.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "Where should we go next?",
                    "label": 0
                },
                {
                    "sent": "I don't have time to present all that I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "But let me just sort of.",
                    "label": 0
                },
                {
                    "sent": "We've done a good group here.",
                    "label": 0
                },
                {
                    "sent": "I can almost sort of wrap up and talk about the future from here, but let me just tell you some of the things that we had more time we might talk about.",
                    "label": 0
                },
                {
                    "sent": "OK, first we would talk about estimating instead of state values.",
                    "label": 0
                },
                {
                    "sent": "We talked about estimating action values, 'cause you know, really, for control you want to action values and it's not.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That different you would just estimate coopi instead of the pie, and it's going to be lower case because that's this is the true value function coopi and you would estimate it with an action value estimate since it's just tabular, I can say big Q of essay for the actual state match encountered in this update.",
                    "label": 0
                },
                {
                    "sent": "This update is essentially the TD update.",
                    "label": 0
                },
                {
                    "sent": "It's just state action pairs rather than I'm states, right?",
                    "label": 0
                },
                {
                    "sent": "This is still a TD error.",
                    "label": 0
                },
                {
                    "sent": "This is my original estimate.",
                    "label": 0
                },
                {
                    "sent": "This is the estimate for the next state.",
                    "label": 0
                },
                {
                    "sent": "This is the Sarsa algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's that's quite straightforward, and so that's our salgo rhythm ends up being that rule that update done over and over again.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And some examples keuler.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ling Q Learning is almost the same.",
                    "label": 0
                },
                {
                    "sent": "Rule is you were doing updating state state action value.",
                    "label": 0
                },
                {
                    "sent": "The new one is the old 1 + a step size times of TD error with the TD error is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "The TD error.",
                    "label": 0
                },
                {
                    "sent": "We were comparing our original estimate to next reward plus something of the next state.",
                    "label": 0
                },
                {
                    "sent": "But it's the Max of our possible actions at the next date.",
                    "label": 0
                },
                {
                    "sent": "That's Q learning.",
                    "label": 0
                },
                {
                    "sent": "I like to draw this picture.",
                    "label": 0
                },
                {
                    "sent": "This is it's picture.",
                    "label": 0
                },
                {
                    "sent": "The picture says you know I'm updating a state action.",
                    "label": 0
                },
                {
                    "sent": "Put all the possible estimated action values.",
                    "label": 0
                },
                {
                    "sent": "Take the maximum.",
                    "label": 0
                },
                {
                    "sent": "That's this part of the rule, right Max or all possible a.",
                    "label": 0
                },
                {
                    "sent": "And then I back up the Max to improve the estimate of this guy at the top.",
                    "label": 0
                },
                {
                    "sent": "That's Q learning.",
                    "label": 0
                },
                {
                    "sent": "It's a TD algorithm with that particular target.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a nice example.",
                    "label": 0
                },
                {
                    "sent": "Cliff Walk is a nice example comparing source in Q learning sources and on policy method.",
                    "label": 0
                },
                {
                    "sent": "Q Learning is an off policy method and.",
                    "label": 0
                },
                {
                    "sent": "We see that actually here the Y axis or the wax is reward for episode.",
                    "label": 0
                },
                {
                    "sent": "This will actually workout better.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to explain this example.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't want to do something it.",
                    "label": 0
                },
                {
                    "sent": "You guys didn't really understand.",
                    "label": 0
                },
                {
                    "sent": "Better to skip over it.",
                    "label": 0
                },
                {
                    "sent": "OK, here's one sort of new algorithm, expected Sarsa.",
                    "label": 0
                },
                {
                    "sent": "Expected size, so if you look at the picture right in Q, learning is taking all these possible things you might do and you take the best of them.",
                    "label": 0
                },
                {
                    "sent": "Take the Max from the arc means Max.",
                    "label": 0
                },
                {
                    "sent": "And if you have, if you don't have an arc, then it means expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, so expected so you don't take the best of the things you might do.",
                    "label": 0
                },
                {
                    "sent": "You take the the expectation based on how you would actually do them according to your policy.",
                    "label": 0
                },
                {
                    "sent": "So here we are, we're summing over all the things we might do.",
                    "label": 0
                },
                {
                    "sent": "How likely are we to do it under our policy, which we know?",
                    "label": 0
                },
                {
                    "sent": "We know our policy.",
                    "label": 0
                },
                {
                    "sent": "We know how likely we are like.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's epsilon greedy.",
                    "label": 0
                },
                {
                    "sent": "And we take the expectation.",
                    "label": 0
                },
                {
                    "sent": "The action value times our likelihood of doing that action and we back up that OK, and that's that's arguably listen, improve.",
                    "label": 0
                },
                {
                    "sent": "Our version of Sarsa.",
                    "label": 0
                },
                {
                    "sent": "And it can also be made an off policy version of Sarsa.",
                    "label": 0
                },
                {
                    "sent": "And there's some other.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Novel Tees you can.",
                    "label": 0
                },
                {
                    "sent": "You can do an off policy version of expected services.",
                    "label": 0
                },
                {
                    "sent": "I've used the word off of.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See a couple of times that explaining it.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry about that, but off policy means that you're learning about a policy that's different than the one you're following.",
                    "label": 0
                },
                {
                    "sent": "OK, and on policy means you're learning about the same policy is that when you're following the same one that's generating the data.",
                    "label": 0
                },
                {
                    "sent": "So the way to remember is that on policy is almost one policy, and in on policy methods there is only one policy.",
                    "label": 0
                },
                {
                    "sent": "Some policy you're doing its policy learning about, but very often these want to be different, like you want to do something that is more exploratory and you might want to learn the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're going to learn the optimal policy, but you're going to actually get your data and exploratory way, we're just not going to be optimal.",
                    "label": 0
                },
                {
                    "sent": "Then you have two policies.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you're in the realm of off policy learning.",
                    "label": 0
                },
                {
                    "sent": "Q Learning does this, but off policy learning is is theoretically more difficult and more challenging for our methods.",
                    "label": 0
                },
                {
                    "sent": "OK. That's that's all.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Policy OK so.",
                    "label": 0
                },
                {
                    "sent": "So I basically just extended these things to control OK Now.",
                    "label": 0
                },
                {
                    "sent": "We've seen some.",
                    "label": 0
                },
                {
                    "sent": "Some methods that can do the on policy case in the off policy case.",
                    "label": 0
                },
                {
                    "sent": "We didn't talk about double Q learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've talked a lot about about do we want to use Monte Carlo or do we want to use TD OK and it says there's a sense in which we don't have to choose 'cause if you use an algorithm like if you use TD, you can parametrically very Lambda or vary the height of your backups to get to give you any intermediate point between one step TD in Monte Carlo you can get both and a kenite way of doing this is with the parameter.",
                    "label": 0
                },
                {
                    "sent": "Lambda the bootstrapping parameter which isn't really talked about, but it is a way to very parametrically between TD and Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "So if land equals 0, which is the left side of all these graphs, that's pure TD pure bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "OK, if land is one, that means you're not doing any bootstrapping, it's Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so now all these graphs have Lambda can cross the bottom, so it's basically like this to pure to know bootstrapping and they all have a measure performance on the top where in all cases lower is better.",
                    "label": 1
                },
                {
                    "sent": "OK so it's like it's like Mountain Car an you want to have a few steps to get to the top of the Hill.",
                    "label": 0
                },
                {
                    "sent": "OK So what you see looking at this is that you know performance depends on Lambda this is this is.",
                    "label": 0
                },
                {
                    "sent": "Random walk and it's actually not best at land equals zero.",
                    "label": 0
                },
                {
                    "sent": "Pure TD is not the best you can do better if you do some amount of TD intermediate between Pure TD and Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "But if you go all the way to land equals equals one, then things get really bad.",
                    "label": 0
                },
                {
                    "sent": "That's like the worst case in general, and that's the pattern.",
                    "label": 1
                },
                {
                    "sent": "Land equals zero is Monte Carlo, and Monte Carlo has really high variance and it has to be ruled out.",
                    "label": 0
                },
                {
                    "sent": "It's not very happy if you are committed to Monte Carlo now you can do TDD and say, oh, I can pick any step in between.",
                    "label": 0
                },
                {
                    "sent": "That's what you want to have this facility of doing some bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "And that's sort of some evidence for that, even though this is old data, I think you know, like Peter would agree that Monte Carlo is is not really an efficient strategy to do it in a pure way.",
                    "label": 0
                },
                {
                    "sent": "OK, now another I want to give it 1 slide also for taking questions on the linear case case with the real function approximation.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go to nonlinear networks, but I want to go to something which five talks someone theoretically about which is the linear case so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Close reading linear function approximation, linear function means that our estimate estimated value is formed as an inner product between a parameter.",
                    "label": 1
                },
                {
                    "sent": "A weight vector and a feature vector.",
                    "label": 1
                },
                {
                    "sent": "OK, so feature vectors are.",
                    "label": 1
                },
                {
                    "sent": "Fee fee for feature fee of T is our feature vector from the state at time T and the parameter vectors Theta, so that might think about the weights of your network.",
                    "label": 0
                },
                {
                    "sent": "This is a linear network.",
                    "label": 0
                },
                {
                    "sent": "OK, so do we take the inner product and so this transpose thing means the inner product so Theta inner product with fee?",
                    "label": 1
                },
                {
                    "sent": "Is our estimated value of state T use?",
                    "label": 0
                },
                {
                    "sent": "It's just made value of the state at time T because this is the feature vector for the state at time T. OK, so this is our estimate.",
                    "label": 1
                },
                {
                    "sent": "This is estimated value of time T. This is our estimated value at time T + 1.",
                    "label": 0
                },
                {
                    "sent": "So this really is a TD error.",
                    "label": 1
                },
                {
                    "sent": "And this is a TD rule.",
                    "label": 0
                },
                {
                    "sent": "The TD rule is that the parameters are the old parameters plus step size times RTD error and this need the gradient and the general in general, nonlinear cases would be a gradient of the.",
                    "label": 0
                },
                {
                    "sent": "Of the prediction with respect to the parameters in the linear case is just the feature vector Phi Phi.",
                    "label": 1
                },
                {
                    "sent": "Excuse me.",
                    "label": 0
                },
                {
                    "sent": "OK, so that rule should be fairly familiar to you now.",
                    "label": 1
                },
                {
                    "sent": "It's just a TV rule using a stochastic.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Um, it's.",
                    "label": 1
                },
                {
                    "sent": "Lots could be said about that, but that's that's the standard team linear TV T0.",
                    "label": 1
                },
                {
                    "sent": "And if you look at this you can of course write it like this.",
                    "label": 0
                },
                {
                    "sent": "You can take the file the fee and carried inside here it's there and you carried inside here with little some transpose the stuff you can write it like that and this is a vector if you think the expectation we're going to take the expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, so in expectation the new feature vector is the old 1 + a step size and this thing this thing in expectation.",
                    "label": 1
                },
                {
                    "sent": "What is it?",
                    "label": 1
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Well I'm just going to make some names for it.",
                    "label": 0
                },
                {
                    "sent": "This thing is a vector and this thing is a matrix times Theta.",
                    "label": 0
                },
                {
                    "sent": "OK so B is going to call that vector so that vector B is just the expected value of this thing.",
                    "label": 0
                },
                {
                    "sent": "It is a well defined vector.",
                    "label": 0
                },
                {
                    "sent": "You don't know it, but it's there and this thing is a matrix 'cause it's an outer product of feature vector with the change in the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So the expectation of that matrix is what I'm going to call a.",
                    "label": 0
                },
                {
                    "sent": "So let me let me write the whole expectation like this, and I'm interested in the and what happens at the fixed point?",
                    "label": 0
                },
                {
                    "sent": "Where will this settle?",
                    "label": 1
                },
                {
                    "sent": "An expectation will converge.",
                    "label": 1
                },
                {
                    "sent": "Where will the expected update be zero?",
                    "label": 0
                },
                {
                    "sent": "Well, the expected update.",
                    "label": 0
                },
                {
                    "sent": "Is basically this part, so I want to win.",
                    "label": 0
                },
                {
                    "sent": "Is that zero?",
                    "label": 1
                },
                {
                    "sent": "Well, that's going to zero in B = a Theta.",
                    "label": 0
                },
                {
                    "sent": "Or when B -- 8 Theta is zero OK and that that Theta?",
                    "label": 0
                },
                {
                    "sent": "That's a special Theta for which this is true for B -- a Theta equals zero.",
                    "label": 1
                },
                {
                    "sent": "I'm going to call it Theta TD 'cause it's the fixed point that TD converges to linear TV converges to OK, and then you can just compute it will be is.",
                    "label": 0
                },
                {
                    "sent": "Is a type Theta and so you have to take the inverse of A and you get.",
                    "label": 0
                },
                {
                    "sent": "The TV fixed point is the A inverse B and which by the way is a key to another algorithm.",
                    "label": 0
                },
                {
                    "sent": "Least squares algorithm says estimate a directly even though it's a matrix can take its inverse.",
                    "label": 1
                },
                {
                    "sent": "An also estimate be directly and then multiply them together to get least squares.",
                    "label": 1
                },
                {
                    "sent": "TV works OK.",
                    "label": 0
                },
                {
                    "sent": "But this way of computing what what?",
                    "label": 0
                },
                {
                    "sent": "What the algorithm converges to, and then you can say something theoretical about it.",
                    "label": 0
                },
                {
                    "sent": "This is your guarantee that we get that the mean square value error measure how the values are is bounded by an expansion times the mean square value of the best data.",
                    "label": 0
                },
                {
                    "sent": "So this means that we don't find the best Theta.",
                    "label": 0
                },
                {
                    "sent": "OK, but we do get an expansion of it anyway, so that's what the theory would look like if we had more time to talk about it.",
                    "label": 0
                },
                {
                    "sent": "Unless you want to.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mention quickly some parts of the frontiers, some things that people are working on now, so off policy prediction is a big area people are working on trying to generalize to the off policy case.",
                    "label": 0
                },
                {
                    "sent": "Also, we'd like to talk about non theory for the case of non linear function approximation.",
                    "label": 1
                },
                {
                    "sent": "There's just a little bit of that.",
                    "label": 0
                },
                {
                    "sent": "There's also very little convergence theory for control methods.",
                    "label": 1
                },
                {
                    "sent": "And I think chavaux.",
                    "label": 1
                },
                {
                    "sent": "Maybe we'll talk about that tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And we also like to say things beyond convergence.",
                    "label": 0
                },
                {
                    "sent": "Would like to know how well can you do in a finite amount of time, how fast you converge.",
                    "label": 0
                },
                {
                    "sent": "Now when you combine TV with with deep learning, a lot of different issues come up and I think there's just a lot of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "Do we really need a replay buffer so that one of the folk theorems it always?",
                    "label": 0
                },
                {
                    "sent": "Especially you have instability in correlation quote correlation.",
                    "label": 0
                },
                {
                    "sent": "So we need this thing called the replay buffer, but I think it's really.",
                    "label": 0
                },
                {
                    "sent": "There's lots of questions about what happens when we combine TD with deep learning and finally, the idea of predicting things other than reward.",
                    "label": 1
                },
                {
                    "sent": "Remember, I started with that.",
                    "label": 0
                },
                {
                    "sent": "We might want this is tedious.",
                    "label": 0
                },
                {
                    "sent": "A general prediction method.",
                    "label": 0
                },
                {
                    "sent": "Multi step prediction methods.",
                    "label": 0
                },
                {
                    "sent": "We want to use it to predict other things and in particular we want to learn it to use learn a model of the world.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion I guess.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to say is something like this that E learning is a uniquely important kind of learning.",
                    "label": 1
                },
                {
                    "sent": "Anyway, maybe it's ubiquitous.",
                    "label": 0
                },
                {
                    "sent": "We're always going to be using it, and I think this may be true.",
                    "label": 0
                },
                {
                    "sent": "It's a hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So anyway, it's learning to predict, which is perhaps the only scalable kind of learning.",
                    "label": 1
                },
                {
                    "sent": "It's A kind of running this specialized for general multi step prediction, which may be the key to perception modeling the world.",
                    "label": 1
                },
                {
                    "sent": "The meaning of our knowledge, it's key ideas to take advantage of the state property.",
                    "label": 0
                },
                {
                    "sent": "Which can make it fast and efficient, but can also make it asymptotically biased and its other key claim to fame is that is computationally cheap congenial, and we're only beginning to use to explore different ways we use it for things other than reward.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}