{
    "id": "xj3du73qtcqg5gmjyygnvjyx7x6fd2o3",
    "title": "Nonparametric Factor Analysis with Beta Process Priors",
    "info": {
        "author": [
            "John Paisley, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Sept. 17, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_paisley_nfa/",
    "segmentation": [
        [
            "I'll be presenting the paper nonparametric factor analysis with beta process priors.",
            "This is done with my advisor, Doctor Lawrence Karen at Duke University, in the Department of ECE."
        ],
        [
            "So here's an outline of my talk.",
            "I'll first give an introduction followed by a review of the beta process.",
            "I'll then discuss the model presented in this paper.",
            "The beta process factor analysis model followed by a brief discussion of variational inference for the model.",
            "Some of the experiments that we ran in the paper and then a conclusion.",
            "So nonparametric."
        ],
        [
            "Bayesian priors are useful for finding compact statistical representations of a data set without limiting the potential complexity of the model, so a commonly used example in the machine learning community is the deerslayer process for mixture modeling, where we allow the number of mixture components to theoretically grow to an infant number while the data set selects kind of the right amount that somehow right for it.",
            "So factor analysis modeling is a framework where nonparametric priors can also be useful.",
            "FA models are used to separate the important covariance structure of a data set from idiosyncratic noise.",
            "So here we have a data set X of N observations in AD dimensional space.",
            "And we model it as a matrix product that contains the structure of the set plus the idiosyncratic noise where this matrix Phi is of the factor loadings can be thought of as a Dictionary of vectors of K vectors that that each observation can select from via Z factor scores, which says which vectors and how much of each vector is used to compose X.",
            "So this value K is something that in a lot of models has to be set.",
            "But just like with mixture modeling, it suggests the use of a nonparametric prior.",
            "And so for in this paper we look at the beta process or a two parameter extension of the beta process as a nonparametric prior that allows K to go to Infinity.",
            "In this in this model, while still some selecting only a small subset of the columns of five in compose in comprising X, we denote this process H drawn from BP ABH not where A&B are two positive scalars.",
            "And H not is a prior measure.",
            "In this case, H not would be a multivariate Gaussian distribution.",
            "So I'll explain the beta process just through some pictures and through one example so.",
            "Define H not to be a uniform distribution on."
        ],
        [
            "Zero to 1, so here's a uniform density.",
            "And then we partition this up into K partitions of equal measure 1 / K, where we're interested in the limit as K goes to Infinity.",
            "So what the beta process does is it takes each partition?",
            "It takes each partition and draws a new measure on that partition.",
            "\u03a0 sabca parameterized by beta distribution a / 8 times the measure on that partition B * 1 minus the measure on that partition, which in this case is a / K and B * K -- 1 / K. And so you can see that as cago it gets really large.",
            "The probability that pie some K any price of K is going to be substantial is going to be very small.",
            "OK, I'm sorry you can't see that.",
            "OK, I'm sorry.",
            "So the probability that any.",
            "Any price of K is going to be very small because this is going to this is going to 0 but at the same time the number of Pi sabca's which are drawing values for is going to Infinity.",
            "So here's just a."
        ],
        [
            "For example, where we set a equal to 5B equal to two and K equal to 1000.",
            "And then Drew the 1000 pies of K values accordingly and you can see that in the infinite case, visually, when look really any different from this you can see that essentially most locations, the new measure that you draw is zero except for a few.",
            "A few outlier locations.",
            "So what do we do with this?",
            "With the by way of example for this, where the deer sleep process, this would just be replaced with the gamma distribution, and then you would normalize the result.",
            "You would then draw an indicator for each observation, which would.",
            "So for instance, this one would occur frequently where you would then get this point, which would then parameterise a distribution from which you would then draw the observation.",
            "But for the beta process, what you do is you then use this H to parameterise a Bernoulli process."
        ],
        [
            "Where you go point by point and you draw a binary indicator 01 with the probability of a one being the pie sub K value at that location.",
            "So you can see that drawing number from the burning process parameterized by H is going to give you mostly most of locations will hardly ever occur, and then you will get a lot of repeats and overlap for a lot of these locations with substantial probability mass.",
            "So here's four hypothetical draws.",
            "Where you see the binary vector and you see that they occur at the at the locations of substantial mass.",
            "They a lot of them overlap, but also a lot of them occur at new locations."
        ],
        [
            "So before I discuss how we actually use this, I'll talk.",
            "I'll first talk about just the properties of of A&B and how they affect those binary vectors.",
            "So by integrating out each piece of K, we obtain what's called the Indian buffet process.",
            "Um and letting K go to Infinity.",
            "We obtain the Indian buffet process and the relation of the IVP to the beta process is basically the same as the relationship of the Chinese restaurant process to the dearsley process.",
            "And in doing so we can say two things.",
            "We can say that the number of locations at any the number of non zero locations in any binary vector is distributed as Poss on a / B.",
            "And we can also say that for any set of N binary vectors Z1 through ZN, if we call CCV, the total number of unique locations where there's a one, then that has a distribution POS unaware of this summation.",
            "Where this term is getting smaller and smaller as N gets larger, so new factors are introduced at a slower rate as the number of samples increases.",
            "But of interest is is that this diverges to Infinity as N goes to Infinity, so the entire space is eventually sampled?",
            "So here's how."
        ],
        [
            "We use this in beta process factor analysis.",
            "For the factor analysis model, we model the generation of the data set X using a finite approximation to the beta process.",
            "This approximation allows for variational inference to be performed.",
            "So below here is a noiseless unweighted example of what a draw would look like, and we show how the sparseness of the BP prior.",
            "How it works by what we did was in this previous slide we use the same pie values to draw the binary indicators and then instead of being an U01 we basically we detach the measure from the location and then we define just drawing from some D dimensional multivariate Gaussian distribution.",
            "So this actually extends to 1000 dictionary elements and this to 1000 as well.",
            "But you can see that because most of those values were zero, the number of factors introduced in these 100 samples, there were only 23 that were ended up being used, and most of them were the same."
        ],
        [
            "So here's the generative process of the model.",
            "Again, we truncate K capital K to just a large number, and we draw the locations from the prior measure and then we draw the corresponding corresponding measures from the beta distribution, parameterized as discussed before.",
            "So this is an approximation to the beta process and this approximation.",
            "Is essentially it's a similar to modeling of dearsley process mixture model by just simply using a a big finite dearsley Alpha over K where you set K to be just a really big number and you find when you do that that gives you just as good results and it's the model simplifies to the subset of the mixture components.",
            "Um?",
            "So then this then that constitutes the approximation.",
            "Then, for any observation XI, we draw the binary indicator vector from the Bernoulli distributions parameterized by the by \u03c0 subcase.",
            "We allow for an additional, uh, wait to add more flexibility, and then also the noise.",
            "So the meaning the covariance under this truncation is given at the right, and we can see with the covariance that the model remains well defined, defined as K goes to Infinity.",
            "In this paper we we derived and provide."
        ],
        [
            "The variational update equations, which again was possible because of the full conjugacy within the hierarchy because of the approximation that we make.",
            "I won't go into detail now, but they're all going to be on my poster tonight so anybody can stop by and discuss then.",
            "So we."
        ],
        [
            "We looked at three weeks."
        ],
        [
            "Moments one on toy data, one on the emnace digits data set and one on the HDD PC pH cell line panel.",
            "So for the toy data set we generated.",
            "250 samples in a 25 dimensional space, sampling H with A&B equal to 1.",
            "We fixed W equal to 1 and we in the model we truncated K equal to 1000.",
            "So here is the ground truth where we in both of these we reorder the indices just for presentation.",
            "So here's the ground truth, and here is the uncovered model where the essential structure was uncovered.",
            "But what we found why some of these are missing and why it went from 100 to 11 instead of 102.",
            "Seven was that the model VB inference because it's a local optimal solution.",
            "It converges before some factors can kind of be merged.",
            "So if you look at what's missing here, it's accounted for.",
            "Down here in the corresponding factor, loading vectors are essentially the same.",
            "And so the reconstruction of the original data set using this output was essentially perfect except for the noise."
        ],
        [
            "On the emnace digits data set, we looked at 2500 odd digits.",
            "We in the model we set A&B equal to 1 and K equal to 100.",
            "Here is an ordering of the probabilities that I just apply values as discussed before and you can see that it finds some sparse subset to use.",
            "Here is just as just to try to assess the what it gave.",
            "Here's here's a factor sharing intermap where it what I show is.",
            "I showed the expected number of factors shared between 2.",
            "Any two observations within classes and across classes, and so you can see that digits within the same class heavily used the same factors.",
            "Three and five share a lot of factors.",
            "Seven and nine share a lot of factors, which makes sense because of the way they look.",
            "Here also, just as a point of interest, I showed the most used factor for each of the five digits where you see that three and five use the same factor very heavily, but then the second most used, given this one is one that would differentiate the two.",
            "Also mentioned that inference was fast because of the deterministic nature of the variational methods, so each iteration required basically the same computational resources that you would require for a Gibbs iteration, but requires.",
            "Maybe in this case, I think 35 iterations are supposed to thousands of iterations.",
            "Did you get that?"
        ],
        [
            "I'm in Miami.",
            "So then, here's the final example experiment that we looked at.",
            "The HDD PC PHL line panel which is a data set of roughly 1000 observations in 33177 dimensional space of Geno types, measured from different population groups around the world.",
            "And in this set we were interested in seeing more as opposed to the endless digits where I showed kind of what the factors look like.",
            "I'm showing what the reconstruction looks like and we can see that this model can be potentially useful for denoising datasets.",
            "So here is the original data set projected along the 1st 20 principle components.",
            "And then here is the reconstructed data set projected along those same bases basis vectors as as this and so we can see that basically the essential structure.",
            "Of the data set was preserved, but a lot of the additional noise was reduced.",
            "And then that's also shown here for the first 150 PCA dimensions of the original data set.",
            "Where we see the variance along those along those directions of the original data set.",
            "And then the variance along the reconstructed data set.",
            "Um?",
            "Which is significantly less.",
            "OK, early.",
            "Are we really slow?"
        ],
        [
            "So in conclusion.",
            "We've presented a nonparametric model for performing factor analysis that uses an approximation to the beta process prior that's.",
            "That's essentially the same approximation.",
            "I mean it's the same same approximation that we would make when we would just use a finite directly prior for mixture modeling, and this approximation allowed for variational inference to be performed because of the analytical posterior updates and the full conjugacy within the model.",
            "We're currently in the process of expanding these ideas in a full length paper where we'll look more in depth at applications in different uses.",
            "And also I mean I just mentioned that we've recently derived the stick breaking construction of the beta process, or believe we have for the more general case where A&B are not fixed, which is I think different from the Indian buffet process where B is equal to 1, which for which is stick breaking construction exists but future work is needed to rigorously prove convergence properties which is out of my League and so any help would be welcome on that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be presenting the paper nonparametric factor analysis with beta process priors.",
                    "label": 0
                },
                {
                    "sent": "This is done with my advisor, Doctor Lawrence Karen at Duke University, in the Department of ECE.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll first give an introduction followed by a review of the beta process.",
                    "label": 0
                },
                {
                    "sent": "I'll then discuss the model presented in this paper.",
                    "label": 0
                },
                {
                    "sent": "The beta process factor analysis model followed by a brief discussion of variational inference for the model.",
                    "label": 1
                },
                {
                    "sent": "Some of the experiments that we ran in the paper and then a conclusion.",
                    "label": 0
                },
                {
                    "sent": "So nonparametric.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bayesian priors are useful for finding compact statistical representations of a data set without limiting the potential complexity of the model, so a commonly used example in the machine learning community is the deerslayer process for mixture modeling, where we allow the number of mixture components to theoretically grow to an infant number while the data set selects kind of the right amount that somehow right for it.",
                    "label": 1
                },
                {
                    "sent": "So factor analysis modeling is a framework where nonparametric priors can also be useful.",
                    "label": 1
                },
                {
                    "sent": "FA models are used to separate the important covariance structure of a data set from idiosyncratic noise.",
                    "label": 1
                },
                {
                    "sent": "So here we have a data set X of N observations in AD dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And we model it as a matrix product that contains the structure of the set plus the idiosyncratic noise where this matrix Phi is of the factor loadings can be thought of as a Dictionary of vectors of K vectors that that each observation can select from via Z factor scores, which says which vectors and how much of each vector is used to compose X.",
                    "label": 0
                },
                {
                    "sent": "So this value K is something that in a lot of models has to be set.",
                    "label": 0
                },
                {
                    "sent": "But just like with mixture modeling, it suggests the use of a nonparametric prior.",
                    "label": 0
                },
                {
                    "sent": "And so for in this paper we look at the beta process or a two parameter extension of the beta process as a nonparametric prior that allows K to go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In this in this model, while still some selecting only a small subset of the columns of five in compose in comprising X, we denote this process H drawn from BP ABH not where A&B are two positive scalars.",
                    "label": 0
                },
                {
                    "sent": "And H not is a prior measure.",
                    "label": 0
                },
                {
                    "sent": "In this case, H not would be a multivariate Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So I'll explain the beta process just through some pictures and through one example so.",
                    "label": 0
                },
                {
                    "sent": "Define H not to be a uniform distribution on.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zero to 1, so here's a uniform density.",
                    "label": 0
                },
                {
                    "sent": "And then we partition this up into K partitions of equal measure 1 / K, where we're interested in the limit as K goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So what the beta process does is it takes each partition?",
                    "label": 1
                },
                {
                    "sent": "It takes each partition and draws a new measure on that partition.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 sabca parameterized by beta distribution a / 8 times the measure on that partition B * 1 minus the measure on that partition, which in this case is a / K and B * K -- 1 / K. And so you can see that as cago it gets really large.",
                    "label": 0
                },
                {
                    "sent": "The probability that pie some K any price of K is going to be substantial is going to be very small.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry you can't see that.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So the probability that any.",
                    "label": 0
                },
                {
                    "sent": "Any price of K is going to be very small because this is going to this is going to 0 but at the same time the number of Pi sabca's which are drawing values for is going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So here's just a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, where we set a equal to 5B equal to two and K equal to 1000.",
                    "label": 0
                },
                {
                    "sent": "And then Drew the 1000 pies of K values accordingly and you can see that in the infinite case, visually, when look really any different from this you can see that essentially most locations, the new measure that you draw is zero except for a few.",
                    "label": 0
                },
                {
                    "sent": "A few outlier locations.",
                    "label": 0
                },
                {
                    "sent": "So what do we do with this?",
                    "label": 0
                },
                {
                    "sent": "With the by way of example for this, where the deer sleep process, this would just be replaced with the gamma distribution, and then you would normalize the result.",
                    "label": 0
                },
                {
                    "sent": "You would then draw an indicator for each observation, which would.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this one would occur frequently where you would then get this point, which would then parameterise a distribution from which you would then draw the observation.",
                    "label": 0
                },
                {
                    "sent": "But for the beta process, what you do is you then use this H to parameterise a Bernoulli process.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you go point by point and you draw a binary indicator 01 with the probability of a one being the pie sub K value at that location.",
                    "label": 0
                },
                {
                    "sent": "So you can see that drawing number from the burning process parameterized by H is going to give you mostly most of locations will hardly ever occur, and then you will get a lot of repeats and overlap for a lot of these locations with substantial probability mass.",
                    "label": 0
                },
                {
                    "sent": "So here's four hypothetical draws.",
                    "label": 0
                },
                {
                    "sent": "Where you see the binary vector and you see that they occur at the at the locations of substantial mass.",
                    "label": 0
                },
                {
                    "sent": "They a lot of them overlap, but also a lot of them occur at new locations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I discuss how we actually use this, I'll talk.",
                    "label": 0
                },
                {
                    "sent": "I'll first talk about just the properties of of A&B and how they affect those binary vectors.",
                    "label": 0
                },
                {
                    "sent": "So by integrating out each piece of K, we obtain what's called the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "Um and letting K go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We obtain the Indian buffet process and the relation of the IVP to the beta process is basically the same as the relationship of the Chinese restaurant process to the dearsley process.",
                    "label": 0
                },
                {
                    "sent": "And in doing so we can say two things.",
                    "label": 0
                },
                {
                    "sent": "We can say that the number of locations at any the number of non zero locations in any binary vector is distributed as Poss on a / B.",
                    "label": 0
                },
                {
                    "sent": "And we can also say that for any set of N binary vectors Z1 through ZN, if we call CCV, the total number of unique locations where there's a one, then that has a distribution POS unaware of this summation.",
                    "label": 1
                },
                {
                    "sent": "Where this term is getting smaller and smaller as N gets larger, so new factors are introduced at a slower rate as the number of samples increases.",
                    "label": 0
                },
                {
                    "sent": "But of interest is is that this diverges to Infinity as N goes to Infinity, so the entire space is eventually sampled?",
                    "label": 0
                },
                {
                    "sent": "So here's how.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use this in beta process factor analysis.",
                    "label": 0
                },
                {
                    "sent": "For the factor analysis model, we model the generation of the data set X using a finite approximation to the beta process.",
                    "label": 1
                },
                {
                    "sent": "This approximation allows for variational inference to be performed.",
                    "label": 0
                },
                {
                    "sent": "So below here is a noiseless unweighted example of what a draw would look like, and we show how the sparseness of the BP prior.",
                    "label": 0
                },
                {
                    "sent": "How it works by what we did was in this previous slide we use the same pie values to draw the binary indicators and then instead of being an U01 we basically we detach the measure from the location and then we define just drawing from some D dimensional multivariate Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So this actually extends to 1000 dictionary elements and this to 1000 as well.",
                    "label": 0
                },
                {
                    "sent": "But you can see that because most of those values were zero, the number of factors introduced in these 100 samples, there were only 23 that were ended up being used, and most of them were the same.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the generative process of the model.",
                    "label": 1
                },
                {
                    "sent": "Again, we truncate K capital K to just a large number, and we draw the locations from the prior measure and then we draw the corresponding corresponding measures from the beta distribution, parameterized as discussed before.",
                    "label": 1
                },
                {
                    "sent": "So this is an approximation to the beta process and this approximation.",
                    "label": 0
                },
                {
                    "sent": "Is essentially it's a similar to modeling of dearsley process mixture model by just simply using a a big finite dearsley Alpha over K where you set K to be just a really big number and you find when you do that that gives you just as good results and it's the model simplifies to the subset of the mixture components.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So then this then that constitutes the approximation.",
                    "label": 0
                },
                {
                    "sent": "Then, for any observation XI, we draw the binary indicator vector from the Bernoulli distributions parameterized by the by \u03c0 subcase.",
                    "label": 0
                },
                {
                    "sent": "We allow for an additional, uh, wait to add more flexibility, and then also the noise.",
                    "label": 0
                },
                {
                    "sent": "So the meaning the covariance under this truncation is given at the right, and we can see with the covariance that the model remains well defined, defined as K goes to Infinity.",
                    "label": 1
                },
                {
                    "sent": "In this paper we we derived and provide.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variational update equations, which again was possible because of the full conjugacy within the hierarchy because of the approximation that we make.",
                    "label": 0
                },
                {
                    "sent": "I won't go into detail now, but they're all going to be on my poster tonight so anybody can stop by and discuss then.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at three weeks.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moments one on toy data, one on the emnace digits data set and one on the HDD PC pH cell line panel.",
                    "label": 1
                },
                {
                    "sent": "So for the toy data set we generated.",
                    "label": 1
                },
                {
                    "sent": "250 samples in a 25 dimensional space, sampling H with A&B equal to 1.",
                    "label": 0
                },
                {
                    "sent": "We fixed W equal to 1 and we in the model we truncated K equal to 1000.",
                    "label": 1
                },
                {
                    "sent": "So here is the ground truth where we in both of these we reorder the indices just for presentation.",
                    "label": 0
                },
                {
                    "sent": "So here's the ground truth, and here is the uncovered model where the essential structure was uncovered.",
                    "label": 0
                },
                {
                    "sent": "But what we found why some of these are missing and why it went from 100 to 11 instead of 102.",
                    "label": 0
                },
                {
                    "sent": "Seven was that the model VB inference because it's a local optimal solution.",
                    "label": 0
                },
                {
                    "sent": "It converges before some factors can kind of be merged.",
                    "label": 0
                },
                {
                    "sent": "So if you look at what's missing here, it's accounted for.",
                    "label": 0
                },
                {
                    "sent": "Down here in the corresponding factor, loading vectors are essentially the same.",
                    "label": 0
                },
                {
                    "sent": "And so the reconstruction of the original data set using this output was essentially perfect except for the noise.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the emnace digits data set, we looked at 2500 odd digits.",
                    "label": 0
                },
                {
                    "sent": "We in the model we set A&B equal to 1 and K equal to 100.",
                    "label": 1
                },
                {
                    "sent": "Here is an ordering of the probabilities that I just apply values as discussed before and you can see that it finds some sparse subset to use.",
                    "label": 0
                },
                {
                    "sent": "Here is just as just to try to assess the what it gave.",
                    "label": 0
                },
                {
                    "sent": "Here's here's a factor sharing intermap where it what I show is.",
                    "label": 0
                },
                {
                    "sent": "I showed the expected number of factors shared between 2.",
                    "label": 0
                },
                {
                    "sent": "Any two observations within classes and across classes, and so you can see that digits within the same class heavily used the same factors.",
                    "label": 0
                },
                {
                    "sent": "Three and five share a lot of factors.",
                    "label": 0
                },
                {
                    "sent": "Seven and nine share a lot of factors, which makes sense because of the way they look.",
                    "label": 0
                },
                {
                    "sent": "Here also, just as a point of interest, I showed the most used factor for each of the five digits where you see that three and five use the same factor very heavily, but then the second most used, given this one is one that would differentiate the two.",
                    "label": 0
                },
                {
                    "sent": "Also mentioned that inference was fast because of the deterministic nature of the variational methods, so each iteration required basically the same computational resources that you would require for a Gibbs iteration, but requires.",
                    "label": 1
                },
                {
                    "sent": "Maybe in this case, I think 35 iterations are supposed to thousands of iterations.",
                    "label": 0
                },
                {
                    "sent": "Did you get that?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm in Miami.",
                    "label": 0
                },
                {
                    "sent": "So then, here's the final example experiment that we looked at.",
                    "label": 0
                },
                {
                    "sent": "The HDD PC PHL line panel which is a data set of roughly 1000 observations in 33177 dimensional space of Geno types, measured from different population groups around the world.",
                    "label": 0
                },
                {
                    "sent": "And in this set we were interested in seeing more as opposed to the endless digits where I showed kind of what the factors look like.",
                    "label": 0
                },
                {
                    "sent": "I'm showing what the reconstruction looks like and we can see that this model can be potentially useful for denoising datasets.",
                    "label": 0
                },
                {
                    "sent": "So here is the original data set projected along the 1st 20 principle components.",
                    "label": 0
                },
                {
                    "sent": "And then here is the reconstructed data set projected along those same bases basis vectors as as this and so we can see that basically the essential structure.",
                    "label": 1
                },
                {
                    "sent": "Of the data set was preserved, but a lot of the additional noise was reduced.",
                    "label": 1
                },
                {
                    "sent": "And then that's also shown here for the first 150 PCA dimensions of the original data set.",
                    "label": 0
                },
                {
                    "sent": "Where we see the variance along those along those directions of the original data set.",
                    "label": 0
                },
                {
                    "sent": "And then the variance along the reconstructed data set.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which is significantly less.",
                    "label": 0
                },
                {
                    "sent": "OK, early.",
                    "label": 0
                },
                {
                    "sent": "Are we really slow?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "We've presented a nonparametric model for performing factor analysis that uses an approximation to the beta process prior that's.",
                    "label": 1
                },
                {
                    "sent": "That's essentially the same approximation.",
                    "label": 0
                },
                {
                    "sent": "I mean it's the same same approximation that we would make when we would just use a finite directly prior for mixture modeling, and this approximation allowed for variational inference to be performed because of the analytical posterior updates and the full conjugacy within the model.",
                    "label": 0
                },
                {
                    "sent": "We're currently in the process of expanding these ideas in a full length paper where we'll look more in depth at applications in different uses.",
                    "label": 1
                },
                {
                    "sent": "And also I mean I just mentioned that we've recently derived the stick breaking construction of the beta process, or believe we have for the more general case where A&B are not fixed, which is I think different from the Indian buffet process where B is equal to 1, which for which is stick breaking construction exists but future work is needed to rigorously prove convergence properties which is out of my League and so any help would be welcome on that.",
                    "label": 0
                }
            ]
        }
    }
}