{
    "id": "al66qoos666by5jlqwml3srapqgaa3xs",
    "title": "Machine learning for the semantic web",
    "info": {
        "author": [
            "Marko Grobelnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Sept. 5, 2011",
        "recorded": "August 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/sssc2011_grobelnik_machinelearning/",
    "segmentation": [
        [
            "Are the topic of today's talk is on how to represent text?",
            "I think there are a couple of presentations.",
            "This summer school on LB.",
            "Text and so on.",
            "So the idea of this talk is somehow to go through.",
            "Different ways how to represent text so different research communities approach attacks from different sites.",
            "Somehow the idea would be somehow to browse quickly through this different.",
            "Representations which typically map back to to either research communities problems which could be solved, and so on.",
            "We have kind of 1st quick."
        ],
        [
            "Introduction more like higher level and then the rest will be more like.",
            "Levels of representations with lots of demos.",
            "So and please ask questions.",
            "Whenever you feel are something is not clear."
        ],
        [
            "OK. Um?",
            "OK, I'll approach from machine learning, but I could approach from from from any other field.",
            "So if cratic way of explaining what actually machine learning does or this machine learning engineering part of machine learning, which typically nobody explains first is first topic is first step is always choosing right representation, so this is what we call feature engineering and then the second step is modeling.",
            "This usually means statistics and.",
            "Optimization the same would be true.",
            "Let's see if we would.",
            "Approach from I don't know semantics of semantic web for ontology engineering.",
            "Typically we use first we decide about the presentation about this language and so on.",
            "Then we do modeling or whatever we do.",
            "So typically people do, let's say in these two steps modeling quite well and.",
            "They often are almost always listing machine learning.",
            "They ignore data representation, but the key message here is that good representation with better algorithms gives typically much better results than good algorithms with better presentation, which is.",
            "Not surprising, but somehow research communities tend to get stuck with certain representations and then they develop just the algorithmic parts.",
            "So here the idea is basically of this talk to go through these different representation and show you what is possible at what costs.",
            "For each individual level."
        ],
        [
            "For presentations OK, first, let's start with one quick example, so why representation matters?",
            "I will give you an example from machine learning which is which basically applies to all other fields as well.",
            "So we'll start with the demo.",
            "I have this on the slides as well, but let's make it live.",
            "OK, so this is 1 demo which uses this very popular and one of the more successful methods for statistical modeling machine learning called SVM support vector machines.",
            "Now if you.",
            "But if we go to the heart of machine learning is always trying to model some kind of.",
            "To model data points in some kind of space, typically high dimension, here will be we will live in two dimensions, but basically the same applies for anything else, where typically we would live up to.",
            "I don't know hundreds of thousands of dimensions, but the same method.",
            "So we will have some red crosses.",
            "And blue crosses now the whole problem is how to divide red and blue.",
            "So if we have very simple representation, then we can do so here should be easy.",
            "So this level.",
            "So we will select something which we call linear kernel which in normal language would mean just straight lines.",
            "Press a button and.",
            "To get the model so the model is so this would be sort of optimal straight line which divides threats from blue ones.",
            "But of course as soon as we.",
            "Boots.",
            "A little bit more friends here.",
            "For instance, and some blue ones here.",
            "Then the well this straight lines are not good enough anymore.",
            "See this red crosses are already on the other side, so these are obviously mistakes, but it's still kind of clear that we could potentially divide the cause, but we would need another representation.",
            "Other type of language.",
            "So instead of straight lines will use polynomials here.",
            "I see OK, so this.",
            "Fell sometimes.",
            "Not even polynomials are good enough, but generally generally should.",
            "Let's make a new example.",
            "So.",
            "This.",
            "And.",
            "Something like this and now polynomials would be so straight lines.",
            "Not good enough, but Polynome is now divided.",
            "OK, we increase the power of the language a little bit and now we are able to model the data in slightly better way.",
            "But now if you get way more evil and we put let's say blue points here.",
            "And maybe Red points here.",
            "Now let's say straight lines for sure.",
            "It will be.",
            "So this is this optimization algorithm which basically doesn't give any results.",
            "So this is there's some American optimization going on, which sometimes is not.",
            "We are finished, but we want to wait.",
            "OK, let's let's just rerun this.",
            "It's not my software right now.",
            "Uh.",
            "This is just for demo purposes, so that's why it's not otherwise these things and usually.",
            "Let's make this evil example.",
            "Again.",
            "Like this?",
            "Blue points here blue points here, so now straight lines.",
            "Missy, it's complete chaos.",
            "So this level of language wouldn't work polynome's.",
            "I hope it wants so.",
            "It's also chaos, complete curves but.",
            "Huh?",
            "This is a decision.",
            "This is the decision line.",
            "So on both sides, so it's always the central line, the central line, which tries to be optimal according to some according to these criteria.",
            "But since you the language is constrained, you cannot, you cannot do miracles here, so you're not allowed to make curves and so on.",
            "But let's see if we use.",
            "Representation this would be so called Gaussian kernels.",
            "Gaussians basically means that you put a lot of this Gaussian Bell curves on the surface and this resent then the model.",
            "Now we get and we can be even very more evil and still it would work.",
            "So like let's put a little bit of here.",
            "Some red dots.",
            "In the middle.",
            "I hope it will work for.",
            "I don't understand the visualization, so areas that are the one in the right areas at the other one.",
            "But you're saying their line is somewhere else.",
            "We actually would be the line up the line because so the line now would be in high dimensional space, would still still be somewhere hyperplane, you just the way how it's expressed.",
            "So it's basically this is not clear from.",
            "I would need to go a little bit more into SVS, which I don't want now, but basically we expand this 2 dimensional space where.",
            "Each point has X&Y, we expand each each point into high dimensional space through the language of the model, and then in this high dimensional space somewhere straight straight plane appears, so we see points in original space, but the plane we cannot see in this negative space because it's very high dimensional, so this is explanation of this.",
            "This example.",
            "Can you do it with projecting them into the third dimension and cutting through with it?",
            "Three wouldn't be enough.",
            "It wouldn't be enough.",
            "So this is the whole point of this kernel.",
            "Somehow that sometimes you have even this endless endless dimensional space, endless dimension space, you would find somewhere the hyperplane.",
            "But these are the details of the of the algorithm, which is not very complex.",
            "Later on I will show the version of the algorithm, which is pretty much 10 lines of codes, But this this shows you.",
            "The message of this demo is it with the proper representation and we change the representation through this kernel through the kernels.",
            "Somehow we can.",
            "We can get way more expressive power or we can at least tune the expressive power of our model and adapted to the problem.",
            "So in the rest of the talk we will.",
            "Try to touch different different representations or different types of.",
            "Let's say this feature spaces which we can extract out of text and some are more appropriate for certain problems.",
            "Some are more appropriate for some other problems.",
            "And points won't be only in two dimensions, but typically they would be in fame or dimensions.",
            "But the intuition of principle is pretty much the same.",
            "Done.",
            "Can you repeat the question?",
            "Dictation.",
            "Yeah, in this case I'm mixing a little bit Sir presentation and representation, but internally there's native representation for each point, which is just X&Y.",
            "So the point is positioned, but when I press the button, when I select the kernel and I press the button, then each point gets expanded into different, so this X&Y gets way more additional features.",
            "And then in that extra feature space then then we.",
            "This is basically the trick which I wanted more like to skip here, which still we deal with represent changing representation, but it's not visible on the from the from the image.",
            "But we are classification then operates on this extended representation of the points.",
            "So this is one way how we can explain this.",
            "OK. Let's go back to slides.",
            "So."
        ],
        [
            "I have a couple of."
        ],
        [
            "Fix this."
        ],
        [
            "Some examples which I was showing pretty much."
        ],
        [
            "Couple of generic slides, so this is 1 which Lane and then he would know so.",
            "This is.",
            "The trials how to.",
            "Puts this data processing into one lesson 3 into 3 dimensions.",
            "So on one side we have different data modalities.",
            "So from completely non structured data.",
            "So this would be signals coming out of sensor networks and so on.",
            "Then a little bit more structure if you take audio, video and so on is still very chaotic but a little bit more structure is in there than if we add some more.",
            "Structure, then we would come to texts which has structure but still typically we will consider taxes, unstructured data.",
            "Then we add a little bit more.",
            "We would come to networks graphs.",
            "So this would be semi structured.",
            "So graphs are structured.",
            "But again it's lots of freedom what we can represent and then structured data would be traditional databases.",
            "So records with fields and so on.",
            "And if we add even more structure than we would come to something which we could call ontology slightly.",
            "So ontologies generally would be the most structured.",
            "Data objects we typically deal with.",
            "Sometimes they are even two structured two to come too complex too.",
            "To deal with them.",
            "So this is the spectrum of different data modalities which you would find now for each of these data modalities we have set of operators which we try to apply to them.",
            "So from very simple ones like collecting the data.",
            "But in the case of text this would be crawling and so on preparing pre processing representing the data for additional for modeling.",
            "So modeling themselves modeling could be either.",
            "Handmaids could be automatic, semi-automatic, and so on.",
            "So on then reasoning of various kinds, logic, reasoning, probabilistic reasoning, and so on.",
            "And the end.",
            "Let's say presentation, visualization and so on delivering results.",
            "And we have third dimension which is a little bit less.",
            "A little bit less.",
            "It doesn't really represent a spectrum, but there are other issues which we need to be aware of, like.",
            "Like scalability, so sometimes they are operated just with a small amounts of data.",
            "Sometimes we operate with huge amounts of data.",
            "Today this big data movement is.",
            "Popular, so this capability definitely an issue today.",
            "The enemy city.",
            "So data change two times so we need to be aware of this context so data themselves or data plus context.",
            "So this certainly is an issue.",
            "Quality of of data.",
            "This is always an issue in all data related fields usage.",
            "So the way how we use it and so on.",
            "So there are many other issues which we try to."
        ],
        [
            "Which we try to deal with now if we take a subset of this 3 dimensional cube, different subsets, we would get different research areas like machine learning would typically deal more like on the.",
            "On the right side, right side of this dimension would typically deal more like with this middle representation and modeling and with some of these aspects.",
            "So this sub sub sub cube would define let's say machine learning and data mining for the same reason.",
            "Information retrieval similar would be on texts.",
            "Typically, representation, modeling and maybe scale and we would get search engines.",
            "Natural language again texts modeling more like representation.",
            "A little bit of modeling and.",
            "Maybe a little bit of context and we would get NLP.",
            "Semantic Web fits more like on the on that side, so structured objects maybe context quality and more like representation, reasoning, modeling.",
            "So a little bit on that side.",
            "So different subsets of this cube represent different research areas.",
            "So today we'll touch more like this text ontologies and this middle middle side of this dimension."
        ],
        [
            "Now.",
            "If we, let's say here we.",
            "Mentioned set of operators.",
            "So I tried to classic cluster these different approaches into three major categories.",
            "How we approach?",
            "How to how we approach solving different?",
            "Let's say scientific are just.",
            "Engineering problems typically today we would have three approaches, so top down approaches which are model driven.",
            "So this is model driven approaches were more like characteristic for old times, like 80s, early 90s.",
            "Why just cause capacities of computers were smaller, but there were no data available and typical paradigm was smart researcher.",
            "Malcolm was observing the data and put a hypothesis hypothesis was in the form of a model and then the this model was tested in one or the other way.",
            "So this is model driven.",
            "So inside model and testing, so statistics has still a lot of this style of approach is also.",
            "NLP.",
            "And it will be vision and so on from the end of 80s was these areas were completely through.",
            "And now once the data appeared, this happened more like the turn of the now in early 2000s.",
            "So there was definitely the area are different areas which are more like socalled data driven or bottom up approaches appear, so machine learning, data mining.",
            "And for the same reason, also information trivial and many other fields like also natural language processing, vision and so on.",
            "They all turned to be way more data driven or some areas completely changed to be data driven today.",
            "Let's say if you would go to vision conferences or NLP conferences like 90% plus of papers would be about data driven.",
            "That's not about model division in 80s.",
            "End of 80s.",
            "Let's say everything would still be modeled rule, so this is different paradigm now.",
            "There's also the third one, which appeared more like in the last 10 years.",
            "So these are these collaborative approaches or socially driven.",
            "So like Web 2.0 social computing where.",
            "The idea is the following.",
            "We say OK, here we have a problem.",
            "It's hard to solve.",
            "We cannot even be.",
            "We certainly we don't have methods to solve this.",
            "So let's ask people.",
            "But people are expensive or not too knowledgeable and this area basically deals.",
            "So this would be human computation.",
            "Social computing and so on.",
            "Where people give small contributions and we would come pretty much with the solve problem at the end.",
            "So this would include everything from things like Wikipedia.",
            "The socially driven on the ontology.",
            "So taxonomies like delicious and many others so they would come from this area so.",
            "Generally, each solution to a problem we could, we could typically classify in one of these three.",
            "Of course, they also combinations of that."
        ],
        [
            "No.",
            "Before going to the actual.",
            "Technical part of the talk.",
            "So there's this one slide which I typically show.",
            "So if you talk about the text processing, so there's the several areas which research areas which deal with text.",
            "It's like you know this story about this elephant and blind men, which approaches elephant.",
            "Everybody touched elephant from one side.",
            "So and this people came back and they were asked well how elephant looks like and some for this guy.",
            "Elephant is like a rope.",
            "For this guy at the front is like a spear for this guy.",
            "It's like a wall so everything is right.",
            "But none of them is completely correct.",
            "So the same way areas dealing with texts.",
            "This approach.",
            "Problem how to model how to solve problems on the text.",
            "So if you take computational linguistics, everything is about the language the unit of processing is typically sentence.",
            "They hardly even go beyond the sentence level maybe, but document is already a big big unit for processing for them that 2.0 everything will be about community.",
            "So we ask communities semantic.",
            "Rap is mostly about interoperability, text mining, scale analytics.",
            "So machine learning would be more like in the recent years, more like statistical approaches then would have information.",
            "Three over everything is about search how to find things.",
            "And maybe machine translation.",
            "Also an area where everything is about mappings.",
            "So if we go to the core of machine translation is about how to map one representative textual representation to other.",
            "And they all have similar insights, but still different enough so that they are they living typically in separate spaces.",
            "Kind of tend to go to most of these events from these areas and.",
            "It's interesting to see that something which is extremely easy and clear for, let's say, information through our machine learning guys.",
            "It's not so clear to let's say semantic guys or Web 2.0 guys.",
            "So are the only discovering things which were known.",
            "Let's say 15 years ago there.",
            "OK, so this is more like intro to the talk."
        ],
        [
            "So.",
            "Let's go more like to this technical part.",
            "How do we represent text so?"
        ],
        [
            "There would be, so this is 1 trial.",
            "How to classify different levels of text representation?",
            "I tried to put them into this.",
            "So 544 so.",
            "Searching levels.",
            "From very simple ones to, let's say the most complex ones, and this is 1.",
            "This is debatable and we could change add some things.",
            "Remove some levels and so on.",
            "But well, this roughly corresponds to how things are done, so this would be more like lexical levels where we we try to be aware of.",
            "Only about the terms words typically, so we are not aware of the context.",
            "Too much and so on.",
            "So the language or the basic units from the text would be the main domain trigger for building representation, then syntactic where we have already notion of some kind of structure within a text, and then there would be set of so-called semantic levels where we try to dig a little bit more into the.",
            "Uh, not really.",
            "Meaning of what's being written.",
            "Although there are trials, but we try to extract also the semantics so.",
            "Less obvious relationships between the items which appear in the text."
        ],
        [
            "OK, now.",
            "If we go just briefly through all the representations, let's say very simple.",
            "So if you take a document and we just extract the characters, we don't care for words, nothing.",
            "Are we able to solve some problems?",
            "Yes, let's say 80s before all these texts, related areas were around.",
            "Somehow people were actually using just.",
            "Simple character representations and they were able to solve a couple of problems like language identification, copy detection to find whether somebody is copying.",
            "Homework so you don't need much you need just this very simple representation and it's enough language identification.",
            "How to identify the language out of the out of the document.",
            "The language document is being written in, then this is enough.",
            "Then if you add a little bit more so, then we have words.",
            "Phrases or words would be just words themselves, phrases, sequences of words, then part of speech very adds some, let's say category supports.",
            "So at this level we can already do, let's say name, entity extraction.",
            "This is one of important industrial segments dealing with texts.",
            "Then if we had a little bit more so we would come to this taxonomy so it is Aris.",
            "However, we have clusters of words and so on, and we know a little bit more about things, but still it's just about the lexical level.",
            "Then if we switch to this something which we could call as a syntactic level.",
            "So we come to this extremely or probably the most important representation nowadays on the market vector space model.",
            "So here we can do text categorization, clustering, search summarization and so on.",
            "So things happen here.",
            "Touch this a little bit more in detail afterwards.",
            "So let's say Google works on this level.",
            "All search engines use this representation.",
            "La.",
            "Then if you go a little bit further yet, language models a little bit more structuring to the representation, then let's say machine translation typically would operate here language models.",
            "So where we are aware of sequences of words, spam filtering typically would use this as well.",
            "Then we have parsing across modalities.",
            "An interesting representation how to how to match problems like multilingual Search.",
            "So shifting text with images or with other data modalities, or this would be a little bit extended representation from vector space model.",
            "Then if you go to.",
            "Semantic levels, let's say collaborative tagging Web 2.0, so this is more like unifying semantics of data.",
            "This is one way how we can approach us, so this is more like collaborative approach and link data.",
            "OK, you will hear more on other talks on linked data and it's more like about data integration.",
            "And so if we come down to the bottom then we can talk about reasoning semantic search this style of problems.",
            "Which is.",
            "Extremely hard, but can be also very beneficial if you do it in the right way.",
            "So this is 1 trial.",
            "How to go through different representations?",
            "And what sort of problems are?",
            "Possible to solve on this?",
            "On each of them now in the rest of the talk will go from the top to the bottom.",
            "And touch not really all of them, but some of them and.",
            "Try to see some issues related and then are there any questions here?",
            "OK then let's go."
        ],
        [
            "Continue, so let's first to have this characters."
        ],
        [
            "I said before that character representation, although it's very simple.",
            "Still, it's quite powerful in many ways, at least for some problems.",
            "So typically, let's say 1 one way how we can represent text?",
            "So if we have a quick rat so we just make let's say 2, three in this case 3 grams.",
            "So all sequences of three subsequent word letters, nothing else.",
            "And this is then put into either machine learning or something else.",
            "And what we can do with this?",
            "With this representation we can, let's say, detects languages.",
            "We can detect copies.",
            "We can even do classification to some degree and so on.",
            "So let's say."
        ],
        [
            "Good and bad sides.",
            "1st, it's extremely robust, so it captures simple patterns in the text and it's suitable for lots of analytical tasks because it's just set of flat features, nothing else.",
            "But for any deeper, deeper semantic tasks.",
            "Of course this wouldn't work."
        ],
        [
            "Here's an example of this language identification.",
            "LA.",
            "So if you take a document.",
            "Just count the.",
            "Percentage of particular engrams are two grams of letters.",
            "We get sort of signature for each language, so this is signature for English language.",
            "So if the distribution of N grams to grams bigrams in this case would be close to this distribution, then this would be English.",
            "If, say, German language would have another typical.",
            "The spectrum of this distribution of bigrams, and so on, and basically this is this is the whole thing.",
            "So here we have one simple demo.",
            "Maybe it works.",
            "It's not ours.",
            "Disclaimer or.",
            "Just as long as you have running.",
            "The saying you can do that with a big grams.",
            "You were mentioning tree gum support.",
            "Is there any?",
            "Evaluation, like what end in N Grams, works best for what task like?",
            "So.",
            "Typically this the end would relate to the amount of data you're dealing with.",
            "So if you go to I don't know more.",
            "Let's say four 5 grams.",
            "Then of course everything would be nice except you you have lots of so think 1 gram.",
            "So single letters you will have 25 by grams.",
            "You would have all of them are 625, three grams times 25 so.",
            "So if you want to have statistically significant.",
            "Accounts so that you can compare.",
            "Then you run into this so called sparsity problem.",
            "So that's why you try to keep this end as low as possible, but still not too low, so single letters are typically not enough.",
            "I mean they each language has some kind of distribution of over single letters, but not not typically by grounds would be best.",
            "Let's especially if you want to extract bigrams from an average document, then with three grams you would.",
            "You wouldn't.",
            "You wouldn't get proper proper distribution, so so it's typically it's related amounts of data.",
            "If you have lots of data, then you can afford hyeran.",
            "Character.",
            "So."
        ],
        [
            "So I said so you can.",
            "You can do everything pretty much from from classific classification, clustering.",
            "Everything is just because expressive power is so so low in this presentation that things wouldn't work well.",
            "This is.",
            "So let's see if we go back to this example here."
        ],
        [
            "You are saying that actually even things like topic detection.",
            "Sure sure you just get out of this you these 3 grams or 4 grams.",
            "Whatever are so ambiguous it wouldn't capture it's much.",
            "Let's say if you go to the word level of phrase level, you capture way more way more.",
            "Information out of, let's say this proper.",
            "Proper features which are, let's say phrases or words then from from these three 3 grams which or you need a lot of data to approach the performance of this stronger representation.",
            "So this would be generally the balance, but it's very nice because it's extremely simple, so it's well language independent and has lots of nice features.",
            "But let's say I remember in 80s one side was approached by police or some defense or spice.",
            "Whatever they were using this because nothing else was available out there on the market at that time.",
            "So I was impressed.",
            "I mean, this actually worked for them, maybe didn't country collapsed afterwards.",
            "OK, this demo since it's not ours doesn't work so.",
            "Misguided this type of language and edification demos you paste in a document and tells you the probability for particular language."
        ],
        [
            "OK, maybe."
        ],
        [
            "Just one more.",
            "You sure about the characters when you when we deal on the level of characters, everything looks like similar, but sometimes things are not that easy, so.",
            "Different characters can be written in different ways, less if we go to Unicode.",
            "So we need to be careful also about this kind of simple things.",
            "So this character can be written at least in these two ways, or sometimes they are even more so, so this normal is a character level normalization, some sometimes it's not.",
            "Easy, so if we leave in seven bit ASCII then life is very easy, but we don't usually."
        ],
        [
            "OK, let's go now to.",
            "You can just check the time.",
            "I have 45 minutes only so will skip some of the things.",
            "Otherwise this is more like 3 hour tutorial altogether.",
            "So."
        ],
        [
            "Words.",
            "Words are typically the most common representation.",
            "When we saw it's versus the unit of text in a way so it's very natural somehow to deal with words."
        ],
        [
            "And when dealing with words, we need to be aware of this four properties of words which which make life much harder.",
            "So if each word would be uniquely defining some kind of sense or meaning would be much easier.",
            "But let's say we have.",
            "We deal with this hanami where we have the same form of the word but different meanings or like banquet meaning River bank or financial institution or this policy me where same form related meaning synonymy where we have different forms and same meaning.",
            "And hyponymy, which one word denotes a subclass of the other one?",
            "So when dealing good words, so in a way with the methods which operate on text somehow need to compensate this.",
            "This for features which are all four are sort of anomalies.",
            "So these are the bets sides but easier site of dealing with taxes.",
            "Is that word frequencies in text have this power law distribution?",
            "This is maybe not quite obvious, but why dealing with taxes?",
            "Easy because of this single feature, so there's so much redundancy in text, so that even if we remove lots of information out of out of the.",
            "The raw text representation still thinks working things work because of this power law distribution, so I won't go into detail, but this is.",
            "This is one of the very fundamental features dealing with text."
        ],
        [
            "OK, maybe just to add couple of details so software this is something which one needs to be aware.",
            "So for simple text processing we typically remove the functional words which don't have lots of meaning and typically with removing this stop words.",
            "So this would be English, Dutch, Slovenian.",
            "Each language has specific one specific set with our help algorithms out not to come into."
        ],
        [
            "What troubles?",
            "Related to this, we have stemming.",
            "Can Lemmatization, stemming is a very simple procedure which tries to normalize or put the words in some kind of Canonical form.",
            "Stemming is really just takes the stem the stem of the word tries and makes lots of mistakes.",
            "But it's very cheap cheap procedure to do it.",
            "So let's say this would be better.",
            "Better example Universe, University, University, universal.",
            "Everything would be translated into universe which is for practical persons.",
            "Purposes in many ways, in many most cases actually enough little bit better, but a little bit harder procedure is limitation where we try to really put the word in a proper normalized form like universe would be universe, University, universities and so on would be University universal would be universal, so this is the difference.",
            "So this is something which is typically underlying the if you go to Google and type in the word in this inflected form first thing what would happen would actually.",
            "The world would be normalized in one of these ways, and then it would go to section.",
            "Well."
        ],
        [
            "Call stemming work so This is why stemming is so inviting because you go to this website and take the procedure down.",
            "So it's so called Porter Stemmer.",
            "Whichever is using a set of cascade rules which transform words like these are the rules.",
            "So if the word ends with national, then transform it this suffix into eight so relational relate conditional condition.",
            "So instead of this set of rules which are.",
            "Hand mate, some other work, sort of well or well enough so that everybody is using it."
        ],
        [
            "OK, let's go to the next level phrases phrase."
        ],
        [
            "This would be 16 sequences of words which have a little bit stronger, stronger meaning when they are together, so like.",
            "Artificial intelligence or text mining or word for Windows.",
            "So word for Windows has a meaning.",
            "It's very clear what it is.",
            "Is a piece of software and so on.",
            "We have instead, if we had just words and four and Windows, we would need quite some data to identify this as that this actually should be somehow together and statistics or some methods would would would detect this relationship between these three words.",
            "So if you had the algorithms come out.",
            "To tell them that, well, this is the phrase which you should use, then of course.",
            "Of course, it's now.",
            "We typically get better results.",
            "OK, I won't go too much into the details, but I."
        ],
        [
            "So you just this data set which is available.",
            "So this is this Google Ngram corpus which was released in 2006.",
            "Which body I think everybody can still download, I hope.",
            "So in the end, So what give Google gives you it's 24 gigabytes of compressed text files, which altogether it has.",
            "A lot of zeros.",
            "Millions of trillion.",
            "Now we learn all these big numbers because of the crisis.",
            "Three and three and tokens, so which were extracted out of 95 billion sentences.",
            "So this would.",
            "This is pretty much from the English part of the web.",
            "These are all engrams of size 1234 and five which appear which appeared in 2006 more than 40 times on the web.",
            "So if you would look at uni Grams so single words, this would be a little bit roughly 13 million of them.",
            "And if you go to let's say four 5 grams, it's a little bit more than billion.",
            "So this typically.",
            "So this."
        ],
        [
            "N grams look like this so it's just flat files with the words and the frequency behind.",
            "So this is I would say.",
            "Extremely valuable resource which is under exploited by research community in many ways.",
            "Typically you would.",
            "You would have everything but what's relevant in the text or also what was being used in the text."
        ],
        [
            "OK, part of speech will just briefly touch a bit so."
        ],
        [
            "Part of speech text.",
            "So this is when."
        ],
        [
            "Be I will show."
        ],
        [
            "You're just an example.",
            "If we have a text so part of speech tagging means that we add to each words it's linguistic rules.",
            "So and once we have this sort of semi meta data more like linguistic metadata, shallow one.",
            "Then we can put different patterns on top of this and with this level we can we can do let's in entity extraction and so on.",
            "Much easier.",
            "So noun phrases would be very easily extra identified and then plus a couple of patterns we would get name entity extractor on this level already.",
            "And typically you find for all major languages part of speech taggers.",
            "This is something which is available today."
        ],
        [
            "OK taxonomies."
        ],
        [
            "So let me suggest you all know roughly so the most known one is."
        ],
        [
            "Word net, which may be few words about word net.",
            "So it's a graph.",
            "Basically it consists from 4 databases, database of nouns, verbs, adjectives and adverbs.",
            "So let's say we would have 94,000 unique forms and 160,000 census.",
            "So because each words can appear in several senses, so it's that's why the difference.",
            "And at the end the.",
            "I don't have a picture."
        ],
        [
            "OK. Basically, the each note of this graph.",
            "Each note of this graph is a set of synonyms, so words which mean pretty much the same."
        ],
        [
            "Examples would be like musician, instrumentalist player, so this would be.",
            "In one note of the graph or person, individual someone or lifeform Organism being so all the surface forms which mean the same they are in one node of the graph.",
            "And now this no."
        ],
        [
            "So the graph are connected with the relationships and we have something like 16 relationships out of which this would be probably most important, like hyper name, let's say from lower to higher concepts.",
            "Breakfast meal would be connected like this or hypernym.",
            "From concepts of solving it's so me lunch or has member from groups to their members, faculty and professor.",
            "Part of so from our course meal, or Anthony opposites like Leader follower.",
            "So words are senses of words.",
            "I connected with these relationships and obviously we can exploit this mostly.",
            "If you would go through the.",
            "Sets of research papers ever is trying to use only this hypernym, so other relationships are less exploited."
        ],
        [
            "OK, now we went through this lexical representation.",
            "Now let's say switch to this syntactic ones first will stop with this vector space model, which is way the most important representation on this level.",
            "And here we have also a couple of demos."
        ],
        [
            "Um?",
            "So.",
            "So what we do here?",
            "This is more like analytic view, so we take the.",
            "We take the document, we transform them the document into sparse numeric vectors, and then we perform all kinds of linear algebra operations on it.",
            "So this is this vector space model, so if you look at search which make it Google or any other search engine is actually it's simple.",
            "Dot product from linear algebra.",
            "Of course it's done in optimized way, but it's still it's a dot product product, nothing else.",
            "So let's."
        ],
        [
            "See what we do actually.",
            "So we take a document, we just take individual words or phrases.",
            "And construct a vector vector of frequencies.",
            "Nothing that's now this vector.",
            "Then we can post, process it and so on.",
            "But basically all this mathematical machinery operates on these vectors and we can do classification, clustering, search, comparison, everything functions them on this vector.",
            "So this is this is it?",
            "Why it's nice?",
            "Becausw, well, mathematicians are this American mathematicians developed all this fancy methods to deal with vectors and matrices and all the machinery, then usable for the."
        ],
        [
            "This representation.",
            "Maybe it's worth."
        ],
        [
            "Mentioning so typically we don't operate on.",
            "Vectors of frequencies.",
            "But we transform these frequencies into weights WHI because, let's say words like.",
            "Let's say the and all these frequent frequent words typically would appear extremely too many Times Now we need some kind of weighting of importance for the words and this weighting scheme is called."
        ],
        [
            "Why the F?",
            "Which basically has just two factors.",
            "So the word is more important if it appears several times in the target document and the word is more important if it appears in less documents.",
            "So it has these two bits and with these two factors we try to express this.",
            "So basically intuition is if if if a word appears all over all the documents many times, then we push it down.",
            "If it appears in a concentrated way then we push it up.",
            "So this is.",
            "This weighting basically gives you.",
            "An exam."
        ],
        [
            "Apple.",
            "Soon article about Donald Trump.",
            "So.",
            "So it's just the document.",
            "And then when we transform it into this bag of words representation.",
            "And this is, I think, sorted according to the wait.",
            "You see that the words like resorts class Trump voting states come up.",
            "And this is this simple formula which doesn't know anything about the context.",
            "And already we this could be actually out of this.",
            "We typically generate clouds, so tech clouds are just a visualization of these vectors, nothing else.",
            "So foremost, foremost.",
            "Vector space operations we use.",
            "Let's say this weighting schema."
        ],
        [
            "There was extensive extend to extensive research in 90s.",
            "Mostly what would be the magic formula here and they tried everything possible.",
            "You have papers with hundreds and hundreds of different formulas, but at the end is simple simple thing somehow still worked with major best.",
            "Nowadays, we can artificially generate this optimal weighting scheme As for particular needs, but it takes a little bit more little bit more time.",
            "Are typically we need a little bit more."
        ],
        [
            "Our extra information as well so."
        ],
        [
            "OK.",
            "This is 1 very basic operation, which again.",
            "I'll try to explain it in simple way so if we have to document, we try to compare them.",
            "Are they similar or not?",
            "So and basically what we do.",
            "Since every document becomes a vector.",
            "So now this is two dimension space.",
            "First we have first document which is vector and the second document which is vector.",
            "And then we calculate the cosine.",
            "Simply 'cause I'm of this angle so.",
            "If the document is almost the same, so the angle is small, then we get one causing and if the documents are completely orthogonal.",
            "Well then it's zero, so that's the whole intuition.",
            "So this is this dot product.",
            "With this with the similarity measure, then we can perform search like search engines.",
            "Typically, if you wouldn't go to almost any.",
            "Serious Database which supports also text, text blobs and they would have TF IDF and this.",
            "Causing similarity.",
            "So basically you get search for free so search engines generally would work quite well.",
            "If they would use just TF IDF vectors and causing similarity, you don't need much.",
            "And this is mathematics which you already learned in high school, so it's nothing special."
        ],
        [
            "No.",
            "Let's go to document categories categorization.",
            "Now what is document categorization?",
            "So if we have set of documents labeled with Doc content categories, the goal is to build a model statistical model which would automatically assign the right content categories or labels to new unlabeled documents.",
            "Content this content categories can be their unstructured.",
            "Let's say Brotis corpus is very famous by this so 120 flat categories or structured.",
            "So let's say Yahoo or Dimas or Medline taxonomies which so this problem is a little bit harder.",
            "So."
        ],
        [
            "And for this document categorization, we have lots of methods, so lots of this text mining and machining efforts at the end of 90s in beginning of 2000s where dealing on how to do text classification by many different ways.",
            "So before I show this support support vector machines which today people mostly use, I mean there are the newer algorithms as well CRF's and so on.",
            "Logistics regression is very popular as well.",
            "Perceptron I will show.",
            "So this is works as good as this support vector machine, you just 10 lines of code is very small Navy base and others.",
            "So they're just set of methods.",
            "We won't go too much into the details here."
        ],
        [
            "I would just show this perceptron algorithm.",
            "I won't go into details, but this is the whole code.",
            "Together with the comments.",
            "So this.",
            "If you understand what this few lines of codes work, then you can do most of machine learning nowadays.",
            "If you deliver the proper feature features into the.",
            "Into the system.",
            "Features we get with either engrams, words, phrases and so on.",
            "As we talked before.",
            "So I don't have any buffer proper visualization of how this works, but basically just pushing one hyperplane here and there and somehow algorithm has this nice property that it converges very fast and you get the proper model."
        ],
        [
            "OK, I'll skip this evaluation."
        ],
        [
            "So first we mention document categorization and the second task which is very important is also document clustering.",
            "So here we operate with the documents which don't have labels and we would try to make groups groups of documents which are kind of similar.",
            "So here we will have this so famous came in Salgo rhythm.",
            "We have a couple of more, but K means is mostly used in practice, which."
        ],
        [
            "This is the whole the whole idea, so again I won't go into the code, but it's extremely simple.",
            "All together would be, let's say maybe 1020 lines of code."
        ],
        [
            "OK, let's now this one visualization."
        ],
        [
            "It's less."
        ],
        [
            "So let's keep letting semantic indexing, which is just another way of doing clustering.",
            "And let's go now to a couple of demos on for classification."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the topic of today's talk is on how to represent text?",
                    "label": 1
                },
                {
                    "sent": "I think there are a couple of presentations.",
                    "label": 0
                },
                {
                    "sent": "This summer school on LB.",
                    "label": 0
                },
                {
                    "sent": "Text and so on.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this talk is somehow to go through.",
                    "label": 0
                },
                {
                    "sent": "Different ways how to represent text so different research communities approach attacks from different sites.",
                    "label": 0
                },
                {
                    "sent": "Somehow the idea would be somehow to browse quickly through this different.",
                    "label": 0
                },
                {
                    "sent": "Representations which typically map back to to either research communities problems which could be solved, and so on.",
                    "label": 0
                },
                {
                    "sent": "We have kind of 1st quick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction more like higher level and then the rest will be more like.",
                    "label": 0
                },
                {
                    "sent": "Levels of representations with lots of demos.",
                    "label": 1
                },
                {
                    "sent": "So and please ask questions.",
                    "label": 0
                },
                {
                    "sent": "Whenever you feel are something is not clear.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll approach from machine learning, but I could approach from from from any other field.",
                    "label": 0
                },
                {
                    "sent": "So if cratic way of explaining what actually machine learning does or this machine learning engineering part of machine learning, which typically nobody explains first is first topic is first step is always choosing right representation, so this is what we call feature engineering and then the second step is modeling.",
                    "label": 0
                },
                {
                    "sent": "This usually means statistics and.",
                    "label": 0
                },
                {
                    "sent": "Optimization the same would be true.",
                    "label": 0
                },
                {
                    "sent": "Let's see if we would.",
                    "label": 0
                },
                {
                    "sent": "Approach from I don't know semantics of semantic web for ontology engineering.",
                    "label": 0
                },
                {
                    "sent": "Typically we use first we decide about the presentation about this language and so on.",
                    "label": 0
                },
                {
                    "sent": "Then we do modeling or whatever we do.",
                    "label": 1
                },
                {
                    "sent": "So typically people do, let's say in these two steps modeling quite well and.",
                    "label": 1
                },
                {
                    "sent": "They often are almost always listing machine learning.",
                    "label": 0
                },
                {
                    "sent": "They ignore data representation, but the key message here is that good representation with better algorithms gives typically much better results than good algorithms with better presentation, which is.",
                    "label": 1
                },
                {
                    "sent": "Not surprising, but somehow research communities tend to get stuck with certain representations and then they develop just the algorithmic parts.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is basically of this talk to go through these different representation and show you what is possible at what costs.",
                    "label": 0
                },
                {
                    "sent": "For each individual level.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For presentations OK, first, let's start with one quick example, so why representation matters?",
                    "label": 1
                },
                {
                    "sent": "I will give you an example from machine learning which is which basically applies to all other fields as well.",
                    "label": 0
                },
                {
                    "sent": "So we'll start with the demo.",
                    "label": 0
                },
                {
                    "sent": "I have this on the slides as well, but let's make it live.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 demo which uses this very popular and one of the more successful methods for statistical modeling machine learning called SVM support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Now if you.",
                    "label": 0
                },
                {
                    "sent": "But if we go to the heart of machine learning is always trying to model some kind of.",
                    "label": 0
                },
                {
                    "sent": "To model data points in some kind of space, typically high dimension, here will be we will live in two dimensions, but basically the same applies for anything else, where typically we would live up to.",
                    "label": 0
                },
                {
                    "sent": "I don't know hundreds of thousands of dimensions, but the same method.",
                    "label": 0
                },
                {
                    "sent": "So we will have some red crosses.",
                    "label": 0
                },
                {
                    "sent": "And blue crosses now the whole problem is how to divide red and blue.",
                    "label": 0
                },
                {
                    "sent": "So if we have very simple representation, then we can do so here should be easy.",
                    "label": 0
                },
                {
                    "sent": "So this level.",
                    "label": 0
                },
                {
                    "sent": "So we will select something which we call linear kernel which in normal language would mean just straight lines.",
                    "label": 0
                },
                {
                    "sent": "Press a button and.",
                    "label": 0
                },
                {
                    "sent": "To get the model so the model is so this would be sort of optimal straight line which divides threats from blue ones.",
                    "label": 0
                },
                {
                    "sent": "But of course as soon as we.",
                    "label": 0
                },
                {
                    "sent": "Boots.",
                    "label": 0
                },
                {
                    "sent": "A little bit more friends here.",
                    "label": 0
                },
                {
                    "sent": "For instance, and some blue ones here.",
                    "label": 0
                },
                {
                    "sent": "Then the well this straight lines are not good enough anymore.",
                    "label": 0
                },
                {
                    "sent": "See this red crosses are already on the other side, so these are obviously mistakes, but it's still kind of clear that we could potentially divide the cause, but we would need another representation.",
                    "label": 0
                },
                {
                    "sent": "Other type of language.",
                    "label": 0
                },
                {
                    "sent": "So instead of straight lines will use polynomials here.",
                    "label": 0
                },
                {
                    "sent": "I see OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Fell sometimes.",
                    "label": 0
                },
                {
                    "sent": "Not even polynomials are good enough, but generally generally should.",
                    "label": 0
                },
                {
                    "sent": "Let's make a new example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Something like this and now polynomials would be so straight lines.",
                    "label": 0
                },
                {
                    "sent": "Not good enough, but Polynome is now divided.",
                    "label": 0
                },
                {
                    "sent": "OK, we increase the power of the language a little bit and now we are able to model the data in slightly better way.",
                    "label": 0
                },
                {
                    "sent": "But now if you get way more evil and we put let's say blue points here.",
                    "label": 0
                },
                {
                    "sent": "And maybe Red points here.",
                    "label": 0
                },
                {
                    "sent": "Now let's say straight lines for sure.",
                    "label": 0
                },
                {
                    "sent": "It will be.",
                    "label": 0
                },
                {
                    "sent": "So this is this optimization algorithm which basically doesn't give any results.",
                    "label": 0
                },
                {
                    "sent": "So this is there's some American optimization going on, which sometimes is not.",
                    "label": 0
                },
                {
                    "sent": "We are finished, but we want to wait.",
                    "label": 0
                },
                {
                    "sent": "OK, let's let's just rerun this.",
                    "label": 0
                },
                {
                    "sent": "It's not my software right now.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "This is just for demo purposes, so that's why it's not otherwise these things and usually.",
                    "label": 0
                },
                {
                    "sent": "Let's make this evil example.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Blue points here blue points here, so now straight lines.",
                    "label": 0
                },
                {
                    "sent": "Missy, it's complete chaos.",
                    "label": 0
                },
                {
                    "sent": "So this level of language wouldn't work polynome's.",
                    "label": 0
                },
                {
                    "sent": "I hope it wants so.",
                    "label": 0
                },
                {
                    "sent": "It's also chaos, complete curves but.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "This is a decision.",
                    "label": 0
                },
                {
                    "sent": "This is the decision line.",
                    "label": 0
                },
                {
                    "sent": "So on both sides, so it's always the central line, the central line, which tries to be optimal according to some according to these criteria.",
                    "label": 0
                },
                {
                    "sent": "But since you the language is constrained, you cannot, you cannot do miracles here, so you're not allowed to make curves and so on.",
                    "label": 0
                },
                {
                    "sent": "But let's see if we use.",
                    "label": 0
                },
                {
                    "sent": "Representation this would be so called Gaussian kernels.",
                    "label": 0
                },
                {
                    "sent": "Gaussians basically means that you put a lot of this Gaussian Bell curves on the surface and this resent then the model.",
                    "label": 0
                },
                {
                    "sent": "Now we get and we can be even very more evil and still it would work.",
                    "label": 0
                },
                {
                    "sent": "So like let's put a little bit of here.",
                    "label": 0
                },
                {
                    "sent": "Some red dots.",
                    "label": 0
                },
                {
                    "sent": "In the middle.",
                    "label": 0
                },
                {
                    "sent": "I hope it will work for.",
                    "label": 0
                },
                {
                    "sent": "I don't understand the visualization, so areas that are the one in the right areas at the other one.",
                    "label": 0
                },
                {
                    "sent": "But you're saying their line is somewhere else.",
                    "label": 0
                },
                {
                    "sent": "We actually would be the line up the line because so the line now would be in high dimensional space, would still still be somewhere hyperplane, you just the way how it's expressed.",
                    "label": 0
                },
                {
                    "sent": "So it's basically this is not clear from.",
                    "label": 0
                },
                {
                    "sent": "I would need to go a little bit more into SVS, which I don't want now, but basically we expand this 2 dimensional space where.",
                    "label": 0
                },
                {
                    "sent": "Each point has X&Y, we expand each each point into high dimensional space through the language of the model, and then in this high dimensional space somewhere straight straight plane appears, so we see points in original space, but the plane we cannot see in this negative space because it's very high dimensional, so this is explanation of this.",
                    "label": 0
                },
                {
                    "sent": "This example.",
                    "label": 0
                },
                {
                    "sent": "Can you do it with projecting them into the third dimension and cutting through with it?",
                    "label": 0
                },
                {
                    "sent": "Three wouldn't be enough.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't be enough.",
                    "label": 0
                },
                {
                    "sent": "So this is the whole point of this kernel.",
                    "label": 0
                },
                {
                    "sent": "Somehow that sometimes you have even this endless endless dimensional space, endless dimension space, you would find somewhere the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "But these are the details of the of the algorithm, which is not very complex.",
                    "label": 0
                },
                {
                    "sent": "Later on I will show the version of the algorithm, which is pretty much 10 lines of codes, But this this shows you.",
                    "label": 0
                },
                {
                    "sent": "The message of this demo is it with the proper representation and we change the representation through this kernel through the kernels.",
                    "label": 0
                },
                {
                    "sent": "Somehow we can.",
                    "label": 0
                },
                {
                    "sent": "We can get way more expressive power or we can at least tune the expressive power of our model and adapted to the problem.",
                    "label": 0
                },
                {
                    "sent": "So in the rest of the talk we will.",
                    "label": 0
                },
                {
                    "sent": "Try to touch different different representations or different types of.",
                    "label": 0
                },
                {
                    "sent": "Let's say this feature spaces which we can extract out of text and some are more appropriate for certain problems.",
                    "label": 0
                },
                {
                    "sent": "Some are more appropriate for some other problems.",
                    "label": 0
                },
                {
                    "sent": "And points won't be only in two dimensions, but typically they would be in fame or dimensions.",
                    "label": 0
                },
                {
                    "sent": "But the intuition of principle is pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the question?",
                    "label": 0
                },
                {
                    "sent": "Dictation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this case I'm mixing a little bit Sir presentation and representation, but internally there's native representation for each point, which is just X&Y.",
                    "label": 0
                },
                {
                    "sent": "So the point is positioned, but when I press the button, when I select the kernel and I press the button, then each point gets expanded into different, so this X&Y gets way more additional features.",
                    "label": 0
                },
                {
                    "sent": "And then in that extra feature space then then we.",
                    "label": 0
                },
                {
                    "sent": "This is basically the trick which I wanted more like to skip here, which still we deal with represent changing representation, but it's not visible on the from the from the image.",
                    "label": 0
                },
                {
                    "sent": "But we are classification then operates on this extended representation of the points.",
                    "label": 0
                },
                {
                    "sent": "So this is one way how we can explain this.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's go back to slides.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a couple of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fix this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some examples which I was showing pretty much.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple of generic slides, so this is 1 which Lane and then he would know so.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The trials how to.",
                    "label": 0
                },
                {
                    "sent": "Puts this data processing into one lesson 3 into 3 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So on one side we have different data modalities.",
                    "label": 0
                },
                {
                    "sent": "So from completely non structured data.",
                    "label": 0
                },
                {
                    "sent": "So this would be signals coming out of sensor networks and so on.",
                    "label": 0
                },
                {
                    "sent": "Then a little bit more structure if you take audio, video and so on is still very chaotic but a little bit more structure is in there than if we add some more.",
                    "label": 0
                },
                {
                    "sent": "Structure, then we would come to texts which has structure but still typically we will consider taxes, unstructured data.",
                    "label": 0
                },
                {
                    "sent": "Then we add a little bit more.",
                    "label": 0
                },
                {
                    "sent": "We would come to networks graphs.",
                    "label": 0
                },
                {
                    "sent": "So this would be semi structured.",
                    "label": 0
                },
                {
                    "sent": "So graphs are structured.",
                    "label": 0
                },
                {
                    "sent": "But again it's lots of freedom what we can represent and then structured data would be traditional databases.",
                    "label": 0
                },
                {
                    "sent": "So records with fields and so on.",
                    "label": 0
                },
                {
                    "sent": "And if we add even more structure than we would come to something which we could call ontology slightly.",
                    "label": 0
                },
                {
                    "sent": "So ontologies generally would be the most structured.",
                    "label": 0
                },
                {
                    "sent": "Data objects we typically deal with.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they are even two structured two to come too complex too.",
                    "label": 0
                },
                {
                    "sent": "To deal with them.",
                    "label": 0
                },
                {
                    "sent": "So this is the spectrum of different data modalities which you would find now for each of these data modalities we have set of operators which we try to apply to them.",
                    "label": 0
                },
                {
                    "sent": "So from very simple ones like collecting the data.",
                    "label": 0
                },
                {
                    "sent": "But in the case of text this would be crawling and so on preparing pre processing representing the data for additional for modeling.",
                    "label": 0
                },
                {
                    "sent": "So modeling themselves modeling could be either.",
                    "label": 0
                },
                {
                    "sent": "Handmaids could be automatic, semi-automatic, and so on.",
                    "label": 0
                },
                {
                    "sent": "So on then reasoning of various kinds, logic, reasoning, probabilistic reasoning, and so on.",
                    "label": 0
                },
                {
                    "sent": "And the end.",
                    "label": 0
                },
                {
                    "sent": "Let's say presentation, visualization and so on delivering results.",
                    "label": 0
                },
                {
                    "sent": "And we have third dimension which is a little bit less.",
                    "label": 0
                },
                {
                    "sent": "A little bit less.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really represent a spectrum, but there are other issues which we need to be aware of, like.",
                    "label": 0
                },
                {
                    "sent": "Like scalability, so sometimes they are operated just with a small amounts of data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we operate with huge amounts of data.",
                    "label": 0
                },
                {
                    "sent": "Today this big data movement is.",
                    "label": 0
                },
                {
                    "sent": "Popular, so this capability definitely an issue today.",
                    "label": 0
                },
                {
                    "sent": "The enemy city.",
                    "label": 0
                },
                {
                    "sent": "So data change two times so we need to be aware of this context so data themselves or data plus context.",
                    "label": 0
                },
                {
                    "sent": "So this certainly is an issue.",
                    "label": 0
                },
                {
                    "sent": "Quality of of data.",
                    "label": 0
                },
                {
                    "sent": "This is always an issue in all data related fields usage.",
                    "label": 0
                },
                {
                    "sent": "So the way how we use it and so on.",
                    "label": 1
                },
                {
                    "sent": "So there are many other issues which we try to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which we try to deal with now if we take a subset of this 3 dimensional cube, different subsets, we would get different research areas like machine learning would typically deal more like on the.",
                    "label": 0
                },
                {
                    "sent": "On the right side, right side of this dimension would typically deal more like with this middle representation and modeling and with some of these aspects.",
                    "label": 0
                },
                {
                    "sent": "So this sub sub sub cube would define let's say machine learning and data mining for the same reason.",
                    "label": 0
                },
                {
                    "sent": "Information retrieval similar would be on texts.",
                    "label": 0
                },
                {
                    "sent": "Typically, representation, modeling and maybe scale and we would get search engines.",
                    "label": 0
                },
                {
                    "sent": "Natural language again texts modeling more like representation.",
                    "label": 0
                },
                {
                    "sent": "A little bit of modeling and.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit of context and we would get NLP.",
                    "label": 0
                },
                {
                    "sent": "Semantic Web fits more like on the on that side, so structured objects maybe context quality and more like representation, reasoning, modeling.",
                    "label": 1
                },
                {
                    "sent": "So a little bit on that side.",
                    "label": 0
                },
                {
                    "sent": "So different subsets of this cube represent different research areas.",
                    "label": 1
                },
                {
                    "sent": "So today we'll touch more like this text ontologies and this middle middle side of this dimension.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we, let's say here we.",
                    "label": 0
                },
                {
                    "sent": "Mentioned set of operators.",
                    "label": 0
                },
                {
                    "sent": "So I tried to classic cluster these different approaches into three major categories.",
                    "label": 0
                },
                {
                    "sent": "How we approach?",
                    "label": 0
                },
                {
                    "sent": "How to how we approach solving different?",
                    "label": 0
                },
                {
                    "sent": "Let's say scientific are just.",
                    "label": 0
                },
                {
                    "sent": "Engineering problems typically today we would have three approaches, so top down approaches which are model driven.",
                    "label": 1
                },
                {
                    "sent": "So this is model driven approaches were more like characteristic for old times, like 80s, early 90s.",
                    "label": 0
                },
                {
                    "sent": "Why just cause capacities of computers were smaller, but there were no data available and typical paradigm was smart researcher.",
                    "label": 0
                },
                {
                    "sent": "Malcolm was observing the data and put a hypothesis hypothesis was in the form of a model and then the this model was tested in one or the other way.",
                    "label": 0
                },
                {
                    "sent": "So this is model driven.",
                    "label": 0
                },
                {
                    "sent": "So inside model and testing, so statistics has still a lot of this style of approach is also.",
                    "label": 0
                },
                {
                    "sent": "NLP.",
                    "label": 0
                },
                {
                    "sent": "And it will be vision and so on from the end of 80s was these areas were completely through.",
                    "label": 0
                },
                {
                    "sent": "And now once the data appeared, this happened more like the turn of the now in early 2000s.",
                    "label": 1
                },
                {
                    "sent": "So there was definitely the area are different areas which are more like socalled data driven or bottom up approaches appear, so machine learning, data mining.",
                    "label": 0
                },
                {
                    "sent": "And for the same reason, also information trivial and many other fields like also natural language processing, vision and so on.",
                    "label": 0
                },
                {
                    "sent": "They all turned to be way more data driven or some areas completely changed to be data driven today.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you would go to vision conferences or NLP conferences like 90% plus of papers would be about data driven.",
                    "label": 0
                },
                {
                    "sent": "That's not about model division in 80s.",
                    "label": 0
                },
                {
                    "sent": "End of 80s.",
                    "label": 0
                },
                {
                    "sent": "Let's say everything would still be modeled rule, so this is different paradigm now.",
                    "label": 0
                },
                {
                    "sent": "There's also the third one, which appeared more like in the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "So these are these collaborative approaches or socially driven.",
                    "label": 0
                },
                {
                    "sent": "So like Web 2.0 social computing where.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following.",
                    "label": 0
                },
                {
                    "sent": "We say OK, here we have a problem.",
                    "label": 0
                },
                {
                    "sent": "It's hard to solve.",
                    "label": 0
                },
                {
                    "sent": "We cannot even be.",
                    "label": 0
                },
                {
                    "sent": "We certainly we don't have methods to solve this.",
                    "label": 0
                },
                {
                    "sent": "So let's ask people.",
                    "label": 0
                },
                {
                    "sent": "But people are expensive or not too knowledgeable and this area basically deals.",
                    "label": 0
                },
                {
                    "sent": "So this would be human computation.",
                    "label": 0
                },
                {
                    "sent": "Social computing and so on.",
                    "label": 0
                },
                {
                    "sent": "Where people give small contributions and we would come pretty much with the solve problem at the end.",
                    "label": 0
                },
                {
                    "sent": "So this would include everything from things like Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "The socially driven on the ontology.",
                    "label": 0
                },
                {
                    "sent": "So taxonomies like delicious and many others so they would come from this area so.",
                    "label": 0
                },
                {
                    "sent": "Generally, each solution to a problem we could, we could typically classify in one of these three.",
                    "label": 0
                },
                {
                    "sent": "Of course, they also combinations of that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Before going to the actual.",
                    "label": 0
                },
                {
                    "sent": "Technical part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So there's this one slide which I typically show.",
                    "label": 0
                },
                {
                    "sent": "So if you talk about the text processing, so there's the several areas which research areas which deal with text.",
                    "label": 0
                },
                {
                    "sent": "It's like you know this story about this elephant and blind men, which approaches elephant.",
                    "label": 0
                },
                {
                    "sent": "Everybody touched elephant from one side.",
                    "label": 0
                },
                {
                    "sent": "So and this people came back and they were asked well how elephant looks like and some for this guy.",
                    "label": 0
                },
                {
                    "sent": "Elephant is like a rope.",
                    "label": 0
                },
                {
                    "sent": "For this guy at the front is like a spear for this guy.",
                    "label": 0
                },
                {
                    "sent": "It's like a wall so everything is right.",
                    "label": 0
                },
                {
                    "sent": "But none of them is completely correct.",
                    "label": 0
                },
                {
                    "sent": "So the same way areas dealing with texts.",
                    "label": 0
                },
                {
                    "sent": "This approach.",
                    "label": 0
                },
                {
                    "sent": "Problem how to model how to solve problems on the text.",
                    "label": 0
                },
                {
                    "sent": "So if you take computational linguistics, everything is about the language the unit of processing is typically sentence.",
                    "label": 0
                },
                {
                    "sent": "They hardly even go beyond the sentence level maybe, but document is already a big big unit for processing for them that 2.0 everything will be about community.",
                    "label": 0
                },
                {
                    "sent": "So we ask communities semantic.",
                    "label": 0
                },
                {
                    "sent": "Rap is mostly about interoperability, text mining, scale analytics.",
                    "label": 0
                },
                {
                    "sent": "So machine learning would be more like in the recent years, more like statistical approaches then would have information.",
                    "label": 0
                },
                {
                    "sent": "Three over everything is about search how to find things.",
                    "label": 0
                },
                {
                    "sent": "And maybe machine translation.",
                    "label": 0
                },
                {
                    "sent": "Also an area where everything is about mappings.",
                    "label": 0
                },
                {
                    "sent": "So if we go to the core of machine translation is about how to map one representative textual representation to other.",
                    "label": 0
                },
                {
                    "sent": "And they all have similar insights, but still different enough so that they are they living typically in separate spaces.",
                    "label": 0
                },
                {
                    "sent": "Kind of tend to go to most of these events from these areas and.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to see that something which is extremely easy and clear for, let's say, information through our machine learning guys.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear to let's say semantic guys or Web 2.0 guys.",
                    "label": 0
                },
                {
                    "sent": "So are the only discovering things which were known.",
                    "label": 0
                },
                {
                    "sent": "Let's say 15 years ago there.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is more like intro to the talk.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's go more like to this technical part.",
                    "label": 0
                },
                {
                    "sent": "How do we represent text so?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There would be, so this is 1 trial.",
                    "label": 0
                },
                {
                    "sent": "How to classify different levels of text representation?",
                    "label": 1
                },
                {
                    "sent": "I tried to put them into this.",
                    "label": 0
                },
                {
                    "sent": "So 544 so.",
                    "label": 0
                },
                {
                    "sent": "Searching levels.",
                    "label": 0
                },
                {
                    "sent": "From very simple ones to, let's say the most complex ones, and this is 1.",
                    "label": 0
                },
                {
                    "sent": "This is debatable and we could change add some things.",
                    "label": 0
                },
                {
                    "sent": "Remove some levels and so on.",
                    "label": 0
                },
                {
                    "sent": "But well, this roughly corresponds to how things are done, so this would be more like lexical levels where we we try to be aware of.",
                    "label": 0
                },
                {
                    "sent": "Only about the terms words typically, so we are not aware of the context.",
                    "label": 0
                },
                {
                    "sent": "Too much and so on.",
                    "label": 0
                },
                {
                    "sent": "So the language or the basic units from the text would be the main domain trigger for building representation, then syntactic where we have already notion of some kind of structure within a text, and then there would be set of so-called semantic levels where we try to dig a little bit more into the.",
                    "label": 0
                },
                {
                    "sent": "Uh, not really.",
                    "label": 0
                },
                {
                    "sent": "Meaning of what's being written.",
                    "label": 0
                },
                {
                    "sent": "Although there are trials, but we try to extract also the semantics so.",
                    "label": 0
                },
                {
                    "sent": "Less obvious relationships between the items which appear in the text.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "If we go just briefly through all the representations, let's say very simple.",
                    "label": 0
                },
                {
                    "sent": "So if you take a document and we just extract the characters, we don't care for words, nothing.",
                    "label": 0
                },
                {
                    "sent": "Are we able to solve some problems?",
                    "label": 0
                },
                {
                    "sent": "Yes, let's say 80s before all these texts, related areas were around.",
                    "label": 0
                },
                {
                    "sent": "Somehow people were actually using just.",
                    "label": 0
                },
                {
                    "sent": "Simple character representations and they were able to solve a couple of problems like language identification, copy detection to find whether somebody is copying.",
                    "label": 0
                },
                {
                    "sent": "Homework so you don't need much you need just this very simple representation and it's enough language identification.",
                    "label": 0
                },
                {
                    "sent": "How to identify the language out of the out of the document.",
                    "label": 0
                },
                {
                    "sent": "The language document is being written in, then this is enough.",
                    "label": 0
                },
                {
                    "sent": "Then if you add a little bit more so, then we have words.",
                    "label": 0
                },
                {
                    "sent": "Phrases or words would be just words themselves, phrases, sequences of words, then part of speech very adds some, let's say category supports.",
                    "label": 0
                },
                {
                    "sent": "So at this level we can already do, let's say name, entity extraction.",
                    "label": 0
                },
                {
                    "sent": "This is one of important industrial segments dealing with texts.",
                    "label": 0
                },
                {
                    "sent": "Then if we had a little bit more so we would come to this taxonomy so it is Aris.",
                    "label": 0
                },
                {
                    "sent": "However, we have clusters of words and so on, and we know a little bit more about things, but still it's just about the lexical level.",
                    "label": 0
                },
                {
                    "sent": "Then if we switch to this something which we could call as a syntactic level.",
                    "label": 0
                },
                {
                    "sent": "So we come to this extremely or probably the most important representation nowadays on the market vector space model.",
                    "label": 0
                },
                {
                    "sent": "So here we can do text categorization, clustering, search summarization and so on.",
                    "label": 0
                },
                {
                    "sent": "So things happen here.",
                    "label": 0
                },
                {
                    "sent": "Touch this a little bit more in detail afterwards.",
                    "label": 0
                },
                {
                    "sent": "So let's say Google works on this level.",
                    "label": 0
                },
                {
                    "sent": "All search engines use this representation.",
                    "label": 0
                },
                {
                    "sent": "La.",
                    "label": 0
                },
                {
                    "sent": "Then if you go a little bit further yet, language models a little bit more structuring to the representation, then let's say machine translation typically would operate here language models.",
                    "label": 1
                },
                {
                    "sent": "So where we are aware of sequences of words, spam filtering typically would use this as well.",
                    "label": 0
                },
                {
                    "sent": "Then we have parsing across modalities.",
                    "label": 0
                },
                {
                    "sent": "An interesting representation how to how to match problems like multilingual Search.",
                    "label": 0
                },
                {
                    "sent": "So shifting text with images or with other data modalities, or this would be a little bit extended representation from vector space model.",
                    "label": 0
                },
                {
                    "sent": "Then if you go to.",
                    "label": 0
                },
                {
                    "sent": "Semantic levels, let's say collaborative tagging Web 2.0, so this is more like unifying semantics of data.",
                    "label": 1
                },
                {
                    "sent": "This is one way how we can approach us, so this is more like collaborative approach and link data.",
                    "label": 1
                },
                {
                    "sent": "OK, you will hear more on other talks on linked data and it's more like about data integration.",
                    "label": 0
                },
                {
                    "sent": "And so if we come down to the bottom then we can talk about reasoning semantic search this style of problems.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Extremely hard, but can be also very beneficial if you do it in the right way.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 trial.",
                    "label": 0
                },
                {
                    "sent": "How to go through different representations?",
                    "label": 0
                },
                {
                    "sent": "And what sort of problems are?",
                    "label": 0
                },
                {
                    "sent": "Possible to solve on this?",
                    "label": 0
                },
                {
                    "sent": "On each of them now in the rest of the talk will go from the top to the bottom.",
                    "label": 0
                },
                {
                    "sent": "And touch not really all of them, but some of them and.",
                    "label": 0
                },
                {
                    "sent": "Try to see some issues related and then are there any questions here?",
                    "label": 0
                },
                {
                    "sent": "OK then let's go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue, so let's first to have this characters.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said before that character representation, although it's very simple.",
                    "label": 0
                },
                {
                    "sent": "Still, it's quite powerful in many ways, at least for some problems.",
                    "label": 0
                },
                {
                    "sent": "So typically, let's say 1 one way how we can represent text?",
                    "label": 0
                },
                {
                    "sent": "So if we have a quick rat so we just make let's say 2, three in this case 3 grams.",
                    "label": 0
                },
                {
                    "sent": "So all sequences of three subsequent word letters, nothing else.",
                    "label": 0
                },
                {
                    "sent": "And this is then put into either machine learning or something else.",
                    "label": 0
                },
                {
                    "sent": "And what we can do with this?",
                    "label": 0
                },
                {
                    "sent": "With this representation we can, let's say, detects languages.",
                    "label": 0
                },
                {
                    "sent": "We can detect copies.",
                    "label": 0
                },
                {
                    "sent": "We can even do classification to some degree and so on.",
                    "label": 0
                },
                {
                    "sent": "So let's say.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good and bad sides.",
                    "label": 0
                },
                {
                    "sent": "1st, it's extremely robust, so it captures simple patterns in the text and it's suitable for lots of analytical tasks because it's just set of flat features, nothing else.",
                    "label": 0
                },
                {
                    "sent": "But for any deeper, deeper semantic tasks.",
                    "label": 0
                },
                {
                    "sent": "Of course this wouldn't work.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of this language identification.",
                    "label": 0
                },
                {
                    "sent": "LA.",
                    "label": 0
                },
                {
                    "sent": "So if you take a document.",
                    "label": 0
                },
                {
                    "sent": "Just count the.",
                    "label": 0
                },
                {
                    "sent": "Percentage of particular engrams are two grams of letters.",
                    "label": 0
                },
                {
                    "sent": "We get sort of signature for each language, so this is signature for English language.",
                    "label": 0
                },
                {
                    "sent": "So if the distribution of N grams to grams bigrams in this case would be close to this distribution, then this would be English.",
                    "label": 0
                },
                {
                    "sent": "If, say, German language would have another typical.",
                    "label": 0
                },
                {
                    "sent": "The spectrum of this distribution of bigrams, and so on, and basically this is this is the whole thing.",
                    "label": 0
                },
                {
                    "sent": "So here we have one simple demo.",
                    "label": 0
                },
                {
                    "sent": "Maybe it works.",
                    "label": 0
                },
                {
                    "sent": "It's not ours.",
                    "label": 0
                },
                {
                    "sent": "Disclaimer or.",
                    "label": 0
                },
                {
                    "sent": "Just as long as you have running.",
                    "label": 0
                },
                {
                    "sent": "The saying you can do that with a big grams.",
                    "label": 0
                },
                {
                    "sent": "You were mentioning tree gum support.",
                    "label": 0
                },
                {
                    "sent": "Is there any?",
                    "label": 0
                },
                {
                    "sent": "Evaluation, like what end in N Grams, works best for what task like?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Typically this the end would relate to the amount of data you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "So if you go to I don't know more.",
                    "label": 0
                },
                {
                    "sent": "Let's say four 5 grams.",
                    "label": 0
                },
                {
                    "sent": "Then of course everything would be nice except you you have lots of so think 1 gram.",
                    "label": 0
                },
                {
                    "sent": "So single letters you will have 25 by grams.",
                    "label": 0
                },
                {
                    "sent": "You would have all of them are 625, three grams times 25 so.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have statistically significant.",
                    "label": 0
                },
                {
                    "sent": "Accounts so that you can compare.",
                    "label": 0
                },
                {
                    "sent": "Then you run into this so called sparsity problem.",
                    "label": 0
                },
                {
                    "sent": "So that's why you try to keep this end as low as possible, but still not too low, so single letters are typically not enough.",
                    "label": 0
                },
                {
                    "sent": "I mean they each language has some kind of distribution of over single letters, but not not typically by grounds would be best.",
                    "label": 0
                },
                {
                    "sent": "Let's especially if you want to extract bigrams from an average document, then with three grams you would.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't get proper proper distribution, so so it's typically it's related amounts of data.",
                    "label": 0
                },
                {
                    "sent": "If you have lots of data, then you can afford hyeran.",
                    "label": 0
                },
                {
                    "sent": "Character.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I said so you can.",
                    "label": 0
                },
                {
                    "sent": "You can do everything pretty much from from classific classification, clustering.",
                    "label": 0
                },
                {
                    "sent": "Everything is just because expressive power is so so low in this presentation that things wouldn't work well.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we go back to this example here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You are saying that actually even things like topic detection.",
                    "label": 0
                },
                {
                    "sent": "Sure sure you just get out of this you these 3 grams or 4 grams.",
                    "label": 0
                },
                {
                    "sent": "Whatever are so ambiguous it wouldn't capture it's much.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you go to the word level of phrase level, you capture way more way more.",
                    "label": 0
                },
                {
                    "sent": "Information out of, let's say this proper.",
                    "label": 0
                },
                {
                    "sent": "Proper features which are, let's say phrases or words then from from these three 3 grams which or you need a lot of data to approach the performance of this stronger representation.",
                    "label": 0
                },
                {
                    "sent": "So this would be generally the balance, but it's very nice because it's extremely simple, so it's well language independent and has lots of nice features.",
                    "label": 0
                },
                {
                    "sent": "But let's say I remember in 80s one side was approached by police or some defense or spice.",
                    "label": 0
                },
                {
                    "sent": "Whatever they were using this because nothing else was available out there on the market at that time.",
                    "label": 0
                },
                {
                    "sent": "So I was impressed.",
                    "label": 0
                },
                {
                    "sent": "I mean, this actually worked for them, maybe didn't country collapsed afterwards.",
                    "label": 0
                },
                {
                    "sent": "OK, this demo since it's not ours doesn't work so.",
                    "label": 0
                },
                {
                    "sent": "Misguided this type of language and edification demos you paste in a document and tells you the probability for particular language.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, maybe.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just one more.",
                    "label": 0
                },
                {
                    "sent": "You sure about the characters when you when we deal on the level of characters, everything looks like similar, but sometimes things are not that easy, so.",
                    "label": 0
                },
                {
                    "sent": "Different characters can be written in different ways, less if we go to Unicode.",
                    "label": 0
                },
                {
                    "sent": "So we need to be careful also about this kind of simple things.",
                    "label": 0
                },
                {
                    "sent": "So this character can be written at least in these two ways, or sometimes they are even more so, so this normal is a character level normalization, some sometimes it's not.",
                    "label": 0
                },
                {
                    "sent": "Easy, so if we leave in seven bit ASCII then life is very easy, but we don't usually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's go now to.",
                    "label": 0
                },
                {
                    "sent": "You can just check the time.",
                    "label": 0
                },
                {
                    "sent": "I have 45 minutes only so will skip some of the things.",
                    "label": 0
                },
                {
                    "sent": "Otherwise this is more like 3 hour tutorial altogether.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "Words are typically the most common representation.",
                    "label": 0
                },
                {
                    "sent": "When we saw it's versus the unit of text in a way so it's very natural somehow to deal with words.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when dealing with words, we need to be aware of this four properties of words which which make life much harder.",
                    "label": 0
                },
                {
                    "sent": "So if each word would be uniquely defining some kind of sense or meaning would be much easier.",
                    "label": 0
                },
                {
                    "sent": "But let's say we have.",
                    "label": 0
                },
                {
                    "sent": "We deal with this hanami where we have the same form of the word but different meanings or like banquet meaning River bank or financial institution or this policy me where same form related meaning synonymy where we have different forms and same meaning.",
                    "label": 0
                },
                {
                    "sent": "And hyponymy, which one word denotes a subclass of the other one?",
                    "label": 0
                },
                {
                    "sent": "So when dealing good words, so in a way with the methods which operate on text somehow need to compensate this.",
                    "label": 0
                },
                {
                    "sent": "This for features which are all four are sort of anomalies.",
                    "label": 0
                },
                {
                    "sent": "So these are the bets sides but easier site of dealing with taxes.",
                    "label": 0
                },
                {
                    "sent": "Is that word frequencies in text have this power law distribution?",
                    "label": 0
                },
                {
                    "sent": "This is maybe not quite obvious, but why dealing with taxes?",
                    "label": 0
                },
                {
                    "sent": "Easy because of this single feature, so there's so much redundancy in text, so that even if we remove lots of information out of out of the.",
                    "label": 0
                },
                {
                    "sent": "The raw text representation still thinks working things work because of this power law distribution, so I won't go into detail, but this is.",
                    "label": 0
                },
                {
                    "sent": "This is one of the very fundamental features dealing with text.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, maybe just to add couple of details so software this is something which one needs to be aware.",
                    "label": 0
                },
                {
                    "sent": "So for simple text processing we typically remove the functional words which don't have lots of meaning and typically with removing this stop words.",
                    "label": 0
                },
                {
                    "sent": "So this would be English, Dutch, Slovenian.",
                    "label": 0
                },
                {
                    "sent": "Each language has specific one specific set with our help algorithms out not to come into.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What troubles?",
                    "label": 0
                },
                {
                    "sent": "Related to this, we have stemming.",
                    "label": 0
                },
                {
                    "sent": "Can Lemmatization, stemming is a very simple procedure which tries to normalize or put the words in some kind of Canonical form.",
                    "label": 0
                },
                {
                    "sent": "Stemming is really just takes the stem the stem of the word tries and makes lots of mistakes.",
                    "label": 0
                },
                {
                    "sent": "But it's very cheap cheap procedure to do it.",
                    "label": 0
                },
                {
                    "sent": "So let's say this would be better.",
                    "label": 0
                },
                {
                    "sent": "Better example Universe, University, University, universal.",
                    "label": 0
                },
                {
                    "sent": "Everything would be translated into universe which is for practical persons.",
                    "label": 0
                },
                {
                    "sent": "Purposes in many ways, in many most cases actually enough little bit better, but a little bit harder procedure is limitation where we try to really put the word in a proper normalized form like universe would be universe, University, universities and so on would be University universal would be universal, so this is the difference.",
                    "label": 0
                },
                {
                    "sent": "So this is something which is typically underlying the if you go to Google and type in the word in this inflected form first thing what would happen would actually.",
                    "label": 0
                },
                {
                    "sent": "The world would be normalized in one of these ways, and then it would go to section.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call stemming work so This is why stemming is so inviting because you go to this website and take the procedure down.",
                    "label": 0
                },
                {
                    "sent": "So it's so called Porter Stemmer.",
                    "label": 0
                },
                {
                    "sent": "Whichever is using a set of cascade rules which transform words like these are the rules.",
                    "label": 0
                },
                {
                    "sent": "So if the word ends with national, then transform it this suffix into eight so relational relate conditional condition.",
                    "label": 0
                },
                {
                    "sent": "So instead of this set of rules which are.",
                    "label": 0
                },
                {
                    "sent": "Hand mate, some other work, sort of well or well enough so that everybody is using it.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's go to the next level phrases phrase.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would be 16 sequences of words which have a little bit stronger, stronger meaning when they are together, so like.",
                    "label": 0
                },
                {
                    "sent": "Artificial intelligence or text mining or word for Windows.",
                    "label": 0
                },
                {
                    "sent": "So word for Windows has a meaning.",
                    "label": 0
                },
                {
                    "sent": "It's very clear what it is.",
                    "label": 0
                },
                {
                    "sent": "Is a piece of software and so on.",
                    "label": 0
                },
                {
                    "sent": "We have instead, if we had just words and four and Windows, we would need quite some data to identify this as that this actually should be somehow together and statistics or some methods would would would detect this relationship between these three words.",
                    "label": 0
                },
                {
                    "sent": "So if you had the algorithms come out.",
                    "label": 0
                },
                {
                    "sent": "To tell them that, well, this is the phrase which you should use, then of course.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's now.",
                    "label": 0
                },
                {
                    "sent": "We typically get better results.",
                    "label": 0
                },
                {
                    "sent": "OK, I won't go too much into the details, but I.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you just this data set which is available.",
                    "label": 0
                },
                {
                    "sent": "So this is this Google Ngram corpus which was released in 2006.",
                    "label": 0
                },
                {
                    "sent": "Which body I think everybody can still download, I hope.",
                    "label": 0
                },
                {
                    "sent": "So in the end, So what give Google gives you it's 24 gigabytes of compressed text files, which altogether it has.",
                    "label": 0
                },
                {
                    "sent": "A lot of zeros.",
                    "label": 0
                },
                {
                    "sent": "Millions of trillion.",
                    "label": 0
                },
                {
                    "sent": "Now we learn all these big numbers because of the crisis.",
                    "label": 0
                },
                {
                    "sent": "Three and three and tokens, so which were extracted out of 95 billion sentences.",
                    "label": 0
                },
                {
                    "sent": "So this would.",
                    "label": 0
                },
                {
                    "sent": "This is pretty much from the English part of the web.",
                    "label": 0
                },
                {
                    "sent": "These are all engrams of size 1234 and five which appear which appeared in 2006 more than 40 times on the web.",
                    "label": 0
                },
                {
                    "sent": "So if you would look at uni Grams so single words, this would be a little bit roughly 13 million of them.",
                    "label": 0
                },
                {
                    "sent": "And if you go to let's say four 5 grams, it's a little bit more than billion.",
                    "label": 0
                },
                {
                    "sent": "So this typically.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "N grams look like this so it's just flat files with the words and the frequency behind.",
                    "label": 0
                },
                {
                    "sent": "So this is I would say.",
                    "label": 0
                },
                {
                    "sent": "Extremely valuable resource which is under exploited by research community in many ways.",
                    "label": 0
                },
                {
                    "sent": "Typically you would.",
                    "label": 0
                },
                {
                    "sent": "You would have everything but what's relevant in the text or also what was being used in the text.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, part of speech will just briefly touch a bit so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of speech text.",
                    "label": 0
                },
                {
                    "sent": "So this is when.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be I will show.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're just an example.",
                    "label": 0
                },
                {
                    "sent": "If we have a text so part of speech tagging means that we add to each words it's linguistic rules.",
                    "label": 0
                },
                {
                    "sent": "So and once we have this sort of semi meta data more like linguistic metadata, shallow one.",
                    "label": 0
                },
                {
                    "sent": "Then we can put different patterns on top of this and with this level we can we can do let's in entity extraction and so on.",
                    "label": 0
                },
                {
                    "sent": "Much easier.",
                    "label": 0
                },
                {
                    "sent": "So noun phrases would be very easily extra identified and then plus a couple of patterns we would get name entity extractor on this level already.",
                    "label": 0
                },
                {
                    "sent": "And typically you find for all major languages part of speech taggers.",
                    "label": 0
                },
                {
                    "sent": "This is something which is available today.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK taxonomies.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me suggest you all know roughly so the most known one is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Word net, which may be few words about word net.",
                    "label": 0
                },
                {
                    "sent": "So it's a graph.",
                    "label": 0
                },
                {
                    "sent": "Basically it consists from 4 databases, database of nouns, verbs, adjectives and adverbs.",
                    "label": 0
                },
                {
                    "sent": "So let's say we would have 94,000 unique forms and 160,000 census.",
                    "label": 0
                },
                {
                    "sent": "So because each words can appear in several senses, so it's that's why the difference.",
                    "label": 0
                },
                {
                    "sent": "And at the end the.",
                    "label": 0
                },
                {
                    "sent": "I don't have a picture.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Basically, the each note of this graph.",
                    "label": 0
                },
                {
                    "sent": "Each note of this graph is a set of synonyms, so words which mean pretty much the same.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples would be like musician, instrumentalist player, so this would be.",
                    "label": 0
                },
                {
                    "sent": "In one note of the graph or person, individual someone or lifeform Organism being so all the surface forms which mean the same they are in one node of the graph.",
                    "label": 0
                },
                {
                    "sent": "And now this no.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the graph are connected with the relationships and we have something like 16 relationships out of which this would be probably most important, like hyper name, let's say from lower to higher concepts.",
                    "label": 0
                },
                {
                    "sent": "Breakfast meal would be connected like this or hypernym.",
                    "label": 0
                },
                {
                    "sent": "From concepts of solving it's so me lunch or has member from groups to their members, faculty and professor.",
                    "label": 0
                },
                {
                    "sent": "Part of so from our course meal, or Anthony opposites like Leader follower.",
                    "label": 0
                },
                {
                    "sent": "So words are senses of words.",
                    "label": 0
                },
                {
                    "sent": "I connected with these relationships and obviously we can exploit this mostly.",
                    "label": 0
                },
                {
                    "sent": "If you would go through the.",
                    "label": 0
                },
                {
                    "sent": "Sets of research papers ever is trying to use only this hypernym, so other relationships are less exploited.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we went through this lexical representation.",
                    "label": 0
                },
                {
                    "sent": "Now let's say switch to this syntactic ones first will stop with this vector space model, which is way the most important representation on this level.",
                    "label": 0
                },
                {
                    "sent": "And here we have also a couple of demos.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we do here?",
                    "label": 0
                },
                {
                    "sent": "This is more like analytic view, so we take the.",
                    "label": 0
                },
                {
                    "sent": "We take the document, we transform them the document into sparse numeric vectors, and then we perform all kinds of linear algebra operations on it.",
                    "label": 0
                },
                {
                    "sent": "So this is this vector space model, so if you look at search which make it Google or any other search engine is actually it's simple.",
                    "label": 0
                },
                {
                    "sent": "Dot product from linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Of course it's done in optimized way, but it's still it's a dot product product, nothing else.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See what we do actually.",
                    "label": 0
                },
                {
                    "sent": "So we take a document, we just take individual words or phrases.",
                    "label": 0
                },
                {
                    "sent": "And construct a vector vector of frequencies.",
                    "label": 0
                },
                {
                    "sent": "Nothing that's now this vector.",
                    "label": 0
                },
                {
                    "sent": "Then we can post, process it and so on.",
                    "label": 0
                },
                {
                    "sent": "But basically all this mathematical machinery operates on these vectors and we can do classification, clustering, search, comparison, everything functions them on this vector.",
                    "label": 1
                },
                {
                    "sent": "So this is this is it?",
                    "label": 1
                },
                {
                    "sent": "Why it's nice?",
                    "label": 0
                },
                {
                    "sent": "Becausw, well, mathematicians are this American mathematicians developed all this fancy methods to deal with vectors and matrices and all the machinery, then usable for the.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This representation.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's worth.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mentioning so typically we don't operate on.",
                    "label": 0
                },
                {
                    "sent": "Vectors of frequencies.",
                    "label": 0
                },
                {
                    "sent": "But we transform these frequencies into weights WHI because, let's say words like.",
                    "label": 0
                },
                {
                    "sent": "Let's say the and all these frequent frequent words typically would appear extremely too many Times Now we need some kind of weighting of importance for the words and this weighting scheme is called.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why the F?",
                    "label": 0
                },
                {
                    "sent": "Which basically has just two factors.",
                    "label": 0
                },
                {
                    "sent": "So the word is more important if it appears several times in the target document and the word is more important if it appears in less documents.",
                    "label": 1
                },
                {
                    "sent": "So it has these two bits and with these two factors we try to express this.",
                    "label": 0
                },
                {
                    "sent": "So basically intuition is if if if a word appears all over all the documents many times, then we push it down.",
                    "label": 0
                },
                {
                    "sent": "If it appears in a concentrated way then we push it up.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This weighting basically gives you.",
                    "label": 0
                },
                {
                    "sent": "An exam.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "Soon article about Donald Trump.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So it's just the document.",
                    "label": 0
                },
                {
                    "sent": "And then when we transform it into this bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "And this is, I think, sorted according to the wait.",
                    "label": 0
                },
                {
                    "sent": "You see that the words like resorts class Trump voting states come up.",
                    "label": 0
                },
                {
                    "sent": "And this is this simple formula which doesn't know anything about the context.",
                    "label": 0
                },
                {
                    "sent": "And already we this could be actually out of this.",
                    "label": 0
                },
                {
                    "sent": "We typically generate clouds, so tech clouds are just a visualization of these vectors, nothing else.",
                    "label": 0
                },
                {
                    "sent": "So foremost, foremost.",
                    "label": 0
                },
                {
                    "sent": "Vector space operations we use.",
                    "label": 0
                },
                {
                    "sent": "Let's say this weighting schema.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There was extensive extend to extensive research in 90s.",
                    "label": 0
                },
                {
                    "sent": "Mostly what would be the magic formula here and they tried everything possible.",
                    "label": 0
                },
                {
                    "sent": "You have papers with hundreds and hundreds of different formulas, but at the end is simple simple thing somehow still worked with major best.",
                    "label": 0
                },
                {
                    "sent": "Nowadays, we can artificially generate this optimal weighting scheme As for particular needs, but it takes a little bit more little bit more time.",
                    "label": 0
                },
                {
                    "sent": "Are typically we need a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our extra information as well so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is 1 very basic operation, which again.",
                    "label": 0
                },
                {
                    "sent": "I'll try to explain it in simple way so if we have to document, we try to compare them.",
                    "label": 0
                },
                {
                    "sent": "Are they similar or not?",
                    "label": 0
                },
                {
                    "sent": "So and basically what we do.",
                    "label": 0
                },
                {
                    "sent": "Since every document becomes a vector.",
                    "label": 0
                },
                {
                    "sent": "So now this is two dimension space.",
                    "label": 0
                },
                {
                    "sent": "First we have first document which is vector and the second document which is vector.",
                    "label": 0
                },
                {
                    "sent": "And then we calculate the cosine.",
                    "label": 0
                },
                {
                    "sent": "Simply 'cause I'm of this angle so.",
                    "label": 0
                },
                {
                    "sent": "If the document is almost the same, so the angle is small, then we get one causing and if the documents are completely orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Well then it's zero, so that's the whole intuition.",
                    "label": 0
                },
                {
                    "sent": "So this is this dot product.",
                    "label": 0
                },
                {
                    "sent": "With this with the similarity measure, then we can perform search like search engines.",
                    "label": 0
                },
                {
                    "sent": "Typically, if you wouldn't go to almost any.",
                    "label": 0
                },
                {
                    "sent": "Serious Database which supports also text, text blobs and they would have TF IDF and this.",
                    "label": 0
                },
                {
                    "sent": "Causing similarity.",
                    "label": 0
                },
                {
                    "sent": "So basically you get search for free so search engines generally would work quite well.",
                    "label": 0
                },
                {
                    "sent": "If they would use just TF IDF vectors and causing similarity, you don't need much.",
                    "label": 0
                },
                {
                    "sent": "And this is mathematics which you already learned in high school, so it's nothing special.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Let's go to document categories categorization.",
                    "label": 0
                },
                {
                    "sent": "Now what is document categorization?",
                    "label": 0
                },
                {
                    "sent": "So if we have set of documents labeled with Doc content categories, the goal is to build a model statistical model which would automatically assign the right content categories or labels to new unlabeled documents.",
                    "label": 0
                },
                {
                    "sent": "Content this content categories can be their unstructured.",
                    "label": 0
                },
                {
                    "sent": "Let's say Brotis corpus is very famous by this so 120 flat categories or structured.",
                    "label": 0
                },
                {
                    "sent": "So let's say Yahoo or Dimas or Medline taxonomies which so this problem is a little bit harder.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for this document categorization, we have lots of methods, so lots of this text mining and machining efforts at the end of 90s in beginning of 2000s where dealing on how to do text classification by many different ways.",
                    "label": 0
                },
                {
                    "sent": "So before I show this support support vector machines which today people mostly use, I mean there are the newer algorithms as well CRF's and so on.",
                    "label": 0
                },
                {
                    "sent": "Logistics regression is very popular as well.",
                    "label": 0
                },
                {
                    "sent": "Perceptron I will show.",
                    "label": 0
                },
                {
                    "sent": "So this is works as good as this support vector machine, you just 10 lines of code is very small Navy base and others.",
                    "label": 0
                },
                {
                    "sent": "So they're just set of methods.",
                    "label": 0
                },
                {
                    "sent": "We won't go too much into the details here.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would just show this perceptron algorithm.",
                    "label": 1
                },
                {
                    "sent": "I won't go into details, but this is the whole code.",
                    "label": 0
                },
                {
                    "sent": "Together with the comments.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "If you understand what this few lines of codes work, then you can do most of machine learning nowadays.",
                    "label": 0
                },
                {
                    "sent": "If you deliver the proper feature features into the.",
                    "label": 0
                },
                {
                    "sent": "Into the system.",
                    "label": 0
                },
                {
                    "sent": "Features we get with either engrams, words, phrases and so on.",
                    "label": 0
                },
                {
                    "sent": "As we talked before.",
                    "label": 0
                },
                {
                    "sent": "So I don't have any buffer proper visualization of how this works, but basically just pushing one hyperplane here and there and somehow algorithm has this nice property that it converges very fast and you get the proper model.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'll skip this evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we mention document categorization and the second task which is very important is also document clustering.",
                    "label": 1
                },
                {
                    "sent": "So here we operate with the documents which don't have labels and we would try to make groups groups of documents which are kind of similar.",
                    "label": 0
                },
                {
                    "sent": "So here we will have this so famous came in Salgo rhythm.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of more, but K means is mostly used in practice, which.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the whole the whole idea, so again I won't go into the code, but it's extremely simple.",
                    "label": 0
                },
                {
                    "sent": "All together would be, let's say maybe 1020 lines of code.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's now this one visualization.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's less.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's keep letting semantic indexing, which is just another way of doing clustering.",
                    "label": 0
                },
                {
                    "sent": "And let's go now to a couple of demos on for classification.",
                    "label": 0
                }
            ]
        }
    }
}