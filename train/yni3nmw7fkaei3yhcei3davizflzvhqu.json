{
    "id": "yni3nmw7fkaei3yhcei3davizflzvhqu",
    "title": "Text Mining and Link Analysis for Web and Semantic Web",
    "info": {
        "coauthor": [
            "Bla\u017e Fortuna, Jo\u017eef Stefan Institute"
        ],
        "author": [
            "Marko Grobelnik, Jo\u017eef Stefan Institute",
            "Dunja Mladeni\u0107, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Aug. 12, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Web Mining->Link Analysis",
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/kdd07_grobelnik_tmala/",
    "segmentation": [
        [
            "OK, hello everybody.",
            "So my name is Marco Marco Bernick.",
            "My quarter is do not injure she she's unfortunately.",
            "She didn't come to the conference, so I will present the whole tutorial.",
            "So this tutorial is about basically about 3 topics.",
            "It's about text mining.",
            "It's about link analysis and semantic web in a way we can say that text mining can link analysis serve.",
            "Well, techniques and to techniques and applications on web and semantic web.",
            "So this would be kind of relationship and will also try to present the whole thing in such a way."
        ],
        [
            "OK.",
            "So I said so.",
            "Basically, text mining would be more or less the first part of the tutorial and here will try to answer at least the most important questions on how to deal with textual data and will try to discuss this different levels of representation techniques and so on, which we usually use.",
            "Next topic, which will be slightly shorter is link analysis.",
            "So how to analyze graphs and link analysis somehow related to text as well?",
            "Because they're sharing similar statistical properties, so we'll touch it a little bit.",
            "More in brief.",
            "And the last part will be on semantic Web, so semantic web is topic which attracted quite some attention in the last couple of years, especially Web 2.0, which is even more popular popular recently and will try to relate this with text mining.",
            "So it's machine learning site of the analysis on one side and on the other side.",
            "With more traditional artificial intelligence techniques from 80s, which I think are still quite valid for nowadays.",
            "OK, so."
        ],
        [
            "This would be three major topics."
        ],
        [
            "Now starting with textmining.",
            "So first very short question, why do we analysis analyze text?",
            "Basically there is ultimate goal.",
            "Why we do anything with taxes?",
            "Basically at the end to understand the textual content.",
            "But this is so difficult problem to understand the text in a in a similar various people do, so we have number of easier subtasks which are still useful and in some situations they are quite relevant even for commercial or non commercial use.",
            "So we can solve this easier problems quite easily, but certainly we cannot solve the.",
            "His ultimate goal, but well, hopefully we are approaching this at the end of this tutorial.",
            "Will see a couple of examples on.",
            "How, let's say or what would be state of the art in understanding text?"
        ],
        [
            "Oh first, maybe at the beginning kind of definition of text mining.",
            "This is just adapted definition from Osama fired.",
            "So we can say that text mining could be finding interesting circularities in large textual data sets where this interesting means non trivial hidden previously unknown.",
            "An certainly useful or hopefully useful.",
            "And other one other possible definition would be finding semantic and abstract information from surface form of textual data.",
            "This would be also maybe even more accurate actually for text."
        ],
        [
            "Finding now how two sides of the coin why?",
            "On one side dealing with taxes stuff?",
            "So dealing with Texas stuff specially becausw the concepts we're dealing with are quite abstract and quite difficult to represent as well so.",
            "We have literally countless combinations, number of combinations of really abstract concepts which are kind of interrelated, so this is usually the text.",
            "Apart from this we have problems with synonymity like especially Flying Sorcerer UFO.",
            "So they all mean the same but somehow for machine on the surface level, certainly they are different.",
            "Then concepts are quite different, too difficult to visualize.",
            "Then we deal with high dimensionality, which means that all statistical techniques might have problems and so on.",
            "And also we deal with very high number of features.",
            "So hundreds of thousands, so it's not so unusual.",
            "So all these properties make the whole problem."
        ],
        [
            "Quite difficult on the other side.",
            "Text has one feature which is very pleasant or friendly for analytic methods, and this is the fact that the data is highly redundant and basically all practically all methods count on this single property, which makes actually.",
            "So easy for everybody to experiment with very simple ideas.",
            "Also in text and most likely everybody will get some kind of result and.",
            "Very simple recipe for kind of ad hoc text.",
            "Magic method would be just take out some phrases out of the text, find some kind of related words and create some kind of summary out of it.",
            "And this most likely will work and a lot of these text mining papers would would more or less."
        ],
        [
            "Follow this recipe.",
            "OK, now let's put the text mining in a context.",
            "So who in the scientific area actually works in this text analysis arena?",
            "So on one side?",
            "It's machine learning context mining, so this would be this community of this KDD conference and may be icle seminar conferences for machine learning part on the other side.",
            "We have this natural language processing so this would be linguists so there's a conference is like ACL and so on so they're more interested in linguistic aspects of text.",
            "Then in the last decade, very popular field information theory.",
            "Also, this area is more less concerned with search and related problems and recently so semantic web and even more recently Web 2.0 which actually also deal with text quite a lot.",
            "Although they have also other problems as well.",
            "So this is roughly the landscape of the areas or communities which deal with text and what's interesting.",
            "Some of these communities have almost no overlap, to be honest.",
            "Especially semantic web and machine learning.",
            "Overlap is almost zero, while with this other fields there's a little bit more.",
            "So this would be the typical tasks each of these communities does.",
            "So data analysis, computational linguistics, reason, knowledge representation and reasoning, and search and databases for."
        ],
        [
            "Information through.",
            "OK, so if you take text analytics.",
            "And try to decompose.",
            "The main.",
            "Areas of work are the most relevant concepts which we're dealing with when dealing with text, so we can say that first we need to be aware of what kind of representations we use for representing text, so this would be.",
            "This would range from character level or presentations.",
            "Very simple ones on one side and on the other side to 1st order theories.",
            "So in the next slides will go through all these levels.",
            "Next techniques.",
            "How how we deal with the with the text.",
            "So on one side we have manual work.",
            "Still a lot of manual work actually is being done on text.",
            "Then learning could be sort of intermediate step in the most difficult one would be reasoning with text or with the content which which we extract from text and the tasks.",
            "So this would be the actual algorithms are applications at the end which we have.",
            "So this would be search very popular task.",
            "Then all kinds of learning, unsupervised, semi supervised and supervised learning visualization, summarization, machine translation and so on.",
            "So in the second part of this text mining part will will deal with that."
        ],
        [
            "So what's interesting is that these three main group groups of concepts which are used for dealing with text.",
            "Somehow they all contribute.",
            "Are they all share ideas, intuitions, methods, even the data, datasets, and so on?",
            "And all these communities are kind of fat with these ideas, although as I said, so they often they just don't communicate or.",
            "Somehow people are too busy and they don't share ideas on the social level, but technically they are quite similar.",
            "So in a way, we can say that up here is politics and down here is scientific work."
        ],
        [
            "What's?",
            "May be interesting to mention, so there's a new initiative by mainly by.",
            "W3C people somehow to establish this so called websites, and this is the architecture of this website, and so we can see the details here on this site.",
            "So usually, let's say our community KDD community with appear somewhere here in AI and computer science while everything else is around this from media law economics on one side and the other side, we have mathematics, ecology and so all these contextual.",
            "Usciences, which provide the context for websites.",
            "I know this is relatively new, so that's why I cannot say much more."
        ],
        [
            "Next, we'll discuss how do we represent text?"
        ],
        [
            "We have at least three levels of representations.",
            "So we'll discuss each of each of them a little bit more in detail, but let's say just let's make quick walk over all of them.",
            "So on one side we have the simplest one, so this would be character level.",
            "Then from characters we compose words we can deal with words, then phrases, and part of speech, text and taxonomies, like word, net, and so on.",
            "So this would be kind of lexical.",
            "Type of representation next.",
            "Next we have vector space model language models.",
            "Parsing and cross modality.",
            "So I put this representations under this so called syntactic syntactic level and they are the most used actually text mining.",
            "So specially vector space model would be.",
            "Most often useless in information three will text learning and so on, and the semantic representations would be, let's say collaborative tagging.",
            "So somehow Web 2.0 would be hidden somewhere here.",
            "Templates frame, so popular representation and well, the most complex ones ontologies and 1st order theories.",
            "So.",
            "This'll be roughly semantic so or collaborative tagging.",
            "It's hard to say whether this is really semantics or not, but somehow it's closer to semantics.",
            "Then let's say to just pure syntactic level.",
            "Ha."
        ],
        [
            "OK, now let's go first to this."
        ],
        [
            "Character level.",
            "So character level means simply just taking a document, taking the text, and split it on a characters and maybe use just sequences of two 3 characters, and that's it.",
            "And out of these sequences of characters we constructed constructive vectors of frequencies, and this is then used by learning and cause we said that text has this nice property.",
            "It has a lot of redundant data.",
            "It's a lot of redundancy in the information, so it's."
        ],
        [
            "Likely that will be able to solve some of the problems, so this representation has a couple of very important strengths.",
            "First, it's very robust 'cause it avoids language morphology.",
            "And especially this representation is used for language identification.",
            "Let's say a lot of applications including I think, let's say Google and so on.",
            "Somehow using language indication, and this can be done very efficiently just by checking the frequencies of.",
            "Character sequences then you can also capture the simple patterns on character level and this is then used for spam detection or copy detection, so these problems are fit very nicely to this presentation.",
            "So we can do also learning clustering search with this representation.",
            "But somehow this can get quite limited.",
            "Also quite popular in special in this kernel methods support vector machine, so-called string kernels which are trying to capture this more complex sequences of characters.",
            "But somehow this can get quite complicated and little bit less efficient, but as a.",
            "It's an experiment, I guess.",
            "This was quite well accepted from the community for some deeper semantic tasks.",
            "Tasks like understanding the text text.",
            "This presentation would be wouldn't be appropriate is simply too weak.",
            "We don't have even birds.",
            "We have just a couple of characters."
        ],
        [
            "This is so for this next level we can go to the level of words."
        ],
        [
            "Um?",
            "Obviously this is very obvious representation of text document, so that we just do tokenization and split the document into the vert items.",
            "You can find numerous of taxation software packages on the web, so this is quite easy.",
            "What's also important to know that word for some languages at least is not so well defined.",
            "Let's in China some informed somehow words words are not.",
            "So nicely.",
            "Split as investor language.",
            "So this is just."
        ],
        [
            "Good to know.",
            "What are the most relevant words properties so the main ones would be the following, so homonymic so we have the same surface form but different meaning.",
            "Let's say riverbank, our financial institution.",
            "So the bank means both and we need to be able to decompose these two meanings disambiguate these two meanings in the proper one, then policy me where we have the same form and related meaning, like the bank would be either blood bank or financial institution.",
            "Again, two meanings but similar ones.",
            "Synonyme this is usually the most problematic one.",
            "Different form and same meaning like Singer or vocalist.",
            "So they mean the same, but they have different.",
            "Surface form and hyponymy, where one word denotes a subclass of the other one, like breakfast in the middle would be in such a relationship.",
            "So we need to be able to deal with this properties when we're doing some kind of analysis of texts.",
            "It's also very important to say so.",
            "Word frequencies in Texas have this so called Power Distribution, which we will see later on.",
            "When will discuss graphs and link link structures where basically it boils down to the following two properties.",
            "We have small number of very frequent words like the end and so on and big number of very low frequency words.",
            "And basically this structure generates redundancy in the text, so this is very important underlying property which.",
            "Usually we are not even aware of it, but somehow this generates a lot of have alot of consequences on algorithms which we use for dealing with text."
        ],
        [
            "So few more things about this word level representation, so we often use so called stop words.",
            "Stop words would be the words which have from this non linguistic view.",
            "Don't carry any information and we usually just remove them.",
            "They have just dysfunctional roles.",
            "So let's say for English this would be about, above, across, and so on.",
            "And usually we just remove these words and algorithms work better.",
            "After after this.",
            "Similar list different similar but different lists we have for other languages."
        ],
        [
            "Is about.",
            "What's also interesting to mention, which usually people don't talk about this, but.",
            "This can be quite annoying, so the characters are not necessarily even.",
            "The characters are not necessarily written in the same way.",
            "Let's especially if we use Unicode.",
            "Let's say this.",
            "With circle above it, so it has two representations in Unicode, and if we want to.",
            "Do normalization of words we need to be aware of this, and so this means that usually we need to use some.",
            "Some.",
            "Proper package software packages for this kind of normalization?"
        ],
        [
            "Last thing which we need to be aware of when dealing with words is stemming so stemming is.",
            "Just the procedure which.",
            "Transforms different forms of the same words into.",
            "It's more less normal normalized form, so we can have stemming, which usually just takes the root of the word or lemmatization, which really takes the generates the normalized proper normalized words."
        ],
        [
            "Just to skip these details, I can just show you the most often used Stemmer, which is also in public domain from this website.",
            "So this is a set of rules for English which so these are kind of rules which we apply one after another.",
            "Hi, just transformation rules on a suffix of the word.",
            "Let's say this rule would apply in such a case or relational would be would get transformed into relates the second one conditional condition and so on.",
            "And by applying these rules then most likely not often, but not always, but most often somehow we get normalized words, but occasionally you can get also errors like universe and University.",
            "I think they would.",
            "Get normalized into the same into the same version, which is not correct.",
            "But this Porter stemmer.",
            "Somehow this is small as the factor standard for English language.",
            "Other other languages have their own sets of rules.",
            "Occasionally you have even the rules which are learned by machine learning techniques specially for the, let's say, smaller languages.",
            "And this can be quite quite efficient actually."
        ],
        [
            "OK, the next level.",
            "Which is sort of similar to words, but still different are phrases."
        ],
        [
            "So phrases flow instead of just having single words.",
            "We can have sequences of words.",
            "We have two types of phrases, so one would be this frequent contiguous word sequences.",
            "Are possibilities also to have frequent non contiguous word sequences which.",
            "The location that we call a proximity features.",
            "Both type of features can be identified with relatively simple dynamic programming algorithms, so this is not big science, and but it's true that often this is not being done under this preprocessing level.",
            "When dealing with text.",
            "Why do we need this?",
            "Just by having dealing with phrases means that we can more precisely identify sense and we can get rid of.",
            "We can get rid of some of these annoying properties which I mentioned before.",
            "Likes it."
        ],
        [
            "Only me and so on.",
            "What's worth saying here is that Google released last year the biggest ngram corpus, so you can get it on this.",
            "Link I think which.",
            "So if you order this then you get 24 gigabytes of compressed text files basically.",
            "In the data set, we have 13 million unigrams, so single words.",
            "314 millions by grams.",
            "So double double words.",
            "It goes up to 5 grams which is over 1 billion.",
            "So if you unpack the."
        ],
        [
            "DVDs, then you would get such.",
            "Engrams and frequency of these engrams on I guess on Google Corpus?",
            "So this is I think, quite quite valuable corpus to play with."
        ],
        [
            "The next level of textual representation would be part of speech text.",
            "So this is already step away from.",
            "Words it's more like category of."
        ],
        [
            "Sore now.",
            "By introducing this part of speech text somehow we try to annotate different word functions.",
            "Why do we need this?",
            "So most often in text mining we would use this for information extraction techniques, which we will see a little bit afterwards.",
            "And one other possible uses just to perform sort of feature selection so that we reduce the vocabulary of all words which we deal with.",
            "So let's say it's quite well known that the most information is carried by nouns, and we can easily extract just nouns by pre processing the text with part of speech tagger.",
            "Part of speech tagging is usually our taggers are learned by hidden Markov model algorithms.",
            "So first we have a corpus which is manually annotated and then with hidden Markov model we learn the model and then we apply this also to unseen previously unseen."
        ],
        [
            "Texts.",
            "So these are most typical categories when we try to annotate documents.",
            "So so these are categories which we use to annotate words so where words are verbs, nouns, adjectives, adverbs, pronouns, preposition, conjunction and interjection.",
            "And well, you probably know what the function of each of the words or works would be more like action or state.",
            "Now this will be things and so on.",
            "So these are categories."
        ],
        [
            "Now if you look at one example, so what's the result of part of speech taggers?",
            "So if this is the sentence John Works, then John would be annotated this now and works as work.",
            "And this is more less output of part of speech tagger or little bit longer sentence would look like this.",
            "OK, so this is a set, so occasionally this additional information can be used."
        ],
        [
            "For some of the tasks.",
            "Next level of presence of text representation is also are also taxonomies or taxonomy."
        ],
        [
            "And the.",
            "The function of taxonomies is that they connect different surface forward forms with the same meaning into one sense.",
            "So synonyms are connected and in addition to synonyms, words are also connected.",
            "In this Hypernym, relationship and some other relationship which we will see afterwards.",
            "So the most commonly used general is overseas word net.",
            "Which originally was made for English and now I think it exists for many languages in the world.",
            "So in Europe we have this so called Euro Wordnet which covers seven 810 languages."
        ],
        [
            "So a few more words about the word net.",
            "So basically it consists from 4 databases from nouns, verbs, adjectives and adverbs and each database consists of sense entries.",
            "Basically this is a graph, an each node in a graph has set of synonyms designating one single meaning, let's say musician instrumentalist player.",
            "So this would be one sense and here we can see roughly the number of these nodes in a graph nouns we would have.",
            "Over 100,000.",
            "Senses for nouns and so on."
        ],
        [
            "So this is kind of picture how this looks like.",
            "Let's say if we have a sense of birth.",
            "And birth would be in.",
            "Relationship is with the animal.",
            "Wink is in a relationship, part of with the birds, so link is a part of birds.",
            "Gooses in an instance of a bird and so on.",
            "So this is the graph looks like this.",
            "How?"
        ],
        [
            "So.",
            "And these are relationships which are."
        ],
        [
            "So on the.",
            "On the edges between the senses we have, I think, 26 different."
        ],
        [
            "Nation ships and these are some of them.",
            "Hyper name so from lower to higher concepts of like breakfast and meals would be connected with this kind of relationship, then hypernym from concept to sub ordinates like Milan.",
            "Lunch has member so from groups to their members like faculty and professors would be connected in this way.",
            "Part off we saw part of an antonym would be opposite, so leader and follower would be connected with this relationships.",
            "So with this kind of information then we are able to.",
            "So it's not that we are able to see everything, but we can certainly see much more than we just playing set of words.",
            "Especially with this hypernym relationship, so people usually play with this hypernym relation."
        ],
        [
            "OK, switching now from this lexical relationship.",
            "Lexical level representations to syntactic."
        ],
        [
            "One so first we go to this vector space model which is.",
            "Mostly their presentation, which is most commonly used in text mining and information trivial.",
            "So.",
            "The idea here is to transform a document into numeric vectors which are.",
            "Sparse vectors and then apply linear algebra operations on top of it and most of data mining techniques actually is exactly applying linear algebra operations on this so called sparse numeric vectors.",
            "By doing this, we completely forget about the linguistic structure within a text.",
            "And we call this structural curves becausw by forgetting this structure.",
            "For most of the problems, actually it doesn't harm efficiency of the solution which is.",
            "Well, specially for newcomers into the area is a little bit surprising, but again here we can say that this redundancy in the textual data helps us to overcome this problem."
        ],
        [
            "This vector space models also use bag of words.",
            "This is more less just a synonym for vector space model and the typical operations which we operate this representations would be classification, clustering, visualization and so on.",
            "So this is, let's say how it looks like.",
            "So if you have a document like this then we just take all the words and build a vector with the frequencies and then we operate further on with this vector.",
            "So this very simple representation, nothing special."
        ],
        [
            "Instead of using frequencies, we usually use some other.",
            "Some other waiting schemas so frequency is 1 possible way what we have in these vectors.",
            "The other more popular and quite efficient one is so-called TF IDF.",
            "So this is very simple formula which actually captures captures two important concepts when dealing with words so.",
            "The words is more important.",
            "If it appears, let's say several times in target document, so this is set by this first factor in the second factor is saying that the work is more important if it appears in less documents.",
            "So if you have a words, some words which appear in all possible documents, then most likely they are not too informative like the would be such a word which appears everywhere and certainly is not too informative.",
            "But so we are normally.",
            "So in this formula this.",
            "Which produces one.",
            "Real number basically captures this concepts and usually we have some kind of balance between this.",
            "To effectors written in this weight vectors."
        ],
        [
            "It's called TF IDF.",
            "So this is an example of such a TF IDF vector.",
            "So if this is the document, then this document would get transformed in such a vector and this weights behind here would be TF IDF weights.",
            "This TF IDF came from information retrieval from beginning of 80s, I think."
        ],
        [
            "Another important thing which.",
            "Appears everywhere basically in text mining and also information retrieval is similarity.",
            "How to measure similarity between 2:00?",
            "Document vectors.",
            "As we said, so each document in this vector space model is represented as a vector of weights.",
            "Now what similar?",
            "What is similarity now between two documents are presented in such a way.",
            "It's simply the cosine cosine of the angle between these two vectors, which is calculated with this simple cosine.",
            "Formula which we all know from the high school, so there's no Big Magic in this and this works very well.",
            "This is used in.",
            "No, I would say most of the products you can buy on the market nowadays.",
            "Why it's this simple formula is so interesting because it can be calculated very efficiently and it has a couple of other very nice properties."
        ],
        [
            "Next, representation is so-called language.",
            "Models will just briefly mention."
        ],
        [
            "And it so this language model representation is basically this formula.",
            "And it captures.",
            "The fact that language modeling can be also about probability of a sequence of words.",
            "So basically what everything gets reduced to estimating the probabilities of the next word given one or two previous words.",
            "So this would be called 3 trigram model, and this is this simple formula.",
            "So we have two frequencies and we can estimate this probability and having this.",
            "Probabilities pre calculated and this can be very efficiently used for speech recognition, OCR handwriting recognition, machine translation and spelling correction as well.",
            "So this very simple formula actually can solve.",
            "Certainly a lot of problems.",
            "And this is also quite quite old already in text mining is not used so often.",
            "But let's say this other related fields will be a little bit more."
        ],
        [
            "Full parsing will also just briefly touch this."
        ],
        [
            "So what is full parsing?",
            "So parsing of text basically provides the maximum structural information of a sentence.",
            "What's the scenario?",
            "So on input we get we put a sentence and on the output together parts 3.",
            "So this would be an example of a part three, so Jaune hit the ball and the parser would give us this tree.",
            "The structure above this sentence.",
            "So this is non phrase where.",
            "Again, noun phrase we determiner and now now.",
            "What's interesting is that this a lot of structure, which we which we get on the top of the text is actually quite hard to use for most of the tasks.",
            "Later on we'll see some examples where we can use this information, but this is.",
            "This was a lot of discussion is still a lot of discussion how to use this additional structure information.",
            "Usually we just remove it to be honest and you just maybe some of the."
        ],
        [
            "So last syntactic level representation would be something which usually is not even mentioned, but I think it's quite quite relevant is called."
        ],
        [
            "Call it cross modality so the data which we can get out of the.",
            "Let's say on the web or our application can be represented in different modalities.",
            "So here we talk mainly about textual documents and the most common would be English texts in some kind of textual documents.",
            "But well text can be also multilingual.",
            "Information can be provided also in images in video in social networks sensor networks are getting very popular nowadays so the question here is now if you have the same object described with several modalities of the same time.",
            "This would be.",
            "Let's say Flickr would have image photo keywords.",
            "Maybe some additional text on top of it.",
            "Now question is can we exploit all these different information to get better results?"
        ],
        [
            "So one this is 1 example.",
            "Let's say if we have the word tie.",
            "We can have several representations for these words.",
            "I just actually the worst Tyus.",
            "It's written here multilingual so in different languages we would have we would get different representations then audio.",
            "Certainly if we have just the audio file with the word tie, this is another representation for the same object or image or even video like here.",
            "So these are alternative representations.",
            "Now the question is can we benefit out of this?",
            "So I want to go here much into the details but.",
            "This is semantic, which we use by the often when we try to relate images.",
            "Let's say we text or textual representation in different languages, which generates mappings between these different representations and basically represent the information in sort of modality neutral way.",
            "So let's see if we deal with images and texts, then we are able with such a representation to transition from images into text and back.",
            "So getting avert we would get back images.",
            "So this is quite efficient technique."
        ],
        [
            "Our last three representations, which I will just briefly mention.",
            "So first one would be this collaborative tagging which appeared as a relevant representation only recently with this."
        ],
        [
            "Of Web 2.0 and basically collaborative tagging is a process of adding metadata to annotate some kind of content and this content can be documents, websites, photos and so on.",
            "Most often we have this annotations in the form of a textual keywords and what's important is that this is being done in collaborative way by many users from large community.",
            "So here we discuss about this long tail and so on.",
            "Which so this big community has collectively a lot of knowledge and somehow we would try to align all the participants of the community in such a way that they expressed their knowledge more less in the same language.",
            "And this language would be this keyboard annotations, most often.",
            "As a result we get this annotated data more or less for the low, relatively low cost, and then we can operate with this data.",
            "Now here we have two exam."
        ],
        [
            "So Flickr, so if you go on a Flickr.",
            "So here we have photos which certainly it's hard to deal actually with images.",
            "And now since everybody who uploads the photos at this text.",
            "In the form of keywords, suddenly this these photos are getting comperable because we can just now compare just the keywords and this works quite well actually."
        ],
        [
            "In the same way this would work for this so called Elite."
        ],
        [
            "So here we are annotating photo."
        ],
        [
            "Ossona Flickr delicious.",
            "Is this folksonomy which?",
            "Where people at our annotate websites.",
            "So this is the search for text mining, can hear more less.",
            "You get all kinds of text mining websites since they share these text mining keyboard somehow.",
            "Then we get easily this results.",
            "Let's say Google would have a little bit more problems by getting such a relatively clean clean list of websites.",
            "Her.",
            "Idea behind this collaborative work is basically there's still a lot of people around the world which have enough free time to at this annotations, and somehow we are exploiting exploiting this goodwill and free time of the people."
        ],
        [
            "Templates and frames, so this would be a little bit more sophisticated."
        ],
        [
            "Technique, so the idea here is to extract information from text by using some kind of domain specific linguistic path."
        ],
        [
            "Scale I will show some of these patterns.",
            "So let's say this would be from this system called.",
            "Know it all.",
            "This will be set of patterns.",
            "I will show you basically how this works.",
            "Let's see if it if we put in a Google pattern like this city such such as.",
            "Now we get a list of documents and most likely whenever this city such as appears will be behind behind, it will be the name of the city.",
            "And so we recognize the model is the type of the word just by.",
            "Applying this linguistic patterns.",
            "So on the by using this linguistic pattern to for the search or, let's say, cities such as Glasgow.",
            "And so on.",
            "So in the same way we can, we can develop or even learn linguistic patterns for which are domain specific, and we can capture a lot of information out of it.",
            "So this is an interesting technique which is recently used I guess in semantic web alot becausw with this kind of patterns we can model different relationships and concepts and so on."
        ],
        [
            "And the last one, and will touch this small S at the end of the tutorial ontology."
        ],
        [
            "So here the idea is really to represent the content of the text in First Logic, 1st order logic and ontology is more less vocabulary which carries this information and the relationships between pieces of information.",
            "So I'll skip the details here because we'll touch semantic web."
        ],
        [
            "At the end, maybe just to show you an example how text can be translated into first order logic.",
            "Let's say if this is an example from psych system which will see afterwards.",
            "So if you have a sentence like this, terrorist groups are capable of directing assassination, so this can be translated into first order logic so that if something which variable groups if something is a terrorist group then this implies that this group is also behavioral.",
            "So in the relationship.",
            "Behavioural capable with assassinating something, so these are concepts from the ontology and I don't know about this so this is also sort of relationship and here it's a little bit longer sentence which is translated in the 1st order theory, so this is.",
            "This is something which we would like to automate so a lot, but it's still too difficult at the moment.",
            "At the end I will show how these kind of representations can be used for very complex question answering."
        ],
        [
            "OK, so this so much about representations about the text.",
            "Now once we have text representative in such a way, what can we do with it?",
            "So here I will show just the most typical tasks which we can perform there many more tasks.",
            "But let's say just couple of the main ones."
        ],
        [
            "First, document summarization.",
            "This is so summarization is quite on.",
            "Text is quite hard problem.",
            "Actually nobody really succeeded to do it properly.",
            "Oh so what's the task?",
            "Basically, the task is very simple just to produce shorter summary version of original document.",
            "Nothing more than this.",
            "And we have two main approaches to this.",
            "One would be so-called selection based, where the summer is just a selection of sentences from an original document.",
            "So this is something which you can get in Microsoft Word or in some of these.",
            "Commercial products.",
            "While the other one which is knowledge reach which actually tries to understand the text a little bit more, which performs some kind of form of semantic analysis, and then it represents meaning and out of the generates smaller version of the same content.",
            "Somehow this is a little bit harder problem.",
            "Now let's see."
        ],
        [
            "Keyboard techniques?",
            "First, we have this selection based.",
            "Are there usually it has very three phases and they all three quite simple.",
            "So first we take a text, analyze it.",
            "So this analyzing means just splitting down into the basic units which are sentences, then determining it's important units.",
            "This would be sentences or pieces of parts of sentences by some kind of simple formula usually, and then we are synthesizing the appropriate output.",
            "And this is this would be.",
            "Kind of formula which which is often used.",
            "In determining this important points so.",
            "Weight or importance of a sentence or parts of part of the sentences.",
            "Determined by a couple of attributes like location in the text, some Q phrases of special words which are used in the sentence.",
            "Let's say the sentence starts with.",
            "It's important to know.",
            "Then of course, then the system says, well, it must be important and let's include it in summary.",
            "Couple of statistics, statistical information, maybe a little bit of these TF IDF weights and so on which we mentioned before and maybe some.",
            "Additional heuristics which.",
            "The authors include and that's it.",
            "This formula produces some kind of number and then all the sentences in the document are just sorted by the weight and just the top.",
            "Most sentences are written on the output and that's it so."
        ],
        [
            "If you're.",
            "This is the screen from Microsoft Word, so here we have the threshold and by moving this threshold basically you get different sentences from the text and that's it.",
            "Obviously, if the sentence appears at the beginning of the paragraph, then it's more important than you can see that this one of these factors from this formula giving some results.",
            "Huh?"
        ],
        [
            "The other technique, our set of techniques I so called knowledge, reach summarization where we said that somehow here we try to understand the text a little bit more.",
            "And.",
            "So I will present here one example, one which we did with a colleague from Microsoft Research."
        ],
        [
            "There basically the idea was the following.",
            "Let's say these are 10 step procedure.",
            "What we did.",
            "Again, at the beginning we have a document which we split into sentences.",
            "Then we perform deep parsing of this document.",
            "Then the next step is named entity disambiguation.",
            "So each different forms of the name of the person or company or place which appear in the document we try to consolidate in the same form.",
            "So let's say George Bush or Bush or US President would fall in the same category.",
            "Here then we perform anaphora solution, which means that all the pronouns like he sheet would be replaced with the proper name.",
            "And then we extract subject, predicate, object triples and we generate a graph.",
            "And then by having such a graph, then we learn which parts of this graph are more relevant and which are less.",
            "We'll see this from the by simply by machine learning techniques.",
            "So we have corpus of documents and summaries.",
            "And out of this corpus we learn.",
            "How summaries look like in a graphs and by having such a model.",
            "Then we can extract also important parts of this semantic graphs, also on unseen documents.",
            "So here is an example.",
            "So if you have a document.",
            "Tom went to town in the bookstore.",
            "He bought a large book so this Tom would get replaced here.",
            "This he would get replaced and then we would extract to.",
            "Three points out of it, and then we more or less generate such a graph and."
        ],
        [
            "We learn on such a graph, so this would be an example.",
            "So this is the full document and then summarization is actually extracting just the most relevant parts of this graph.",
            "As I said, so this works quite well actually.",
            "It's not really overperforming, but still it this is an interesting alternative representation of a content of a core content."
        ],
        [
            "Of the document.",
            "And this is another example.",
            "So if this is the full text."
        ],
        [
            "This is a summary, so this would be the summary graph of such a document.",
            "I will show you now short demo how we can.",
            "Play video content of a document.",
            "How?",
            "So this is a new document from.",
            "One of Ellen News, or something from 92.",
            "So this was the test corpus.",
            "Increase little bit.",
            "So this is the full document.",
            "Now if we reduce so here we did all the learning and the model is behind and for each piece of content here we know it's important how likely or what's its weight so that it should be included in a summary or not.",
            "And now if we reduce the whole document down to one.",
            "Triple.",
            "So we would see that there's just this report.",
            "President Bush ordered embargo, this embargo was economic for Iraq and so on.",
            "So this is more less one triple summary of the whole document.",
            "And now if we increase the document a little bit the threshold a little bit, then the next on the force Mount resistance.",
            "OK, army seized buildings.",
            "This powerful Iraqi army seized government and other buildings.",
            "Then Turkey cut experts because the stories about these experts and so on.",
            "Then Army seized Palace, and so on.",
            "And now we're decreasing this threshold and the story starts appearing in front of us and at the end more less.",
            "We have the full, the full story.",
            "So this was done in such in this pipeline way as I described before, so this would be, let's say sort of knowledge reach summarization.",
            "Of a document.",
            "What's nice about this, that actually the most important attributes to decide which parts of the concepts are the most relevant come from the topology of this graph, not from the content.",
            "And this was, let's say, this little bit discontented contribution of this work.",
            "OK, let's go back.",
            "To.",
            "The presentation."
        ],
        [
            "Text segmentation I guess."
        ],
        [
            "You can just keep this one.",
            "This is not that simple."
        ],
        [
            "Ignant learning we have two minutes to the coffee break.",
            "Maybe we can just see a little bit of it.",
            "Learning case, I didn't say that the slides which are shown here are kind of.",
            "I added some slides in some slides.",
            "Most of the slides are there, but maybe sometimes in a different order.",
            "So sorry for this."
        ],
        [
            "Supervised learning, so this is probably one of the key tasks for text learning or text mining, so.",
            "It can be called also categorization, classification or valid machine learning language supervised learning.",
            "So typical scenario is the following.",
            "We have set of documents labeled with some kind of content categories and the goal is to build a model statistical model usually which would automatically assign the right content categories to this new unlabeled documents.",
            "And usually we have two subproblems.",
            "One would be so called this unstructured.",
            "Their set of categories of this Reuters data set is the most popular for this problem in structured one, which would be like Yahoo Subject Index or Dimas or Medline, and so on.",
            "Later on, I will show a demo for this."
        ],
        [
            "So this is the same scenario.",
            "So first we have set of labeled documents, so this would be our training corpus.",
            "The documents which we are using to train the model.",
            "Then we perform some kind of machine learning algorithm.",
            "We get the document classifier and that's the result, and now we have a new document which doesn't have this label or category and we apply.",
            "We put this unlabeled document into the document classifier and somehow this classifier assigns a category to this text.",
            "So this is the scenario which we are trying to."
        ],
        [
            "So what are algorithms which we are trying to use, so these are?",
            "So the most popular ones would be the following, so support vector machines, which we probably, which you probably all know.",
            "Logistic regression perceptron, which I will show on the next slide.",
            "Now you base we know nearest neighbor and so on.",
            "There more, but let's say.",
            "Huh, when it gets to some real application that we would use normally support vector machines, logistic regression or maybe Perceptron algorithm.",
            "So these are specially the perceptron."
        ],
        [
            "Which is so this is the whole.",
            "The whole algorithm.",
            "Basically perceptron.",
            "So this couple of lines of code performs almost as good as SVM, so this is not that hard to implement even for a knowledgeable engineers.",
            "So if we just read what we have, so our set of documents is represented in the form of TF IDF numeric vectors.",
            "So this is what we said before how the documents are.",
            "Represent it, and each document has labeled either plus one or minus one, so this is positive plus negative class and on the output we get linear model.",
            "So one wait pervert for the vocabulary so the higher is weighted.",
            "More relevant is that particular word for deciding whether the document should belong to that positive category or not.",
            "And now what we do first initialize the model so all these weights.",
            "Just to zero and then we add, iterate, iterate N times.",
            "Usually it converges very fast, so N would be maybe 10 or 20 times.",
            "So for each document in our collection.",
            "Using this current model.",
            "Of weights we classify this document D. So this mean you're just multiplying two vectors, nothing else.",
            "Now if this.",
            "Oh sum is greater than zero.",
            "Then classifying the documents as positive.",
            "Otherwise we classify classify this negative.",
            "And now if this classification which comes out of this is wrong, then we just adjust these weights a little bit.",
            "So this means the vector our model is not good enough and then we slightly push.",
            "This hyperplane a little bit towards the direction of the document and that's it.",
            "And we're just repeating this convergence and that's it.",
            "I mean, this is the whole algorithm and it's really hard to beat this.",
            "This simple procedure and this is already, so this is the algorithm which was invented already in 60s, so.",
            "Let's say going one slide back so this support vector machine would be maybe just a few percent better, usually on average.",
            "Or maybe just before the break I can show you the demo of support vector machines, so this is may be fun to see.",
            "So this is the demo which is written by large part owner sitting there.",
            "So we have pluses and minuses.",
            "So pluses would be.",
            "Our positive documents in minus is would be everything else so and we would like to isolate these pluses from minuses.",
            "Now let's make one simple example.",
            "So now we are using here linear kernel.",
            "What is kernel for SVM?",
            "Kernel is language which is used for.",
            "Expressing the model so we can have very simple language to express the model.",
            "In this case this linear kernel would mean just the straight lines, nothing else.",
            "Later on will increase complexity to some non straight lines as well.",
            "So this was pretty easy job, so the model set well is just.",
            "This red line and on one side are positive examples.",
            "On the other side, negative ones.",
            "OK if we add.",
            "Little things like this man and say, well.",
            "Make the straight line so this is already a little bit more complex.",
            "This is a separable by straight line, but let's see if we use polynomial kernel.",
            "So here we use the power.",
            "Basically the curves which have which are still constrained, but by the polynomial of certain degree.",
            "Or so here we see that it's well, maybe we can increase.",
            "Actually the degree little bit.",
            "OK, well the model tries to do its best, but still it's not good enough.",
            "Well, we use the strongest kernel here, so this is it.",
            "So we can be actually very.",
            "So we can give very complicated example.",
            "Let's say let's add some pluses here.",
            "Some classes here.",
            "And it separates nicely everything, maybe even.",
            "Works now we can put some minuses here.",
            "Well, decent job as well.",
            "So this is SVM actually.",
            "So what it tries to do basically is just to find the.",
            "Hyperplane so this what we're seeing is seeing here in these two dimensions as a hyperplane.",
            "As a curve in very high dimension place, this is still straight hyperplane.",
            "So you just transformed by this direction formula and.",
            "So this is the way how we can think about also modeling any kind of.",
            "Data we are.",
            "While analyzing, quit SVM now.",
            "What's funny with the text is that we're always using just linear kernel, almost always, which is completely UN useful here.",
            "So why again?",
            "Because it's here, we're living in two dimensions, and this life in two dimensions is very difficult.",
            "Well, we're leaving in 3 dimensions, and if we know that life is not easy, so text text actually lives in.",
            "Let's say 10 or 100,000 dimensions, and then within 100,000 Americans it's really easy to find a straight line which separates pluses from minuses and this is.",
            "Most.",
            "Commonly used solution as well.",
            "Let's say if you would use I think Oracle database.",
            "It has SVM with linear kernel or some other.",
            "On the other commercial product as well.",
            "OK, so now is the time for coffee break and we can meet in something like 25 minutes back."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, hello everybody.",
                    "label": 0
                },
                {
                    "sent": "So my name is Marco Marco Bernick.",
                    "label": 0
                },
                {
                    "sent": "My quarter is do not injure she she's unfortunately.",
                    "label": 0
                },
                {
                    "sent": "She didn't come to the conference, so I will present the whole tutorial.",
                    "label": 0
                },
                {
                    "sent": "So this tutorial is about basically about 3 topics.",
                    "label": 0
                },
                {
                    "sent": "It's about text mining.",
                    "label": 0
                },
                {
                    "sent": "It's about link analysis and semantic web in a way we can say that text mining can link analysis serve.",
                    "label": 1
                },
                {
                    "sent": "Well, techniques and to techniques and applications on web and semantic web.",
                    "label": 0
                },
                {
                    "sent": "So this would be kind of relationship and will also try to present the whole thing in such a way.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I said so.",
                    "label": 0
                },
                {
                    "sent": "Basically, text mining would be more or less the first part of the tutorial and here will try to answer at least the most important questions on how to deal with textual data and will try to discuss this different levels of representation techniques and so on, which we usually use.",
                    "label": 0
                },
                {
                    "sent": "Next topic, which will be slightly shorter is link analysis.",
                    "label": 0
                },
                {
                    "sent": "So how to analyze graphs and link analysis somehow related to text as well?",
                    "label": 1
                },
                {
                    "sent": "Because they're sharing similar statistical properties, so we'll touch it a little bit.",
                    "label": 0
                },
                {
                    "sent": "More in brief.",
                    "label": 0
                },
                {
                    "sent": "And the last part will be on semantic Web, so semantic web is topic which attracted quite some attention in the last couple of years, especially Web 2.0, which is even more popular popular recently and will try to relate this with text mining.",
                    "label": 0
                },
                {
                    "sent": "So it's machine learning site of the analysis on one side and on the other side.",
                    "label": 0
                },
                {
                    "sent": "With more traditional artificial intelligence techniques from 80s, which I think are still quite valid for nowadays.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would be three major topics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now starting with textmining.",
                    "label": 0
                },
                {
                    "sent": "So first very short question, why do we analysis analyze text?",
                    "label": 1
                },
                {
                    "sent": "Basically there is ultimate goal.",
                    "label": 0
                },
                {
                    "sent": "Why we do anything with taxes?",
                    "label": 1
                },
                {
                    "sent": "Basically at the end to understand the textual content.",
                    "label": 0
                },
                {
                    "sent": "But this is so difficult problem to understand the text in a in a similar various people do, so we have number of easier subtasks which are still useful and in some situations they are quite relevant even for commercial or non commercial use.",
                    "label": 1
                },
                {
                    "sent": "So we can solve this easier problems quite easily, but certainly we cannot solve the.",
                    "label": 0
                },
                {
                    "sent": "His ultimate goal, but well, hopefully we are approaching this at the end of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Will see a couple of examples on.",
                    "label": 0
                },
                {
                    "sent": "How, let's say or what would be state of the art in understanding text?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh first, maybe at the beginning kind of definition of text mining.",
                    "label": 0
                },
                {
                    "sent": "This is just adapted definition from Osama fired.",
                    "label": 0
                },
                {
                    "sent": "So we can say that text mining could be finding interesting circularities in large textual data sets where this interesting means non trivial hidden previously unknown.",
                    "label": 1
                },
                {
                    "sent": "An certainly useful or hopefully useful.",
                    "label": 0
                },
                {
                    "sent": "And other one other possible definition would be finding semantic and abstract information from surface form of textual data.",
                    "label": 1
                },
                {
                    "sent": "This would be also maybe even more accurate actually for text.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finding now how two sides of the coin why?",
                    "label": 0
                },
                {
                    "sent": "On one side dealing with taxes stuff?",
                    "label": 0
                },
                {
                    "sent": "So dealing with Texas stuff specially becausw the concepts we're dealing with are quite abstract and quite difficult to represent as well so.",
                    "label": 1
                },
                {
                    "sent": "We have literally countless combinations, number of combinations of really abstract concepts which are kind of interrelated, so this is usually the text.",
                    "label": 0
                },
                {
                    "sent": "Apart from this we have problems with synonymity like especially Flying Sorcerer UFO.",
                    "label": 0
                },
                {
                    "sent": "So they all mean the same but somehow for machine on the surface level, certainly they are different.",
                    "label": 1
                },
                {
                    "sent": "Then concepts are quite different, too difficult to visualize.",
                    "label": 1
                },
                {
                    "sent": "Then we deal with high dimensionality, which means that all statistical techniques might have problems and so on.",
                    "label": 0
                },
                {
                    "sent": "And also we deal with very high number of features.",
                    "label": 1
                },
                {
                    "sent": "So hundreds of thousands, so it's not so unusual.",
                    "label": 0
                },
                {
                    "sent": "So all these properties make the whole problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite difficult on the other side.",
                    "label": 0
                },
                {
                    "sent": "Text has one feature which is very pleasant or friendly for analytic methods, and this is the fact that the data is highly redundant and basically all practically all methods count on this single property, which makes actually.",
                    "label": 1
                },
                {
                    "sent": "So easy for everybody to experiment with very simple ideas.",
                    "label": 0
                },
                {
                    "sent": "Also in text and most likely everybody will get some kind of result and.",
                    "label": 0
                },
                {
                    "sent": "Very simple recipe for kind of ad hoc text.",
                    "label": 0
                },
                {
                    "sent": "Magic method would be just take out some phrases out of the text, find some kind of related words and create some kind of summary out of it.",
                    "label": 1
                },
                {
                    "sent": "And this most likely will work and a lot of these text mining papers would would more or less.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Follow this recipe.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's put the text mining in a context.",
                    "label": 0
                },
                {
                    "sent": "So who in the scientific area actually works in this text analysis arena?",
                    "label": 1
                },
                {
                    "sent": "So on one side?",
                    "label": 0
                },
                {
                    "sent": "It's machine learning context mining, so this would be this community of this KDD conference and may be icle seminar conferences for machine learning part on the other side.",
                    "label": 0
                },
                {
                    "sent": "We have this natural language processing so this would be linguists so there's a conference is like ACL and so on so they're more interested in linguistic aspects of text.",
                    "label": 0
                },
                {
                    "sent": "Then in the last decade, very popular field information theory.",
                    "label": 0
                },
                {
                    "sent": "Also, this area is more less concerned with search and related problems and recently so semantic web and even more recently Web 2.0 which actually also deal with text quite a lot.",
                    "label": 0
                },
                {
                    "sent": "Although they have also other problems as well.",
                    "label": 0
                },
                {
                    "sent": "So this is roughly the landscape of the areas or communities which deal with text and what's interesting.",
                    "label": 0
                },
                {
                    "sent": "Some of these communities have almost no overlap, to be honest.",
                    "label": 0
                },
                {
                    "sent": "Especially semantic web and machine learning.",
                    "label": 0
                },
                {
                    "sent": "Overlap is almost zero, while with this other fields there's a little bit more.",
                    "label": 0
                },
                {
                    "sent": "So this would be the typical tasks each of these communities does.",
                    "label": 1
                },
                {
                    "sent": "So data analysis, computational linguistics, reason, knowledge representation and reasoning, and search and databases for.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information through.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you take text analytics.",
                    "label": 1
                },
                {
                    "sent": "And try to decompose.",
                    "label": 0
                },
                {
                    "sent": "The main.",
                    "label": 0
                },
                {
                    "sent": "Areas of work are the most relevant concepts which we're dealing with when dealing with text, so we can say that first we need to be aware of what kind of representations we use for representing text, so this would be.",
                    "label": 0
                },
                {
                    "sent": "This would range from character level or presentations.",
                    "label": 0
                },
                {
                    "sent": "Very simple ones on one side and on the other side to 1st order theories.",
                    "label": 0
                },
                {
                    "sent": "So in the next slides will go through all these levels.",
                    "label": 0
                },
                {
                    "sent": "Next techniques.",
                    "label": 0
                },
                {
                    "sent": "How how we deal with the with the text.",
                    "label": 1
                },
                {
                    "sent": "So on one side we have manual work.",
                    "label": 0
                },
                {
                    "sent": "Still a lot of manual work actually is being done on text.",
                    "label": 0
                },
                {
                    "sent": "Then learning could be sort of intermediate step in the most difficult one would be reasoning with text or with the content which which we extract from text and the tasks.",
                    "label": 0
                },
                {
                    "sent": "So this would be the actual algorithms are applications at the end which we have.",
                    "label": 0
                },
                {
                    "sent": "So this would be search very popular task.",
                    "label": 0
                },
                {
                    "sent": "Then all kinds of learning, unsupervised, semi supervised and supervised learning visualization, summarization, machine translation and so on.",
                    "label": 1
                },
                {
                    "sent": "So in the second part of this text mining part will will deal with that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's interesting is that these three main group groups of concepts which are used for dealing with text.",
                    "label": 0
                },
                {
                    "sent": "Somehow they all contribute.",
                    "label": 0
                },
                {
                    "sent": "Are they all share ideas, intuitions, methods, even the data, datasets, and so on?",
                    "label": 1
                },
                {
                    "sent": "And all these communities are kind of fat with these ideas, although as I said, so they often they just don't communicate or.",
                    "label": 0
                },
                {
                    "sent": "Somehow people are too busy and they don't share ideas on the social level, but technically they are quite similar.",
                    "label": 1
                },
                {
                    "sent": "So in a way, we can say that up here is politics and down here is scientific work.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's?",
                    "label": 0
                },
                {
                    "sent": "May be interesting to mention, so there's a new initiative by mainly by.",
                    "label": 0
                },
                {
                    "sent": "W3C people somehow to establish this so called websites, and this is the architecture of this website, and so we can see the details here on this site.",
                    "label": 0
                },
                {
                    "sent": "So usually, let's say our community KDD community with appear somewhere here in AI and computer science while everything else is around this from media law economics on one side and the other side, we have mathematics, ecology and so all these contextual.",
                    "label": 0
                },
                {
                    "sent": "Usciences, which provide the context for websites.",
                    "label": 0
                },
                {
                    "sent": "I know this is relatively new, so that's why I cannot say much more.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we'll discuss how do we represent text?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have at least three levels of representations.",
                    "label": 1
                },
                {
                    "sent": "So we'll discuss each of each of them a little bit more in detail, but let's say just let's make quick walk over all of them.",
                    "label": 0
                },
                {
                    "sent": "So on one side we have the simplest one, so this would be character level.",
                    "label": 0
                },
                {
                    "sent": "Then from characters we compose words we can deal with words, then phrases, and part of speech, text and taxonomies, like word, net, and so on.",
                    "label": 0
                },
                {
                    "sent": "So this would be kind of lexical.",
                    "label": 0
                },
                {
                    "sent": "Type of representation next.",
                    "label": 0
                },
                {
                    "sent": "Next we have vector space model language models.",
                    "label": 1
                },
                {
                    "sent": "Parsing and cross modality.",
                    "label": 0
                },
                {
                    "sent": "So I put this representations under this so called syntactic syntactic level and they are the most used actually text mining.",
                    "label": 0
                },
                {
                    "sent": "So specially vector space model would be.",
                    "label": 0
                },
                {
                    "sent": "Most often useless in information three will text learning and so on, and the semantic representations would be, let's say collaborative tagging.",
                    "label": 0
                },
                {
                    "sent": "So somehow Web 2.0 would be hidden somewhere here.",
                    "label": 0
                },
                {
                    "sent": "Templates frame, so popular representation and well, the most complex ones ontologies and 1st order theories.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This'll be roughly semantic so or collaborative tagging.",
                    "label": 0
                },
                {
                    "sent": "It's hard to say whether this is really semantics or not, but somehow it's closer to semantics.",
                    "label": 0
                },
                {
                    "sent": "Then let's say to just pure syntactic level.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now let's go first to this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Character level.",
                    "label": 0
                },
                {
                    "sent": "So character level means simply just taking a document, taking the text, and split it on a characters and maybe use just sequences of two 3 characters, and that's it.",
                    "label": 1
                },
                {
                    "sent": "And out of these sequences of characters we constructed constructive vectors of frequencies, and this is then used by learning and cause we said that text has this nice property.",
                    "label": 0
                },
                {
                    "sent": "It has a lot of redundant data.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of redundancy in the information, so it's.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likely that will be able to solve some of the problems, so this representation has a couple of very important strengths.",
                    "label": 0
                },
                {
                    "sent": "First, it's very robust 'cause it avoids language morphology.",
                    "label": 1
                },
                {
                    "sent": "And especially this representation is used for language identification.",
                    "label": 0
                },
                {
                    "sent": "Let's say a lot of applications including I think, let's say Google and so on.",
                    "label": 1
                },
                {
                    "sent": "Somehow using language indication, and this can be done very efficiently just by checking the frequencies of.",
                    "label": 0
                },
                {
                    "sent": "Character sequences then you can also capture the simple patterns on character level and this is then used for spam detection or copy detection, so these problems are fit very nicely to this presentation.",
                    "label": 1
                },
                {
                    "sent": "So we can do also learning clustering search with this representation.",
                    "label": 0
                },
                {
                    "sent": "But somehow this can get quite limited.",
                    "label": 1
                },
                {
                    "sent": "Also quite popular in special in this kernel methods support vector machine, so-called string kernels which are trying to capture this more complex sequences of characters.",
                    "label": 0
                },
                {
                    "sent": "But somehow this can get quite complicated and little bit less efficient, but as a.",
                    "label": 0
                },
                {
                    "sent": "It's an experiment, I guess.",
                    "label": 0
                },
                {
                    "sent": "This was quite well accepted from the community for some deeper semantic tasks.",
                    "label": 0
                },
                {
                    "sent": "Tasks like understanding the text text.",
                    "label": 0
                },
                {
                    "sent": "This presentation would be wouldn't be appropriate is simply too weak.",
                    "label": 0
                },
                {
                    "sent": "We don't have even birds.",
                    "label": 0
                },
                {
                    "sent": "We have just a couple of characters.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is so for this next level we can go to the level of words.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Obviously this is very obvious representation of text document, so that we just do tokenization and split the document into the vert items.",
                    "label": 1
                },
                {
                    "sent": "You can find numerous of taxation software packages on the web, so this is quite easy.",
                    "label": 0
                },
                {
                    "sent": "What's also important to know that word for some languages at least is not so well defined.",
                    "label": 1
                },
                {
                    "sent": "Let's in China some informed somehow words words are not.",
                    "label": 0
                },
                {
                    "sent": "So nicely.",
                    "label": 0
                },
                {
                    "sent": "Split as investor language.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good to know.",
                    "label": 0
                },
                {
                    "sent": "What are the most relevant words properties so the main ones would be the following, so homonymic so we have the same surface form but different meaning.",
                    "label": 0
                },
                {
                    "sent": "Let's say riverbank, our financial institution.",
                    "label": 0
                },
                {
                    "sent": "So the bank means both and we need to be able to decompose these two meanings disambiguate these two meanings in the proper one, then policy me where we have the same form and related meaning, like the bank would be either blood bank or financial institution.",
                    "label": 0
                },
                {
                    "sent": "Again, two meanings but similar ones.",
                    "label": 0
                },
                {
                    "sent": "Synonyme this is usually the most problematic one.",
                    "label": 0
                },
                {
                    "sent": "Different form and same meaning like Singer or vocalist.",
                    "label": 0
                },
                {
                    "sent": "So they mean the same, but they have different.",
                    "label": 0
                },
                {
                    "sent": "Surface form and hyponymy, where one word denotes a subclass of the other one, like breakfast in the middle would be in such a relationship.",
                    "label": 1
                },
                {
                    "sent": "So we need to be able to deal with this properties when we're doing some kind of analysis of texts.",
                    "label": 0
                },
                {
                    "sent": "It's also very important to say so.",
                    "label": 1
                },
                {
                    "sent": "Word frequencies in Texas have this so called Power Distribution, which we will see later on.",
                    "label": 0
                },
                {
                    "sent": "When will discuss graphs and link link structures where basically it boils down to the following two properties.",
                    "label": 0
                },
                {
                    "sent": "We have small number of very frequent words like the end and so on and big number of very low frequency words.",
                    "label": 1
                },
                {
                    "sent": "And basically this structure generates redundancy in the text, so this is very important underlying property which.",
                    "label": 0
                },
                {
                    "sent": "Usually we are not even aware of it, but somehow this generates a lot of have alot of consequences on algorithms which we use for dealing with text.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So few more things about this word level representation, so we often use so called stop words.",
                    "label": 0
                },
                {
                    "sent": "Stop words would be the words which have from this non linguistic view.",
                    "label": 0
                },
                {
                    "sent": "Don't carry any information and we usually just remove them.",
                    "label": 0
                },
                {
                    "sent": "They have just dysfunctional roles.",
                    "label": 1
                },
                {
                    "sent": "So let's say for English this would be about, above, across, and so on.",
                    "label": 1
                },
                {
                    "sent": "And usually we just remove these words and algorithms work better.",
                    "label": 0
                },
                {
                    "sent": "After after this.",
                    "label": 0
                },
                {
                    "sent": "Similar list different similar but different lists we have for other languages.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is about.",
                    "label": 0
                },
                {
                    "sent": "What's also interesting to mention, which usually people don't talk about this, but.",
                    "label": 0
                },
                {
                    "sent": "This can be quite annoying, so the characters are not necessarily even.",
                    "label": 0
                },
                {
                    "sent": "The characters are not necessarily written in the same way.",
                    "label": 1
                },
                {
                    "sent": "Let's especially if we use Unicode.",
                    "label": 0
                },
                {
                    "sent": "Let's say this.",
                    "label": 1
                },
                {
                    "sent": "With circle above it, so it has two representations in Unicode, and if we want to.",
                    "label": 0
                },
                {
                    "sent": "Do normalization of words we need to be aware of this, and so this means that usually we need to use some.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Proper package software packages for this kind of normalization?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last thing which we need to be aware of when dealing with words is stemming so stemming is.",
                    "label": 0
                },
                {
                    "sent": "Just the procedure which.",
                    "label": 0
                },
                {
                    "sent": "Transforms different forms of the same words into.",
                    "label": 1
                },
                {
                    "sent": "It's more less normal normalized form, so we can have stemming, which usually just takes the root of the word or lemmatization, which really takes the generates the normalized proper normalized words.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to skip these details, I can just show you the most often used Stemmer, which is also in public domain from this website.",
                    "label": 0
                },
                {
                    "sent": "So this is a set of rules for English which so these are kind of rules which we apply one after another.",
                    "label": 0
                },
                {
                    "sent": "Hi, just transformation rules on a suffix of the word.",
                    "label": 0
                },
                {
                    "sent": "Let's say this rule would apply in such a case or relational would be would get transformed into relates the second one conditional condition and so on.",
                    "label": 0
                },
                {
                    "sent": "And by applying these rules then most likely not often, but not always, but most often somehow we get normalized words, but occasionally you can get also errors like universe and University.",
                    "label": 0
                },
                {
                    "sent": "I think they would.",
                    "label": 0
                },
                {
                    "sent": "Get normalized into the same into the same version, which is not correct.",
                    "label": 0
                },
                {
                    "sent": "But this Porter stemmer.",
                    "label": 0
                },
                {
                    "sent": "Somehow this is small as the factor standard for English language.",
                    "label": 0
                },
                {
                    "sent": "Other other languages have their own sets of rules.",
                    "label": 0
                },
                {
                    "sent": "Occasionally you have even the rules which are learned by machine learning techniques specially for the, let's say, smaller languages.",
                    "label": 0
                },
                {
                    "sent": "And this can be quite quite efficient actually.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the next level.",
                    "label": 0
                },
                {
                    "sent": "Which is sort of similar to words, but still different are phrases.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So phrases flow instead of just having single words.",
                    "label": 1
                },
                {
                    "sent": "We can have sequences of words.",
                    "label": 0
                },
                {
                    "sent": "We have two types of phrases, so one would be this frequent contiguous word sequences.",
                    "label": 1
                },
                {
                    "sent": "Are possibilities also to have frequent non contiguous word sequences which.",
                    "label": 0
                },
                {
                    "sent": "The location that we call a proximity features.",
                    "label": 0
                },
                {
                    "sent": "Both type of features can be identified with relatively simple dynamic programming algorithms, so this is not big science, and but it's true that often this is not being done under this preprocessing level.",
                    "label": 0
                },
                {
                    "sent": "When dealing with text.",
                    "label": 0
                },
                {
                    "sent": "Why do we need this?",
                    "label": 1
                },
                {
                    "sent": "Just by having dealing with phrases means that we can more precisely identify sense and we can get rid of.",
                    "label": 0
                },
                {
                    "sent": "We can get rid of some of these annoying properties which I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Likes it.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only me and so on.",
                    "label": 0
                },
                {
                    "sent": "What's worth saying here is that Google released last year the biggest ngram corpus, so you can get it on this.",
                    "label": 0
                },
                {
                    "sent": "Link I think which.",
                    "label": 0
                },
                {
                    "sent": "So if you order this then you get 24 gigabytes of compressed text files basically.",
                    "label": 0
                },
                {
                    "sent": "In the data set, we have 13 million unigrams, so single words.",
                    "label": 0
                },
                {
                    "sent": "314 millions by grams.",
                    "label": 0
                },
                {
                    "sent": "So double double words.",
                    "label": 0
                },
                {
                    "sent": "It goes up to 5 grams which is over 1 billion.",
                    "label": 0
                },
                {
                    "sent": "So if you unpack the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DVDs, then you would get such.",
                    "label": 0
                },
                {
                    "sent": "Engrams and frequency of these engrams on I guess on Google Corpus?",
                    "label": 0
                },
                {
                    "sent": "So this is I think, quite quite valuable corpus to play with.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next level of textual representation would be part of speech text.",
                    "label": 0
                },
                {
                    "sent": "So this is already step away from.",
                    "label": 0
                },
                {
                    "sent": "Words it's more like category of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sore now.",
                    "label": 0
                },
                {
                    "sent": "By introducing this part of speech text somehow we try to annotate different word functions.",
                    "label": 1
                },
                {
                    "sent": "Why do we need this?",
                    "label": 0
                },
                {
                    "sent": "So most often in text mining we would use this for information extraction techniques, which we will see a little bit afterwards.",
                    "label": 1
                },
                {
                    "sent": "And one other possible uses just to perform sort of feature selection so that we reduce the vocabulary of all words which we deal with.",
                    "label": 1
                },
                {
                    "sent": "So let's say it's quite well known that the most information is carried by nouns, and we can easily extract just nouns by pre processing the text with part of speech tagger.",
                    "label": 1
                },
                {
                    "sent": "Part of speech tagging is usually our taggers are learned by hidden Markov model algorithms.",
                    "label": 0
                },
                {
                    "sent": "So first we have a corpus which is manually annotated and then with hidden Markov model we learn the model and then we apply this also to unseen previously unseen.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Texts.",
                    "label": 0
                },
                {
                    "sent": "So these are most typical categories when we try to annotate documents.",
                    "label": 0
                },
                {
                    "sent": "So so these are categories which we use to annotate words so where words are verbs, nouns, adjectives, adverbs, pronouns, preposition, conjunction and interjection.",
                    "label": 0
                },
                {
                    "sent": "And well, you probably know what the function of each of the words or works would be more like action or state.",
                    "label": 0
                },
                {
                    "sent": "Now this will be things and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are categories.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you look at one example, so what's the result of part of speech taggers?",
                    "label": 0
                },
                {
                    "sent": "So if this is the sentence John Works, then John would be annotated this now and works as work.",
                    "label": 0
                },
                {
                    "sent": "And this is more less output of part of speech tagger or little bit longer sentence would look like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a set, so occasionally this additional information can be used.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For some of the tasks.",
                    "label": 0
                },
                {
                    "sent": "Next level of presence of text representation is also are also taxonomies or taxonomy.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The function of taxonomies is that they connect different surface forward forms with the same meaning into one sense.",
                    "label": 1
                },
                {
                    "sent": "So synonyms are connected and in addition to synonyms, words are also connected.",
                    "label": 0
                },
                {
                    "sent": "In this Hypernym, relationship and some other relationship which we will see afterwards.",
                    "label": 1
                },
                {
                    "sent": "So the most commonly used general is overseas word net.",
                    "label": 0
                },
                {
                    "sent": "Which originally was made for English and now I think it exists for many languages in the world.",
                    "label": 0
                },
                {
                    "sent": "So in Europe we have this so called Euro Wordnet which covers seven 810 languages.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few more words about the word net.",
                    "label": 0
                },
                {
                    "sent": "So basically it consists from 4 databases from nouns, verbs, adjectives and adverbs and each database consists of sense entries.",
                    "label": 1
                },
                {
                    "sent": "Basically this is a graph, an each node in a graph has set of synonyms designating one single meaning, let's say musician instrumentalist player.",
                    "label": 0
                },
                {
                    "sent": "So this would be one sense and here we can see roughly the number of these nodes in a graph nouns we would have.",
                    "label": 0
                },
                {
                    "sent": "Over 100,000.",
                    "label": 0
                },
                {
                    "sent": "Senses for nouns and so on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is kind of picture how this looks like.",
                    "label": 0
                },
                {
                    "sent": "Let's say if we have a sense of birth.",
                    "label": 0
                },
                {
                    "sent": "And birth would be in.",
                    "label": 0
                },
                {
                    "sent": "Relationship is with the animal.",
                    "label": 0
                },
                {
                    "sent": "Wink is in a relationship, part of with the birds, so link is a part of birds.",
                    "label": 0
                },
                {
                    "sent": "Gooses in an instance of a bird and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is the graph looks like this.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And these are relationships which are.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on the.",
                    "label": 0
                },
                {
                    "sent": "On the edges between the senses we have, I think, 26 different.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation ships and these are some of them.",
                    "label": 0
                },
                {
                    "sent": "Hyper name so from lower to higher concepts of like breakfast and meals would be connected with this kind of relationship, then hypernym from concept to sub ordinates like Milan.",
                    "label": 1
                },
                {
                    "sent": "Lunch has member so from groups to their members like faculty and professors would be connected in this way.",
                    "label": 0
                },
                {
                    "sent": "Part off we saw part of an antonym would be opposite, so leader and follower would be connected with this relationships.",
                    "label": 0
                },
                {
                    "sent": "So with this kind of information then we are able to.",
                    "label": 0
                },
                {
                    "sent": "So it's not that we are able to see everything, but we can certainly see much more than we just playing set of words.",
                    "label": 0
                },
                {
                    "sent": "Especially with this hypernym relationship, so people usually play with this hypernym relation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, switching now from this lexical relationship.",
                    "label": 0
                },
                {
                    "sent": "Lexical level representations to syntactic.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One so first we go to this vector space model which is.",
                    "label": 0
                },
                {
                    "sent": "Mostly their presentation, which is most commonly used in text mining and information trivial.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to transform a document into numeric vectors which are.",
                    "label": 0
                },
                {
                    "sent": "Sparse vectors and then apply linear algebra operations on top of it and most of data mining techniques actually is exactly applying linear algebra operations on this so called sparse numeric vectors.",
                    "label": 1
                },
                {
                    "sent": "By doing this, we completely forget about the linguistic structure within a text.",
                    "label": 1
                },
                {
                    "sent": "And we call this structural curves becausw by forgetting this structure.",
                    "label": 0
                },
                {
                    "sent": "For most of the problems, actually it doesn't harm efficiency of the solution which is.",
                    "label": 0
                },
                {
                    "sent": "Well, specially for newcomers into the area is a little bit surprising, but again here we can say that this redundancy in the textual data helps us to overcome this problem.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This vector space models also use bag of words.",
                    "label": 0
                },
                {
                    "sent": "This is more less just a synonym for vector space model and the typical operations which we operate this representations would be classification, clustering, visualization and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is, let's say how it looks like.",
                    "label": 0
                },
                {
                    "sent": "So if you have a document like this then we just take all the words and build a vector with the frequencies and then we operate further on with this vector.",
                    "label": 0
                },
                {
                    "sent": "So this very simple representation, nothing special.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead of using frequencies, we usually use some other.",
                    "label": 0
                },
                {
                    "sent": "Some other waiting schemas so frequency is 1 possible way what we have in these vectors.",
                    "label": 0
                },
                {
                    "sent": "The other more popular and quite efficient one is so-called TF IDF.",
                    "label": 0
                },
                {
                    "sent": "So this is very simple formula which actually captures captures two important concepts when dealing with words so.",
                    "label": 0
                },
                {
                    "sent": "The words is more important.",
                    "label": 1
                },
                {
                    "sent": "If it appears, let's say several times in target document, so this is set by this first factor in the second factor is saying that the work is more important if it appears in less documents.",
                    "label": 1
                },
                {
                    "sent": "So if you have a words, some words which appear in all possible documents, then most likely they are not too informative like the would be such a word which appears everywhere and certainly is not too informative.",
                    "label": 0
                },
                {
                    "sent": "But so we are normally.",
                    "label": 0
                },
                {
                    "sent": "So in this formula this.",
                    "label": 0
                },
                {
                    "sent": "Which produces one.",
                    "label": 0
                },
                {
                    "sent": "Real number basically captures this concepts and usually we have some kind of balance between this.",
                    "label": 0
                },
                {
                    "sent": "To effectors written in this weight vectors.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's called TF IDF.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of such a TF IDF vector.",
                    "label": 0
                },
                {
                    "sent": "So if this is the document, then this document would get transformed in such a vector and this weights behind here would be TF IDF weights.",
                    "label": 0
                },
                {
                    "sent": "This TF IDF came from information retrieval from beginning of 80s, I think.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another important thing which.",
                    "label": 0
                },
                {
                    "sent": "Appears everywhere basically in text mining and also information retrieval is similarity.",
                    "label": 0
                },
                {
                    "sent": "How to measure similarity between 2:00?",
                    "label": 1
                },
                {
                    "sent": "Document vectors.",
                    "label": 0
                },
                {
                    "sent": "As we said, so each document in this vector space model is represented as a vector of weights.",
                    "label": 1
                },
                {
                    "sent": "Now what similar?",
                    "label": 0
                },
                {
                    "sent": "What is similarity now between two documents are presented in such a way.",
                    "label": 1
                },
                {
                    "sent": "It's simply the cosine cosine of the angle between these two vectors, which is calculated with this simple cosine.",
                    "label": 0
                },
                {
                    "sent": "Formula which we all know from the high school, so there's no Big Magic in this and this works very well.",
                    "label": 0
                },
                {
                    "sent": "This is used in.",
                    "label": 0
                },
                {
                    "sent": "No, I would say most of the products you can buy on the market nowadays.",
                    "label": 0
                },
                {
                    "sent": "Why it's this simple formula is so interesting because it can be calculated very efficiently and it has a couple of other very nice properties.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, representation is so-called language.",
                    "label": 0
                },
                {
                    "sent": "Models will just briefly mention.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it so this language model representation is basically this formula.",
                    "label": 0
                },
                {
                    "sent": "And it captures.",
                    "label": 0
                },
                {
                    "sent": "The fact that language modeling can be also about probability of a sequence of words.",
                    "label": 1
                },
                {
                    "sent": "So basically what everything gets reduced to estimating the probabilities of the next word given one or two previous words.",
                    "label": 1
                },
                {
                    "sent": "So this would be called 3 trigram model, and this is this simple formula.",
                    "label": 0
                },
                {
                    "sent": "So we have two frequencies and we can estimate this probability and having this.",
                    "label": 0
                },
                {
                    "sent": "Probabilities pre calculated and this can be very efficiently used for speech recognition, OCR handwriting recognition, machine translation and spelling correction as well.",
                    "label": 1
                },
                {
                    "sent": "So this very simple formula actually can solve.",
                    "label": 0
                },
                {
                    "sent": "Certainly a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "And this is also quite quite old already in text mining is not used so often.",
                    "label": 0
                },
                {
                    "sent": "But let's say this other related fields will be a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Full parsing will also just briefly touch this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is full parsing?",
                    "label": 0
                },
                {
                    "sent": "So parsing of text basically provides the maximum structural information of a sentence.",
                    "label": 1
                },
                {
                    "sent": "What's the scenario?",
                    "label": 0
                },
                {
                    "sent": "So on input we get we put a sentence and on the output together parts 3.",
                    "label": 1
                },
                {
                    "sent": "So this would be an example of a part three, so Jaune hit the ball and the parser would give us this tree.",
                    "label": 0
                },
                {
                    "sent": "The structure above this sentence.",
                    "label": 0
                },
                {
                    "sent": "So this is non phrase where.",
                    "label": 0
                },
                {
                    "sent": "Again, noun phrase we determiner and now now.",
                    "label": 1
                },
                {
                    "sent": "What's interesting is that this a lot of structure, which we which we get on the top of the text is actually quite hard to use for most of the tasks.",
                    "label": 0
                },
                {
                    "sent": "Later on we'll see some examples where we can use this information, but this is.",
                    "label": 0
                },
                {
                    "sent": "This was a lot of discussion is still a lot of discussion how to use this additional structure information.",
                    "label": 0
                },
                {
                    "sent": "Usually we just remove it to be honest and you just maybe some of the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So last syntactic level representation would be something which usually is not even mentioned, but I think it's quite quite relevant is called.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call it cross modality so the data which we can get out of the.",
                    "label": 1
                },
                {
                    "sent": "Let's say on the web or our application can be represented in different modalities.",
                    "label": 0
                },
                {
                    "sent": "So here we talk mainly about textual documents and the most common would be English texts in some kind of textual documents.",
                    "label": 0
                },
                {
                    "sent": "But well text can be also multilingual.",
                    "label": 0
                },
                {
                    "sent": "Information can be provided also in images in video in social networks sensor networks are getting very popular nowadays so the question here is now if you have the same object described with several modalities of the same time.",
                    "label": 1
                },
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "Let's say Flickr would have image photo keywords.",
                    "label": 1
                },
                {
                    "sent": "Maybe some additional text on top of it.",
                    "label": 0
                },
                {
                    "sent": "Now question is can we exploit all these different information to get better results?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one this is 1 example.",
                    "label": 0
                },
                {
                    "sent": "Let's say if we have the word tie.",
                    "label": 1
                },
                {
                    "sent": "We can have several representations for these words.",
                    "label": 0
                },
                {
                    "sent": "I just actually the worst Tyus.",
                    "label": 0
                },
                {
                    "sent": "It's written here multilingual so in different languages we would have we would get different representations then audio.",
                    "label": 0
                },
                {
                    "sent": "Certainly if we have just the audio file with the word tie, this is another representation for the same object or image or even video like here.",
                    "label": 0
                },
                {
                    "sent": "So these are alternative representations.",
                    "label": 0
                },
                {
                    "sent": "Now the question is can we benefit out of this?",
                    "label": 0
                },
                {
                    "sent": "So I want to go here much into the details but.",
                    "label": 0
                },
                {
                    "sent": "This is semantic, which we use by the often when we try to relate images.",
                    "label": 0
                },
                {
                    "sent": "Let's say we text or textual representation in different languages, which generates mappings between these different representations and basically represent the information in sort of modality neutral way.",
                    "label": 1
                },
                {
                    "sent": "So let's see if we deal with images and texts, then we are able with such a representation to transition from images into text and back.",
                    "label": 0
                },
                {
                    "sent": "So getting avert we would get back images.",
                    "label": 0
                },
                {
                    "sent": "So this is quite efficient technique.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our last three representations, which I will just briefly mention.",
                    "label": 0
                },
                {
                    "sent": "So first one would be this collaborative tagging which appeared as a relevant representation only recently with this.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of Web 2.0 and basically collaborative tagging is a process of adding metadata to annotate some kind of content and this content can be documents, websites, photos and so on.",
                    "label": 1
                },
                {
                    "sent": "Most often we have this annotations in the form of a textual keywords and what's important is that this is being done in collaborative way by many users from large community.",
                    "label": 1
                },
                {
                    "sent": "So here we discuss about this long tail and so on.",
                    "label": 0
                },
                {
                    "sent": "Which so this big community has collectively a lot of knowledge and somehow we would try to align all the participants of the community in such a way that they expressed their knowledge more less in the same language.",
                    "label": 1
                },
                {
                    "sent": "And this language would be this keyboard annotations, most often.",
                    "label": 0
                },
                {
                    "sent": "As a result we get this annotated data more or less for the low, relatively low cost, and then we can operate with this data.",
                    "label": 0
                },
                {
                    "sent": "Now here we have two exam.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Flickr, so if you go on a Flickr.",
                    "label": 0
                },
                {
                    "sent": "So here we have photos which certainly it's hard to deal actually with images.",
                    "label": 0
                },
                {
                    "sent": "And now since everybody who uploads the photos at this text.",
                    "label": 0
                },
                {
                    "sent": "In the form of keywords, suddenly this these photos are getting comperable because we can just now compare just the keywords and this works quite well actually.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the same way this would work for this so called Elite.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we are annotating photo.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ossona Flickr delicious.",
                    "label": 0
                },
                {
                    "sent": "Is this folksonomy which?",
                    "label": 0
                },
                {
                    "sent": "Where people at our annotate websites.",
                    "label": 0
                },
                {
                    "sent": "So this is the search for text mining, can hear more less.",
                    "label": 0
                },
                {
                    "sent": "You get all kinds of text mining websites since they share these text mining keyboard somehow.",
                    "label": 0
                },
                {
                    "sent": "Then we get easily this results.",
                    "label": 0
                },
                {
                    "sent": "Let's say Google would have a little bit more problems by getting such a relatively clean clean list of websites.",
                    "label": 0
                },
                {
                    "sent": "Her.",
                    "label": 0
                },
                {
                    "sent": "Idea behind this collaborative work is basically there's still a lot of people around the world which have enough free time to at this annotations, and somehow we are exploiting exploiting this goodwill and free time of the people.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Templates and frames, so this would be a little bit more sophisticated.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technique, so the idea here is to extract information from text by using some kind of domain specific linguistic path.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scale I will show some of these patterns.",
                    "label": 0
                },
                {
                    "sent": "So let's say this would be from this system called.",
                    "label": 0
                },
                {
                    "sent": "Know it all.",
                    "label": 0
                },
                {
                    "sent": "This will be set of patterns.",
                    "label": 0
                },
                {
                    "sent": "I will show you basically how this works.",
                    "label": 0
                },
                {
                    "sent": "Let's see if it if we put in a Google pattern like this city such such as.",
                    "label": 0
                },
                {
                    "sent": "Now we get a list of documents and most likely whenever this city such as appears will be behind behind, it will be the name of the city.",
                    "label": 0
                },
                {
                    "sent": "And so we recognize the model is the type of the word just by.",
                    "label": 0
                },
                {
                    "sent": "Applying this linguistic patterns.",
                    "label": 0
                },
                {
                    "sent": "So on the by using this linguistic pattern to for the search or, let's say, cities such as Glasgow.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So in the same way we can, we can develop or even learn linguistic patterns for which are domain specific, and we can capture a lot of information out of it.",
                    "label": 0
                },
                {
                    "sent": "So this is an interesting technique which is recently used I guess in semantic web alot becausw with this kind of patterns we can model different relationships and concepts and so on.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last one, and will touch this small S at the end of the tutorial ontology.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here the idea is really to represent the content of the text in First Logic, 1st order logic and ontology is more less vocabulary which carries this information and the relationships between pieces of information.",
                    "label": 0
                },
                {
                    "sent": "So I'll skip the details here because we'll touch semantic web.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the end, maybe just to show you an example how text can be translated into first order logic.",
                    "label": 0
                },
                {
                    "sent": "Let's say if this is an example from psych system which will see afterwards.",
                    "label": 0
                },
                {
                    "sent": "So if you have a sentence like this, terrorist groups are capable of directing assassination, so this can be translated into first order logic so that if something which variable groups if something is a terrorist group then this implies that this group is also behavioral.",
                    "label": 1
                },
                {
                    "sent": "So in the relationship.",
                    "label": 0
                },
                {
                    "sent": "Behavioural capable with assassinating something, so these are concepts from the ontology and I don't know about this so this is also sort of relationship and here it's a little bit longer sentence which is translated in the 1st order theory, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is something which we would like to automate so a lot, but it's still too difficult at the moment.",
                    "label": 0
                },
                {
                    "sent": "At the end I will show how these kind of representations can be used for very complex question answering.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this so much about representations about the text.",
                    "label": 0
                },
                {
                    "sent": "Now once we have text representative in such a way, what can we do with it?",
                    "label": 0
                },
                {
                    "sent": "So here I will show just the most typical tasks which we can perform there many more tasks.",
                    "label": 1
                },
                {
                    "sent": "But let's say just couple of the main ones.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, document summarization.",
                    "label": 0
                },
                {
                    "sent": "This is so summarization is quite on.",
                    "label": 0
                },
                {
                    "sent": "Text is quite hard problem.",
                    "label": 0
                },
                {
                    "sent": "Actually nobody really succeeded to do it properly.",
                    "label": 0
                },
                {
                    "sent": "Oh so what's the task?",
                    "label": 0
                },
                {
                    "sent": "Basically, the task is very simple just to produce shorter summary version of original document.",
                    "label": 1
                },
                {
                    "sent": "Nothing more than this.",
                    "label": 1
                },
                {
                    "sent": "And we have two main approaches to this.",
                    "label": 1
                },
                {
                    "sent": "One would be so-called selection based, where the summer is just a selection of sentences from an original document.",
                    "label": 0
                },
                {
                    "sent": "So this is something which you can get in Microsoft Word or in some of these.",
                    "label": 0
                },
                {
                    "sent": "Commercial products.",
                    "label": 0
                },
                {
                    "sent": "While the other one which is knowledge reach which actually tries to understand the text a little bit more, which performs some kind of form of semantic analysis, and then it represents meaning and out of the generates smaller version of the same content.",
                    "label": 0
                },
                {
                    "sent": "Somehow this is a little bit harder problem.",
                    "label": 0
                },
                {
                    "sent": "Now let's see.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Keyboard techniques?",
                    "label": 0
                },
                {
                    "sent": "First, we have this selection based.",
                    "label": 1
                },
                {
                    "sent": "Are there usually it has very three phases and they all three quite simple.",
                    "label": 0
                },
                {
                    "sent": "So first we take a text, analyze it.",
                    "label": 0
                },
                {
                    "sent": "So this analyzing means just splitting down into the basic units which are sentences, then determining it's important units.",
                    "label": 1
                },
                {
                    "sent": "This would be sentences or pieces of parts of sentences by some kind of simple formula usually, and then we are synthesizing the appropriate output.",
                    "label": 0
                },
                {
                    "sent": "And this is this would be.",
                    "label": 1
                },
                {
                    "sent": "Kind of formula which which is often used.",
                    "label": 0
                },
                {
                    "sent": "In determining this important points so.",
                    "label": 0
                },
                {
                    "sent": "Weight or importance of a sentence or parts of part of the sentences.",
                    "label": 0
                },
                {
                    "sent": "Determined by a couple of attributes like location in the text, some Q phrases of special words which are used in the sentence.",
                    "label": 0
                },
                {
                    "sent": "Let's say the sentence starts with.",
                    "label": 0
                },
                {
                    "sent": "It's important to know.",
                    "label": 0
                },
                {
                    "sent": "Then of course, then the system says, well, it must be important and let's include it in summary.",
                    "label": 0
                },
                {
                    "sent": "Couple of statistics, statistical information, maybe a little bit of these TF IDF weights and so on which we mentioned before and maybe some.",
                    "label": 0
                },
                {
                    "sent": "Additional heuristics which.",
                    "label": 0
                },
                {
                    "sent": "The authors include and that's it.",
                    "label": 0
                },
                {
                    "sent": "This formula produces some kind of number and then all the sentences in the document are just sorted by the weight and just the top.",
                    "label": 0
                },
                {
                    "sent": "Most sentences are written on the output and that's it so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're.",
                    "label": 0
                },
                {
                    "sent": "This is the screen from Microsoft Word, so here we have the threshold and by moving this threshold basically you get different sentences from the text and that's it.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if the sentence appears at the beginning of the paragraph, then it's more important than you can see that this one of these factors from this formula giving some results.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other technique, our set of techniques I so called knowledge, reach summarization where we said that somehow here we try to understand the text a little bit more.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So I will present here one example, one which we did with a colleague from Microsoft Research.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There basically the idea was the following.",
                    "label": 0
                },
                {
                    "sent": "Let's say these are 10 step procedure.",
                    "label": 0
                },
                {
                    "sent": "What we did.",
                    "label": 0
                },
                {
                    "sent": "Again, at the beginning we have a document which we split into sentences.",
                    "label": 0
                },
                {
                    "sent": "Then we perform deep parsing of this document.",
                    "label": 0
                },
                {
                    "sent": "Then the next step is named entity disambiguation.",
                    "label": 0
                },
                {
                    "sent": "So each different forms of the name of the person or company or place which appear in the document we try to consolidate in the same form.",
                    "label": 0
                },
                {
                    "sent": "So let's say George Bush or Bush or US President would fall in the same category.",
                    "label": 0
                },
                {
                    "sent": "Here then we perform anaphora solution, which means that all the pronouns like he sheet would be replaced with the proper name.",
                    "label": 0
                },
                {
                    "sent": "And then we extract subject, predicate, object triples and we generate a graph.",
                    "label": 0
                },
                {
                    "sent": "And then by having such a graph, then we learn which parts of this graph are more relevant and which are less.",
                    "label": 0
                },
                {
                    "sent": "We'll see this from the by simply by machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "So we have corpus of documents and summaries.",
                    "label": 0
                },
                {
                    "sent": "And out of this corpus we learn.",
                    "label": 0
                },
                {
                    "sent": "How summaries look like in a graphs and by having such a model.",
                    "label": 0
                },
                {
                    "sent": "Then we can extract also important parts of this semantic graphs, also on unseen documents.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "So if you have a document.",
                    "label": 0
                },
                {
                    "sent": "Tom went to town in the bookstore.",
                    "label": 1
                },
                {
                    "sent": "He bought a large book so this Tom would get replaced here.",
                    "label": 0
                },
                {
                    "sent": "This he would get replaced and then we would extract to.",
                    "label": 0
                },
                {
                    "sent": "Three points out of it, and then we more or less generate such a graph and.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn on such a graph, so this would be an example.",
                    "label": 0
                },
                {
                    "sent": "So this is the full document and then summarization is actually extracting just the most relevant parts of this graph.",
                    "label": 0
                },
                {
                    "sent": "As I said, so this works quite well actually.",
                    "label": 0
                },
                {
                    "sent": "It's not really overperforming, but still it this is an interesting alternative representation of a content of a core content.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the document.",
                    "label": 0
                },
                {
                    "sent": "And this is another example.",
                    "label": 0
                },
                {
                    "sent": "So if this is the full text.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a summary, so this would be the summary graph of such a document.",
                    "label": 1
                },
                {
                    "sent": "I will show you now short demo how we can.",
                    "label": 0
                },
                {
                    "sent": "Play video content of a document.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "So this is a new document from.",
                    "label": 0
                },
                {
                    "sent": "One of Ellen News, or something from 92.",
                    "label": 0
                },
                {
                    "sent": "So this was the test corpus.",
                    "label": 0
                },
                {
                    "sent": "Increase little bit.",
                    "label": 0
                },
                {
                    "sent": "So this is the full document.",
                    "label": 0
                },
                {
                    "sent": "Now if we reduce so here we did all the learning and the model is behind and for each piece of content here we know it's important how likely or what's its weight so that it should be included in a summary or not.",
                    "label": 0
                },
                {
                    "sent": "And now if we reduce the whole document down to one.",
                    "label": 0
                },
                {
                    "sent": "Triple.",
                    "label": 0
                },
                {
                    "sent": "So we would see that there's just this report.",
                    "label": 0
                },
                {
                    "sent": "President Bush ordered embargo, this embargo was economic for Iraq and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is more less one triple summary of the whole document.",
                    "label": 0
                },
                {
                    "sent": "And now if we increase the document a little bit the threshold a little bit, then the next on the force Mount resistance.",
                    "label": 0
                },
                {
                    "sent": "OK, army seized buildings.",
                    "label": 0
                },
                {
                    "sent": "This powerful Iraqi army seized government and other buildings.",
                    "label": 0
                },
                {
                    "sent": "Then Turkey cut experts because the stories about these experts and so on.",
                    "label": 0
                },
                {
                    "sent": "Then Army seized Palace, and so on.",
                    "label": 0
                },
                {
                    "sent": "And now we're decreasing this threshold and the story starts appearing in front of us and at the end more less.",
                    "label": 0
                },
                {
                    "sent": "We have the full, the full story.",
                    "label": 0
                },
                {
                    "sent": "So this was done in such in this pipeline way as I described before, so this would be, let's say sort of knowledge reach summarization.",
                    "label": 0
                },
                {
                    "sent": "Of a document.",
                    "label": 0
                },
                {
                    "sent": "What's nice about this, that actually the most important attributes to decide which parts of the concepts are the most relevant come from the topology of this graph, not from the content.",
                    "label": 0
                },
                {
                    "sent": "And this was, let's say, this little bit discontented contribution of this work.",
                    "label": 0
                },
                {
                    "sent": "OK, let's go back.",
                    "label": 0
                },
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "The presentation.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text segmentation I guess.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can just keep this one.",
                    "label": 0
                },
                {
                    "sent": "This is not that simple.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ignant learning we have two minutes to the coffee break.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can just see a little bit of it.",
                    "label": 0
                },
                {
                    "sent": "Learning case, I didn't say that the slides which are shown here are kind of.",
                    "label": 0
                },
                {
                    "sent": "I added some slides in some slides.",
                    "label": 0
                },
                {
                    "sent": "Most of the slides are there, but maybe sometimes in a different order.",
                    "label": 0
                },
                {
                    "sent": "So sorry for this.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supervised learning, so this is probably one of the key tasks for text learning or text mining, so.",
                    "label": 0
                },
                {
                    "sent": "It can be called also categorization, classification or valid machine learning language supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So typical scenario is the following.",
                    "label": 0
                },
                {
                    "sent": "We have set of documents labeled with some kind of content categories and the goal is to build a model statistical model usually which would automatically assign the right content categories to this new unlabeled documents.",
                    "label": 1
                },
                {
                    "sent": "And usually we have two subproblems.",
                    "label": 0
                },
                {
                    "sent": "One would be so called this unstructured.",
                    "label": 0
                },
                {
                    "sent": "Their set of categories of this Reuters data set is the most popular for this problem in structured one, which would be like Yahoo Subject Index or Dimas or Medline, and so on.",
                    "label": 0
                },
                {
                    "sent": "Later on, I will show a demo for this.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the same scenario.",
                    "label": 0
                },
                {
                    "sent": "So first we have set of labeled documents, so this would be our training corpus.",
                    "label": 1
                },
                {
                    "sent": "The documents which we are using to train the model.",
                    "label": 0
                },
                {
                    "sent": "Then we perform some kind of machine learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "We get the document classifier and that's the result, and now we have a new document which doesn't have this label or category and we apply.",
                    "label": 0
                },
                {
                    "sent": "We put this unlabeled document into the document classifier and somehow this classifier assigns a category to this text.",
                    "label": 0
                },
                {
                    "sent": "So this is the scenario which we are trying to.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are algorithms which we are trying to use, so these are?",
                    "label": 0
                },
                {
                    "sent": "So the most popular ones would be the following, so support vector machines, which we probably, which you probably all know.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression perceptron, which I will show on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Now you base we know nearest neighbor and so on.",
                    "label": 0
                },
                {
                    "sent": "There more, but let's say.",
                    "label": 0
                },
                {
                    "sent": "Huh, when it gets to some real application that we would use normally support vector machines, logistic regression or maybe Perceptron algorithm.",
                    "label": 1
                },
                {
                    "sent": "So these are specially the perceptron.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is so this is the whole.",
                    "label": 0
                },
                {
                    "sent": "The whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically perceptron.",
                    "label": 0
                },
                {
                    "sent": "So this couple of lines of code performs almost as good as SVM, so this is not that hard to implement even for a knowledgeable engineers.",
                    "label": 0
                },
                {
                    "sent": "So if we just read what we have, so our set of documents is represented in the form of TF IDF numeric vectors.",
                    "label": 1
                },
                {
                    "sent": "So this is what we said before how the documents are.",
                    "label": 1
                },
                {
                    "sent": "Represent it, and each document has labeled either plus one or minus one, so this is positive plus negative class and on the output we get linear model.",
                    "label": 0
                },
                {
                    "sent": "So one wait pervert for the vocabulary so the higher is weighted.",
                    "label": 1
                },
                {
                    "sent": "More relevant is that particular word for deciding whether the document should belong to that positive category or not.",
                    "label": 0
                },
                {
                    "sent": "And now what we do first initialize the model so all these weights.",
                    "label": 0
                },
                {
                    "sent": "Just to zero and then we add, iterate, iterate N times.",
                    "label": 0
                },
                {
                    "sent": "Usually it converges very fast, so N would be maybe 10 or 20 times.",
                    "label": 1
                },
                {
                    "sent": "So for each document in our collection.",
                    "label": 0
                },
                {
                    "sent": "Using this current model.",
                    "label": 0
                },
                {
                    "sent": "Of weights we classify this document D. So this mean you're just multiplying two vectors, nothing else.",
                    "label": 0
                },
                {
                    "sent": "Now if this.",
                    "label": 0
                },
                {
                    "sent": "Oh sum is greater than zero.",
                    "label": 0
                },
                {
                    "sent": "Then classifying the documents as positive.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we classify classify this negative.",
                    "label": 0
                },
                {
                    "sent": "And now if this classification which comes out of this is wrong, then we just adjust these weights a little bit.",
                    "label": 0
                },
                {
                    "sent": "So this means the vector our model is not good enough and then we slightly push.",
                    "label": 0
                },
                {
                    "sent": "This hyperplane a little bit towards the direction of the document and that's it.",
                    "label": 0
                },
                {
                    "sent": "And we're just repeating this convergence and that's it.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is the whole algorithm and it's really hard to beat this.",
                    "label": 0
                },
                {
                    "sent": "This simple procedure and this is already, so this is the algorithm which was invented already in 60s, so.",
                    "label": 0
                },
                {
                    "sent": "Let's say going one slide back so this support vector machine would be maybe just a few percent better, usually on average.",
                    "label": 0
                },
                {
                    "sent": "Or maybe just before the break I can show you the demo of support vector machines, so this is may be fun to see.",
                    "label": 0
                },
                {
                    "sent": "So this is the demo which is written by large part owner sitting there.",
                    "label": 0
                },
                {
                    "sent": "So we have pluses and minuses.",
                    "label": 0
                },
                {
                    "sent": "So pluses would be.",
                    "label": 0
                },
                {
                    "sent": "Our positive documents in minus is would be everything else so and we would like to isolate these pluses from minuses.",
                    "label": 0
                },
                {
                    "sent": "Now let's make one simple example.",
                    "label": 0
                },
                {
                    "sent": "So now we are using here linear kernel.",
                    "label": 0
                },
                {
                    "sent": "What is kernel for SVM?",
                    "label": 0
                },
                {
                    "sent": "Kernel is language which is used for.",
                    "label": 0
                },
                {
                    "sent": "Expressing the model so we can have very simple language to express the model.",
                    "label": 0
                },
                {
                    "sent": "In this case this linear kernel would mean just the straight lines, nothing else.",
                    "label": 0
                },
                {
                    "sent": "Later on will increase complexity to some non straight lines as well.",
                    "label": 0
                },
                {
                    "sent": "So this was pretty easy job, so the model set well is just.",
                    "label": 0
                },
                {
                    "sent": "This red line and on one side are positive examples.",
                    "label": 0
                },
                {
                    "sent": "On the other side, negative ones.",
                    "label": 0
                },
                {
                    "sent": "OK if we add.",
                    "label": 0
                },
                {
                    "sent": "Little things like this man and say, well.",
                    "label": 0
                },
                {
                    "sent": "Make the straight line so this is already a little bit more complex.",
                    "label": 0
                },
                {
                    "sent": "This is a separable by straight line, but let's see if we use polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "So here we use the power.",
                    "label": 0
                },
                {
                    "sent": "Basically the curves which have which are still constrained, but by the polynomial of certain degree.",
                    "label": 0
                },
                {
                    "sent": "Or so here we see that it's well, maybe we can increase.",
                    "label": 0
                },
                {
                    "sent": "Actually the degree little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, well the model tries to do its best, but still it's not good enough.",
                    "label": 0
                },
                {
                    "sent": "Well, we use the strongest kernel here, so this is it.",
                    "label": 0
                },
                {
                    "sent": "So we can be actually very.",
                    "label": 0
                },
                {
                    "sent": "So we can give very complicated example.",
                    "label": 0
                },
                {
                    "sent": "Let's say let's add some pluses here.",
                    "label": 0
                },
                {
                    "sent": "Some classes here.",
                    "label": 0
                },
                {
                    "sent": "And it separates nicely everything, maybe even.",
                    "label": 0
                },
                {
                    "sent": "Works now we can put some minuses here.",
                    "label": 0
                },
                {
                    "sent": "Well, decent job as well.",
                    "label": 0
                },
                {
                    "sent": "So this is SVM actually.",
                    "label": 0
                },
                {
                    "sent": "So what it tries to do basically is just to find the.",
                    "label": 0
                },
                {
                    "sent": "Hyperplane so this what we're seeing is seeing here in these two dimensions as a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "As a curve in very high dimension place, this is still straight hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So you just transformed by this direction formula and.",
                    "label": 0
                },
                {
                    "sent": "So this is the way how we can think about also modeling any kind of.",
                    "label": 0
                },
                {
                    "sent": "Data we are.",
                    "label": 0
                },
                {
                    "sent": "While analyzing, quit SVM now.",
                    "label": 0
                },
                {
                    "sent": "What's funny with the text is that we're always using just linear kernel, almost always, which is completely UN useful here.",
                    "label": 0
                },
                {
                    "sent": "So why again?",
                    "label": 0
                },
                {
                    "sent": "Because it's here, we're living in two dimensions, and this life in two dimensions is very difficult.",
                    "label": 0
                },
                {
                    "sent": "Well, we're leaving in 3 dimensions, and if we know that life is not easy, so text text actually lives in.",
                    "label": 0
                },
                {
                    "sent": "Let's say 10 or 100,000 dimensions, and then within 100,000 Americans it's really easy to find a straight line which separates pluses from minuses and this is.",
                    "label": 0
                },
                {
                    "sent": "Most.",
                    "label": 0
                },
                {
                    "sent": "Commonly used solution as well.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you would use I think Oracle database.",
                    "label": 0
                },
                {
                    "sent": "It has SVM with linear kernel or some other.",
                    "label": 0
                },
                {
                    "sent": "On the other commercial product as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so now is the time for coffee break and we can meet in something like 25 minutes back.",
                    "label": 0
                }
            ]
        }
    }
}