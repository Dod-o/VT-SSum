{
    "id": "yzbk5ehqpxu5brjycqdd4jktrfnguiyx",
    "title": "Learning Theory",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_shawe_taylor_lt/",
    "segmentation": [
        [
            "Welcome back I'm John short Taylor and thank the organizers for inviting me to give this talk.",
            "They gave me a very general topic of learning theory, but I took that to mean statistical learning theory, but I will actually try and sort of paint a little bit of a picture at the beginning of how I see learning theory interacting with practice in general, and I would like learning theory to be thought of as much broader than statistical learning theory, but I my main topic will be.",
            "A background of statistical learning theory and since this is a summer school, I took it as my my.",
            "Role to try and introduce some of the key sort of concepts that have been developed in statistical learning theory, particularly the.",
            "Uniform convergence and probability approximately correct learning, so that's quite old VC dimension stuff.",
            "It's been around for a long time, but I think it's a very nice sort of coherent theory in itself.",
            "I'll make the argument that in many respects it's not up to the task of analyzing modern machine learning algorithms, but nevertheless I think it gives useful insights into into learning.",
            "And then that will be mainly the topic for today as I'll show you when I start giving my summaries and so on.",
            "But tomorrow I'll then pick up with more recent development of Rademacher complexity.",
            "Again, not very recent, but it's a slightly more refined method of analyzing the generalization of learning systems, and again has a nice link across from the pact learning, and I think again provides insights into the way.",
            "Maybe traditional statistics and and learning and to some extent Bayesian methods fit together into into a bigger picture, so I'm hoping to paint it sort of these connect things together for you.",
            "As much as I can, but also tell you some of the key ideas behind a lot of these original techniques.",
            "So this is the way I've structured it and party is today part be tomorrow, so I'll give this more sort of general introduction.",
            "Then these pack ideas and their generalization to real valued function classes.",
            "And then tomorrow pick up with the Rademacher complexity and its application to classification.",
            "I should say that my main focus will be on classification, particularly Rademacher theory can be very naturally applied to other problems such as regression, but and also unsupervised learning, but I think I won't have time unfortunately to cover any of that.",
            "So sort of overarching, as I've already said, aims are sort of some thoughts on why theory introduced introduction of the basic techniques, with some deference to history.",
            "And perhaps this is my main hope is that I will provide some insight into the proof techniques.",
            "I don't want to give you just results.",
            "I don't want to give you you nasty detailed proofs, but I would like to sort of give you a feel of what's behind some of the proofs and some of the techniques, because you may find that they spark new algorithms, new ideas, new analysis in your minds for something that you're doing or interested in at the moment.",
            "'cause I think a lot of these are quite generic techniques and have many.",
            "Potential uses elsewhere.",
            "So that's sort of my main aim.",
            "If you like as I said, I'll talk about the Rademacher tomorrow and concentration inequalities and how they link together.",
            "What I won't be doing is most general results.",
            "As I've already said.",
            "You know, Tisis results or that sort of thing.",
            "I won't be attempting a complete history in any sense, and apologies right away if you feel that I've missed out key ideas or key things that you know about or even have been involved in.",
            "I lecture of this sort.",
            "One has to pick and choose.",
            "I'm afraid I won't be talking about Bayesian inference.",
            "I certainly don't want to.",
            "You know that for me is part of learning theory, but as I said, I'm going to concentrate on statistical learning theory.",
            "I'm not also going to talk about the most recent developments.",
            "Which are, you know, things like the PAC Bayes analysis, local Rademacher complexity.",
            "I mean they're not that recent, but more recent.",
            "And these particularly.",
            "I mean, I'll allude to at one point, but it's a very nice link between the sort of Bayesian and the and the statistical learning theory approaches.",
            "So here we are party OK so others said I'll start with some thoughts about what theory should be doing for us and why we're doing theory.",
            "You know, I don't think I'm.",
            "You know, I'm sort of pushing it an open door.",
            "I think most people appreciate the need for theory, but but sometimes it's good to sort of reflect on why.",
            "What are we trying to achieve with this?",
            "Is it?",
            "Is it really useful and what's the what's the purpose?",
            "So in statistical learning theory, the main idea is to view learning from a statistical viewpoint.",
            "By that I mean that you know the core of much of learning is about trying to find out some truths about the world or about the phenomenon you're observing from a finite sample.",
            "I mean, that's the key sort of ingredient here.",
            "You're taking some observations, a finite sample of observation."
        ],
        [
            "And you're hoping from that finite sample to learn something that's actually true about the generating process that gave you that sample.",
            "And so you sort of going from particular to general in that sense.",
            "And of course, the danger is that you misinterpret some feature of the sample as something that's fundamental and true, whereas actually it's just a coincidence of that particular sample that you had that particular.",
            "Feature present so the aim of learning theory is to try and gauge that likelihood that you're being misled in some sense by the sample to conclude something that's actually not true from, you.",
            "Know some specific configuration of the data that you happen to observe.",
            "Of course, you could take a second sample and do the same thing, but your that would just check for one one sort of thing you might be interested in, but in learning we're generally interested in checking for a.",
            "A whole swathe of properties at the same time you know trying to fit a whole function class and picking out the best function in the whole class to that data.",
            "So we need to be able to check if you like how we are being misled on any of these functions, not just on one of them.",
            "And that's the key behind much of the analysis is trying to see how much information we can actually squeeze out of us finite sample that will allow us to be confident about our conclusions that we make over a whole swathe.",
            "Of possible functions or analysis that we might make.",
            "So in many cases in many ways it's connected to multiple hypothesis testing.",
            "If you're familiar with statistics of hypothesis testing where you might have a hypothesis you want to test, and you observe a sample and you see whether it's likely that hypothesis would be true, given that sample and you might do multiple hypothesis testings and in a way machine learning can be seen as doing very very many multiple hypothesis tests.",
            "I'm trying to do that in an efficient and reliable way.",
            "OK, so that's that's just .1.",
            "There is 1.2 aim of any theories to model real world, an artificial phenomena because we want to exploit them.",
            "We want to understand why things are the way they are.",
            "In order to lever that knowledge.",
            "So for instance, if we can be confident that this inference that we're making from a finite sample is real, then we can be exploit that to analyze new data and.",
            "Get accurate predictions on new data.",
            "Until we have that confidence, then we're in the risk of not being able to accurately predict on new data.",
            "So statistical learning theory is just one approach to understanding predicting exploiting learning systems.",
            "Others include as I've already mentioned Bayesian Inference Inductive Inferences.",
            "Another statistical physics has been used in this connection and traditional statistical analysis, so there are many competing theories here.",
            "And I'm certainly not trying to imply it's discal.",
            "Any theory is the best in any sort of absolute sense.",
            "It has.",
            "Each theory will have its own strengths and weaknesses.",
            "So each theory will make some assumptions about the phenomenon of learning.",
            "And based on these derived predictions of behavior.",
            "And so I've already sort of alluded to every predictive theory automatically implies a possible algorithm, because if you know about your prediction, then you can simply optimize the quantities that improve the predictions.",
            "So if you know, for instance, that the number of features will impact on the quality of your learning, then you can try.",
            "You know, the smaller the better.",
            "Then you can try and keep the number of features small and very valga rhythms that do that.",
            "Or if you if you know your VC dimension is some critical feature, you try and keep that small while obviously getting a good fit on the data.",
            "So these are these are sort of natural consequences of any theory and this would be my argument about why theory is good.",
            "Theory is good if it can drive you to new and better algorithms that will perform more efficient and more efficiencies.",
            "And dealt with here, but more more.",
            "With higher predictive accuracy.",
            "So the theory is have strengths and weaknesses as I've already mentioned.",
            "Generally speaking, the more assumptions you make, the more act."
        ],
        [
            "The predictions or the more sort of refined predictions you can make, but of course you're.",
            "Predictions will be dependent on those assumptions being correct or approximately correct.",
            "I mean most assumptions that you make can never be exactly verified and probably aren't true.",
            "So the question is, have you abstracted some assumptions that are reasonably good and actually capture the key phenomenon of what's going on, or have you put in some if you like poison into the mix?",
            "That actually is completely mis skewing your analysis and making your predictions.",
            "Inaccurate.",
            "So it depends on capturing the right aspects of the phenomenon.",
            "And obviously you want to try and keep it as general as possible, but if it's too general then you won't be able to say very much, and in a way you know that I'll make the distinction between the Bayesian and perhaps statistical models.",
            "The Bayesian typically will make more assumptions about some prior distributions and noise models, but in many cases they seem to be maybe not exactly right, but good enough.",
            "And you get good results out.",
            "Statistical learning theory makes rather less assumptions in that it just assumes an underlying distribution and that the training data is drawn IID independently, identically from that distribution.",
            "So it seems like a weaker assumption, and typically you may get weaker results for that reason, but.",
            "In fact, you do get surprisingly strong results given the weakness of those assumptions, but even those assumptions are not necessarily going to be true in any practical application.",
            "It may not be the case that your training data is independently generated.",
            "There may be connections between the way you've generated the data and so on, but if we take a sort of simple case of trying to classify cancerous tissue from healthy tissue, we can imagine there are two distributions, cells that are generated from that are actually not cancerous, and those that are cancerous.",
            "And assuming that there's some process by which we select individuals, take samples from those individuals and measure those cells, then we might expect that those samples are indeed IID and we just have a random sample.",
            "But you can, I'm sure, think of ways in which that wouldn't be the case very quick.",
            "You know, just subtle effects that would come from particular population of visiting a hospital, etc, etc.",
            "So in these in this sort of theory of statistical learning theory, we're sort of making that distribution.",
            "The repository of all that we're interested in everything that we're interested in is represented in that distribution.",
            "It's the way in which the processes of the natural artificial world we're studying are given to us through that distribution, and we access it.",
            "As I've already said, through a training sample.",
            "So it's a set of data, and I'm already thinking."
        ],
        [
            "The classification, or at least the supervised learning case where we have an input and an output.",
            "We're interested in predicting.",
            "So here's the first example an here's the NTH example.",
            "I tend to use M as the number of training examples, which is at odds with the way statisticians."
        ],
        [
            "Work, I'm afraid, and some other learning theories, but.",
            "It may be worth trying to make that more coordinated with other, with statisticians perhaps at some point.",
            "Anyway, this is the training sample which we assume is generated IID from that underlying distribution P, and so really the task of learning is saying I've got this finite set of information about which is sort of colored by P and what I want to do is learn something about P. That's true in general, independent of that particular sample.",
            "So I'm going to talk now about the generalization of a learner, because this isn't a way for classification and probably progression.",
            "Two is the key concept that we are interested in in statistical learning theory.",
            "So I'm going to spend a little bit of time looking at it, and I'm also going to plot some curves to show how it looks for some learning problems.",
            "So I think that will give us hopefully a kind of insight into what learning theory is actually trying to give us.",
            "So if we have a learning algorithm A, then what it does is it takes a training set S and typically selects from some function space FA function that it considers the right.",
            "Function for that training set so it does a best fit.",
            "It may be an empirical risk.",
            "It may be a regularised risk or whatever.",
            "Some method that has has built into it that selects a function to best fit from that training set.",
            "And now what we're interested in is how well that function performs on new data.",
            "How really, how really good is that function?",
            "OK may fit on the training set, but is it good on new data?",
            "And this is the idea of the generalization and so.",
            "This is this quantity epsilon of SAF that I I'm writing here, which is the expectation under a randomly generated nupoint new test point of the loss of that function when applied to X and compared with the correct output Y.",
            "So X&Y are the."
        ],
        [
            "A given input output generated according to the distribution AFS is the function we learned and the loss is telling us how well that function fits.",
            "So this is the expectation if you like of our loss as we start to use that.",
            "That learned function in in practice.",
            "And if we think of classification, the loss would just be one.",
            "If the two values disagree."
        ],
        [
            "And zero otherwise, while for regression it might be the squared difference between the function applied to X and the correct output.",
            "So this random variable is we're going to call this the generalization of the learner, but note that it is a random variable because it depends on the training set S, which is a random quantity.",
            "Remember, it's generated IID from that same distribution, so we now have a random variable which we're thinking of as the generalization, so it's something that will depend on the training set and what we're really interested in is not being screwed up by a training set.",
            "So we want to guard against being screwed up by a training set in this in this measure.",
            "So what I'm going to do is show you now an example of where things go wrong.",
            "If we have a rather simplistic learning algorithm and how you know we can get quite bad performance from different training sets.",
            "And so this is taking this idea of the breast cancer datasets from UCI, and we're just going to use a very simple parsing window classifier.",
            "W Plus is the average of the positive training examples W minus average of the negative training examples, and just create a weight vector between those two."
        ],
        [
            "You know, if we.",
            "You know, if you think there's a positives here.",
            "And the negatives here.",
            "Here is the average of the negative sees the average of the positives.",
            "Draw a vector between them and take the bisecting hyperplane.",
            "That's the classifier that I'm thinking of here so very simplistic classifier.",
            "And.",
            "We're going to apply it to this training data now.",
            "We wanted to actually measure the expected value of on a randomly drawn test point, so I'm just going to use the.",
            "You know, the second half of the sample that we didn't use for training as the testing half.",
            "So it's an A proxy for the true generalization error in this case.",
            "And that's the way I'm going to estimate this quantity.",
            "An I'm going to do it for different training set sizes.",
            "This is half of the data.",
            "And so on."
        ],
        [
            "And the idea is, I'm going to repeatedly draw samples of these, say this size here.",
            "342 use that algorithm.",
            "To generate a classifier and then test it on the remaining data.",
            "So every time I do that, I get one generalization error 1 sample from that random variable here.",
            "Right?",
            "I'm assuming M fixed.",
            "OK, so I'm choosing the sample sizes fixed here, but obviously every time I draw a sample of that size I get a different value of this epsilon.",
            "So I'm now going to repeatedly draw that, you know, say 1010 thousand times, and I'm going to keep a histogram of those values so we can see the kind of errors we're getting and the range of errors we get from different training sets.",
            "We may be lucky on some training sets to do very well on some we do very badly, and so on, just to sort of give you an idea of how well this particular algorithm can do.",
            "I'm going at first show you.",
            "The performance on the complete training set, so there's you know it's just test error in this case.",
            "So that's this here.",
            "So the sort of .15 is approximately the error you get if you just train on all of the data.",
            "Note that the IC."
        ],
        [
            "Effective value of this classifier is actually constant throughout training set sizes, because it's just the expectation of the positive data minus the expectation of the negative data.",
            "So it's just the expected average.",
            "If there's a cloud here.",
            "Of.",
            "Data that could be.",
            "Have been generated.",
            "You know this is a finite sample from that cloud.",
            "This average the expected value of this for a particular training set is the same as the expected value for a.",
            "The data, so this expected value of this classifier is actually.",
            "The same whatever the training set sizes, so we're only talking about variance here in the actual performance.",
            "And that's what we're interested in.",
            "How does it?",
            "How does it vary?",
            "OK, so now I'm going to show you these plots, so here's you know, half the training set and here's the range you get.",
            "So here's that average, now of accords very closely to the average.",
            "You know this point 15.",
            "Here you can see it's almost identical, but we're getting range from below .1 up to just above .2.",
            "As of you know, the variance in the actual.",
            "Error that we get.",
            "So some training set."
        ],
        [
            "Getting as high errors .2 and some is getting it lowers .09, so there's a bit of variance there.",
            "Just to remind you, know what we're interested in, you don't."
        ],
        [
            "A chance to sort of try a few training sets.",
            "You just get one training set so you don't want to be in this situation.",
            "Out here you want to be guarding against that situation and that's you know the danger is that you with one training set, could be anywhere in this range.",
            "And if there's a large sort of tail here, you're in danger of being in a bad situation.",
            "So here if I just now start to reduce the training set size, you can start to see you know a growth of some tail here and it starts to even with size 25 you know you're already in creepage over over the .2.",
            "And quite a large proportion of the of the training sets are giving errors."
        ],
        [
            "As high as you know, quite a large number around .2."
        ],
        [
            "The average note is still .15, so if you're just talking about the expected value of your error on a randomly drawn training."
        ],
        [
            "Set find no problems, looks good.",
            "But you could quite likely be in a pretty bad situation relative to that.",
            "Quite a bad, not very bad, but you know a bit quite a bit worse now things even with 68 start to get quite significantly worse.",
            "You know there's a large number of."
        ],
        [
            "Training sets that are landing you in errors over .25 know the average is still pretty close to .15, maybe .16 now?",
            "And things start."
        ],
        [
            "To deteriorate further, we're even getting ones training set here that had worse than random.",
            "The average is still below.",
            "Well, it's about .18 now .17."
        ],
        [
            "And things sort of then start to deteriorate badly.",
            "And we can see the former."
        ],
        [
            "As the number of training examples drops to about the dimension of the space, things start to look very bad OK?"
        ],
        [
            "So.",
            "The point there I was trying to."
        ],
        [
            "To sort of push home is this idea that working with the AH."
        ],
        [
            "Average the expected value of that generalization error on a randomly drawn training set could be dangerous.",
            "You know it could look actually quite good.",
            "Some algorithm that optimize that could look quite good.",
            "In fact, that algorithm doesn't seem to be doing too badly, but actually the variance of those errors is pretty high, and that could mean that on any particular training set that you got when you really were working in practice, you could be a lot worse than your.",
            "Bound was suggesting or your sort of algorithm was suggesting.",
            "Now I here's a slight sort of.",
            "Criticism of perhaps some more sort of traditional ways of working, which I mean traditional statistics, is generally worked on looking at the expected value of this error, and particularly how it goes with the sample size.",
            "Does it converge to the Bayes risk?",
            "So this is the idea of consistency of A of a classifier.",
            "If the as the sample size tends to Infinity, the expected value of this tends to the Bayes risk where the Bayes risk is.",
            "Basically, the best you could do with the data if you have to make a choice, you choose the one for which the probability of that label is higher than the probability of the other label.",
            "We're assuming you know there's possibly confusion that not.",
            "There's not always a clean classification that both X1 and X0 could with arise with some probability.",
            "So for a finite sample, the generalization has a distribution as I've shown you, and as I say, the mean of that distribution could be misleading, and I think you know low fold cross validation.",
            "Maybe an example where this this type of.",
            "High variance might occur.",
            "So one is statistical learning theory does is try to work with that complete distribution or in particular look at the tail of that distribution and it tries to.",
            "Find a bound which holds with high probability on the tail of that distribution.",
            "So what is the way it looks a bit like a statistical test?",
            "It's sort of putting a number.",
            "If I go back to one of those plots."
        ],
        [
            "It's sort of saying, let's say I say here OK sort of saying OK, I'm prepared to suffer, you know, a as in a statistical test you make a statistical test.",
            "You say 1% confidence."
        ],
        [
            "What that means is the conclusion you made could be wrong with 100 probability.",
            "OK, So what I'm going to suggest is the statistical learning theory does is say OK?",
            "I'm going to make this conclusion about the generalization performance, but I could be wrong with probability one in 100, and that would mean that I was just unlucky with my training data.",
            "My training data looked good.",
            "I kind of got a good fit on it, but actually it was just very unrepresentative in some way.",
            "And that means the chopping off a tale of this distribution.",
            "It's sort of saying, OK, you know, let's ignore this little tale here.",
            "These are just really bad luck.",
            "You're always going to have bad luck.",
            "I mean, you just may consider an IID sample.",
            "You could just get you know the same example drawn each time, and it just doesn't tell you anything about.",
            "I mean, obviously it's very unlikely, but it's still so you allow a sort of a tale to be thrown away.",
            "And then you say what you're interested in.",
            "Given that I've thrown away that tail.",
            "What is my upper bound on this distribution?",
            "We generated this was this your resampling from the same complete set.",
            "This re dividing it yes exactly.",
            "Yeah this is so many other highly correlated right now it's true.",
            "I mean, that's my point about the proxy for the test is a bit cheating.",
            "You know?",
            "I mean, you would really need to have a completely separate distribution and then subsample from.",
            "I mean I could have done that, but the training set is quite small, so yeah, but ideally you would want to have it as a separate.",
            "This is correlated here.",
            "So it's not generalization error, it's it's test error.",
            "Proxy is a proxy for the generalization.",
            "Division for solve this error from its mean, which is the knowledge.",
            "So I would I mean what I'm thinking of is the generalization error.",
            "Is this distribution complete?",
            "But what I'm interested in is sort of putting an upper bound on this distribution, given that I'm allowing myself to throw away a tail.",
            "OK, so that's sort of saying with a certain confidence I can be, you know.",
            "OK, I may be unlucky, but.",
            "Barring that, I can be confident that the error will not be worse than this.",
            "Quantity OK. Is that OK?",
            "OK, so if we.",
            "So this is the source of this acronym.",
            "Pak Pak stands for probably approximately correct.",
            "The confidence parameter Delta is the size of the tail."
        ],
        [
            "Well that we cut off.",
            "And that's the probable.",
            "So it's the probability of the sort of the conclusion holding and with that probability the function we've got is approximately correct in the sense that it's error is better than that sort of bound on that on the distribution at that cutoff point.",
            "So we're trying to bound the top end of that cut off, and obviously when I say bound that will potentially lead to algorithms that will optimize that and try to bring that bound as low as possible.",
            "OK. OK, so that that sort of hopefully got you into the way of.",
            "The Curious way of thinking of statistical learning theory and so now I'm going to sort of start putting in some sorry, some simple.",
            "Ideas that show you how to work with this quantities and build up to this VC theory that indeed bounds this tail of the distribution.",
            "So the first thing is that the key technique really is to always think of bounding the probability of being misled.",
            "I think that's the way you need to."
        ],
        [
            "About it, in order to get these, you know to get your head round how to actually do these bounds.",
            "So what you're really interested in is the bounding the probability of the training set being a bad one.",
            "So it's like hypothesis testing, so let's just take a very simple example.",
            "Let's imagine there's just one function, and it's a bad function.",
            "It's got error less bigger than epsilon.",
            "So that's what I mean by bad, but it looks good.",
            "It's got zero training error.",
            "What's the probability of that occurring?",
            "So it's here it is.",
            "The probability again, is the probability over the generation of the sample.",
            "So this probability over the generation of the sample.",
            "Again, it's IID that the error actually is 0, but it's a bad function.",
            "Well, because the sample is generated ID you can simply workout this probability very easily is just one minus the error of F. That's the probability that it looks good on that particular example and to the power M because they're independent.",
            "It's M independent.",
            "Tries that because this error F is bigger than epsilon is less than or equal to 1 minus epsilon to the M, and this is an approximation that's often used because it makes the analysis the mathematics easier is to approximate this upper bound.",
            "This one minus epsilon by the exponential of minus epsilon N. You can see you know if you just add in here, 1 minus epsilon plus epsilon squared over 2 factorial minus epsilon cubed room.",
            "Then this is E to the minus epsilon and that's either the minus epsilon episode.",
            "So pretty simple approximation.",
            "OK so notice how this is exponentially decaying with the sample size and this is clearly the thing, the kind of thing we want to see.",
            "So if we choose epsilon to be logged one on T / M and just plug that in here.",
            "Then we this will be equal to T here or you know think of log one on Delta if you like divided by M, this will be less than or equal to Delta, so this will tell us the if we take this epsilon then our probability of being misled is is T by that by a sample for that one function.",
            "OK.",
            "So now what about?",
            "Putting in a few more funk."
        ],
        [
            "Since we don't want to learn with just one function, we'd like to learn with a class of functions.",
            "So let's now move to a finite or countable function class where we can enumerate the functions.",
            "And we're going to think of the sort of dividing the.",
            "Probability of being misled among these functions with a distribution QN.",
            "Where we're sort of allowing dividing the Delta if you like among these functions with each function FN getting assigned QN fraction of the Delta.",
            "Where we assume that the sum of the Q NS is less than or equal to 1.",
            "And then we can do what is known as a union bound by applying that previous result here.",
            "For each function individually and bounding its probability is being less than QN Delta.",
            "And therefore if we just sum up the probabilities of all of these, any of these misleading us we get because the sum of the cubes was one, we get Delta.",
            "So in other words, the likelihood that any of them mislead us.",
            "Otherwise there exists an FN for which it has zero training error, but its error is bigger than the corresponding epsilon, which is what I computed as one on M log one on Q and Delta.",
            "That was just the.",
            "This quantity, here with T substituted for by Q and Delta here so the the probability that any of these is doesn't hold is less than or equal to Delta.",
            "So I mean, if you're union bound is a simple idea, but it's just basically.",
            "You know, if you think of a space here of fall.",
            "Samples that you might generate.",
            "There's a little region where function one might mislead us, and there's another region where function 2 might mislead us, and this has size Q 1D, and this has size Q.",
            "2D.",
            "They might of course overlap.",
            "You know, Q 3D might overlap and so on.",
            "But if the sum of all of these, assuming they don't overlap is less than Delta, then certainly if they do overlap, it's going to be less than Delta.",
            "So that's just the basis of the Union bound.",
            "Very simple idea.",
            "And so we get the result there.",
            "So this doesn't take into account any connections between the functions.",
            "This is assuming there in some way no relationship between them at all.",
            "And clearly that's a weakness of this bound.",
            "But we can turn it around and we get our first theorem.",
            "So all of these are say when we do the analysis you want to think of it in bounding this probability of being misled, But this doesn't really read very well to a practitioner of machine learning, so you have to sort of turn it around and turn it into a.",
            "Statement about the generalization performance.",
            "So we do that by just saying OK. That"
        ],
        [
            "Is the probability of being misled.",
            "So with probability 1 minus that.",
            "The content of that we weren't misled, right?",
            "So with probability at least one minus Delta over the random M samples.",
            "The generalization error function FN.",
            "There is this nice relation if it has zero training error, then it's true.",
            "Error is bounded by this quantity.",
            "OK.",
            "In other words, we weren't misled by any of the functions, so any function that has zero training error as this bound.",
            "So this is a first learning theory bound and it has a very sort of standard form.",
            "Nearly all learning theory, statistical learning theory bounds have something of that shape.",
            "To them, they're typically one on M. They typically have a log, one on Delta, buried in there somewhere, and that's a nice feature because it means that the size of the sample you need in order to sort of.",
            "Up your confidence about the result is very benign.",
            "In other words, you know you can sort of have Delta or multiply Delta by 1 E and you only have to add 1 extra example.",
            "So you're kind of in a very nice sort of benign situation.",
            "As far as dependence on Delta is concerned.",
            "Now the dependent.",
            "The other term.",
            "Here log one on QN is something you can think of as the complexity of the function FN that we're interested in, measured in some sort of.",
            "Bit sore or E bits or whatever in the way that the distribution.",
            "But we chose QN determine, so we're going to choose Q and we can think of this in a Bayesian sense as a prior over functions, we're thinking that they're more likely functions.",
            "We're going to give higher Q NS-2 and going to suffer therefore a better bound here, but the ones that are less likely QM will be larger and we may be in a worse situation.",
            "As far as this bound, but it's still log one on Q and so again it's quite a benign.",
            "Entry so we can think of that as the sort of complexity or description length of the function FN.",
            "Notice that we have to define that function QM before we see the data.",
            "We can't cheat."
        ],
        [
            "If we did have a distribution that we knew that was the distribution governing the likelihood of different functions, then the expected generalization error will be best.",
            "If we choose Q equal to P and that sort of Bayesian idea.",
            "Indeed, this sort of is.",
            "If you like the starting point of the PAC Bayes analysis, sort of the relationship between over a Bayesian prior and so on.",
            "So can you clarify?",
            "The sort of meaning of QM.",
            "If I if I put a lot of mass on certain hypotheses.",
            "Does this tell us how we should go about changing our learning procedure?",
            "Potentially it does.",
            "Yeah, I mean, potentially it's going to influence if you want to minimize the bound it's going to say look, I mean it's going to bias you as you would in a Bayesian sense towards using those functions which have high prior probability exactly.",
            "So you know if you think of this as a driving an algorithm as you might do, you know you choose the function that minimize this.",
            "Of course, we don't have a training area here, because it's a very simple case, but if you did add a training error and then it would appear as an extra term here, which would be typically in training error plus actually the square root of this, or some combination of that type would come in and so you would have a form where you would be trading off, making low error against.",
            "Prior probability, so it's exactly the Bayesian approach.",
            "Or regularise approach in that sense, yeah.",
            "And indeed, if you if you do work out the expected loss compared to the expected loss you would if you had the true prior.",
            "It's a KL divergent between the two distributions is what comes out is sort of again sort of pushing towards the PAC Bayes analysis.",
            "OK, so."
        ],
        [
            "We're not going to take that route.",
            "As I say, you know, one route now would be just sort of follow the back bays route and look at how that develops, but I'm.",
            "Sort of feeling the weight of history and I have to, you know, sort of go through the some of the key ideas that were developed and the first sort of.",
            "Next step that was taken.",
            "The really went beyond those simple counting arguments.",
            "Was this idea of moving to infinite function classes in the VC dimension?",
            "So that's what I want to talk about next.",
            "I should say that you know you don't need the VC dimension to apply this sort of thing to say, you know algorithms that work over countable structures.",
            "You can just apply.",
            "You know this directly to that kind of.",
            "You know, maybe monomials or sorry, yeah.",
            "Plus we will have you and close to zero and order from will be like things.",
            "I mean, it's well.",
            "It depends how how large?",
            "Yeah, I mean you need to reduce.",
            "You know if you think of monomials over some.",
            "Then it's not very big.",
            "You know it's maybe 2 to the OR.",
            "Do you include the monomial or not?",
            "You know, maybe you have fixed number K monomial, so it's two to the K possible functions.",
            "Or maybe you can negate them so it's three to the K you know, so it's K log 3 and it's OK if you have a training set size that's bigger than the number of features.",
            "But yeah, you're right.",
            "I mean if you go to yeah.",
            "Sorry yeah, OK, so the question was, you know?",
            "Basically there's a danger that that for any significant size of function class this QN will be too small and log QM will be much bigger than M. That was, your question wasn't so.",
            "I tried to give an example where that was in that case, yeah.",
            "Sorry, assumption that is it.",
            "Just the assumption that is being errors.",
            "Yeah, I just use that as an assumption here to simplify the analysis so you can do this with the non zero training.",
            "I'll give an example of an Andrea or training error later, but I've kind of swept it under the carpet but you can do the whole thing with an zero training error and you get a similar sort of band.",
            "OK, Yep.",
            "Additional functions being drawn at random because the functions in practice will be determined by the data sample edges.",
            "Yeah, what does it mean to think of functions being generated at random?",
            "You mean the QN?",
            "Or yeah, yeah?",
            "Opinion right?",
            "OK, well it's the Bayesian idea that there's some you know.",
            "Underlying belief about the likelihood that different functions will arise in a particular scenario that you're studying.",
            "So if you knew which function was going to arise, obviously don't need to learn right so?",
            "You're done.",
            "If you don't, then you may have some belief about what functions are likely or more likely to occur, and that is what PN is giving you.",
            "Of course, you know there are various philosophical interpretations of what that belief means.",
            "You know, because you can't repeat the experiment in some sense, but.",
            "But you know, I wouldn't.",
            "Be able to give very informed views about that, but.",
            "I mean, the algorithm is going to generate a function looking at the training.",
            "Yes, exactly, but this is so.",
            "The idea is that function that distribution is defined before you see the training example before you know when you arrive and you say.",
            "I'm going to study, I don't know.",
            "Biological networks, you know?",
            "Then I have some prior knowledge that tells me the likelihood of certain functions arising.",
            "An I have to think of all that and put it out there ahead of time and then I get my training sample and I try and.",
            "You know, find the best function.",
            "Biased with that prior probability.",
            "Anything that's not make sense to me that you said that we assume that we don't know the probability distribution of the training sample that you don't know the probability of Y given X, right?",
            "So it seems to me that for any given algorithm, the probability of a function being coming up or coming out of the algorithm is going to be a function of that distribution.",
            "Of course it is, but I think the.",
            "The assumption is that you know very little about.",
            "I mean, the assumption there is only the relationship between X&Y that you're assuming you know something about.",
            "You're not assuming anything about the distribution.",
            "Of X, so there I guess is where you know, yeah.",
            "The confusion is that in your original setup you say we have a fixed speed, right?",
            "Then we can.",
            "Yes.",
            "And for any P, there's an optimal F. So yeah, I guess now if you're saying that we have a prior over functions, I think maybe we have a prior possible peas which will give up the mall F. So this gives a prior auth.",
            "Yeah.",
            "Error in this framework 'cause there's no one fixed.",
            "Yeah, I think it's only a prior over the relationship between X&Y that the PES will satisfy.",
            "It's not a it's not an assumption about the distributions over the.",
            "The marginal distribution of X.",
            "There's no assumption made about that.",
            "You said, I mean, I mean what you're doing saying is I'm expecting these types of functions to arise.",
            "Those functions will tell us given an X, how to get the Y out, but they won't tell you anything about the distribution over X independently of wire.",
            "I mean, it's the same answer I just gave a moment ago.",
            "But we can talk about it offline, maybe, but.",
            "You're not convinced I can see, so yeah, I'll move on if it will come back in.",
            "How exactly is he changed?",
            "Different.",
            "Right, so OK, I unfortunately don't have this.",
            "You know what I would need is the bound with the training error in here.",
            "But imagine there's a term in here with the training error plus this quantity here.",
            "OK, so now if I have a particular choice of Qi will minimize this combination and one choice of Q will bias me towards using a function.",
            "So maybe I have two functions that are roughly equivalent in terms of training error and in one of the choices of Q.",
            "One is preferred and then the other choice of Q.",
            "The other is preferred so it would change that.",
            "Why you need?",
            "So the learning algorithm takes is a function from training sets to a choice of an element to that right.",
            "Yes, where does the Q?",
            "Yes, yes, exactly exactly.",
            "It's sort of regularised.",
            "Or you know.",
            "Biased algorithm."
        ],
        [
            "This is exactly what the learning algorithm is for.",
            "Well, in that case, it would simply be minimizing an algorithm that would minimize this expression together with the training error, so it would find the function in the class that would have.",
            "You know the the combination of training error plus this quantity would be minimized.",
            "But the bound holds even if you don't use that algorithm, right?",
            "Yeah, yeah, exactly.",
            "Yeah, yeah, exactly the bound is true.",
            "Even your learning algorithm how if you define no, you have to define Q in order to define the bound.",
            "Given Q, you have a bound given another Q, you have a different bound, but alright.",
            "Learning algorithm is just going to be to put my training set into some computer program black box and it spits out a choice of action, right?",
            "Does this result apply to that?",
            "You have to specify it before.",
            "I mean you just.",
            "You can specify in any way you want.",
            "But I don't know it's a black box and just putting things in, you know there's this one thing is the algorithm, the other is the bound, right?",
            "The algorithm you just plug it in and outcomes the function in order to get the bound a number out from the bound, you have to feed into it a queue.",
            "Alright, so you can choose your Q.",
            "How do you like ahead of time but you have to do it ahead of time.",
            "That's the only.",
            "Yeah, sure.",
            "I think confusion that if you actually join your function to have your training at a dealer is bounded.",
            "But if you actually function from their class of functions that minimize the training error on this particular training sample.",
            "Then the body.",
            "This is your bigger function function from a quality distribution que no.",
            "No no no.",
            "No, no, this is true for all functions in the class.",
            "Debating, yeah, absolutely.",
            "'cause it's?",
            "I mean, that's the whole idea of it being uniform.",
            "It holds for every function in the class.",
            "OK. Yep.",
            "For the optimal percent you through the frequencies of particular algorithms or functions being the result, then yeah, I think that's exactly right.",
            "That's exactly right.",
            "I mean, yeah.",
            "How do you mean?",
            "What do you mean by that?",
            "You know what I mean?",
            "You have to set an example of learning that would somehow.",
            "But yes, absolutely.",
            "Any other questions?",
            "I like it more questions.",
            "This is good.",
            "OK, yes.",
            "Which the.",
            "Well, I mean they could, you could envisage.",
            "Trying to I mean learn if you do it by learning algorithms like learning the prior, you know it's like having a hyper prior and they're only based on the algorithm.",
            "Little Network The Accent Qi or yeah, So what you mean?",
            "Yeah, I mean, as I say, it's your prior knowledge being fed into that Q.",
            "For example, I have.",
            "Messages.",
            "It's it's not so much that there will be no error.",
            "It's it's a question of which functions you think will have low error or you know, I mean the 0 error case is probably a bit misleading anyway, 'cause you're not going to experience them.",
            "Really good things.",
            "Decided.",
            "Yeah, I mean, you know, let's say you had a tree structure.",
            "Your qis would typically be higher for sort of, you know, shallow trees rather than deep trees.",
            "Just the natural thing you would expect because you hoping to find a simple measure, a simple.",
            "But you know, you can actually choose them how you want.",
            "You could choose one deep tree to have a very low Q if you want.",
            "A very high Q. I'm low one.",
            "Talk to you.",
            "OK, yes.",
            "I guess here one of the issues is that if you so cute you're saying Q is chosen according to your.",
            "The function you think would be more likely in your settings, so in some sense, if you choose a bad queue for your framework, your bounds won't be very useful exactly, and so maybe one way to look at this is that.",
            "For each problem, so each.",
            "There, so each problem distribution defined.",
            "Of them all type of functions that could be why given.",
            "So.",
            "For each but specific you there will be some.",
            "Y given X for which this bond would be extremely loose and probably believe misleading an and for the.",
            "For the white given X, which are well matched by the kind of functions we get from this bound, then those bound would be probably very precise, and if you optimize that naturally it will get very good function is that I think that's fair.",
            "Yeah, yeah, I think that's fine.",
            "I mean, in a way it's like doing Bayesian inference, but being told when it's not working, you know.",
            "As opposed to just doing it, hoping it works.",
            "I mean, I'm not that's not meant to be criticism, it's just you know, if you like, that's the advantage.",
            "I mean, that was one reason why Matthias Seeger did the PAC.",
            "Bayes analysis was precisely too.",
            "Sort of understand when the Gaussian process classification was.",
            "Was was reliable in some sense.",
            "Yeah, OK, yeah, another question.",
            "Can we take that offline?",
            "OK.",
            "Yes.",
            "Same question.",
            "Possible I think you right.",
            "What you're thinking out?",
            "There's another one, right?",
            "So say if it was possible, polynomial functions.",
            "Yeah, you could say that Q is one over the maximal power, yeah?",
            "Yeah, actually you're jumping a little bit ahead there in what you are there.",
            "Taking a whole class of functions and giving them as well as he is, and it will only one function, but exactly the same thing applies.",
            "Yes exactly, so Q would then be one on the number of, but you have to make sure there's some of the cues.",
            "Yeah, yeah, exactly.",
            "I'll mention that a bit later we'll come back to that if we get OK. Good, I'm going to go ahead.",
            "OK, So what we do is we have uncountably many functions, so the key idea here we can't do a union bound over infinite number of functions, because clearly."
        ],
        [
            "Work.",
            "So we want to somehow get down to a finite sample.",
            "Uh, sorry a finite set of functions.",
            "And how are we going to do that?",
            "So we have to somehow kind of replace, you know groups of functions by one representative function, and then you know that representative will hopefully take care of the whole group.",
            "And the way we try and do that is by.",
            "Actually getting down to a finite sample of a finite set of examples and then by classification you can only have a finite set of things happening on that sample, because you can only do plus minus ones on those examples.",
            "But the problem is that we always have this test point which we don't know what it is.",
            "So the first thing we do is replace the test point by ago sample.",
            "And then we can sort of argue just in terms of two samples drawn from that distribution.",
            "And then it's a finite set.",
            "So this is known as the double sample trick.",
            "An effect."
        ],
        [
            "Tively gets you from dealing with an unknown test example to just dealing with two random samples.",
            "One is your training set and one is a ghost sample.",
            "OK.",
            "So it's a it's a useful trick that can be applied.",
            "Again, I'm trying to sort of, you know, give you little sort of units of.",
            "Thinking that you could use in different contexts, so this is how the double sample trick works.",
            "What we are interested in doing is, again, you know, think in terms of bounding the probability of being misled, things going wrong, whatever.",
            "So we were interested in this probability that there exists a function that again, I'm just taking the zero training error for simplicity has zero training error, high true error, bigger than epsilon, and what we want to do is to replace that probability by the probability that basically we have two samples X&Y.",
            "Again both generated ID and there's an H. In function class that looks good on the first half and bad on the second half.",
            "And notice that there the probability is twice here and there's an epsilon 2 here.",
            "So we sort of replace the test error by a finite sample.",
            "Test so there are basically three events concerned.",
            "One is that the training error zero.",
            "The other is that it has high generalization error and the third is that it has high error on the go sample.",
            "And the point about this is that if it has tried high generalization error, then with high probability it's going to have a high error on the go sample 'cause we go sample is generated independently and so there's no reason for it to mislead us.",
            "You know we're trying to use the training sample to do all sorts of nasty stuff, which makes it likely that we're going to be misled by the training example, 'cause we're training our algorithms on it.",
            "But the ghost sample is is independent, so the this is basically encapsulated in this statement here.",
            "So the probability that this is the go sample having high error given that there's.",
            "Zero training error and the generalization."
        ],
        [
            "Error is poor is actually this.",
            "Go sample doesn't care about the training sample, that's a separate sample, so we can just throw that away.",
            "So it's just the probability that the there is high error on the on the go sample.",
            "Given that there is high error true error, and that's very clearly just a binomial tail bound, it's very easy to show that that's going to be bigger than .5 for reasonable sizes of training set.",
            "I mean very, very small sizes.",
            "So this now can be put together with the if we just do this sequence here, probability over the double sample of.",
            "Low zero training error and high error is bigger than.",
            "Sorry and High Ghost sample."
        ],
        [
            "Is greater than or equal to just, including in an extra condition that it actually has high generalization error, and then we can just separate this into this probability and a conditional probability that's the conditional probability we just bounded and so we can now turn this around.",
            "This is 1/2 and we get this quantity, which is the thing we're interested in here.",
            "But how big is that finite set?",
            "So the question is, how many functions can you get when you look at just a finite set of examples.",
            "So you've got a function Class H which could.",
            "I mean, I will have infinitely many functions potentially, but we're going to restrict ourselves to looking at its performance on a finite set of size M, or in fact will be interested in 2M."
        ],
        [
            "This will be a double sample.",
            "Interested in how many functions do we actually get?",
            "Because those are our actual equivalence classes.",
            "We only have to think about those when we're doing our analysis, and that's where we'll do the Union bound over those.",
            "So we will actually have.",
            "So I mean, it could be as high as two to the M because there are two to the impossible functions classification functions on endpoints, so the log of this number is less than or equal to M. But obviously we want to have much, much smaller than that for the.",
            "For the analysis to work.",
            "So if we just look at what happens if we plot this with a set of linear functions in a 20 dimensional space, this is."
        ],
        [
            "Ratio of BHM to two to the M. So up to about 21.",
            "Oh, it looks a bit higher, but anyway, up to something close to.",
            "The dimension here we get this ratio being equal to 1, but then about you know twice the dimension.",
            "The things suddenly zooms down and the fraction rapidly gets very close to zero and this is the kind of regime where learning kicks in where we can get the number you know, because we're going to be a union bound over this number.",
            "BH of North we want this number to be much, much smaller than the size of the training set in order to get that ratio.",
            "To be small.",
            "But this is a this is looking like good news.",
            "It looks like the number does fall off in the way that we would like so that the actual number of functions equivalence classes we need to consider is small.",
            "And this is what the vatnik chervin link."
        ],
        [
            "This dimension measures OK, so that makes heavy German ink is dimension is the size for which we can get.",
            "Find some examples and we can do every possible classification.",
            "In other words, it's the point here where this.",
            "Curves leaves the.",
            "The value one and starts to dip down below one, but the VC dimension is defined for general function classes, not just for linear function classes.",
            "For linear function classes, it just turns out that the VC dimension is the linear dimension plus one.",
            "So what we're but we can apply this idea more generally, so it's just saying the largest sample size for which we can find some inputs for which we can do every possible classification.",
            "OK.",
            "I mean, if I do it in the case of linear functions, I can just show you quickly works.",
            "Search this is linear threshold functions.",
            "If I take 3 points, I can separate them in the following way with plus minus, say making these two plus these minus or the other way around.",
            "This plus and these two minus and then I can separate these two from this one or these two from this one and I can also make them all positive or all negative and so I can do every possible classification of three points that's in two dimensions.",
            "The dimensions 2 here, but if I move too.",
            "Four points, then there's no way I can get every possible classification.",
            "For instance, making these two positive and these negative, I would need to basically.",
            "Do something like that, which is not a straight line.",
            "And however, you can figure the points you know there's actually you can choose the points how you want, but ever however you choose them, you can't.",
            "So the VC dimension of linear threshold functions in two dimensions is 3, because there are three points that we can do every possible classification, but not for.",
            "So that's a general result for any dimension.",
            "OK, so that's the VC dimension.",
            "And the key result that actually gives us learning if you like is this relationship between the growth function and the VC dimension.",
            "So the VC dimension.",
            "Tells us how fast this growth function grows once we take sample size is larger."
        ],
        [
            "Then the VC dimension.",
            "So up to the VC dimension.",
            "Obviously the growth function is growing exponentially because we're getting every possible function.",
            "So up to M = D, We're getting 2 to the M number of functions, but once we go beyond D, we're actually having a number that's bounded by this polynomial with exponent D. So we have a clearly two regimes.",
            "The growth function is exponential up to the VC dimension and then polynomial, and there's no in between, and this is a completely general result for any set of classic classifiers.",
            "So this is a very nice and I think you know surprising.",
            "Result due to it's called Sours Llama, but it's actually was proved by or a slightly weaker version by Vatican Chevening Kiss and also by Sheller.",
            "So it has 3 three different people who were interested in this.",
            "Obviously back in Germany were interested for this learning application, but the others were interested for just combinatorial.",
            "Interest, separate combinatorial interest.",
            "OK, so given this, we're now in a position to really put together the basic theorem of statistical learning theory.",
            "So here it goes, we want to bound this probability of being misled.",
            "You know the usual thing error zero.",
            "True error big.",
            "We apply the double sample trick that's less than twice the probability of zero training error an.",
            "Bad error on the on the second sample and now we do a union bound over the possible sets of function possible functions we can get on that double sample and B."
        ],
        [
            "Is there only BH of two M of 'em we can pull out that as a multiplier here, and this is now the probability on a double sample for a fixed H that we have 0 error on the first half and higher on the second half.",
            "So.",
            "Now the final ingredient is to bound this quantity here, and that's done by another trick known as symmetrization.",
            "And the idea here is to, you know, just looking at this.",
            "If you realize that this function H was chosen and fixed, it seems pretty weird that all the errors should have gone into the second half.",
            "There's no reason why aren't half of them in the first half, you know?",
            "Because the function was fixed and then we did the sample right?",
            "So think about swapping them and the chances of the swapping actually putting all the errors in the second half.",
            "That's what Symmetrization does, basically.",
            "So the way formerly you do it is you."
        ],
        [
            "Say that you could generate a sample by because all of the examples are independent.",
            "You could, after generating your sample, apply a random permutation and still have the same probability of those points.",
            "The order in which they occur is completely arbitrary, and so we think of this distribution which says Generator 2 sample and then actually apply a an element from the some permutation group and.",
            "Mute the entries in the actual.",
            "In in, in the sequence in the sample.",
            "Now if we want to look at the probability of an event, it's the same if we do it with this distribution or with this distribution.",
            "You know applying the permutations or not applying them.",
            "And if we look at this one, we can think of doing the permutations 1st and then doing an expectation according to the examples and so now."
        ],
        [
            "Sorry, doing any expectation according to an example and given the set of examples, then doing a random permutation and the probability of that event under that random permutation.",
            "Now we're very close 'cause we're thinking of we've got a sample.",
            "What is the chance under a random permutation all the errors in the second half?",
            "OK, it's clear that's going to be very low, and we do that by just considering these permutations that swap corresponding elements in the first half and second half of the sample.",
            "That's the particular.",
            "Set of permutations we consider and if you do that, the chances that they're all in the second half.",
            "There's epsilon M / 2 of them two to the minus epsilon M / 2 because we just have to make sure all of those that were an error for that particular swap, they have to go into the second half so.",
            "The second half right?",
            "So now that just puts in the final ingredient here that was."
        ],
        [
            "The this probability remember of that first half, 0 second, half epsilon on 2 for a random permutation is to the minus epsilon M / 2 and the expectation of that is constant.",
            "So it's two to the minus epsilon M / 2 and so now we go back to the sequence I had before.",
            "We've bounded this thing here by two to the minus epsilon M / 2.",
            "We've got a bound on this from Sam's lemma, and so we've got a bound on this thing that we're interested in bounding this probability of being misled.",
            "Putting all those together, we and turning it around as usual to make a theorem.",
            "This is the result you get with probability 1 minus Delta over the M samples."
        ],
        [
            "The probability the generalization error of a function H chosen from a class with VC dimension D which has zero training error is bounded by this quantity.",
            "Where you know gain the form is very similar, constant, slightly different to the one we had before the log.",
            "Two on Delta.",
            "In this case two and M, but the complexity term is now this two log 2:00 AM on Delta which comes from the Sauer's lemma or the bound on the growth function.",
            "If you just go back quickly, I can show you that.",
            "Here it is.",
            "It's the log of this quantity here where N is replaced by two M because we have the double sample.",
            "So that's where that quantity here comes from.",
            "OK. And so there you are.",
            "That's that's if you like the VC dimension results.",
            "We can think of the D as a measure of the complexity or capacity of the Class H. But it doesn't actually distinguish between functions in H, so you can think of it as sort of a uniform prior over the functions in H. If you're thinking based in terms.",
            "It doesn't.",
            "Prior, it doesn't actually, you know, so the way to optimize this bound given a particular H is simply minimize the training error.",
            "There are corresponding lower bounds which are pretty close match.",
            "But they rely on they don't apply in general for every distribution they say given the."
        ],
        [
            "You got a VC class.",
            "You can find the distribution that forces an error of.",
            "The same order as that upper bound.",
            "So this is a slight mismatch and remember you know the upper bound is for all distributions, but the lower bound is.",
            "You know there exist distributions are basically put the distributions on points that can do every possible classification and you can imagine that given that you can do every classification on those points, it's going to be tough to learn anything from the classification of some subset of them, and that's basically the trick that's used here.",
            "And here's the non zero training error version.",
            "So basically the error on the training set plus now the square root of these quantities comes in, which is a lot weaker.",
            "But there's actually the PAC Bayes bounds that can interpolate between these two even with the VC dimension involved.",
            "OK, so you."
        ],
        [
            "There are sort of, you know, because when you have zero training error, you suddenly switch to a non square root version here which is alot alot tighter but you can interpolate roughly between those two.",
            "OK.",
            "So that, jeez, I've almost run out of time, but I will just cover this."
        ],
        [
            "Which is alluding to what you asked earlier.",
            "I mean, if you're thinking about actually now doing something a bit like.",
            "You know a Bayesian way of learning where you're thinking of putting weights or higher probability on lower more lower complexity functions.",
            "The way to do that in the VC world is so called structural risk minimization, so the structural risk minimization assumes that we can form a hierarchy of classes.",
            "Of increasing complexity, where I'm assuming, for instance, that this has VC dimension one this to this D in this K, so we've ordered them in VC classes.",
            "Now if we want to find a function in each class with minimal empirical error, which is the thing you do if you learn in a single class.",
            "You can then minimize this quantity, which is the generalization error bound over all of the classes.",
            "And by doing that you're trading off.",
            "Your empirical error against your complexity term, the usual thing.",
            "Putting up if you like a bias aprior towards simpler BC classes, but obviously only at the expense of making more training errors.",
            "By applying that theorem that I gave you on the previous slide, for each of these classes and doing a union bound, we get adjust K here for the number of classes and so this is actually not only the thing to minimize, but it also is an error bound on the resulting function that you choose by that algorithm.",
            "So again, the the bound gives you an algorithm an the result of the algorithm that you can apply the bound to.",
            "I'm not saying it's a very good algorithm, but you know it's just.",
            "Tying the two together, I think is important.",
            "OK, and the point you were making, there's no reason I've actually just done a union bound here with equal probabilities for each of these classes.",
            "But I could have put an equal Q1Q2 up to QK and put a distribution over those classes and put my bias that I think you know these could be polynomials for instance and I would say I think a simple polynomial is more likely.",
            "In this case I'm going to put a higher value of Q and I would suffer here.",
            "QD sorry 1 / Q D here rather than a fixed K here.",
            "So you would again bias yourself towards.",
            "The I mean you're paying twice for complexity in that case because you're playing here for the increasing VC dimension and you pay here by your belief about certain.",
            "Function classes being more likely.",
            "OK, so I've just got."
        ],
        [
            "To criticize the pack theory, and then I'll come back to tomorrow to sort of pick this up so the theory is certainly valid, and it's sort of, you know, bit scary, because there are lower bounds, so it looks like that's it.",
            "You know people were packing their bags and off, you know to the.",
            "For a vacation or something, because it seemed like learning theory was closed out.",
            "But it doesn't accord with experience for those applying learning, there seems to be a bit of a big mismatch between theory and practice, and the biggest example is support vector machines.",
            "So if we're thinking about applying support vector machines in high dimensional feature spaces, which is what you do with the kernel, you can say for instance, take a Gaussian kernel and and be in an infinite dimensional space, so.",
            "This theory just says nothing.",
            "I mean, it's an infinite VC dimension, so forget it.",
            "You know you're not actually getting any bound out, and yet you're getting very impressive performance, so something's happening that this theory is failing to capture, and it's a little strange because we had the lower bound.",
            "So what went wrong?",
            "So this is just a quick review of what a SVM does is just a linear classifier in this feature space that's defined implicitly via kernel.",
            "And we minimize the norm of the weight vector squared.",
            "So we can think of that again as a prior being."
        ],
        [
            "Over the likely weight vectors, maybe with the Gaussian centered at the origin, and this is some slack variables which we could sort of think of as measuring noise, so it's sort of an illusion too.",
            "A Bayesian learning here.",
            "So the intuition is that the."
        ],
        [
            "Margin is what causes this reduction in complexity.",
            "And the point is that actually we know because of the lower bound that the fact that we're learning must be something to do with the distribution, right?",
            "Because we know that worst case distributions cause us to have the same error as the VC bound, the fact that we're doing better than the VC bound means there must be something about the distribution that's helping us to learn.",
            "So I think this is an insight that is come out of this, that perhaps you know we can maybe haven't made as much use of as we might do, so there is something in the distribution.",
            "It must be benign, somehow aligned with the thing we're trying to learn that is getting us out of that worst case generalization.",
            "There has been a suggestion.",
            "You can use structural risk minimization over a hierarchy determined by the margin of different classifiers, but that's actually not valid becausw you need to define to do structural risk minimization, you need to define the function classes ahead of time, but you can't know which functions would have large margin until you see the data.",
            "So you can't actually do that hierarchy until you've seen.",
            "The data, so the approach that you need is a sort of data dependent structural risk minimization, which will set up the hierarchy based on the sample.",
            "So simple structural risk minimization won't work."
        ],
        [
            "So this is what we're looking for, and the approach sort of uses this idea of lucky distributions or luckiness that tell us that the actual classifier is aligned in some way with the distribution.",
            "And the margin is a measure of how benign the distribution is, how well the fit is with the distribution.",
            "So I think that's a good point to break, so I'll come back next time tomorrow."
        ],
        [
            "And pick up here and show you how the analysis goes through for support vector machines and hopefully a little bit into Rademacher complexity.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome back I'm John short Taylor and thank the organizers for inviting me to give this talk.",
                    "label": 0
                },
                {
                    "sent": "They gave me a very general topic of learning theory, but I took that to mean statistical learning theory, but I will actually try and sort of paint a little bit of a picture at the beginning of how I see learning theory interacting with practice in general, and I would like learning theory to be thought of as much broader than statistical learning theory, but I my main topic will be.",
                    "label": 0
                },
                {
                    "sent": "A background of statistical learning theory and since this is a summer school, I took it as my my.",
                    "label": 1
                },
                {
                    "sent": "Role to try and introduce some of the key sort of concepts that have been developed in statistical learning theory, particularly the.",
                    "label": 0
                },
                {
                    "sent": "Uniform convergence and probability approximately correct learning, so that's quite old VC dimension stuff.",
                    "label": 0
                },
                {
                    "sent": "It's been around for a long time, but I think it's a very nice sort of coherent theory in itself.",
                    "label": 0
                },
                {
                    "sent": "I'll make the argument that in many respects it's not up to the task of analyzing modern machine learning algorithms, but nevertheless I think it gives useful insights into into learning.",
                    "label": 0
                },
                {
                    "sent": "And then that will be mainly the topic for today as I'll show you when I start giving my summaries and so on.",
                    "label": 0
                },
                {
                    "sent": "But tomorrow I'll then pick up with more recent development of Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "Again, not very recent, but it's a slightly more refined method of analyzing the generalization of learning systems, and again has a nice link across from the pact learning, and I think again provides insights into the way.",
                    "label": 0
                },
                {
                    "sent": "Maybe traditional statistics and and learning and to some extent Bayesian methods fit together into into a bigger picture, so I'm hoping to paint it sort of these connect things together for you.",
                    "label": 0
                },
                {
                    "sent": "As much as I can, but also tell you some of the key ideas behind a lot of these original techniques.",
                    "label": 0
                },
                {
                    "sent": "So this is the way I've structured it and party is today part be tomorrow, so I'll give this more sort of general introduction.",
                    "label": 0
                },
                {
                    "sent": "Then these pack ideas and their generalization to real valued function classes.",
                    "label": 0
                },
                {
                    "sent": "And then tomorrow pick up with the Rademacher complexity and its application to classification.",
                    "label": 0
                },
                {
                    "sent": "I should say that my main focus will be on classification, particularly Rademacher theory can be very naturally applied to other problems such as regression, but and also unsupervised learning, but I think I won't have time unfortunately to cover any of that.",
                    "label": 0
                },
                {
                    "sent": "So sort of overarching, as I've already said, aims are sort of some thoughts on why theory introduced introduction of the basic techniques, with some deference to history.",
                    "label": 0
                },
                {
                    "sent": "And perhaps this is my main hope is that I will provide some insight into the proof techniques.",
                    "label": 0
                },
                {
                    "sent": "I don't want to give you just results.",
                    "label": 0
                },
                {
                    "sent": "I don't want to give you you nasty detailed proofs, but I would like to sort of give you a feel of what's behind some of the proofs and some of the techniques, because you may find that they spark new algorithms, new ideas, new analysis in your minds for something that you're doing or interested in at the moment.",
                    "label": 0
                },
                {
                    "sent": "'cause I think a lot of these are quite generic techniques and have many.",
                    "label": 0
                },
                {
                    "sent": "Potential uses elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of my main aim.",
                    "label": 0
                },
                {
                    "sent": "If you like as I said, I'll talk about the Rademacher tomorrow and concentration inequalities and how they link together.",
                    "label": 0
                },
                {
                    "sent": "What I won't be doing is most general results.",
                    "label": 0
                },
                {
                    "sent": "As I've already said.",
                    "label": 0
                },
                {
                    "sent": "You know, Tisis results or that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "I won't be attempting a complete history in any sense, and apologies right away if you feel that I've missed out key ideas or key things that you know about or even have been involved in.",
                    "label": 0
                },
                {
                    "sent": "I lecture of this sort.",
                    "label": 0
                },
                {
                    "sent": "One has to pick and choose.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid I won't be talking about Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "I certainly don't want to.",
                    "label": 0
                },
                {
                    "sent": "You know that for me is part of learning theory, but as I said, I'm going to concentrate on statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "I'm not also going to talk about the most recent developments.",
                    "label": 0
                },
                {
                    "sent": "Which are, you know, things like the PAC Bayes analysis, local Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "I mean they're not that recent, but more recent.",
                    "label": 0
                },
                {
                    "sent": "And these particularly.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'll allude to at one point, but it's a very nice link between the sort of Bayesian and the and the statistical learning theory approaches.",
                    "label": 0
                },
                {
                    "sent": "So here we are party OK so others said I'll start with some thoughts about what theory should be doing for us and why we're doing theory.",
                    "label": 0
                },
                {
                    "sent": "You know, I don't think I'm.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm sort of pushing it an open door.",
                    "label": 0
                },
                {
                    "sent": "I think most people appreciate the need for theory, but but sometimes it's good to sort of reflect on why.",
                    "label": 0
                },
                {
                    "sent": "What are we trying to achieve with this?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Is it really useful and what's the what's the purpose?",
                    "label": 0
                },
                {
                    "sent": "So in statistical learning theory, the main idea is to view learning from a statistical viewpoint.",
                    "label": 0
                },
                {
                    "sent": "By that I mean that you know the core of much of learning is about trying to find out some truths about the world or about the phenomenon you're observing from a finite sample.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the key sort of ingredient here.",
                    "label": 0
                },
                {
                    "sent": "You're taking some observations, a finite sample of observation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you're hoping from that finite sample to learn something that's actually true about the generating process that gave you that sample.",
                    "label": 0
                },
                {
                    "sent": "And so you sort of going from particular to general in that sense.",
                    "label": 0
                },
                {
                    "sent": "And of course, the danger is that you misinterpret some feature of the sample as something that's fundamental and true, whereas actually it's just a coincidence of that particular sample that you had that particular.",
                    "label": 0
                },
                {
                    "sent": "Feature present so the aim of learning theory is to try and gauge that likelihood that you're being misled in some sense by the sample to conclude something that's actually not true from, you.",
                    "label": 0
                },
                {
                    "sent": "Know some specific configuration of the data that you happen to observe.",
                    "label": 0
                },
                {
                    "sent": "Of course, you could take a second sample and do the same thing, but your that would just check for one one sort of thing you might be interested in, but in learning we're generally interested in checking for a.",
                    "label": 0
                },
                {
                    "sent": "A whole swathe of properties at the same time you know trying to fit a whole function class and picking out the best function in the whole class to that data.",
                    "label": 0
                },
                {
                    "sent": "So we need to be able to check if you like how we are being misled on any of these functions, not just on one of them.",
                    "label": 0
                },
                {
                    "sent": "And that's the key behind much of the analysis is trying to see how much information we can actually squeeze out of us finite sample that will allow us to be confident about our conclusions that we make over a whole swathe.",
                    "label": 0
                },
                {
                    "sent": "Of possible functions or analysis that we might make.",
                    "label": 0
                },
                {
                    "sent": "So in many cases in many ways it's connected to multiple hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with statistics of hypothesis testing where you might have a hypothesis you want to test, and you observe a sample and you see whether it's likely that hypothesis would be true, given that sample and you might do multiple hypothesis testings and in a way machine learning can be seen as doing very very many multiple hypothesis tests.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to do that in an efficient and reliable way.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's just .1.",
                    "label": 0
                },
                {
                    "sent": "There is 1.2 aim of any theories to model real world, an artificial phenomena because we want to exploit them.",
                    "label": 1
                },
                {
                    "sent": "We want to understand why things are the way they are.",
                    "label": 0
                },
                {
                    "sent": "In order to lever that knowledge.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we can be confident that this inference that we're making from a finite sample is real, then we can be exploit that to analyze new data and.",
                    "label": 0
                },
                {
                    "sent": "Get accurate predictions on new data.",
                    "label": 0
                },
                {
                    "sent": "Until we have that confidence, then we're in the risk of not being able to accurately predict on new data.",
                    "label": 1
                },
                {
                    "sent": "So statistical learning theory is just one approach to understanding predicting exploiting learning systems.",
                    "label": 1
                },
                {
                    "sent": "Others include as I've already mentioned Bayesian Inference Inductive Inferences.",
                    "label": 0
                },
                {
                    "sent": "Another statistical physics has been used in this connection and traditional statistical analysis, so there are many competing theories here.",
                    "label": 0
                },
                {
                    "sent": "And I'm certainly not trying to imply it's discal.",
                    "label": 0
                },
                {
                    "sent": "Any theory is the best in any sort of absolute sense.",
                    "label": 0
                },
                {
                    "sent": "It has.",
                    "label": 0
                },
                {
                    "sent": "Each theory will have its own strengths and weaknesses.",
                    "label": 0
                },
                {
                    "sent": "So each theory will make some assumptions about the phenomenon of learning.",
                    "label": 0
                },
                {
                    "sent": "And based on these derived predictions of behavior.",
                    "label": 0
                },
                {
                    "sent": "And so I've already sort of alluded to every predictive theory automatically implies a possible algorithm, because if you know about your prediction, then you can simply optimize the quantities that improve the predictions.",
                    "label": 0
                },
                {
                    "sent": "So if you know, for instance, that the number of features will impact on the quality of your learning, then you can try.",
                    "label": 0
                },
                {
                    "sent": "You know, the smaller the better.",
                    "label": 0
                },
                {
                    "sent": "Then you can try and keep the number of features small and very valga rhythms that do that.",
                    "label": 0
                },
                {
                    "sent": "Or if you if you know your VC dimension is some critical feature, you try and keep that small while obviously getting a good fit on the data.",
                    "label": 0
                },
                {
                    "sent": "So these are these are sort of natural consequences of any theory and this would be my argument about why theory is good.",
                    "label": 0
                },
                {
                    "sent": "Theory is good if it can drive you to new and better algorithms that will perform more efficient and more efficiencies.",
                    "label": 0
                },
                {
                    "sent": "And dealt with here, but more more.",
                    "label": 0
                },
                {
                    "sent": "With higher predictive accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the theory is have strengths and weaknesses as I've already mentioned.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking, the more assumptions you make, the more act.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The predictions or the more sort of refined predictions you can make, but of course you're.",
                    "label": 0
                },
                {
                    "sent": "Predictions will be dependent on those assumptions being correct or approximately correct.",
                    "label": 0
                },
                {
                    "sent": "I mean most assumptions that you make can never be exactly verified and probably aren't true.",
                    "label": 0
                },
                {
                    "sent": "So the question is, have you abstracted some assumptions that are reasonably good and actually capture the key phenomenon of what's going on, or have you put in some if you like poison into the mix?",
                    "label": 0
                },
                {
                    "sent": "That actually is completely mis skewing your analysis and making your predictions.",
                    "label": 0
                },
                {
                    "sent": "Inaccurate.",
                    "label": 0
                },
                {
                    "sent": "So it depends on capturing the right aspects of the phenomenon.",
                    "label": 1
                },
                {
                    "sent": "And obviously you want to try and keep it as general as possible, but if it's too general then you won't be able to say very much, and in a way you know that I'll make the distinction between the Bayesian and perhaps statistical models.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian typically will make more assumptions about some prior distributions and noise models, but in many cases they seem to be maybe not exactly right, but good enough.",
                    "label": 0
                },
                {
                    "sent": "And you get good results out.",
                    "label": 0
                },
                {
                    "sent": "Statistical learning theory makes rather less assumptions in that it just assumes an underlying distribution and that the training data is drawn IID independently, identically from that distribution.",
                    "label": 0
                },
                {
                    "sent": "So it seems like a weaker assumption, and typically you may get weaker results for that reason, but.",
                    "label": 0
                },
                {
                    "sent": "In fact, you do get surprisingly strong results given the weakness of those assumptions, but even those assumptions are not necessarily going to be true in any practical application.",
                    "label": 0
                },
                {
                    "sent": "It may not be the case that your training data is independently generated.",
                    "label": 0
                },
                {
                    "sent": "There may be connections between the way you've generated the data and so on, but if we take a sort of simple case of trying to classify cancerous tissue from healthy tissue, we can imagine there are two distributions, cells that are generated from that are actually not cancerous, and those that are cancerous.",
                    "label": 0
                },
                {
                    "sent": "And assuming that there's some process by which we select individuals, take samples from those individuals and measure those cells, then we might expect that those samples are indeed IID and we just have a random sample.",
                    "label": 0
                },
                {
                    "sent": "But you can, I'm sure, think of ways in which that wouldn't be the case very quick.",
                    "label": 0
                },
                {
                    "sent": "You know, just subtle effects that would come from particular population of visiting a hospital, etc, etc.",
                    "label": 0
                },
                {
                    "sent": "So in these in this sort of theory of statistical learning theory, we're sort of making that distribution.",
                    "label": 0
                },
                {
                    "sent": "The repository of all that we're interested in everything that we're interested in is represented in that distribution.",
                    "label": 0
                },
                {
                    "sent": "It's the way in which the processes of the natural artificial world we're studying are given to us through that distribution, and we access it.",
                    "label": 0
                },
                {
                    "sent": "As I've already said, through a training sample.",
                    "label": 0
                },
                {
                    "sent": "So it's a set of data, and I'm already thinking.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The classification, or at least the supervised learning case where we have an input and an output.",
                    "label": 0
                },
                {
                    "sent": "We're interested in predicting.",
                    "label": 0
                },
                {
                    "sent": "So here's the first example an here's the NTH example.",
                    "label": 0
                },
                {
                    "sent": "I tend to use M as the number of training examples, which is at odds with the way statisticians.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work, I'm afraid, and some other learning theories, but.",
                    "label": 0
                },
                {
                    "sent": "It may be worth trying to make that more coordinated with other, with statisticians perhaps at some point.",
                    "label": 0
                },
                {
                    "sent": "Anyway, this is the training sample which we assume is generated IID from that underlying distribution P, and so really the task of learning is saying I've got this finite set of information about which is sort of colored by P and what I want to do is learn something about P. That's true in general, independent of that particular sample.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk now about the generalization of a learner, because this isn't a way for classification and probably progression.",
                    "label": 0
                },
                {
                    "sent": "Two is the key concept that we are interested in in statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to spend a little bit of time looking at it, and I'm also going to plot some curves to show how it looks for some learning problems.",
                    "label": 0
                },
                {
                    "sent": "So I think that will give us hopefully a kind of insight into what learning theory is actually trying to give us.",
                    "label": 1
                },
                {
                    "sent": "So if we have a learning algorithm A, then what it does is it takes a training set S and typically selects from some function space FA function that it considers the right.",
                    "label": 0
                },
                {
                    "sent": "Function for that training set so it does a best fit.",
                    "label": 0
                },
                {
                    "sent": "It may be an empirical risk.",
                    "label": 0
                },
                {
                    "sent": "It may be a regularised risk or whatever.",
                    "label": 0
                },
                {
                    "sent": "Some method that has has built into it that selects a function to best fit from that training set.",
                    "label": 0
                },
                {
                    "sent": "And now what we're interested in is how well that function performs on new data.",
                    "label": 0
                },
                {
                    "sent": "How really, how really good is that function?",
                    "label": 0
                },
                {
                    "sent": "OK may fit on the training set, but is it good on new data?",
                    "label": 0
                },
                {
                    "sent": "And this is the idea of the generalization and so.",
                    "label": 0
                },
                {
                    "sent": "This is this quantity epsilon of SAF that I I'm writing here, which is the expectation under a randomly generated nupoint new test point of the loss of that function when applied to X and compared with the correct output Y.",
                    "label": 0
                },
                {
                    "sent": "So X&Y are the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A given input output generated according to the distribution AFS is the function we learned and the loss is telling us how well that function fits.",
                    "label": 0
                },
                {
                    "sent": "So this is the expectation if you like of our loss as we start to use that.",
                    "label": 0
                },
                {
                    "sent": "That learned function in in practice.",
                    "label": 0
                },
                {
                    "sent": "And if we think of classification, the loss would just be one.",
                    "label": 0
                },
                {
                    "sent": "If the two values disagree.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And zero otherwise, while for regression it might be the squared difference between the function applied to X and the correct output.",
                    "label": 1
                },
                {
                    "sent": "So this random variable is we're going to call this the generalization of the learner, but note that it is a random variable because it depends on the training set S, which is a random quantity.",
                    "label": 1
                },
                {
                    "sent": "Remember, it's generated IID from that same distribution, so we now have a random variable which we're thinking of as the generalization, so it's something that will depend on the training set and what we're really interested in is not being screwed up by a training set.",
                    "label": 0
                },
                {
                    "sent": "So we want to guard against being screwed up by a training set in this in this measure.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is show you now an example of where things go wrong.",
                    "label": 0
                },
                {
                    "sent": "If we have a rather simplistic learning algorithm and how you know we can get quite bad performance from different training sets.",
                    "label": 0
                },
                {
                    "sent": "And so this is taking this idea of the breast cancer datasets from UCI, and we're just going to use a very simple parsing window classifier.",
                    "label": 0
                },
                {
                    "sent": "W Plus is the average of the positive training examples W minus average of the negative training examples, and just create a weight vector between those two.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, if we.",
                    "label": 0
                },
                {
                    "sent": "You know, if you think there's a positives here.",
                    "label": 0
                },
                {
                    "sent": "And the negatives here.",
                    "label": 0
                },
                {
                    "sent": "Here is the average of the negative sees the average of the positives.",
                    "label": 1
                },
                {
                    "sent": "Draw a vector between them and take the bisecting hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That's the classifier that I'm thinking of here so very simplistic classifier.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We're going to apply it to this training data now.",
                    "label": 0
                },
                {
                    "sent": "We wanted to actually measure the expected value of on a randomly drawn test point, so I'm just going to use the.",
                    "label": 0
                },
                {
                    "sent": "You know, the second half of the sample that we didn't use for training as the testing half.",
                    "label": 0
                },
                {
                    "sent": "So it's an A proxy for the true generalization error in this case.",
                    "label": 0
                },
                {
                    "sent": "And that's the way I'm going to estimate this quantity.",
                    "label": 0
                },
                {
                    "sent": "An I'm going to do it for different training set sizes.",
                    "label": 0
                },
                {
                    "sent": "This is half of the data.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is, I'm going to repeatedly draw samples of these, say this size here.",
                    "label": 0
                },
                {
                    "sent": "342 use that algorithm.",
                    "label": 0
                },
                {
                    "sent": "To generate a classifier and then test it on the remaining data.",
                    "label": 0
                },
                {
                    "sent": "So every time I do that, I get one generalization error 1 sample from that random variable here.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I'm assuming M fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm choosing the sample sizes fixed here, but obviously every time I draw a sample of that size I get a different value of this epsilon.",
                    "label": 0
                },
                {
                    "sent": "So I'm now going to repeatedly draw that, you know, say 1010 thousand times, and I'm going to keep a histogram of those values so we can see the kind of errors we're getting and the range of errors we get from different training sets.",
                    "label": 0
                },
                {
                    "sent": "We may be lucky on some training sets to do very well on some we do very badly, and so on, just to sort of give you an idea of how well this particular algorithm can do.",
                    "label": 0
                },
                {
                    "sent": "I'm going at first show you.",
                    "label": 0
                },
                {
                    "sent": "The performance on the complete training set, so there's you know it's just test error in this case.",
                    "label": 0
                },
                {
                    "sent": "So that's this here.",
                    "label": 0
                },
                {
                    "sent": "So the sort of .15 is approximately the error you get if you just train on all of the data.",
                    "label": 0
                },
                {
                    "sent": "Note that the IC.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Effective value of this classifier is actually constant throughout training set sizes, because it's just the expectation of the positive data minus the expectation of the negative data.",
                    "label": 1
                },
                {
                    "sent": "So it's just the expected average.",
                    "label": 0
                },
                {
                    "sent": "If there's a cloud here.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Data that could be.",
                    "label": 0
                },
                {
                    "sent": "Have been generated.",
                    "label": 0
                },
                {
                    "sent": "You know this is a finite sample from that cloud.",
                    "label": 1
                },
                {
                    "sent": "This average the expected value of this for a particular training set is the same as the expected value for a.",
                    "label": 0
                },
                {
                    "sent": "The data, so this expected value of this classifier is actually.",
                    "label": 1
                },
                {
                    "sent": "The same whatever the training set sizes, so we're only talking about variance here in the actual performance.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "How does it?",
                    "label": 1
                },
                {
                    "sent": "How does it vary?",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to show you these plots, so here's you know, half the training set and here's the range you get.",
                    "label": 0
                },
                {
                    "sent": "So here's that average, now of accords very closely to the average.",
                    "label": 0
                },
                {
                    "sent": "You know this point 15.",
                    "label": 0
                },
                {
                    "sent": "Here you can see it's almost identical, but we're getting range from below .1 up to just above .2.",
                    "label": 0
                },
                {
                    "sent": "As of you know, the variance in the actual.",
                    "label": 0
                },
                {
                    "sent": "Error that we get.",
                    "label": 0
                },
                {
                    "sent": "So some training set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting as high errors .2 and some is getting it lowers .09, so there's a bit of variance there.",
                    "label": 0
                },
                {
                    "sent": "Just to remind you, know what we're interested in, you don't.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A chance to sort of try a few training sets.",
                    "label": 0
                },
                {
                    "sent": "You just get one training set so you don't want to be in this situation.",
                    "label": 0
                },
                {
                    "sent": "Out here you want to be guarding against that situation and that's you know the danger is that you with one training set, could be anywhere in this range.",
                    "label": 0
                },
                {
                    "sent": "And if there's a large sort of tail here, you're in danger of being in a bad situation.",
                    "label": 0
                },
                {
                    "sent": "So here if I just now start to reduce the training set size, you can start to see you know a growth of some tail here and it starts to even with size 25 you know you're already in creepage over over the .2.",
                    "label": 0
                },
                {
                    "sent": "And quite a large proportion of the of the training sets are giving errors.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As high as you know, quite a large number around .2.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The average note is still .15, so if you're just talking about the expected value of your error on a randomly drawn training.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set find no problems, looks good.",
                    "label": 0
                },
                {
                    "sent": "But you could quite likely be in a pretty bad situation relative to that.",
                    "label": 0
                },
                {
                    "sent": "Quite a bad, not very bad, but you know a bit quite a bit worse now things even with 68 start to get quite significantly worse.",
                    "label": 0
                },
                {
                    "sent": "You know there's a large number of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training sets that are landing you in errors over .25 know the average is still pretty close to .15, maybe .16 now?",
                    "label": 0
                },
                {
                    "sent": "And things start.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To deteriorate further, we're even getting ones training set here that had worse than random.",
                    "label": 0
                },
                {
                    "sent": "The average is still below.",
                    "label": 0
                },
                {
                    "sent": "Well, it's about .18 now .17.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And things sort of then start to deteriorate badly.",
                    "label": 0
                },
                {
                    "sent": "And we can see the former.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As the number of training examples drops to about the dimension of the space, things start to look very bad OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The point there I was trying to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To sort of push home is this idea that working with the AH.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Average the expected value of that generalization error on a randomly drawn training set could be dangerous.",
                    "label": 0
                },
                {
                    "sent": "You know it could look actually quite good.",
                    "label": 0
                },
                {
                    "sent": "Some algorithm that optimize that could look quite good.",
                    "label": 0
                },
                {
                    "sent": "In fact, that algorithm doesn't seem to be doing too badly, but actually the variance of those errors is pretty high, and that could mean that on any particular training set that you got when you really were working in practice, you could be a lot worse than your.",
                    "label": 0
                },
                {
                    "sent": "Bound was suggesting or your sort of algorithm was suggesting.",
                    "label": 0
                },
                {
                    "sent": "Now I here's a slight sort of.",
                    "label": 0
                },
                {
                    "sent": "Criticism of perhaps some more sort of traditional ways of working, which I mean traditional statistics, is generally worked on looking at the expected value of this error, and particularly how it goes with the sample size.",
                    "label": 0
                },
                {
                    "sent": "Does it converge to the Bayes risk?",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of consistency of A of a classifier.",
                    "label": 0
                },
                {
                    "sent": "If the as the sample size tends to Infinity, the expected value of this tends to the Bayes risk where the Bayes risk is.",
                    "label": 1
                },
                {
                    "sent": "Basically, the best you could do with the data if you have to make a choice, you choose the one for which the probability of that label is higher than the probability of the other label.",
                    "label": 0
                },
                {
                    "sent": "We're assuming you know there's possibly confusion that not.",
                    "label": 0
                },
                {
                    "sent": "There's not always a clean classification that both X1 and X0 could with arise with some probability.",
                    "label": 0
                },
                {
                    "sent": "So for a finite sample, the generalization has a distribution as I've shown you, and as I say, the mean of that distribution could be misleading, and I think you know low fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "Maybe an example where this this type of.",
                    "label": 0
                },
                {
                    "sent": "High variance might occur.",
                    "label": 0
                },
                {
                    "sent": "So one is statistical learning theory does is try to work with that complete distribution or in particular look at the tail of that distribution and it tries to.",
                    "label": 0
                },
                {
                    "sent": "Find a bound which holds with high probability on the tail of that distribution.",
                    "label": 0
                },
                {
                    "sent": "So what is the way it looks a bit like a statistical test?",
                    "label": 0
                },
                {
                    "sent": "It's sort of putting a number.",
                    "label": 0
                },
                {
                    "sent": "If I go back to one of those plots.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's sort of saying, let's say I say here OK sort of saying OK, I'm prepared to suffer, you know, a as in a statistical test you make a statistical test.",
                    "label": 0
                },
                {
                    "sent": "You say 1% confidence.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What that means is the conclusion you made could be wrong with 100 probability.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to suggest is the statistical learning theory does is say OK?",
                    "label": 1
                },
                {
                    "sent": "I'm going to make this conclusion about the generalization performance, but I could be wrong with probability one in 100, and that would mean that I was just unlucky with my training data.",
                    "label": 0
                },
                {
                    "sent": "My training data looked good.",
                    "label": 0
                },
                {
                    "sent": "I kind of got a good fit on it, but actually it was just very unrepresentative in some way.",
                    "label": 0
                },
                {
                    "sent": "And that means the chopping off a tale of this distribution.",
                    "label": 0
                },
                {
                    "sent": "It's sort of saying, OK, you know, let's ignore this little tale here.",
                    "label": 0
                },
                {
                    "sent": "These are just really bad luck.",
                    "label": 0
                },
                {
                    "sent": "You're always going to have bad luck.",
                    "label": 0
                },
                {
                    "sent": "I mean, you just may consider an IID sample.",
                    "label": 0
                },
                {
                    "sent": "You could just get you know the same example drawn each time, and it just doesn't tell you anything about.",
                    "label": 0
                },
                {
                    "sent": "I mean, obviously it's very unlikely, but it's still so you allow a sort of a tale to be thrown away.",
                    "label": 0
                },
                {
                    "sent": "And then you say what you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Given that I've thrown away that tail.",
                    "label": 0
                },
                {
                    "sent": "What is my upper bound on this distribution?",
                    "label": 0
                },
                {
                    "sent": "We generated this was this your resampling from the same complete set.",
                    "label": 0
                },
                {
                    "sent": "This re dividing it yes exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah this is so many other highly correlated right now it's true.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's my point about the proxy for the test is a bit cheating.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "I mean, you would really need to have a completely separate distribution and then subsample from.",
                    "label": 0
                },
                {
                    "sent": "I mean I could have done that, but the training set is quite small, so yeah, but ideally you would want to have it as a separate.",
                    "label": 0
                },
                {
                    "sent": "This is correlated here.",
                    "label": 0
                },
                {
                    "sent": "So it's not generalization error, it's it's test error.",
                    "label": 0
                },
                {
                    "sent": "Proxy is a proxy for the generalization.",
                    "label": 0
                },
                {
                    "sent": "Division for solve this error from its mean, which is the knowledge.",
                    "label": 0
                },
                {
                    "sent": "So I would I mean what I'm thinking of is the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Is this distribution complete?",
                    "label": 0
                },
                {
                    "sent": "But what I'm interested in is sort of putting an upper bound on this distribution, given that I'm allowing myself to throw away a tail.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort of saying with a certain confidence I can be, you know.",
                    "label": 0
                },
                {
                    "sent": "OK, I may be unlucky, but.",
                    "label": 0
                },
                {
                    "sent": "Barring that, I can be confident that the error will not be worse than this.",
                    "label": 0
                },
                {
                    "sent": "Quantity OK. Is that OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we.",
                    "label": 0
                },
                {
                    "sent": "So this is the source of this acronym.",
                    "label": 1
                },
                {
                    "sent": "Pak Pak stands for probably approximately correct.",
                    "label": 0
                },
                {
                    "sent": "The confidence parameter Delta is the size of the tail.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well that we cut off.",
                    "label": 1
                },
                {
                    "sent": "And that's the probable.",
                    "label": 0
                },
                {
                    "sent": "So it's the probability of the sort of the conclusion holding and with that probability the function we've got is approximately correct in the sense that it's error is better than that sort of bound on that on the distribution at that cutoff point.",
                    "label": 1
                },
                {
                    "sent": "So we're trying to bound the top end of that cut off, and obviously when I say bound that will potentially lead to algorithms that will optimize that and try to bring that bound as low as possible.",
                    "label": 1
                },
                {
                    "sent": "OK. OK, so that that sort of hopefully got you into the way of.",
                    "label": 1
                },
                {
                    "sent": "The Curious way of thinking of statistical learning theory and so now I'm going to sort of start putting in some sorry, some simple.",
                    "label": 0
                },
                {
                    "sent": "Ideas that show you how to work with this quantities and build up to this VC theory that indeed bounds this tail of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is that the key technique really is to always think of bounding the probability of being misled.",
                    "label": 0
                },
                {
                    "sent": "I think that's the way you need to.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About it, in order to get these, you know to get your head round how to actually do these bounds.",
                    "label": 0
                },
                {
                    "sent": "So what you're really interested in is the bounding the probability of the training set being a bad one.",
                    "label": 1
                },
                {
                    "sent": "So it's like hypothesis testing, so let's just take a very simple example.",
                    "label": 1
                },
                {
                    "sent": "Let's imagine there's just one function, and it's a bad function.",
                    "label": 0
                },
                {
                    "sent": "It's got error less bigger than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So that's what I mean by bad, but it looks good.",
                    "label": 1
                },
                {
                    "sent": "It's got zero training error.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of that occurring?",
                    "label": 0
                },
                {
                    "sent": "So it's here it is.",
                    "label": 0
                },
                {
                    "sent": "The probability again, is the probability over the generation of the sample.",
                    "label": 0
                },
                {
                    "sent": "So this probability over the generation of the sample.",
                    "label": 0
                },
                {
                    "sent": "Again, it's IID that the error actually is 0, but it's a bad function.",
                    "label": 0
                },
                {
                    "sent": "Well, because the sample is generated ID you can simply workout this probability very easily is just one minus the error of F. That's the probability that it looks good on that particular example and to the power M because they're independent.",
                    "label": 0
                },
                {
                    "sent": "It's M independent.",
                    "label": 0
                },
                {
                    "sent": "Tries that because this error F is bigger than epsilon is less than or equal to 1 minus epsilon to the M, and this is an approximation that's often used because it makes the analysis the mathematics easier is to approximate this upper bound.",
                    "label": 0
                },
                {
                    "sent": "This one minus epsilon by the exponential of minus epsilon N. You can see you know if you just add in here, 1 minus epsilon plus epsilon squared over 2 factorial minus epsilon cubed room.",
                    "label": 0
                },
                {
                    "sent": "Then this is E to the minus epsilon and that's either the minus epsilon episode.",
                    "label": 0
                },
                {
                    "sent": "So pretty simple approximation.",
                    "label": 0
                },
                {
                    "sent": "OK so notice how this is exponentially decaying with the sample size and this is clearly the thing, the kind of thing we want to see.",
                    "label": 0
                },
                {
                    "sent": "So if we choose epsilon to be logged one on T / M and just plug that in here.",
                    "label": 0
                },
                {
                    "sent": "Then we this will be equal to T here or you know think of log one on Delta if you like divided by M, this will be less than or equal to Delta, so this will tell us the if we take this epsilon then our probability of being misled is is T by that by a sample for that one function.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now what about?",
                    "label": 0
                },
                {
                    "sent": "Putting in a few more funk.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since we don't want to learn with just one function, we'd like to learn with a class of functions.",
                    "label": 0
                },
                {
                    "sent": "So let's now move to a finite or countable function class where we can enumerate the functions.",
                    "label": 1
                },
                {
                    "sent": "And we're going to think of the sort of dividing the.",
                    "label": 0
                },
                {
                    "sent": "Probability of being misled among these functions with a distribution QN.",
                    "label": 0
                },
                {
                    "sent": "Where we're sort of allowing dividing the Delta if you like among these functions with each function FN getting assigned QN fraction of the Delta.",
                    "label": 1
                },
                {
                    "sent": "Where we assume that the sum of the Q NS is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And then we can do what is known as a union bound by applying that previous result here.",
                    "label": 0
                },
                {
                    "sent": "For each function individually and bounding its probability is being less than QN Delta.",
                    "label": 0
                },
                {
                    "sent": "And therefore if we just sum up the probabilities of all of these, any of these misleading us we get because the sum of the cubes was one, we get Delta.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the likelihood that any of them mislead us.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there exists an FN for which it has zero training error, but its error is bigger than the corresponding epsilon, which is what I computed as one on M log one on Q and Delta.",
                    "label": 0
                },
                {
                    "sent": "That was just the.",
                    "label": 0
                },
                {
                    "sent": "This quantity, here with T substituted for by Q and Delta here so the the probability that any of these is doesn't hold is less than or equal to Delta.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if you're union bound is a simple idea, but it's just basically.",
                    "label": 0
                },
                {
                    "sent": "You know, if you think of a space here of fall.",
                    "label": 0
                },
                {
                    "sent": "Samples that you might generate.",
                    "label": 0
                },
                {
                    "sent": "There's a little region where function one might mislead us, and there's another region where function 2 might mislead us, and this has size Q 1D, and this has size Q.",
                    "label": 0
                },
                {
                    "sent": "2D.",
                    "label": 0
                },
                {
                    "sent": "They might of course overlap.",
                    "label": 0
                },
                {
                    "sent": "You know, Q 3D might overlap and so on.",
                    "label": 1
                },
                {
                    "sent": "But if the sum of all of these, assuming they don't overlap is less than Delta, then certainly if they do overlap, it's going to be less than Delta.",
                    "label": 0
                },
                {
                    "sent": "So that's just the basis of the Union bound.",
                    "label": 0
                },
                {
                    "sent": "Very simple idea.",
                    "label": 0
                },
                {
                    "sent": "And so we get the result there.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't take into account any connections between the functions.",
                    "label": 0
                },
                {
                    "sent": "This is assuming there in some way no relationship between them at all.",
                    "label": 0
                },
                {
                    "sent": "And clearly that's a weakness of this bound.",
                    "label": 0
                },
                {
                    "sent": "But we can turn it around and we get our first theorem.",
                    "label": 0
                },
                {
                    "sent": "So all of these are say when we do the analysis you want to think of it in bounding this probability of being misled, But this doesn't really read very well to a practitioner of machine learning, so you have to sort of turn it around and turn it into a.",
                    "label": 0
                },
                {
                    "sent": "Statement about the generalization performance.",
                    "label": 0
                },
                {
                    "sent": "So we do that by just saying OK. That",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the probability of being misled.",
                    "label": 0
                },
                {
                    "sent": "So with probability 1 minus that.",
                    "label": 0
                },
                {
                    "sent": "The content of that we weren't misled, right?",
                    "label": 0
                },
                {
                    "sent": "So with probability at least one minus Delta over the random M samples.",
                    "label": 1
                },
                {
                    "sent": "The generalization error function FN.",
                    "label": 0
                },
                {
                    "sent": "There is this nice relation if it has zero training error, then it's true.",
                    "label": 1
                },
                {
                    "sent": "Error is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, we weren't misled by any of the functions, so any function that has zero training error as this bound.",
                    "label": 0
                },
                {
                    "sent": "So this is a first learning theory bound and it has a very sort of standard form.",
                    "label": 0
                },
                {
                    "sent": "Nearly all learning theory, statistical learning theory bounds have something of that shape.",
                    "label": 0
                },
                {
                    "sent": "To them, they're typically one on M. They typically have a log, one on Delta, buried in there somewhere, and that's a nice feature because it means that the size of the sample you need in order to sort of.",
                    "label": 0
                },
                {
                    "sent": "Up your confidence about the result is very benign.",
                    "label": 0
                },
                {
                    "sent": "In other words, you know you can sort of have Delta or multiply Delta by 1 E and you only have to add 1 extra example.",
                    "label": 0
                },
                {
                    "sent": "So you're kind of in a very nice sort of benign situation.",
                    "label": 0
                },
                {
                    "sent": "As far as dependence on Delta is concerned.",
                    "label": 0
                },
                {
                    "sent": "Now the dependent.",
                    "label": 0
                },
                {
                    "sent": "The other term.",
                    "label": 0
                },
                {
                    "sent": "Here log one on QN is something you can think of as the complexity of the function FN that we're interested in, measured in some sort of.",
                    "label": 0
                },
                {
                    "sent": "Bit sore or E bits or whatever in the way that the distribution.",
                    "label": 0
                },
                {
                    "sent": "But we chose QN determine, so we're going to choose Q and we can think of this in a Bayesian sense as a prior over functions, we're thinking that they're more likely functions.",
                    "label": 0
                },
                {
                    "sent": "We're going to give higher Q NS-2 and going to suffer therefore a better bound here, but the ones that are less likely QM will be larger and we may be in a worse situation.",
                    "label": 0
                },
                {
                    "sent": "As far as this bound, but it's still log one on Q and so again it's quite a benign.",
                    "label": 0
                },
                {
                    "sent": "Entry so we can think of that as the sort of complexity or description length of the function FN.",
                    "label": 0
                },
                {
                    "sent": "Notice that we have to define that function QM before we see the data.",
                    "label": 0
                },
                {
                    "sent": "We can't cheat.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we did have a distribution that we knew that was the distribution governing the likelihood of different functions, then the expected generalization error will be best.",
                    "label": 1
                },
                {
                    "sent": "If we choose Q equal to P and that sort of Bayesian idea.",
                    "label": 0
                },
                {
                    "sent": "Indeed, this sort of is.",
                    "label": 0
                },
                {
                    "sent": "If you like the starting point of the PAC Bayes analysis, sort of the relationship between over a Bayesian prior and so on.",
                    "label": 1
                },
                {
                    "sent": "So can you clarify?",
                    "label": 0
                },
                {
                    "sent": "The sort of meaning of QM.",
                    "label": 0
                },
                {
                    "sent": "If I if I put a lot of mass on certain hypotheses.",
                    "label": 0
                },
                {
                    "sent": "Does this tell us how we should go about changing our learning procedure?",
                    "label": 0
                },
                {
                    "sent": "Potentially it does.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, potentially it's going to influence if you want to minimize the bound it's going to say look, I mean it's going to bias you as you would in a Bayesian sense towards using those functions which have high prior probability exactly.",
                    "label": 0
                },
                {
                    "sent": "So you know if you think of this as a driving an algorithm as you might do, you know you choose the function that minimize this.",
                    "label": 0
                },
                {
                    "sent": "Of course, we don't have a training area here, because it's a very simple case, but if you did add a training error and then it would appear as an extra term here, which would be typically in training error plus actually the square root of this, or some combination of that type would come in and so you would have a form where you would be trading off, making low error against.",
                    "label": 0
                },
                {
                    "sent": "Prior probability, so it's exactly the Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "Or regularise approach in that sense, yeah.",
                    "label": 0
                },
                {
                    "sent": "And indeed, if you if you do work out the expected loss compared to the expected loss you would if you had the true prior.",
                    "label": 0
                },
                {
                    "sent": "It's a KL divergent between the two distributions is what comes out is sort of again sort of pushing towards the PAC Bayes analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're not going to take that route.",
                    "label": 0
                },
                {
                    "sent": "As I say, you know, one route now would be just sort of follow the back bays route and look at how that develops, but I'm.",
                    "label": 0
                },
                {
                    "sent": "Sort of feeling the weight of history and I have to, you know, sort of go through the some of the key ideas that were developed and the first sort of.",
                    "label": 0
                },
                {
                    "sent": "Next step that was taken.",
                    "label": 0
                },
                {
                    "sent": "The really went beyond those simple counting arguments.",
                    "label": 0
                },
                {
                    "sent": "Was this idea of moving to infinite function classes in the VC dimension?",
                    "label": 0
                },
                {
                    "sent": "So that's what I want to talk about next.",
                    "label": 0
                },
                {
                    "sent": "I should say that you know you don't need the VC dimension to apply this sort of thing to say, you know algorithms that work over countable structures.",
                    "label": 0
                },
                {
                    "sent": "You can just apply.",
                    "label": 0
                },
                {
                    "sent": "You know this directly to that kind of.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe monomials or sorry, yeah.",
                    "label": 0
                },
                {
                    "sent": "Plus we will have you and close to zero and order from will be like things.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's well.",
                    "label": 0
                },
                {
                    "sent": "It depends how how large?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you need to reduce.",
                    "label": 0
                },
                {
                    "sent": "You know if you think of monomials over some.",
                    "label": 0
                },
                {
                    "sent": "Then it's not very big.",
                    "label": 0
                },
                {
                    "sent": "You know it's maybe 2 to the OR.",
                    "label": 0
                },
                {
                    "sent": "Do you include the monomial or not?",
                    "label": 0
                },
                {
                    "sent": "You know, maybe you have fixed number K monomial, so it's two to the K possible functions.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you can negate them so it's three to the K you know, so it's K log 3 and it's OK if you have a training set size that's bigger than the number of features.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "I mean if you go to yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah, OK, so the question was, you know?",
                    "label": 0
                },
                {
                    "sent": "Basically there's a danger that that for any significant size of function class this QN will be too small and log QM will be much bigger than M. That was, your question wasn't so.",
                    "label": 0
                },
                {
                    "sent": "I tried to give an example where that was in that case, yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry, assumption that is it.",
                    "label": 0
                },
                {
                    "sent": "Just the assumption that is being errors.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I just use that as an assumption here to simplify the analysis so you can do this with the non zero training.",
                    "label": 1
                },
                {
                    "sent": "I'll give an example of an Andrea or training error later, but I've kind of swept it under the carpet but you can do the whole thing with an zero training error and you get a similar sort of band.",
                    "label": 0
                },
                {
                    "sent": "OK, Yep.",
                    "label": 0
                },
                {
                    "sent": "Additional functions being drawn at random because the functions in practice will be determined by the data sample edges.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what does it mean to think of functions being generated at random?",
                    "label": 0
                },
                {
                    "sent": "You mean the QN?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, yeah?",
                    "label": 0
                },
                {
                    "sent": "Opinion right?",
                    "label": 0
                },
                {
                    "sent": "OK, well it's the Bayesian idea that there's some you know.",
                    "label": 0
                },
                {
                    "sent": "Underlying belief about the likelihood that different functions will arise in a particular scenario that you're studying.",
                    "label": 0
                },
                {
                    "sent": "So if you knew which function was going to arise, obviously don't need to learn right so?",
                    "label": 0
                },
                {
                    "sent": "You're done.",
                    "label": 0
                },
                {
                    "sent": "If you don't, then you may have some belief about what functions are likely or more likely to occur, and that is what PN is giving you.",
                    "label": 0
                },
                {
                    "sent": "Of course, you know there are various philosophical interpretations of what that belief means.",
                    "label": 0
                },
                {
                    "sent": "You know, because you can't repeat the experiment in some sense, but.",
                    "label": 0
                },
                {
                    "sent": "But you know, I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "Be able to give very informed views about that, but.",
                    "label": 0
                },
                {
                    "sent": "I mean, the algorithm is going to generate a function looking at the training.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly, but this is so.",
                    "label": 1
                },
                {
                    "sent": "The idea is that function that distribution is defined before you see the training example before you know when you arrive and you say.",
                    "label": 0
                },
                {
                    "sent": "I'm going to study, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Biological networks, you know?",
                    "label": 0
                },
                {
                    "sent": "Then I have some prior knowledge that tells me the likelihood of certain functions arising.",
                    "label": 0
                },
                {
                    "sent": "An I have to think of all that and put it out there ahead of time and then I get my training sample and I try and.",
                    "label": 0
                },
                {
                    "sent": "You know, find the best function.",
                    "label": 0
                },
                {
                    "sent": "Biased with that prior probability.",
                    "label": 0
                },
                {
                    "sent": "Anything that's not make sense to me that you said that we assume that we don't know the probability distribution of the training sample that you don't know the probability of Y given X, right?",
                    "label": 0
                },
                {
                    "sent": "So it seems to me that for any given algorithm, the probability of a function being coming up or coming out of the algorithm is going to be a function of that distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course it is, but I think the.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that you know very little about.",
                    "label": 0
                },
                {
                    "sent": "I mean, the assumption there is only the relationship between X&Y that you're assuming you know something about.",
                    "label": 0
                },
                {
                    "sent": "You're not assuming anything about the distribution.",
                    "label": 0
                },
                {
                    "sent": "Of X, so there I guess is where you know, yeah.",
                    "label": 0
                },
                {
                    "sent": "The confusion is that in your original setup you say we have a fixed speed, right?",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And for any P, there's an optimal F. So yeah, I guess now if you're saying that we have a prior over functions, I think maybe we have a prior possible peas which will give up the mall F. So this gives a prior auth.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Error in this framework 'cause there's no one fixed.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I think it's only a prior over the relationship between X&Y that the PES will satisfy.",
                    "label": 0
                },
                {
                    "sent": "It's not a it's not an assumption about the distributions over the.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution of X.",
                    "label": 0
                },
                {
                    "sent": "There's no assumption made about that.",
                    "label": 0
                },
                {
                    "sent": "You said, I mean, I mean what you're doing saying is I'm expecting these types of functions to arise.",
                    "label": 0
                },
                {
                    "sent": "Those functions will tell us given an X, how to get the Y out, but they won't tell you anything about the distribution over X independently of wire.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's the same answer I just gave a moment ago.",
                    "label": 0
                },
                {
                    "sent": "But we can talk about it offline, maybe, but.",
                    "label": 0
                },
                {
                    "sent": "You're not convinced I can see, so yeah, I'll move on if it will come back in.",
                    "label": 0
                },
                {
                    "sent": "How exactly is he changed?",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "Right, so OK, I unfortunately don't have this.",
                    "label": 0
                },
                {
                    "sent": "You know what I would need is the bound with the training error in here.",
                    "label": 0
                },
                {
                    "sent": "But imagine there's a term in here with the training error plus this quantity here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if I have a particular choice of Qi will minimize this combination and one choice of Q will bias me towards using a function.",
                    "label": 0
                },
                {
                    "sent": "So maybe I have two functions that are roughly equivalent in terms of training error and in one of the choices of Q.",
                    "label": 0
                },
                {
                    "sent": "One is preferred and then the other choice of Q.",
                    "label": 0
                },
                {
                    "sent": "The other is preferred so it would change that.",
                    "label": 0
                },
                {
                    "sent": "Why you need?",
                    "label": 1
                },
                {
                    "sent": "So the learning algorithm takes is a function from training sets to a choice of an element to that right.",
                    "label": 0
                },
                {
                    "sent": "Yes, where does the Q?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, exactly exactly.",
                    "label": 0
                },
                {
                    "sent": "It's sort of regularised.",
                    "label": 0
                },
                {
                    "sent": "Or you know.",
                    "label": 0
                },
                {
                    "sent": "Biased algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is exactly what the learning algorithm is for.",
                    "label": 0
                },
                {
                    "sent": "Well, in that case, it would simply be minimizing an algorithm that would minimize this expression together with the training error, so it would find the function in the class that would have.",
                    "label": 0
                },
                {
                    "sent": "You know the the combination of training error plus this quantity would be minimized.",
                    "label": 1
                },
                {
                    "sent": "But the bound holds even if you don't use that algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly the bound is true.",
                    "label": 1
                },
                {
                    "sent": "Even your learning algorithm how if you define no, you have to define Q in order to define the bound.",
                    "label": 0
                },
                {
                    "sent": "Given Q, you have a bound given another Q, you have a different bound, but alright.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm is just going to be to put my training set into some computer program black box and it spits out a choice of action, right?",
                    "label": 0
                },
                {
                    "sent": "Does this result apply to that?",
                    "label": 0
                },
                {
                    "sent": "You have to specify it before.",
                    "label": 0
                },
                {
                    "sent": "I mean you just.",
                    "label": 0
                },
                {
                    "sent": "You can specify in any way you want.",
                    "label": 0
                },
                {
                    "sent": "But I don't know it's a black box and just putting things in, you know there's this one thing is the algorithm, the other is the bound, right?",
                    "label": 0
                },
                {
                    "sent": "The algorithm you just plug it in and outcomes the function in order to get the bound a number out from the bound, you have to feed into it a queue.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you can choose your Q.",
                    "label": 0
                },
                {
                    "sent": "How do you like ahead of time but you have to do it ahead of time.",
                    "label": 0
                },
                {
                    "sent": "That's the only.",
                    "label": 1
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "I think confusion that if you actually join your function to have your training at a dealer is bounded.",
                    "label": 0
                },
                {
                    "sent": "But if you actually function from their class of functions that minimize the training error on this particular training sample.",
                    "label": 0
                },
                {
                    "sent": "Then the body.",
                    "label": 0
                },
                {
                    "sent": "This is your bigger function function from a quality distribution que no.",
                    "label": 0
                },
                {
                    "sent": "No no no.",
                    "label": 0
                },
                {
                    "sent": "No, no, this is true for all functions in the class.",
                    "label": 0
                },
                {
                    "sent": "Debating, yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "'cause it's?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the whole idea of it being uniform.",
                    "label": 0
                },
                {
                    "sent": "It holds for every function in the class.",
                    "label": 0
                },
                {
                    "sent": "OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "For the optimal percent you through the frequencies of particular algorithms or functions being the result, then yeah, I think that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "That's exactly right.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah.",
                    "label": 0
                },
                {
                    "sent": "How do you mean?",
                    "label": 0
                },
                {
                    "sent": "What do you mean by that?",
                    "label": 0
                },
                {
                    "sent": "You know what I mean?",
                    "label": 0
                },
                {
                    "sent": "You have to set an example of learning that would somehow.",
                    "label": 0
                },
                {
                    "sent": "But yes, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "I like it more questions.",
                    "label": 0
                },
                {
                    "sent": "This is good.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Which the.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean they could, you could envisage.",
                    "label": 0
                },
                {
                    "sent": "Trying to I mean learn if you do it by learning algorithms like learning the prior, you know it's like having a hyper prior and they're only based on the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Little Network The Accent Qi or yeah, So what you mean?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, as I say, it's your prior knowledge being fed into that Q.",
                    "label": 0
                },
                {
                    "sent": "For example, I have.",
                    "label": 0
                },
                {
                    "sent": "Messages.",
                    "label": 0
                },
                {
                    "sent": "It's it's not so much that there will be no error.",
                    "label": 0
                },
                {
                    "sent": "It's it's a question of which functions you think will have low error or you know, I mean the 0 error case is probably a bit misleading anyway, 'cause you're not going to experience them.",
                    "label": 0
                },
                {
                    "sent": "Really good things.",
                    "label": 0
                },
                {
                    "sent": "Decided.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, you know, let's say you had a tree structure.",
                    "label": 0
                },
                {
                    "sent": "Your qis would typically be higher for sort of, you know, shallow trees rather than deep trees.",
                    "label": 0
                },
                {
                    "sent": "Just the natural thing you would expect because you hoping to find a simple measure, a simple.",
                    "label": 0
                },
                {
                    "sent": "But you know, you can actually choose them how you want.",
                    "label": 0
                },
                {
                    "sent": "You could choose one deep tree to have a very low Q if you want.",
                    "label": 0
                },
                {
                    "sent": "A very high Q. I'm low one.",
                    "label": 0
                },
                {
                    "sent": "Talk to you.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "I guess here one of the issues is that if you so cute you're saying Q is chosen according to your.",
                    "label": 0
                },
                {
                    "sent": "The function you think would be more likely in your settings, so in some sense, if you choose a bad queue for your framework, your bounds won't be very useful exactly, and so maybe one way to look at this is that.",
                    "label": 0
                },
                {
                    "sent": "For each problem, so each.",
                    "label": 0
                },
                {
                    "sent": "There, so each problem distribution defined.",
                    "label": 0
                },
                {
                    "sent": "Of them all type of functions that could be why given.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For each but specific you there will be some.",
                    "label": 0
                },
                {
                    "sent": "Y given X for which this bond would be extremely loose and probably believe misleading an and for the.",
                    "label": 0
                },
                {
                    "sent": "For the white given X, which are well matched by the kind of functions we get from this bound, then those bound would be probably very precise, and if you optimize that naturally it will get very good function is that I think that's fair.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I think that's fine.",
                    "label": 0
                },
                {
                    "sent": "I mean, in a way it's like doing Bayesian inference, but being told when it's not working, you know.",
                    "label": 0
                },
                {
                    "sent": "As opposed to just doing it, hoping it works.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not that's not meant to be criticism, it's just you know, if you like, that's the advantage.",
                    "label": 0
                },
                {
                    "sent": "I mean, that was one reason why Matthias Seeger did the PAC.",
                    "label": 0
                },
                {
                    "sent": "Bayes analysis was precisely too.",
                    "label": 0
                },
                {
                    "sent": "Sort of understand when the Gaussian process classification was.",
                    "label": 0
                },
                {
                    "sent": "Was was reliable in some sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yeah, another question.",
                    "label": 0
                },
                {
                    "sent": "Can we take that offline?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Same question.",
                    "label": 0
                },
                {
                    "sent": "Possible I think you right.",
                    "label": 0
                },
                {
                    "sent": "What you're thinking out?",
                    "label": 0
                },
                {
                    "sent": "There's another one, right?",
                    "label": 0
                },
                {
                    "sent": "So say if it was possible, polynomial functions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could say that Q is one over the maximal power, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually you're jumping a little bit ahead there in what you are there.",
                    "label": 0
                },
                {
                    "sent": "Taking a whole class of functions and giving them as well as he is, and it will only one function, but exactly the same thing applies.",
                    "label": 0
                },
                {
                    "sent": "Yes exactly, so Q would then be one on the number of, but you have to make sure there's some of the cues.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "I'll mention that a bit later we'll come back to that if we get OK. Good, I'm going to go ahead.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is we have uncountably many functions, so the key idea here we can't do a union bound over infinite number of functions, because clearly.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So we want to somehow get down to a finite sample.",
                    "label": 0
                },
                {
                    "sent": "Uh, sorry a finite set of functions.",
                    "label": 0
                },
                {
                    "sent": "And how are we going to do that?",
                    "label": 0
                },
                {
                    "sent": "So we have to somehow kind of replace, you know groups of functions by one representative function, and then you know that representative will hopefully take care of the whole group.",
                    "label": 0
                },
                {
                    "sent": "And the way we try and do that is by.",
                    "label": 0
                },
                {
                    "sent": "Actually getting down to a finite sample of a finite set of examples and then by classification you can only have a finite set of things happening on that sample, because you can only do plus minus ones on those examples.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that we always have this test point which we don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is replace the test point by ago sample.",
                    "label": 0
                },
                {
                    "sent": "And then we can sort of argue just in terms of two samples drawn from that distribution.",
                    "label": 0
                },
                {
                    "sent": "And then it's a finite set.",
                    "label": 1
                },
                {
                    "sent": "So this is known as the double sample trick.",
                    "label": 0
                },
                {
                    "sent": "An effect.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tively gets you from dealing with an unknown test example to just dealing with two random samples.",
                    "label": 0
                },
                {
                    "sent": "One is your training set and one is a ghost sample.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a useful trick that can be applied.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm trying to sort of, you know, give you little sort of units of.",
                    "label": 0
                },
                {
                    "sent": "Thinking that you could use in different contexts, so this is how the double sample trick works.",
                    "label": 1
                },
                {
                    "sent": "What we are interested in doing is, again, you know, think in terms of bounding the probability of being misled, things going wrong, whatever.",
                    "label": 0
                },
                {
                    "sent": "So we were interested in this probability that there exists a function that again, I'm just taking the zero training error for simplicity has zero training error, high true error, bigger than epsilon, and what we want to do is to replace that probability by the probability that basically we have two samples X&Y.",
                    "label": 1
                },
                {
                    "sent": "Again both generated ID and there's an H. In function class that looks good on the first half and bad on the second half.",
                    "label": 0
                },
                {
                    "sent": "And notice that there the probability is twice here and there's an epsilon 2 here.",
                    "label": 0
                },
                {
                    "sent": "So we sort of replace the test error by a finite sample.",
                    "label": 1
                },
                {
                    "sent": "Test so there are basically three events concerned.",
                    "label": 0
                },
                {
                    "sent": "One is that the training error zero.",
                    "label": 0
                },
                {
                    "sent": "The other is that it has high generalization error and the third is that it has high error on the go sample.",
                    "label": 0
                },
                {
                    "sent": "And the point about this is that if it has tried high generalization error, then with high probability it's going to have a high error on the go sample 'cause we go sample is generated independently and so there's no reason for it to mislead us.",
                    "label": 0
                },
                {
                    "sent": "You know we're trying to use the training sample to do all sorts of nasty stuff, which makes it likely that we're going to be misled by the training example, 'cause we're training our algorithms on it.",
                    "label": 0
                },
                {
                    "sent": "But the ghost sample is is independent, so the this is basically encapsulated in this statement here.",
                    "label": 0
                },
                {
                    "sent": "So the probability that this is the go sample having high error given that there's.",
                    "label": 0
                },
                {
                    "sent": "Zero training error and the generalization.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Error is poor is actually this.",
                    "label": 0
                },
                {
                    "sent": "Go sample doesn't care about the training sample, that's a separate sample, so we can just throw that away.",
                    "label": 0
                },
                {
                    "sent": "So it's just the probability that the there is high error on the on the go sample.",
                    "label": 0
                },
                {
                    "sent": "Given that there is high error true error, and that's very clearly just a binomial tail bound, it's very easy to show that that's going to be bigger than .5 for reasonable sizes of training set.",
                    "label": 1
                },
                {
                    "sent": "I mean very, very small sizes.",
                    "label": 0
                },
                {
                    "sent": "So this now can be put together with the if we just do this sequence here, probability over the double sample of.",
                    "label": 0
                },
                {
                    "sent": "Low zero training error and high error is bigger than.",
                    "label": 0
                },
                {
                    "sent": "Sorry and High Ghost sample.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is greater than or equal to just, including in an extra condition that it actually has high generalization error, and then we can just separate this into this probability and a conditional probability that's the conditional probability we just bounded and so we can now turn this around.",
                    "label": 0
                },
                {
                    "sent": "This is 1/2 and we get this quantity, which is the thing we're interested in here.",
                    "label": 0
                },
                {
                    "sent": "But how big is that finite set?",
                    "label": 0
                },
                {
                    "sent": "So the question is, how many functions can you get when you look at just a finite set of examples.",
                    "label": 0
                },
                {
                    "sent": "So you've got a function Class H which could.",
                    "label": 0
                },
                {
                    "sent": "I mean, I will have infinitely many functions potentially, but we're going to restrict ourselves to looking at its performance on a finite set of size M, or in fact will be interested in 2M.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This will be a double sample.",
                    "label": 0
                },
                {
                    "sent": "Interested in how many functions do we actually get?",
                    "label": 1
                },
                {
                    "sent": "Because those are our actual equivalence classes.",
                    "label": 1
                },
                {
                    "sent": "We only have to think about those when we're doing our analysis, and that's where we'll do the Union bound over those.",
                    "label": 1
                },
                {
                    "sent": "So we will actually have.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it could be as high as two to the M because there are two to the impossible functions classification functions on endpoints, so the log of this number is less than or equal to M. But obviously we want to have much, much smaller than that for the.",
                    "label": 1
                },
                {
                    "sent": "For the analysis to work.",
                    "label": 0
                },
                {
                    "sent": "So if we just look at what happens if we plot this with a set of linear functions in a 20 dimensional space, this is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ratio of BHM to two to the M. So up to about 21.",
                    "label": 1
                },
                {
                    "sent": "Oh, it looks a bit higher, but anyway, up to something close to.",
                    "label": 0
                },
                {
                    "sent": "The dimension here we get this ratio being equal to 1, but then about you know twice the dimension.",
                    "label": 0
                },
                {
                    "sent": "The things suddenly zooms down and the fraction rapidly gets very close to zero and this is the kind of regime where learning kicks in where we can get the number you know, because we're going to be a union bound over this number.",
                    "label": 0
                },
                {
                    "sent": "BH of North we want this number to be much, much smaller than the size of the training set in order to get that ratio.",
                    "label": 0
                },
                {
                    "sent": "To be small.",
                    "label": 0
                },
                {
                    "sent": "But this is a this is looking like good news.",
                    "label": 0
                },
                {
                    "sent": "It looks like the number does fall off in the way that we would like so that the actual number of functions equivalence classes we need to consider is small.",
                    "label": 0
                },
                {
                    "sent": "And this is what the vatnik chervin link.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This dimension measures OK, so that makes heavy German ink is dimension is the size for which we can get.",
                    "label": 0
                },
                {
                    "sent": "Find some examples and we can do every possible classification.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's the point here where this.",
                    "label": 0
                },
                {
                    "sent": "Curves leaves the.",
                    "label": 0
                },
                {
                    "sent": "The value one and starts to dip down below one, but the VC dimension is defined for general function classes, not just for linear function classes.",
                    "label": 0
                },
                {
                    "sent": "For linear function classes, it just turns out that the VC dimension is the linear dimension plus one.",
                    "label": 1
                },
                {
                    "sent": "So what we're but we can apply this idea more generally, so it's just saying the largest sample size for which we can find some inputs for which we can do every possible classification.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, if I do it in the case of linear functions, I can just show you quickly works.",
                    "label": 0
                },
                {
                    "sent": "Search this is linear threshold functions.",
                    "label": 0
                },
                {
                    "sent": "If I take 3 points, I can separate them in the following way with plus minus, say making these two plus these minus or the other way around.",
                    "label": 0
                },
                {
                    "sent": "This plus and these two minus and then I can separate these two from this one or these two from this one and I can also make them all positive or all negative and so I can do every possible classification of three points that's in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "The dimensions 2 here, but if I move too.",
                    "label": 0
                },
                {
                    "sent": "Four points, then there's no way I can get every possible classification.",
                    "label": 0
                },
                {
                    "sent": "For instance, making these two positive and these negative, I would need to basically.",
                    "label": 0
                },
                {
                    "sent": "Do something like that, which is not a straight line.",
                    "label": 0
                },
                {
                    "sent": "And however, you can figure the points you know there's actually you can choose the points how you want, but ever however you choose them, you can't.",
                    "label": 0
                },
                {
                    "sent": "So the VC dimension of linear threshold functions in two dimensions is 3, because there are three points that we can do every possible classification, but not for.",
                    "label": 0
                },
                {
                    "sent": "So that's a general result for any dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And the key result that actually gives us learning if you like is this relationship between the growth function and the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Tells us how fast this growth function grows once we take sample size is larger.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So up to the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Obviously the growth function is growing exponentially because we're getting every possible function.",
                    "label": 0
                },
                {
                    "sent": "So up to M = D, We're getting 2 to the M number of functions, but once we go beyond D, we're actually having a number that's bounded by this polynomial with exponent D. So we have a clearly two regimes.",
                    "label": 1
                },
                {
                    "sent": "The growth function is exponential up to the VC dimension and then polynomial, and there's no in between, and this is a completely general result for any set of classic classifiers.",
                    "label": 1
                },
                {
                    "sent": "So this is a very nice and I think you know surprising.",
                    "label": 0
                },
                {
                    "sent": "Result due to it's called Sours Llama, but it's actually was proved by or a slightly weaker version by Vatican Chevening Kiss and also by Sheller.",
                    "label": 0
                },
                {
                    "sent": "So it has 3 three different people who were interested in this.",
                    "label": 0
                },
                {
                    "sent": "Obviously back in Germany were interested for this learning application, but the others were interested for just combinatorial.",
                    "label": 0
                },
                {
                    "sent": "Interest, separate combinatorial interest.",
                    "label": 0
                },
                {
                    "sent": "OK, so given this, we're now in a position to really put together the basic theorem of statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "So here it goes, we want to bound this probability of being misled.",
                    "label": 0
                },
                {
                    "sent": "You know the usual thing error zero.",
                    "label": 0
                },
                {
                    "sent": "True error big.",
                    "label": 0
                },
                {
                    "sent": "We apply the double sample trick that's less than twice the probability of zero training error an.",
                    "label": 0
                },
                {
                    "sent": "Bad error on the on the second sample and now we do a union bound over the possible sets of function possible functions we can get on that double sample and B.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is there only BH of two M of 'em we can pull out that as a multiplier here, and this is now the probability on a double sample for a fixed H that we have 0 error on the first half and higher on the second half.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now the final ingredient is to bound this quantity here, and that's done by another trick known as symmetrization.",
                    "label": 1
                },
                {
                    "sent": "And the idea here is to, you know, just looking at this.",
                    "label": 0
                },
                {
                    "sent": "If you realize that this function H was chosen and fixed, it seems pretty weird that all the errors should have gone into the second half.",
                    "label": 0
                },
                {
                    "sent": "There's no reason why aren't half of them in the first half, you know?",
                    "label": 0
                },
                {
                    "sent": "Because the function was fixed and then we did the sample right?",
                    "label": 1
                },
                {
                    "sent": "So think about swapping them and the chances of the swapping actually putting all the errors in the second half.",
                    "label": 0
                },
                {
                    "sent": "That's what Symmetrization does, basically.",
                    "label": 0
                },
                {
                    "sent": "So the way formerly you do it is you.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say that you could generate a sample by because all of the examples are independent.",
                    "label": 1
                },
                {
                    "sent": "You could, after generating your sample, apply a random permutation and still have the same probability of those points.",
                    "label": 0
                },
                {
                    "sent": "The order in which they occur is completely arbitrary, and so we think of this distribution which says Generator 2 sample and then actually apply a an element from the some permutation group and.",
                    "label": 0
                },
                {
                    "sent": "Mute the entries in the actual.",
                    "label": 0
                },
                {
                    "sent": "In in, in the sequence in the sample.",
                    "label": 0
                },
                {
                    "sent": "Now if we want to look at the probability of an event, it's the same if we do it with this distribution or with this distribution.",
                    "label": 1
                },
                {
                    "sent": "You know applying the permutations or not applying them.",
                    "label": 0
                },
                {
                    "sent": "And if we look at this one, we can think of doing the permutations 1st and then doing an expectation according to the examples and so now.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, doing any expectation according to an example and given the set of examples, then doing a random permutation and the probability of that event under that random permutation.",
                    "label": 0
                },
                {
                    "sent": "Now we're very close 'cause we're thinking of we've got a sample.",
                    "label": 0
                },
                {
                    "sent": "What is the chance under a random permutation all the errors in the second half?",
                    "label": 0
                },
                {
                    "sent": "OK, it's clear that's going to be very low, and we do that by just considering these permutations that swap corresponding elements in the first half and second half of the sample.",
                    "label": 1
                },
                {
                    "sent": "That's the particular.",
                    "label": 0
                },
                {
                    "sent": "Set of permutations we consider and if you do that, the chances that they're all in the second half.",
                    "label": 0
                },
                {
                    "sent": "There's epsilon M / 2 of them two to the minus epsilon M / 2 because we just have to make sure all of those that were an error for that particular swap, they have to go into the second half so.",
                    "label": 0
                },
                {
                    "sent": "The second half right?",
                    "label": 0
                },
                {
                    "sent": "So now that just puts in the final ingredient here that was.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The this probability remember of that first half, 0 second, half epsilon on 2 for a random permutation is to the minus epsilon M / 2 and the expectation of that is constant.",
                    "label": 0
                },
                {
                    "sent": "So it's two to the minus epsilon M / 2 and so now we go back to the sequence I had before.",
                    "label": 0
                },
                {
                    "sent": "We've bounded this thing here by two to the minus epsilon M / 2.",
                    "label": 0
                },
                {
                    "sent": "We've got a bound on this from Sam's lemma, and so we've got a bound on this thing that we're interested in bounding this probability of being misled.",
                    "label": 0
                },
                {
                    "sent": "Putting all those together, we and turning it around as usual to make a theorem.",
                    "label": 0
                },
                {
                    "sent": "This is the result you get with probability 1 minus Delta over the M samples.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The probability the generalization error of a function H chosen from a class with VC dimension D which has zero training error is bounded by this quantity.",
                    "label": 1
                },
                {
                    "sent": "Where you know gain the form is very similar, constant, slightly different to the one we had before the log.",
                    "label": 0
                },
                {
                    "sent": "Two on Delta.",
                    "label": 0
                },
                {
                    "sent": "In this case two and M, but the complexity term is now this two log 2:00 AM on Delta which comes from the Sauer's lemma or the bound on the growth function.",
                    "label": 0
                },
                {
                    "sent": "If you just go back quickly, I can show you that.",
                    "label": 0
                },
                {
                    "sent": "Here it is.",
                    "label": 0
                },
                {
                    "sent": "It's the log of this quantity here where N is replaced by two M because we have the double sample.",
                    "label": 0
                },
                {
                    "sent": "So that's where that quantity here comes from.",
                    "label": 0
                },
                {
                    "sent": "OK. And so there you are.",
                    "label": 0
                },
                {
                    "sent": "That's that's if you like the VC dimension results.",
                    "label": 0
                },
                {
                    "sent": "We can think of the D as a measure of the complexity or capacity of the Class H. But it doesn't actually distinguish between functions in H, so you can think of it as sort of a uniform prior over the functions in H. If you're thinking based in terms.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "Prior, it doesn't actually, you know, so the way to optimize this bound given a particular H is simply minimize the training error.",
                    "label": 0
                },
                {
                    "sent": "There are corresponding lower bounds which are pretty close match.",
                    "label": 0
                },
                {
                    "sent": "But they rely on they don't apply in general for every distribution they say given the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You got a VC class.",
                    "label": 0
                },
                {
                    "sent": "You can find the distribution that forces an error of.",
                    "label": 1
                },
                {
                    "sent": "The same order as that upper bound.",
                    "label": 0
                },
                {
                    "sent": "So this is a slight mismatch and remember you know the upper bound is for all distributions, but the lower bound is.",
                    "label": 0
                },
                {
                    "sent": "You know there exist distributions are basically put the distributions on points that can do every possible classification and you can imagine that given that you can do every classification on those points, it's going to be tough to learn anything from the classification of some subset of them, and that's basically the trick that's used here.",
                    "label": 1
                },
                {
                    "sent": "And here's the non zero training error version.",
                    "label": 0
                },
                {
                    "sent": "So basically the error on the training set plus now the square root of these quantities comes in, which is a lot weaker.",
                    "label": 0
                },
                {
                    "sent": "But there's actually the PAC Bayes bounds that can interpolate between these two even with the VC dimension involved.",
                    "label": 0
                },
                {
                    "sent": "OK, so you.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are sort of, you know, because when you have zero training error, you suddenly switch to a non square root version here which is alot alot tighter but you can interpolate roughly between those two.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that, jeez, I've almost run out of time, but I will just cover this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is alluding to what you asked earlier.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're thinking about actually now doing something a bit like.",
                    "label": 0
                },
                {
                    "sent": "You know a Bayesian way of learning where you're thinking of putting weights or higher probability on lower more lower complexity functions.",
                    "label": 0
                },
                {
                    "sent": "The way to do that in the VC world is so called structural risk minimization, so the structural risk minimization assumes that we can form a hierarchy of classes.",
                    "label": 1
                },
                {
                    "sent": "Of increasing complexity, where I'm assuming, for instance, that this has VC dimension one this to this D in this K, so we've ordered them in VC classes.",
                    "label": 1
                },
                {
                    "sent": "Now if we want to find a function in each class with minimal empirical error, which is the thing you do if you learn in a single class.",
                    "label": 0
                },
                {
                    "sent": "You can then minimize this quantity, which is the generalization error bound over all of the classes.",
                    "label": 0
                },
                {
                    "sent": "And by doing that you're trading off.",
                    "label": 0
                },
                {
                    "sent": "Your empirical error against your complexity term, the usual thing.",
                    "label": 0
                },
                {
                    "sent": "Putting up if you like a bias aprior towards simpler BC classes, but obviously only at the expense of making more training errors.",
                    "label": 0
                },
                {
                    "sent": "By applying that theorem that I gave you on the previous slide, for each of these classes and doing a union bound, we get adjust K here for the number of classes and so this is actually not only the thing to minimize, but it also is an error bound on the resulting function that you choose by that algorithm.",
                    "label": 0
                },
                {
                    "sent": "So again, the the bound gives you an algorithm an the result of the algorithm that you can apply the bound to.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying it's a very good algorithm, but you know it's just.",
                    "label": 0
                },
                {
                    "sent": "Tying the two together, I think is important.",
                    "label": 0
                },
                {
                    "sent": "OK, and the point you were making, there's no reason I've actually just done a union bound here with equal probabilities for each of these classes.",
                    "label": 0
                },
                {
                    "sent": "But I could have put an equal Q1Q2 up to QK and put a distribution over those classes and put my bias that I think you know these could be polynomials for instance and I would say I think a simple polynomial is more likely.",
                    "label": 0
                },
                {
                    "sent": "In this case I'm going to put a higher value of Q and I would suffer here.",
                    "label": 0
                },
                {
                    "sent": "QD sorry 1 / Q D here rather than a fixed K here.",
                    "label": 0
                },
                {
                    "sent": "So you would again bias yourself towards.",
                    "label": 0
                },
                {
                    "sent": "The I mean you're paying twice for complexity in that case because you're playing here for the increasing VC dimension and you pay here by your belief about certain.",
                    "label": 0
                },
                {
                    "sent": "Function classes being more likely.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've just got.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To criticize the pack theory, and then I'll come back to tomorrow to sort of pick this up so the theory is certainly valid, and it's sort of, you know, bit scary, because there are lower bounds, so it looks like that's it.",
                    "label": 1
                },
                {
                    "sent": "You know people were packing their bags and off, you know to the.",
                    "label": 0
                },
                {
                    "sent": "For a vacation or something, because it seemed like learning theory was closed out.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't accord with experience for those applying learning, there seems to be a bit of a big mismatch between theory and practice, and the biggest example is support vector machines.",
                    "label": 1
                },
                {
                    "sent": "So if we're thinking about applying support vector machines in high dimensional feature spaces, which is what you do with the kernel, you can say for instance, take a Gaussian kernel and and be in an infinite dimensional space, so.",
                    "label": 0
                },
                {
                    "sent": "This theory just says nothing.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's an infinite VC dimension, so forget it.",
                    "label": 0
                },
                {
                    "sent": "You know you're not actually getting any bound out, and yet you're getting very impressive performance, so something's happening that this theory is failing to capture, and it's a little strange because we had the lower bound.",
                    "label": 0
                },
                {
                    "sent": "So what went wrong?",
                    "label": 0
                },
                {
                    "sent": "So this is just a quick review of what a SVM does is just a linear classifier in this feature space that's defined implicitly via kernel.",
                    "label": 0
                },
                {
                    "sent": "And we minimize the norm of the weight vector squared.",
                    "label": 0
                },
                {
                    "sent": "So we can think of that again as a prior being.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the likely weight vectors, maybe with the Gaussian centered at the origin, and this is some slack variables which we could sort of think of as measuring noise, so it's sort of an illusion too.",
                    "label": 0
                },
                {
                    "sent": "A Bayesian learning here.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is that the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Margin is what causes this reduction in complexity.",
                    "label": 1
                },
                {
                    "sent": "And the point is that actually we know because of the lower bound that the fact that we're learning must be something to do with the distribution, right?",
                    "label": 1
                },
                {
                    "sent": "Because we know that worst case distributions cause us to have the same error as the VC bound, the fact that we're doing better than the VC bound means there must be something about the distribution that's helping us to learn.",
                    "label": 0
                },
                {
                    "sent": "So I think this is an insight that is come out of this, that perhaps you know we can maybe haven't made as much use of as we might do, so there is something in the distribution.",
                    "label": 0
                },
                {
                    "sent": "It must be benign, somehow aligned with the thing we're trying to learn that is getting us out of that worst case generalization.",
                    "label": 0
                },
                {
                    "sent": "There has been a suggestion.",
                    "label": 0
                },
                {
                    "sent": "You can use structural risk minimization over a hierarchy determined by the margin of different classifiers, but that's actually not valid becausw you need to define to do structural risk minimization, you need to define the function classes ahead of time, but you can't know which functions would have large margin until you see the data.",
                    "label": 1
                },
                {
                    "sent": "So you can't actually do that hierarchy until you've seen.",
                    "label": 0
                },
                {
                    "sent": "The data, so the approach that you need is a sort of data dependent structural risk minimization, which will set up the hierarchy based on the sample.",
                    "label": 0
                },
                {
                    "sent": "So simple structural risk minimization won't work.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what we're looking for, and the approach sort of uses this idea of lucky distributions or luckiness that tell us that the actual classifier is aligned in some way with the distribution.",
                    "label": 0
                },
                {
                    "sent": "And the margin is a measure of how benign the distribution is, how well the fit is with the distribution.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a good point to break, so I'll come back next time tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And pick up here and show you how the analysis goes through for support vector machines and hopefully a little bit into Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}