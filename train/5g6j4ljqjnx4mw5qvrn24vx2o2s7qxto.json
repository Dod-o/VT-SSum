{
    "id": "5g6j4ljqjnx4mw5qvrn24vx2o2s7qxto",
    "title": "Crowdsourcing Linked Data Quality Assessment",
    "info": {
        "author": [
            "Maribel Acosta, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Crowdsourcing",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_acosta_quality_assessment/",
    "segmentation": [
        [
            "Good afternoon, my name is Maribel Acosta.",
            "I'm from the Carl String Institute of Technology, Germany.",
            "This is a joint work of foreign institutions an it's titled Crowdsourcing Link data quality assessment."
        ],
        [
            "As we know, the linking open data it's a great source of data.",
            "However, the writing quality of linked data sources often imposes serious problems for developers and also for researchers from different areas to consume an integrating data into their solutions.",
            "Keeping aside the factual flaws of the linked data sources, some of this link data quality issues are actually generated during the RDF fication process.",
            "So indeed we can apply some automatic mechanisms to detect and correct quality issues, but however there are others quality issues that require further interpretation and this can be easily performed by humans.",
            "For instance, I have a DB pedia triple where it states that the property is the date of birth.",
            "Sorry, the value is 3 so we can understand that this is not a valid date and therefore this DP that Ripple is incorrect.",
            "So in our proposed solution, we propose to include human verification in the process of filling data quality assessment.",
            "A direct application of our methodology is to detect patterns in errors of RDF triples in order to identify and hopefully to correct, the extraction mechanisms that created this errors at the first place."
        ],
        [
            "So in this work we formulated the following research questions.",
            "The first one is, is it possible to detect quality quality issues in datasets via crowdsourcing mechanisms?",
            "The second one is assuming that we have different crowds is what is the time of crowd that is most suitable for each type of quality issue.",
            "And finally, if we can distinguish between link data experts and non skilled crowds, what type of errors are made by this crowds when assessing?",
            "RDF triples."
        ],
        [
            "We took a look at the related work and we found out that the link data Community has proposed several solutions with crowdsourcing the process.",
            "Most prominent one is the Pedia, which is defined as a crowdsourced data set.",
            "On the other hand, we also have several approaches, mechanisms to describe the quality assessment in the web of data.",
            "Our work is situated in the intersections of these two fields.",
            "Since you want to apply crowdsourcing in order to perform quality assessment over linked data."
        ],
        [
            "In our approach, we propose a myth."
        ],
        [
            "Mythology where we can access RDF triples from RDF datasets.",
            "This triples are going to be shown or displayed to a group of individuals who are going to assess the triples they have to answer whether the triples are correct or not.",
            "In latter case, the experts or the crowd has to answer whether the triple is incorrect.",
            "They have to choose the type of quality issue, so the steps to implement our methodology.",
            "Adam following first we have to select the in data quality issues that we think that they can be crowdsourced.",
            "The second step is to select the appropriate crowd sourcing approaches and the appropriate crowds for each type of quality issue.",
            "And the third step is to design and generate the user interfaces in order to show the RDF triples to the people."
        ],
        [
            "In the first step we have to select the link data quality issues that we have to crowd source.",
            "We took a look at the literature and we found out that they are free categories at the triple level based of quality problems in the pedia.",
            "And we think that this can be crowd source.",
            "So in the first quality issue we have the incorrect object.",
            "I already presented this example.",
            "This is when the value of the triple is actually incorrect.",
            "So we have the data.",
            "1st of a person and value is free.",
            "This is obviously incorrect.",
            "In the second type of quality issue we have when the data types, types or language tags are incorrect.",
            "In this particular case we have the name, the property, fourth name and this is obviously not in English as a state in the triple.",
            "And the third type of quality issues is when the incorrect link to external web pages or do a pictures or external sources from not link data unnecessarily.",
            "So in this triple we have a link to external web page and actually the URL is not associated at all to the subject of the trip.",
            "Anne."
        ],
        [
            "Then we have to select the appropriate crowdsourcing approaches.",
            "There are several ways to apply crowdsourcing.",
            "Two of them are contest than micro tasks.",
            "We also applied a very well known crowdsourcing approach which states that project a crowdsourcing project can be the composed into different stages.",
            "These different stages can be implemented with different crowds with different skills and different types of payment.",
            "In we implemented there find stage and verify stage in the final stage or we contacted Lynn data experts and they participated in a contest.",
            "We ask them to perform a difficult task.",
            "They had to go to tool that Triple Check Mate tool and assess TV pedia triples whether when they find dependent ripples that they consider incorrect, they have to annotate them with the corresponding quality issue.",
            "The contestant with the higher number of submissions was awarded with a final price.",
            "All the list of triples found incorrect by the experts annotated with the corresponding quality issues were submitted to a microtask platform and was so Mechanical Turk and we ask workers to perform an easy task.",
            "This task was to verify the output of the experts.",
            "We performed a micro payments to all the workers that participated in our crowdsourcing experiment."
        ],
        [
            "Of course, when we are talking about RDF and link data experts, we can only present the RDF triples, but when we are reaching a non expert crowd we have to present the triples in such a way that they can actually understand and analyze the triple and successfully complete the task.",
            "So in this case we try to generate user friendly interfaces in order to show them the triples and we try to exploit the human readable descriptions of the triples.",
            "In this case we just retrieve the fourth name.",
            "In the RDF's label of values for the substance of the triple, we also implemented a very simple wrapper in order to extract the information from Wikipedia infoboxes in order to provide some extra context so the crowd could compare the values from the regional source and DB pedia.",
            "We also provided the links to all their Wikipedia articles such that they could compare the values from DB Pedia and Wikipedia.",
            "And finally for the interlinking task we also provided a preview of the external web page or the picture so they could easily see the content of the external web page.",
            "As you could see here, these are the three types of interfaces that we generated and they will never mentioned words, RDF subject or entity.",
            "It's very simple images and interfaces for the crowd."
        ],
        [
            "We conducted an experimental study."
        ],
        [
            "We evaluated a crowdsourcing approach.",
            "The methodology in the final stages I said we provided we've lunchtime contest with Lynn data experts and the Verify stage was implemented with Mechanical Turk Microtasks.",
            "We ask five different workers and the correct answer was selected by applying majority voting.",
            "Also, two authors of this paper created a gold standard.",
            "We sat down and we assessed all the triples generated in the contest.",
            "Also, we disagreements where sold by a mutual agreement.",
            "Finally, we wanted to measure the quality of our approach.",
            "So we measure the precision of the crowd with.",
            "Is this not working properly?",
            "Sorry, OK, so we measure them.",
            "Their quality of our approach by measuring precision so the precision was measures that ripple through positive, which is a tribal that is identified as incorrect by the crowd and is actually incorrect in the data set.",
            "And the false positives are triples that are unified incorrect by the crowd, but they are actually correct in the data set."
        ],
        [
            "So in our overall results, we had that 50 different link data experts participated in the contest in our micro test experiment, 80 different workers.",
            "The total time for the contest was three weeks.",
            "This was a pretty fine time and then Microtask took four days to complete.",
            "The experts evaluated, at total of around 1500 triples and their microtask workers only 1030 three 73 triples.",
            "Because we eliminated duplicates or some triples that we could easily check via automatic mechanisms.",
            "The total cost of our contest was 400 US dollars and the total cost for the micro test experiments was only 43 US dollars."
        ],
        [
            "So in the position we had in the incorrect object task, we found out that the end turkeys can actually reduce the error rate of the link data experts.",
            "As we can see here, the experts rich a precision around of 71%, while the Mechanical Turk workers rich deposition of almost 8989% also 117 DVD at triples had predicates related to date an all the values were incorrect.",
            "Like the one I'm showing here, this suggests that we should take a look at the DB pedia.",
            "Rappers that are extracting dates from certain Wikipedia infoboxes.",
            "Also 52 DP that triples had the runs values that they were dragged from the regional source like the one I'm showing here with a question mark at the end as a value of the triple.",
            "The experts classify these triples as incorrect but the workers could actually compare the values to the 1% in Wikipedia.",
            "And they assign them as correct since the values were actually consistent with their original source."
        ],
        [
            "And the second type of task with the incorrect data type or language tag.",
            "As expected.",
            "The link data ex."
        ],
        [
            "It had a high of precision, in this case 82%, but the M Turk workers did not perform so well with only 47% of precision.",
            "We put an effort to explain data types.",
            "We provided positive examples, negative examples, but this is obviously not enough.",
            "So."
        ],
        [
            "We took a look at the results and then this suggests that crowd actually perform a lot of false positives when assessing numerical values.",
            "They could not distinguish if a number was actually a month or a date or any other type of numerical value."
        ],
        [
            "And then finally, for the incorrect link task, we also implement."
        ],
        [
            "At a baseline approach, it was a very simple approach.",
            "We only took the object of the triple and we analyze the content and if the if they meant if the subject was not mentioned in the in the web page, then we assume that this was not associated to the subject.",
            "This basin approach reached a position of 25%.",
            "Of course, this failed when we were assessing pictures.",
            "On the."
        ],
        [
            "Oversized link data experts perform amazingly, very bad with only 15% of precision.",
            "While the Mechanical Turk would successfully reach a precision of 94%, we analyzed and 189 misclassifications of the experts and we found out that out of this 189 triples, 50% of them were free.",
            "Based links.",
            "Also, 39% wear Wikipedia images or uploads and finally 11% external links.",
            "The 6% classifications by the workers correspond to external web pages in a different language than English, and also with no pictures or no graphical representations, so they could actually do not solve if the triple if the subject was associated to the object."
        ],
        [
            "So coming back to our original research questions, we can answer that well.",
            "Both forms of crowd sources can be applied to detect certain data quality issues.",
            "Regarding the type of crowds for each type of quality, issue the effort of the experts through all will be focused on domain specific task, while the entire crowd performs really well when you present data comparisons.",
            "So comparing Wikipedia entries versus DB pedia values for instance.",
            "And finally, the type of errors made by the lay users correspond to domain specific tasks.",
            "They are not able to understand.",
            "In this case, what is a data type and for the?",
            "Case of the experts, they do not perform so well when they have to do some extra work than just applying their skills.",
            "So too."
        ],
        [
            "Include we have."
        ],
        [
            "Presented a crowd sourcing methodology for Lynn Data quality assessment.",
            "We implemented a very well known crowdsourcing pattern defined stage.",
            "We in contact the link data experts an in the verify stage M Turk workers the crowd sourcing approaches are visible to assess link data quality issues an the application for this work will be to detect patterns and then to correct extraction mechanisms that created these errors at the beginning.",
            "As future work we plan to conducting experiments with other quality issues and of course other datasets with different domains an integrate the crowd into the creation process so it could be part of a automatic workflow with some automatic tools."
        ],
        [
            "Some of the reference that I've been pointing out in the slides and the acknowledgements an."
        ],
        [
            "Thank you very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, my name is Maribel Acosta.",
                    "label": 1
                },
                {
                    "sent": "I'm from the Carl String Institute of Technology, Germany.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work of foreign institutions an it's titled Crowdsourcing Link data quality assessment.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we know, the linking open data it's a great source of data.",
                    "label": 0
                },
                {
                    "sent": "However, the writing quality of linked data sources often imposes serious problems for developers and also for researchers from different areas to consume an integrating data into their solutions.",
                    "label": 0
                },
                {
                    "sent": "Keeping aside the factual flaws of the linked data sources, some of this link data quality issues are actually generated during the RDF fication process.",
                    "label": 0
                },
                {
                    "sent": "So indeed we can apply some automatic mechanisms to detect and correct quality issues, but however there are others quality issues that require further interpretation and this can be easily performed by humans.",
                    "label": 1
                },
                {
                    "sent": "For instance, I have a DB pedia triple where it states that the property is the date of birth.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the value is 3 so we can understand that this is not a valid date and therefore this DP that Ripple is incorrect.",
                    "label": 1
                },
                {
                    "sent": "So in our proposed solution, we propose to include human verification in the process of filling data quality assessment.",
                    "label": 1
                },
                {
                    "sent": "A direct application of our methodology is to detect patterns in errors of RDF triples in order to identify and hopefully to correct, the extraction mechanisms that created this errors at the first place.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we formulated the following research questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is, is it possible to detect quality quality issues in datasets via crowdsourcing mechanisms?",
                    "label": 1
                },
                {
                    "sent": "The second one is assuming that we have different crowds is what is the time of crowd that is most suitable for each type of quality issue.",
                    "label": 0
                },
                {
                    "sent": "And finally, if we can distinguish between link data experts and non skilled crowds, what type of errors are made by this crowds when assessing?",
                    "label": 0
                },
                {
                    "sent": "RDF triples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We took a look at the related work and we found out that the link data Community has proposed several solutions with crowdsourcing the process.",
                    "label": 0
                },
                {
                    "sent": "Most prominent one is the Pedia, which is defined as a crowdsourced data set.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we also have several approaches, mechanisms to describe the quality assessment in the web of data.",
                    "label": 1
                },
                {
                    "sent": "Our work is situated in the intersections of these two fields.",
                    "label": 1
                },
                {
                    "sent": "Since you want to apply crowdsourcing in order to perform quality assessment over linked data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our approach, we propose a myth.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mythology where we can access RDF triples from RDF datasets.",
                    "label": 0
                },
                {
                    "sent": "This triples are going to be shown or displayed to a group of individuals who are going to assess the triples they have to answer whether the triples are correct or not.",
                    "label": 0
                },
                {
                    "sent": "In latter case, the experts or the crowd has to answer whether the triple is incorrect.",
                    "label": 1
                },
                {
                    "sent": "They have to choose the type of quality issue, so the steps to implement our methodology.",
                    "label": 1
                },
                {
                    "sent": "Adam following first we have to select the in data quality issues that we think that they can be crowdsourced.",
                    "label": 0
                },
                {
                    "sent": "The second step is to select the appropriate crowd sourcing approaches and the appropriate crowds for each type of quality issue.",
                    "label": 0
                },
                {
                    "sent": "And the third step is to design and generate the user interfaces in order to show the RDF triples to the people.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the first step we have to select the link data quality issues that we have to crowd source.",
                    "label": 0
                },
                {
                    "sent": "We took a look at the literature and we found out that they are free categories at the triple level based of quality problems in the pedia.",
                    "label": 0
                },
                {
                    "sent": "And we think that this can be crowd source.",
                    "label": 1
                },
                {
                    "sent": "So in the first quality issue we have the incorrect object.",
                    "label": 0
                },
                {
                    "sent": "I already presented this example.",
                    "label": 0
                },
                {
                    "sent": "This is when the value of the triple is actually incorrect.",
                    "label": 0
                },
                {
                    "sent": "So we have the data.",
                    "label": 0
                },
                {
                    "sent": "1st of a person and value is free.",
                    "label": 0
                },
                {
                    "sent": "This is obviously incorrect.",
                    "label": 0
                },
                {
                    "sent": "In the second type of quality issue we have when the data types, types or language tags are incorrect.",
                    "label": 1
                },
                {
                    "sent": "In this particular case we have the name, the property, fourth name and this is obviously not in English as a state in the triple.",
                    "label": 0
                },
                {
                    "sent": "And the third type of quality issues is when the incorrect link to external web pages or do a pictures or external sources from not link data unnecessarily.",
                    "label": 1
                },
                {
                    "sent": "So in this triple we have a link to external web page and actually the URL is not associated at all to the subject of the trip.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we have to select the appropriate crowdsourcing approaches.",
                    "label": 1
                },
                {
                    "sent": "There are several ways to apply crowdsourcing.",
                    "label": 0
                },
                {
                    "sent": "Two of them are contest than micro tasks.",
                    "label": 0
                },
                {
                    "sent": "We also applied a very well known crowdsourcing approach which states that project a crowdsourcing project can be the composed into different stages.",
                    "label": 0
                },
                {
                    "sent": "These different stages can be implemented with different crowds with different skills and different types of payment.",
                    "label": 0
                },
                {
                    "sent": "In we implemented there find stage and verify stage in the final stage or we contacted Lynn data experts and they participated in a contest.",
                    "label": 1
                },
                {
                    "sent": "We ask them to perform a difficult task.",
                    "label": 0
                },
                {
                    "sent": "They had to go to tool that Triple Check Mate tool and assess TV pedia triples whether when they find dependent ripples that they consider incorrect, they have to annotate them with the corresponding quality issue.",
                    "label": 0
                },
                {
                    "sent": "The contestant with the higher number of submissions was awarded with a final price.",
                    "label": 0
                },
                {
                    "sent": "All the list of triples found incorrect by the experts annotated with the corresponding quality issues were submitted to a microtask platform and was so Mechanical Turk and we ask workers to perform an easy task.",
                    "label": 0
                },
                {
                    "sent": "This task was to verify the output of the experts.",
                    "label": 0
                },
                {
                    "sent": "We performed a micro payments to all the workers that participated in our crowdsourcing experiment.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, when we are talking about RDF and link data experts, we can only present the RDF triples, but when we are reaching a non expert crowd we have to present the triples in such a way that they can actually understand and analyze the triple and successfully complete the task.",
                    "label": 0
                },
                {
                    "sent": "So in this case we try to generate user friendly interfaces in order to show them the triples and we try to exploit the human readable descriptions of the triples.",
                    "label": 0
                },
                {
                    "sent": "In this case we just retrieve the fourth name.",
                    "label": 0
                },
                {
                    "sent": "In the RDF's label of values for the substance of the triple, we also implemented a very simple wrapper in order to extract the information from Wikipedia infoboxes in order to provide some extra context so the crowd could compare the values from the regional source and DB pedia.",
                    "label": 1
                },
                {
                    "sent": "We also provided the links to all their Wikipedia articles such that they could compare the values from DB Pedia and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And finally for the interlinking task we also provided a preview of the external web page or the picture so they could easily see the content of the external web page.",
                    "label": 0
                },
                {
                    "sent": "As you could see here, these are the three types of interfaces that we generated and they will never mentioned words, RDF subject or entity.",
                    "label": 0
                },
                {
                    "sent": "It's very simple images and interfaces for the crowd.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We conducted an experimental study.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated a crowdsourcing approach.",
                    "label": 0
                },
                {
                    "sent": "The methodology in the final stages I said we provided we've lunchtime contest with Lynn data experts and the Verify stage was implemented with Mechanical Turk Microtasks.",
                    "label": 0
                },
                {
                    "sent": "We ask five different workers and the correct answer was selected by applying majority voting.",
                    "label": 0
                },
                {
                    "sent": "Also, two authors of this paper created a gold standard.",
                    "label": 1
                },
                {
                    "sent": "We sat down and we assessed all the triples generated in the contest.",
                    "label": 1
                },
                {
                    "sent": "Also, we disagreements where sold by a mutual agreement.",
                    "label": 0
                },
                {
                    "sent": "Finally, we wanted to measure the quality of our approach.",
                    "label": 0
                },
                {
                    "sent": "So we measure the precision of the crowd with.",
                    "label": 0
                },
                {
                    "sent": "Is this not working properly?",
                    "label": 0
                },
                {
                    "sent": "Sorry, OK, so we measure them.",
                    "label": 0
                },
                {
                    "sent": "Their quality of our approach by measuring precision so the precision was measures that ripple through positive, which is a tribal that is identified as incorrect by the crowd and is actually incorrect in the data set.",
                    "label": 0
                },
                {
                    "sent": "And the false positives are triples that are unified incorrect by the crowd, but they are actually correct in the data set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our overall results, we had that 50 different link data experts participated in the contest in our micro test experiment, 80 different workers.",
                    "label": 1
                },
                {
                    "sent": "The total time for the contest was three weeks.",
                    "label": 1
                },
                {
                    "sent": "This was a pretty fine time and then Microtask took four days to complete.",
                    "label": 1
                },
                {
                    "sent": "The experts evaluated, at total of around 1500 triples and their microtask workers only 1030 three 73 triples.",
                    "label": 1
                },
                {
                    "sent": "Because we eliminated duplicates or some triples that we could easily check via automatic mechanisms.",
                    "label": 0
                },
                {
                    "sent": "The total cost of our contest was 400 US dollars and the total cost for the micro test experiments was only 43 US dollars.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the position we had in the incorrect object task, we found out that the end turkeys can actually reduce the error rate of the link data experts.",
                    "label": 1
                },
                {
                    "sent": "As we can see here, the experts rich a precision around of 71%, while the Mechanical Turk workers rich deposition of almost 8989% also 117 DVD at triples had predicates related to date an all the values were incorrect.",
                    "label": 0
                },
                {
                    "sent": "Like the one I'm showing here, this suggests that we should take a look at the DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Rappers that are extracting dates from certain Wikipedia infoboxes.",
                    "label": 1
                },
                {
                    "sent": "Also 52 DP that triples had the runs values that they were dragged from the regional source like the one I'm showing here with a question mark at the end as a value of the triple.",
                    "label": 0
                },
                {
                    "sent": "The experts classify these triples as incorrect but the workers could actually compare the values to the 1% in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And they assign them as correct since the values were actually consistent with their original source.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second type of task with the incorrect data type or language tag.",
                    "label": 0
                },
                {
                    "sent": "As expected.",
                    "label": 0
                },
                {
                    "sent": "The link data ex.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It had a high of precision, in this case 82%, but the M Turk workers did not perform so well with only 47% of precision.",
                    "label": 0
                },
                {
                    "sent": "We put an effort to explain data types.",
                    "label": 0
                },
                {
                    "sent": "We provided positive examples, negative examples, but this is obviously not enough.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We took a look at the results and then this suggests that crowd actually perform a lot of false positives when assessing numerical values.",
                    "label": 0
                },
                {
                    "sent": "They could not distinguish if a number was actually a month or a date or any other type of numerical value.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally, for the incorrect link task, we also implement.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At a baseline approach, it was a very simple approach.",
                    "label": 0
                },
                {
                    "sent": "We only took the object of the triple and we analyze the content and if the if they meant if the subject was not mentioned in the in the web page, then we assume that this was not associated to the subject.",
                    "label": 0
                },
                {
                    "sent": "This basin approach reached a position of 25%.",
                    "label": 0
                },
                {
                    "sent": "Of course, this failed when we were assessing pictures.",
                    "label": 0
                },
                {
                    "sent": "On the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oversized link data experts perform amazingly, very bad with only 15% of precision.",
                    "label": 0
                },
                {
                    "sent": "While the Mechanical Turk would successfully reach a precision of 94%, we analyzed and 189 misclassifications of the experts and we found out that out of this 189 triples, 50% of them were free.",
                    "label": 1
                },
                {
                    "sent": "Based links.",
                    "label": 0
                },
                {
                    "sent": "Also, 39% wear Wikipedia images or uploads and finally 11% external links.",
                    "label": 1
                },
                {
                    "sent": "The 6% classifications by the workers correspond to external web pages in a different language than English, and also with no pictures or no graphical representations, so they could actually do not solve if the triple if the subject was associated to the object.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So coming back to our original research questions, we can answer that well.",
                    "label": 0
                },
                {
                    "sent": "Both forms of crowd sources can be applied to detect certain data quality issues.",
                    "label": 1
                },
                {
                    "sent": "Regarding the type of crowds for each type of quality, issue the effort of the experts through all will be focused on domain specific task, while the entire crowd performs really well when you present data comparisons.",
                    "label": 1
                },
                {
                    "sent": "So comparing Wikipedia entries versus DB pedia values for instance.",
                    "label": 0
                },
                {
                    "sent": "And finally, the type of errors made by the lay users correspond to domain specific tasks.",
                    "label": 0
                },
                {
                    "sent": "They are not able to understand.",
                    "label": 0
                },
                {
                    "sent": "In this case, what is a data type and for the?",
                    "label": 0
                },
                {
                    "sent": "Case of the experts, they do not perform so well when they have to do some extra work than just applying their skills.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Include we have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Presented a crowd sourcing methodology for Lynn Data quality assessment.",
                    "label": 1
                },
                {
                    "sent": "We implemented a very well known crowdsourcing pattern defined stage.",
                    "label": 1
                },
                {
                    "sent": "We in contact the link data experts an in the verify stage M Turk workers the crowd sourcing approaches are visible to assess link data quality issues an the application for this work will be to detect patterns and then to correct extraction mechanisms that created these errors at the beginning.",
                    "label": 0
                },
                {
                    "sent": "As future work we plan to conducting experiments with other quality issues and of course other datasets with different domains an integrate the crowd into the creation process so it could be part of a automatic workflow with some automatic tools.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the reference that I've been pointing out in the slides and the acknowledgements an.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}