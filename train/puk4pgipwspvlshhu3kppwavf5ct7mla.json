{
    "id": "puk4pgipwspvlshhu3kppwavf5ct7mla",
    "title": "Graph Transduction via Alternating Minimization",
    "info": {
        "author": [
            "Jun Wang, Department of Electrical Engineering, Columbia University"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_wang_gtvam/",
    "segmentation": [
        [
            "Good morning everyone.",
            "My name is Jin Wang.",
            "My talk is about graph graph transduction.",
            "Why alternating memorization this work was surprised by Professor Tony Jabra and shifting from Columbia University."
        ],
        [
            "First, let's touch the afternoon of this presentation.",
            "Basically the full path starting from brief introduction of graph transduction and we point out some common problem on existing graph transduction algorithm.",
            "Then we propose our solution to get a more robust transduction learning algorithm.",
            "And finally I."
        ],
        [
            "They show some example.",
            "Here is the graph transduction.",
            "Yeah.",
            "In this framework, giving a set of numbers, both labeled, unlabeled and we expect to get some classification result by using both labeled and unlabeled data.",
            "Here you can see in the left side.",
            "We have a lot of unlabeled data and only two labels for these two class problem and this is the expected and classification result.",
            "The center demo shows how the graph propagation works first.",
            "We construct the graph at the bus judges just by connecting ages.",
            "Then we begin to propagation to get the soft label.",
            "Finally we do some thresholding or ranking to get the hard labels."
        ],
        [
            "Now we formalize previous nights, give some notation in this presentation, give a data set.",
            "Is X have two parts, one is labeled Subset, Excel and unlabeled subset.",
            "Xu here we use graph transaction based on the undirected graph.",
            "This is a note is just the samples and ages is just use some simple similarity metrics to calculate similarity similarity measure.",
            "The edge weight, then we can get a weight matrix W here and then we just sum each each column of W guitar node degree D and the graph not flashing, just the difference of D&W the normalized flashes just divided by the square root of the degree metrics.",
            "We have the label matrix Y.",
            "This is an example of the label matrix for two class problem.",
            "Here we have four nodes.",
            "The top three is just labeled 2 for the.",
            "Example and one is negative sum is this one is labeled as positive.",
            "We just set is 1 the if the column vector is all zero means this is a."
        ],
        [
            "Labeled data.",
            "OK, for traditional graph transduction method you really want to estimate our classification function which is minimized predefined costs.",
            "This cost function usually try to trade off 2 components.",
            "One is the smoothness of the function on the graph.",
            "Another part is the local fitting on the giving labels."
        ],
        [
            "Here are two periods of work, or with different cost function definition.",
            "While it's called Gaussian fields and harmonic function and the other is local and global consistency.",
            "Two basic difference here.",
            "For the Gaussian fields method because they said hard constraint on the.",
            "Giving labels to push the to enforce the classification function F exactly equal to the given labels.",
            "So the second term, the fitnesses just vanished and only the smoothly evaluation and for the local and global consistency use L2 form to to calculate the local fitness and use coefficient mu to balance these two part.",
            "Another yeah, for the caution methods they use standard graph lashing and for the local and global consistent use some just alarm."
        ],
        [
            "It's the graph location.",
            "OK, in previous work these two method works very good and have some empirical success in the data.",
            "But there clearly is some problem hasn't been solved here.",
            "I should problem case.",
            "The first 2 is about graph construction.",
            "The first one you can see this is over connected graph because our natural age is here we can show how this kind of bad graph affects the result.",
            "You can see that too closed nodes but assigned different labels.",
            "Just here, so this over connected graph, just misleading the transaction result.",
            "And the second is about which usually the calculate the wage you some kernel function.",
            "But when you set improper concise the wait will be changed.",
            "You can see here the strong wages among these two data, many photo.",
            "So the propagation result also has some problem because the two close nodes but assign different labels.",
            "And here is another two bad example about the initialization of the labor metrics.",
            "Why?",
            "Because sometimes we cannot expect good initial labels.",
            "For example here you can see the two labels.",
            "When is red on one screen but the green one is locked in the space area, so this one will place very less important during propagation, so we can see how the result looks like.",
            "This is a result.",
            "Can see prefer to the red red class.",
            "And this is another problem is called Imbalance label issue.",
            "If the giving neighbors is not proportional to the class size.",
            "Then the final propagation result will prefer to class with more initial labels.",
            "So Green one is dominant here.",
            "OK.",
            "So this problem is."
        ],
        [
            "The basic motivation.",
            "Obama new graph transduction approach.",
            "Now I began to present our solution and formulation.",
            "First we start from the local and global consistent cost function.",
            "Here we just use the metric formulation here.",
            "This is a graph location and this is the second term about fitness.",
            "We make two modification for the original 1.",
            "First we change this cost function from single variable to buy variable.",
            "The two variables here while the label matrix.",
            "In other words, why is not constant now it has some initialization value but later can be updated.",
            "Second modifications.",
            "We ate some term cutaway.",
            "This is a label regularizer.",
            "We call this.",
            "Blame it calculated by use this equation and the small way just diagonal vector of the big way.",
            "I will show example how to calculate this label regularizer."
        ],
        [
            "This this term is for normalization.",
            "He has two important here.",
            "First, is normalized levels among different classes to handle the imbalance issue, a second of which labels based on its degree.",
            "Here example, you can see the full note here from the degrees from one to four.",
            "This is the most sparse area and this is this area.",
            "We have three nodes already labeled.",
            "The first one, second one server.",
            "These two is assigned the same label and this is a negative label.",
            "The normalization calculation, just like this one for the first notes because it belonged to positive label.",
            "So we just 1 / 1 + 4.",
            "Three 3 is coming from the third node.",
            "Degree is 3 because the first one celebrates in same class.",
            "So here now after calculation is 1 by 4 as this one is 3 by 4.",
            "OK, after this calculation can see the label from different class.",
            "Now it's just some to one.",
            "This is 1 important thing here.",
            "Actually if we know the knowledge of the class size we can incorporate it to further calculation.",
            "And the second importances.",
            "Here you can see the sparse label here.",
            "Place less importance when we calculate the fitness as that."
        ],
        [
            "One is more important.",
            "OK, now we have a new optimization problem.",
            "Have two variables, one is FY is Y.",
            "This is a hard problem because F is continuous.",
            "But why is binary and with some constraint hyy has some constant because here we assign only one class label to each nodes.",
            "So each column in the why just one nonzero element.",
            "So this problem we just try to optimize one by one.",
            "So we transferred to our alternating minimization problem with two sub problems.",
            "The first day we try to solve the continuous F because this way is convex.",
            "Optimization is easy to do optimization.",
            "This is just the result is which is well known and for the most previous work we just use this one as a final result and stop.",
            "Here you can see the final classification function.",
            "F is just a linear relationship of the given label matrix.",
            "Why?",
            "So, that's the reason why final result is so sensitive to the initial labels.",
            "OK, then in our own method we insert this optimal F. To the original cost function, we have a new cost function with only variable Y.",
            "So in this new formulation we.",
            "Do the optimized optimization have implicitly but explicitly we optimize why directly?"
        ],
        [
            "OK, we re write this new formulation of the cost function in terms of why so why is binary and have some constraint?",
            "This is very similar to the Max cut problem, just with some very slightly changes which try to simplifies it.",
            "Then with you some variable replacements here with UZZ equal we multiple Y.",
            "This is a normalized level metrics and a is calculated as.",
            "This one is constant actually because it just depends on the graph construction.",
            "So now we have the new cost function in terms of variables.",
            "The this is just a quadratic form, but it cannot be used as a convex optimization because this has some constraint these calculated from the binary Y. OK, so motivated by the greater search on a Max cut problem here we try to use some gradient grid method to optimize the Q function which use the chain rule to calculate the partial differential or code in terms of why we get this formulation and partial pressures is easy to compute Justin."
        ],
        [
            "Form.",
            "OK, you know what method we try to search the granted?"
        ],
        [
            "Matrix which is just in the."
        ],
        [
            "This one and find the position which can reduce the costs most before leveling.",
            "So this is just find the optimal position for labeling and with leveling procedure is just change the certain element environmetrics from zero to 1 here example.",
            "In the term in current iteration we have two labeled data.",
            "The first 2 and current the granted matrix is in this kind of formulation because for the labeled data we don't care the granted, so just use stuff.",
            "But here we have four values.",
            "It means here if we leave this one, change the value from a wife from zero to one, we can get the cost down is 0.3, so this is the most beneficial beneficial one so we can find then the optimal position is I = 3, the third nodes.",
            "It's better to laugh as the first class, so I will update the wide metrics, get one new label.",
            "We repeat this procedure because we are why updated.",
            "We can get a new granite metrics until all the labels are labeled just the unable to set is."
        ],
        [
            "Is empty.",
            "Finally, we have an algorithm is just very easy with five steps.",
            "First one from start from the initial labor metrics, we calculate the way metrics the label recognizing metrics that we calculate the gradient metrics.",
            "We find optimal position.",
            "I started yesterday and label is the corresponding.",
            "Why matrix?",
            "Then we update the way get the next iteration, which in order to avoid the oscillation which usually would remove the newly labeled data.",
            "And put it in the lab, set an update.",
            "These two set X T + 1 and as U T + 1 and repeat this whole procedure until there's no label."
        ],
        [
            "For leveling.",
            "Here is some intuition of our method.",
            "First previous method.",
            "T and Gaussian fields method that just prematurely to commit leveling procedure that just do once of the optimization.",
            "But our method is relatively in for the levels and each iteration we only select the most beneficial one for labeling the gradient.",
            "The Max cut problem can be solved by greedy search with can achieve 0.5 to the to the optimum.",
            "The best solution is my SDP.",
            "The value is 0.878 but is too."
        ],
        [
            "No.",
            "OK, here is the intuition of how our method work.",
            "Here we have the true label.",
            "This is a label with very bad label location for the green class.",
            "OK, let's start seeing them again.",
            "Each time we assign one label.",
            "Meanwhile we change the scale of the nodes because this is corresponding to the to the label recognizer.",
            "It means that after each battle, each new labor, we also change update the web metrics."
        ],
        [
            "OK, so the computation efficiency basically is just comical time, but we can run it more efficiently than three reasons why it can be more efficiently.",
            "First, we can apply superposition approach to achieve the incremental update of the grand metrics.",
            "Similar technique is applied in our recent 3 PR work.",
            "The second reason we can only stop because.",
            "After a week getting enough neighbors, we can just.",
            "Catch the final result by calculating the F classification function.",
            "The third reason is we can do multiple loads labeling for each iteration.",
            "Here in our agreement we just show one load for each iteration.",
            "Actually, we can do."
        ],
        [
            "Multiple notes.",
            "OK, we have some experiment to show.",
            "Here is a very difficult case because this is a non separable case the two.",
            "Data many photo but with background noise and also the labor location is not good.",
            "This one is close to the intersection and this one is far away so let's see how this method works here.",
            "We also show the the cost function during convergence procedure.",
            "Yeah sorry it's not start from the initial statement we can so you can see the two phenomena from this demo first.",
            "In the first maybe 30 to 40 rounds is the costly.",
            "Reduce very fast.",
            "And the second is the.",
            "Convergence procedure is not monotonic because we set the granted step sizes is just one unit, so it is the discrete step size.",
            "Yep.",
            "OK.",
            "Secure, which avoids table.",
            "We only label one notes so it's quite slow this initial stage.",
            "Early stage yeah.",
            "So the size is changed during the propagation.",
            "After the propagation you can see the dense area node sizes bigger and the sparse area like here is very small because the propagation is very not not that important during propagation."
        ],
        [
            "Now here we have some experiments on the real data.",
            "This is for text classification is webcap data is it contains 1000 documents is 2 classes cost and non costs and the feature is pretty high, is close to 5000.",
            "We compare with the mass report in these people as an hour 2005.",
            "This is about not pushing as well and not pushing recognized least square.",
            "This is 2 method which is already compared with transductive SVM, which is proud very good so we use 100 random.",
            "Test evaluation is based on the average error rate, so with such the label from five we change the initial labels size from 5 to 40.",
            "Except the first one, the extreme case will only 5 notes.",
            "Our method is not as good as this this one as it's three.",
            "In other case I must improve the performance print or not."
        ],
        [
            "OK, this is another very successful example is about image classification, which is we compared with lips paper about local and global consistency.",
            "We classify four data for digital and 1234 and the the sample size is 3800 and the image is 16 by 16.",
            "And compare with this three, actually 4 methods and we run 20 times random test evaluation step based on the mean average average error rate and start from 5 initial labels to 50.",
            "The red lines.",
            "Our method actually with only five labels total, we can achieve only 1% average which is increased a lot.",
            "Compare this to experiment, can see in the image dataset we improved the performance almost several 100% reason is the graph construction here step a very important role for this data set is more samples close to 4000 and the dimensions is very low 25 twenty 2200.",
            "So the graph is very short, but for this one this less samples and high dimension and also the feature is discrete.",
            "It's just a 01 so.",
            "We can have very stable results with only five levels here."
        ],
        [
            "OK, this is summary of our approach.",
            "Here we propose a new graph.",
            "Transaction learning method is concert cost graph transduction as cost over both Y&F we ate labor normalization term and we use a graphic optimization.",
            "Great automated optimization method to optimize F&Y.",
            "Here F is implicitly optimized and why is explicitly optimized this produce gradually propagation start algorithm.",
            "It's fast and robust to to the initial label, and we reduce the average at least more than half on the two very long data set.",
            "Is there some open problems we haven't touched in this work?",
            "For example, we didn't know how to extend this method to unseen data, just how to train a good model and applied to the new data.",
            "We haven't sold here.",
            "Yep, that's it.",
            "Thank you.",
            "Thank you very much.",
            "Are there any questions?",
            "Yep.",
            "So, so your formulation seems like the key differences introduced this two set variables F&Y and you have particular restriction on Y and the normalization applied on why.",
            "I was wondering if you apply the same thing to have, for instance restricted solution F to be binary with appropriate normalization with that sort of serving the same purpose rather than adding additional terms.",
            "I mean just add some constant.",
            "Optimization.",
            "I mean just add some constraints over the function and do optimization."
        ],
        [
            "So at least my understanding of the formulations that you introduce two set variables, right?",
            "So haven't Y and the key difference between Evan Wise you have binary restriction of Y and then you have certain normalization applied to Y.",
            "So I was wondering if you just directly apply those things to F would then lens on the same layer results or you have some other concern for this particular?",
            "Formulation yeah, yeah, it's a good question, but here we this is cost function basically is based on the graph regularization, the two components one is trying to fix, this one on the given labels.",
            "Why so?",
            "Why is some some constant?",
            "I mean have some give the initial label and F is a classification function.",
            "So if you exist count constraint on a function, how do you provide the initial label information here?",
            "So yeah.",
            "Thanks, are there any other questions?",
            "OK, well then let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Jin Wang.",
                    "label": 0
                },
                {
                    "sent": "My talk is about graph graph transduction.",
                    "label": 1
                },
                {
                    "sent": "Why alternating memorization this work was surprised by Professor Tony Jabra and shifting from Columbia University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, let's touch the afternoon of this presentation.",
                    "label": 0
                },
                {
                    "sent": "Basically the full path starting from brief introduction of graph transduction and we point out some common problem on existing graph transduction algorithm.",
                    "label": 1
                },
                {
                    "sent": "Then we propose our solution to get a more robust transduction learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally I.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They show some example.",
                    "label": 0
                },
                {
                    "sent": "Here is the graph transduction.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "In this framework, giving a set of numbers, both labeled, unlabeled and we expect to get some classification result by using both labeled and unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Here you can see in the left side.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of unlabeled data and only two labels for these two class problem and this is the expected and classification result.",
                    "label": 0
                },
                {
                    "sent": "The center demo shows how the graph propagation works first.",
                    "label": 0
                },
                {
                    "sent": "We construct the graph at the bus judges just by connecting ages.",
                    "label": 0
                },
                {
                    "sent": "Then we begin to propagation to get the soft label.",
                    "label": 0
                },
                {
                    "sent": "Finally we do some thresholding or ranking to get the hard labels.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we formalize previous nights, give some notation in this presentation, give a data set.",
                    "label": 0
                },
                {
                    "sent": "Is X have two parts, one is labeled Subset, Excel and unlabeled subset.",
                    "label": 1
                },
                {
                    "sent": "Xu here we use graph transaction based on the undirected graph.",
                    "label": 1
                },
                {
                    "sent": "This is a note is just the samples and ages is just use some simple similarity metrics to calculate similarity similarity measure.",
                    "label": 0
                },
                {
                    "sent": "The edge weight, then we can get a weight matrix W here and then we just sum each each column of W guitar node degree D and the graph not flashing, just the difference of D&W the normalized flashes just divided by the square root of the degree metrics.",
                    "label": 1
                },
                {
                    "sent": "We have the label matrix Y.",
                    "label": 0
                },
                {
                    "sent": "This is an example of the label matrix for two class problem.",
                    "label": 0
                },
                {
                    "sent": "Here we have four nodes.",
                    "label": 0
                },
                {
                    "sent": "The top three is just labeled 2 for the.",
                    "label": 0
                },
                {
                    "sent": "Example and one is negative sum is this one is labeled as positive.",
                    "label": 0
                },
                {
                    "sent": "We just set is 1 the if the column vector is all zero means this is a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Labeled data.",
                    "label": 0
                },
                {
                    "sent": "OK, for traditional graph transduction method you really want to estimate our classification function which is minimized predefined costs.",
                    "label": 1
                },
                {
                    "sent": "This cost function usually try to trade off 2 components.",
                    "label": 0
                },
                {
                    "sent": "One is the smoothness of the function on the graph.",
                    "label": 0
                },
                {
                    "sent": "Another part is the local fitting on the giving labels.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are two periods of work, or with different cost function definition.",
                    "label": 0
                },
                {
                    "sent": "While it's called Gaussian fields and harmonic function and the other is local and global consistency.",
                    "label": 1
                },
                {
                    "sent": "Two basic difference here.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian fields method because they said hard constraint on the.",
                    "label": 0
                },
                {
                    "sent": "Giving labels to push the to enforce the classification function F exactly equal to the given labels.",
                    "label": 0
                },
                {
                    "sent": "So the second term, the fitnesses just vanished and only the smoothly evaluation and for the local and global consistency use L2 form to to calculate the local fitness and use coefficient mu to balance these two part.",
                    "label": 0
                },
                {
                    "sent": "Another yeah, for the caution methods they use standard graph lashing and for the local and global consistent use some just alarm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the graph location.",
                    "label": 0
                },
                {
                    "sent": "OK, in previous work these two method works very good and have some empirical success in the data.",
                    "label": 0
                },
                {
                    "sent": "But there clearly is some problem hasn't been solved here.",
                    "label": 0
                },
                {
                    "sent": "I should problem case.",
                    "label": 0
                },
                {
                    "sent": "The first 2 is about graph construction.",
                    "label": 0
                },
                {
                    "sent": "The first one you can see this is over connected graph because our natural age is here we can show how this kind of bad graph affects the result.",
                    "label": 0
                },
                {
                    "sent": "You can see that too closed nodes but assigned different labels.",
                    "label": 0
                },
                {
                    "sent": "Just here, so this over connected graph, just misleading the transaction result.",
                    "label": 1
                },
                {
                    "sent": "And the second is about which usually the calculate the wage you some kernel function.",
                    "label": 0
                },
                {
                    "sent": "But when you set improper concise the wait will be changed.",
                    "label": 0
                },
                {
                    "sent": "You can see here the strong wages among these two data, many photo.",
                    "label": 0
                },
                {
                    "sent": "So the propagation result also has some problem because the two close nodes but assign different labels.",
                    "label": 0
                },
                {
                    "sent": "And here is another two bad example about the initialization of the labor metrics.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because sometimes we cannot expect good initial labels.",
                    "label": 0
                },
                {
                    "sent": "For example here you can see the two labels.",
                    "label": 0
                },
                {
                    "sent": "When is red on one screen but the green one is locked in the space area, so this one will place very less important during propagation, so we can see how the result looks like.",
                    "label": 0
                },
                {
                    "sent": "This is a result.",
                    "label": 0
                },
                {
                    "sent": "Can see prefer to the red red class.",
                    "label": 0
                },
                {
                    "sent": "And this is another problem is called Imbalance label issue.",
                    "label": 0
                },
                {
                    "sent": "If the giving neighbors is not proportional to the class size.",
                    "label": 0
                },
                {
                    "sent": "Then the final propagation result will prefer to class with more initial labels.",
                    "label": 0
                },
                {
                    "sent": "So Green one is dominant here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this problem is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic motivation.",
                    "label": 0
                },
                {
                    "sent": "Obama new graph transduction approach.",
                    "label": 1
                },
                {
                    "sent": "Now I began to present our solution and formulation.",
                    "label": 0
                },
                {
                    "sent": "First we start from the local and global consistent cost function.",
                    "label": 0
                },
                {
                    "sent": "Here we just use the metric formulation here.",
                    "label": 0
                },
                {
                    "sent": "This is a graph location and this is the second term about fitness.",
                    "label": 0
                },
                {
                    "sent": "We make two modification for the original 1.",
                    "label": 0
                },
                {
                    "sent": "First we change this cost function from single variable to buy variable.",
                    "label": 0
                },
                {
                    "sent": "The two variables here while the label matrix.",
                    "label": 0
                },
                {
                    "sent": "In other words, why is not constant now it has some initialization value but later can be updated.",
                    "label": 0
                },
                {
                    "sent": "Second modifications.",
                    "label": 0
                },
                {
                    "sent": "We ate some term cutaway.",
                    "label": 0
                },
                {
                    "sent": "This is a label regularizer.",
                    "label": 0
                },
                {
                    "sent": "We call this.",
                    "label": 0
                },
                {
                    "sent": "Blame it calculated by use this equation and the small way just diagonal vector of the big way.",
                    "label": 0
                },
                {
                    "sent": "I will show example how to calculate this label regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This this term is for normalization.",
                    "label": 0
                },
                {
                    "sent": "He has two important here.",
                    "label": 0
                },
                {
                    "sent": "First, is normalized levels among different classes to handle the imbalance issue, a second of which labels based on its degree.",
                    "label": 1
                },
                {
                    "sent": "Here example, you can see the full note here from the degrees from one to four.",
                    "label": 0
                },
                {
                    "sent": "This is the most sparse area and this is this area.",
                    "label": 0
                },
                {
                    "sent": "We have three nodes already labeled.",
                    "label": 0
                },
                {
                    "sent": "The first one, second one server.",
                    "label": 0
                },
                {
                    "sent": "These two is assigned the same label and this is a negative label.",
                    "label": 0
                },
                {
                    "sent": "The normalization calculation, just like this one for the first notes because it belonged to positive label.",
                    "label": 0
                },
                {
                    "sent": "So we just 1 / 1 + 4.",
                    "label": 0
                },
                {
                    "sent": "Three 3 is coming from the third node.",
                    "label": 0
                },
                {
                    "sent": "Degree is 3 because the first one celebrates in same class.",
                    "label": 0
                },
                {
                    "sent": "So here now after calculation is 1 by 4 as this one is 3 by 4.",
                    "label": 0
                },
                {
                    "sent": "OK, after this calculation can see the label from different class.",
                    "label": 0
                },
                {
                    "sent": "Now it's just some to one.",
                    "label": 0
                },
                {
                    "sent": "This is 1 important thing here.",
                    "label": 0
                },
                {
                    "sent": "Actually if we know the knowledge of the class size we can incorporate it to further calculation.",
                    "label": 0
                },
                {
                    "sent": "And the second importances.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the sparse label here.",
                    "label": 0
                },
                {
                    "sent": "Place less importance when we calculate the fitness as that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is more important.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have a new optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Have two variables, one is FY is Y.",
                    "label": 0
                },
                {
                    "sent": "This is a hard problem because F is continuous.",
                    "label": 1
                },
                {
                    "sent": "But why is binary and with some constraint hyy has some constant because here we assign only one class label to each nodes.",
                    "label": 0
                },
                {
                    "sent": "So each column in the why just one nonzero element.",
                    "label": 0
                },
                {
                    "sent": "So this problem we just try to optimize one by one.",
                    "label": 1
                },
                {
                    "sent": "So we transferred to our alternating minimization problem with two sub problems.",
                    "label": 0
                },
                {
                    "sent": "The first day we try to solve the continuous F because this way is convex.",
                    "label": 0
                },
                {
                    "sent": "Optimization is easy to do optimization.",
                    "label": 0
                },
                {
                    "sent": "This is just the result is which is well known and for the most previous work we just use this one as a final result and stop.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the final classification function.",
                    "label": 0
                },
                {
                    "sent": "F is just a linear relationship of the given label matrix.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "So, that's the reason why final result is so sensitive to the initial labels.",
                    "label": 0
                },
                {
                    "sent": "OK, then in our own method we insert this optimal F. To the original cost function, we have a new cost function with only variable Y.",
                    "label": 0
                },
                {
                    "sent": "So in this new formulation we.",
                    "label": 0
                },
                {
                    "sent": "Do the optimized optimization have implicitly but explicitly we optimize why directly?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we re write this new formulation of the cost function in terms of why so why is binary and have some constraint?",
                    "label": 0
                },
                {
                    "sent": "This is very similar to the Max cut problem, just with some very slightly changes which try to simplifies it.",
                    "label": 1
                },
                {
                    "sent": "Then with you some variable replacements here with UZZ equal we multiple Y.",
                    "label": 0
                },
                {
                    "sent": "This is a normalized level metrics and a is calculated as.",
                    "label": 0
                },
                {
                    "sent": "This one is constant actually because it just depends on the graph construction.",
                    "label": 0
                },
                {
                    "sent": "So now we have the new cost function in terms of variables.",
                    "label": 0
                },
                {
                    "sent": "The this is just a quadratic form, but it cannot be used as a convex optimization because this has some constraint these calculated from the binary Y. OK, so motivated by the greater search on a Max cut problem here we try to use some gradient grid method to optimize the Q function which use the chain rule to calculate the partial differential or code in terms of why we get this formulation and partial pressures is easy to compute Justin.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Form.",
                    "label": 0
                },
                {
                    "sent": "OK, you know what method we try to search the granted?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix which is just in the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one and find the position which can reduce the costs most before leveling.",
                    "label": 0
                },
                {
                    "sent": "So this is just find the optimal position for labeling and with leveling procedure is just change the certain element environmetrics from zero to 1 here example.",
                    "label": 0
                },
                {
                    "sent": "In the term in current iteration we have two labeled data.",
                    "label": 0
                },
                {
                    "sent": "The first 2 and current the granted matrix is in this kind of formulation because for the labeled data we don't care the granted, so just use stuff.",
                    "label": 0
                },
                {
                    "sent": "But here we have four values.",
                    "label": 0
                },
                {
                    "sent": "It means here if we leave this one, change the value from a wife from zero to one, we can get the cost down is 0.3, so this is the most beneficial beneficial one so we can find then the optimal position is I = 3, the third nodes.",
                    "label": 0
                },
                {
                    "sent": "It's better to laugh as the first class, so I will update the wide metrics, get one new label.",
                    "label": 0
                },
                {
                    "sent": "We repeat this procedure because we are why updated.",
                    "label": 0
                },
                {
                    "sent": "We can get a new granite metrics until all the labels are labeled just the unable to set is.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is empty.",
                    "label": 0
                },
                {
                    "sent": "Finally, we have an algorithm is just very easy with five steps.",
                    "label": 0
                },
                {
                    "sent": "First one from start from the initial labor metrics, we calculate the way metrics the label recognizing metrics that we calculate the gradient metrics.",
                    "label": 0
                },
                {
                    "sent": "We find optimal position.",
                    "label": 0
                },
                {
                    "sent": "I started yesterday and label is the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Why matrix?",
                    "label": 0
                },
                {
                    "sent": "Then we update the way get the next iteration, which in order to avoid the oscillation which usually would remove the newly labeled data.",
                    "label": 0
                },
                {
                    "sent": "And put it in the lab, set an update.",
                    "label": 0
                },
                {
                    "sent": "These two set X T + 1 and as U T + 1 and repeat this whole procedure until there's no label.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For leveling.",
                    "label": 0
                },
                {
                    "sent": "Here is some intuition of our method.",
                    "label": 1
                },
                {
                    "sent": "First previous method.",
                    "label": 0
                },
                {
                    "sent": "T and Gaussian fields method that just prematurely to commit leveling procedure that just do once of the optimization.",
                    "label": 0
                },
                {
                    "sent": "But our method is relatively in for the levels and each iteration we only select the most beneficial one for labeling the gradient.",
                    "label": 1
                },
                {
                    "sent": "The Max cut problem can be solved by greedy search with can achieve 0.5 to the to the optimum.",
                    "label": 1
                },
                {
                    "sent": "The best solution is my SDP.",
                    "label": 0
                },
                {
                    "sent": "The value is 0.878 but is too.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, here is the intuition of how our method work.",
                    "label": 0
                },
                {
                    "sent": "Here we have the true label.",
                    "label": 0
                },
                {
                    "sent": "This is a label with very bad label location for the green class.",
                    "label": 0
                },
                {
                    "sent": "OK, let's start seeing them again.",
                    "label": 0
                },
                {
                    "sent": "Each time we assign one label.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile we change the scale of the nodes because this is corresponding to the to the label recognizer.",
                    "label": 1
                },
                {
                    "sent": "It means that after each battle, each new labor, we also change update the web metrics.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the computation efficiency basically is just comical time, but we can run it more efficiently than three reasons why it can be more efficiently.",
                    "label": 1
                },
                {
                    "sent": "First, we can apply superposition approach to achieve the incremental update of the grand metrics.",
                    "label": 1
                },
                {
                    "sent": "Similar technique is applied in our recent 3 PR work.",
                    "label": 0
                },
                {
                    "sent": "The second reason we can only stop because.",
                    "label": 0
                },
                {
                    "sent": "After a week getting enough neighbors, we can just.",
                    "label": 0
                },
                {
                    "sent": "Catch the final result by calculating the F classification function.",
                    "label": 0
                },
                {
                    "sent": "The third reason is we can do multiple loads labeling for each iteration.",
                    "label": 1
                },
                {
                    "sent": "Here in our agreement we just show one load for each iteration.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multiple notes.",
                    "label": 0
                },
                {
                    "sent": "OK, we have some experiment to show.",
                    "label": 0
                },
                {
                    "sent": "Here is a very difficult case because this is a non separable case the two.",
                    "label": 0
                },
                {
                    "sent": "Data many photo but with background noise and also the labor location is not good.",
                    "label": 0
                },
                {
                    "sent": "This one is close to the intersection and this one is far away so let's see how this method works here.",
                    "label": 0
                },
                {
                    "sent": "We also show the the cost function during convergence procedure.",
                    "label": 0
                },
                {
                    "sent": "Yeah sorry it's not start from the initial statement we can so you can see the two phenomena from this demo first.",
                    "label": 0
                },
                {
                    "sent": "In the first maybe 30 to 40 rounds is the costly.",
                    "label": 0
                },
                {
                    "sent": "Reduce very fast.",
                    "label": 0
                },
                {
                    "sent": "And the second is the.",
                    "label": 0
                },
                {
                    "sent": "Convergence procedure is not monotonic because we set the granted step sizes is just one unit, so it is the discrete step size.",
                    "label": 1
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Secure, which avoids table.",
                    "label": 0
                },
                {
                    "sent": "We only label one notes so it's quite slow this initial stage.",
                    "label": 0
                },
                {
                    "sent": "Early stage yeah.",
                    "label": 0
                },
                {
                    "sent": "So the size is changed during the propagation.",
                    "label": 0
                },
                {
                    "sent": "After the propagation you can see the dense area node sizes bigger and the sparse area like here is very small because the propagation is very not not that important during propagation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here we have some experiments on the real data.",
                    "label": 0
                },
                {
                    "sent": "This is for text classification is webcap data is it contains 1000 documents is 2 classes cost and non costs and the feature is pretty high, is close to 5000.",
                    "label": 0
                },
                {
                    "sent": "We compare with the mass report in these people as an hour 2005.",
                    "label": 0
                },
                {
                    "sent": "This is about not pushing as well and not pushing recognized least square.",
                    "label": 0
                },
                {
                    "sent": "This is 2 method which is already compared with transductive SVM, which is proud very good so we use 100 random.",
                    "label": 0
                },
                {
                    "sent": "Test evaluation is based on the average error rate, so with such the label from five we change the initial labels size from 5 to 40.",
                    "label": 1
                },
                {
                    "sent": "Except the first one, the extreme case will only 5 notes.",
                    "label": 0
                },
                {
                    "sent": "Our method is not as good as this this one as it's three.",
                    "label": 0
                },
                {
                    "sent": "In other case I must improve the performance print or not.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is another very successful example is about image classification, which is we compared with lips paper about local and global consistency.",
                    "label": 0
                },
                {
                    "sent": "We classify four data for digital and 1234 and the the sample size is 3800 and the image is 16 by 16.",
                    "label": 0
                },
                {
                    "sent": "And compare with this three, actually 4 methods and we run 20 times random test evaluation step based on the mean average average error rate and start from 5 initial labels to 50.",
                    "label": 1
                },
                {
                    "sent": "The red lines.",
                    "label": 0
                },
                {
                    "sent": "Our method actually with only five labels total, we can achieve only 1% average which is increased a lot.",
                    "label": 0
                },
                {
                    "sent": "Compare this to experiment, can see in the image dataset we improved the performance almost several 100% reason is the graph construction here step a very important role for this data set is more samples close to 4000 and the dimensions is very low 25 twenty 2200.",
                    "label": 0
                },
                {
                    "sent": "So the graph is very short, but for this one this less samples and high dimension and also the feature is discrete.",
                    "label": 0
                },
                {
                    "sent": "It's just a 01 so.",
                    "label": 0
                },
                {
                    "sent": "We can have very stable results with only five levels here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is summary of our approach.",
                    "label": 0
                },
                {
                    "sent": "Here we propose a new graph.",
                    "label": 0
                },
                {
                    "sent": "Transaction learning method is concert cost graph transduction as cost over both Y&F we ate labor normalization term and we use a graphic optimization.",
                    "label": 1
                },
                {
                    "sent": "Great automated optimization method to optimize F&Y.",
                    "label": 0
                },
                {
                    "sent": "Here F is implicitly optimized and why is explicitly optimized this produce gradually propagation start algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's fast and robust to to the initial label, and we reduce the average at least more than half on the two very long data set.",
                    "label": 1
                },
                {
                    "sent": "Is there some open problems we haven't touched in this work?",
                    "label": 0
                },
                {
                    "sent": "For example, we didn't know how to extend this method to unseen data, just how to train a good model and applied to the new data.",
                    "label": 0
                },
                {
                    "sent": "We haven't sold here.",
                    "label": 0
                },
                {
                    "sent": "Yep, that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So, so your formulation seems like the key differences introduced this two set variables F&Y and you have particular restriction on Y and the normalization applied on why.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you apply the same thing to have, for instance restricted solution F to be binary with appropriate normalization with that sort of serving the same purpose rather than adding additional terms.",
                    "label": 0
                },
                {
                    "sent": "I mean just add some constant.",
                    "label": 0
                },
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "I mean just add some constraints over the function and do optimization.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at least my understanding of the formulations that you introduce two set variables, right?",
                    "label": 0
                },
                {
                    "sent": "So haven't Y and the key difference between Evan Wise you have binary restriction of Y and then you have certain normalization applied to Y.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering if you just directly apply those things to F would then lens on the same layer results or you have some other concern for this particular?",
                    "label": 0
                },
                {
                    "sent": "Formulation yeah, yeah, it's a good question, but here we this is cost function basically is based on the graph regularization, the two components one is trying to fix, this one on the given labels.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "Why is some some constant?",
                    "label": 0
                },
                {
                    "sent": "I mean have some give the initial label and F is a classification function.",
                    "label": 0
                },
                {
                    "sent": "So if you exist count constraint on a function, how do you provide the initial label information here?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Thanks, are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, well then let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}