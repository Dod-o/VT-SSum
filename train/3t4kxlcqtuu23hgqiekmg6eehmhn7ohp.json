{
    "id": "3t4kxlcqtuu23hgqiekmg6eehmhn7ohp",
    "title": "Exp-Concavity of Proper Composite Losses",
    "info": {
        "author": [
            "Parameswaran Kamalaruban, College of Engineering and Computer Science, Australian National University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_kamalaruban_composite_losses/",
    "segmentation": [
        [
            "So this is joint work with my supervisor, Bob Williamson and Xenazine apps, concavity of proper composite losses, proper composite loss is a composition of a propolis and then invertible link."
        ],
        [
            "The main setting that we consider in this work is game of prediction with expert advice.",
            "I guess in most of the talk this is well explained, so I'll quickly go through it.",
            "Why is an outcome space and maybe the prediction space an L is the loss function, then the game of prediction with expert advice can be described as at each time instance in experts will make their prediction in the prediction space and the learner will merge the experts prediction to come up with his own decision and then the environment will reveal his reveals the actual.",
            "The goal of the.",
            "Learner is to minimize the regret which is the difference between the cumulative loss of the learner and the cumulative loss of the best expert.",
            "So."
        ],
        [
            "Of when the outcome space is in class, multiclass loss can be represented by.",
            "By a vector of partial losses and the Super prediction set of that multiclass loss can be defined by the set of all points, which is about the last curve in the N dimensional space.",
            "So mainly we consider two types of last classes.",
            "One is beta Mixable and then another one is Alpha X concave.",
            "The beta mixable loss can be defined as follows.",
            "For which.",
            "The negative beta exponentiated super predictions set should be convex and for Alpha is concorrenti.",
            "The negative Alpha exponentiated partial losses should be concave.",
            "Why we consider these two types of loss function in the game of prediction?",
            "With expert advice, there are two special instances in which constant bound is guaranteed.",
            "The one is if we use the beta mixable losses with aggregating algorithm with learning rate beta, we are guaranteed to achieve a regret bound of Logan over beta.",
            "And the same time if you for Alpha X concave losses, if you use the weighted average algorithm with learning rate Alpha, we are guaranteed to get regret bound of Logan over Alpha.",
            "But if a loss is Alpha is concave, it is guaranteed that it is beta mixable.",
            "For some beta greater than or equals Alpha, but the other direction is generally not guaranteed, so theoretically the aggregating algorithm is more stronger than weighted average algorithm, but computationally weighted average algorithm is more efficient.",
            "So the main question is if you're given a proper beta mixable loss, possibly non X concave, is it possible to re parameterized that loss such that it can be beaten mixable with the same?",
            "Beta Epsilon gave the same beta and we can use the close with weighted average algorithm to achieve the better regret bound.",
            "And for the binary, for the binary case, we actually come up with two complete link functions to complete.",
            "Reparameterization and for the multiclass case, we provide an approximate solution, so I don't think that you can explain the approaches, but I'll quickly go through the results first."
        ],
        [
            "If we consider binary squared plus with the partial loss is given by this and.",
            "The mixability consent of this binary squared loss is 1 and it's in its original form.",
            "It's one by four A1 by for ex con K, so we provide actually two complete definition of link functions that can make this binary squared loss as One X concave.",
            "And for the boosting loss with a partial loss is defined by this one.",
            "In its original form, the Mixability constraint is true, and it's not expensive.",
            "It is possible to be parameterized this boosting loss by using this tooling functions."
        ],
        [
            "Actually, I'll quickly go through the approach in the geometric approach."
        ],
        [
            "We actually use the geometry of the F exponentiated super prediction set to construct that link function and in the."
        ],
        [
            "Calculus approach we actually use the basic property of the proper proposes to construct an integral form of that link function."
        ],
        [
            "And then in the approximation approach, we actually had to remove some hyperplanes to in the hyperplanes in the exponentiated super prediction set to come up with a solution.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with my supervisor, Bob Williamson and Xenazine apps, concavity of proper composite losses, proper composite loss is a composition of a propolis and then invertible link.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main setting that we consider in this work is game of prediction with expert advice.",
                    "label": 1
                },
                {
                    "sent": "I guess in most of the talk this is well explained, so I'll quickly go through it.",
                    "label": 0
                },
                {
                    "sent": "Why is an outcome space and maybe the prediction space an L is the loss function, then the game of prediction with expert advice can be described as at each time instance in experts will make their prediction in the prediction space and the learner will merge the experts prediction to come up with his own decision and then the environment will reveal his reveals the actual.",
                    "label": 1
                },
                {
                    "sent": "The goal of the.",
                    "label": 0
                },
                {
                    "sent": "Learner is to minimize the regret which is the difference between the cumulative loss of the learner and the cumulative loss of the best expert.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of when the outcome space is in class, multiclass loss can be represented by.",
                    "label": 1
                },
                {
                    "sent": "By a vector of partial losses and the Super prediction set of that multiclass loss can be defined by the set of all points, which is about the last curve in the N dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So mainly we consider two types of last classes.",
                    "label": 0
                },
                {
                    "sent": "One is beta Mixable and then another one is Alpha X concave.",
                    "label": 0
                },
                {
                    "sent": "The beta mixable loss can be defined as follows.",
                    "label": 0
                },
                {
                    "sent": "For which.",
                    "label": 0
                },
                {
                    "sent": "The negative beta exponentiated super predictions set should be convex and for Alpha is concorrenti.",
                    "label": 0
                },
                {
                    "sent": "The negative Alpha exponentiated partial losses should be concave.",
                    "label": 0
                },
                {
                    "sent": "Why we consider these two types of loss function in the game of prediction?",
                    "label": 0
                },
                {
                    "sent": "With expert advice, there are two special instances in which constant bound is guaranteed.",
                    "label": 0
                },
                {
                    "sent": "The one is if we use the beta mixable losses with aggregating algorithm with learning rate beta, we are guaranteed to achieve a regret bound of Logan over beta.",
                    "label": 0
                },
                {
                    "sent": "And the same time if you for Alpha X concave losses, if you use the weighted average algorithm with learning rate Alpha, we are guaranteed to get regret bound of Logan over Alpha.",
                    "label": 1
                },
                {
                    "sent": "But if a loss is Alpha is concave, it is guaranteed that it is beta mixable.",
                    "label": 0
                },
                {
                    "sent": "For some beta greater than or equals Alpha, but the other direction is generally not guaranteed, so theoretically the aggregating algorithm is more stronger than weighted average algorithm, but computationally weighted average algorithm is more efficient.",
                    "label": 1
                },
                {
                    "sent": "So the main question is if you're given a proper beta mixable loss, possibly non X concave, is it possible to re parameterized that loss such that it can be beaten mixable with the same?",
                    "label": 0
                },
                {
                    "sent": "Beta Epsilon gave the same beta and we can use the close with weighted average algorithm to achieve the better regret bound.",
                    "label": 0
                },
                {
                    "sent": "And for the binary, for the binary case, we actually come up with two complete link functions to complete.",
                    "label": 0
                },
                {
                    "sent": "Reparameterization and for the multiclass case, we provide an approximate solution, so I don't think that you can explain the approaches, but I'll quickly go through the results first.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we consider binary squared plus with the partial loss is given by this and.",
                    "label": 0
                },
                {
                    "sent": "The mixability consent of this binary squared loss is 1 and it's in its original form.",
                    "label": 0
                },
                {
                    "sent": "It's one by four A1 by for ex con K, so we provide actually two complete definition of link functions that can make this binary squared loss as One X concave.",
                    "label": 0
                },
                {
                    "sent": "And for the boosting loss with a partial loss is defined by this one.",
                    "label": 0
                },
                {
                    "sent": "In its original form, the Mixability constraint is true, and it's not expensive.",
                    "label": 0
                },
                {
                    "sent": "It is possible to be parameterized this boosting loss by using this tooling functions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, I'll quickly go through the approach in the geometric approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually use the geometry of the F exponentiated super prediction set to construct that link function and in the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculus approach we actually use the basic property of the proper proposes to construct an integral form of that link function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the approximation approach, we actually had to remove some hyperplanes to in the hyperplanes in the exponentiated super prediction set to come up with a solution.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}