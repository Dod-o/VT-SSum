{
    "id": "v5nq47hn3llhv7jymutsdslg6iiacxtn",
    "title": "Parallel Online Learning",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Software and Tools"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_langford_pol/",
    "segmentation": [
        [
            "Right so.",
            "I'm going to talk about parallel online learning.",
            "So.",
            "Wait?",
            "There we go.",
            "Oh, but how do we turn off the auto advance?",
            "He said OK.",
            "But OK. OK, So what we're talking about?",
            "Parallel learning and learning with lots of data and trying to make fast learning algorithms and the basic question is."
        ],
        [
            "So we try to make fast learning algorithms right, and there's a lot of people who adopt an approach which is.",
            "Oh, I do it again there.",
            "OK, I ended it."
        ],
        [
            "I learn the keys is leveled up.",
            "The approach of speeding up a slow learning algorithm right, and it's very satisfying to speed up slow learning algorithm.",
            "Of course, because often there's a lot of room to speed it up and so you can get big speedups, right?",
            "That's nice so."
        ],
        [
            "Canonical example of this is there's a lot of statistical query algorithms which you can implement on a MapReduce architecture and and you know that they go faster.",
            "That's great.",
            "Another approach."
        ],
        [
            "Which which we see pretty often is changing the architecture that you're working on.",
            "So if you implement something on GPU, then maybe it's faster and this seems to be seems to be gentlemen genuinely true, even for algorithms, which is sort of the best known for resolving that kind of problem.",
            "In terms of computational cost, but there's a caveat to this in the caveat.",
            "Is."
        ],
        [
            "It.",
            "Doesn't address problems with large amounts of data.",
            "Because you have a GPU.",
            "At some point you just can't shove data across the GPU any faster.",
            "Maintenance, it's kind of a bummer.",
            "So those."
        ],
        [
            "Kind of the third approach, which is, which is what we're working on.",
            "Which is you start designing new learning algorithms which actually explicitly deal with the constraints imposed by April architecture.",
            "Um?",
            "OK, so.",
            "So we're going to start with a fast learning algorithm.",
            "We can design a new parallel algorithm that competes with it.",
            "Canonical example of the fast learning algorithm.",
            "Really fast, and then we can go through a lot of data very fast is a linear prediction which you can train via online gradient descent.",
            "And the question is, can we actually speed this up?",
            "So."
        ],
        [
            "The core difficulty is bandwidth and latency.",
            "Maybe you have a one Gigabit Ethernet for your computer.",
            "In which case you can share maybe 400 gigabytes per hour through the Ethernet interface.",
            "It's actually you couldn't actually get that much, but you get close to it.",
            "So this implies that you can work with maybe a Tera feature size datasets.",
            "If you just kind of.",
            "Think about it.",
            "Typical latency between two different computers is tends to be on the order of a millisecond or maybe less, but it the the critical thing about the latency is that the latency is often much greater than the time to to train from 1.",
            "Single example.",
            "Right, and that means that the latency really affects the speed at which you can train."
        ],
        [
            "OK, so there's a lot of tricks to reduce this problem.",
            "You can have a sparse feature representation that reduces the amount of data.",
            "You can try to have an implicit feature representation which.",
            "Also, reduce the amount of data sometimes, and sometimes you can have a compressed format and that helps a lot.",
            "So.",
            "We use all of these, but still.",
            "This isn't enough because still there are datasets which are large enough that.",
            "That even with all these tricks to reduce the amount of data which is turning up across an interface, you have too much data.",
            "OK.",
            "So the core problem that the latency introduces is some sort of delay.",
            "So you have a is going to be some kind of delay in the system because you're going to have to.",
            "Somehow have less data going to each individual node.",
            "Maybe you have fewer examples, maybe have fewer features, and that means you're you're learning algorithm is going to have some sort of delay and its updates and some sort of incoherent since updates.",
            "So in the."
        ],
        [
            "Basic news from from learning theories.",
            "That delay is pretty bad.",
            "It's just to be avoided at all costs.",
            "At least in the worst case.",
            "If you have delayed updates, that reduces your convergence by the delay factor.",
            "If you turn the speed up, a learning algorithm, that course is essentially useless.",
            "Now in practice.",
            "R. I mean, this is the worst case theory and in practice of course often you don't.",
            "You don't run into the worst case.",
            "But I have never seen situations, especially where you have some sort of local structure in your examples.",
            "That where any kind of delay is actually pretty catastrophic to your performance.",
            "OK, so.",
            "Yeah.",
            "So many batches of form of delay.",
            "So at least it's part of the theory goes.",
            "It's just you can just interpret it as delay.",
            "And you're talking about in practice.",
            "Yeah, so I think the real answer is it depends on the data set.",
            "There are some datasets where it may hurt you and there somewhere it it definitely will not hurt you.",
            "You don't need any.",
            "Are you going to talk about this later, OK?",
            "Alright, so so delay is kind of bad.",
            "And the question is."
        ],
        [
            "Is what do we do so I'm going to tell you one way to avoid delay at a cost.",
            "So we have an example.",
            "What we're going to do is we're first going to.",
            "Chopper"
        ],
        [
            "This example we're going to send the label to each of two different nodes.",
            "And then.",
            "And then we're going to do."
        ],
        [
            "Is we're going to learn?",
            "A predictor based on the subset of features that we have in this node.",
            "A separate one in this note over here.",
            "You should use the pointer.",
            "So we have we have a learning algorithm running here and learning algorithm running here.",
            "And then we have the predictions which come out.",
            "And we also have labels are going to pass to some node which charges aggregate things."
        ],
        [
            "And then of course we have another learning algorithm which runs here.",
            "Using these predictions features.",
            "OK, so.",
            "So this is interesting algorithm.",
            "Because there's no delays being introduced.",
            "What's happened is we've partitioned the features, which is which is kind of battle.",
            "Get to that in a moment, but we can do an immediate update right here.",
            "An immediate update right there.",
            "Any update right there.",
            "When we're doing the online gradient descent.",
            "And that's great as far as delays go."
        ],
        [
            "OK, so.",
            "So several things are nice about this.",
            "So there are things to so there's no delay.",
            "That's really nice.",
            "You might be worried about the bandwidth usage by."
        ],
        [
            "For this step here, because you're sending entire datasets across the network, right?",
            "But but this is this is a stateless step.",
            "And that means that of course that it can be done once and you can just cash things right?",
            "So this is something which can be amortized.",
            "You can think of this as part of your data preparation process if you want to."
        ],
        [
            "You end up with a bandwidth requirement, which is a few bytes per example per node.",
            "If you have a single master that that suggests you can get to the sort of terror examples of these datasets, maybe that's not big enough for you, in which case, of course you can.",
            "You can build some sort of hierarchy.",
            "With the same idea.",
            "OK."
        ],
        [
            "OK, so there's some bad news here.",
            "The bad news is you can no longer compete with all linear predictors using this algorithm.",
            "It's about so.",
            "Here's the counter example, is actually a very simple one.",
            "So you have.",
            "You have 3 features and you have a label and you have a probability for each of these rows which is specified.",
            "OK, so now what happens here is.",
            "X1 is correlated with Y so.",
            "3/4 of the time agrees with why.",
            "In X2 is correlated with Y, so 3/4 of the time and agrees with Y.",
            "In X3 is completely uncorrelated, so this is .1 two.",
            "5.125 is .25 chance of being.",
            "One, if the labels 1.25 tenths of being zero labels wine.",
            "Similarly, if the label is 0.",
            "OK so X3 is completely uncorrelated.",
            "And that means that if you send X3 to its own little machine learning prediction based upon a clean correlated feature, then it's going to be.",
            "Didn't have no information right?",
            "So the information in X3 will not be passed to the summarizing mode.",
            "OK, it can't be passed.",
            "And that means that you cannot learn the predictor which is.",
            "U1 X One X2 and X3 which has 0 error rate.",
            "Right, instead you have to learn to predict which is has been some weight on X1 and X2, which has a substantial error rate.",
            "So this is it says that.",
            "You are indeed losing some power by breaking things up.",
            "But may."
        ],
        [
            "You don't lose too much power.",
            "Uh.",
            "You can say look maybe naive Bayes holds.",
            "And then and then of course, it becomes very straightforward to prove that it works out.",
            "But of course, Neves is a very strong assumption, and we're not really keen on the naive Bayes learning algorithms.",
            "So, so maybe it's important to notice that.",
            "X1 and X2.",
            "Instead of being individual features can be the set of all features going to a particular Shard.",
            "Right then you can compete with the linear predictors on that set.",
            "Which is of course much stronger than naive Bayes.",
            "And then you can notice that because we're doing learning of when we join things together.",
            "We avoid that problem with naive Bayes where we're not big.",
            "It's really overconfident about some particular label, right?",
            "So there's some extra power there that.",
            "That the I guess I don't know quite the right.",
            "The best way to specify yet, but this is definitely something extra which is going on, which is which is easily done and so we can predict better.",
            "OK, so."
        ],
        [
            "So the question is, how does this work?",
            "So I applied this to I guess would consider to be sort of a medium sized ad data set now.",
            "I say medium size because it fits under 1 machine.",
            "So.",
            "If you if you have the text from the data set, it's about 100 gigabytes.",
            "And there's about 10,000,000 examples.",
            "It is about 25 gigabyte, none giga non 0 features.",
            "So this is this is not even quite the terror feature scale, but it's big enough that we can actually run it for a few minutes and see if there's a speed up and not worry about startup and shutdown costs and things like that.",
            "Oh, and I should mention.",
            "One of the things which is that I'm using a trick to generate implicit features, which means that.",
            "So although I started out by saying we're bandwidth constrained, there's actually a computational constraint here too.",
            "It's not too different from the bandwidth constraint.",
            "But you should understand that when you look at the experiments.",
            "OK, so.",
            "I'm going to report the.",
            "So I'm trying to predict the probability here and I'm using squared loss, so I'm going to report.",
            "Progressive validation squared loss.",
            "Who knows what progressive validation is?",
            "OK, two people very good.",
            "OK, so I guess the idea with progressive validation is like cross validation for online learning, right?",
            "So you train on something and then you test on something and you train and then you test and then he trained then OK and then you take the average of.",
            "Of of the five different losses, and that claim is that this gives you deviations like a test set of this size with respect to the uniform average of the different predictors that you tested on.",
            "Right, so so it's like a test loss, except that.",
            "There's some training going on, but that's OK because we proved.",
            "And then I was going to report the relative wall Clock time where this is relative to just running on a single node with with.",
            "Yeah, single node.",
            "OK."
        ],
        [
            "So.",
            "There's two steps that are going on here.",
            "One of them is you chop up the examples and you send them off and do a little bit of training.",
            "Right?",
            "And.",
            "Um?",
            "OK, so let's look at the time first of all so.",
            "Or starting with one node going to 2, four and eight.",
            "And.",
            "So the time is slightly fact larger than one because you know, there's a little bit of extra overhead associated with shipping data around, but not too much.",
            "And then at two it starts dropping, it drops and drops, but some sort of floor being introduced.",
            "And the question is, why is there a floor and the reason why is because there's a bandwidth limit we're actually getting.",
            "Fairly near the bandwidth limit here was within a factor of 2.",
            "Kind of interesting.",
            "But remember this that sharding step is of course.",
            "Definitely Paralyzable incassable so, so maybe there's not a real limit to the system.",
            "And then we can look at the average relative squared loss performance on the different subsets of the data, where the subsets are the same number of examples.",
            "But if your features, for example, right?",
            "And it is expected to get worse, right?",
            "So we start here and then it gets worse and worse and worse.",
            "It's a little bit worse.",
            "Not not dramatically worse, but.",
            "But still still, this would matter a lot because.",
            "Often if you have a large data set, you could of course train on a smaller data set and get worse performance much faster, right?",
            "OK, so.",
            "So this is interesting."
        ],
        [
            "You know this is the really interesting one.",
            "So this is, this is where we take.",
            "We take the outputs of the individual predictors and we would join them together and we make a prediction based upon the joint, right?",
            "OK, so now the computational time again.",
            "There's some sort of floor.",
            "I think this has to do with.",
            "I'm not necessarily the best programmer, so I use select rather than equal weight.",
            "Is that sometime soon?",
            "But this is very interesting.",
            "Relative squared loss is smaller than one.",
            "Yeah.",
            "Around the square.",
            "Lots of smaller than one.",
            "And that's even true when we only have one node.",
            "Which you know, at first I thought with the bug.",
            "But it's not a bug.",
            "And the reason why?",
            "Is because.",
            "You know?"
        ],
        [
            "When I design A learning algorithm, I want to think worry about special cases like you really ought to compete with the constant feature, right?",
            "So of course I added a constant feature to each of these implicitly.",
            "Right?",
            "And.",
            "What's going on?",
            "We just have one.",
            "Is that you you have?",
            "You know prediction that goes out.",
            "So this prediction, by the way, is clipped to the interval 01 because you know probabilities are always between zero and one, so it doesn't make any sense to go outside that interval.",
            "And then that that introduces a non linearity.",
            "Anne then.",
            "Right and here you end up re calibrating your output.",
            "Which of course greatly improves your squared loss.",
            "I had not really appreciated how important the link function was."
        ],
        [
            "Before I saw that.",
            "OK, so.",
            "So this is this is sort of where I'm at."
        ],
        [
            "OK, so.",
            "One thing which which?"
        ],
        [
            "Might bother you.",
            "But by the way, these two these two things can be combined, right?",
            "So you can do everything all at once in one pass and you get very similar timings.",
            "One thing which might bind bother you is that we're not getting like a factor of eight improvement here, but I would like to mention that this is.",
            "There's a lot of other things that I haven't told you."
        ],
        [
            "Which has to do with how the learning algorithm itself was optimized and it was optimized enough that there are other papers having to do with how how that particular online gradient was optimized.",
            "Things related to hashing and caching and stuff like that.",
            "It's also a multi core gradient descent.",
            "Which is also in that multicore.",
            "It's also a feature based split.",
            "Right?",
            "So there's a bunch of other optimizations which are already going on, and of course it becomes so.",
            "I'm not complaining.",
            "I think that I can make this be better, but you should understand that.",
            "There's it's already a very very optimized baseline.",
            "OK.",
            "So you are probably going to hear a lot of different techniques for parallelization.",
            "And.",
            "Of course, maybe the best technique is not even needing to use it, but if you need to use it, I'm actually.",
            "Pretty.",
            "Um?",
            "I'm pretty convinced now that a feature based split is sort of the right way to go.",
            "And the reason why is so if you're doing something linear, of course you can try to average things, but in my experience that didn't work that well.",
            "And.",
            "The feature based split is kind of fully applicable to non linear systems, right?",
            "So if you're interested at all in in kind of nonlinear learning algorithms, then then you should be thinking about feature based splits because.",
            "Everything doesn't make sense if you have.",
            "If you have nonlinear learning algorithms.",
            "The thing which is which is really pretty convincing to me is."
        ],
        [
            "I accidentally improved performance.",
            "And you could think.",
            "Well, maybe maybe this.",
            "Maybe you just don't know how to train things, but but actually the previous solution was pretty good."
        ],
        [
            "2.",
            "Uh.",
            "OK, so there was a paper that we had at this nips which was talking about an algorithm which.",
            "Which is."
        ],
        [
            "For this one, except that after you predict you propagate the gradients back down and then you update things according to the gradients or according to or.",
            "In order to optimize the full linear prediction, so I expect to implement that fairly soon."
        ],
        [
            "That will be in the code, and the interesting thing perhaps to you guys is that the code is there.",
            "Right, so this is the code is in this both web it online learning project.",
            "And.",
            "I'm going to be giving a tutorial about this at 2:00 PM If you're interested in taking a look at it.",
            "So I can but done, except there's there's a little bit of discussion at hunts.net if you're interested in parallel lines.",
            "OK, thank you.",
            "Yeah, actually.",
            "That's so I'm doing it.",
            "You're asking if I'm doing our strict partition or if it's an overcomplete partition.",
            "So it is a partisan over complete partition, but it's almost district partition.",
            "The reason why it's so I expect.",
            "Actually, in actuality, if you do an even more overcomplete partition then you'll even improve your performance just.",
            "Just because this squared loss is convex and just inequality applied so forth but.",
            "The reason why it's it's not a perfect partition is because it's inconvenient to make it so because I was using implicit features so.",
            "He crossing some features with some other features.",
            "These features maybe get replicated twice.",
            "Or four times rather than just once.",
            "Other questions.",
            "Yeah.",
            "There was no link function.",
            "The loss function with squared loss.",
            "Optimize.",
            "Yeah.",
            "I guess you could think of that as link function as you want.",
            "Yeah.",
            "No, this is using the the hashing trick where you have you.",
            "You have features which are like words and they have a hash function that's applied to them to get an index and then you.",
            "Split based on the index.",
            "Right?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about parallel online learning.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "Oh, but how do we turn off the auto advance?",
                    "label": 0
                },
                {
                    "sent": "He said OK.",
                    "label": 0
                },
                {
                    "sent": "But OK. OK, So what we're talking about?",
                    "label": 0
                },
                {
                    "sent": "Parallel learning and learning with lots of data and trying to make fast learning algorithms and the basic question is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we try to make fast learning algorithms right, and there's a lot of people who adopt an approach which is.",
                    "label": 1
                },
                {
                    "sent": "Oh, I do it again there.",
                    "label": 0
                },
                {
                    "sent": "OK, I ended it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I learn the keys is leveled up.",
                    "label": 0
                },
                {
                    "sent": "The approach of speeding up a slow learning algorithm right, and it's very satisfying to speed up slow learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "Of course, because often there's a lot of room to speed it up and so you can get big speedups, right?",
                    "label": 0
                },
                {
                    "sent": "That's nice so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Canonical example of this is there's a lot of statistical query algorithms which you can implement on a MapReduce architecture and and you know that they go faster.",
                    "label": 1
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "Another approach.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which which we see pretty often is changing the architecture that you're working on.",
                    "label": 0
                },
                {
                    "sent": "So if you implement something on GPU, then maybe it's faster and this seems to be seems to be gentlemen genuinely true, even for algorithms, which is sort of the best known for resolving that kind of problem.",
                    "label": 0
                },
                {
                    "sent": "In terms of computational cost, but there's a caveat to this in the caveat.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Doesn't address problems with large amounts of data.",
                    "label": 1
                },
                {
                    "sent": "Because you have a GPU.",
                    "label": 0
                },
                {
                    "sent": "At some point you just can't shove data across the GPU any faster.",
                    "label": 0
                },
                {
                    "sent": "Maintenance, it's kind of a bummer.",
                    "label": 0
                },
                {
                    "sent": "So those.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of the third approach, which is, which is what we're working on.",
                    "label": 0
                },
                {
                    "sent": "Which is you start designing new learning algorithms which actually explicitly deal with the constraints imposed by April architecture.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So we're going to start with a fast learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "We can design a new parallel algorithm that competes with it.",
                    "label": 1
                },
                {
                    "sent": "Canonical example of the fast learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Really fast, and then we can go through a lot of data very fast is a linear prediction which you can train via online gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And the question is, can we actually speed this up?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The core difficulty is bandwidth and latency.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have a one Gigabit Ethernet for your computer.",
                    "label": 0
                },
                {
                    "sent": "In which case you can share maybe 400 gigabytes per hour through the Ethernet interface.",
                    "label": 0
                },
                {
                    "sent": "It's actually you couldn't actually get that much, but you get close to it.",
                    "label": 0
                },
                {
                    "sent": "So this implies that you can work with maybe a Tera feature size datasets.",
                    "label": 0
                },
                {
                    "sent": "If you just kind of.",
                    "label": 0
                },
                {
                    "sent": "Think about it.",
                    "label": 0
                },
                {
                    "sent": "Typical latency between two different computers is tends to be on the order of a millisecond or maybe less, but it the the critical thing about the latency is that the latency is often much greater than the time to to train from 1.",
                    "label": 0
                },
                {
                    "sent": "Single example.",
                    "label": 0
                },
                {
                    "sent": "Right, and that means that the latency really affects the speed at which you can train.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's a lot of tricks to reduce this problem.",
                    "label": 1
                },
                {
                    "sent": "You can have a sparse feature representation that reduces the amount of data.",
                    "label": 0
                },
                {
                    "sent": "You can try to have an implicit feature representation which.",
                    "label": 1
                },
                {
                    "sent": "Also, reduce the amount of data sometimes, and sometimes you can have a compressed format and that helps a lot.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We use all of these, but still.",
                    "label": 1
                },
                {
                    "sent": "This isn't enough because still there are datasets which are large enough that.",
                    "label": 0
                },
                {
                    "sent": "That even with all these tricks to reduce the amount of data which is turning up across an interface, you have too much data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the core problem that the latency introduces is some sort of delay.",
                    "label": 0
                },
                {
                    "sent": "So you have a is going to be some kind of delay in the system because you're going to have to.",
                    "label": 0
                },
                {
                    "sent": "Somehow have less data going to each individual node.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have fewer examples, maybe have fewer features, and that means you're you're learning algorithm is going to have some sort of delay and its updates and some sort of incoherent since updates.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic news from from learning theories.",
                    "label": 0
                },
                {
                    "sent": "That delay is pretty bad.",
                    "label": 1
                },
                {
                    "sent": "It's just to be avoided at all costs.",
                    "label": 1
                },
                {
                    "sent": "At least in the worst case.",
                    "label": 1
                },
                {
                    "sent": "If you have delayed updates, that reduces your convergence by the delay factor.",
                    "label": 0
                },
                {
                    "sent": "If you turn the speed up, a learning algorithm, that course is essentially useless.",
                    "label": 0
                },
                {
                    "sent": "Now in practice.",
                    "label": 0
                },
                {
                    "sent": "R. I mean, this is the worst case theory and in practice of course often you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't run into the worst case.",
                    "label": 0
                },
                {
                    "sent": "But I have never seen situations, especially where you have some sort of local structure in your examples.",
                    "label": 0
                },
                {
                    "sent": "That where any kind of delay is actually pretty catastrophic to your performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 1
                },
                {
                    "sent": "So many batches of form of delay.",
                    "label": 0
                },
                {
                    "sent": "So at least it's part of the theory goes.",
                    "label": 1
                },
                {
                    "sent": "It's just you can just interpret it as delay.",
                    "label": 0
                },
                {
                    "sent": "And you're talking about in practice.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think the real answer is it depends on the data set.",
                    "label": 0
                },
                {
                    "sent": "There are some datasets where it may hurt you and there somewhere it it definitely will not hurt you.",
                    "label": 0
                },
                {
                    "sent": "You don't need any.",
                    "label": 0
                },
                {
                    "sent": "Are you going to talk about this later, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so so delay is kind of bad.",
                    "label": 0
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is what do we do so I'm going to tell you one way to avoid delay at a cost.",
                    "label": 1
                },
                {
                    "sent": "So we have an example.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're first going to.",
                    "label": 0
                },
                {
                    "sent": "Chopper",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This example we're going to send the label to each of two different nodes.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we're going to learn?",
                    "label": 0
                },
                {
                    "sent": "A predictor based on the subset of features that we have in this node.",
                    "label": 0
                },
                {
                    "sent": "A separate one in this note over here.",
                    "label": 0
                },
                {
                    "sent": "You should use the pointer.",
                    "label": 0
                },
                {
                    "sent": "So we have we have a learning algorithm running here and learning algorithm running here.",
                    "label": 0
                },
                {
                    "sent": "And then we have the predictions which come out.",
                    "label": 0
                },
                {
                    "sent": "And we also have labels are going to pass to some node which charges aggregate things.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course we have another learning algorithm which runs here.",
                    "label": 0
                },
                {
                    "sent": "Using these predictions features.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting algorithm.",
                    "label": 0
                },
                {
                    "sent": "Because there's no delays being introduced.",
                    "label": 0
                },
                {
                    "sent": "What's happened is we've partitioned the features, which is which is kind of battle.",
                    "label": 0
                },
                {
                    "sent": "Get to that in a moment, but we can do an immediate update right here.",
                    "label": 0
                },
                {
                    "sent": "An immediate update right there.",
                    "label": 0
                },
                {
                    "sent": "Any update right there.",
                    "label": 0
                },
                {
                    "sent": "When we're doing the online gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And that's great as far as delays go.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So several things are nice about this.",
                    "label": 0
                },
                {
                    "sent": "So there are things to so there's no delay.",
                    "label": 0
                },
                {
                    "sent": "That's really nice.",
                    "label": 0
                },
                {
                    "sent": "You might be worried about the bandwidth usage by.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this step here, because you're sending entire datasets across the network, right?",
                    "label": 0
                },
                {
                    "sent": "But but this is this is a stateless step.",
                    "label": 0
                },
                {
                    "sent": "And that means that of course that it can be done once and you can just cash things right?",
                    "label": 0
                },
                {
                    "sent": "So this is something which can be amortized.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as part of your data preparation process if you want to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You end up with a bandwidth requirement, which is a few bytes per example per node.",
                    "label": 1
                },
                {
                    "sent": "If you have a single master that that suggests you can get to the sort of terror examples of these datasets, maybe that's not big enough for you, in which case, of course you can.",
                    "label": 0
                },
                {
                    "sent": "You can build some sort of hierarchy.",
                    "label": 0
                },
                {
                    "sent": "With the same idea.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's some bad news here.",
                    "label": 0
                },
                {
                    "sent": "The bad news is you can no longer compete with all linear predictors using this algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's about so.",
                    "label": 0
                },
                {
                    "sent": "Here's the counter example, is actually a very simple one.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 1
                },
                {
                    "sent": "You have 3 features and you have a label and you have a probability for each of these rows which is specified.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what happens here is.",
                    "label": 0
                },
                {
                    "sent": "X1 is correlated with Y so.",
                    "label": 0
                },
                {
                    "sent": "3/4 of the time agrees with why.",
                    "label": 0
                },
                {
                    "sent": "In X2 is correlated with Y, so 3/4 of the time and agrees with Y.",
                    "label": 0
                },
                {
                    "sent": "In X3 is completely uncorrelated, so this is .1 two.",
                    "label": 0
                },
                {
                    "sent": "5.125 is .25 chance of being.",
                    "label": 0
                },
                {
                    "sent": "One, if the labels 1.25 tenths of being zero labels wine.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if the label is 0.",
                    "label": 0
                },
                {
                    "sent": "OK so X3 is completely uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "And that means that if you send X3 to its own little machine learning prediction based upon a clean correlated feature, then it's going to be.",
                    "label": 0
                },
                {
                    "sent": "Didn't have no information right?",
                    "label": 0
                },
                {
                    "sent": "So the information in X3 will not be passed to the summarizing mode.",
                    "label": 0
                },
                {
                    "sent": "OK, it can't be passed.",
                    "label": 0
                },
                {
                    "sent": "And that means that you cannot learn the predictor which is.",
                    "label": 0
                },
                {
                    "sent": "U1 X One X2 and X3 which has 0 error rate.",
                    "label": 0
                },
                {
                    "sent": "Right, instead you have to learn to predict which is has been some weight on X1 and X2, which has a substantial error rate.",
                    "label": 0
                },
                {
                    "sent": "So this is it says that.",
                    "label": 0
                },
                {
                    "sent": "You are indeed losing some power by breaking things up.",
                    "label": 0
                },
                {
                    "sent": "But may.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You don't lose too much power.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "You can say look maybe naive Bayes holds.",
                    "label": 1
                },
                {
                    "sent": "And then and then of course, it becomes very straightforward to prove that it works out.",
                    "label": 0
                },
                {
                    "sent": "But of course, Neves is a very strong assumption, and we're not really keen on the naive Bayes learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So, so maybe it's important to notice that.",
                    "label": 0
                },
                {
                    "sent": "X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "Instead of being individual features can be the set of all features going to a particular Shard.",
                    "label": 0
                },
                {
                    "sent": "Right then you can compete with the linear predictors on that set.",
                    "label": 0
                },
                {
                    "sent": "Which is of course much stronger than naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "And then you can notice that because we're doing learning of when we join things together.",
                    "label": 0
                },
                {
                    "sent": "We avoid that problem with naive Bayes where we're not big.",
                    "label": 0
                },
                {
                    "sent": "It's really overconfident about some particular label, right?",
                    "label": 0
                },
                {
                    "sent": "So there's some extra power there that.",
                    "label": 0
                },
                {
                    "sent": "That the I guess I don't know quite the right.",
                    "label": 0
                },
                {
                    "sent": "The best way to specify yet, but this is definitely something extra which is going on, which is which is easily done and so we can predict better.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, how does this work?",
                    "label": 0
                },
                {
                    "sent": "So I applied this to I guess would consider to be sort of a medium sized ad data set now.",
                    "label": 1
                },
                {
                    "sent": "I say medium size because it fits under 1 machine.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you if you have the text from the data set, it's about 100 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "And there's about 10,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "It is about 25 gigabyte, none giga non 0 features.",
                    "label": 0
                },
                {
                    "sent": "So this is this is not even quite the terror feature scale, but it's big enough that we can actually run it for a few minutes and see if there's a speed up and not worry about startup and shutdown costs and things like that.",
                    "label": 0
                },
                {
                    "sent": "Oh, and I should mention.",
                    "label": 1
                },
                {
                    "sent": "One of the things which is that I'm using a trick to generate implicit features, which means that.",
                    "label": 1
                },
                {
                    "sent": "So although I started out by saying we're bandwidth constrained, there's actually a computational constraint here too.",
                    "label": 0
                },
                {
                    "sent": "It's not too different from the bandwidth constraint.",
                    "label": 0
                },
                {
                    "sent": "But you should understand that when you look at the experiments.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to report the.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to predict the probability here and I'm using squared loss, so I'm going to report.",
                    "label": 0
                },
                {
                    "sent": "Progressive validation squared loss.",
                    "label": 0
                },
                {
                    "sent": "Who knows what progressive validation is?",
                    "label": 1
                },
                {
                    "sent": "OK, two people very good.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess the idea with progressive validation is like cross validation for online learning, right?",
                    "label": 0
                },
                {
                    "sent": "So you train on something and then you test on something and you train and then you test and then he trained then OK and then you take the average of.",
                    "label": 0
                },
                {
                    "sent": "Of of the five different losses, and that claim is that this gives you deviations like a test set of this size with respect to the uniform average of the different predictors that you tested on.",
                    "label": 0
                },
                {
                    "sent": "Right, so so it's like a test loss, except that.",
                    "label": 0
                },
                {
                    "sent": "There's some training going on, but that's OK because we proved.",
                    "label": 0
                },
                {
                    "sent": "And then I was going to report the relative wall Clock time where this is relative to just running on a single node with with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, single node.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's two steps that are going on here.",
                    "label": 0
                },
                {
                    "sent": "One of them is you chop up the examples and you send them off and do a little bit of training.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at the time first of all so.",
                    "label": 0
                },
                {
                    "sent": "Or starting with one node going to 2, four and eight.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the time is slightly fact larger than one because you know, there's a little bit of extra overhead associated with shipping data around, but not too much.",
                    "label": 0
                },
                {
                    "sent": "And then at two it starts dropping, it drops and drops, but some sort of floor being introduced.",
                    "label": 0
                },
                {
                    "sent": "And the question is, why is there a floor and the reason why is because there's a bandwidth limit we're actually getting.",
                    "label": 0
                },
                {
                    "sent": "Fairly near the bandwidth limit here was within a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "Kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "But remember this that sharding step is of course.",
                    "label": 0
                },
                {
                    "sent": "Definitely Paralyzable incassable so, so maybe there's not a real limit to the system.",
                    "label": 0
                },
                {
                    "sent": "And then we can look at the average relative squared loss performance on the different subsets of the data, where the subsets are the same number of examples.",
                    "label": 1
                },
                {
                    "sent": "But if your features, for example, right?",
                    "label": 0
                },
                {
                    "sent": "And it is expected to get worse, right?",
                    "label": 0
                },
                {
                    "sent": "So we start here and then it gets worse and worse and worse.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit worse.",
                    "label": 0
                },
                {
                    "sent": "Not not dramatically worse, but.",
                    "label": 0
                },
                {
                    "sent": "But still still, this would matter a lot because.",
                    "label": 0
                },
                {
                    "sent": "Often if you have a large data set, you could of course train on a smaller data set and get worse performance much faster, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know this is the really interesting one.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is where we take.",
                    "label": 0
                },
                {
                    "sent": "We take the outputs of the individual predictors and we would join them together and we make a prediction based upon the joint, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so now the computational time again.",
                    "label": 0
                },
                {
                    "sent": "There's some sort of floor.",
                    "label": 0
                },
                {
                    "sent": "I think this has to do with.",
                    "label": 0
                },
                {
                    "sent": "I'm not necessarily the best programmer, so I use select rather than equal weight.",
                    "label": 0
                },
                {
                    "sent": "Is that sometime soon?",
                    "label": 0
                },
                {
                    "sent": "But this is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Relative squared loss is smaller than one.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Around the square.",
                    "label": 0
                },
                {
                    "sent": "Lots of smaller than one.",
                    "label": 0
                },
                {
                    "sent": "And that's even true when we only have one node.",
                    "label": 0
                },
                {
                    "sent": "Which you know, at first I thought with the bug.",
                    "label": 0
                },
                {
                    "sent": "But it's not a bug.",
                    "label": 0
                },
                {
                    "sent": "And the reason why?",
                    "label": 0
                },
                {
                    "sent": "Is because.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I design A learning algorithm, I want to think worry about special cases like you really ought to compete with the constant feature, right?",
                    "label": 0
                },
                {
                    "sent": "So of course I added a constant feature to each of these implicitly.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "We just have one.",
                    "label": 0
                },
                {
                    "sent": "Is that you you have?",
                    "label": 0
                },
                {
                    "sent": "You know prediction that goes out.",
                    "label": 0
                },
                {
                    "sent": "So this prediction, by the way, is clipped to the interval 01 because you know probabilities are always between zero and one, so it doesn't make any sense to go outside that interval.",
                    "label": 0
                },
                {
                    "sent": "And then that that introduces a non linearity.",
                    "label": 0
                },
                {
                    "sent": "Anne then.",
                    "label": 0
                },
                {
                    "sent": "Right and here you end up re calibrating your output.",
                    "label": 0
                },
                {
                    "sent": "Which of course greatly improves your squared loss.",
                    "label": 0
                },
                {
                    "sent": "I had not really appreciated how important the link function was.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I saw that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is this is sort of where I'm at.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One thing which which?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might bother you.",
                    "label": 0
                },
                {
                    "sent": "But by the way, these two these two things can be combined, right?",
                    "label": 0
                },
                {
                    "sent": "So you can do everything all at once in one pass and you get very similar timings.",
                    "label": 0
                },
                {
                    "sent": "One thing which might bind bother you is that we're not getting like a factor of eight improvement here, but I would like to mention that this is.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of other things that I haven't told you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which has to do with how the learning algorithm itself was optimized and it was optimized enough that there are other papers having to do with how how that particular online gradient was optimized.",
                    "label": 0
                },
                {
                    "sent": "Things related to hashing and caching and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "It's also a multi core gradient descent.",
                    "label": 1
                },
                {
                    "sent": "Which is also in that multicore.",
                    "label": 0
                },
                {
                    "sent": "It's also a feature based split.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So there's a bunch of other optimizations which are already going on, and of course it becomes so.",
                    "label": 0
                },
                {
                    "sent": "I'm not complaining.",
                    "label": 0
                },
                {
                    "sent": "I think that I can make this be better, but you should understand that.",
                    "label": 0
                },
                {
                    "sent": "There's it's already a very very optimized baseline.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you are probably going to hear a lot of different techniques for parallelization.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Of course, maybe the best technique is not even needing to use it, but if you need to use it, I'm actually.",
                    "label": 0
                },
                {
                    "sent": "Pretty.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm pretty convinced now that a feature based split is sort of the right way to go.",
                    "label": 0
                },
                {
                    "sent": "And the reason why is so if you're doing something linear, of course you can try to average things, but in my experience that didn't work that well.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The feature based split is kind of fully applicable to non linear systems, right?",
                    "label": 1
                },
                {
                    "sent": "So if you're interested at all in in kind of nonlinear learning algorithms, then then you should be thinking about feature based splits because.",
                    "label": 0
                },
                {
                    "sent": "Everything doesn't make sense if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have nonlinear learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "The thing which is which is really pretty convincing to me is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I accidentally improved performance.",
                    "label": 0
                },
                {
                    "sent": "And you could think.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe maybe this.",
                    "label": 0
                },
                {
                    "sent": "Maybe you just don't know how to train things, but but actually the previous solution was pretty good.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "OK, so there was a paper that we had at this nips which was talking about an algorithm which.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this one, except that after you predict you propagate the gradients back down and then you update things according to the gradients or according to or.",
                    "label": 0
                },
                {
                    "sent": "In order to optimize the full linear prediction, so I expect to implement that fairly soon.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That will be in the code, and the interesting thing perhaps to you guys is that the code is there.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is the code is in this both web it online learning project.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be giving a tutorial about this at 2:00 PM If you're interested in taking a look at it.",
                    "label": 0
                },
                {
                    "sent": "So I can but done, except there's there's a little bit of discussion at hunts.net if you're interested in parallel lines.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually.",
                    "label": 0
                },
                {
                    "sent": "That's so I'm doing it.",
                    "label": 0
                },
                {
                    "sent": "You're asking if I'm doing our strict partition or if it's an overcomplete partition.",
                    "label": 0
                },
                {
                    "sent": "So it is a partisan over complete partition, but it's almost district partition.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's so I expect.",
                    "label": 0
                },
                {
                    "sent": "Actually, in actuality, if you do an even more overcomplete partition then you'll even improve your performance just.",
                    "label": 0
                },
                {
                    "sent": "Just because this squared loss is convex and just inequality applied so forth but.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's it's not a perfect partition is because it's inconvenient to make it so because I was using implicit features so.",
                    "label": 0
                },
                {
                    "sent": "He crossing some features with some other features.",
                    "label": 0
                },
                {
                    "sent": "These features maybe get replicated twice.",
                    "label": 0
                },
                {
                    "sent": "Or four times rather than just once.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There was no link function.",
                    "label": 0
                },
                {
                    "sent": "The loss function with squared loss.",
                    "label": 0
                },
                {
                    "sent": "Optimize.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I guess you could think of that as link function as you want.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, this is using the the hashing trick where you have you.",
                    "label": 0
                },
                {
                    "sent": "You have features which are like words and they have a hash function that's applied to them to get an index and then you.",
                    "label": 0
                },
                {
                    "sent": "Split based on the index.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        }
    }
}