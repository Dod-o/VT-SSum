{
    "id": "42tawoh43perigw44i6yt65c7klgos6t",
    "title": "Support Vector Machines",
    "info": {
        "author": [
            "Chih-Jen Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_lin_svm/",
    "segmentation": [
        [
            "OK, thank you so so today I will talk about support vector machines, so this is a specific classification and regression method.",
            "So we want to get into some details about."
        ],
        [
            "This method so the plan is like this.",
            "So we have several topics, so I'm hoping to OK.",
            "So for the first part we hope to finish the first 2.",
            "So there are basic concepts and also to derive the SVM primal and dual problems.",
            "Then for the second part I will talk about training and linear nonlinear support vector machines.",
            "Let me also touch the issue of parameter and kernel selection and also some practical issues.",
            "Also mentioned some possible research issues in support vector machines with some of.",
            "Some of those issues will be left to the third part.",
            "Then we also mention multi class classification and then have some discussion.",
            "So please feel free to interrupt me if you have any questions."
        ],
        [
            "OK, so maybe now question is why we're here to talk about support vector machines?",
            "Well, at least right now.",
            "It is one of the very popular class, so in quite a few situations it can give you competitive performance with existing classification methods where I'm not claiming it is, it is better.",
            "We are usually it maybe not, but it is usually reasonably good.",
            "Then about using such a method, well, here I would say it is relatively easy to use.",
            "Well after sale is only relatively well.",
            "I mean don't convert well I can hear is never of course.",
            "I mean doing any data classification.",
            "You want to try cleaning with, never just like the same set on Monday, but compared to some very sophisticated classification techniques where it is not.",
            "So difficult to use so.",
            "Seems like that may be one reason why.",
            "It is popular right now is via neither special kernel kernel technique, so will the whole kernel area is a bigger area.",
            "So next week I think Alex smaller is going to give a tutorial on kernel techniques, so there are many extensions, so SVN miso is only a kind of kernel methods."
        ],
        [
            "So let's talk about two class classification first.",
            "So we're thinking about a very simple situation.",
            "So let's assume that there are training instances, so we have XII is from one to L. So there are vectors in Euclidean Euclidean space.",
            "And each each vector is the so called feature vectors.",
            "So for example, if you are representing each patient as a feature vector, then you may have components like height, weight, blood pressure, etc.",
            "So you have a you get along vector.",
            "So usually for such a vector we create a data instance.",
            "Now we consider a simple situation with only two classes of data.",
            "So in practice your data may have many, many classes.",
            "For example, if you are doing handwritten digit recognition, then you are going to have 10 classes, but not for.",
            "For SVM it starts from from dealing with the case of two classes only, so for easier description we define an indicator vector for the Y.",
            "So this why is a vector with length L remember Arizona number of training instances, so this each component of why he has value either plus or minus one.",
            "So we have here that if the ice training instance is in the first class, then we say why I is plus one.",
            "But if XI is in the second class then we define why I could be minus one.",
            "So actually we define this Y and it plays a role.",
            "An indicator vector.",
            "Let the basic idea of support vector machines is to find a so called set."
        ],
        [
            "Rating Hyper plan with just like this figure shows.",
            "So we have two classes of training instances.",
            "Some are circles and some are triangles.",
            "So SVM tries to find the separating hyperplane to separate all the training instances well in a 2 dimensional situation then this is just a straight line but in a higher dimensional space we call it a hyperplane.",
            "So what we intend to find is this started online.",
            "So once we have restarted online then we have a model to do prediction.",
            "But the reason is so if you use this study tonight as your model, then for any new data you just check whether that data is on the right hand side or on the left hand side.",
            "If on the right you predict it as a circle.",
            "Otherwise you say it is a triangle, so it is so simple.",
            "So that's the basic idea.",
            "Basically, as a linear classifier.",
            "So the question is how to find this.",
            "Study the line.",
            "So here I show 2 figures and actually layer layer the same training instances.",
            "OK so you know two Vigors flow circles and triangles there in the same position.",
            "And I also show 2 dotted lines to separate training instances.",
            "Will they both fully separate our training data so the question is, which one might be better?",
            "Yeah, so for this one.",
            "All training instances are correctly separated, but this one is as well.",
            "So then the question is which one might be better?",
            "Intuitively, we say the right hand side one is better.",
            "So how's the reason?",
            "Looking at loss training instances roughly, we see that circles there about around this rich.",
            "Conceptually, we can say that yeah, so from those circles we say for most circles they should appear in this area and also for triangles.",
            "They should appear in this area, so if you have a model to be like this, like this one, then some circles here they may be wrongly predicted and also some triangles.",
            "Here they may be wrongly predicted.",
            "You don't know.",
            "Maybe for testing data, some circles or triangles they just appear in those.",
            "Regions here then, for if you use this model, your testing performance won't be so good, so we think the right hand side one is actually better.",
            "So in that sense, we think it is more general.",
            "It better captures the distribution of the two classes of training instances.",
            "Well, of course there is some theory behind that, but now we start from the more geometric POV saying that this kind of separating hyperplane is actually better.",
            "So now the question is how to find out.",
            "This kind of dotted line so that it is the most general one.",
            "So, so we need to define certain equations.",
            "Well, for any hyperplane we can represent it as a W. Transpose X + B = 0.",
            "So so X is the variable and adopt you and be there somehow, like coefficients of this hyperplane.",
            "So once W and VR decided then then you have the equation.",
            "So our job right now it's not find X.",
            "Instead we try to find out W enemy is once we have W&B then we have Amanda.",
            "So mathematically to say that training instances our correctly separated is bylaws inequalities.",
            "So evil eyes training instance is on the right hand side.",
            "That means it is a circle.",
            "Then if we put XI into this phone then it should be greater than 0.",
            "So we see if y = 1 will remember.",
            "Why is the indicator vector?",
            "So so for this data, which is a circle, you should be greater than 0, otherwise it should be less than zero.",
            "So what is your decision function?",
            "So once you have this started online, or equivalently say you have WMV, then this is your decision function.",
            "So that means for any new data X you put that into W, transpose X + B.",
            "Then you check whether it is greater or less than zero.",
            "So if greater than user it is a circle.",
            "Yeah, so the remaining job is to decide W&B.",
            "So we have just said that we would like to have A to have the most general one.",
            "So in a sense we think if we draw a tube like this so now OK.",
            "So now from this started online we barely move it to the first class and also move it to the second place so we can have two additional lines layer code W transpose X + B equals plus or minus one will not even say 5 one and plus 1 -- 1.",
            "Actually, you can use classify or minus five, but that's just an issue of scaling.",
            "But for example, if you think from this now you have W transpose X + B = 0.",
            "If you multiply it by 5 or by 10 or less, still the same hyperplane.",
            "So it doesn't matter if you use any value here, But anyway, for simplicity, of course we use plus and minus one that is easier.",
            "So once we have lost two lines, then somehow we say that the most general one general hyperplanes are one.",
            "So let the width of this tube is maximized.",
            "Yeah, so we want to find out the width of."
        ],
        [
            "Is this tube, but it turns out that the distance between, so that's actually the same as the distance between two parallel lines.",
            "So if you calculate the distance between W, transpose X + B equals plus and minus one, that's actually 2 divided by the Dome of W. So that's the same as 2 / sqrt W transpose standing.",
            "So we are trying to maximize this turn.",
            "If is among so many possible W&B, we would like to find out.",
            "So that this value is maximized, so this is so called.",
            "This is called maximum maximum margin.",
            "That's one special thing about SVM and the whole maximum margin classifiers.",
            "So finally we have a quadratic programming problem, so this is the first somehow you can say this is the first SVM formulation.",
            "We we try to maximize this, but usually we like to do minimization while maximizing something is equivalent equivalent to minimizing the reciprocal.",
            "So we have we can minimize the number of W / 2, but square root is an increasing function, so we can remove it.",
            "Therefore we get W transpose tablet divided by two.",
            "So this is the objective function that we would like to minimize.",
            "Learn subject to certain constraints, so other constraints will now.",
            "We still hope to.",
            "Classify all the training instances.",
            "This allows XI four circles.",
            "They should be greater equal to 1 with those two additional lines.",
            "Now we don't see greater than zero with a greater equal to plus one and four triangles.",
            "We say less than or equal to minus one, but we don't like to write two types of inequalities, so we try to combine them together.",
            "So if you multiply Y on both sides, so remember, now we have less than or equal to minus one, But if we multiply Y on both sides then.",
            "This minus 1 * -- 1 becomes one Anna user have to change the inequality sign, so then you get greater or equal to 1 as well.",
            "So those two inequalities they can be combined together as a single form.",
            "So we have L training instances and for each one we hope it is correctly classified.",
            "This is the quadratic programming that SVM solves."
        ],
        [
            "Of course this is not good because data may not be linearly separable, so we cannot always use a linear classifier.",
            "This is an example that you can never use a straight line to separate the two classes of training instances.",
            "Instead, you can use a nonlinear curve.",
            "So we do two things here.",
            "Firstly, we must allow training errors.",
            "If you don't do that, then.",
            "Then this is a so called infeasible optimization problem.",
            "The idea of a constrained optimization problem is that from from the set of candidates which satisfy all constraints, then you select the one so that the objective function is minimized.",
            "But now this.",
            "There's no tablet be satisfying all those constraints because you cannot find any straight line 40 separate.",
            "All the training instances.",
            "So you even don't have any candidates to select before doing the minimization, so this is a so called invisible optimization problem, so you need to allow training errors, say OK, Now I give up separating all the training instances.",
            "I just try to separate and maybe some of that.",
            "But later we will say that it is generally not a good idea to fit all the training instances.",
            "So the purpose of allowing training errors is not too is not only to make this optimization problem feasible.",
            "We user hope.",
            "Actually we hope to get even better, better testing accuracy.",
            "So if you overfill the training data, then maybe the performance isn't so good.",
            "So this is one thing we're going to do, and then another thing is, instead of directly using certain non linear curves, it is very difficult to model nonlinear curves.",
            "But we didn't learn many nonlinear curves.",
            "Baking high school.",
            "A very smart idea is that now instead of modeling nonlinear curves in the original space, we try to make the data into a higher dimensional feature space.",
            "That means we use the function by.",
            "So for any eggs we make X2 maybe a much higher dimensional space.",
            "So intuitively you can say that it so now in this original space it is smaller, so it's more difficult to linearly separate your training instances.",
            "But if you map all those data into a very high dimensional space, it is much bigger.",
            "So then it may be easier to linearly separately."
        ],
        [
            "So that's the idea.",
            "This is the the standard support vector machines that people are solving right now.",
            "Of course there are a lot of multiplications, but this is the most commonly used formulation.",
            "So now you can see where we do the changes.",
            "So this is the first one, but we didn't do anything different.",
            "We just made the data first.",
            "So originally we are in the in the input feature input space, but we made everything to higher dimensional space.",
            "Then we also put ourselves into that higher dimensional space.",
            "So we do the same thing we still do.",
            "We still try to get a linear classifier.",
            "So that's why there is amazing here.",
            "So I did show an example.",
            "So suppose the original your data in the industry dimensional space.",
            "So you have three components.",
            "Then we can use such a mapping function.",
            "So we made it to the 10 dimensional space.",
            "So we have 10 components.",
            "You can see that we have the first, second and the third components.",
            "We also have squared, but we also have X 1 * X Two.",
            "So now you see why this is maybe reasonable using only single features may not be.",
            "Enough to to give you information on separating data, but by combining like 2 features, then let me provide you more information to separate data.",
            "OK, so we have done amazing function.",
            "Learn the second thing is that we must allow training errors 'cause then after the meeting it is still possible that you cannot find any any hyperplane to separate all the training instances.",
            "So how do we?",
            "How do we did is to introduce a slack variable.",
            "So we minus a negative value here.",
            "So this is this one is now negative, so why is not negative?",
            "Well, if it is negative, say for example if you have 1 -- 3 -- 5, so this becomes 66 is even greater than one.",
            "So positive purpose of doing that so well.",
            "Now the problem is we will realize the left hand side maybe left maybe less than one.",
            "So then, even if inside it, the best you can get, it's only minus five.",
            "Then you can never modify minus.",
            "Five is not smaller than is not greater than or equal to 1.",
            "Yeah, so to make this feasible, then you have to introduce a variable here.",
            "So if this is minus 5, then you do 1 -- 6 OK. Then the inequality holds.",
            "So that's the idea.",
            "By introducing those select variables.",
            "Then the optimization problem is guaranteed to be feasible.",
            "So you can always find out WB and also can see so that those inequalities are satisfied.",
            "Well, but you don't want to have too many training errors.",
            "Is the original idea is still to separate as many training instances as possible.",
            "So we introduce an additional term in the objective function so.",
            "We some loss select variables and the plus so called penalty parameters.",
            "Busy if you have a really large see here then 'cause you are doing minimization of both terms.",
            "So this submission should be should be as small as possible there for quite a few select variables they will be going back to zero.",
            "Well if if this variable goes back to zero that means this training instance is correctly separated.",
            "So by introducing this circle.",
            "The term then most training instances may still be correctly classified, so that's the idea.",
            "So now we have shown the basically the standard form of support vector machines.",
            "OK, so this looks OK and it is quite simple but but we have problems to find out the decision function.",
            "So to every decision function you need to solve this optimization problem and then you you need.",
            "That means you need to find out in particular WMV.",
            "Once you have W and then you have the decision function.",
            "Try to guess what's the value of the constant C. Oh, you have to decide that.",
            "Well, that's not an easy, easy thing, so we will will discuss that issue later.",
            "But the problem is I can't see who is correct version.",
            "OK, so really.",
            "So the system isn't so good.",
            "OK so so now yeah, so deciding this season issue and not only learn how to decide this mapping function?",
            "Yeah, that's another issue, right?",
            "Yeah, but it's not."
        ],
        [
            "Sorry about that.",
            "At this moment we try to solve the optimization problem first.",
            "Well, there's an immediate difficulty here.",
            "This value is a variable of the optimization problem, and it may be a very long vector variable is you are going.",
            "You may have an infinite number of variables in your optimization problem if you're moving function makes data to a very very high dimensional space even to an infinite dimensional space, then you are going to have infinite infinitely many variables for this study.",
            "So this is this is going to be very difficult as it is usually difficult to solve an infinite optimization problem.",
            "So the tree.",
            "Glad most people are doing right now, is to derive something called the deal optimization problem.",
            "We later we will we really derive this?",
            "But now let's just under try to understand what this this optimization problem is.",
            "It is well, of course, this problem is closely related to.",
            "To the standard SVM problem.",
            "Now we call this one primal problem and using optimization theory with the Rep, something called the dual problem.",
            "So this is again a quadratic optimization problem, so the variable is Alpha.",
            "This other is a vector variable.",
            "Lenses L, so remember L is the number of training instances.",
            "So the good thing here is the number of variables is finite.",
            "Number of variables is the same as the number of training instances.",
            "So instead of using an infinitely many variables.",
            "So let's check the objective function of this deal problem.",
            "This is a quadratic term.",
            "We have our transpose QR&Q is the square matrix.",
            "More precisely, and L by L square matrix, and we saw a component to be to be this.",
            "So this is essentially the inner product between the ice training instance and the Jays training instance, and then also multiply their class labels where yyj are either plus or minus one.",
            "So this is a quadratic term.",
            "Then there's also linear turn we minus Y transpose Alpha or what is victory?",
            "Either vector of all ones, so minus the transpose of a mix minus the summation of sort of I.",
            "Then there are a bunch of constraints on Alpha, so I must be between zero and the sea.",
            "Will remember C is the penalty term introduced.",
            "Previous slide here.",
            "So I must be between zero and a C letter is also a linear constraint that this indicator vector Y transpose Alpha must be 0.",
            "So under certain constraints, we minimize a quadratic function, so if we can solve this dual problem and obtain Alpha, then there's a relationship saying that the optimal adaptive of the primal problem.",
            "So that means the original SVM problem is actually a linear combination of training instances.",
            "So you can see if you have Alpha and you also have wife and let you also have lamenting vectors here ever and why they are scalars.",
            "So this summation is a linear combination of training instances.",
            "So for optimization theory we know that W is actually a linear combination of training instances.",
            "This means if we can solve the dual problem, then using this formulation we can obtain the primal optimal solution.",
            "Looks good, but we need to handle this first before solving a dual problem.",
            "You must be able to write down the problem.",
            "Acueza is Ringer product, but remember I said that this may be an infinite vector.",
            "So how to do an infinite dimensional inner product may be very difficult so you don't even get QA.",
            "Then of course you cannot sell."
        ],
        [
            "The dual problem.",
            "So SVM use kernel tricks.",
            "Here's some very special mapping functions so that this inner product can be easily calculated, so that's the main idea.",
            "So you don't use general infinite vectors.",
            "Use some very special ones.",
            "So even if they are infinite vectors, as long as we can get a closed form, then this QA can be easy, easily calculated.",
            "So here is 1 example.",
            "So I have a well, this is the same as the one that I gave earlier.",
            "So now you will understand why we introduce square root Square Root 2 here.",
            "This looks a little bit weird, but because of such spatial things then the inner product can be easily calculated.",
            "So let's check that.",
            "So if you are mapping function is like this.",
            "This is from a 3 dimensional space to attend dimensional space.",
            "If you do inner product then you have to do 10 multiple occasions and then to sum all those values.",
            "Then there's no additions.",
            "So you need 19 operations.",
            "It's basically we can show that this inner product is the same as the inner product in the original space, so you calculate only XI transports XJ first, then plus one and two squared operation.",
            "So you only need this is in a 3 dimensional space, so you need a three multiplications, two additions and 1 addition and one multiplication.",
            "So the number of operations is much smaller than doing a direct inner product, so that's the kernel trick.",
            "So we only need so we define something called Colonel.",
            "So instead of writing this mapping function, we write a function of the inner product, usually code.",
            "This kernel function K of XY and X&Y are two training or testing instances.",
            "There's several common kernels, so that means if you write down this this kernel function, it can actually be written as the inner product of two vectors.",
            "So that's the idea.",
            "This is called radial basis function kernel or Gaussian kernel.",
            "This is a very commonly used kernel.",
            "This one is also very frequently used.",
            "That's polynomial kernel, so you have inner product in the original space, then divided by something plus something 2 to the degree of the.",
            "For our example, is actually a special case of polynomial kernel.",
            "So so the whole idea is we use on very special mapping function so that the inner product.",
            "Can be easily calculated, but if you don't believe that this is an inner product of two vectors, we must look at a very simple situation.",
            "Busy hold I'm claiming here is that the this obvious kernel is actually the inner product between."
        ],
        [
            "So infinite vectors.",
            "It actually, even if your original training, training or testing data is in only one dimensional space, it actually made it through an infinite dimensional space.",
            "So let's do some calculation.",
            "So now let's assume that all those training producing data instances they are in 1 dimensional space.",
            "So this is the formulation of this RBF kernel because it is 1 dimensional, so we can remove this no notation, so we actually calculate XI minus X ^2.",
            "So there is.",
            "So there is 110 related to both XI and XJ, but then two other terms are either related to XI only or XJ only.",
            "So then we can.",
            "We can.",
            "Overflows exciting extra terms full of beginning, then by Taylor expansion of this XI XJ turn, then we get the summation of infinite components, and each term is actually a polynomial of XI times XJ.",
            "So you can separate each term into something related to only two XI and times something related.",
            "So if you will, let's look at this so you have XI squared.",
            "You also have extra square.",
            "Then so this is so from this you already see this is an inner product of two infinite dimensional vectors.",
            "So if you write your mapping function to be like this and you multiply, then that's exactly this one.",
            "So this is a very simple way to show you why we're really moving data to an infinite dimensional space.",
            "So there's a.",
            "There's a parameter gamma here.",
            "Now we call these kernel parameters.",
            "Or a B or D gritty layer.",
            "Kernel parameters.",
            "So different kernel parameters correspond to different mapping functions, so similar to that you have to decide the penalty parameters C. Then you also have to decide the kernel parameter parameters, or a B&D, and so that's always an issue."
        ],
        [
            "So now let's talk more about kernels.",
            "So now you may have equation.",
            "That is, it really useful to make the data to a high dimensional space?",
            "Yeah, how, how?",
            "How could I convince you that by moving data to a bigger space then data can really be separated?",
            "There are some theoretical results proving that will improbability that in a higher spaceland probability data can be separated is bigger.",
            "Well there are such results.",
            "Another thing I can tell you here is.",
            "That kernels may may help to separate training data.",
            "Business CDs simple situation in another space for any and independent vectors then actually are linearly separable.",
            "But why is that?",
            "Where you just you just solve linear equation so you have suppose you have L training instances and they are linearly independent.",
            "No, not at all.",
            "OK. Well, I shouldn't say this is our, so let's say I am.",
            "So suppose I have.",
            "I have mapped each training instance toward two and L dimensional space to enable dimensional space.",
            "So let's consider in the eye or space any error independent vectors and I have L training instances, so I actually have a square matrix here.",
            "So I write the square matrix here.",
            "Then I solve a simple linear equation using this square matrix due to independence.",
            "This measure is easy invertible, so I can always, so I just saw this and I put the right hand side to be plus or minus one.",
            "I mean, depending on whether this data is in the 1st or the 2nd class, so I'm very sure to separate the training instances.",
            "OK, so once we have this factor, this is what we do now.",
            "So if the if your kernel matrix is positive definite.",
            "If your kernel matrix is positive definite, then you are guaranteed to separate all training instances.",
            "So if you are mapping function lead you to a positive definite kernel matrix then your data can be fully separated.",
            "OK, so let me that actually may not be good, because then we want to avoid overfitting so.",
            "So on the one hand we we have a we have a meeting so that.",
            "In principle, training data can can all be separated them.",
            "From that we use other adjustments like the parameters C or something else.",
            "So we avoid overfitting.",
            "Yeah, but the reason of mentioning you this is that.",
            "Well, if you're a Colonel is positive definite, then data can be for these positive definite.",
            "You can do a trustee factorization, two LL transpose, then in a sentence you can say that you are transforming your training instances to two independent vectors in R. Well, this is not I, this is RL is now K is LILL by L square matrix and air is the number of training instances.",
            "So this is one way to explain that kernels help to separate."
        ],
        [
            "Training data.",
            "So there are a lot of issues.",
            "So now you may ask, what kind of kernels should I use?",
            "So or or you are asking what kind of mapping functions should I use?",
            "Then another issue is what kind of functions they are valid kernels.",
            "So if you are given just a function of X&Y, how do you know that it can be separated into the inner product of two vectors?",
            "Then we also mentioned kernel parameters.",
            "So for the same type of kernels you have different parameters.",
            "Actually mean mean means different mapping functions.",
            "So how to decide that?",
            "So all those are issues that we have to handle, but we will discuss them later."
        ],
        [
            "So let's assume that oh, now we already have a kernel function and we have not only that.",
            "We have even solve the dual problem by introducing the deal, then by introducing the kernel Now we can write down this matrix Q.",
            "So we really have the dual problem.",
            "Well, how to solve it is another issue, but let's assume that we already solve it.",
            "So we get the optimal Alpha.",
            "So if the optimal Alpha, then by the primordial relationship we also have the optimal W. Well that looks good, but you may say we still have problems.",
            "If it, if it's moving function isn't, it is an infinite vector, then it is impossible to to explicitly write down the vector W right?",
            "Even even if we know those coefficients are, so there's no way to write on this vector W. Well, it turns out that you don't have to do that using this relationship.",
            "We put it into the decision function so we have W transpose.",
            "This testing data X + B.",
            "Then we put this summation for into this inner product.",
            "We get the summation of actually training instance and testing instance.",
            "So if we can calculate the inner product between training and testing data, so that means the kernel function then we are fine.",
            "We never have to write down this vector W. The only thing we need to know is Alpha.",
            "So once we have Alpha and also know how to calculate the kernel then we can get a value.",
            "Then that's our decision value.",
            "So this becomes our decision function using the dual dual variable Alpha.",
            "Lynn listen special thing here.",
            "Remembering the dual problem.",
            "We say this after I must satisfy certain so-called bounded constraints that each other, each component of I must be between zero and see, and somehow it Optima very often quite a few of our I optimal fi.",
            "They're actually zero that happens in certain situations.",
            "So if for those after I, even if we look at the decision function, if I is 0 then we don't have to calculate the inner product between the eyes training instance and the test instance.",
            "Quiet because no matter what this value is, then multiply to to 0, then you still get 0, so that links these eyes.",
            "Training instance is not used indecision.",
            "Because Alpha is 0, so in in doing prediction only those data with Alpha I greater than zero, they are really used.",
            "So those data are called support vectors.",
            "That's how the name of this method comes from."
        ],
        [
            "So this figure shows hot support vectors are so essentially after solving the SVM dual problem you get a subset of your training instances.",
            "There are more important and they are called support vectors.",
            "Here we have two classes of training instances, where some are circles, some across Marks and I also have some red points.",
            "We lose red points, maybe either circle or maybe close marks.",
            "They are our support vectors.",
            "This is not a linear linearly separable situation, so our decision boundary should be nonlinear.",
            "So roughly it is like this.",
            "The point somehow close to the decision boundary, they become support vectors.",
            "So now we know why this technique is called support vector machine."
        ],
        [
            "So we have roughly shown some basic ideas of support vector machines.",
            "Will now.",
            "Right now I can.",
            "I can show you a simple demonstration so we have we have we have a toy toy here to to see how SVN works.",
            "So this is a 3 dimensional space, so let's try to draw some points here, so use this.",
            "This panel on the right hand side to give points where the way we drop points in a 3 dimensional space is to think that this is X.",
            "This is XY plan and singular the Z.",
            "The Z axis is perpendicular to to this plan.",
            "OK, so if I move I use mouse to move then that means I change the Z value.",
            "So let's try to have some points here.",
            "Well, OK, so now I so I have given some training instances.",
            "Let's try to.",
            "Try to have more.",
            "OK, then I'm going to change to a different class.",
            "So I have data.",
            "In a different place.",
            "I guess this is enough, so let's see what happens.",
            "So let's try to run.",
            "So essentially we now we are solving the dual problem.",
            "So once we have solved you OK.",
            "So it's really easy.",
            "Easiest way to see how data.",
            "OK, so roughly is our place.",
            "So we have data over the first class we learn about in this area and also some in this area and data in the second class they are about in this region.",
            "So SVN after solving the deal you actually get a nonlinear curve to separate all those training instances.",
            "So this roughly gives you an idea how support vector machine works.",
            "So in high dimensional space we still solve linear separating hyperplane but big.",
            "To the original space, it is still a nonlinear curve.",
            "Yes.",
            "God here this is RBF kernel.",
            "So obviously Colonel easier, easier Colonel so that you can always overfit training data.",
            "So by using this toy I mean no matter what kind of points you job, as long as you don't have two points at the same position but in different classes, then you are guaranteed to find to find the nonlinear curve so that training data correctly separated.",
            "So you can easily overfit training data, but usually that's not useful.",
            "That may not be useful.",
            "Yeah, so so this is the web address of these.",
            "Is toy program so you can try to play with it.",
            "Well, there there are a couple of books about support vector machines.",
            "Yeah, so here at least two and one is bye bye.",
            "And Alex smaller, so we may recover a lot more materials.",
            "There's also.",
            "There's also a website called Kernel Machine.",
            "Webs have to code kernel machines and it has a blackboard.",
            "But I think this is quite active discussion board, so you have any kind of questions regarding theory or how to use SVM software.",
            "All those kinds of things.",
            "Then you can go there.",
            "Usually there are some people to help to answer your questions.",
            "So so any questions so far?",
            "If not, then we will.",
            "We will start deriving the SVM Primulina deal problems.",
            "Well maybe I can say a few things about this.",
            "This is actually a small, small interesting research problem.",
            "Here is how to set testing time the way how we draw this decision function.",
            "This decision surface is by testing every point.",
            "In the three dimensional space so.",
            "So we do agreed technique point.",
            "Here we check the decision value.",
            "That's the way how we show this.",
            "This is a linear curve 'cause you don't have the explicit form of the knowledge.",
            "But once we have after you do have a nonlinear equation.",
            "So how to efficiently?",
            "Identify this surface and draw it.",
            "There will be other.",
            "There's an interesting thing, so this is actually slightly related to how to speed up testing, but the situation is not general building in general when you do testing, you don't know those relationship between those testing instances, But here you do know you know those are.",
            "Discrete points in the industry.",
            "Dimensional cube come you know that so the situation is slightly different, so you may find out some special ways to speed speed up testing procedure."
        ],
        [
            "So now we need to tell you how this deal optimization comes."
        ],
        [
            "Bra so we need to do some calculation.",
            "We consider a simple situation without the select variables.",
            "So earlier in our University and formulation we have, we have a penalty term and we also have a select variable here to allow training errors.",
            "But this will complicate the derivation of LTU.",
            "So we consider a situation without them.",
            "So this is a primal SVM problem.",
            "Then the deal will be like this.",
            "So the difference from the one that we mentioned earlier is that you don't have the inequality that after I is less than or equal to, see and you don't have that.",
            "So now I has no upper bound and all other things are the same.",
            "So what I'm going to show you is how from this problem we can derive this one and."
        ],
        [
            "Yeah, so we're going to use something called Lagrangian deal.",
            "But this is a commonly used technique in nonlinear optimization.",
            "More specifically, actually convex optimization.",
            "So we define something called Lagrangian function.",
            "Well so W and be there original variables, so we have the objective function here and we also have inequalities or constraints.",
            "So we introduce something called Lagrangian multipliers after I.",
            "Well, now we call it out because in the end it is really the other eye of our dual problem.",
            "So we minus the summation of I times those equalities, yeah?",
            "So this is a quote.",
            "The Lagrangian function, then the Lagrangian deal is defined to be like this, so this is.",
            "So we maximize with respect to Alpha and then inside this parenthesis we must we minimize with respect to W&V.",
            "So that's the definition of Lagrangian do, so it is it is a different optimization problem, so forth.",
            "So now for from the SVN primal optimization problem.",
            "So we define this Lagrangian deal.",
            "So it is a different.",
            "It's a different optimization problem.",
            "So for this Lagrangian deal, there are some good properties, especially this is a strong development, so strong duality says that the minimum of the primal problem is the same as this maximize the maximum of the Lagrangian deal.",
            "So there are objective optimal objective values are actually the same, and that's the property.",
            "But you want to be careful about this because not for every function this property holds and will go back to that later.",
            "So you want to be careful not for every function you have such a property only for certain functions.",
            "Of course, for our function right now it will be OK. Well, I think tomorrow in mountains talk he's going to say something about more about developing.",
            "He's going to define something called conjugate function.",
            "Yeah, that's actually the general singing in convex optimization, so this Lagrangian deal is actually also derived from that so called conjugate function, But these are.",
            "Engine deal is basically specifically for constraint optimization problem, But anyway, so now we have defined what Lagrangian deal is and also how how it looks like.",
            "So the next thing we're going to do is to simplify this Lagrangian.",
            "Do try to change it to a simpler form.",
            "Yeah.",
            "Why do you want to increase the number of support vectors?",
            "Increase the number of support vectors.",
            "OK yeah, well that's a good question.",
            "So essentially you are asking for the idea behind Primal and do well.",
            "That's a big question.",
            "You can explain prime and deal from from some economic theory actually.",
            "Well, I can give you some very simple explanation here.",
            "So OK, so let's.",
            "Well, let's assume that our final goal is for him.",
            "Somehow their objective objective values to be the same as soon as that's the goal we want.",
            "Reasonable, because now we're going to find out a different problem, and this program must be related to the original 1, right?",
            "So we assume that we're hoping to have this.",
            "So that means somehow this value should be the same as the minimum flow prime.",
            "But if we look at least Lagrangian function, so now we have the objective function.",
            "OK, so this is the same as the objective function over the prime and we also write downloads inequalities losing equalities.",
            "So for any feasible W, so that means they satisfy inequality that this is greater equal to 0 for the primal.",
            "This is this is a positive value, at least not negative.",
            "This is not negative value.",
            "In the midst of a, this Alpha is also not negative, so you have the multiplication of two positive values, so then you're minus something.",
            "Wow, so you have the objective value minus something, so this should be smaller.",
            "Should be smaller than the original objective value, right?",
            "So then this is not good.",
            "This violates our goal of maybe making those two things to be together.",
            "So then we have no choice but to hard to maximize the whole thing somehow.",
            "Try to maximize with respect to Alpha then.",
            "That is more possible that by doing so, then the right hand side the Maxima can reach the minimum of the primal.",
            "So essentially the explanation that I just gave is called weak duality.",
            "So we can do it.",
            "He says that this minimum of primal is greater equal to the maximum of this and this whole week duality.",
            "So that's roughly an explanation, but this has nothing to do with support vectors when doing these primal and dual optimization.",
            "People know this is nothing to do with support vector machines.",
            "This is just pure quadratic optimization problem so.",
            "At least from the point that point of view, we don't worry about why we are.",
            "We don't worry about the meaning of support vectors, but maybe you can think more from that point of view.",
            "Well, I don't know.",
            "Maybe you can come up with a new explanation of primal deal using from the support vector machines point of view will be very good.",
            "I I don't have a good answer about that right now.",
            "OK, so now let's just say OK, this is the Lagrangian deal.",
            "So the next thing we would like to do is to simplify the dual problem.",
            "So how to simplify it is well, this is looks like a complicated optimization problem.",
            "You have maximize and user minimize.",
            "So what we can do is to assume that to think about the situation when Alpha is fixed.",
            "So let's check if Alpha is fixed then how how to solve the minimization problem inside this big parenthesis."
        ],
        [
            "So this is another easy fixed is fixed then immediately what we can see is if summation of iy is not equal to 0 then we can meet.",
            "We can make this minimization to go to minus Infinity.",
            "Well, how to see that?",
            "So you can see you have Alpha, IY, times be well so you can.",
            "You can pull me out so this becomes B times submission of I * Y.",
            "So if submission of iy is not zero and you are minimizing with respect to W&B so you can just move B to either plus or minus Infinity depending on whether this is positive or or negative.",
            "Well if this is positive and just move B2 plus.",
            "Infinity, then this minus, then you you get minus Infinity here.",
            "So this this minimization problem with respect to WB can be rewritten as this.",
            "So we have two situations.",
            "So the first one is if this Y transpose Alpha mention is actually why transpose Alpha is not is not zero, then these minima has the value minus Infinity, but on the other hand if Y transpose Alpha is actually 0, then we don't need to write this term with.",
            "Related to be, if you have this summation to be 0, then you can remove this term so we don't have.",
            "We don't have be here, we only have to minimize with respect to W. OK, so we simple Violet do a little bit.",
            "Really well, actually it looks more complicated for anyway, so now so now this is the problem we have so far."
        ],
        [
            "Len, then we check the second situation harder.",
            "If Y transpose Alpha is 0.",
            "But now we are going to minimize with respect to tablet and the least function so.",
            "This function right now OK, so now is fixed, so W is the only variable.",
            "It is strictly convex function of W. Well, I assume you know about convex function is or strict convex function is.",
            "This function is strictly convex because of this quadratic term W transport server.",
            "So from optimization theory we know that the optimal happens when.",
            "Partial derivative with respect to WE 0 so we do this so then well, so we do so for derivative of W transpose W that's actually W, and then for this part we actually remove W so we have W is equal to the linear combination of training instances.",
            "So now we have list, so that means if Alpha is fixed and why transfers are but is 0, then this optimal damn it must be.",
            "Linear combination of training data using this Alpha.",
            "So we can further simplify our deal, Prob."
        ],
        [
            "So now we have the form of W in terms of Alpha, right?",
            "So we have so we can represent the W to be a linear combination, so using the Alpha.",
            "But we want to remove remove W when the reason is we want to simplify this so we don't.",
            "We hope that we don't have this minimization, so we hope to remove variables W&V.",
            "In this formulation.",
            "So we put this W back to this objective function so we have to do some calculation.",
            "So this chick W transport stopping first.",
            "So now we have W to be this linear combination, transpose to this linear combination will essentially see summation IJ of.",
            "XI transfers XJ well, so this is this is already very close to the quadratic term of our deal problem.",
            "OK, so now we have reached our transpose, Cuba, so in our earlier formulation we use vector and matrix formulation instead of summation, so this is already Alpha transpose QR.",
            "But then there's also linear turn, right?",
            "There's a linear term here.",
            "We also put the W. Into this, but if you look at this so if you if you move W in into this position, if we moved up here.",
            "So essentially what you have is this is I mentioned our five Yr XI watch list.",
            "This is another W so once you have this relationship, essentially you are minus minus.",
            "So you do minus W transpose W here so you have held W transpose minus the whole W transpose W. So how do you have is minus you have minus health W transport study?",
            "Yeah, so this is the new quadratic term, but we also have a linear term that's this.",
            "One is minus one and minus I mentioned Alpha.",
            "So we have a summation Alpha here.",
            "So minus minus becomes puffs.",
            "Therefore we have this submission of Alpha I minus something.",
            "So this is already very close to our dual optimization problem is remember we are doing maximization here.",
            "So if you reverse it to minimization then that's already have times of our transpose QR minus some Asian up.",
            "I was already our objective function of the deal that we mentioned earlier, so we're almost there.",
            "So we still have two situations that whether if why transfers are easier or if why transfer server is not zero.",
            "Then the tricky thing of doing this maximization is when you are doing maximization right.",
            "So then you can see their own different kinds of Alpha then for certain Alpha they give you objective value minus Infinity.",
            "But you are doing maximization, so of course minus in."
        ],
        [
            "And it can never be your optimal objective value.",
            "There's now we're doing this Lagrangian deal, so we're doing maximization.",
            "So of course, minus Infinity.",
            "Definitely not Maxima of the deal.",
            "So that means fewer optimal solution does not happen when this summer.",
            "This why transports are but is not zero.",
            "Therefore we only have two.",
            "So we don't have to consider this situation at all.",
            "We don't have to consider this situation.",
            "We only have to consider the first situation.",
            "Will we only consider the first decision?",
            "This becomes a kind of conditions on Alpha, so that becomes a constraint of the deal.",
            "So we move this to be a constraint of this optimization problem.",
            "So so writing this and also after greater or equal to 0, then we have the dual constraints and then we have this maximization.",
            "Well, this is the same as the object of the objective function that we mentioned early, so this is already the deal that we gave.",
            "So this is the.",
            "The typical way of deriving the dual problem with one thing I want to mention here is.",
            "Well, if you look at some papers they don't use really exact will use exactly the same way as I am using here.",
            "Uh.",
            "To explain to explain that summit why in the dual problem this why transfers after should be zero will some people did this?"
        ],
        [
            "So they write the Lagrangian function.",
            "OK, then they know that they're going to minimize with respect to W&B, so they do partial derivative with respect to W. And also be yeah, well, just like we do, we do W here so they.",
            "They just check the derivative with respect to VPSB is a linear term visa.",
            "This visa this is a linear turn of me, so if you do partial derivative when you get some mention of iy and they just say oh let's put it to be zero, well that's not very right.",
            "If you're following this this this this derivation you cannot do that.",
            "The reason is because.",
            "Because of this, I mean if you check this, this is not a zero, then you can have minus Infinity and later we have like minus Infinity is is not going to be the optimal for the deal, therefore it is not considered.",
            "Just do partial derivative.",
            "You need to have a strictly convex function of the variable, but our objective function is only strictly strictly convex on W but not on be.",
            "If you have a linear turn of me, it is not a strictly convex term.",
            "Yeah, so in this derivation, usually if you see some people they they just do partial derivative with respect to be, well, that is not very very correct way.",
            "Then I want to say a little bit more about your problems.",
            "So after SVM is a popular technique, so quite a few people, they just think that for any optimization problem Miss Lagrangian do exists and also strong duality holds.",
            "Well, this is actually wrong.",
            "In optimization theory, now there's nothing like that.",
            "We only have series saying that if this optimization problem is a convex problem, so convex programming means the objective function is convex and also the physical region.",
            "OK, physical range means the set of points satisfying all the constraints, so this set is also is also convex.",
            "Then we have a convex programming problem and this is not enough.",
            "There's something called constraint qualification, so your constraints must satisfy certain conditions only for this kind of problems, then you have less strong duality.",
            "OK, you cannot just write a new optimization problem and then just write down the Lagrangian deal and say, oh strong developing hosts.",
            "No, that's not right.",
            "So now we have we for our optimization problem.",
            "It satisfies all the conditions.",
            "First the SVM optimization problem is convex.",
            "Well, it is strictly convex on W but also convex on B and also select variables.",
            "So it is convex.",
            "So we are fine for the first condition.",
            "And another important effect for constraint qualification is if you have so-called linear time constraints.",
            "Then your constraint qualification is always satisfied, let's see.",
            "And except the, let's see the primal problem.",
            "OK, so here is the primal problem.",
            "So how do we have is?",
            "Like we have several linear inequality is not a variable establishment we so this is this is returned linearly in WMV.",
            "For this kind of inequalities then constraint qualification is always satisfied, so you don't have to check to check that.",
            "So you need to be more careful if you have nonlinear constraints, then you want to be careful on using the Lagrangian duality.",
            "But now we are fine for this.",
            "Yeah, OK.",
            "But the problem is.",
            "You usually.",
            "So when I review some papers then yes some papers late, they just write the new formulation.",
            "Then they say OK. Now Granger, interior and even the problem is not convex.",
            "But then I don't know if that's right or or correct.",
            "The theory says if convex or certain conditions then strong duality holds.",
            "Maybe for some certain non convex problems we also have strong duality, we just don't know.",
            "So it is for this kind of work then I would always have problems that I need to find a counterexample or things like that.",
            "That's not very good, so usually you are yeah so.",
            "So if you want to remember this when you when doing the using the duality, you need to check certain."
        ],
        [
            "Thanks, now you may have another problem that you may have another question.",
            "Our optimization problem is an infinite programming problem, right?",
            "Well yes, because we have mapped data to a high dimensional space, we have infinitely many variables.",
            "So now you may say who does the Lagrangian duality still hold for this kind of space?",
            "But the answer is yes.",
            "The Lagrangian duality theory can hold for yeah for Hilbert space or even banner has space for loss.",
            "In those spaces, it is still right so so we can do that.",
            "Basically we can do the same thing.",
            "Well, I have a paper doing a rigorous discussion on this issue.",
            "Yeah, so this one is fine.",
            "We also so now we have finished discussing primal and dual optimization problems, so any questions?",
            "Right now.",
            "Yes.",
            "Play conveyor.",
            "An example for Lavinia concerns.",
            "No dimension.",
            "Nonlinear constraints then you have to be careful on using Lagrangian duality, and that's that's what I want to tell you.",
            "Yeah, you want to be a little bit careful you to really check something called constraint qualification.",
            "But you you have to check.",
            "Well I I don't have time to mention those things here, but you just go to get any nonlinear programming books then.",
            "Go to check the chapter of Lagrangian duality and then we will talk about something called constraint qualification and there are different types of constraint qualifications, so it's possible that if you have certain nonlinear conditions nonlinear constraints then.",
            "Even satisfy one of constraint qualifications, then you are OK.",
            "So optimization researchers may develop different types of constraint qualification.",
            "So it's like different kinds of kinds of tests.",
            "So we check whether your past trends satisfy one of them.",
            "If that's the case, then you are fine for doing for using Lagrangian duality nicely.",
            "The idea.",
            "Yes.",
            "So now we have the.",
            "This is standard from the format of certain information for visiting.",
            "I didn't understand what is the need of dual automation.",
            "So what we need to do?",
            "The reason is simple, because we don't want to solve an infinite dimensional problem.",
            "So the primal.",
            "Yeah, yeah.",
            "Yeah, yeah, so this is the primal problem.",
            "So you have an infinite vector here, so you're going to have an infinite dimensional vector vector variables.",
            "We don't want to deal with such a situation, so solving a deal may be easy.",
            "Yeah, yeah, so that's a computational issue.",
            "If you solve the deal then then it is easy.",
            "So of course I later I will tell you how right now how we solve with you.",
            "If you problem is very small then just use any optimization package and you can solve the deal.",
            "And that's another reason.",
            "So any more questions?",
            "Yeah, yeah.",
            "OK so finally I see.",
            "Where did the eaters go between the primal and dual problems?",
            "Even if we have selected area, you mean if we have select variables?",
            "Yes.",
            "First part is simply disappear.",
            "You could just see the CPT.",
            "OK, yeah, because I seem for OK.",
            "So I don't.",
            "I don't include the penalty term here.",
            "Well, that's just for for simplifying the derivation of the deal so it will be.",
            "It will be a good exercise if you add that back and following the same procedure then you will get the deal that we used earlier.",
            "I'm talking about the first part of it, OK?",
            "Yeah so yeah.",
            "When you present individual with this constant, yes, let's see here.",
            "Yes yeah.",
            "So even a primal problem.",
            "You have this penalty term if you know primal problem you have this penalty term then.",
            "Do you have this upper bound?",
            "See here, but if you live in a primal you don't have this term, then in a deal you don't have this upper bound.",
            "Leslie situation.",
            "What does that answer your question?",
            "It does go between the panel and OK right now, so the same question is where?",
            "To be late, disappear right so from the primal.",
            "So in a primer we have variable WMV, but then they disappear in a dual function.",
            "Well, that's the same.",
            "So the same apply applies to select variables.",
            "Now in this primal formulation actually WB and also can see they are all considered variables that are in the same position.",
            "Then when deriving the deal they disappear together.",
            "Does that answer your question?",
            "Yeah, I'll probably hope.",
            "Well maybe I think yeah.",
            "How can we be better represented?",
            "How can?",
            "Copy.",
            "Possibility is that with representative yeah you missed this term.",
            "Yeah, so hard question about this time.",
            "We should be also the.",
            "NATO.",
            "B should be high dimensional while as a crazy idea I never thought about that.",
            "OK, we all you can do that, maybe I don't know, but this is already complicated enough to make data space.",
            "Well, I think you can do that.",
            "Scott, yeah yeah you can.",
            "You can do.",
            "You may do that OK?",
            "OK, OK, you mean OK, so that's a more reasonable question.",
            "With instead of making the.",
            "So how to obtain?",
            "Be?",
            "Yes, that's a good question.",
            "Because I didn't say that.",
            "Do you see how to have W?",
            "I say, oh, if you have the optimal deal then you are going to get the optimal doubling, but didn't say how to get this week.",
            "So now you know.",
            "I somehow I skipped that because it is not easy to say how to obtain it, so because it is not easy.",
            "Well, I think I can personally tell you how to do that, but I don't think we have time.",
            "Yeah, it's not.",
            "It's not easy.",
            "Yeah, you need some certain properties.",
            "You need certain properties.",
            "We let's see.",
            "OK, I think I can have a geometric interpretation here.",
            "OK.",
            "Hold on, hold on, let me finish it at first.",
            "OK, so there's a property here that for for for such a point because it satisfied W transpose X I + V = -- 1.",
            "So this is the way to explain that.",
            "So for this point if we know it is on this straight line, so we know it satisfies W transpose XI.",
            "Suppose this is XI plus B = + 1.",
            "And if we know W and we know XI, we know plus one, then we know we so roughly, that's one way to calculate me.",
            "But in practice we need something else.",
            "So, so here's another question.",
            "Here Oh yeah.",
            "Set up an E on the hyper pray that it cannot be classified.",
            "You mean it is test data is still is wrongly classified or something?",
            "Of course you can.",
            "Then you can never achieve 100% testing accuracy, but you don't have test data so you can 100% separate training instances but not for testing.",
            "But we can make it to a high dimensional space.",
            "That's OK. Then we do testing there.",
            "Well, so I think it's better to have a break, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you so so today I will talk about support vector machines, so this is a specific classification and regression method.",
                    "label": 0
                },
                {
                    "sent": "So we want to get into some details about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This method so the plan is like this.",
                    "label": 0
                },
                {
                    "sent": "So we have several topics, so I'm hoping to OK.",
                    "label": 0
                },
                {
                    "sent": "So for the first part we hope to finish the first 2.",
                    "label": 0
                },
                {
                    "sent": "So there are basic concepts and also to derive the SVM primal and dual problems.",
                    "label": 1
                },
                {
                    "sent": "Then for the second part I will talk about training and linear nonlinear support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Let me also touch the issue of parameter and kernel selection and also some practical issues.",
                    "label": 1
                },
                {
                    "sent": "Also mentioned some possible research issues in support vector machines with some of.",
                    "label": 0
                },
                {
                    "sent": "Some of those issues will be left to the third part.",
                    "label": 0
                },
                {
                    "sent": "Then we also mention multi class classification and then have some discussion.",
                    "label": 0
                },
                {
                    "sent": "So please feel free to interrupt me if you have any questions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so maybe now question is why we're here to talk about support vector machines?",
                    "label": 0
                },
                {
                    "sent": "Well, at least right now.",
                    "label": 0
                },
                {
                    "sent": "It is one of the very popular class, so in quite a few situations it can give you competitive performance with existing classification methods where I'm not claiming it is, it is better.",
                    "label": 1
                },
                {
                    "sent": "We are usually it maybe not, but it is usually reasonably good.",
                    "label": 0
                },
                {
                    "sent": "Then about using such a method, well, here I would say it is relatively easy to use.",
                    "label": 1
                },
                {
                    "sent": "Well after sale is only relatively well.",
                    "label": 0
                },
                {
                    "sent": "I mean don't convert well I can hear is never of course.",
                    "label": 0
                },
                {
                    "sent": "I mean doing any data classification.",
                    "label": 0
                },
                {
                    "sent": "You want to try cleaning with, never just like the same set on Monday, but compared to some very sophisticated classification techniques where it is not.",
                    "label": 0
                },
                {
                    "sent": "So difficult to use so.",
                    "label": 0
                },
                {
                    "sent": "Seems like that may be one reason why.",
                    "label": 0
                },
                {
                    "sent": "It is popular right now is via neither special kernel kernel technique, so will the whole kernel area is a bigger area.",
                    "label": 1
                },
                {
                    "sent": "So next week I think Alex smaller is going to give a tutorial on kernel techniques, so there are many extensions, so SVN miso is only a kind of kernel methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about two class classification first.",
                    "label": 0
                },
                {
                    "sent": "So we're thinking about a very simple situation.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that there are training instances, so we have XII is from one to L. So there are vectors in Euclidean Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "And each each vector is the so called feature vectors.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you are representing each patient as a feature vector, then you may have components like height, weight, blood pressure, etc.",
                    "label": 1
                },
                {
                    "sent": "So you have a you get along vector.",
                    "label": 0
                },
                {
                    "sent": "So usually for such a vector we create a data instance.",
                    "label": 0
                },
                {
                    "sent": "Now we consider a simple situation with only two classes of data.",
                    "label": 1
                },
                {
                    "sent": "So in practice your data may have many, many classes.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are doing handwritten digit recognition, then you are going to have 10 classes, but not for.",
                    "label": 0
                },
                {
                    "sent": "For SVM it starts from from dealing with the case of two classes only, so for easier description we define an indicator vector for the Y.",
                    "label": 1
                },
                {
                    "sent": "So this why is a vector with length L remember Arizona number of training instances, so this each component of why he has value either plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So we have here that if the ice training instance is in the first class, then we say why I is plus one.",
                    "label": 0
                },
                {
                    "sent": "But if XI is in the second class then we define why I could be minus one.",
                    "label": 0
                },
                {
                    "sent": "So actually we define this Y and it plays a role.",
                    "label": 0
                },
                {
                    "sent": "An indicator vector.",
                    "label": 0
                },
                {
                    "sent": "Let the basic idea of support vector machines is to find a so called set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rating Hyper plan with just like this figure shows.",
                    "label": 0
                },
                {
                    "sent": "So we have two classes of training instances.",
                    "label": 0
                },
                {
                    "sent": "Some are circles and some are triangles.",
                    "label": 0
                },
                {
                    "sent": "So SVM tries to find the separating hyperplane to separate all the training instances well in a 2 dimensional situation then this is just a straight line but in a higher dimensional space we call it a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So what we intend to find is this started online.",
                    "label": 0
                },
                {
                    "sent": "So once we have restarted online then we have a model to do prediction.",
                    "label": 0
                },
                {
                    "sent": "But the reason is so if you use this study tonight as your model, then for any new data you just check whether that data is on the right hand side or on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "If on the right you predict it as a circle.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you say it is a triangle, so it is so simple.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "Basically, as a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So the question is how to find this.",
                    "label": 0
                },
                {
                    "sent": "Study the line.",
                    "label": 0
                },
                {
                    "sent": "So here I show 2 figures and actually layer layer the same training instances.",
                    "label": 0
                },
                {
                    "sent": "OK so you know two Vigors flow circles and triangles there in the same position.",
                    "label": 0
                },
                {
                    "sent": "And I also show 2 dotted lines to separate training instances.",
                    "label": 0
                },
                {
                    "sent": "Will they both fully separate our training data so the question is, which one might be better?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for this one.",
                    "label": 0
                },
                {
                    "sent": "All training instances are correctly separated, but this one is as well.",
                    "label": 0
                },
                {
                    "sent": "So then the question is which one might be better?",
                    "label": 0
                },
                {
                    "sent": "Intuitively, we say the right hand side one is better.",
                    "label": 0
                },
                {
                    "sent": "So how's the reason?",
                    "label": 0
                },
                {
                    "sent": "Looking at loss training instances roughly, we see that circles there about around this rich.",
                    "label": 0
                },
                {
                    "sent": "Conceptually, we can say that yeah, so from those circles we say for most circles they should appear in this area and also for triangles.",
                    "label": 0
                },
                {
                    "sent": "They should appear in this area, so if you have a model to be like this, like this one, then some circles here they may be wrongly predicted and also some triangles.",
                    "label": 0
                },
                {
                    "sent": "Here they may be wrongly predicted.",
                    "label": 0
                },
                {
                    "sent": "You don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe for testing data, some circles or triangles they just appear in those.",
                    "label": 0
                },
                {
                    "sent": "Regions here then, for if you use this model, your testing performance won't be so good, so we think the right hand side one is actually better.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, we think it is more general.",
                    "label": 0
                },
                {
                    "sent": "It better captures the distribution of the two classes of training instances.",
                    "label": 0
                },
                {
                    "sent": "Well, of course there is some theory behind that, but now we start from the more geometric POV saying that this kind of separating hyperplane is actually better.",
                    "label": 0
                },
                {
                    "sent": "So now the question is how to find out.",
                    "label": 0
                },
                {
                    "sent": "This kind of dotted line so that it is the most general one.",
                    "label": 0
                },
                {
                    "sent": "So, so we need to define certain equations.",
                    "label": 0
                },
                {
                    "sent": "Well, for any hyperplane we can represent it as a W. Transpose X + B = 0.",
                    "label": 0
                },
                {
                    "sent": "So so X is the variable and adopt you and be there somehow, like coefficients of this hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So once W and VR decided then then you have the equation.",
                    "label": 1
                },
                {
                    "sent": "So our job right now it's not find X.",
                    "label": 0
                },
                {
                    "sent": "Instead we try to find out W enemy is once we have W&B then we have Amanda.",
                    "label": 0
                },
                {
                    "sent": "So mathematically to say that training instances our correctly separated is bylaws inequalities.",
                    "label": 0
                },
                {
                    "sent": "So evil eyes training instance is on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "That means it is a circle.",
                    "label": 0
                },
                {
                    "sent": "Then if we put XI into this phone then it should be greater than 0.",
                    "label": 0
                },
                {
                    "sent": "So we see if y = 1 will remember.",
                    "label": 0
                },
                {
                    "sent": "Why is the indicator vector?",
                    "label": 0
                },
                {
                    "sent": "So so for this data, which is a circle, you should be greater than 0, otherwise it should be less than zero.",
                    "label": 0
                },
                {
                    "sent": "So what is your decision function?",
                    "label": 1
                },
                {
                    "sent": "So once you have this started online, or equivalently say you have WMV, then this is your decision function.",
                    "label": 0
                },
                {
                    "sent": "So that means for any new data X you put that into W, transpose X + B.",
                    "label": 0
                },
                {
                    "sent": "Then you check whether it is greater or less than zero.",
                    "label": 0
                },
                {
                    "sent": "So if greater than user it is a circle.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the remaining job is to decide W&B.",
                    "label": 0
                },
                {
                    "sent": "So we have just said that we would like to have A to have the most general one.",
                    "label": 0
                },
                {
                    "sent": "So in a sense we think if we draw a tube like this so now OK.",
                    "label": 0
                },
                {
                    "sent": "So now from this started online we barely move it to the first class and also move it to the second place so we can have two additional lines layer code W transpose X + B equals plus or minus one will not even say 5 one and plus 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can use classify or minus five, but that's just an issue of scaling.",
                    "label": 0
                },
                {
                    "sent": "But for example, if you think from this now you have W transpose X + B = 0.",
                    "label": 0
                },
                {
                    "sent": "If you multiply it by 5 or by 10 or less, still the same hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't matter if you use any value here, But anyway, for simplicity, of course we use plus and minus one that is easier.",
                    "label": 0
                },
                {
                    "sent": "So once we have lost two lines, then somehow we say that the most general one general hyperplanes are one.",
                    "label": 0
                },
                {
                    "sent": "So let the width of this tube is maximized.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we want to find out the width of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this tube, but it turns out that the distance between, so that's actually the same as the distance between two parallel lines.",
                    "label": 0
                },
                {
                    "sent": "So if you calculate the distance between W, transpose X + B equals plus and minus one, that's actually 2 divided by the Dome of W. So that's the same as 2 / sqrt W transpose standing.",
                    "label": 0
                },
                {
                    "sent": "So we are trying to maximize this turn.",
                    "label": 0
                },
                {
                    "sent": "If is among so many possible W&B, we would like to find out.",
                    "label": 0
                },
                {
                    "sent": "So that this value is maximized, so this is so called.",
                    "label": 0
                },
                {
                    "sent": "This is called maximum maximum margin.",
                    "label": 0
                },
                {
                    "sent": "That's one special thing about SVM and the whole maximum margin classifiers.",
                    "label": 0
                },
                {
                    "sent": "So finally we have a quadratic programming problem, so this is the first somehow you can say this is the first SVM formulation.",
                    "label": 1
                },
                {
                    "sent": "We we try to maximize this, but usually we like to do minimization while maximizing something is equivalent equivalent to minimizing the reciprocal.",
                    "label": 0
                },
                {
                    "sent": "So we have we can minimize the number of W / 2, but square root is an increasing function, so we can remove it.",
                    "label": 0
                },
                {
                    "sent": "Therefore we get W transpose tablet divided by two.",
                    "label": 0
                },
                {
                    "sent": "So this is the objective function that we would like to minimize.",
                    "label": 1
                },
                {
                    "sent": "Learn subject to certain constraints, so other constraints will now.",
                    "label": 0
                },
                {
                    "sent": "We still hope to.",
                    "label": 0
                },
                {
                    "sent": "Classify all the training instances.",
                    "label": 0
                },
                {
                    "sent": "This allows XI four circles.",
                    "label": 0
                },
                {
                    "sent": "They should be greater equal to 1 with those two additional lines.",
                    "label": 0
                },
                {
                    "sent": "Now we don't see greater than zero with a greater equal to plus one and four triangles.",
                    "label": 0
                },
                {
                    "sent": "We say less than or equal to minus one, but we don't like to write two types of inequalities, so we try to combine them together.",
                    "label": 0
                },
                {
                    "sent": "So if you multiply Y on both sides, so remember, now we have less than or equal to minus one, But if we multiply Y on both sides then.",
                    "label": 0
                },
                {
                    "sent": "This minus 1 * -- 1 becomes one Anna user have to change the inequality sign, so then you get greater or equal to 1 as well.",
                    "label": 0
                },
                {
                    "sent": "So those two inequalities they can be combined together as a single form.",
                    "label": 0
                },
                {
                    "sent": "So we have L training instances and for each one we hope it is correctly classified.",
                    "label": 0
                },
                {
                    "sent": "This is the quadratic programming that SVM solves.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course this is not good because data may not be linearly separable, so we cannot always use a linear classifier.",
                    "label": 1
                },
                {
                    "sent": "This is an example that you can never use a straight line to separate the two classes of training instances.",
                    "label": 0
                },
                {
                    "sent": "Instead, you can use a nonlinear curve.",
                    "label": 0
                },
                {
                    "sent": "So we do two things here.",
                    "label": 1
                },
                {
                    "sent": "Firstly, we must allow training errors.",
                    "label": 0
                },
                {
                    "sent": "If you don't do that, then.",
                    "label": 0
                },
                {
                    "sent": "Then this is a so called infeasible optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The idea of a constrained optimization problem is that from from the set of candidates which satisfy all constraints, then you select the one so that the objective function is minimized.",
                    "label": 0
                },
                {
                    "sent": "But now this.",
                    "label": 0
                },
                {
                    "sent": "There's no tablet be satisfying all those constraints because you cannot find any straight line 40 separate.",
                    "label": 0
                },
                {
                    "sent": "All the training instances.",
                    "label": 0
                },
                {
                    "sent": "So you even don't have any candidates to select before doing the minimization, so this is a so called invisible optimization problem, so you need to allow training errors, say OK, Now I give up separating all the training instances.",
                    "label": 0
                },
                {
                    "sent": "I just try to separate and maybe some of that.",
                    "label": 0
                },
                {
                    "sent": "But later we will say that it is generally not a good idea to fit all the training instances.",
                    "label": 0
                },
                {
                    "sent": "So the purpose of allowing training errors is not too is not only to make this optimization problem feasible.",
                    "label": 0
                },
                {
                    "sent": "We user hope.",
                    "label": 0
                },
                {
                    "sent": "Actually we hope to get even better, better testing accuracy.",
                    "label": 0
                },
                {
                    "sent": "So if you overfill the training data, then maybe the performance isn't so good.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing we're going to do, and then another thing is, instead of directly using certain non linear curves, it is very difficult to model nonlinear curves.",
                    "label": 0
                },
                {
                    "sent": "But we didn't learn many nonlinear curves.",
                    "label": 0
                },
                {
                    "sent": "Baking high school.",
                    "label": 0
                },
                {
                    "sent": "A very smart idea is that now instead of modeling nonlinear curves in the original space, we try to make the data into a higher dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "That means we use the function by.",
                    "label": 0
                },
                {
                    "sent": "So for any eggs we make X2 maybe a much higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So intuitively you can say that it so now in this original space it is smaller, so it's more difficult to linearly separate your training instances.",
                    "label": 0
                },
                {
                    "sent": "But if you map all those data into a very high dimensional space, it is much bigger.",
                    "label": 0
                },
                {
                    "sent": "So then it may be easier to linearly separately.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "This is the the standard support vector machines that people are solving right now.",
                    "label": 0
                },
                {
                    "sent": "Of course there are a lot of multiplications, but this is the most commonly used formulation.",
                    "label": 0
                },
                {
                    "sent": "So now you can see where we do the changes.",
                    "label": 0
                },
                {
                    "sent": "So this is the first one, but we didn't do anything different.",
                    "label": 0
                },
                {
                    "sent": "We just made the data first.",
                    "label": 0
                },
                {
                    "sent": "So originally we are in the in the input feature input space, but we made everything to higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then we also put ourselves into that higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we do the same thing we still do.",
                    "label": 0
                },
                {
                    "sent": "We still try to get a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So that's why there is amazing here.",
                    "label": 0
                },
                {
                    "sent": "So I did show an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose the original your data in the industry dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So you have three components.",
                    "label": 0
                },
                {
                    "sent": "Then we can use such a mapping function.",
                    "label": 0
                },
                {
                    "sent": "So we made it to the 10 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we have 10 components.",
                    "label": 0
                },
                {
                    "sent": "You can see that we have the first, second and the third components.",
                    "label": 0
                },
                {
                    "sent": "We also have squared, but we also have X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "So now you see why this is maybe reasonable using only single features may not be.",
                    "label": 0
                },
                {
                    "sent": "Enough to to give you information on separating data, but by combining like 2 features, then let me provide you more information to separate data.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have done amazing function.",
                    "label": 0
                },
                {
                    "sent": "Learn the second thing is that we must allow training errors 'cause then after the meeting it is still possible that you cannot find any any hyperplane to separate all the training instances.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we did is to introduce a slack variable.",
                    "label": 0
                },
                {
                    "sent": "So we minus a negative value here.",
                    "label": 0
                },
                {
                    "sent": "So this is this one is now negative, so why is not negative?",
                    "label": 0
                },
                {
                    "sent": "Well, if it is negative, say for example if you have 1 -- 3 -- 5, so this becomes 66 is even greater than one.",
                    "label": 0
                },
                {
                    "sent": "So positive purpose of doing that so well.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is we will realize the left hand side maybe left maybe less than one.",
                    "label": 0
                },
                {
                    "sent": "So then, even if inside it, the best you can get, it's only minus five.",
                    "label": 0
                },
                {
                    "sent": "Then you can never modify minus.",
                    "label": 0
                },
                {
                    "sent": "Five is not smaller than is not greater than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so to make this feasible, then you have to introduce a variable here.",
                    "label": 0
                },
                {
                    "sent": "So if this is minus 5, then you do 1 -- 6 OK. Then the inequality holds.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "By introducing those select variables.",
                    "label": 0
                },
                {
                    "sent": "Then the optimization problem is guaranteed to be feasible.",
                    "label": 0
                },
                {
                    "sent": "So you can always find out WB and also can see so that those inequalities are satisfied.",
                    "label": 0
                },
                {
                    "sent": "Well, but you don't want to have too many training errors.",
                    "label": 0
                },
                {
                    "sent": "Is the original idea is still to separate as many training instances as possible.",
                    "label": 0
                },
                {
                    "sent": "So we introduce an additional term in the objective function so.",
                    "label": 0
                },
                {
                    "sent": "We some loss select variables and the plus so called penalty parameters.",
                    "label": 0
                },
                {
                    "sent": "Busy if you have a really large see here then 'cause you are doing minimization of both terms.",
                    "label": 0
                },
                {
                    "sent": "So this submission should be should be as small as possible there for quite a few select variables they will be going back to zero.",
                    "label": 0
                },
                {
                    "sent": "Well if if this variable goes back to zero that means this training instance is correctly separated.",
                    "label": 0
                },
                {
                    "sent": "So by introducing this circle.",
                    "label": 0
                },
                {
                    "sent": "The term then most training instances may still be correctly classified, so that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So now we have shown the basically the standard form of support vector machines.",
                    "label": 0
                },
                {
                    "sent": "OK, so this looks OK and it is quite simple but but we have problems to find out the decision function.",
                    "label": 0
                },
                {
                    "sent": "So to every decision function you need to solve this optimization problem and then you you need.",
                    "label": 0
                },
                {
                    "sent": "That means you need to find out in particular WMV.",
                    "label": 0
                },
                {
                    "sent": "Once you have W and then you have the decision function.",
                    "label": 0
                },
                {
                    "sent": "Try to guess what's the value of the constant C. Oh, you have to decide that.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not an easy, easy thing, so we will will discuss that issue later.",
                    "label": 0
                },
                {
                    "sent": "But the problem is I can't see who is correct version.",
                    "label": 0
                },
                {
                    "sent": "OK, so really.",
                    "label": 0
                },
                {
                    "sent": "So the system isn't so good.",
                    "label": 0
                },
                {
                    "sent": "OK so so now yeah, so deciding this season issue and not only learn how to decide this mapping function?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's another issue, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it's not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "At this moment we try to solve the optimization problem first.",
                    "label": 0
                },
                {
                    "sent": "Well, there's an immediate difficulty here.",
                    "label": 0
                },
                {
                    "sent": "This value is a variable of the optimization problem, and it may be a very long vector variable is you are going.",
                    "label": 0
                },
                {
                    "sent": "You may have an infinite number of variables in your optimization problem if you're moving function makes data to a very very high dimensional space even to an infinite dimensional space, then you are going to have infinite infinitely many variables for this study.",
                    "label": 0
                },
                {
                    "sent": "So this is this is going to be very difficult as it is usually difficult to solve an infinite optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So the tree.",
                    "label": 0
                },
                {
                    "sent": "Glad most people are doing right now, is to derive something called the deal optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We later we will we really derive this?",
                    "label": 0
                },
                {
                    "sent": "But now let's just under try to understand what this this optimization problem is.",
                    "label": 0
                },
                {
                    "sent": "It is well, of course, this problem is closely related to.",
                    "label": 0
                },
                {
                    "sent": "To the standard SVM problem.",
                    "label": 0
                },
                {
                    "sent": "Now we call this one primal problem and using optimization theory with the Rep, something called the dual problem.",
                    "label": 1
                },
                {
                    "sent": "So this is again a quadratic optimization problem, so the variable is Alpha.",
                    "label": 0
                },
                {
                    "sent": "This other is a vector variable.",
                    "label": 0
                },
                {
                    "sent": "Lenses L, so remember L is the number of training instances.",
                    "label": 0
                },
                {
                    "sent": "So the good thing here is the number of variables is finite.",
                    "label": 0
                },
                {
                    "sent": "Number of variables is the same as the number of training instances.",
                    "label": 0
                },
                {
                    "sent": "So instead of using an infinitely many variables.",
                    "label": 0
                },
                {
                    "sent": "So let's check the objective function of this deal problem.",
                    "label": 0
                },
                {
                    "sent": "This is a quadratic term.",
                    "label": 0
                },
                {
                    "sent": "We have our transpose QR&Q is the square matrix.",
                    "label": 0
                },
                {
                    "sent": "More precisely, and L by L square matrix, and we saw a component to be to be this.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the inner product between the ice training instance and the Jays training instance, and then also multiply their class labels where yyj are either plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So this is a quadratic term.",
                    "label": 0
                },
                {
                    "sent": "Then there's also linear turn we minus Y transpose Alpha or what is victory?",
                    "label": 0
                },
                {
                    "sent": "Either vector of all ones, so minus the transpose of a mix minus the summation of sort of I.",
                    "label": 0
                },
                {
                    "sent": "Then there are a bunch of constraints on Alpha, so I must be between zero and the sea.",
                    "label": 0
                },
                {
                    "sent": "Will remember C is the penalty term introduced.",
                    "label": 0
                },
                {
                    "sent": "Previous slide here.",
                    "label": 0
                },
                {
                    "sent": "So I must be between zero and a C letter is also a linear constraint that this indicator vector Y transpose Alpha must be 0.",
                    "label": 0
                },
                {
                    "sent": "So under certain constraints, we minimize a quadratic function, so if we can solve this dual problem and obtain Alpha, then there's a relationship saying that the optimal adaptive of the primal problem.",
                    "label": 0
                },
                {
                    "sent": "So that means the original SVM problem is actually a linear combination of training instances.",
                    "label": 0
                },
                {
                    "sent": "So you can see if you have Alpha and you also have wife and let you also have lamenting vectors here ever and why they are scalars.",
                    "label": 0
                },
                {
                    "sent": "So this summation is a linear combination of training instances.",
                    "label": 0
                },
                {
                    "sent": "So for optimization theory we know that W is actually a linear combination of training instances.",
                    "label": 0
                },
                {
                    "sent": "This means if we can solve the dual problem, then using this formulation we can obtain the primal optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Looks good, but we need to handle this first before solving a dual problem.",
                    "label": 0
                },
                {
                    "sent": "You must be able to write down the problem.",
                    "label": 0
                },
                {
                    "sent": "Acueza is Ringer product, but remember I said that this may be an infinite vector.",
                    "label": 0
                },
                {
                    "sent": "So how to do an infinite dimensional inner product may be very difficult so you don't even get QA.",
                    "label": 0
                },
                {
                    "sent": "Then of course you cannot sell.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The dual problem.",
                    "label": 0
                },
                {
                    "sent": "So SVM use kernel tricks.",
                    "label": 1
                },
                {
                    "sent": "Here's some very special mapping functions so that this inner product can be easily calculated, so that's the main idea.",
                    "label": 0
                },
                {
                    "sent": "So you don't use general infinite vectors.",
                    "label": 0
                },
                {
                    "sent": "Use some very special ones.",
                    "label": 0
                },
                {
                    "sent": "So even if they are infinite vectors, as long as we can get a closed form, then this QA can be easy, easily calculated.",
                    "label": 1
                },
                {
                    "sent": "So here is 1 example.",
                    "label": 0
                },
                {
                    "sent": "So I have a well, this is the same as the one that I gave earlier.",
                    "label": 0
                },
                {
                    "sent": "So now you will understand why we introduce square root Square Root 2 here.",
                    "label": 0
                },
                {
                    "sent": "This looks a little bit weird, but because of such spatial things then the inner product can be easily calculated.",
                    "label": 0
                },
                {
                    "sent": "So let's check that.",
                    "label": 0
                },
                {
                    "sent": "So if you are mapping function is like this.",
                    "label": 0
                },
                {
                    "sent": "This is from a 3 dimensional space to attend dimensional space.",
                    "label": 0
                },
                {
                    "sent": "If you do inner product then you have to do 10 multiple occasions and then to sum all those values.",
                    "label": 0
                },
                {
                    "sent": "Then there's no additions.",
                    "label": 0
                },
                {
                    "sent": "So you need 19 operations.",
                    "label": 0
                },
                {
                    "sent": "It's basically we can show that this inner product is the same as the inner product in the original space, so you calculate only XI transports XJ first, then plus one and two squared operation.",
                    "label": 0
                },
                {
                    "sent": "So you only need this is in a 3 dimensional space, so you need a three multiplications, two additions and 1 addition and one multiplication.",
                    "label": 0
                },
                {
                    "sent": "So the number of operations is much smaller than doing a direct inner product, so that's the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So we only need so we define something called Colonel.",
                    "label": 0
                },
                {
                    "sent": "So instead of writing this mapping function, we write a function of the inner product, usually code.",
                    "label": 0
                },
                {
                    "sent": "This kernel function K of XY and X&Y are two training or testing instances.",
                    "label": 0
                },
                {
                    "sent": "There's several common kernels, so that means if you write down this this kernel function, it can actually be written as the inner product of two vectors.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "This is called radial basis function kernel or Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "This is a very commonly used kernel.",
                    "label": 1
                },
                {
                    "sent": "This one is also very frequently used.",
                    "label": 0
                },
                {
                    "sent": "That's polynomial kernel, so you have inner product in the original space, then divided by something plus something 2 to the degree of the.",
                    "label": 0
                },
                {
                    "sent": "For our example, is actually a special case of polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "So so the whole idea is we use on very special mapping function so that the inner product.",
                    "label": 0
                },
                {
                    "sent": "Can be easily calculated, but if you don't believe that this is an inner product of two vectors, we must look at a very simple situation.",
                    "label": 0
                },
                {
                    "sent": "Busy hold I'm claiming here is that the this obvious kernel is actually the inner product between.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So infinite vectors.",
                    "label": 0
                },
                {
                    "sent": "It actually, even if your original training, training or testing data is in only one dimensional space, it actually made it through an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So let's do some calculation.",
                    "label": 0
                },
                {
                    "sent": "So now let's assume that all those training producing data instances they are in 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is the formulation of this RBF kernel because it is 1 dimensional, so we can remove this no notation, so we actually calculate XI minus X ^2.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "So there is 110 related to both XI and XJ, but then two other terms are either related to XI only or XJ only.",
                    "label": 0
                },
                {
                    "sent": "So then we can.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Overflows exciting extra terms full of beginning, then by Taylor expansion of this XI XJ turn, then we get the summation of infinite components, and each term is actually a polynomial of XI times XJ.",
                    "label": 0
                },
                {
                    "sent": "So you can separate each term into something related to only two XI and times something related.",
                    "label": 0
                },
                {
                    "sent": "So if you will, let's look at this so you have XI squared.",
                    "label": 0
                },
                {
                    "sent": "You also have extra square.",
                    "label": 0
                },
                {
                    "sent": "Then so this is so from this you already see this is an inner product of two infinite dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "So if you write your mapping function to be like this and you multiply, then that's exactly this one.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple way to show you why we're really moving data to an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a parameter gamma here.",
                    "label": 0
                },
                {
                    "sent": "Now we call these kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "Or a B or D gritty layer.",
                    "label": 0
                },
                {
                    "sent": "Kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "So different kernel parameters correspond to different mapping functions, so similar to that you have to decide the penalty parameters C. Then you also have to decide the kernel parameter parameters, or a B&D, and so that's always an issue.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's talk more about kernels.",
                    "label": 1
                },
                {
                    "sent": "So now you may have equation.",
                    "label": 0
                },
                {
                    "sent": "That is, it really useful to make the data to a high dimensional space?",
                    "label": 0
                },
                {
                    "sent": "Yeah, how, how?",
                    "label": 0
                },
                {
                    "sent": "How could I convince you that by moving data to a bigger space then data can really be separated?",
                    "label": 0
                },
                {
                    "sent": "There are some theoretical results proving that will improbability that in a higher spaceland probability data can be separated is bigger.",
                    "label": 0
                },
                {
                    "sent": "Well there are such results.",
                    "label": 0
                },
                {
                    "sent": "Another thing I can tell you here is.",
                    "label": 0
                },
                {
                    "sent": "That kernels may may help to separate training data.",
                    "label": 1
                },
                {
                    "sent": "Business CDs simple situation in another space for any and independent vectors then actually are linearly separable.",
                    "label": 0
                },
                {
                    "sent": "But why is that?",
                    "label": 0
                },
                {
                    "sent": "Where you just you just solve linear equation so you have suppose you have L training instances and they are linearly independent.",
                    "label": 0
                },
                {
                    "sent": "No, not at all.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, I shouldn't say this is our, so let's say I am.",
                    "label": 0
                },
                {
                    "sent": "So suppose I have.",
                    "label": 0
                },
                {
                    "sent": "I have mapped each training instance toward two and L dimensional space to enable dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So let's consider in the eye or space any error independent vectors and I have L training instances, so I actually have a square matrix here.",
                    "label": 0
                },
                {
                    "sent": "So I write the square matrix here.",
                    "label": 0
                },
                {
                    "sent": "Then I solve a simple linear equation using this square matrix due to independence.",
                    "label": 0
                },
                {
                    "sent": "This measure is easy invertible, so I can always, so I just saw this and I put the right hand side to be plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "I mean, depending on whether this data is in the 1st or the 2nd class, so I'm very sure to separate the training instances.",
                    "label": 1
                },
                {
                    "sent": "OK, so once we have this factor, this is what we do now.",
                    "label": 0
                },
                {
                    "sent": "So if the if your kernel matrix is positive definite.",
                    "label": 0
                },
                {
                    "sent": "If your kernel matrix is positive definite, then you are guaranteed to separate all training instances.",
                    "label": 0
                },
                {
                    "sent": "So if you are mapping function lead you to a positive definite kernel matrix then your data can be fully separated.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me that actually may not be good, because then we want to avoid overfitting so.",
                    "label": 0
                },
                {
                    "sent": "So on the one hand we we have a we have a meeting so that.",
                    "label": 0
                },
                {
                    "sent": "In principle, training data can can all be separated them.",
                    "label": 0
                },
                {
                    "sent": "From that we use other adjustments like the parameters C or something else.",
                    "label": 0
                },
                {
                    "sent": "So we avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but the reason of mentioning you this is that.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're a Colonel is positive definite, then data can be for these positive definite.",
                    "label": 0
                },
                {
                    "sent": "You can do a trustee factorization, two LL transpose, then in a sentence you can say that you are transforming your training instances to two independent vectors in R. Well, this is not I, this is RL is now K is LILL by L square matrix and air is the number of training instances.",
                    "label": 0
                },
                {
                    "sent": "So this is one way to explain that kernels help to separate.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training data.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of issues.",
                    "label": 0
                },
                {
                    "sent": "So now you may ask, what kind of kernels should I use?",
                    "label": 1
                },
                {
                    "sent": "So or or you are asking what kind of mapping functions should I use?",
                    "label": 0
                },
                {
                    "sent": "Then another issue is what kind of functions they are valid kernels.",
                    "label": 1
                },
                {
                    "sent": "So if you are given just a function of X&Y, how do you know that it can be separated into the inner product of two vectors?",
                    "label": 0
                },
                {
                    "sent": "Then we also mentioned kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "So for the same type of kernels you have different parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually mean mean means different mapping functions.",
                    "label": 0
                },
                {
                    "sent": "So how to decide that?",
                    "label": 0
                },
                {
                    "sent": "So all those are issues that we have to handle, but we will discuss them later.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's assume that oh, now we already have a kernel function and we have not only that.",
                    "label": 0
                },
                {
                    "sent": "We have even solve the dual problem by introducing the deal, then by introducing the kernel Now we can write down this matrix Q.",
                    "label": 0
                },
                {
                    "sent": "So we really have the dual problem.",
                    "label": 0
                },
                {
                    "sent": "Well, how to solve it is another issue, but let's assume that we already solve it.",
                    "label": 0
                },
                {
                    "sent": "So we get the optimal Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if the optimal Alpha, then by the primordial relationship we also have the optimal W. Well that looks good, but you may say we still have problems.",
                    "label": 0
                },
                {
                    "sent": "If it, if it's moving function isn't, it is an infinite vector, then it is impossible to to explicitly write down the vector W right?",
                    "label": 0
                },
                {
                    "sent": "Even even if we know those coefficients are, so there's no way to write on this vector W. Well, it turns out that you don't have to do that using this relationship.",
                    "label": 0
                },
                {
                    "sent": "We put it into the decision function so we have W transpose.",
                    "label": 0
                },
                {
                    "sent": "This testing data X + B.",
                    "label": 0
                },
                {
                    "sent": "Then we put this summation for into this inner product.",
                    "label": 0
                },
                {
                    "sent": "We get the summation of actually training instance and testing instance.",
                    "label": 0
                },
                {
                    "sent": "So if we can calculate the inner product between training and testing data, so that means the kernel function then we are fine.",
                    "label": 0
                },
                {
                    "sent": "We never have to write down this vector W. The only thing we need to know is Alpha.",
                    "label": 0
                },
                {
                    "sent": "So once we have Alpha and also know how to calculate the kernel then we can get a value.",
                    "label": 0
                },
                {
                    "sent": "Then that's our decision value.",
                    "label": 0
                },
                {
                    "sent": "So this becomes our decision function using the dual dual variable Alpha.",
                    "label": 0
                },
                {
                    "sent": "Lynn listen special thing here.",
                    "label": 0
                },
                {
                    "sent": "Remembering the dual problem.",
                    "label": 0
                },
                {
                    "sent": "We say this after I must satisfy certain so-called bounded constraints that each other, each component of I must be between zero and see, and somehow it Optima very often quite a few of our I optimal fi.",
                    "label": 0
                },
                {
                    "sent": "They're actually zero that happens in certain situations.",
                    "label": 0
                },
                {
                    "sent": "So if for those after I, even if we look at the decision function, if I is 0 then we don't have to calculate the inner product between the eyes training instance and the test instance.",
                    "label": 0
                },
                {
                    "sent": "Quiet because no matter what this value is, then multiply to to 0, then you still get 0, so that links these eyes.",
                    "label": 0
                },
                {
                    "sent": "Training instance is not used indecision.",
                    "label": 0
                },
                {
                    "sent": "Because Alpha is 0, so in in doing prediction only those data with Alpha I greater than zero, they are really used.",
                    "label": 0
                },
                {
                    "sent": "So those data are called support vectors.",
                    "label": 0
                },
                {
                    "sent": "That's how the name of this method comes from.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this figure shows hot support vectors are so essentially after solving the SVM dual problem you get a subset of your training instances.",
                    "label": 0
                },
                {
                    "sent": "There are more important and they are called support vectors.",
                    "label": 1
                },
                {
                    "sent": "Here we have two classes of training instances, where some are circles, some across Marks and I also have some red points.",
                    "label": 0
                },
                {
                    "sent": "We lose red points, maybe either circle or maybe close marks.",
                    "label": 0
                },
                {
                    "sent": "They are our support vectors.",
                    "label": 0
                },
                {
                    "sent": "This is not a linear linearly separable situation, so our decision boundary should be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So roughly it is like this.",
                    "label": 0
                },
                {
                    "sent": "The point somehow close to the decision boundary, they become support vectors.",
                    "label": 0
                },
                {
                    "sent": "So now we know why this technique is called support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have roughly shown some basic ideas of support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Will now.",
                    "label": 0
                },
                {
                    "sent": "Right now I can.",
                    "label": 0
                },
                {
                    "sent": "I can show you a simple demonstration so we have we have we have a toy toy here to to see how SVN works.",
                    "label": 0
                },
                {
                    "sent": "So this is a 3 dimensional space, so let's try to draw some points here, so use this.",
                    "label": 0
                },
                {
                    "sent": "This panel on the right hand side to give points where the way we drop points in a 3 dimensional space is to think that this is X.",
                    "label": 0
                },
                {
                    "sent": "This is XY plan and singular the Z.",
                    "label": 0
                },
                {
                    "sent": "The Z axis is perpendicular to to this plan.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I move I use mouse to move then that means I change the Z value.",
                    "label": 0
                },
                {
                    "sent": "So let's try to have some points here.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so now I so I have given some training instances.",
                    "label": 0
                },
                {
                    "sent": "Let's try to.",
                    "label": 0
                },
                {
                    "sent": "Try to have more.",
                    "label": 0
                },
                {
                    "sent": "OK, then I'm going to change to a different class.",
                    "label": 0
                },
                {
                    "sent": "So I have data.",
                    "label": 0
                },
                {
                    "sent": "In a different place.",
                    "label": 0
                },
                {
                    "sent": "I guess this is enough, so let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "So let's try to run.",
                    "label": 0
                },
                {
                    "sent": "So essentially we now we are solving the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So once we have solved you OK.",
                    "label": 0
                },
                {
                    "sent": "So it's really easy.",
                    "label": 0
                },
                {
                    "sent": "Easiest way to see how data.",
                    "label": 0
                },
                {
                    "sent": "OK, so roughly is our place.",
                    "label": 0
                },
                {
                    "sent": "So we have data over the first class we learn about in this area and also some in this area and data in the second class they are about in this region.",
                    "label": 0
                },
                {
                    "sent": "So SVN after solving the deal you actually get a nonlinear curve to separate all those training instances.",
                    "label": 0
                },
                {
                    "sent": "So this roughly gives you an idea how support vector machine works.",
                    "label": 0
                },
                {
                    "sent": "So in high dimensional space we still solve linear separating hyperplane but big.",
                    "label": 0
                },
                {
                    "sent": "To the original space, it is still a nonlinear curve.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "God here this is RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "So obviously Colonel easier, easier Colonel so that you can always overfit training data.",
                    "label": 0
                },
                {
                    "sent": "So by using this toy I mean no matter what kind of points you job, as long as you don't have two points at the same position but in different classes, then you are guaranteed to find to find the nonlinear curve so that training data correctly separated.",
                    "label": 0
                },
                {
                    "sent": "So you can easily overfit training data, but usually that's not useful.",
                    "label": 0
                },
                {
                    "sent": "That may not be useful.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so this is the web address of these.",
                    "label": 0
                },
                {
                    "sent": "Is toy program so you can try to play with it.",
                    "label": 0
                },
                {
                    "sent": "Well, there there are a couple of books about support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here at least two and one is bye bye.",
                    "label": 0
                },
                {
                    "sent": "And Alex smaller, so we may recover a lot more materials.",
                    "label": 0
                },
                {
                    "sent": "There's also.",
                    "label": 0
                },
                {
                    "sent": "There's also a website called Kernel Machine.",
                    "label": 0
                },
                {
                    "sent": "Webs have to code kernel machines and it has a blackboard.",
                    "label": 0
                },
                {
                    "sent": "But I think this is quite active discussion board, so you have any kind of questions regarding theory or how to use SVM software.",
                    "label": 0
                },
                {
                    "sent": "All those kinds of things.",
                    "label": 0
                },
                {
                    "sent": "Then you can go there.",
                    "label": 0
                },
                {
                    "sent": "Usually there are some people to help to answer your questions.",
                    "label": 0
                },
                {
                    "sent": "So so any questions so far?",
                    "label": 0
                },
                {
                    "sent": "If not, then we will.",
                    "label": 0
                },
                {
                    "sent": "We will start deriving the SVM Primulina deal problems.",
                    "label": 0
                },
                {
                    "sent": "Well maybe I can say a few things about this.",
                    "label": 0
                },
                {
                    "sent": "This is actually a small, small interesting research problem.",
                    "label": 0
                },
                {
                    "sent": "Here is how to set testing time the way how we draw this decision function.",
                    "label": 0
                },
                {
                    "sent": "This decision surface is by testing every point.",
                    "label": 0
                },
                {
                    "sent": "In the three dimensional space so.",
                    "label": 0
                },
                {
                    "sent": "So we do agreed technique point.",
                    "label": 0
                },
                {
                    "sent": "Here we check the decision value.",
                    "label": 0
                },
                {
                    "sent": "That's the way how we show this.",
                    "label": 0
                },
                {
                    "sent": "This is a linear curve 'cause you don't have the explicit form of the knowledge.",
                    "label": 0
                },
                {
                    "sent": "But once we have after you do have a nonlinear equation.",
                    "label": 0
                },
                {
                    "sent": "So how to efficiently?",
                    "label": 0
                },
                {
                    "sent": "Identify this surface and draw it.",
                    "label": 0
                },
                {
                    "sent": "There will be other.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting thing, so this is actually slightly related to how to speed up testing, but the situation is not general building in general when you do testing, you don't know those relationship between those testing instances, But here you do know you know those are.",
                    "label": 0
                },
                {
                    "sent": "Discrete points in the industry.",
                    "label": 0
                },
                {
                    "sent": "Dimensional cube come you know that so the situation is slightly different, so you may find out some special ways to speed speed up testing procedure.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we need to tell you how this deal optimization comes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bra so we need to do some calculation.",
                    "label": 0
                },
                {
                    "sent": "We consider a simple situation without the select variables.",
                    "label": 0
                },
                {
                    "sent": "So earlier in our University and formulation we have, we have a penalty term and we also have a select variable here to allow training errors.",
                    "label": 0
                },
                {
                    "sent": "But this will complicate the derivation of LTU.",
                    "label": 0
                },
                {
                    "sent": "So we consider a situation without them.",
                    "label": 0
                },
                {
                    "sent": "So this is a primal SVM problem.",
                    "label": 0
                },
                {
                    "sent": "Then the deal will be like this.",
                    "label": 0
                },
                {
                    "sent": "So the difference from the one that we mentioned earlier is that you don't have the inequality that after I is less than or equal to, see and you don't have that.",
                    "label": 0
                },
                {
                    "sent": "So now I has no upper bound and all other things are the same.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to show you is how from this problem we can derive this one and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so we're going to use something called Lagrangian deal.",
                    "label": 0
                },
                {
                    "sent": "But this is a commonly used technique in nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "More specifically, actually convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So we define something called Lagrangian function.",
                    "label": 0
                },
                {
                    "sent": "Well so W and be there original variables, so we have the objective function here and we also have inequalities or constraints.",
                    "label": 0
                },
                {
                    "sent": "So we introduce something called Lagrangian multipliers after I.",
                    "label": 0
                },
                {
                    "sent": "Well, now we call it out because in the end it is really the other eye of our dual problem.",
                    "label": 0
                },
                {
                    "sent": "So we minus the summation of I times those equalities, yeah?",
                    "label": 0
                },
                {
                    "sent": "So this is a quote.",
                    "label": 0
                },
                {
                    "sent": "The Lagrangian function, then the Lagrangian deal is defined to be like this, so this is.",
                    "label": 0
                },
                {
                    "sent": "So we maximize with respect to Alpha and then inside this parenthesis we must we minimize with respect to W&V.",
                    "label": 0
                },
                {
                    "sent": "So that's the definition of Lagrangian do, so it is it is a different optimization problem, so forth.",
                    "label": 0
                },
                {
                    "sent": "So now for from the SVN primal optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we define this Lagrangian deal.",
                    "label": 0
                },
                {
                    "sent": "So it is a different.",
                    "label": 0
                },
                {
                    "sent": "It's a different optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So for this Lagrangian deal, there are some good properties, especially this is a strong development, so strong duality says that the minimum of the primal problem is the same as this maximize the maximum of the Lagrangian deal.",
                    "label": 0
                },
                {
                    "sent": "So there are objective optimal objective values are actually the same, and that's the property.",
                    "label": 0
                },
                {
                    "sent": "But you want to be careful about this because not for every function this property holds and will go back to that later.",
                    "label": 1
                },
                {
                    "sent": "So you want to be careful not for every function you have such a property only for certain functions.",
                    "label": 0
                },
                {
                    "sent": "Of course, for our function right now it will be OK. Well, I think tomorrow in mountains talk he's going to say something about more about developing.",
                    "label": 0
                },
                {
                    "sent": "He's going to define something called conjugate function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's actually the general singing in convex optimization, so this Lagrangian deal is actually also derived from that so called conjugate function, But these are.",
                    "label": 0
                },
                {
                    "sent": "Engine deal is basically specifically for constraint optimization problem, But anyway, so now we have defined what Lagrangian deal is and also how how it looks like.",
                    "label": 0
                },
                {
                    "sent": "So the next thing we're going to do is to simplify this Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "Do try to change it to a simpler form.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why do you want to increase the number of support vectors?",
                    "label": 0
                },
                {
                    "sent": "Increase the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, well that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So essentially you are asking for the idea behind Primal and do well.",
                    "label": 0
                },
                {
                    "sent": "That's a big question.",
                    "label": 0
                },
                {
                    "sent": "You can explain prime and deal from from some economic theory actually.",
                    "label": 0
                },
                {
                    "sent": "Well, I can give you some very simple explanation here.",
                    "label": 0
                },
                {
                    "sent": "So OK, so let's.",
                    "label": 0
                },
                {
                    "sent": "Well, let's assume that our final goal is for him.",
                    "label": 0
                },
                {
                    "sent": "Somehow their objective objective values to be the same as soon as that's the goal we want.",
                    "label": 0
                },
                {
                    "sent": "Reasonable, because now we're going to find out a different problem, and this program must be related to the original 1, right?",
                    "label": 0
                },
                {
                    "sent": "So we assume that we're hoping to have this.",
                    "label": 0
                },
                {
                    "sent": "So that means somehow this value should be the same as the minimum flow prime.",
                    "label": 0
                },
                {
                    "sent": "But if we look at least Lagrangian function, so now we have the objective function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the same as the objective function over the prime and we also write downloads inequalities losing equalities.",
                    "label": 0
                },
                {
                    "sent": "So for any feasible W, so that means they satisfy inequality that this is greater equal to 0 for the primal.",
                    "label": 0
                },
                {
                    "sent": "This is this is a positive value, at least not negative.",
                    "label": 0
                },
                {
                    "sent": "This is not negative value.",
                    "label": 0
                },
                {
                    "sent": "In the midst of a, this Alpha is also not negative, so you have the multiplication of two positive values, so then you're minus something.",
                    "label": 0
                },
                {
                    "sent": "Wow, so you have the objective value minus something, so this should be smaller.",
                    "label": 0
                },
                {
                    "sent": "Should be smaller than the original objective value, right?",
                    "label": 0
                },
                {
                    "sent": "So then this is not good.",
                    "label": 0
                },
                {
                    "sent": "This violates our goal of maybe making those two things to be together.",
                    "label": 0
                },
                {
                    "sent": "So then we have no choice but to hard to maximize the whole thing somehow.",
                    "label": 0
                },
                {
                    "sent": "Try to maximize with respect to Alpha then.",
                    "label": 0
                },
                {
                    "sent": "That is more possible that by doing so, then the right hand side the Maxima can reach the minimum of the primal.",
                    "label": 0
                },
                {
                    "sent": "So essentially the explanation that I just gave is called weak duality.",
                    "label": 0
                },
                {
                    "sent": "So we can do it.",
                    "label": 0
                },
                {
                    "sent": "He says that this minimum of primal is greater equal to the maximum of this and this whole week duality.",
                    "label": 0
                },
                {
                    "sent": "So that's roughly an explanation, but this has nothing to do with support vectors when doing these primal and dual optimization.",
                    "label": 0
                },
                {
                    "sent": "People know this is nothing to do with support vector machines.",
                    "label": 0
                },
                {
                    "sent": "This is just pure quadratic optimization problem so.",
                    "label": 0
                },
                {
                    "sent": "At least from the point that point of view, we don't worry about why we are.",
                    "label": 0
                },
                {
                    "sent": "We don't worry about the meaning of support vectors, but maybe you can think more from that point of view.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can come up with a new explanation of primal deal using from the support vector machines point of view will be very good.",
                    "label": 0
                },
                {
                    "sent": "I I don't have a good answer about that right now.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's just say OK, this is the Lagrangian deal.",
                    "label": 0
                },
                {
                    "sent": "So the next thing we would like to do is to simplify the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So how to simplify it is well, this is looks like a complicated optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You have maximize and user minimize.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is to assume that to think about the situation when Alpha is fixed.",
                    "label": 0
                },
                {
                    "sent": "So let's check if Alpha is fixed then how how to solve the minimization problem inside this big parenthesis.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is another easy fixed is fixed then immediately what we can see is if summation of iy is not equal to 0 then we can meet.",
                    "label": 0
                },
                {
                    "sent": "We can make this minimization to go to minus Infinity.",
                    "label": 0
                },
                {
                    "sent": "Well, how to see that?",
                    "label": 0
                },
                {
                    "sent": "So you can see you have Alpha, IY, times be well so you can.",
                    "label": 0
                },
                {
                    "sent": "You can pull me out so this becomes B times submission of I * Y.",
                    "label": 0
                },
                {
                    "sent": "So if submission of iy is not zero and you are minimizing with respect to W&B so you can just move B to either plus or minus Infinity depending on whether this is positive or or negative.",
                    "label": 0
                },
                {
                    "sent": "Well if this is positive and just move B2 plus.",
                    "label": 0
                },
                {
                    "sent": "Infinity, then this minus, then you you get minus Infinity here.",
                    "label": 0
                },
                {
                    "sent": "So this this minimization problem with respect to WB can be rewritten as this.",
                    "label": 0
                },
                {
                    "sent": "So we have two situations.",
                    "label": 0
                },
                {
                    "sent": "So the first one is if this Y transpose Alpha mention is actually why transpose Alpha is not is not zero, then these minima has the value minus Infinity, but on the other hand if Y transpose Alpha is actually 0, then we don't need to write this term with.",
                    "label": 0
                },
                {
                    "sent": "Related to be, if you have this summation to be 0, then you can remove this term so we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have be here, we only have to minimize with respect to W. OK, so we simple Violet do a little bit.",
                    "label": 0
                },
                {
                    "sent": "Really well, actually it looks more complicated for anyway, so now so now this is the problem we have so far.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Len, then we check the second situation harder.",
                    "label": 0
                },
                {
                    "sent": "If Y transpose Alpha is 0.",
                    "label": 0
                },
                {
                    "sent": "But now we are going to minimize with respect to tablet and the least function so.",
                    "label": 0
                },
                {
                    "sent": "This function right now OK, so now is fixed, so W is the only variable.",
                    "label": 0
                },
                {
                    "sent": "It is strictly convex function of W. Well, I assume you know about convex function is or strict convex function is.",
                    "label": 0
                },
                {
                    "sent": "This function is strictly convex because of this quadratic term W transport server.",
                    "label": 1
                },
                {
                    "sent": "So from optimization theory we know that the optimal happens when.",
                    "label": 1
                },
                {
                    "sent": "Partial derivative with respect to WE 0 so we do this so then well, so we do so for derivative of W transpose W that's actually W, and then for this part we actually remove W so we have W is equal to the linear combination of training instances.",
                    "label": 0
                },
                {
                    "sent": "So now we have list, so that means if Alpha is fixed and why transfers are but is 0, then this optimal damn it must be.",
                    "label": 0
                },
                {
                    "sent": "Linear combination of training data using this Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we can further simplify our deal, Prob.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have the form of W in terms of Alpha, right?",
                    "label": 0
                },
                {
                    "sent": "So we have so we can represent the W to be a linear combination, so using the Alpha.",
                    "label": 0
                },
                {
                    "sent": "But we want to remove remove W when the reason is we want to simplify this so we don't.",
                    "label": 0
                },
                {
                    "sent": "We hope that we don't have this minimization, so we hope to remove variables W&V.",
                    "label": 0
                },
                {
                    "sent": "In this formulation.",
                    "label": 0
                },
                {
                    "sent": "So we put this W back to this objective function so we have to do some calculation.",
                    "label": 0
                },
                {
                    "sent": "So this chick W transport stopping first.",
                    "label": 0
                },
                {
                    "sent": "So now we have W to be this linear combination, transpose to this linear combination will essentially see summation IJ of.",
                    "label": 0
                },
                {
                    "sent": "XI transfers XJ well, so this is this is already very close to the quadratic term of our deal problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have reached our transpose, Cuba, so in our earlier formulation we use vector and matrix formulation instead of summation, so this is already Alpha transpose QR.",
                    "label": 0
                },
                {
                    "sent": "But then there's also linear turn, right?",
                    "label": 0
                },
                {
                    "sent": "There's a linear term here.",
                    "label": 0
                },
                {
                    "sent": "We also put the W. Into this, but if you look at this so if you if you move W in into this position, if we moved up here.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you have is this is I mentioned our five Yr XI watch list.",
                    "label": 0
                },
                {
                    "sent": "This is another W so once you have this relationship, essentially you are minus minus.",
                    "label": 0
                },
                {
                    "sent": "So you do minus W transpose W here so you have held W transpose minus the whole W transpose W. So how do you have is minus you have minus health W transport study?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is the new quadratic term, but we also have a linear term that's this.",
                    "label": 0
                },
                {
                    "sent": "One is minus one and minus I mentioned Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we have a summation Alpha here.",
                    "label": 0
                },
                {
                    "sent": "So minus minus becomes puffs.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have this submission of Alpha I minus something.",
                    "label": 0
                },
                {
                    "sent": "So this is already very close to our dual optimization problem is remember we are doing maximization here.",
                    "label": 0
                },
                {
                    "sent": "So if you reverse it to minimization then that's already have times of our transpose QR minus some Asian up.",
                    "label": 0
                },
                {
                    "sent": "I was already our objective function of the deal that we mentioned earlier, so we're almost there.",
                    "label": 0
                },
                {
                    "sent": "So we still have two situations that whether if why transfers are easier or if why transfer server is not zero.",
                    "label": 0
                },
                {
                    "sent": "Then the tricky thing of doing this maximization is when you are doing maximization right.",
                    "label": 0
                },
                {
                    "sent": "So then you can see their own different kinds of Alpha then for certain Alpha they give you objective value minus Infinity.",
                    "label": 0
                },
                {
                    "sent": "But you are doing maximization, so of course minus in.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it can never be your optimal objective value.",
                    "label": 0
                },
                {
                    "sent": "There's now we're doing this Lagrangian deal, so we're doing maximization.",
                    "label": 0
                },
                {
                    "sent": "So of course, minus Infinity.",
                    "label": 0
                },
                {
                    "sent": "Definitely not Maxima of the deal.",
                    "label": 1
                },
                {
                    "sent": "So that means fewer optimal solution does not happen when this summer.",
                    "label": 1
                },
                {
                    "sent": "This why transports are but is not zero.",
                    "label": 0
                },
                {
                    "sent": "Therefore we only have two.",
                    "label": 0
                },
                {
                    "sent": "So we don't have to consider this situation at all.",
                    "label": 0
                },
                {
                    "sent": "We don't have to consider this situation.",
                    "label": 0
                },
                {
                    "sent": "We only have to consider the first situation.",
                    "label": 0
                },
                {
                    "sent": "Will we only consider the first decision?",
                    "label": 0
                },
                {
                    "sent": "This becomes a kind of conditions on Alpha, so that becomes a constraint of the deal.",
                    "label": 0
                },
                {
                    "sent": "So we move this to be a constraint of this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So so writing this and also after greater or equal to 0, then we have the dual constraints and then we have this maximization.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the same as the object of the objective function that we mentioned early, so this is already the deal that we gave.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "The typical way of deriving the dual problem with one thing I want to mention here is.",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at some papers they don't use really exact will use exactly the same way as I am using here.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "To explain to explain that summit why in the dual problem this why transfers after should be zero will some people did this?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they write the Lagrangian function.",
                    "label": 0
                },
                {
                    "sent": "OK, then they know that they're going to minimize with respect to W&B, so they do partial derivative with respect to W. And also be yeah, well, just like we do, we do W here so they.",
                    "label": 0
                },
                {
                    "sent": "They just check the derivative with respect to VPSB is a linear term visa.",
                    "label": 0
                },
                {
                    "sent": "This visa this is a linear turn of me, so if you do partial derivative when you get some mention of iy and they just say oh let's put it to be zero, well that's not very right.",
                    "label": 0
                },
                {
                    "sent": "If you're following this this this this derivation you cannot do that.",
                    "label": 0
                },
                {
                    "sent": "The reason is because.",
                    "label": 0
                },
                {
                    "sent": "Because of this, I mean if you check this, this is not a zero, then you can have minus Infinity and later we have like minus Infinity is is not going to be the optimal for the deal, therefore it is not considered.",
                    "label": 0
                },
                {
                    "sent": "Just do partial derivative.",
                    "label": 0
                },
                {
                    "sent": "You need to have a strictly convex function of the variable, but our objective function is only strictly strictly convex on W but not on be.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear turn of me, it is not a strictly convex term.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in this derivation, usually if you see some people they they just do partial derivative with respect to be, well, that is not very very correct way.",
                    "label": 0
                },
                {
                    "sent": "Then I want to say a little bit more about your problems.",
                    "label": 0
                },
                {
                    "sent": "So after SVM is a popular technique, so quite a few people, they just think that for any optimization problem Miss Lagrangian do exists and also strong duality holds.",
                    "label": 1
                },
                {
                    "sent": "Well, this is actually wrong.",
                    "label": 0
                },
                {
                    "sent": "In optimization theory, now there's nothing like that.",
                    "label": 0
                },
                {
                    "sent": "We only have series saying that if this optimization problem is a convex problem, so convex programming means the objective function is convex and also the physical region.",
                    "label": 0
                },
                {
                    "sent": "OK, physical range means the set of points satisfying all the constraints, so this set is also is also convex.",
                    "label": 0
                },
                {
                    "sent": "Then we have a convex programming problem and this is not enough.",
                    "label": 0
                },
                {
                    "sent": "There's something called constraint qualification, so your constraints must satisfy certain conditions only for this kind of problems, then you have less strong duality.",
                    "label": 0
                },
                {
                    "sent": "OK, you cannot just write a new optimization problem and then just write down the Lagrangian deal and say, oh strong developing hosts.",
                    "label": 0
                },
                {
                    "sent": "No, that's not right.",
                    "label": 0
                },
                {
                    "sent": "So now we have we for our optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It satisfies all the conditions.",
                    "label": 0
                },
                {
                    "sent": "First the SVM optimization problem is convex.",
                    "label": 0
                },
                {
                    "sent": "Well, it is strictly convex on W but also convex on B and also select variables.",
                    "label": 0
                },
                {
                    "sent": "So it is convex.",
                    "label": 0
                },
                {
                    "sent": "So we are fine for the first condition.",
                    "label": 0
                },
                {
                    "sent": "And another important effect for constraint qualification is if you have so-called linear time constraints.",
                    "label": 0
                },
                {
                    "sent": "Then your constraint qualification is always satisfied, let's see.",
                    "label": 0
                },
                {
                    "sent": "And except the, let's see the primal problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the primal problem.",
                    "label": 0
                },
                {
                    "sent": "So how do we have is?",
                    "label": 0
                },
                {
                    "sent": "Like we have several linear inequality is not a variable establishment we so this is this is returned linearly in WMV.",
                    "label": 0
                },
                {
                    "sent": "For this kind of inequalities then constraint qualification is always satisfied, so you don't have to check to check that.",
                    "label": 0
                },
                {
                    "sent": "So you need to be more careful if you have nonlinear constraints, then you want to be careful on using the Lagrangian duality.",
                    "label": 0
                },
                {
                    "sent": "But now we are fine for this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "But the problem is.",
                    "label": 0
                },
                {
                    "sent": "You usually.",
                    "label": 0
                },
                {
                    "sent": "So when I review some papers then yes some papers late, they just write the new formulation.",
                    "label": 0
                },
                {
                    "sent": "Then they say OK. Now Granger, interior and even the problem is not convex.",
                    "label": 0
                },
                {
                    "sent": "But then I don't know if that's right or or correct.",
                    "label": 0
                },
                {
                    "sent": "The theory says if convex or certain conditions then strong duality holds.",
                    "label": 0
                },
                {
                    "sent": "Maybe for some certain non convex problems we also have strong duality, we just don't know.",
                    "label": 0
                },
                {
                    "sent": "So it is for this kind of work then I would always have problems that I need to find a counterexample or things like that.",
                    "label": 0
                },
                {
                    "sent": "That's not very good, so usually you are yeah so.",
                    "label": 0
                },
                {
                    "sent": "So if you want to remember this when you when doing the using the duality, you need to check certain.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, now you may have another problem that you may have another question.",
                    "label": 0
                },
                {
                    "sent": "Our optimization problem is an infinite programming problem, right?",
                    "label": 0
                },
                {
                    "sent": "Well yes, because we have mapped data to a high dimensional space, we have infinitely many variables.",
                    "label": 0
                },
                {
                    "sent": "So now you may say who does the Lagrangian duality still hold for this kind of space?",
                    "label": 0
                },
                {
                    "sent": "But the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "The Lagrangian duality theory can hold for yeah for Hilbert space or even banner has space for loss.",
                    "label": 0
                },
                {
                    "sent": "In those spaces, it is still right so so we can do that.",
                    "label": 0
                },
                {
                    "sent": "Basically we can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a paper doing a rigorous discussion on this issue.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so this one is fine.",
                    "label": 0
                },
                {
                    "sent": "We also so now we have finished discussing primal and dual optimization problems, so any questions?",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Play conveyor.",
                    "label": 0
                },
                {
                    "sent": "An example for Lavinia concerns.",
                    "label": 0
                },
                {
                    "sent": "No dimension.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear constraints then you have to be careful on using Lagrangian duality, and that's that's what I want to tell you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you want to be a little bit careful you to really check something called constraint qualification.",
                    "label": 0
                },
                {
                    "sent": "But you you have to check.",
                    "label": 0
                },
                {
                    "sent": "Well I I don't have time to mention those things here, but you just go to get any nonlinear programming books then.",
                    "label": 0
                },
                {
                    "sent": "Go to check the chapter of Lagrangian duality and then we will talk about something called constraint qualification and there are different types of constraint qualifications, so it's possible that if you have certain nonlinear conditions nonlinear constraints then.",
                    "label": 0
                },
                {
                    "sent": "Even satisfy one of constraint qualifications, then you are OK.",
                    "label": 0
                },
                {
                    "sent": "So optimization researchers may develop different types of constraint qualification.",
                    "label": 0
                },
                {
                    "sent": "So it's like different kinds of kinds of tests.",
                    "label": 0
                },
                {
                    "sent": "So we check whether your past trends satisfy one of them.",
                    "label": 1
                },
                {
                    "sent": "If that's the case, then you are fine for doing for using Lagrangian duality nicely.",
                    "label": 0
                },
                {
                    "sent": "The idea.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So now we have the.",
                    "label": 0
                },
                {
                    "sent": "This is standard from the format of certain information for visiting.",
                    "label": 0
                },
                {
                    "sent": "I didn't understand what is the need of dual automation.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do?",
                    "label": 1
                },
                {
                    "sent": "The reason is simple, because we don't want to solve an infinite dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "So the primal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so this is the primal problem.",
                    "label": 0
                },
                {
                    "sent": "So you have an infinite vector here, so you're going to have an infinite dimensional vector vector variables.",
                    "label": 0
                },
                {
                    "sent": "We don't want to deal with such a situation, so solving a deal may be easy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so that's a computational issue.",
                    "label": 0
                },
                {
                    "sent": "If you solve the deal then then it is easy.",
                    "label": 0
                },
                {
                    "sent": "So of course I later I will tell you how right now how we solve with you.",
                    "label": 0
                },
                {
                    "sent": "If you problem is very small then just use any optimization package and you can solve the deal.",
                    "label": 0
                },
                {
                    "sent": "And that's another reason.",
                    "label": 0
                },
                {
                    "sent": "So any more questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK so finally I see.",
                    "label": 0
                },
                {
                    "sent": "Where did the eaters go between the primal and dual problems?",
                    "label": 0
                },
                {
                    "sent": "Even if we have selected area, you mean if we have select variables?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "First part is simply disappear.",
                    "label": 0
                },
                {
                    "sent": "You could just see the CPT.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, because I seem for OK.",
                    "label": 0
                },
                {
                    "sent": "So I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't include the penalty term here.",
                    "label": 0
                },
                {
                    "sent": "Well, that's just for for simplifying the derivation of the deal so it will be.",
                    "label": 0
                },
                {
                    "sent": "It will be a good exercise if you add that back and following the same procedure then you will get the deal that we used earlier.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about the first part of it, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah so yeah.",
                    "label": 0
                },
                {
                    "sent": "When you present individual with this constant, yes, let's see here.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So even a primal problem.",
                    "label": 0
                },
                {
                    "sent": "You have this penalty term if you know primal problem you have this penalty term then.",
                    "label": 0
                },
                {
                    "sent": "Do you have this upper bound?",
                    "label": 0
                },
                {
                    "sent": "See here, but if you live in a primal you don't have this term, then in a deal you don't have this upper bound.",
                    "label": 0
                },
                {
                    "sent": "Leslie situation.",
                    "label": 0
                },
                {
                    "sent": "What does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "It does go between the panel and OK right now, so the same question is where?",
                    "label": 0
                },
                {
                    "sent": "To be late, disappear right so from the primal.",
                    "label": 0
                },
                {
                    "sent": "So in a primer we have variable WMV, but then they disappear in a dual function.",
                    "label": 0
                },
                {
                    "sent": "Well, that's the same.",
                    "label": 0
                },
                {
                    "sent": "So the same apply applies to select variables.",
                    "label": 0
                },
                {
                    "sent": "Now in this primal formulation actually WB and also can see they are all considered variables that are in the same position.",
                    "label": 0
                },
                {
                    "sent": "Then when deriving the deal they disappear together.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll probably hope.",
                    "label": 0
                },
                {
                    "sent": "Well maybe I think yeah.",
                    "label": 0
                },
                {
                    "sent": "How can we be better represented?",
                    "label": 0
                },
                {
                    "sent": "How can?",
                    "label": 0
                },
                {
                    "sent": "Copy.",
                    "label": 0
                },
                {
                    "sent": "Possibility is that with representative yeah you missed this term.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so hard question about this time.",
                    "label": 0
                },
                {
                    "sent": "We should be also the.",
                    "label": 0
                },
                {
                    "sent": "NATO.",
                    "label": 0
                },
                {
                    "sent": "B should be high dimensional while as a crazy idea I never thought about that.",
                    "label": 0
                },
                {
                    "sent": "OK, we all you can do that, maybe I don't know, but this is already complicated enough to make data space.",
                    "label": 0
                },
                {
                    "sent": "Well, I think you can do that.",
                    "label": 0
                },
                {
                    "sent": "Scott, yeah yeah you can.",
                    "label": 0
                },
                {
                    "sent": "You can do.",
                    "label": 0
                },
                {
                    "sent": "You may do that OK?",
                    "label": 0
                },
                {
                    "sent": "OK, OK, you mean OK, so that's a more reasonable question.",
                    "label": 0
                },
                {
                    "sent": "With instead of making the.",
                    "label": 0
                },
                {
                    "sent": "So how to obtain?",
                    "label": 0
                },
                {
                    "sent": "Be?",
                    "label": 0
                },
                {
                    "sent": "Yes, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Because I didn't say that.",
                    "label": 0
                },
                {
                    "sent": "Do you see how to have W?",
                    "label": 0
                },
                {
                    "sent": "I say, oh, if you have the optimal deal then you are going to get the optimal doubling, but didn't say how to get this week.",
                    "label": 0
                },
                {
                    "sent": "So now you know.",
                    "label": 0
                },
                {
                    "sent": "I somehow I skipped that because it is not easy to say how to obtain it, so because it is not easy.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I can personally tell you how to do that, but I don't think we have time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not easy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you need some certain properties.",
                    "label": 0
                },
                {
                    "sent": "You need certain properties.",
                    "label": 0
                },
                {
                    "sent": "We let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I can have a geometric interpretation here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Hold on, hold on, let me finish it at first.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a property here that for for for such a point because it satisfied W transpose X I + V = -- 1.",
                    "label": 0
                },
                {
                    "sent": "So this is the way to explain that.",
                    "label": 0
                },
                {
                    "sent": "So for this point if we know it is on this straight line, so we know it satisfies W transpose XI.",
                    "label": 0
                },
                {
                    "sent": "Suppose this is XI plus B = + 1.",
                    "label": 0
                },
                {
                    "sent": "And if we know W and we know XI, we know plus one, then we know we so roughly, that's one way to calculate me.",
                    "label": 0
                },
                {
                    "sent": "But in practice we need something else.",
                    "label": 0
                },
                {
                    "sent": "So, so here's another question.",
                    "label": 0
                },
                {
                    "sent": "Here Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Set up an E on the hyper pray that it cannot be classified.",
                    "label": 0
                },
                {
                    "sent": "You mean it is test data is still is wrongly classified or something?",
                    "label": 0
                },
                {
                    "sent": "Of course you can.",
                    "label": 0
                },
                {
                    "sent": "Then you can never achieve 100% testing accuracy, but you don't have test data so you can 100% separate training instances but not for testing.",
                    "label": 0
                },
                {
                    "sent": "But we can make it to a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "That's OK. Then we do testing there.",
                    "label": 0
                },
                {
                    "sent": "Well, so I think it's better to have a break, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}