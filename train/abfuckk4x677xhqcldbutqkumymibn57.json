{
    "id": "abfuckk4x677xhqcldbutqkumymibn57",
    "title": "Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost",
    "info": {
        "author": [
            "Thomas Mensink, INRIA Grenoble Rh\u00f4ne-Alpes"
        ],
        "chairman": [
            "Michal Irani, Weizmann Institute of Science",
            "Andrea Vedaldi, Department of Engineering Science, University of Oxford"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/eccv2012_mensink_learning/",
    "segmentation": [
        [
            "I'm pleased to present my paper metric learning for large scale image classification, generalizing to new classes at near 0 cost.",
            "This is joint work with Yacob for bake fluent Petronet in Gabriela to Gram."
        ],
        [
            "The main motivation for our work is that real life datasets are evolving overtime, for example at Flickr, Twitter Facebook and Amazon.",
            "New images or items or tweets are edit every second.",
            "When new labels, tags, and products are incorporated overtime, there is a need to index to retrieve to certain to categorize for those new items and those new classes.",
            "Therefore we are interested in methods for large scale visual classification where we can.",
            "Add new images and new classes at near near 0 cost and on the fly."
        ],
        [
            "The outline of my talk will be as follows.",
            "I will first give a brief introduction and then detailed distance based classifiers before introducing our main contribution of the paper, which is on the about a new metric learning approach for the nearest class mean classifier at then will represent our experimental evaluation and finally our conclusions."
        ],
        [
            "In our field there has been a recent focus on large scale image classification which is emphasized by the introduction of the image net data set with currently holds over 14,000,000 images an over 20,000 different classes.",
            "The Imagenet data set has been an important benchmarking tool ever since its introduction to good high quality performance on this large scale data set.",
            "Usually high dimensional image features are used such as the Super factor encoding on the Fisher vector encoding and we train one versus rest.",
            "SVM classifier using stochastic gradient descent.",
            "However."
        ],
        [
            "When used on opening the datasets, there are limitations on the 1 first rest SVM framework.",
            "For example, when a new image arrives, training has to continue, but also for a new class.",
            "Training usually start from scratch.",
            "To alleviate the letter transfer, learning is used to combine class representations from existing classes to obtain a good starting point for a new class.",
            "However, it also still requires a nontrivial amount of training."
        ],
        [
            "Distance based classifiers offer an alternative to one versus rest SVM classifiers."
        ],
        [
            "Overcome some of those problems.",
            "The most classic Hall distance based classifier is probably the K nearest neighbor method, where we assign an image I to the most common class among the K closest images from the training set.",
            "That features of this model is that it's a very flexible model and highly non linear decision boundary.",
            "It's easier to integrate new images or new classes, but just adding it to the database.",
            "However, it's very expensive at this time, especially for the high dimensional image features we consider."
        ],
        [
            "Since we."
        ],
        [
            "Need a proper distance function to obtain high quality performance.",
            "We use large margin nearest neighbors from Lineberger at all."
        ],
        [
            "The older distance based classification classification method we consider is the nearest class mean classifier, where for each class we compute it mean and then we assign an image I to the class is the closest class mean.",
            "Adventures of this model is that's very false at Test time, since it results in a linear model and also in this model it's very easy to integrate new images or new."
        ],
        [
            "Classes were just updating or adding a new mean.",
            "However, since images of classes are only really presented with their mean, it might not be flexible enough for competitive performance."
        ],
        [
            "Therefore, in this paper we introduce a maximum likelihood approach for metric learning in the nearest class mean classifier."
        ],
        [
            "Mr First, before I introduce our new method, let's quickly review what official discriminants might be suboptimal in this very easy example of three not linear separable classes.",
            "The classical fish."
        ],
        [
            "Discriminants.",
            "Will try to maximize the variance between all class means.",
            "The Fisher discriminate direction I depicted here in Gray will in the projection space, the blue and the green class are collapsed, which is clearly suboptimal."
        ],
        [
            "Our approach we introduce also trying to maximize the variance between nearby class means.",
            "However, only between the class means which are easily confused.",
            "The direction we get.",
            "And separates the three classes still reasonably well in the 1 dimensional space."
        ],
        [
            "In order to do so, we learn a low rank MENA hobus distance.",
            "Defined by the projection matrix W, the low rank projection matrix W allows us to have a controllable number of parameters, and it also allows to compress the high dimensional image feature to a lower dimension and compute distances in this lower dimensional space."
        ],
        [
            "For the metric learning the first reformulate millionaires classman classifier into a probabilistic formulation using the softman function, where the probability of a Class C given image X is proportional to the exponent of the negative distance between the image and the class mean.",
            "This model corresponds to a class posterior in a generative model, where the class conditional model is given by a Goshen.",
            "This is shared covariance matrix.",
            "To learn the parameters of our projection matrix W, we have discriminative maximum likelihood training where we optimize or very maximized the likelihood of correct prediction of the images in our training set."
        ],
        [
            "To illustrate the learned distances obtained with this method, we consider the closest class means of the palm tree class according to the L2, including distance and according to our learned men hobus distance.",
            "What we observe from the L2 distance that actually all the closest class means are from unrelated concepts.",
            "They learned Monovisc distance on the other side only returns other trees as close as class means."
        ],
        [
            "We could re late the nearest class mean classifier to other linear classifiers where the score function is defined by a class specific bias Anna weight factor.",
            "If we use linear SVM, we learn the bias and a weight factor pair class separately.",
            "If we would use the recently introduced wasabi methods, it learns a low projection matrix W which is shared among all classes and pair class.",
            "It learns a low dimensional weight factor.",
            "The nearest class mean we consider in our paper only learns a shared projection matrix W and the class specific bias and weight vector are defined by the class means an the learned projection matrix."
        ],
        [
            "Now represent our experimental evaluation."
        ],
        [
            "For our experiments we have used two large scale datasets.",
            "The image net 2010 challenge datasets which contains about one or two million training images of 1000, different classes and the image net 10K data set which contains 4 and half million training images of 10,000 different classes.",
            "As image representations we use 4064 thousand Fisher vectors and we use PQ compression to reduce the memory footprint on the high dimensional features.",
            "For training rely on stochastic gradient dissent.",
            "And we apply.",
            "If you set the learning rates and early stopping to Terry and on a separate validation set."
        ],
        [
            "In his first experiment, we compared the performance of the distance based classifiers on the image.",
            "Net 2010 datasets.",
            "First, we observe that both the K nearest neighbor method and the nearest class mean method significantly improved performance when we use a learned metric over the standard Euclidean distance distance.",
            "2nd, we observe that the nearest classman classifier outperforms the more flexible K nearest neighbor classifier."
        ],
        [
            "1st, when we compare two and baseline of 1 first rest SVM's order is Abby method.",
            "We observe that the distance based methods still perform competitively."
        ],
        [
            "Given those encouraging results, we apply the nearest classman classifier on the image net 10K data sets, and we compute just the means of the 10K classes, which takes about one hour of CPU time, and we reuse the metric we have previously learned.",
            "We compared it to a baseline of one versus rest SVM's where we trained 10,000 different SVM classifiers, which takes about 280 CPU days.",
            "This means that the nearest class mean classifier is faster by a factor of eight and a half thousand."
        ],
        [
            "When we look at the performance, we observed that the features we use of 64 thousand dimension this our SVM baseline obtains state of the art results over previously published methods.",
            "We also observed that the nearest classman classifier still performs very good, especially considering this enormous speedup factor during training."
        ],
        [
            "In a final experiment, the applied the nearest classman classifier in in a transfer learning setting, where we try to obtain a zero shot prior using the image net class hierarchy.",
            "What we do is in the first tab we."
        ],
        [
            "Propagate all the means of our training images to the internal nodes upwards in the hierarchy.",
            "In a second."
        ],
        [
            "Step very first direction and to obtain a mean for a class you have not seen.",
            "At training we propagate down the image of the means of the internal nodes of ascendant nodes of the new class.",
            "To obtain the serial shot prior."
        ],
        [
            "We use is an experiment on the image.",
            "Net 2010 data set where in the first step we learn a metric on 800 classes and we evaluate the performance of this metric on $200.",
            "Class is not used during training.",
            "We compare two models.",
            "One where we use only the data mean for the for the images available for those 200 classes, which we vary from one to a couple of 1000.",
            "Compared to the zero, should prior shown in the previous slide vistas data mean per class what we observe from the results is that the zero should prior significantly improves the results.",
            "Then we have only a few number of samples per class and also that if we have more samples per class, the results keeps improving up to about hundred 2000 images per class."
        ],
        [
            "I will now present our conclusions."
        ],
        [
            "So we have shown the nearest class mean classification method and we have proposed a metric learning by maximum likelihood for the nearest Glassman classifier.",
            "We have shown it outperforms the more flexible K nearest neighbor classifier and it seems on par with SVM's.",
            "However, adventures of our nearest Glassman classifier over the standard one versus rest SVM framework is that it allows us to add new classes or new images at near 0 cost.",
            "You have shown competitive results on unseen classes in image net 2000 image.",
            "Net 10K data set and we have also shown that we can benefit from class priors obtained from the image net hierarchy for small sample sizes to further improve the results, we could extend the model and instead of using a single centroid, the mean.",
            "Because you have multiple centroids per class which is shown our technical report which is available online."
        ],
        [
            "Thank you for your attention.",
            "Questions.",
            "So you mentioned you can use multiple means to describe a class.",
            "Each single classes.",
            "Can you comment a bit on that?",
            "So how much does extra factor performance if my slides still work, I could show you at least formulation which you then you will use so you can take the SIM over central multiple centroids, Ann, since it is the sum over exponents, it's an unlimited.",
            "You obtain a nonlinear classifier.",
            "And depending on the exact use settings, it might improve up to five to 10 more percent in performance.",
            "Any other question?",
            "OK, if that's all, then let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm pleased to present my paper metric learning for large scale image classification, generalizing to new classes at near 0 cost.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Yacob for bake fluent Petronet in Gabriela to Gram.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main motivation for our work is that real life datasets are evolving overtime, for example at Flickr, Twitter Facebook and Amazon.",
                    "label": 0
                },
                {
                    "sent": "New images or items or tweets are edit every second.",
                    "label": 0
                },
                {
                    "sent": "When new labels, tags, and products are incorporated overtime, there is a need to index to retrieve to certain to categorize for those new items and those new classes.",
                    "label": 0
                },
                {
                    "sent": "Therefore we are interested in methods for large scale visual classification where we can.",
                    "label": 1
                },
                {
                    "sent": "Add new images and new classes at near near 0 cost and on the fly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The outline of my talk will be as follows.",
                    "label": 0
                },
                {
                    "sent": "I will first give a brief introduction and then detailed distance based classifiers before introducing our main contribution of the paper, which is on the about a new metric learning approach for the nearest class mean classifier at then will represent our experimental evaluation and finally our conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our field there has been a recent focus on large scale image classification which is emphasized by the introduction of the image net data set with currently holds over 14,000,000 images an over 20,000 different classes.",
                    "label": 1
                },
                {
                    "sent": "The Imagenet data set has been an important benchmarking tool ever since its introduction to good high quality performance on this large scale data set.",
                    "label": 0
                },
                {
                    "sent": "Usually high dimensional image features are used such as the Super factor encoding on the Fisher vector encoding and we train one versus rest.",
                    "label": 1
                },
                {
                    "sent": "SVM classifier using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When used on opening the datasets, there are limitations on the 1 first rest SVM framework.",
                    "label": 0
                },
                {
                    "sent": "For example, when a new image arrives, training has to continue, but also for a new class.",
                    "label": 0
                },
                {
                    "sent": "Training usually start from scratch.",
                    "label": 1
                },
                {
                    "sent": "To alleviate the letter transfer, learning is used to combine class representations from existing classes to obtain a good starting point for a new class.",
                    "label": 1
                },
                {
                    "sent": "However, it also still requires a nontrivial amount of training.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distance based classifiers offer an alternative to one versus rest SVM classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overcome some of those problems.",
                    "label": 0
                },
                {
                    "sent": "The most classic Hall distance based classifier is probably the K nearest neighbor method, where we assign an image I to the most common class among the K closest images from the training set.",
                    "label": 1
                },
                {
                    "sent": "That features of this model is that it's a very flexible model and highly non linear decision boundary.",
                    "label": 0
                },
                {
                    "sent": "It's easier to integrate new images or new classes, but just adding it to the database.",
                    "label": 0
                },
                {
                    "sent": "However, it's very expensive at this time, especially for the high dimensional image features we consider.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need a proper distance function to obtain high quality performance.",
                    "label": 0
                },
                {
                    "sent": "We use large margin nearest neighbors from Lineberger at all.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The older distance based classification classification method we consider is the nearest class mean classifier, where for each class we compute it mean and then we assign an image I to the class is the closest class mean.",
                    "label": 0
                },
                {
                    "sent": "Adventures of this model is that's very false at Test time, since it results in a linear model and also in this model it's very easy to integrate new images or new.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classes were just updating or adding a new mean.",
                    "label": 0
                },
                {
                    "sent": "However, since images of classes are only really presented with their mean, it might not be flexible enough for competitive performance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, in this paper we introduce a maximum likelihood approach for metric learning in the nearest class mean classifier.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mr First, before I introduce our new method, let's quickly review what official discriminants might be suboptimal in this very easy example of three not linear separable classes.",
                    "label": 0
                },
                {
                    "sent": "The classical fish.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discriminants.",
                    "label": 0
                },
                {
                    "sent": "Will try to maximize the variance between all class means.",
                    "label": 1
                },
                {
                    "sent": "The Fisher discriminate direction I depicted here in Gray will in the projection space, the blue and the green class are collapsed, which is clearly suboptimal.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our approach we introduce also trying to maximize the variance between nearby class means.",
                    "label": 1
                },
                {
                    "sent": "However, only between the class means which are easily confused.",
                    "label": 0
                },
                {
                    "sent": "The direction we get.",
                    "label": 0
                },
                {
                    "sent": "And separates the three classes still reasonably well in the 1 dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to do so, we learn a low rank MENA hobus distance.",
                    "label": 0
                },
                {
                    "sent": "Defined by the projection matrix W, the low rank projection matrix W allows us to have a controllable number of parameters, and it also allows to compress the high dimensional image feature to a lower dimension and compute distances in this lower dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the metric learning the first reformulate millionaires classman classifier into a probabilistic formulation using the softman function, where the probability of a Class C given image X is proportional to the exponent of the negative distance between the image and the class mean.",
                    "label": 1
                },
                {
                    "sent": "This model corresponds to a class posterior in a generative model, where the class conditional model is given by a Goshen.",
                    "label": 1
                },
                {
                    "sent": "This is shared covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "To learn the parameters of our projection matrix W, we have discriminative maximum likelihood training where we optimize or very maximized the likelihood of correct prediction of the images in our training set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To illustrate the learned distances obtained with this method, we consider the closest class means of the palm tree class according to the L2, including distance and according to our learned men hobus distance.",
                    "label": 1
                },
                {
                    "sent": "What we observe from the L2 distance that actually all the closest class means are from unrelated concepts.",
                    "label": 0
                },
                {
                    "sent": "They learned Monovisc distance on the other side only returns other trees as close as class means.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could re late the nearest class mean classifier to other linear classifiers where the score function is defined by a class specific bias Anna weight factor.",
                    "label": 1
                },
                {
                    "sent": "If we use linear SVM, we learn the bias and a weight factor pair class separately.",
                    "label": 0
                },
                {
                    "sent": "If we would use the recently introduced wasabi methods, it learns a low projection matrix W which is shared among all classes and pair class.",
                    "label": 0
                },
                {
                    "sent": "It learns a low dimensional weight factor.",
                    "label": 0
                },
                {
                    "sent": "The nearest class mean we consider in our paper only learns a shared projection matrix W and the class specific bias and weight vector are defined by the class means an the learned projection matrix.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now represent our experimental evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For our experiments we have used two large scale datasets.",
                    "label": 0
                },
                {
                    "sent": "The image net 2010 challenge datasets which contains about one or two million training images of 1000, different classes and the image net 10K data set which contains 4 and half million training images of 10,000 different classes.",
                    "label": 0
                },
                {
                    "sent": "As image representations we use 4064 thousand Fisher vectors and we use PQ compression to reduce the memory footprint on the high dimensional features.",
                    "label": 1
                },
                {
                    "sent": "For training rely on stochastic gradient dissent.",
                    "label": 0
                },
                {
                    "sent": "And we apply.",
                    "label": 0
                },
                {
                    "sent": "If you set the learning rates and early stopping to Terry and on a separate validation set.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In his first experiment, we compared the performance of the distance based classifiers on the image.",
                    "label": 0
                },
                {
                    "sent": "Net 2010 datasets.",
                    "label": 0
                },
                {
                    "sent": "First, we observe that both the K nearest neighbor method and the nearest class mean method significantly improved performance when we use a learned metric over the standard Euclidean distance distance.",
                    "label": 0
                },
                {
                    "sent": "2nd, we observe that the nearest classman classifier outperforms the more flexible K nearest neighbor classifier.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st, when we compare two and baseline of 1 first rest SVM's order is Abby method.",
                    "label": 0
                },
                {
                    "sent": "We observe that the distance based methods still perform competitively.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given those encouraging results, we apply the nearest classman classifier on the image net 10K data sets, and we compute just the means of the 10K classes, which takes about one hour of CPU time, and we reuse the metric we have previously learned.",
                    "label": 0
                },
                {
                    "sent": "We compared it to a baseline of one versus rest SVM's where we trained 10,000 different SVM classifiers, which takes about 280 CPU days.",
                    "label": 1
                },
                {
                    "sent": "This means that the nearest class mean classifier is faster by a factor of eight and a half thousand.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we look at the performance, we observed that the features we use of 64 thousand dimension this our SVM baseline obtains state of the art results over previously published methods.",
                    "label": 0
                },
                {
                    "sent": "We also observed that the nearest classman classifier still performs very good, especially considering this enormous speedup factor during training.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a final experiment, the applied the nearest classman classifier in in a transfer learning setting, where we try to obtain a zero shot prior using the image net class hierarchy.",
                    "label": 0
                },
                {
                    "sent": "What we do is in the first tab we.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Propagate all the means of our training images to the internal nodes upwards in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "In a second.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step very first direction and to obtain a mean for a class you have not seen.",
                    "label": 1
                },
                {
                    "sent": "At training we propagate down the image of the means of the internal nodes of ascendant nodes of the new class.",
                    "label": 1
                },
                {
                    "sent": "To obtain the serial shot prior.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use is an experiment on the image.",
                    "label": 0
                },
                {
                    "sent": "Net 2010 data set where in the first step we learn a metric on 800 classes and we evaluate the performance of this metric on $200.",
                    "label": 1
                },
                {
                    "sent": "Class is not used during training.",
                    "label": 0
                },
                {
                    "sent": "We compare two models.",
                    "label": 0
                },
                {
                    "sent": "One where we use only the data mean for the for the images available for those 200 classes, which we vary from one to a couple of 1000.",
                    "label": 0
                },
                {
                    "sent": "Compared to the zero, should prior shown in the previous slide vistas data mean per class what we observe from the results is that the zero should prior significantly improves the results.",
                    "label": 0
                },
                {
                    "sent": "Then we have only a few number of samples per class and also that if we have more samples per class, the results keeps improving up to about hundred 2000 images per class.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will now present our conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have shown the nearest class mean classification method and we have proposed a metric learning by maximum likelihood for the nearest Glassman classifier.",
                    "label": 1
                },
                {
                    "sent": "We have shown it outperforms the more flexible K nearest neighbor classifier and it seems on par with SVM's.",
                    "label": 0
                },
                {
                    "sent": "However, adventures of our nearest Glassman classifier over the standard one versus rest SVM framework is that it allows us to add new classes or new images at near 0 cost.",
                    "label": 0
                },
                {
                    "sent": "You have shown competitive results on unseen classes in image net 2000 image.",
                    "label": 1
                },
                {
                    "sent": "Net 10K data set and we have also shown that we can benefit from class priors obtained from the image net hierarchy for small sample sizes to further improve the results, we could extend the model and instead of using a single centroid, the mean.",
                    "label": 1
                },
                {
                    "sent": "Because you have multiple centroids per class which is shown our technical report which is available online.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned you can use multiple means to describe a class.",
                    "label": 0
                },
                {
                    "sent": "Each single classes.",
                    "label": 0
                },
                {
                    "sent": "Can you comment a bit on that?",
                    "label": 0
                },
                {
                    "sent": "So how much does extra factor performance if my slides still work, I could show you at least formulation which you then you will use so you can take the SIM over central multiple centroids, Ann, since it is the sum over exponents, it's an unlimited.",
                    "label": 0
                },
                {
                    "sent": "You obtain a nonlinear classifier.",
                    "label": 0
                },
                {
                    "sent": "And depending on the exact use settings, it might improve up to five to 10 more percent in performance.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "OK, if that's all, then let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}