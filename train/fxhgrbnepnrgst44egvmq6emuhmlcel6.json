{
    "id": "fxhgrbnepnrgst44egvmq6emuhmlcel6",
    "title": "Large Scale Learning with String Kernels",
    "info": {
        "author": [
            "S\u00f6ren Sonnenburg, Intelligent Data Analysis Group, Fraunhofer Institute for Intelligent Analysis and Information Systems"
        ],
        "published": "Dec. 29, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/eml07_sonnenburg_lsl/",
    "segmentation": [
        [
            "Talking about large scale learning with string kernels, in contrast to Korea talks.",
            "To conclude this talk about large convection currents, we consider string components which are linear in time and we are trying like linear in the length of the string and not quadratic as it wasn't their case and we're trying to speed up such kernels OK."
        ],
        [
            "So OK, we have seen large scale text classification problems or string problems.",
            "They appear in like normal text classification and spend less sperm categorization and basically setup is given end documents with Class A + -- 1 predict predicted text type.",
            "And we have 10 for security.",
            "It could be like like given given executable predicted with an executable.",
            "So virus or not and biology you have the same thing.",
            "Actually I'm working on most of time.",
            "Let's see if splice sites or the start of a gene and you want to predict weather in the center of a sequence.",
            "There's this splice site or not?",
            "OK, and one possible approach is to use a string cone and support vector machine and.",
            "Actually, at least in these biological examples, it is the case that if you use a large number of examples, then you can really get high."
        ],
        [
            "You see?",
            "OK, so formally giving given these N training examples, you have your string kernel like like averting back Conan or came here based kernels like Spectrum or ATT kernel.",
            "And you want to train a kernel machine on, say, 10,000,000 examples and evaluate it on a billion examples or several billion exam."
        ],
        [
            "So how do you think about this?",
            "Look like.",
            "Spectrum kernel basically computes counts of certain strings of length K and.",
            "This concept in summed up for each sequence multiplied and.",
            "Just ended up and this is the result in Kona spectrum.",
            "Colonel Major Economy is quite simple, identifies matching blocks and between two sequences, and the longer the matched.",
            "The higher the higher it is weighted, and this is summed up and all good.",
            "The thing that shrink owners have in common is that they there's usually a large discrete feature space behind it."
        ],
        [
            ".",
            "So in your in your kernel machine that you know it's also rated.",
            "These operated kernels.",
            "You quite often have to compute outputs for all fall test examples.",
            "So for example, you have to compute this linear combination of kernels.",
            "And therefore the effort is for single to computer a single.",
            "To complete the single and outputs, the effort is like in Venice are the number of this.",
            "This kernels that you have in your community in the linear combination and takes the time to computer kernel and so if you have to do it for all them examples in times in 20.",
            "And so for training training, this would be done in squared T. So it's quite.",
            "It's quite costly procedure.",
            "And the problem is that this is used in training engine testing, so it's actually worth tuning this step.",
            "OK, So what can we do with computing the kernel is already linear because I mean the first approach would be just to get the kernel to down to linear time.",
            "And."
        ],
        [
            "Yeah, this will be just limit our goodness.",
            "We psychology.",
            "It's actually quite simple idea and you have your linear combinations of kernels.",
            "You rewrite the delekta konus inner product in the feature space and you basically just compute this explicitly and computed or productive.",
            "So this is not.",
            "It's not at all clear that this should be faster than using the original.",
            "Urgent approach.",
            "However.",
            "It can be in some cases, so when is it possible?",
            "It's possible, definitely.",
            "If this W has has a low dimensionality, like linear and the string length which.",
            "Even even it's even better if it's sparse.",
            "So for example, this just works if you use.",
            "The strings of length 8 and for DNA in spectral kernel.",
            "It works for this way to get kernel of order 20 or for the feature spaces tend to live 14 dimensional.",
            "If you some efficient sparse data structure.",
            "And if this so, then you basically get a speedup of up to factor in."
        ],
        [
            "OK um.",
            "So you have to.",
            "To compute this, we have to be able to use this W. You must be able to access each dimension of the W using some index.",
            "So for example, you have to be able to enumerate all these chamars and you need in principle need only few operations.",
            "You need to be able to set this W20.",
            "You need to be able to add.",
            "Scalar value to one component of W and you must be able to look up one component of the W. So this is the operation that is that has to be quite efficient if you want to do like deal with very large datasets.",
            "Quite quite important, this here is storage, so you should use an explicit explicit map if your dimensionality is quite low of this, just W so.",
            "Then because then look up costs are basically just just in just one memory look up.",
            "You could use sorted area, so just somehow sparser, but contains and look up cost is quite quite high because of do some binary search in there.",
            "But still quite memory efficient if you string length, your K becomes too large.",
            "You want to look up then you should use for example suffix choice or tree so errors."
        ],
        [
            "Um so.",
            "So this table just summarizes the costs that exists on you when you do use one of these data structures, and it looks like the suffix tree is like the most.",
            "The, like the fastest one, because it just has linear.",
            "It's linear time in in.",
            "Look up for all the all the possible kamison in one sequence.",
            "However, it has like a tester like huge overhead in memory storage, so at least 40 unit at least 40 byte to store for one came here to store it.",
            "So there's almost almost no.",
            "I mean, you only need to have a double value here, so there's not really a big memory overhead in this explicit nip.",
            "And then we sorted areas get some more, you need some more memory, but not that much more.",
            "So you can draw a conclusion that if you have like a very, very small alphabet, this explicit map is very, very efficient for like for DNA.",
            "For example, if you have sorted areas, then it will work much better for.",
            "Larger alphabets and you have a really high order and large alphabet and you should use this suffix areas."
        ],
        [
            "Um?",
            "OK, so so this is now when we applied as production classifier we get like huge speedups.",
            "Using this.",
            "I mean we can, we really applied political machines on the whole human genome, so there's a 6 billion data points using this weighted degree kernel.",
            "The only problem is of course that if you have many, many supporters, UW may be extremely huge.",
            "And there's there's a trick to decompose this W into into some subparts and deal to deal with this, but I won't go into detail here the same so it doesn't tell you unfortunately.",
            "Doesn't tell you how you how, you can speed up is perfect machine in training, so there's there's another number of tricks which you can do there.",
            "So when you, when you usually suffer supporting machine, you have like some decomposition algorithm like chunking less ammo and.",
            "Con caching is.",
            "Coaching is.",
            "Quickly becomes quickly infeasible if the number of data points that you consider is like 1,000,000 or larger, because then you just cannot store all these cash discussion lines anymore in memory.",
            "So that is actually where this this trick.",
            "He also will help because you don't need kernel caching at all anymore."
        ],
        [
            "You can, you can grow quite.",
            "I mean you can go a lot lot further.",
            "OK so basically the.",
            "To do with some time concerned, the idea is.",
            "Innocentive chunking SVM.",
            "You have to compute.",
            "You just basically choose select some working set over which you optimize the problem and.",
            "The finger set at each iteration you have to compute the output for all training examples, which is exactly where we could use Linux right?",
            "The good news is that only few of these variables will change and optimization, so we don't need that.",
            "Many of these add operations to the vector to this to some kind of normal vector.",
            "OK, so these are the.",
            "The the the updates this these are the updates you have to compute at each iteration.",
            "This is not really.",
            "It is not efficient at all.",
            "It's why tossing Alchemist suggested that you start with an F of zero and just just update the vectors which which changed in the in the working set.",
            "OK."
        ],
        [
            "And obviously all you have to do is we just use this limit approach.",
            "We basically compute.",
            "The W on the working set.",
            "Which is this quantity?",
            "I'm just just supply.",
            "This technique, and this way we.",
            "Yeah we can.",
            "We can we don't need."
        ],
        [
            "The competition anymore and.",
            "Get get a slightly modified SVM optimizer and."
        ],
        [
            "Um?",
            "Yeah.",
            "Of course.",
            "Still, if the number of examples high then then still most of the time is spent in just computing.",
            "Just computing these outputs so we can still paralyze these.",
            "All these tree lockups and get another get another speedup factor there.",
            "The idea is just that you split parts, just put it in and computation into chunks and distributed to different machines or different CPU's."
        ],
        [
            "OK, now for some experiments.",
            "We did some experiments on Fiona tells and OK.",
            "So on the web spam data set which has 250,000 pages.",
            "Um?",
            "For 50,000 pages and we downloaded some like from random Web pages, 250,000 pages and just use the spectrum corner of order four on using sorted areas, you could have taken up to order order 8 using this using 64 bit.",
            "Integers still and the number the data set sizes like 4 gigabyte already.",
            "And if you use this 64 bit integers then it's going to slightly more than this 4050 gigabytes."
        ],
        [
            "OK.",
            "It's only OK so these are these results using like the normal spectrum kernel and you're basically you basically see it's faster if you have like very few examples, only 10 thousands and from 20,000 examples on it will just.",
            "Just just limit would just be much, much more efficient.",
            "And of course you are getting better test error when you when you use."
        ],
        [
            "Examples.",
            "K sorry.",
            "What?"
        ],
        [
            "Thinking about.",
            "In the SEC in training for trading time, yes, trying time and area under the sea."
        ],
        [
            "In percent.",
            "OK, then supply side recognition we have data set which has about 1550 million examples.",
            "And use this way to get kernel using twice as data structure and the spectrum kernel using."
        ],
        [
            "It's just explicit Maps.",
            "And trained on 10,000,000 examples and tested on five.",
            "So basically you have twice for each position in this virtual kernel."
        ],
        [
            "Um?",
            "And the results are not following.",
            "So this is just full spectrum account.",
            "If you precompute the kernel matrix, which you can do 30,000 about 30,000 examples, then it's just assuming there's just some, just some get for the.",
            "Constant speed up.",
            "For food in it.",
            "And there's not much gain which, like if use more.",
            "More CPUs here because to look up is just so cheap for the spectrum kernel."
        ],
        [
            "Explicit Maps Dennis you kind of similar thing for allegedly Colonel again precomputed Singer Sing Super using normal.",
            "Yeah, just send it to SVM and you get some.",
            "Again some constant offset in using using limit so I think the speedup is about vector 20."
        ],
        [
            "So for.",
            "And you again get like much better results if you use up to 1010 million examples like the area under the precision recall curve."
        ],
        [
            "Steadily increases.",
            "OK, so conclusion it's like quite generous trick to do speedups for these string kernels.",
            "OK, so we were able to like turn on 10 million data points using a shared memory.",
            "Translation got speedups.",
            "Affecter 64 for Spectrum and four for veggie kernel and and.",
            "Yeah, if you further if you further use for CPU's and you get another speedup.",
            "Factor 3 point 3.2.",
            "Um?",
            "And of course, use more data than you.",
            "Then you get state of the art QC and this implemented in the."
        ],
        [
            "Boom boom box.",
            "OK, so one last thing seen lots of people contributing software here so it would be great if you would help our efforts here in machine learning.",
            "Open source software.",
            "So you're trying to establish a new trick.",
            "I mean, we already established a new track and Gemma are so you can publish actually just the software.",
            "And we also set up some website where you can announce news of the packages that you wrote such that other people can know that it exists.",
            "And I hope that you all may contribute if you have something called itself something.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking about large scale learning with string kernels, in contrast to Korea talks.",
                    "label": 0
                },
                {
                    "sent": "To conclude this talk about large convection currents, we consider string components which are linear in time and we are trying like linear in the length of the string and not quadratic as it wasn't their case and we're trying to speed up such kernels OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, we have seen large scale text classification problems or string problems.",
                    "label": 1
                },
                {
                    "sent": "They appear in like normal text classification and spend less sperm categorization and basically setup is given end documents with Class A + -- 1 predict predicted text type.",
                    "label": 1
                },
                {
                    "sent": "And we have 10 for security.",
                    "label": 0
                },
                {
                    "sent": "It could be like like given given executable predicted with an executable.",
                    "label": 0
                },
                {
                    "sent": "So virus or not and biology you have the same thing.",
                    "label": 0
                },
                {
                    "sent": "Actually I'm working on most of time.",
                    "label": 0
                },
                {
                    "sent": "Let's see if splice sites or the start of a gene and you want to predict weather in the center of a sequence.",
                    "label": 1
                },
                {
                    "sent": "There's this splice site or not?",
                    "label": 1
                },
                {
                    "sent": "OK, and one possible approach is to use a string cone and support vector machine and.",
                    "label": 0
                },
                {
                    "sent": "Actually, at least in these biological examples, it is the case that if you use a large number of examples, then you can really get high.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "OK, so formally giving given these N training examples, you have your string kernel like like averting back Conan or came here based kernels like Spectrum or ATT kernel.",
                    "label": 1
                },
                {
                    "sent": "And you want to train a kernel machine on, say, 10,000,000 examples and evaluate it on a billion examples or several billion exam.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do you think about this?",
                    "label": 0
                },
                {
                    "sent": "Look like.",
                    "label": 0
                },
                {
                    "sent": "Spectrum kernel basically computes counts of certain strings of length K and.",
                    "label": 0
                },
                {
                    "sent": "This concept in summed up for each sequence multiplied and.",
                    "label": 0
                },
                {
                    "sent": "Just ended up and this is the result in Kona spectrum.",
                    "label": 0
                },
                {
                    "sent": "Colonel Major Economy is quite simple, identifies matching blocks and between two sequences, and the longer the matched.",
                    "label": 0
                },
                {
                    "sent": "The higher the higher it is weighted, and this is summed up and all good.",
                    "label": 0
                },
                {
                    "sent": "The thing that shrink owners have in common is that they there's usually a large discrete feature space behind it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ".",
                    "label": 0
                },
                {
                    "sent": "So in your in your kernel machine that you know it's also rated.",
                    "label": 1
                },
                {
                    "sent": "These operated kernels.",
                    "label": 0
                },
                {
                    "sent": "You quite often have to compute outputs for all fall test examples.",
                    "label": 1
                },
                {
                    "sent": "So for example, you have to compute this linear combination of kernels.",
                    "label": 0
                },
                {
                    "sent": "And therefore the effort is for single to computer a single.",
                    "label": 0
                },
                {
                    "sent": "To complete the single and outputs, the effort is like in Venice are the number of this.",
                    "label": 0
                },
                {
                    "sent": "This kernels that you have in your community in the linear combination and takes the time to computer kernel and so if you have to do it for all them examples in times in 20.",
                    "label": 0
                },
                {
                    "sent": "And so for training training, this would be done in squared T. So it's quite.",
                    "label": 0
                },
                {
                    "sent": "It's quite costly procedure.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that this is used in training engine testing, so it's actually worth tuning this step.",
                    "label": 1
                },
                {
                    "sent": "OK, So what can we do with computing the kernel is already linear because I mean the first approach would be just to get the kernel to down to linear time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this will be just limit our goodness.",
                    "label": 0
                },
                {
                    "sent": "We psychology.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite simple idea and you have your linear combinations of kernels.",
                    "label": 0
                },
                {
                    "sent": "You rewrite the delekta konus inner product in the feature space and you basically just compute this explicitly and computed or productive.",
                    "label": 0
                },
                {
                    "sent": "So this is not.",
                    "label": 0
                },
                {
                    "sent": "It's not at all clear that this should be faster than using the original.",
                    "label": 0
                },
                {
                    "sent": "Urgent approach.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "It can be in some cases, so when is it possible?",
                    "label": 0
                },
                {
                    "sent": "It's possible, definitely.",
                    "label": 0
                },
                {
                    "sent": "If this W has has a low dimensionality, like linear and the string length which.",
                    "label": 1
                },
                {
                    "sent": "Even even it's even better if it's sparse.",
                    "label": 0
                },
                {
                    "sent": "So for example, this just works if you use.",
                    "label": 0
                },
                {
                    "sent": "The strings of length 8 and for DNA in spectral kernel.",
                    "label": 0
                },
                {
                    "sent": "It works for this way to get kernel of order 20 or for the feature spaces tend to live 14 dimensional.",
                    "label": 1
                },
                {
                    "sent": "If you some efficient sparse data structure.",
                    "label": 0
                },
                {
                    "sent": "And if this so, then you basically get a speedup of up to factor in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "So you have to.",
                    "label": 0
                },
                {
                    "sent": "To compute this, we have to be able to use this W. You must be able to access each dimension of the W using some index.",
                    "label": 0
                },
                {
                    "sent": "So for example, you have to be able to enumerate all these chamars and you need in principle need only few operations.",
                    "label": 0
                },
                {
                    "sent": "You need to be able to set this W20.",
                    "label": 0
                },
                {
                    "sent": "You need to be able to add.",
                    "label": 0
                },
                {
                    "sent": "Scalar value to one component of W and you must be able to look up one component of the W. So this is the operation that is that has to be quite efficient if you want to do like deal with very large datasets.",
                    "label": 0
                },
                {
                    "sent": "Quite quite important, this here is storage, so you should use an explicit explicit map if your dimensionality is quite low of this, just W so.",
                    "label": 0
                },
                {
                    "sent": "Then because then look up costs are basically just just in just one memory look up.",
                    "label": 0
                },
                {
                    "sent": "You could use sorted area, so just somehow sparser, but contains and look up cost is quite quite high because of do some binary search in there.",
                    "label": 0
                },
                {
                    "sent": "But still quite memory efficient if you string length, your K becomes too large.",
                    "label": 0
                },
                {
                    "sent": "You want to look up then you should use for example suffix choice or tree so errors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "So this table just summarizes the costs that exists on you when you do use one of these data structures, and it looks like the suffix tree is like the most.",
                    "label": 0
                },
                {
                    "sent": "The, like the fastest one, because it just has linear.",
                    "label": 0
                },
                {
                    "sent": "It's linear time in in.",
                    "label": 0
                },
                {
                    "sent": "Look up for all the all the possible kamison in one sequence.",
                    "label": 0
                },
                {
                    "sent": "However, it has like a tester like huge overhead in memory storage, so at least 40 unit at least 40 byte to store for one came here to store it.",
                    "label": 0
                },
                {
                    "sent": "So there's almost almost no.",
                    "label": 0
                },
                {
                    "sent": "I mean, you only need to have a double value here, so there's not really a big memory overhead in this explicit nip.",
                    "label": 0
                },
                {
                    "sent": "And then we sorted areas get some more, you need some more memory, but not that much more.",
                    "label": 0
                },
                {
                    "sent": "So you can draw a conclusion that if you have like a very, very small alphabet, this explicit map is very, very efficient for like for DNA.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have sorted areas, then it will work much better for.",
                    "label": 0
                },
                {
                    "sent": "Larger alphabets and you have a really high order and large alphabet and you should use this suffix areas.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is now when we applied as production classifier we get like huge speedups.",
                    "label": 0
                },
                {
                    "sent": "Using this.",
                    "label": 0
                },
                {
                    "sent": "I mean we can, we really applied political machines on the whole human genome, so there's a 6 billion data points using this weighted degree kernel.",
                    "label": 0
                },
                {
                    "sent": "The only problem is of course that if you have many, many supporters, UW may be extremely huge.",
                    "label": 0
                },
                {
                    "sent": "And there's there's a trick to decompose this W into into some subparts and deal to deal with this, but I won't go into detail here the same so it doesn't tell you unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Doesn't tell you how you how, you can speed up is perfect machine in training, so there's there's another number of tricks which you can do there.",
                    "label": 0
                },
                {
                    "sent": "So when you, when you usually suffer supporting machine, you have like some decomposition algorithm like chunking less ammo and.",
                    "label": 0
                },
                {
                    "sent": "Con caching is.",
                    "label": 0
                },
                {
                    "sent": "Coaching is.",
                    "label": 0
                },
                {
                    "sent": "Quickly becomes quickly infeasible if the number of data points that you consider is like 1,000,000 or larger, because then you just cannot store all these cash discussion lines anymore in memory.",
                    "label": 0
                },
                {
                    "sent": "So that is actually where this this trick.",
                    "label": 0
                },
                {
                    "sent": "He also will help because you don't need kernel caching at all anymore.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can, you can grow quite.",
                    "label": 0
                },
                {
                    "sent": "I mean you can go a lot lot further.",
                    "label": 0
                },
                {
                    "sent": "OK so basically the.",
                    "label": 0
                },
                {
                    "sent": "To do with some time concerned, the idea is.",
                    "label": 0
                },
                {
                    "sent": "Innocentive chunking SVM.",
                    "label": 0
                },
                {
                    "sent": "You have to compute.",
                    "label": 0
                },
                {
                    "sent": "You just basically choose select some working set over which you optimize the problem and.",
                    "label": 1
                },
                {
                    "sent": "The finger set at each iteration you have to compute the output for all training examples, which is exactly where we could use Linux right?",
                    "label": 0
                },
                {
                    "sent": "The good news is that only few of these variables will change and optimization, so we don't need that.",
                    "label": 1
                },
                {
                    "sent": "Many of these add operations to the vector to this to some kind of normal vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the.",
                    "label": 0
                },
                {
                    "sent": "The the the updates this these are the updates you have to compute at each iteration.",
                    "label": 1
                },
                {
                    "sent": "This is not really.",
                    "label": 0
                },
                {
                    "sent": "It is not efficient at all.",
                    "label": 1
                },
                {
                    "sent": "It's why tossing Alchemist suggested that you start with an F of zero and just just update the vectors which which changed in the in the working set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And obviously all you have to do is we just use this limit approach.",
                    "label": 0
                },
                {
                    "sent": "We basically compute.",
                    "label": 0
                },
                {
                    "sent": "The W on the working set.",
                    "label": 0
                },
                {
                    "sent": "Which is this quantity?",
                    "label": 0
                },
                {
                    "sent": "I'm just just supply.",
                    "label": 0
                },
                {
                    "sent": "This technique, and this way we.",
                    "label": 0
                },
                {
                    "sent": "Yeah we can.",
                    "label": 0
                },
                {
                    "sent": "We can we don't need.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The competition anymore and.",
                    "label": 0
                },
                {
                    "sent": "Get get a slightly modified SVM optimizer and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Still, if the number of examples high then then still most of the time is spent in just computing.",
                    "label": 0
                },
                {
                    "sent": "Just computing these outputs so we can still paralyze these.",
                    "label": 0
                },
                {
                    "sent": "All these tree lockups and get another get another speedup factor there.",
                    "label": 0
                },
                {
                    "sent": "The idea is just that you split parts, just put it in and computation into chunks and distributed to different machines or different CPU's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now for some experiments.",
                    "label": 0
                },
                {
                    "sent": "We did some experiments on Fiona tells and OK.",
                    "label": 0
                },
                {
                    "sent": "So on the web spam data set which has 250,000 pages.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For 50,000 pages and we downloaded some like from random Web pages, 250,000 pages and just use the spectrum corner of order four on using sorted areas, you could have taken up to order order 8 using this using 64 bit.",
                    "label": 0
                },
                {
                    "sent": "Integers still and the number the data set sizes like 4 gigabyte already.",
                    "label": 0
                },
                {
                    "sent": "And if you use this 64 bit integers then it's going to slightly more than this 4050 gigabytes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's only OK so these are these results using like the normal spectrum kernel and you're basically you basically see it's faster if you have like very few examples, only 10 thousands and from 20,000 examples on it will just.",
                    "label": 0
                },
                {
                    "sent": "Just just limit would just be much, much more efficient.",
                    "label": 0
                },
                {
                    "sent": "And of course you are getting better test error when you when you use.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "K sorry.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thinking about.",
                    "label": 0
                },
                {
                    "sent": "In the SEC in training for trading time, yes, trying time and area under the sea.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In percent.",
                    "label": 0
                },
                {
                    "sent": "OK, then supply side recognition we have data set which has about 1550 million examples.",
                    "label": 0
                },
                {
                    "sent": "And use this way to get kernel using twice as data structure and the spectrum kernel using.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just explicit Maps.",
                    "label": 0
                },
                {
                    "sent": "And trained on 10,000,000 examples and tested on five.",
                    "label": 0
                },
                {
                    "sent": "So basically you have twice for each position in this virtual kernel.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the results are not following.",
                    "label": 0
                },
                {
                    "sent": "So this is just full spectrum account.",
                    "label": 0
                },
                {
                    "sent": "If you precompute the kernel matrix, which you can do 30,000 about 30,000 examples, then it's just assuming there's just some, just some get for the.",
                    "label": 0
                },
                {
                    "sent": "Constant speed up.",
                    "label": 0
                },
                {
                    "sent": "For food in it.",
                    "label": 0
                },
                {
                    "sent": "And there's not much gain which, like if use more.",
                    "label": 0
                },
                {
                    "sent": "More CPUs here because to look up is just so cheap for the spectrum kernel.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explicit Maps Dennis you kind of similar thing for allegedly Colonel again precomputed Singer Sing Super using normal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just send it to SVM and you get some.",
                    "label": 0
                },
                {
                    "sent": "Again some constant offset in using using limit so I think the speedup is about vector 20.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for.",
                    "label": 0
                },
                {
                    "sent": "And you again get like much better results if you use up to 1010 million examples like the area under the precision recall curve.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steadily increases.",
                    "label": 0
                },
                {
                    "sent": "OK, so conclusion it's like quite generous trick to do speedups for these string kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so we were able to like turn on 10 million data points using a shared memory.",
                    "label": 1
                },
                {
                    "sent": "Translation got speedups.",
                    "label": 1
                },
                {
                    "sent": "Affecter 64 for Spectrum and four for veggie kernel and and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you further if you further use for CPU's and you get another speedup.",
                    "label": 0
                },
                {
                    "sent": "Factor 3 point 3.2.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And of course, use more data than you.",
                    "label": 0
                },
                {
                    "sent": "Then you get state of the art QC and this implemented in the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Boom boom box.",
                    "label": 0
                },
                {
                    "sent": "OK, so one last thing seen lots of people contributing software here so it would be great if you would help our efforts here in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Open source software.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to establish a new trick.",
                    "label": 0
                },
                {
                    "sent": "I mean, we already established a new track and Gemma are so you can publish actually just the software.",
                    "label": 1
                },
                {
                    "sent": "And we also set up some website where you can announce news of the packages that you wrote such that other people can know that it exists.",
                    "label": 0
                },
                {
                    "sent": "And I hope that you all may contribute if you have something called itself something.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}