{
    "id": "ia3gvzet4ltan3l4bbzzgmwfm5g546pk",
    "title": "Convergent Learning: Do different neural networks learn the same representations?",
    "info": {
        "author": [
            "Jason Yosinski, Department of Computer Science, Cornell University"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_yosinski_convergent_learning/",
    "segmentation": [
        [
            "Cool, thanks very much.",
            "Before I get started I just wanted to say that.",
            "A lot of this talk is the work of each family here on the on the left."
        ],
        [
            "She's a very bright and hardworking student.",
            "She should be giving this talk, but she couldn't make it for some travel reasons.",
            "So you're stuck with me instead."
        ],
        [
            "So I'd like to start by introducing this concept of a grandmother neuron.",
            "So many of you may be familiar with this concept.",
            "The idea is that somewhere in your brain there's maybe a single neuron kind of corresponds to the concept of your grandmother.",
            "So when we say corresponds, we made a couple things.",
            "So first that means anytime you see your grandmother or maybe think about her, this neuron should fire.",
            "And it also means that it fires only for your grandmother.",
            "In other words, we sort of 1 to one relationship.",
            "This entailment is if is only if if and only of between the concept of your grandmother.",
            "And the firing of this neuron.",
            "So in this talk, we're going to call this type of representation a local representation.",
            "There's some evidence from the neuroscience literature that such representations do exist in the brain.",
            "You may have seen this paper a decade ago or so by Quiroga at all, where they found sort of a Jennifer Aniston neuron.",
            "This paper is a little bit controversial, but we won't get into that at all today."
        ],
        [
            "Because I'm not a neuroscientist and I guess most of you are probably not either.",
            "So, so this is 1 possible representation strategy, but it's not the only one that could be playing itself out in the brain or in artificial neural networks.",
            "Could could be an alternative hypothesis, like representations could be distributed.",
            "So for example, the concept of your grandmother could be represented only ever as the pattern taken across several dimensions.",
            "Several neurons never across a single neuron.",
            "And a key thing we could look for here is that these same neurons would be repurposed for other things as well, like representing the concept of your cat."
        ],
        [
            "So in general, in the neurosciences opposed people would like to measure for whether these you know which types of representational strategies are are used in the brain.",
            "And this is in general quite a hard thing to measure because it's difficult to stick needles in humans and take lots of data and so on.",
            "But you and I are very lucky because we work on these types of networks and we can basically measure anything we want.",
            "So shouldn't we be able to look for these types of representations in the networks we study?",
            "So before I go further, many of you may be thinking you know, isn't this actually already resolved?",
            "There was this great."
        ],
        [
            "Super by Christian Szegedy at all in 2013 and they showed some evidence that you know representations are distributed is just a big tangled mess, and you can't really do anything else about it.",
            "Well, for various reasons we can talk about it.",
            "The poster session.",
            "We think this is kind of an incomplete story.",
            "We do things.",
            "Some representations are at least partially distributed, but they actually showed this result only for the second last layer of the network, not everywhere in the network.",
            "And there's a little more subtlety going on there that we could talk about later also in work with."
        ],
        [
            "So with my collaborators, we showed that in the middle of some networks you see kind of uniquely or locally represented concepts like spiders and detectors for water and detectives for text, which seem to be fairly more locally representational."
        ],
        [
            "So let's imagine you bye bye.",
            "My argument for now that something more subtle might be going on.",
            "We have our neural network here on the right, and we'd like to test for the presence or absence of these different types of representational codes.",
            "So how might we actually go about doing that?",
            "One method you can imagine is we assemble a list of distinct concepts.",
            "We'd like to test for.",
            "So Grandmother and cat and stop sign in lawnmower and so on, and then we assemble kind of labeled training set, corresponding maybe images with each of these labels.",
            "We push all the images through the network we see if there's a neuron that always fires her grandmother never paid for anything else, and so on, and that would kind of sort of work.",
            "We could test for the local hypothesis this way."
        ],
        [
            "Only this method would require lots and lots of concept labels, so a larger label data set that we actually have.",
            "It would be a bit of a hurry."
        ],
        [
            "In effort to collect this entire human annotated data set.",
            "However, actually, if you were here, had a clear last year bolage out took a step up to the Mike.",
            "I don't know.",
            "He took a stab at being Hercules and actually had a bunch of Mechanical Turkers label individual units in a neural network and he found actually some local concepts like lamps and table legs represented halfway through a network in general."
        ],
        [
            "This this paper was was quite amazing.",
            "However, it's not necessarily going to scale to large, more and more concepts because you need to pay lots of Turkers.",
            "Also, there's actually a more subtle issue going on, which is.",
            "This approach tends to work for when your concepts have labels that are recognizable to a human, so for example."
        ],
        [
            "Grandmother Cat stop sign and so on are all concepts that you might find in a dictionary.",
            "There are things that humans found so important that we wrote down, but maybe there are other concepts that are equally important that we haven't given words too.",
            "So basically what what counts is the concept.",
            "Is it just words in a dictionary or something else?",
            "For example, maybe some neuron in your brain or in an artificial neural network would respond to the reflection of the sun off of the water.",
            "So is this an important concept or not?",
            "Also, maybe the particular Tufts of hair on behind dogs ears that.",
            "Used to tell Yorkshire terriers from Norwich Terriers.",
            "It's probably not a concept that matters to any of you unless you are dog breeders or you've been working on image net for way way too long.",
            "So basically what we don't want to do is we don't want to bias or search for concept stored what humans would label this concept.",
            "We want to look for kind of a more general approach.",
            "So in this study were going to make this key definition and assumption that kind of addresses the first concern and answers the second question all at once, and we can argue about whether this is a good assumption or not.",
            "But we're just going to make it so we're going to."
        ],
        [
            "Assumption that concepts are just feature subspaces that are reliably learned in multiple networks.",
            "So to give us more concretely, if many humans end up learning somewhere in the brain, a detector for cats, that means cats are a concept that's worth considering, as like a unique.",
            "Concept that stands on its own.",
            "I guess if many NNS learn a feature for the Sun reflecting off of water, then that also is a concept that deserves to be given kind of.",
            "First Citizen ships that is, and also we should note that these concepts can be multidimensional.",
            "So if all of us learn just banned a space of colors and color is a concept that's worth considering as a concept.",
            "Whether we actually span it using RGB or HSV.",
            "Still, it's now like a 3 dimensional concept that we should consider as a first class citizen.",
            "So this definition assumption leads us sort of directly to our key."
        ],
        [
            "Technique of this study, which is that we can probe for different types of representation, local or distributed by comparing multiple networks.",
            "Trains starting from different random initializations.",
            "So for example, if we train multiple networks from different random initializations.",
            "Anna local code is being used.",
            "We would expect to find features in one network that are relearned by the SEC Network but may be subject to a position permutation.",
            "On the other hand, in the completely distributed case we may find that units are only used as sort of arbitrary basis vectors to span the entire representation space.",
            "And the basis vectors in one network may be kind of a rotated version of the basis vectors in another network.",
            "We also admit a sort of intermediate hypothesis called the partially distributed hypothesis.",
            "In this case, we expect to see low dimensional subspaces that are reliably learned in multiple networks.",
            "But where within that subspace the basis vectors in one, maybe a rotation of the basis vectors in another.",
            "So to show this in a slightly different way."
        ],
        [
            "In the local case, we expect the basis vectors for one network to be almost perfectly aligned with the vectors in another network.",
            "In the partially distributed case, we expect rotations through low dimensional subspaces for one net versus the other.",
            "For example, maybe in only two out of the three dimensions here and in the fully distributed case we expect kind of arbitrary rotations through all feature dimensions.",
            "In this day."
        ],
        [
            "We use the Alex net architecture trained on image that the architecture is identical, just the initializations are different.",
            "If you're curious, the performance of both networks is very, very similar.",
            "So."
        ],
        [
            "Let's start by probing for the presence or absence of this local code.",
            "To do this, first we'll see if we can find units that match, well, one to one between the two networks.",
            "To find matching units one to one, what we do is we push all of Imagenet through the networks."
        ],
        [
            "And we compute correlation statistics between neuron activations in one network versus the other.",
            "Just to be clear, we only ever compare neurons on the same layer from one network to another, never across different layers of the networks.",
            "So first just to start with, let's compute the correlation of all units in net, one with itself on some layer here, one.",
            "So if we visualize this as a matrix and we take the absolute value, we see correlation values at Max of 1 along the diagonal.",
            "So these white diagonal is correlation values of one with a unit with itself.",
            "Of course it should be one.",
            "Everywhere else you see values that are generally quite small, because features are generally fairly orthogonal.",
            "Although you see some bright spots where you know multiple features kind of overlap in their representation somehow.",
            "For Network two we see a very similar very similar picture."
        ],
        [
            "And then finally, what we're interested in is this matrix, so it's the between net or cross net correlation between net one and net 2.",
            "So as a brief aside, you might be wondering, you know.",
            "Correlations are very simple measure.",
            "Is that actually powerful enough to identify the types of correspondences we'd like to?",
            "We also under this.",
            "So in addition to trying correlation, we have a whole section of the paper.",
            "We try mutual information.",
            "We estimate it by using some histograms and the short version of the story is it's all the same.",
            "You can check out the supplementary information from word about that.",
            "OK, so."
        ],
        [
            "Victor Matrix we have this between that correlation matrix net.",
            "Wonder net two for con fun and.",
            "We're interested in seeing in this matrix of these these bright areas, so these bright pixels are entries where neuron in that one in the another neuron in net two are kind of firing all the time for the same thing.",
            "So to find the actual matches between network and the other, we can take a couple of approaches.",
            "So a straightforward approach is to use what we call greedy matching.",
            "Where we simply pick the Max along each row.",
            "So for each neuron in one network, find the most predictive or most correlated unit in the other network.",
            "We can pick the Max along each row and kind of visualize these matches in the following way so."
        ],
        [
            "For each neuron in any any network, we visualize it like this.",
            "We show on the top the top nine patches that cause high activation for this unit and we showed the deconvolution of those top line patches from Zeiler and Fergus 2014."
        ],
        [
            "So here's the network, sorry, here's a unit in the first network and its corresponding unit in the second network.",
            "Here we see both networks.",
            "Learn units that correlate with a value of 1, indicating that they learned kind of very much the same concept.",
            "The same kind of green detector, suggesting that maybe this color spaces not just stand by arbitrary vectors, but maybe this single dimension was very important.",
            "It also could have been that spanned by different factors and you just got really lucky that they correlated so perfectly.",
            "You know they both.",
            "Happened to spend it the same way, but they didn't need to spend it the same way.",
            "If we look for other matches, we can find others."
        ],
        [
            "So for example here, here's two horizontal edge detectors which are very highly correlated, and if we keep looking we can."
        ],
        [
            "Others that are less and less correlated, detecting other types of features.",
            "Eventually."
        ],
        [
            "To continue the process for all units in common, we can see that there are some units here on the right.",
            "They have no great match.",
            "For example, some of those units on the far right, the best match unit in the SEC Network has correlation only like .2 or .3, with the unit in the first network.",
            "This suggests that these features are kind of unique and they will learn once but not the second time."
        ],
        [
            "We can follow the same process for com two finding it."
        ],
        [
            "It's a highly correlated units that are recognizing speckled textures, grids and green objects, and so on."
        ],
        [
            "Repeat the process for COM 3 actually income three.",
            "We see something really cool."
        ],
        [
            "Think it's really cool, so here's a here's a local code."
        ],
        [
            "The left here.",
            "It's a single neuron that learns to represent the concept of black and white objects, and it's learned in both networks, and it's very highly correlated.",
            "So this, to me means like this concept.",
            "Even though you might not have thought of enumerating this concept initially, we're starting to study.",
            "It means this concept is actually present in the network, and it's actually learned reliably.",
            "So maybe it deserves to be considered as like a first class concept that has local representation.",
            "And."
        ],
        [
            "Check out the paper.",
            "You can see other other concepts along this road."
        ],
        [
            "So back to our matrix here.",
            "What I just showed you was the greedy matching approach where we pick the Max along each row."
        ],
        [
            "This is done with replacement, so multiple units in that one could be paired with the same unit in net two.",
            "We don't necessarily have to do it this way.",
            "Alternately, we could find the best match without replacement, so that every unit 1 network has a sort of unique counterpart in the other network.",
            "We can do this by finding the Max weighted bipartite matching, so this matching allows us to take two networks."
        ],
        [
            "Here, with units that are not quite aligned and to bring them into the best possible alignment."
        ],
        [
            "If we visualize the correlation matrix between the approximately aligned networks, it looks something like this."
        ],
        [
            "So this shows.",
            "This shows along the diagonal you can see some relatively bright diagonal showing the cases where the Max weighted bipartite matching was kind of successful in finding a great pair.",
            "Other parts of the diagonal or not so bright, showing that those units did not find a good partner."
        ],
        [
            "In some cases the match is found in.",
            "Both methods are the same in other cases that matches found are different and we can actually gain additional insight by comparing the ones that are the same with the ones that are different.",
            "We do though."
        ],
        [
            "By plotting each unit along the X axis with the correlation of its match along the Y axis.",
            "Here, the correlation of the Max matches shown in light green, which is always higher than the correlation, or at least as high as the correlation shown by the Max weighted bipartite matching.",
            "And we sort the units on the X axis by their correlation value simply for clarity.",
            "So there's a couple of observations we can draw from this."
        ],
        [
            "If you look at these high correlation pairs in the beginning, there matches found by both methods that coincide and they happen to have fairly high correlation.",
            "If we think back to."
        ],
        [
            "Local Distributed thought experiment.",
            "This is what we would expect to see in the cases where local code is being used.",
            "We can also find some units in NA."
        ],
        [
            "And without a high correlation matching that too."
        ],
        [
            "Indicating that these features are, like we said, unique to one network or the other.",
            "This could be we're not actually sure entirely what this means.",
            "It could be that the features are just learned here and they're completely missed by the SEC Network, which would suggest one possible reason why ensemble methods might be effective.",
            "Finally, we see this kind of interesting little read."
        ],
        [
            "Where in some?"
        ],
        [
            "This is the matches are not the same between one method and the other.",
            "So in some of these cases where they differ, the bipartite matching is much worse.",
            "So if you look at this huge spike in the middle, what do you?",
            "What do you think is going on there?",
            "So let's imagine for a second that the partially distributed hypothesis is true.",
            "If it was what we expected."
        ],
        [
            "See.",
            "So we might."
        ],
        [
            "For example, CNET one learn to span a subspace of say, human faces using 4 units, but net two learns to spend the same subspace with only three units.",
            "So if we."
        ],
        [
            "Greedy matching, we might find these pairings with unique bipartite matching.",
            "On the other hand, three of the four you know."
        ],
        [
            "From that one may be matched up to their nearest counterpart, net two, as shown.",
            "But then the 4th unit, although it has a relatively high correlation counterpart to those three units, are already taken, and it ends up being paired with an almost unrelated filter."
        ],
        [
            "Here so we don't take this to be proof positive of a partially distributed hypothesis, but we take it to be a hint that such a code might be being used."
        ],
        [
            "In the paper, if you check it out, you can see similar plots for the first 5, first 5 layers."
        ],
        [
            "OK, so so far I've showed you.",
            "A certain way of looking for possible one to one matchings, and we found that there are some and that explains kind of some of the representations but not everything.",
            "So there's a whole lot of the rest of the network that's not well explained by this one hypothesis.",
            "So what we'd like to do next, and I'll have to go kind of quickly through this, is find kind of.",
            "So take those locally represented things and kind of chop them off and then try to find if we can see if we can find low dimensional subspaces where it's not a single dimension that predicts a single other dimension here, but it's kind of like a few dimensions here that predicts a few dimensions here.",
            "So."
        ],
        [
            "Again, skipping some details for time, we basically learned mapping layers from one network to another, so we predict one networks activations from the other.",
            "We do this using first a dense weight matrix which works fairly well.",
            "Then we kind of have this nob, where we have an L1 sparsity penalty.",
            "We slowly turn that up.",
            "And we find that as we turn it up, we can actually get very sparse solutions without suffering a loss of performance, indicating that actually you don't need the full dimension to predict one network to another.",
            "You can actually use a very small subset.",
            "For convoy and it's generally four or five units to predict one of the units accurately.",
            "Eventually, if you."
        ],
        [
            "Get to sparse prediction fails for comp two.",
            "We see similar results for Common 3, four and five.",
            "The picture is a little trickier and I refer you to the paper for that."
        ],
        [
            "So in interest?",
            "OK, so here's here's an example mapping matrix from one layer to from one network to another.",
            "You can see as much sparser than our correlation matrix we found before.",
            "But the prediction is still just about as good, so if we take a few."
        ],
        [
            "Prices through this matrix we can see some interesting stuff going on.",
            "For example, here is 1 unit 2 that's predicted very well using the activations of three units in that one."
        ],
        [
            "Here's another another slice."
        ],
        [
            "Through that matrix."
        ],
        [
            "OK, so this is the few."
        ],
        [
            "To one mapping, but this still isn't the full partially distributed hypothesis where we have, you know, several to several mappings.",
            "So to find that.",
            "It's even harder and we don't have a super great method."
        ],
        [
            "Yet.",
            "We are using something called hierarchical agglomerative clustering.",
            "For now, this produces."
        ],
        [
            "Is kind of a tree structure which we can then use to permute the units in both networks to kind of align them with this tree.",
            "The tree is generated such that Co predictive clusters end up together if we zoom."
        ],
        [
            "Just some of these Co project."
        ],
        [
            "Clusters we can eventually see.",
            "For example, here is a four dimensional subspace of edge filters where the subspace in one network predicts the subspace.",
            "Another here is afforded."
        ],
        [
            "Subspace of colored edge filters and so on.",
            "This is very much not quite a complete story yet, but it's a kind of first step toward finding these low dimensional subspaces."
        ],
        [
            "To kind of wrap up quickly, sometimes we find some local codes are being used like that black and white neuron.",
            "Other times we have some evidence.",
            "Some preliminary hints.",
            "Let's say of a partially distributed hypothesis being true, at least on some layers, and we definitely don't understand some parts of what's going on so."
        ],
        [
            "To summarize a different way, I think this is actually a pretty interesting research.",
            "Trajectory would be really cool to see anyone work on this.",
            "This paper is really just a first like small step along this direction of training.",
            "Many networks comparing between them and seeing what they can teach us about the representations that are used."
        ],
        [
            "Like to better understand the personally distributed case in the future.",
            "If we could, we'd like to obviously ask him how does this vary with architecture, so do different types of architectures use fundamentally different representations, and is that good or bad?",
            "And if that's good then you know should be encouraged.",
            "Certain types of representations by, for example, having limited limited connectivity or other penalties during training."
        ],
        [
            "Before I end in one minute, I just want to give a completely shameless plug, so."
        ],
        [
            "I am not working at this company called Geometric Intelligence with Gary Marcus.",
            "Senzu bean were really small.",
            "We're getting off the ground now.",
            "If you're interested in maybe joining a very small group of researchers doing some machine learning work where your work would have a fairly large impact, talk to me after."
        ],
        [
            "If you're interested in our code, it's all online here on each band's website you can email any of us with questions.",
            "I'd like to say."
        ],
        [
            "Thanks to my coauthors, and thanks to you guys."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool, thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Before I get started I just wanted to say that.",
                    "label": 0
                },
                {
                    "sent": "A lot of this talk is the work of each family here on the on the left.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She's a very bright and hardworking student.",
                    "label": 0
                },
                {
                    "sent": "She should be giving this talk, but she couldn't make it for some travel reasons.",
                    "label": 0
                },
                {
                    "sent": "So you're stuck with me instead.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to start by introducing this concept of a grandmother neuron.",
                    "label": 1
                },
                {
                    "sent": "So many of you may be familiar with this concept.",
                    "label": 0
                },
                {
                    "sent": "The idea is that somewhere in your brain there's maybe a single neuron kind of corresponds to the concept of your grandmother.",
                    "label": 0
                },
                {
                    "sent": "So when we say corresponds, we made a couple things.",
                    "label": 0
                },
                {
                    "sent": "So first that means anytime you see your grandmother or maybe think about her, this neuron should fire.",
                    "label": 0
                },
                {
                    "sent": "And it also means that it fires only for your grandmother.",
                    "label": 0
                },
                {
                    "sent": "In other words, we sort of 1 to one relationship.",
                    "label": 0
                },
                {
                    "sent": "This entailment is if is only if if and only of between the concept of your grandmother.",
                    "label": 0
                },
                {
                    "sent": "And the firing of this neuron.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, we're going to call this type of representation a local representation.",
                    "label": 0
                },
                {
                    "sent": "There's some evidence from the neuroscience literature that such representations do exist in the brain.",
                    "label": 0
                },
                {
                    "sent": "You may have seen this paper a decade ago or so by Quiroga at all, where they found sort of a Jennifer Aniston neuron.",
                    "label": 1
                },
                {
                    "sent": "This paper is a little bit controversial, but we won't get into that at all today.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I'm not a neuroscientist and I guess most of you are probably not either.",
                    "label": 0
                },
                {
                    "sent": "So, so this is 1 possible representation strategy, but it's not the only one that could be playing itself out in the brain or in artificial neural networks.",
                    "label": 0
                },
                {
                    "sent": "Could could be an alternative hypothesis, like representations could be distributed.",
                    "label": 0
                },
                {
                    "sent": "So for example, the concept of your grandmother could be represented only ever as the pattern taken across several dimensions.",
                    "label": 0
                },
                {
                    "sent": "Several neurons never across a single neuron.",
                    "label": 0
                },
                {
                    "sent": "And a key thing we could look for here is that these same neurons would be repurposed for other things as well, like representing the concept of your cat.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general, in the neurosciences opposed people would like to measure for whether these you know which types of representational strategies are are used in the brain.",
                    "label": 0
                },
                {
                    "sent": "And this is in general quite a hard thing to measure because it's difficult to stick needles in humans and take lots of data and so on.",
                    "label": 0
                },
                {
                    "sent": "But you and I are very lucky because we work on these types of networks and we can basically measure anything we want.",
                    "label": 0
                },
                {
                    "sent": "So shouldn't we be able to look for these types of representations in the networks we study?",
                    "label": 0
                },
                {
                    "sent": "So before I go further, many of you may be thinking you know, isn't this actually already resolved?",
                    "label": 0
                },
                {
                    "sent": "There was this great.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Super by Christian Szegedy at all in 2013 and they showed some evidence that you know representations are distributed is just a big tangled mess, and you can't really do anything else about it.",
                    "label": 1
                },
                {
                    "sent": "Well, for various reasons we can talk about it.",
                    "label": 0
                },
                {
                    "sent": "The poster session.",
                    "label": 0
                },
                {
                    "sent": "We think this is kind of an incomplete story.",
                    "label": 0
                },
                {
                    "sent": "We do things.",
                    "label": 0
                },
                {
                    "sent": "Some representations are at least partially distributed, but they actually showed this result only for the second last layer of the network, not everywhere in the network.",
                    "label": 0
                },
                {
                    "sent": "And there's a little more subtlety going on there that we could talk about later also in work with.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with my collaborators, we showed that in the middle of some networks you see kind of uniquely or locally represented concepts like spiders and detectors for water and detectives for text, which seem to be fairly more locally representational.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's imagine you bye bye.",
                    "label": 0
                },
                {
                    "sent": "My argument for now that something more subtle might be going on.",
                    "label": 0
                },
                {
                    "sent": "We have our neural network here on the right, and we'd like to test for the presence or absence of these different types of representational codes.",
                    "label": 0
                },
                {
                    "sent": "So how might we actually go about doing that?",
                    "label": 0
                },
                {
                    "sent": "One method you can imagine is we assemble a list of distinct concepts.",
                    "label": 0
                },
                {
                    "sent": "We'd like to test for.",
                    "label": 0
                },
                {
                    "sent": "So Grandmother and cat and stop sign in lawnmower and so on, and then we assemble kind of labeled training set, corresponding maybe images with each of these labels.",
                    "label": 1
                },
                {
                    "sent": "We push all the images through the network we see if there's a neuron that always fires her grandmother never paid for anything else, and so on, and that would kind of sort of work.",
                    "label": 0
                },
                {
                    "sent": "We could test for the local hypothesis this way.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only this method would require lots and lots of concept labels, so a larger label data set that we actually have.",
                    "label": 0
                },
                {
                    "sent": "It would be a bit of a hurry.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In effort to collect this entire human annotated data set.",
                    "label": 0
                },
                {
                    "sent": "However, actually, if you were here, had a clear last year bolage out took a step up to the Mike.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "He took a stab at being Hercules and actually had a bunch of Mechanical Turkers label individual units in a neural network and he found actually some local concepts like lamps and table legs represented halfway through a network in general.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this paper was was quite amazing.",
                    "label": 0
                },
                {
                    "sent": "However, it's not necessarily going to scale to large, more and more concepts because you need to pay lots of Turkers.",
                    "label": 0
                },
                {
                    "sent": "Also, there's actually a more subtle issue going on, which is.",
                    "label": 0
                },
                {
                    "sent": "This approach tends to work for when your concepts have labels that are recognizable to a human, so for example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grandmother Cat stop sign and so on are all concepts that you might find in a dictionary.",
                    "label": 1
                },
                {
                    "sent": "There are things that humans found so important that we wrote down, but maybe there are other concepts that are equally important that we haven't given words too.",
                    "label": 1
                },
                {
                    "sent": "So basically what what counts is the concept.",
                    "label": 0
                },
                {
                    "sent": "Is it just words in a dictionary or something else?",
                    "label": 0
                },
                {
                    "sent": "For example, maybe some neuron in your brain or in an artificial neural network would respond to the reflection of the sun off of the water.",
                    "label": 0
                },
                {
                    "sent": "So is this an important concept or not?",
                    "label": 1
                },
                {
                    "sent": "Also, maybe the particular Tufts of hair on behind dogs ears that.",
                    "label": 0
                },
                {
                    "sent": "Used to tell Yorkshire terriers from Norwich Terriers.",
                    "label": 0
                },
                {
                    "sent": "It's probably not a concept that matters to any of you unless you are dog breeders or you've been working on image net for way way too long.",
                    "label": 0
                },
                {
                    "sent": "So basically what we don't want to do is we don't want to bias or search for concept stored what humans would label this concept.",
                    "label": 0
                },
                {
                    "sent": "We want to look for kind of a more general approach.",
                    "label": 0
                },
                {
                    "sent": "So in this study were going to make this key definition and assumption that kind of addresses the first concern and answers the second question all at once, and we can argue about whether this is a good assumption or not.",
                    "label": 0
                },
                {
                    "sent": "But we're just going to make it so we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assumption that concepts are just feature subspaces that are reliably learned in multiple networks.",
                    "label": 1
                },
                {
                    "sent": "So to give us more concretely, if many humans end up learning somewhere in the brain, a detector for cats, that means cats are a concept that's worth considering, as like a unique.",
                    "label": 0
                },
                {
                    "sent": "Concept that stands on its own.",
                    "label": 0
                },
                {
                    "sent": "I guess if many NNS learn a feature for the Sun reflecting off of water, then that also is a concept that deserves to be given kind of.",
                    "label": 0
                },
                {
                    "sent": "First Citizen ships that is, and also we should note that these concepts can be multidimensional.",
                    "label": 0
                },
                {
                    "sent": "So if all of us learn just banned a space of colors and color is a concept that's worth considering as a concept.",
                    "label": 0
                },
                {
                    "sent": "Whether we actually span it using RGB or HSV.",
                    "label": 0
                },
                {
                    "sent": "Still, it's now like a 3 dimensional concept that we should consider as a first class citizen.",
                    "label": 0
                },
                {
                    "sent": "So this definition assumption leads us sort of directly to our key.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Technique of this study, which is that we can probe for different types of representation, local or distributed by comparing multiple networks.",
                    "label": 0
                },
                {
                    "sent": "Trains starting from different random initializations.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we train multiple networks from different random initializations.",
                    "label": 1
                },
                {
                    "sent": "Anna local code is being used.",
                    "label": 0
                },
                {
                    "sent": "We would expect to find features in one network that are relearned by the SEC Network but may be subject to a position permutation.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in the completely distributed case we may find that units are only used as sort of arbitrary basis vectors to span the entire representation space.",
                    "label": 0
                },
                {
                    "sent": "And the basis vectors in one network may be kind of a rotated version of the basis vectors in another network.",
                    "label": 0
                },
                {
                    "sent": "We also admit a sort of intermediate hypothesis called the partially distributed hypothesis.",
                    "label": 0
                },
                {
                    "sent": "In this case, we expect to see low dimensional subspaces that are reliably learned in multiple networks.",
                    "label": 0
                },
                {
                    "sent": "But where within that subspace the basis vectors in one, maybe a rotation of the basis vectors in another.",
                    "label": 0
                },
                {
                    "sent": "So to show this in a slightly different way.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the local case, we expect the basis vectors for one network to be almost perfectly aligned with the vectors in another network.",
                    "label": 0
                },
                {
                    "sent": "In the partially distributed case, we expect rotations through low dimensional subspaces for one net versus the other.",
                    "label": 1
                },
                {
                    "sent": "For example, maybe in only two out of the three dimensions here and in the fully distributed case we expect kind of arbitrary rotations through all feature dimensions.",
                    "label": 0
                },
                {
                    "sent": "In this day.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use the Alex net architecture trained on image that the architecture is identical, just the initializations are different.",
                    "label": 1
                },
                {
                    "sent": "If you're curious, the performance of both networks is very, very similar.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start by probing for the presence or absence of this local code.",
                    "label": 0
                },
                {
                    "sent": "To do this, first we'll see if we can find units that match, well, one to one between the two networks.",
                    "label": 0
                },
                {
                    "sent": "To find matching units one to one, what we do is we push all of Imagenet through the networks.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we compute correlation statistics between neuron activations in one network versus the other.",
                    "label": 1
                },
                {
                    "sent": "Just to be clear, we only ever compare neurons on the same layer from one network to another, never across different layers of the networks.",
                    "label": 0
                },
                {
                    "sent": "So first just to start with, let's compute the correlation of all units in net, one with itself on some layer here, one.",
                    "label": 0
                },
                {
                    "sent": "So if we visualize this as a matrix and we take the absolute value, we see correlation values at Max of 1 along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So these white diagonal is correlation values of one with a unit with itself.",
                    "label": 0
                },
                {
                    "sent": "Of course it should be one.",
                    "label": 0
                },
                {
                    "sent": "Everywhere else you see values that are generally quite small, because features are generally fairly orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Although you see some bright spots where you know multiple features kind of overlap in their representation somehow.",
                    "label": 0
                },
                {
                    "sent": "For Network two we see a very similar very similar picture.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally, what we're interested in is this matrix, so it's the between net or cross net correlation between net one and net 2.",
                    "label": 0
                },
                {
                    "sent": "So as a brief aside, you might be wondering, you know.",
                    "label": 0
                },
                {
                    "sent": "Correlations are very simple measure.",
                    "label": 0
                },
                {
                    "sent": "Is that actually powerful enough to identify the types of correspondences we'd like to?",
                    "label": 0
                },
                {
                    "sent": "We also under this.",
                    "label": 0
                },
                {
                    "sent": "So in addition to trying correlation, we have a whole section of the paper.",
                    "label": 0
                },
                {
                    "sent": "We try mutual information.",
                    "label": 0
                },
                {
                    "sent": "We estimate it by using some histograms and the short version of the story is it's all the same.",
                    "label": 0
                },
                {
                    "sent": "You can check out the supplementary information from word about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Victor Matrix we have this between that correlation matrix net.",
                    "label": 0
                },
                {
                    "sent": "Wonder net two for con fun and.",
                    "label": 1
                },
                {
                    "sent": "We're interested in seeing in this matrix of these these bright areas, so these bright pixels are entries where neuron in that one in the another neuron in net two are kind of firing all the time for the same thing.",
                    "label": 0
                },
                {
                    "sent": "So to find the actual matches between network and the other, we can take a couple of approaches.",
                    "label": 1
                },
                {
                    "sent": "So a straightforward approach is to use what we call greedy matching.",
                    "label": 0
                },
                {
                    "sent": "Where we simply pick the Max along each row.",
                    "label": 1
                },
                {
                    "sent": "So for each neuron in one network, find the most predictive or most correlated unit in the other network.",
                    "label": 0
                },
                {
                    "sent": "We can pick the Max along each row and kind of visualize these matches in the following way so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each neuron in any any network, we visualize it like this.",
                    "label": 0
                },
                {
                    "sent": "We show on the top the top nine patches that cause high activation for this unit and we showed the deconvolution of those top line patches from Zeiler and Fergus 2014.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the network, sorry, here's a unit in the first network and its corresponding unit in the second network.",
                    "label": 0
                },
                {
                    "sent": "Here we see both networks.",
                    "label": 0
                },
                {
                    "sent": "Learn units that correlate with a value of 1, indicating that they learned kind of very much the same concept.",
                    "label": 0
                },
                {
                    "sent": "The same kind of green detector, suggesting that maybe this color spaces not just stand by arbitrary vectors, but maybe this single dimension was very important.",
                    "label": 0
                },
                {
                    "sent": "It also could have been that spanned by different factors and you just got really lucky that they correlated so perfectly.",
                    "label": 0
                },
                {
                    "sent": "You know they both.",
                    "label": 0
                },
                {
                    "sent": "Happened to spend it the same way, but they didn't need to spend it the same way.",
                    "label": 0
                },
                {
                    "sent": "If we look for other matches, we can find others.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example here, here's two horizontal edge detectors which are very highly correlated, and if we keep looking we can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Others that are less and less correlated, detecting other types of features.",
                    "label": 0
                },
                {
                    "sent": "Eventually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To continue the process for all units in common, we can see that there are some units here on the right.",
                    "label": 0
                },
                {
                    "sent": "They have no great match.",
                    "label": 0
                },
                {
                    "sent": "For example, some of those units on the far right, the best match unit in the SEC Network has correlation only like .2 or .3, with the unit in the first network.",
                    "label": 0
                },
                {
                    "sent": "This suggests that these features are kind of unique and they will learn once but not the second time.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can follow the same process for com two finding it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a highly correlated units that are recognizing speckled textures, grids and green objects, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repeat the process for COM 3 actually income three.",
                    "label": 0
                },
                {
                    "sent": "We see something really cool.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think it's really cool, so here's a here's a local code.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The left here.",
                    "label": 0
                },
                {
                    "sent": "It's a single neuron that learns to represent the concept of black and white objects, and it's learned in both networks, and it's very highly correlated.",
                    "label": 0
                },
                {
                    "sent": "So this, to me means like this concept.",
                    "label": 0
                },
                {
                    "sent": "Even though you might not have thought of enumerating this concept initially, we're starting to study.",
                    "label": 0
                },
                {
                    "sent": "It means this concept is actually present in the network, and it's actually learned reliably.",
                    "label": 0
                },
                {
                    "sent": "So maybe it deserves to be considered as like a first class concept that has local representation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check out the paper.",
                    "label": 0
                },
                {
                    "sent": "You can see other other concepts along this road.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So back to our matrix here.",
                    "label": 0
                },
                {
                    "sent": "What I just showed you was the greedy matching approach where we pick the Max along each row.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is done with replacement, so multiple units in that one could be paired with the same unit in net two.",
                    "label": 0
                },
                {
                    "sent": "We don't necessarily have to do it this way.",
                    "label": 0
                },
                {
                    "sent": "Alternately, we could find the best match without replacement, so that every unit 1 network has a sort of unique counterpart in the other network.",
                    "label": 0
                },
                {
                    "sent": "We can do this by finding the Max weighted bipartite matching, so this matching allows us to take two networks.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, with units that are not quite aligned and to bring them into the best possible alignment.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we visualize the correlation matrix between the approximately aligned networks, it looks something like this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this shows.",
                    "label": 0
                },
                {
                    "sent": "This shows along the diagonal you can see some relatively bright diagonal showing the cases where the Max weighted bipartite matching was kind of successful in finding a great pair.",
                    "label": 1
                },
                {
                    "sent": "Other parts of the diagonal or not so bright, showing that those units did not find a good partner.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In some cases the match is found in.",
                    "label": 0
                },
                {
                    "sent": "Both methods are the same in other cases that matches found are different and we can actually gain additional insight by comparing the ones that are the same with the ones that are different.",
                    "label": 0
                },
                {
                    "sent": "We do though.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By plotting each unit along the X axis with the correlation of its match along the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here, the correlation of the Max matches shown in light green, which is always higher than the correlation, or at least as high as the correlation shown by the Max weighted bipartite matching.",
                    "label": 0
                },
                {
                    "sent": "And we sort the units on the X axis by their correlation value simply for clarity.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple of observations we can draw from this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at these high correlation pairs in the beginning, there matches found by both methods that coincide and they happen to have fairly high correlation.",
                    "label": 0
                },
                {
                    "sent": "If we think back to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local Distributed thought experiment.",
                    "label": 0
                },
                {
                    "sent": "This is what we would expect to see in the cases where local code is being used.",
                    "label": 0
                },
                {
                    "sent": "We can also find some units in NA.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And without a high correlation matching that too.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indicating that these features are, like we said, unique to one network or the other.",
                    "label": 0
                },
                {
                    "sent": "This could be we're not actually sure entirely what this means.",
                    "label": 0
                },
                {
                    "sent": "It could be that the features are just learned here and they're completely missed by the SEC Network, which would suggest one possible reason why ensemble methods might be effective.",
                    "label": 0
                },
                {
                    "sent": "Finally, we see this kind of interesting little read.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where in some?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the matches are not the same between one method and the other.",
                    "label": 0
                },
                {
                    "sent": "So in some of these cases where they differ, the bipartite matching is much worse.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this huge spike in the middle, what do you?",
                    "label": 0
                },
                {
                    "sent": "What do you think is going on there?",
                    "label": 0
                },
                {
                    "sent": "So let's imagine for a second that the partially distributed hypothesis is true.",
                    "label": 0
                },
                {
                    "sent": "If it was what we expected.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "So we might.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, CNET one learn to span a subspace of say, human faces using 4 units, but net two learns to spend the same subspace with only three units.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greedy matching, we might find these pairings with unique bipartite matching.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, three of the four you know.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From that one may be matched up to their nearest counterpart, net two, as shown.",
                    "label": 0
                },
                {
                    "sent": "But then the 4th unit, although it has a relatively high correlation counterpart to those three units, are already taken, and it ends up being paired with an almost unrelated filter.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here so we don't take this to be proof positive of a partially distributed hypothesis, but we take it to be a hint that such a code might be being used.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper, if you check it out, you can see similar plots for the first 5, first 5 layers.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so far I've showed you.",
                    "label": 0
                },
                {
                    "sent": "A certain way of looking for possible one to one matchings, and we found that there are some and that explains kind of some of the representations but not everything.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole lot of the rest of the network that's not well explained by this one hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do next, and I'll have to go kind of quickly through this, is find kind of.",
                    "label": 0
                },
                {
                    "sent": "So take those locally represented things and kind of chop them off and then try to find if we can see if we can find low dimensional subspaces where it's not a single dimension that predicts a single other dimension here, but it's kind of like a few dimensions here that predicts a few dimensions here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, skipping some details for time, we basically learned mapping layers from one network to another, so we predict one networks activations from the other.",
                    "label": 0
                },
                {
                    "sent": "We do this using first a dense weight matrix which works fairly well.",
                    "label": 0
                },
                {
                    "sent": "Then we kind of have this nob, where we have an L1 sparsity penalty.",
                    "label": 0
                },
                {
                    "sent": "We slowly turn that up.",
                    "label": 0
                },
                {
                    "sent": "And we find that as we turn it up, we can actually get very sparse solutions without suffering a loss of performance, indicating that actually you don't need the full dimension to predict one network to another.",
                    "label": 0
                },
                {
                    "sent": "You can actually use a very small subset.",
                    "label": 0
                },
                {
                    "sent": "For convoy and it's generally four or five units to predict one of the units accurately.",
                    "label": 0
                },
                {
                    "sent": "Eventually, if you.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get to sparse prediction fails for comp two.",
                    "label": 0
                },
                {
                    "sent": "We see similar results for Common 3, four and five.",
                    "label": 0
                },
                {
                    "sent": "The picture is a little trickier and I refer you to the paper for that.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in interest?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's here's an example mapping matrix from one layer to from one network to another.",
                    "label": 0
                },
                {
                    "sent": "You can see as much sparser than our correlation matrix we found before.",
                    "label": 0
                },
                {
                    "sent": "But the prediction is still just about as good, so if we take a few.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prices through this matrix we can see some interesting stuff going on.",
                    "label": 0
                },
                {
                    "sent": "For example, here is 1 unit 2 that's predicted very well using the activations of three units in that one.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another another slice.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through that matrix.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the few.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To one mapping, but this still isn't the full partially distributed hypothesis where we have, you know, several to several mappings.",
                    "label": 0
                },
                {
                    "sent": "So to find that.",
                    "label": 0
                },
                {
                    "sent": "It's even harder and we don't have a super great method.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet.",
                    "label": 0
                },
                {
                    "sent": "We are using something called hierarchical agglomerative clustering.",
                    "label": 0
                },
                {
                    "sent": "For now, this produces.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is kind of a tree structure which we can then use to permute the units in both networks to kind of align them with this tree.",
                    "label": 0
                },
                {
                    "sent": "The tree is generated such that Co predictive clusters end up together if we zoom.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some of these Co project.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clusters we can eventually see.",
                    "label": 0
                },
                {
                    "sent": "For example, here is a four dimensional subspace of edge filters where the subspace in one network predicts the subspace.",
                    "label": 0
                },
                {
                    "sent": "Another here is afforded.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subspace of colored edge filters and so on.",
                    "label": 0
                },
                {
                    "sent": "This is very much not quite a complete story yet, but it's a kind of first step toward finding these low dimensional subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To kind of wrap up quickly, sometimes we find some local codes are being used like that black and white neuron.",
                    "label": 0
                },
                {
                    "sent": "Other times we have some evidence.",
                    "label": 0
                },
                {
                    "sent": "Some preliminary hints.",
                    "label": 0
                },
                {
                    "sent": "Let's say of a partially distributed hypothesis being true, at least on some layers, and we definitely don't understand some parts of what's going on so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To summarize a different way, I think this is actually a pretty interesting research.",
                    "label": 0
                },
                {
                    "sent": "Trajectory would be really cool to see anyone work on this.",
                    "label": 0
                },
                {
                    "sent": "This paper is really just a first like small step along this direction of training.",
                    "label": 0
                },
                {
                    "sent": "Many networks comparing between them and seeing what they can teach us about the representations that are used.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like to better understand the personally distributed case in the future.",
                    "label": 1
                },
                {
                    "sent": "If we could, we'd like to obviously ask him how does this vary with architecture, so do different types of architectures use fundamentally different representations, and is that good or bad?",
                    "label": 1
                },
                {
                    "sent": "And if that's good then you know should be encouraged.",
                    "label": 0
                },
                {
                    "sent": "Certain types of representations by, for example, having limited limited connectivity or other penalties during training.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I end in one minute, I just want to give a completely shameless plug, so.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am not working at this company called Geometric Intelligence with Gary Marcus.",
                    "label": 1
                },
                {
                    "sent": "Senzu bean were really small.",
                    "label": 0
                },
                {
                    "sent": "We're getting off the ground now.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in maybe joining a very small group of researchers doing some machine learning work where your work would have a fairly large impact, talk to me after.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're interested in our code, it's all online here on each band's website you can email any of us with questions.",
                    "label": 0
                },
                {
                    "sent": "I'd like to say.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks to my coauthors, and thanks to you guys.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}