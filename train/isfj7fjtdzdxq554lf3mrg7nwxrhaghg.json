{
    "id": "isfj7fjtdzdxq554lf3mrg7nwxrhaghg",
    "title": "Parallel Splash Gibbs Sampling",
    "info": {
        "author": [
            "Joseph Gonzalez, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_gonzalez_psg/",
    "segmentation": [
        [
            "So this is joint work with Yucheng Low arthurette."
        ],
        [
            "And Carlos question.",
            "So we looked at the Gibbs sampling algorithm to classic sampling algorithm, which we begin with the initial assignment.",
            "All of our variables in a large graphical model.",
            "Then we select some variable sequentially.",
            "We select the variable and we condition on the adjacent assignments.",
            "Then based not, we flip the value that variable and this gives us a new joint simple, it will repeat this process sequentially during a long sequence of."
        ],
        [
            "Apples, which we can then use to answer interesting questions and machine learning.",
            "Now what's interesting and people haven't really notices that the original paper on Gibbs sampling actually gave an algorithm for parallel Gibbs sampling, and the algorithm is you assign processors to each other.",
            "Variables in R model and then you flip all the variables simultaneously.",
            "And this is what's actually being now used, but unfortunately this converges to the wrong distribution.",
            "OK, in the sea."
        ],
        [
            "I have a little toy example, so I have two variables with strong positive correlations or binary random variables.",
            "If I run the sequential algorithm and I flipped the first variable it with high probability match the other variable in my model.",
            "If I repeat this process, we get along chain of similar variables.",
            "Now if I run this in the parallel setting, each of these variables are going to flip simultaneously and try to match the old assignment of the other variable.",
            "In this case, I get a sequence of samples that alternate between assignments.",
            "The first chain demonstrates strong positive correlation matching with the model reflects the second chain.",
            "Has a strong negative correlation complete opposite of what the models encoding?",
            "This is why this could be a bad strategy for sampling from this model.",
            "So the idea is that adjacent variables cannot be sampled simultaneously and is."
        ],
        [
            "This problem we actually went to a very classic technique in parallel computing and use graph coloring.",
            "So we introduce chromatic sampler where we begin by coloring our graph.",
            "This case we have a two coloring and then we sample all the variables in the same color in parallel like that.",
            "So and then we can do that again for the blue variables as we have in our parallel algorithm, which has a neat property.",
            "Did sequentially consistent.",
            "We can rearrange our variables onto a single line and show that's equivalent to a sequence."
        ],
        [
            "Execution of a good Templar and as a consequence we can prove that it converges to the correct distribution.",
            "Now there correct parallel Gibbs sampler.",
            "Not only that, we can also measure the improvement in mixing."
        ],
        [
            "So the next thing that we might ask is using this idea.",
            "Can we quantify the behavior of the original station or the synchronous Kepler that's being used throughout machine learning right now and actually look at two valuable models we can show that these synchronous distribution converges to a distribution with the stationary distribution encodes an independence assumption between the variables in different colors.",
            "So if I look at variables with within one color, I get the right marginal estimates, but if you look at functions or variables across colors I get the wrong answer, so we can actually quantify that.",
            "But as a corollary, of course we can show that the synchronous Gibbs sampler.",
            "In the case of two colorable models, is actually a valid Gibbs sampler to use if all we want to do is answer questions about individual variable marginals."
        ],
        [
            "OK.",
            "So one of the other things you might ask when you're building a Gibbs sampler is how do you deal with situations you have strong dependencies, and so when you have single variable updates is what the Gibbs sampler is built around.",
            "This can actually be a problem, and so ideally we'd like to draw joint samples and that is not drawing samples synchronously, but actually drawing the joint assignment to a large collection of variables from the joint conditional."
        ],
        [
            "This is called blocking.",
            "And so to do this, we introduced the Splash Gibbs sampler.",
            "So this is an asynchronous Gibbs sampler that adaptively addresses."
        ],
        [
            "Strong dependencies the way it works is we begin by growing multiple parallel slashes and these are spanning trees in our Markov random field, and we can actually grow each of these individually in parallel.",
            "And of course we can grow many of them in parallel, and we ensure that these splashes are conditionally independent, so we can draw joint samples on each splash at the same time.",
            "Alright, then, having these drawn these trees, we use message passing to actually Kaleb."
        ],
        [
            "These junction trees and we can do this again in parallel."
        ],
        [
            "And using the messages we cannot draw joint samples.",
            "And here you'll see many coins flipping simultaneously.",
            "These can all be run in parallel and draw correct joint samples.",
            "And once this is complete, we can repeat this process drawing new."
        ],
        [
            "Peace.",
            "And one thing we can do that's very cool is we can actually adapt these trees to strong dependencies in the model.",
            "So in the left we have a noisy image in the middle.",
            "These are trees drawn using just a regular BFS search on the right.",
            "These are trees drawn using a weighted BFS search that actually sort of adapts to the regions that need to be flipped similar."
        ],
        [
            "Miss Lee and we can show this convergence the correct distribution.",
            "And so some brief experimental results.",
            "We tested this on a large Markov logic network with 10,000 variables in 28,000 factors.",
            "If we look at just the improvement likely to splash technique on these strongly dependent variables actually has a big impact in improving weight of our samples.",
            "We can also show that improves mixing.",
            "And of course we can get a substantial speedup in the generation of samples, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Yucheng Low arthurette.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Carlos question.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the Gibbs sampling algorithm to classic sampling algorithm, which we begin with the initial assignment.",
                    "label": 1
                },
                {
                    "sent": "All of our variables in a large graphical model.",
                    "label": 0
                },
                {
                    "sent": "Then we select some variable sequentially.",
                    "label": 0
                },
                {
                    "sent": "We select the variable and we condition on the adjacent assignments.",
                    "label": 0
                },
                {
                    "sent": "Then based not, we flip the value that variable and this gives us a new joint simple, it will repeat this process sequentially during a long sequence of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples, which we can then use to answer interesting questions and machine learning.",
                    "label": 0
                },
                {
                    "sent": "Now what's interesting and people haven't really notices that the original paper on Gibbs sampling actually gave an algorithm for parallel Gibbs sampling, and the algorithm is you assign processors to each other.",
                    "label": 1
                },
                {
                    "sent": "Variables in R model and then you flip all the variables simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And this is what's actually being now used, but unfortunately this converges to the wrong distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, in the sea.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a little toy example, so I have two variables with strong positive correlations or binary random variables.",
                    "label": 0
                },
                {
                    "sent": "If I run the sequential algorithm and I flipped the first variable it with high probability match the other variable in my model.",
                    "label": 0
                },
                {
                    "sent": "If I repeat this process, we get along chain of similar variables.",
                    "label": 0
                },
                {
                    "sent": "Now if I run this in the parallel setting, each of these variables are going to flip simultaneously and try to match the old assignment of the other variable.",
                    "label": 0
                },
                {
                    "sent": "In this case, I get a sequence of samples that alternate between assignments.",
                    "label": 0
                },
                {
                    "sent": "The first chain demonstrates strong positive correlation matching with the model reflects the second chain.",
                    "label": 0
                },
                {
                    "sent": "Has a strong negative correlation complete opposite of what the models encoding?",
                    "label": 1
                },
                {
                    "sent": "This is why this could be a bad strategy for sampling from this model.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that adjacent variables cannot be sampled simultaneously and is.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem we actually went to a very classic technique in parallel computing and use graph coloring.",
                    "label": 0
                },
                {
                    "sent": "So we introduce chromatic sampler where we begin by coloring our graph.",
                    "label": 0
                },
                {
                    "sent": "This case we have a two coloring and then we sample all the variables in the same color in parallel like that.",
                    "label": 1
                },
                {
                    "sent": "So and then we can do that again for the blue variables as we have in our parallel algorithm, which has a neat property.",
                    "label": 0
                },
                {
                    "sent": "Did sequentially consistent.",
                    "label": 0
                },
                {
                    "sent": "We can rearrange our variables onto a single line and show that's equivalent to a sequence.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Execution of a good Templar and as a consequence we can prove that it converges to the correct distribution.",
                    "label": 1
                },
                {
                    "sent": "Now there correct parallel Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "Not only that, we can also measure the improvement in mixing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next thing that we might ask is using this idea.",
                    "label": 0
                },
                {
                    "sent": "Can we quantify the behavior of the original station or the synchronous Kepler that's being used throughout machine learning right now and actually look at two valuable models we can show that these synchronous distribution converges to a distribution with the stationary distribution encodes an independence assumption between the variables in different colors.",
                    "label": 0
                },
                {
                    "sent": "So if I look at variables with within one color, I get the right marginal estimates, but if you look at functions or variables across colors I get the wrong answer, so we can actually quantify that.",
                    "label": 0
                },
                {
                    "sent": "But as a corollary, of course we can show that the synchronous Gibbs sampler.",
                    "label": 1
                },
                {
                    "sent": "In the case of two colorable models, is actually a valid Gibbs sampler to use if all we want to do is answer questions about individual variable marginals.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one of the other things you might ask when you're building a Gibbs sampler is how do you deal with situations you have strong dependencies, and so when you have single variable updates is what the Gibbs sampler is built around.",
                    "label": 0
                },
                {
                    "sent": "This can actually be a problem, and so ideally we'd like to draw joint samples and that is not drawing samples synchronously, but actually drawing the joint assignment to a large collection of variables from the joint conditional.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is called blocking.",
                    "label": 0
                },
                {
                    "sent": "And so to do this, we introduced the Splash Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "So this is an asynchronous Gibbs sampler that adaptively addresses.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strong dependencies the way it works is we begin by growing multiple parallel slashes and these are spanning trees in our Markov random field, and we can actually grow each of these individually in parallel.",
                    "label": 0
                },
                {
                    "sent": "And of course we can grow many of them in parallel, and we ensure that these splashes are conditionally independent, so we can draw joint samples on each splash at the same time.",
                    "label": 1
                },
                {
                    "sent": "Alright, then, having these drawn these trees, we use message passing to actually Kaleb.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These junction trees and we can do this again in parallel.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And using the messages we cannot draw joint samples.",
                    "label": 0
                },
                {
                    "sent": "And here you'll see many coins flipping simultaneously.",
                    "label": 0
                },
                {
                    "sent": "These can all be run in parallel and draw correct joint samples.",
                    "label": 1
                },
                {
                    "sent": "And once this is complete, we can repeat this process drawing new.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Peace.",
                    "label": 0
                },
                {
                    "sent": "And one thing we can do that's very cool is we can actually adapt these trees to strong dependencies in the model.",
                    "label": 0
                },
                {
                    "sent": "So in the left we have a noisy image in the middle.",
                    "label": 1
                },
                {
                    "sent": "These are trees drawn using just a regular BFS search on the right.",
                    "label": 1
                },
                {
                    "sent": "These are trees drawn using a weighted BFS search that actually sort of adapts to the regions that need to be flipped similar.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miss Lee and we can show this convergence the correct distribution.",
                    "label": 0
                },
                {
                    "sent": "And so some brief experimental results.",
                    "label": 0
                },
                {
                    "sent": "We tested this on a large Markov logic network with 10,000 variables in 28,000 factors.",
                    "label": 1
                },
                {
                    "sent": "If we look at just the improvement likely to splash technique on these strongly dependent variables actually has a big impact in improving weight of our samples.",
                    "label": 0
                },
                {
                    "sent": "We can also show that improves mixing.",
                    "label": 0
                },
                {
                    "sent": "And of course we can get a substantial speedup in the generation of samples, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}