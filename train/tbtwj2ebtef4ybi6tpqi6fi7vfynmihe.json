{
    "id": "tbtwj2ebtef4ybi6tpqi6fi7vfynmihe",
    "title": "Predicting Tissue-Dependent Alternative Splicing Using Bayesian Neural Networks",
    "info": {
        "author": [
            "Hui Xiong, Management Science and Information Systems Department, Rutgers, The State University of New Jersey"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_xiong_ptdasub/",
    "segmentation": [
        [
            "OK, so today I will present our work on predicting tissue dependent alternative splicing using neural Nets.",
            "This is joint work with our Jozef Brush and Brandon's try at U of T."
        ],
        [
            "OK, here is some biological background about alternative splicing, so it's a very important phenomenon.",
            "Alternative splicing is the phenomenon that one gene can produce many transcripts, and this happens a lot in mammals.",
            "For example, in humans there are 90% of genes are Alternatively spliced, and it turns about 20,000 human genes to hundreds of thousands of M RNA in humans and is related to.",
            "Many disease, by one estimation 25 to 50% disease linked to problematic alternative spicy and also some other functions including neurons in our brain from connections from the correct connections by using alternative splicing OK.",
            "So the most common type of alternative splicing is the cassette splicing or the exon skipping event.",
            "So this is the problem we are going to focus on in this presentation.",
            "Here is a schematic, so this is the DNA which is the same as the unspliced RNA.",
            "On here we have three exons.",
            "3 boxes The middle Exxon is the alternative for Exxon, so this one can either be included or excluded, and these are flanking.",
            "Constitutive exons are always always present.",
            "And this difference is tissue dependent.",
            "For example, in brain this Axon may be more inclusive than in muscle, and this work is about.",
            "Predicting the tissue dependent differences.",
            "Are of the inclusion level of the of the Axon of the alternative exon."
        ],
        [
            "OK, here is our problem formulation.",
            "We formulate this problem as a machine learning problem and this is the block diagram.",
            "So on the top here are the DNA sequences and we extract RNA.",
            "Feature set from the eyes.",
            "This is unsupervised on a sequence and this feature set is fit into the splicing code.",
            "Along with the tissue type and the splicing code, can predict whether there is more inclusion or more exclusion in a certain type then compared to Delta X on the average of.",
            "The inclusion level of Death X across all tissues.",
            "So this talk we will discuss.",
            "I'll describe algorithm for constructing this splicing code.",
            "And we had about 3000 training examples.",
            "That we measured the target.",
            "And also extracted the features.",
            "And using this we learn the splicing code.",
            "So the end of this pricing code needs to have predictive ability and also in addition to predictive ability, we sort of want to be able to look inside the splicing code to see what we learned.",
            "So we need to choose a model that enables us to do that."
        ],
        [
            "Here is the raw measurements that we had is is on 3600 mouse, mouse exons.",
            "So this axons are.",
            "Alternative facts Off so they can be included or excluded in exon.",
            "Skipping events.",
            "And we used microarray tool to measure the absolute inclusion level of of these 3000 mouse axons in 27 mouth tissues.",
            "Because this this microarray measurement is very noisy, we introduced a method similar to.",
            "Do PCA to group the tissues.",
            "And instead of predicting the absolute inclusion, for example, this is absolute inclusion like this one, this axle, maybe it's 90% included.",
            "Instead of predicting the absolute inclusion, we predict whether it is more likely to be included in that tissue.",
            "So this becomes a easier problem.",
            "So so I will not talk about in detail.",
            "This method is in this year I SMB."
        ],
        [
            "OK, so.",
            "This is the output of this preprocessing step, it's a.",
            "This outputs are what we define as splicing patterns, so a splicing pattern is multinomial distribution.",
            "With three categories, three categories are the more inclusion, more exclusion and no change.",
            "So if the multinomial distribution of this splicing pattern is all 1 zero and zero 4C NS for example, then it means in CNS this action is more included.",
            "More inclusion in CNS than the than the average inclusion level of that Exxon across all tissues.",
            "And we, as I said before we, we grouped the 27 tissues into four big groups.",
            "The CNS central nervous system, muscle embryo and digestive.",
            "And on the X axis, here are the axons, so this picture shows the splicing pattern, and so each little column here is 1 splicing pattern.",
            "For example, for the first one CLS is inclusion, so from the from the microarray we found strong evidence for more inclusion in CIS for this Axon, and for this Axon is exclusion in embryo.",
            "OK, so looking at this distribution we see most of the axons actually about 90% falls into a node change category.",
            "This means that are this means it's not regulated by CNS.",
            "So in CNS we don't see.",
            "A big change compared to the average across all tissues.",
            "So we have about 3000 axons and only 10% of the axons in each category.",
            "Exhibits of tissue dependent.",
            "Regulation.",
            "Microarray experiments, yeah 100%.",
            "Yeah.",
            "All is the analogue measure, so we have probes on the middle Exxon and the and the flanking exons.",
            "And we measured.",
            "I think we took the ratio of the of the middle X on to the flanking.",
            "To the nearby constitutes excellence that always, always included.",
            "Yeah, so that's sort of correct for the expression.",
            "OK, so this is the this is the data that gets."
        ],
        [
            "Into the algorithm.",
            "So we have the targets now and we discussed the fees."
        ],
        [
            "Jeff here.",
            "So the feature set consists of about 1000 features extracted from the sequence and they are very different.",
            "We've got about 200 known motives, so these are region specific counts of normative, such as Nova and Fox and, say Nova in upstream intron.",
            "And I.",
            "Also computationally identified new motives, so this 300 of these are.",
            "Computationally identified motives that may have a role in alternative splicing regulation.",
            "So the data used for this is many conservation and and we know there are lots of these that are false positives, so we only expect a few numbers in here that actually have a real effect.",
            "And also we included 123 nucleotides long short sequences.",
            "This is just exhaustive counts and also transcript structure, so some examples is Fox One and Fox 2 motive.",
            "For example, in upstream intro, so because they are similar, they are correlated and they are sparse integer values.",
            "Axon length, this is dense integer value.",
            "Dan is heavy tail.",
            "And for example, another example is frequency of a an AC in upstream.",
            "In trying this out question.",
            "Account Holder right?",
            "Yes yes it is.",
            "Account is account and it is.",
            "I think it's also.",
            "Is also weighted by conservation, so if there is more conservation than is more important, yeah.",
            "But it's a.",
            "It's sparse, so anyway, so all the features have very different distributions because they come from different methods.",
            "This is a picture."
        ],
        [
            "The features you can see there are very sparse.",
            "Here."
        ],
        [
            "We discussed the objective function.",
            "So we define.",
            "We define a measure of code quality.",
            "We call it code quality to be.",
            "KL divergent, because these are multinomial distributions, the splicing patterns are multinomial distributions, so the so we just use the KL divergences.",
            "the Q here is the target.",
            "So is the KL from the Q2 Q bar.",
            "Q Bar is the average of Q.",
            "This is a naive guesser.",
            "So is the KL from the target to a naive gather minus the KL from the from the target to our prediction.",
            "The P here is the prediction.",
            "So, and we use a log tool for the car, so this have additional meaning that are how many bits of information does the splicing code give us about alternative splicing and also with some across the axons instead of averaging, because for one Organism there are only so many axons.",
            "That can be alternative spliced.",
            "This cannot grow without bound, and we can say in the end how much information is there in this Organism for alternative splicing or the exon skipping event in alternative splicing.",
            "OK. And you can also see this this Q bar here is basically a constant, is just the average of the targets right?",
            "It's at the bottom, so you can take this out.",
            "This is just a constant will come out and what we left is Q log 2P.",
            "This is this is the likelihood.",
            "This is the likelihood of the targets under our predicted distribution using the partial counts.",
            "Say you can put the queues up here and it's a.",
            "It's a partial count, so optimizing the code quality is just like optimizing likelihood.",
            "01 or are they?",
            "Yeah, they are real real valued, so these are multinomial distributions.",
            "Alot of these are extreme, so a lot of these are like very dark and very white.",
            "But we do get some in the middle.",
            "This is corresponds to the micro area is noisy for that Exxon and we are not very sure.",
            "OK."
        ],
        [
            "This is the model that we used to learn the splicing code.",
            "So on the top is the RNA features and start combined to a few.",
            "Hidden variables.",
            "So these are like meta features.",
            "This is this is a matter of features that are learned using the information.",
            "And this this hidden variables connects to the output of.",
            "It's data that's shared across tissues.",
            "Yeah I have.",
            "I have one now.",
            "OK, so so we choose this.",
            "We choose this model because we can also sort of interpret what what this means.",
            "So the the only features connects to the hidden units.",
            "The connection can be on or off.",
            "And we have a sparse prior on that, so the prior so the connection is the input to hidden connection.",
            "Is the matrix W. And the prior for W is.",
            "Is a spike in the slate is a Delta function plus a normal distribution?",
            "And the other connection from the.",
            "From the hidden units to the output is just a normal distribution.",
            "OK.",
            "So so the hidden units.",
            "We use the sigmoid hidden units and the process is the only features with the sigmoid function here.",
            "So.",
            "What's interesting about the Sigma function here is that when the when the input is 0, the sigmoid function outputs half, and when the input is near 0, the output is linear.",
            "Is a linear function.",
            "Of the input.",
            "But I wonder when the input becomes very negative or very positive, the hidden units saturates so it can account for nonlinear effects of the features to the output and for the output units we use the.",
            "A softmax functions to make sure the output is a multinomial distribution matching our.",
            "The definition of splicing pattern."
        ],
        [
            "So we use the Gibbs sampling to sample the posterior distribution.",
            "And.",
            "And we discretized parameters to be between minus five and five with a step size of .1 because.",
            ".1 is sort of enough because the other.",
            "All the data is very noisy and this is for easy implementation and the facts fast mixing because we had a Delta function.",
            "In the in the prior this is.",
            "This way we can just do give sampling.",
            "We can compute the marginal distribution of each variable.",
            "Exactly.",
            "OK.",
            "So the Alpha here is the is the spike the spike.",
            "At zero is 90%, so we use 90% for 90% of the features inactive, OK?"
        ],
        [
            "This is our.",
            "This is our results.",
            "We can see the OC curves we do very good in CNS inclusion and muscle inclusion but not other signals.",
            "This is the code."
        ],
        [
            "Ality the code quality results.",
            "On on the bottom blue, here are the Bayesian neural Nets.",
            "They are generally better than all other methods on the middle.",
            "Here is the non Bayesian neural Nets.",
            "And we also included an implementation of SVM for comparison.",
            "So let's focus on."
        ],
        [
            "These two this is a Bayesian neural net with two hidden units without sharing.",
            "This is shared, shared hidden units across tissues, and you can see the performance increased a lot.",
            "So so the information from sharing unit improved prediction.",
            "The best one is."
        ],
        [
            "The he is the one with 10 hidden units.",
            "That's the best.",
            "So the.",
            "The network is sort of more complex than what can be captured by two hidden units and with 10 hidden units there are 10,000 parameters, so and only a few 1000 bits of information in the target.",
            "So this method is very good at avoiding overfitting.",
            "So now let's look at this some visualization about what what is actually learned in this."
        ],
        [
            "Hidden unit neural net.",
            "Those numbers mean the numbers don't have percentages after them.",
            "That's code quality.",
            "Yeah, these are the bits.",
            "OK, so first we look at the prediction performance of each sample."
        ],
        [
            "These are the prediction performance of each sample.",
            "They are very.",
            "They are not very good, but when you average them is very very becomes good.",
            "So this is the basic method hedging its bets on the not very good.",
            "On a very wide distribution that doesn't have good performance on their own.",
            "But when average the error cancels out and you get a very good performance.",
            "This is."
        ],
        [
            "Active fishel's on the X axis are how many times a feature is active, so 100% time so some features are very robust and they are active 100% of the time and this is near the prior.",
            "So we had 10% active feature on in the prior and this is because we had two hidden units.",
            "The prior is near 20% so important features are identified."
        ],
        [
            "And this this plot looks at the output of the hidden units as I discussed before.",
            "So the first hidden unit here operates in the linear and linear region.",
            "And the second hidden unit operates almost only in the nonlinear region.",
            "OK."
        ],
        [
            "Here is the.",
            "Here is.",
            "Here are the meta meta features and we look at the look how these two hidden units.",
            "These two hidden variables can't connect to different output units, so we can see them lisnik cluster together and for example, CNS inclusion is here and the hidden Unit 1 has a large weight on it, so CS inclusion is.",
            "Is regulated by Hidden Unit 1.",
            "I mean, how?"
        ],
        [
            "Tracking kidney and identity across all your samples from the posterior right.",
            "It's actually not the hidden units one and hidden Unit 2 are exchangeable, but but they have a such high energy barrier that it doesn't switch in my samples and if you start at different points, the image will be flipped, but but the general trend is like this.",
            "OK, this is my last slide."
        ],
        [
            "Last result slides.",
            "We asked the question if we had enough training data.",
            "Right, so you can you can.",
            "Because we only had 3000.",
            "The answer is no, we don't have enough training data, so the performance did not stop.",
            "So what this shows us is we need more training data and maybe we need to merge."
        ],
        [
            "Organisms and get a better prediction.",
            "So conclusion the Bayesian neural net is 50% better and we found useful on officials.",
            "And also we found features that are that are used across tissues and we can.",
            "We can also sort of see what's going on in this in this model.",
            "And one important thing is that the data set is available online.",
            "You can download it and try your favorite algorithm.",
            "OK."
        ],
        [
            "Yep.",
            "Features which of the irony features more right?",
            "Axon length is very important and some known feature known motives such as Fox, Anna, Nova and PTC.",
            "Yeah.",
            "So I have a question about the network structure.",
            "So you have outstanding features and I have two hidden layer 2 hidden units, yeah?",
            "Classes are full different for different.",
            "For different issues, yeah.",
            "And each tissue is a.",
            "The output is 3 #3 numbers, so it's a splicing pattern.",
            "Every tissue is a softmax function that outputs.",
            "A probability in three categories.",
            "Learning.",
            "Yeah.",
            "There you are occupied for different tasks, right, right?",
            "So the underlying mechanism for regulating different issues is is similar.",
            "Just talking and.",
            "Oh wait wait wait.",
            "We tried 1010 is the performed the best but for this visualization to work I just chose 2 because you can.",
            "Yeah.",
            "But the.",
            "We tried 10, we tried 10.",
            "Yeah.",
            "So you chose to join like all that I mean with all but the four tissue groups, right, right?",
            "And he looked into different issues separately as well, right?",
            "Right?",
            "And so I mean, I guess I'm wondering.",
            "So when is it that issue identity?",
            "The other thing is, I mean, what does it really depend on?",
            "What are the causal factors for different splicing?",
            "Is probably gene expression of some splicing factor, so these kind of things right?",
            "Yeah, so.",
            "But this is not taken into account.",
            "As such, because you only look at with the identity of tissue but not like properties of their tissue, what the gene expression would be awesome?",
            "Right, this is what we are trying to learn, right?",
            "We assume there is a splicing mechanism around around in one tissue and then we want to.",
            "We want to try to learn what this does, what the spicy mechanisms in one tissue does to all the axons right?",
            "But I would say it's not one specific conditions that issue, but it's once.",
            "Basically we can dismiss that works in all tissues.",
            "Yes, yes, that's right.",
            "That's why we had.",
            "We had a input, 2 hidden units are ways shared shared across.",
            "Shared across tissues.",
            "So we are trying to capture that by merging the tissues.",
            "Then let's have a break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so today I will present our work on predicting tissue dependent alternative splicing using neural Nets.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with our Jozef Brush and Brandon's try at U of T.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is some biological background about alternative splicing, so it's a very important phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Alternative splicing is the phenomenon that one gene can produce many transcripts, and this happens a lot in mammals.",
                    "label": 1
                },
                {
                    "sent": "For example, in humans there are 90% of genes are Alternatively spliced, and it turns about 20,000 human genes to hundreds of thousands of M RNA in humans and is related to.",
                    "label": 1
                },
                {
                    "sent": "Many disease, by one estimation 25 to 50% disease linked to problematic alternative spicy and also some other functions including neurons in our brain from connections from the correct connections by using alternative splicing OK.",
                    "label": 1
                },
                {
                    "sent": "So the most common type of alternative splicing is the cassette splicing or the exon skipping event.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem we are going to focus on in this presentation.",
                    "label": 0
                },
                {
                    "sent": "Here is a schematic, so this is the DNA which is the same as the unspliced RNA.",
                    "label": 0
                },
                {
                    "sent": "On here we have three exons.",
                    "label": 0
                },
                {
                    "sent": "3 boxes The middle Exxon is the alternative for Exxon, so this one can either be included or excluded, and these are flanking.",
                    "label": 0
                },
                {
                    "sent": "Constitutive exons are always always present.",
                    "label": 0
                },
                {
                    "sent": "And this difference is tissue dependent.",
                    "label": 0
                },
                {
                    "sent": "For example, in brain this Axon may be more inclusive than in muscle, and this work is about.",
                    "label": 0
                },
                {
                    "sent": "Predicting the tissue dependent differences.",
                    "label": 0
                },
                {
                    "sent": "Are of the inclusion level of the of the Axon of the alternative exon.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here is our problem formulation.",
                    "label": 0
                },
                {
                    "sent": "We formulate this problem as a machine learning problem and this is the block diagram.",
                    "label": 0
                },
                {
                    "sent": "So on the top here are the DNA sequences and we extract RNA.",
                    "label": 0
                },
                {
                    "sent": "Feature set from the eyes.",
                    "label": 0
                },
                {
                    "sent": "This is unsupervised on a sequence and this feature set is fit into the splicing code.",
                    "label": 0
                },
                {
                    "sent": "Along with the tissue type and the splicing code, can predict whether there is more inclusion or more exclusion in a certain type then compared to Delta X on the average of.",
                    "label": 0
                },
                {
                    "sent": "The inclusion level of Death X across all tissues.",
                    "label": 0
                },
                {
                    "sent": "So this talk we will discuss.",
                    "label": 0
                },
                {
                    "sent": "I'll describe algorithm for constructing this splicing code.",
                    "label": 0
                },
                {
                    "sent": "And we had about 3000 training examples.",
                    "label": 0
                },
                {
                    "sent": "That we measured the target.",
                    "label": 0
                },
                {
                    "sent": "And also extracted the features.",
                    "label": 0
                },
                {
                    "sent": "And using this we learn the splicing code.",
                    "label": 0
                },
                {
                    "sent": "So the end of this pricing code needs to have predictive ability and also in addition to predictive ability, we sort of want to be able to look inside the splicing code to see what we learned.",
                    "label": 0
                },
                {
                    "sent": "So we need to choose a model that enables us to do that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the raw measurements that we had is is on 3600 mouse, mouse exons.",
                    "label": 0
                },
                {
                    "sent": "So this axons are.",
                    "label": 0
                },
                {
                    "sent": "Alternative facts Off so they can be included or excluded in exon.",
                    "label": 0
                },
                {
                    "sent": "Skipping events.",
                    "label": 0
                },
                {
                    "sent": "And we used microarray tool to measure the absolute inclusion level of of these 3000 mouse axons in 27 mouth tissues.",
                    "label": 1
                },
                {
                    "sent": "Because this this microarray measurement is very noisy, we introduced a method similar to.",
                    "label": 1
                },
                {
                    "sent": "Do PCA to group the tissues.",
                    "label": 0
                },
                {
                    "sent": "And instead of predicting the absolute inclusion, for example, this is absolute inclusion like this one, this axle, maybe it's 90% included.",
                    "label": 0
                },
                {
                    "sent": "Instead of predicting the absolute inclusion, we predict whether it is more likely to be included in that tissue.",
                    "label": 0
                },
                {
                    "sent": "So this becomes a easier problem.",
                    "label": 0
                },
                {
                    "sent": "So so I will not talk about in detail.",
                    "label": 0
                },
                {
                    "sent": "This method is in this year I SMB.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is the output of this preprocessing step, it's a.",
                    "label": 0
                },
                {
                    "sent": "This outputs are what we define as splicing patterns, so a splicing pattern is multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "With three categories, three categories are the more inclusion, more exclusion and no change.",
                    "label": 1
                },
                {
                    "sent": "So if the multinomial distribution of this splicing pattern is all 1 zero and zero 4C NS for example, then it means in CNS this action is more included.",
                    "label": 0
                },
                {
                    "sent": "More inclusion in CNS than the than the average inclusion level of that Exxon across all tissues.",
                    "label": 1
                },
                {
                    "sent": "And we, as I said before we, we grouped the 27 tissues into four big groups.",
                    "label": 0
                },
                {
                    "sent": "The CNS central nervous system, muscle embryo and digestive.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis, here are the axons, so this picture shows the splicing pattern, and so each little column here is 1 splicing pattern.",
                    "label": 0
                },
                {
                    "sent": "For example, for the first one CLS is inclusion, so from the from the microarray we found strong evidence for more inclusion in CIS for this Axon, and for this Axon is exclusion in embryo.",
                    "label": 0
                },
                {
                    "sent": "OK, so looking at this distribution we see most of the axons actually about 90% falls into a node change category.",
                    "label": 0
                },
                {
                    "sent": "This means that are this means it's not regulated by CNS.",
                    "label": 1
                },
                {
                    "sent": "So in CNS we don't see.",
                    "label": 0
                },
                {
                    "sent": "A big change compared to the average across all tissues.",
                    "label": 0
                },
                {
                    "sent": "So we have about 3000 axons and only 10% of the axons in each category.",
                    "label": 0
                },
                {
                    "sent": "Exhibits of tissue dependent.",
                    "label": 0
                },
                {
                    "sent": "Regulation.",
                    "label": 0
                },
                {
                    "sent": "Microarray experiments, yeah 100%.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "All is the analogue measure, so we have probes on the middle Exxon and the and the flanking exons.",
                    "label": 0
                },
                {
                    "sent": "And we measured.",
                    "label": 0
                },
                {
                    "sent": "I think we took the ratio of the of the middle X on to the flanking.",
                    "label": 0
                },
                {
                    "sent": "To the nearby constitutes excellence that always, always included.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's sort of correct for the expression.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the this is the data that gets.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have the targets now and we discussed the fees.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeff here.",
                    "label": 0
                },
                {
                    "sent": "So the feature set consists of about 1000 features extracted from the sequence and they are very different.",
                    "label": 1
                },
                {
                    "sent": "We've got about 200 known motives, so these are region specific counts of normative, such as Nova and Fox and, say Nova in upstream intron.",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                },
                {
                    "sent": "Also computationally identified new motives, so this 300 of these are.",
                    "label": 0
                },
                {
                    "sent": "Computationally identified motives that may have a role in alternative splicing regulation.",
                    "label": 0
                },
                {
                    "sent": "So the data used for this is many conservation and and we know there are lots of these that are false positives, so we only expect a few numbers in here that actually have a real effect.",
                    "label": 0
                },
                {
                    "sent": "And also we included 123 nucleotides long short sequences.",
                    "label": 1
                },
                {
                    "sent": "This is just exhaustive counts and also transcript structure, so some examples is Fox One and Fox 2 motive.",
                    "label": 0
                },
                {
                    "sent": "For example, in upstream intro, so because they are similar, they are correlated and they are sparse integer values.",
                    "label": 0
                },
                {
                    "sent": "Axon length, this is dense integer value.",
                    "label": 0
                },
                {
                    "sent": "Dan is heavy tail.",
                    "label": 0
                },
                {
                    "sent": "And for example, another example is frequency of a an AC in upstream.",
                    "label": 1
                },
                {
                    "sent": "In trying this out question.",
                    "label": 0
                },
                {
                    "sent": "Account Holder right?",
                    "label": 0
                },
                {
                    "sent": "Yes yes it is.",
                    "label": 0
                },
                {
                    "sent": "Account is account and it is.",
                    "label": 0
                },
                {
                    "sent": "I think it's also.",
                    "label": 0
                },
                {
                    "sent": "Is also weighted by conservation, so if there is more conservation than is more important, yeah.",
                    "label": 0
                },
                {
                    "sent": "But it's a.",
                    "label": 0
                },
                {
                    "sent": "It's sparse, so anyway, so all the features have very different distributions because they come from different methods.",
                    "label": 0
                },
                {
                    "sent": "This is a picture.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features you can see there are very sparse.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We discussed the objective function.",
                    "label": 0
                },
                {
                    "sent": "So we define.",
                    "label": 0
                },
                {
                    "sent": "We define a measure of code quality.",
                    "label": 1
                },
                {
                    "sent": "We call it code quality to be.",
                    "label": 0
                },
                {
                    "sent": "KL divergent, because these are multinomial distributions, the splicing patterns are multinomial distributions, so the so we just use the KL divergences.",
                    "label": 0
                },
                {
                    "sent": "the Q here is the target.",
                    "label": 1
                },
                {
                    "sent": "So is the KL from the Q2 Q bar.",
                    "label": 0
                },
                {
                    "sent": "Q Bar is the average of Q.",
                    "label": 0
                },
                {
                    "sent": "This is a naive guesser.",
                    "label": 1
                },
                {
                    "sent": "So is the KL from the target to a naive gather minus the KL from the from the target to our prediction.",
                    "label": 1
                },
                {
                    "sent": "The P here is the prediction.",
                    "label": 1
                },
                {
                    "sent": "So, and we use a log tool for the car, so this have additional meaning that are how many bits of information does the splicing code give us about alternative splicing and also with some across the axons instead of averaging, because for one Organism there are only so many axons.",
                    "label": 0
                },
                {
                    "sent": "That can be alternative spliced.",
                    "label": 1
                },
                {
                    "sent": "This cannot grow without bound, and we can say in the end how much information is there in this Organism for alternative splicing or the exon skipping event in alternative splicing.",
                    "label": 0
                },
                {
                    "sent": "OK. And you can also see this this Q bar here is basically a constant, is just the average of the targets right?",
                    "label": 0
                },
                {
                    "sent": "It's at the bottom, so you can take this out.",
                    "label": 1
                },
                {
                    "sent": "This is just a constant will come out and what we left is Q log 2P.",
                    "label": 0
                },
                {
                    "sent": "This is this is the likelihood.",
                    "label": 1
                },
                {
                    "sent": "This is the likelihood of the targets under our predicted distribution using the partial counts.",
                    "label": 0
                },
                {
                    "sent": "Say you can put the queues up here and it's a.",
                    "label": 1
                },
                {
                    "sent": "It's a partial count, so optimizing the code quality is just like optimizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "01 or are they?",
                    "label": 0
                },
                {
                    "sent": "Yeah, they are real real valued, so these are multinomial distributions.",
                    "label": 0
                },
                {
                    "sent": "Alot of these are extreme, so a lot of these are like very dark and very white.",
                    "label": 0
                },
                {
                    "sent": "But we do get some in the middle.",
                    "label": 0
                },
                {
                    "sent": "This is corresponds to the micro area is noisy for that Exxon and we are not very sure.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the model that we used to learn the splicing code.",
                    "label": 0
                },
                {
                    "sent": "So on the top is the RNA features and start combined to a few.",
                    "label": 0
                },
                {
                    "sent": "Hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So these are like meta features.",
                    "label": 0
                },
                {
                    "sent": "This is this is a matter of features that are learned using the information.",
                    "label": 0
                },
                {
                    "sent": "And this this hidden variables connects to the output of.",
                    "label": 0
                },
                {
                    "sent": "It's data that's shared across tissues.",
                    "label": 0
                },
                {
                    "sent": "Yeah I have.",
                    "label": 0
                },
                {
                    "sent": "I have one now.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we choose this.",
                    "label": 0
                },
                {
                    "sent": "We choose this model because we can also sort of interpret what what this means.",
                    "label": 0
                },
                {
                    "sent": "So the the only features connects to the hidden units.",
                    "label": 0
                },
                {
                    "sent": "The connection can be on or off.",
                    "label": 0
                },
                {
                    "sent": "And we have a sparse prior on that, so the prior so the connection is the input to hidden connection.",
                    "label": 0
                },
                {
                    "sent": "Is the matrix W. And the prior for W is.",
                    "label": 0
                },
                {
                    "sent": "Is a spike in the slate is a Delta function plus a normal distribution?",
                    "label": 0
                },
                {
                    "sent": "And the other connection from the.",
                    "label": 0
                },
                {
                    "sent": "From the hidden units to the output is just a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so the hidden units.",
                    "label": 0
                },
                {
                    "sent": "We use the sigmoid hidden units and the process is the only features with the sigmoid function here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What's interesting about the Sigma function here is that when the when the input is 0, the sigmoid function outputs half, and when the input is near 0, the output is linear.",
                    "label": 0
                },
                {
                    "sent": "Is a linear function.",
                    "label": 0
                },
                {
                    "sent": "Of the input.",
                    "label": 0
                },
                {
                    "sent": "But I wonder when the input becomes very negative or very positive, the hidden units saturates so it can account for nonlinear effects of the features to the output and for the output units we use the.",
                    "label": 0
                },
                {
                    "sent": "A softmax functions to make sure the output is a multinomial distribution matching our.",
                    "label": 0
                },
                {
                    "sent": "The definition of splicing pattern.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we use the Gibbs sampling to sample the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we discretized parameters to be between minus five and five with a step size of .1 because.",
                    "label": 0
                },
                {
                    "sent": ".1 is sort of enough because the other.",
                    "label": 0
                },
                {
                    "sent": "All the data is very noisy and this is for easy implementation and the facts fast mixing because we had a Delta function.",
                    "label": 0
                },
                {
                    "sent": "In the in the prior this is.",
                    "label": 1
                },
                {
                    "sent": "This way we can just do give sampling.",
                    "label": 0
                },
                {
                    "sent": "We can compute the marginal distribution of each variable.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the Alpha here is the is the spike the spike.",
                    "label": 0
                },
                {
                    "sent": "At zero is 90%, so we use 90% for 90% of the features inactive, OK?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our.",
                    "label": 0
                },
                {
                    "sent": "This is our results.",
                    "label": 0
                },
                {
                    "sent": "We can see the OC curves we do very good in CNS inclusion and muscle inclusion but not other signals.",
                    "label": 1
                },
                {
                    "sent": "This is the code.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ality the code quality results.",
                    "label": 0
                },
                {
                    "sent": "On on the bottom blue, here are the Bayesian neural Nets.",
                    "label": 0
                },
                {
                    "sent": "They are generally better than all other methods on the middle.",
                    "label": 0
                },
                {
                    "sent": "Here is the non Bayesian neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And we also included an implementation of SVM for comparison.",
                    "label": 0
                },
                {
                    "sent": "So let's focus on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These two this is a Bayesian neural net with two hidden units without sharing.",
                    "label": 0
                },
                {
                    "sent": "This is shared, shared hidden units across tissues, and you can see the performance increased a lot.",
                    "label": 0
                },
                {
                    "sent": "So so the information from sharing unit improved prediction.",
                    "label": 0
                },
                {
                    "sent": "The best one is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The he is the one with 10 hidden units.",
                    "label": 0
                },
                {
                    "sent": "That's the best.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The network is sort of more complex than what can be captured by two hidden units and with 10 hidden units there are 10,000 parameters, so and only a few 1000 bits of information in the target.",
                    "label": 0
                },
                {
                    "sent": "So this method is very good at avoiding overfitting.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at this some visualization about what what is actually learned in this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hidden unit neural net.",
                    "label": 0
                },
                {
                    "sent": "Those numbers mean the numbers don't have percentages after them.",
                    "label": 0
                },
                {
                    "sent": "That's code quality.",
                    "label": 0
                },
                {
                    "sent": "Yeah, these are the bits.",
                    "label": 0
                },
                {
                    "sent": "OK, so first we look at the prediction performance of each sample.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the prediction performance of each sample.",
                    "label": 0
                },
                {
                    "sent": "They are very.",
                    "label": 0
                },
                {
                    "sent": "They are not very good, but when you average them is very very becomes good.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic method hedging its bets on the not very good.",
                    "label": 0
                },
                {
                    "sent": "On a very wide distribution that doesn't have good performance on their own.",
                    "label": 0
                },
                {
                    "sent": "But when average the error cancels out and you get a very good performance.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active fishel's on the X axis are how many times a feature is active, so 100% time so some features are very robust and they are active 100% of the time and this is near the prior.",
                    "label": 1
                },
                {
                    "sent": "So we had 10% active feature on in the prior and this is because we had two hidden units.",
                    "label": 1
                },
                {
                    "sent": "The prior is near 20% so important features are identified.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this this plot looks at the output of the hidden units as I discussed before.",
                    "label": 1
                },
                {
                    "sent": "So the first hidden unit here operates in the linear and linear region.",
                    "label": 1
                },
                {
                    "sent": "And the second hidden unit operates almost only in the nonlinear region.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                },
                {
                    "sent": "Here are the meta meta features and we look at the look how these two hidden units.",
                    "label": 0
                },
                {
                    "sent": "These two hidden variables can't connect to different output units, so we can see them lisnik cluster together and for example, CNS inclusion is here and the hidden Unit 1 has a large weight on it, so CS inclusion is.",
                    "label": 0
                },
                {
                    "sent": "Is regulated by Hidden Unit 1.",
                    "label": 1
                },
                {
                    "sent": "I mean, how?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tracking kidney and identity across all your samples from the posterior right.",
                    "label": 0
                },
                {
                    "sent": "It's actually not the hidden units one and hidden Unit 2 are exchangeable, but but they have a such high energy barrier that it doesn't switch in my samples and if you start at different points, the image will be flipped, but but the general trend is like this.",
                    "label": 0
                },
                {
                    "sent": "OK, this is my last slide.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last result slides.",
                    "label": 0
                },
                {
                    "sent": "We asked the question if we had enough training data.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can you can.",
                    "label": 0
                },
                {
                    "sent": "Because we only had 3000.",
                    "label": 0
                },
                {
                    "sent": "The answer is no, we don't have enough training data, so the performance did not stop.",
                    "label": 0
                },
                {
                    "sent": "So what this shows us is we need more training data and maybe we need to merge.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Organisms and get a better prediction.",
                    "label": 0
                },
                {
                    "sent": "So conclusion the Bayesian neural net is 50% better and we found useful on officials.",
                    "label": 0
                },
                {
                    "sent": "And also we found features that are that are used across tissues and we can.",
                    "label": 0
                },
                {
                    "sent": "We can also sort of see what's going on in this in this model.",
                    "label": 0
                },
                {
                    "sent": "And one important thing is that the data set is available online.",
                    "label": 0
                },
                {
                    "sent": "You can download it and try your favorite algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Features which of the irony features more right?",
                    "label": 0
                },
                {
                    "sent": "Axon length is very important and some known feature known motives such as Fox, Anna, Nova and PTC.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I have a question about the network structure.",
                    "label": 0
                },
                {
                    "sent": "So you have outstanding features and I have two hidden layer 2 hidden units, yeah?",
                    "label": 0
                },
                {
                    "sent": "Classes are full different for different.",
                    "label": 0
                },
                {
                    "sent": "For different issues, yeah.",
                    "label": 0
                },
                {
                    "sent": "And each tissue is a.",
                    "label": 0
                },
                {
                    "sent": "The output is 3 #3 numbers, so it's a splicing pattern.",
                    "label": 0
                },
                {
                    "sent": "Every tissue is a softmax function that outputs.",
                    "label": 0
                },
                {
                    "sent": "A probability in three categories.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There you are occupied for different tasks, right, right?",
                    "label": 0
                },
                {
                    "sent": "So the underlying mechanism for regulating different issues is is similar.",
                    "label": 0
                },
                {
                    "sent": "Just talking and.",
                    "label": 0
                },
                {
                    "sent": "Oh wait wait wait.",
                    "label": 0
                },
                {
                    "sent": "We tried 1010 is the performed the best but for this visualization to work I just chose 2 because you can.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But the.",
                    "label": 0
                },
                {
                    "sent": "We tried 10, we tried 10.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you chose to join like all that I mean with all but the four tissue groups, right, right?",
                    "label": 0
                },
                {
                    "sent": "And he looked into different issues separately as well, right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And so I mean, I guess I'm wondering.",
                    "label": 0
                },
                {
                    "sent": "So when is it that issue identity?",
                    "label": 0
                },
                {
                    "sent": "The other thing is, I mean, what does it really depend on?",
                    "label": 0
                },
                {
                    "sent": "What are the causal factors for different splicing?",
                    "label": 0
                },
                {
                    "sent": "Is probably gene expression of some splicing factor, so these kind of things right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "But this is not taken into account.",
                    "label": 0
                },
                {
                    "sent": "As such, because you only look at with the identity of tissue but not like properties of their tissue, what the gene expression would be awesome?",
                    "label": 0
                },
                {
                    "sent": "Right, this is what we are trying to learn, right?",
                    "label": 0
                },
                {
                    "sent": "We assume there is a splicing mechanism around around in one tissue and then we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to try to learn what this does, what the spicy mechanisms in one tissue does to all the axons right?",
                    "label": 0
                },
                {
                    "sent": "But I would say it's not one specific conditions that issue, but it's once.",
                    "label": 0
                },
                {
                    "sent": "Basically we can dismiss that works in all tissues.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "That's why we had.",
                    "label": 0
                },
                {
                    "sent": "We had a input, 2 hidden units are ways shared shared across.",
                    "label": 0
                },
                {
                    "sent": "Shared across tissues.",
                    "label": 0
                },
                {
                    "sent": "So we are trying to capture that by merging the tissues.",
                    "label": 0
                },
                {
                    "sent": "Then let's have a break.",
                    "label": 0
                }
            ]
        }
    }
}