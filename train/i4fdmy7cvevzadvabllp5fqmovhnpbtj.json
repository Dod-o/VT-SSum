{
    "id": "i4fdmy7cvevzadvabllp5fqmovhnpbtj",
    "title": "Compressive neural representation of sparse, high-dimensional probabilities",
    "info": {
        "author": [
            "Xaq Pitkow, Brain and Cognitive Sciences (BCS), University of Rochester"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Biology->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/machine_pitkow_neural/",
    "segmentation": [
        [
            "In traditional compressive sensing you have some sparse signal S which has few big entries and you compress it by multiplying it by some sensing matrix which has relatively few rows.",
            "The measurements that you get under certain conditions on the matrix are guaranteed to represent fully that signal.",
            "This has been pretty handy in a number of applications, but it's less clear how that would apply in my favorite neural information processing system in the brain.",
            "So natural inputs are sparse, but for each input there are an awful lot of neurons, so it's not clear that the brain really needs to compress.",
            "On the other hand, it's clear that the brain computes with probabilities and probabilities in general are exponentially large in the number of variables, so that creates a problem here.",
            "Would I show is that the brain could use nearly plausable resources to actually represent fully these probability distributions under the condition that they are sparse.",
            "That is, there are relatively few states that have an appreciable probability.",
            "And it can do this with resources that are merely linear in the number of variables rather than exponential.",
            "Essentially, the log rhythmic savings that you get from compressive sensing cancels the exponential size of the probability.",
            "So when you apply compressive sensing to a probability distribution, your signal is the sparse signal is P of X indexed by the state of the input X.",
            "And measurements correspond to expectation values over some function ASA by which is a function of the state and the average is taken over the probability distribution of interest.",
            "So what functions should we use?",
            "Well, if we use purely random functions of the input, well, those are arbitrarily complicated, and it would be arbitrarily hard for the brain to do that quickly.",
            "We could make the brain's life a lot easier if instead we"
        ],
        [
            "Used random perceptrons which are just linear weightings of the input passed through some step non linearity.",
            "This produces a sensing matrix which has a mixture of randomness and structure shown on the bottom here.",
            "And I show that asymptotically this will preserve the information in the probability distribution.",
            "So now how do we actually compute the expectation values?",
            "Well, one way is by sampling, so you could sample your inputs, then pass them through the random perceptrons.",
            "And in this case the average activity of each of those random perceptrons fully represents the input distribution, no correlations required.",
            "We can see that this works by X."
        ],
        [
            "We doing a reconstruction of the probability and we find that you're ultimately limited by your sampling rather than by the compression.",
            "Now, I'm not suggesting that the brain is actually going to do reconstruction.",
            "After all, the whole point was that these probability distributions are so large as to be unwieldy.",
            "Instead, the."
        ],
        [
            "And could compute directly in the compressive domain, and that's possible because the metrical and topological properties are preserved there, so I illustrate that with a simple toy example where you have a generative model that produces a set of posteriors which has a circular symmetry and you can see that in this nonlinear embedding 100 dimensional posteriors.",
            "If you now compress each of those posteriors with just 10 random perceptrons, calculate the mean outputs and look at its embedding, you again retrieve the circular symmetry so you.",
            "Preserve that structure and we all know that statistical inference is hard, so maybe the brain is saving some time in space by using this compressive and neurally plausible representation.",
            "For more details, come see me later at poster 27.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In traditional compressive sensing you have some sparse signal S which has few big entries and you compress it by multiplying it by some sensing matrix which has relatively few rows.",
                    "label": 1
                },
                {
                    "sent": "The measurements that you get under certain conditions on the matrix are guaranteed to represent fully that signal.",
                    "label": 0
                },
                {
                    "sent": "This has been pretty handy in a number of applications, but it's less clear how that would apply in my favorite neural information processing system in the brain.",
                    "label": 0
                },
                {
                    "sent": "So natural inputs are sparse, but for each input there are an awful lot of neurons, so it's not clear that the brain really needs to compress.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it's clear that the brain computes with probabilities and probabilities in general are exponentially large in the number of variables, so that creates a problem here.",
                    "label": 0
                },
                {
                    "sent": "Would I show is that the brain could use nearly plausable resources to actually represent fully these probability distributions under the condition that they are sparse.",
                    "label": 0
                },
                {
                    "sent": "That is, there are relatively few states that have an appreciable probability.",
                    "label": 0
                },
                {
                    "sent": "And it can do this with resources that are merely linear in the number of variables rather than exponential.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the log rhythmic savings that you get from compressive sensing cancels the exponential size of the probability.",
                    "label": 0
                },
                {
                    "sent": "So when you apply compressive sensing to a probability distribution, your signal is the sparse signal is P of X indexed by the state of the input X.",
                    "label": 0
                },
                {
                    "sent": "And measurements correspond to expectation values over some function ASA by which is a function of the state and the average is taken over the probability distribution of interest.",
                    "label": 0
                },
                {
                    "sent": "So what functions should we use?",
                    "label": 0
                },
                {
                    "sent": "Well, if we use purely random functions of the input, well, those are arbitrarily complicated, and it would be arbitrarily hard for the brain to do that quickly.",
                    "label": 0
                },
                {
                    "sent": "We could make the brain's life a lot easier if instead we",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used random perceptrons which are just linear weightings of the input passed through some step non linearity.",
                    "label": 1
                },
                {
                    "sent": "This produces a sensing matrix which has a mixture of randomness and structure shown on the bottom here.",
                    "label": 0
                },
                {
                    "sent": "And I show that asymptotically this will preserve the information in the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So now how do we actually compute the expectation values?",
                    "label": 0
                },
                {
                    "sent": "Well, one way is by sampling, so you could sample your inputs, then pass them through the random perceptrons.",
                    "label": 0
                },
                {
                    "sent": "And in this case the average activity of each of those random perceptrons fully represents the input distribution, no correlations required.",
                    "label": 0
                },
                {
                    "sent": "We can see that this works by X.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We doing a reconstruction of the probability and we find that you're ultimately limited by your sampling rather than by the compression.",
                    "label": 0
                },
                {
                    "sent": "Now, I'm not suggesting that the brain is actually going to do reconstruction.",
                    "label": 0
                },
                {
                    "sent": "After all, the whole point was that these probability distributions are so large as to be unwieldy.",
                    "label": 0
                },
                {
                    "sent": "Instead, the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And could compute directly in the compressive domain, and that's possible because the metrical and topological properties are preserved there, so I illustrate that with a simple toy example where you have a generative model that produces a set of posteriors which has a circular symmetry and you can see that in this nonlinear embedding 100 dimensional posteriors.",
                    "label": 0
                },
                {
                    "sent": "If you now compress each of those posteriors with just 10 random perceptrons, calculate the mean outputs and look at its embedding, you again retrieve the circular symmetry so you.",
                    "label": 0
                },
                {
                    "sent": "Preserve that structure and we all know that statistical inference is hard, so maybe the brain is saving some time in space by using this compressive and neurally plausible representation.",
                    "label": 0
                },
                {
                    "sent": "For more details, come see me later at poster 27.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}