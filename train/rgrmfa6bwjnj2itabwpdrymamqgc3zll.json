{
    "id": "rgrmfa6bwjnj2itabwpdrymamqgc3zll",
    "title": "Advanced OpenACC (Part 3)",
    "info": {
        "author": [
            "John Urbanic, Pittsburgh Supercomputing Center"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_urbanic_advanced_openACC/",
    "segmentation": [
        [
            "Now we have enough Slack in the schedule.",
            "Fear not, we won't shortchange you on anything between between this morning and this afternoon that is.",
            "We have we're going to switch gears I've I've given you kind of a slow paced, foundational introduction to open ACC.",
            "We're going to switch gears and cover a lot more ground as I talk about some of the advanced stuff and how open ACC interacts with other other things that live out there in the world and will finish up with a hybrid challenge introduction, which I think will be interesting to you, not only if you're interested in the challenge, but also because it involves hybrid programming.",
            "Again, how these things interact with the rest of the world.",
            "But before we depart from the world of introductory open ACC.",
            "I want to once again make sure everybody in the room is comfortable with either your solution or my solution, which in all likelihood looks a lot like yours to yesterday's exercise.",
            "Does this make sense to everybody here?",
            "If it doesn't, this is a good time to answer some questions again, 'cause once we leave the station here, I'm going to take it as a given that you understand at least the basic components.",
            "At least this solution.",
            "Everybody good.",
            "So little unbelievable, but I'll just be impressed instead of sceptical.",
            "OK, well if that is the case then let's go on and talk about.",
            "We'll start out with some.",
            "This machine will come to life, will start out with some.",
            "Advanced open ACC and then will.",
            "Will get into all kinds of other.",
            "Interesting pieces, but we'll start with this one.",
            "Now what I want to do here?"
        ],
        [
            "Is I won't do the typical approach to talking about advanced open ACC, or for that matter the way people live and talk about any of these MPI or open MP or any kind of library where they categorically go down the list or just go through the catalog the index as it were instead will group these things together into a couple of interesting pieces that may actually be relevant to you."
        ],
        [
            "And we'll look at those.",
            "And then I'll.",
            "I'll tell you all the miscellaneous pieces that are out there.",
            "We won't let him to let us let them distract us, but I'll make sure at the end that I mentioned anything that we overlooked intentionally.",
            "But let's start out with a really interesting pieces of open ACC.",
            "There advanced the first one here, will talk about is optimization using the advanced features because, as I mentioned to you yesterday, I am totally unwilling to trade off using a higher level approach if it costs us performance.",
            "It's the whole point of high performance computing and using machines that cost 100 million dollars.",
            "Is not to say it's acceptable for the sake of programmer productivity.",
            "So for the sake of programmer laziness or lack of skill to let some inefficient code run on the machines that is not our philosophy here, so I am not introducing you to open ACC to enable you to write badly performing GPU codes, and I hope that kernels does the right thing.",
            "The Kernels command that we've learned that puts an awful lot of responsibility for accelerating things on the compiler.",
            "And every year that goes by every release of the compiler that comes along.",
            "You hope it does a better and better job, but let's say that it's not going good enough job and you know that if you were a CUDA programmer working at a low level, you would be able to optimize things more well open ACC gives us the ability to make suggestions to the compiler that affect things at a lower level and allow us to.",
            "We have lots of options and."
        ],
        [
            "Before we before I talk it to death while showing you some of those, let's jump right into how those options work.",
            "The way that open ACC does it is, it targets the architecture.",
            "But at an abstract way so that we don't end up saying, oh, we're working with well pieces.",
            "I'm about to show you in a minute we're working with these low level hardware components instead.",
            "It has an abstraction at a higher level, so we're target."
        ],
        [
            "The architecture, but we're not really admitting it.",
            "If I look at a piece of open ACC codes, it's using this stuff.",
            "I'm not going to see in there things that say we're using a K20 card here, or we're using as EON 5 instead.",
            "We have an abstraction that allows us to map things to a lower level, and it's pretty straightforward and intuitive.",
            "It basically just says that we have these.",
            "Gangs of workers, each one of which has vectors of work to do.",
            "And so by giving a granularity to our work with a couple of hierarchies to it, we can suggest that the compiler a good way to break up our work in distributed or lower level, which is the key to getting these GPU's to work well.",
            "The key is basically saying that if the current if the compiler itself can't guess the right way to distribute work across the many thousands of cores that we have, then we can kind of massage.",
            "That's guess a little bit with these things right here.",
            "So we have a nice abstract."
        ],
        [
            "Hierarchy to work with that underneath the scenes targets hardware you won't see this in the spec.",
            "The spec's job is to say hey, we're hiding this from you.",
            "This is strictly suggestions to the compiler, but in reality this is what ends up happening in reality.",
            "A worker or a gang sort of the top here gang targets at SM which is hardware component that will look at.",
            "In a moment.",
            "It's kind of the largest chunk of of control in a GPU, or if we were targeting something like AZ on Fire device, which I'll talk about in some detail as well.",
            "'cause this is a relevant thing in this day and age, if we're talking 'cause you don't fight.",
            "It would target a CPU.",
            "A worker targets what the GPU is called a warp, and again, I'll I'll define these things for you or on a CPU at Target score.",
            "A vector looks an awful lot like your classical vector size, chunk of work, and any serial computer works with these days, and that's equivalent with red on a GPU.",
            "So these things really do translate to something in the real hardware world, although the spec won't guarantee it, and the compiler is free to reinterpret it.",
            "And as a matter of fact, sometimes does the compiler."
        ],
        [
            "Sometimes thinks that it can do a better job even when you suggest these things to it, and it sometimes does.",
            "Now I've been talking about all of this.",
            "These hardware things right here.",
            "I've started talking about hardware, which again I think it should impress you that we managed to not talk about hardware all yesterday, right?",
            "So far as far as you're concerned, a GPU is a black box, and yet we were able to get a lot of capability on it under the right circumstances.",
            "Now we're going to dive into the hardware little bit and talk about how we these these commands here allow us to target hardware so.",
            "Let's look at how GPS put together well.",
            "GPU has threads at the lowest level of execution.",
            "It executes a bunch of threads.",
            "Thread is something that the many open MP programmers amongst you have heard of before.",
            "A thread of control is something that anybody that does multicore programming on a CPU understands.",
            "Only on a CPU.",
            "A thread is something that you hope doesn't substantial amount of work.",
            "A multi.",
            "When you talk about multi threaded core program on a CPU, that very often corresponds to a real long lasting chunk of work like for example.",
            "Your browsers are multi threaded.",
            "Anna thread corresponds very often to a tab, so these days some of the newest browsers, each tab in your browser is actually a whole separate process.",
            "But in some of them a tab is a threat of work at a tab in your browser, something that hangs around for a long time might do many, many millions of cycles of execution at joining.",
            "Some little video ad in a corner.",
            "It's running some animations somewhere else, so a thread on a CPU, substantial amount of work, and that's the way you think of a thread and open MP typically.",
            "You don't launch a thread to do a couple of ads.",
            "On the other hand, in a GPU, threads are very, very lightweight.",
            "They have very little overhead, none compared to a CPU, so you will launch a thread to do just a handful of math operations on a GPU.",
            "So we have lots of very lightweight threads flying around when we're using GPU's, and that's what corresponds to, and those get executed on an individual core of a GPU.",
            "So you buy a current GPU card.",
            "It's got a couple of 1000 cores on it or more, and each one of those cores.",
            "Is very good at just absorbing these lightweight threads that have a relatively small amount of work, so that's different from a thread that you've seen in again in the CPU world of multiple cores, each one of which is running a thread.",
            "Those threads get coalesce together into blocks.",
            "Those blocks are controlled by streaming multiprocessors with the SME's are and a GPU has some handful some dozen.",
            "Or in that order 10s of SMS."
        ],
        [
            "In the machine to do all the scheduling, let's look at particular card here.",
            "I'll pick a Kepler card, which is a little bit long in the tooth these days and obsolete.",
            "So I was tempted to update this to the very latest and greatest.",
            "But on the other hand, this corresponds to what a lot of you are going to find sitting around in your laptop kind of class.",
            "If you want to try some of these things, and more importantly, I'll throw up a chart showing some of the evolution of these cards and the numbers that correspond to it, and you can see that these things change a whole lot.",
            "Everything about to show you right here changes generation generation the generation.",
            "Come along every 14 months or so with the GPU world and so keeping track of this stuff becomes a real chore at the CUDA level, it's get.",
            "It's another thing about open ACC.",
            "So auto Kepler blocks are divided into 32 thread wide units called warps.",
            "So this is the kind of a chunk of control you've got to be very aware of as a CUDA programmer.",
            "The SM is managing these things at warp granularity, the cores are not completely independent.",
            "Smart little CPUs running away instead their managed in groups.",
            "The groups are slave together and have to have a lot of concurrency amongst them.",
            "And so this is kind of where the the mental picture or model that you have a GPU as not being a CPU kind of happens.",
            "You realize that the cores in a GPU are very simple minded and their slave together in these chunks and so your code can't diverge wildly from one thread to the next like you're welcome to do as an open MP programmer using cores in its CPU.",
            "But part of the magic here of having all of these cores synchronized as they are, is not only can you make a lot more of them because they require less sophistication and less transistors to make, but they have the ability then to coordinate their memory access.",
            "They can coalesce a lot of their memory accesses.",
            "That's one of the keys to the fact that GPU's have this tremendous memory bandwidth."
        ],
        [
            "Is when you have simple minded cores with limited operations and divergent in her behavior.",
            "Your memory system can do a lot more clever optimizations, so these are the kind of numbers that is a good programmer.",
            "You've got to keep in mind and these are the kind of numbers that will with our advanced open ACC features kind of we can.",
            "We can introduce this into our high level code if we think it's going to be helpful in optimization.",
            "So you'll notice that there are lots of of.",
            "In this case Kepler, or in a numbers here about how many threads in a block and a warp.",
            "How many threads in a warp?",
            "How many threads in a block?",
            "And these are suggestions and a CUDA programmer.",
            "When they're writing a code just because they know all of these details does not sit down.",
            "Plug in the numbers into their algorithm and they're confident that they understand exactly how things are contracted.",
            "Optimization instead even experienced good programmer takes a first guess at this stuff, then runs their code.",
            "And they always you tweak things a little bit, and you empirically determine what the best numbers are.",
            "It's impossible to."
        ],
        [
            "Just sit down to be very confident with all the moving pieces inside of a GPU that you know what's going on, so most people will start out using a CUDA programmer using the knowledge of the architecture of the device in front of them, that Kepler or whatever version of car they have.",
            "They'll take a good guess, an educated guess, and then they'll always tweak these numbers a little bit to see what's."
        ],
        [
            "Now the nice thing with open ACC is for you to tweak these numbers is trivially easy.",
            "The idea is that you in telling the compiler that you're going to guess.",
            "So you start out with this first one up here, where you say we've got just a colonels command on this loop, and it does its thing.",
            "And you think, I wonder if it's doing the best that can do or for some reason you're suspicious and you don't think it is.",
            "And by the way, these days there are lots of performance profiling tools you can use to get some idea of what your performance is very easily.",
            "NVIDIA has one called NB Purf which is just free and comes along with the ride and any of you that start doing a lot of this, you should use it.",
            "It's a friendly tool with the Big GUI front end.",
            "It's very easy to deploy and get into your your testing cycle so you don't have to guess in time your code.",
            "Silly a tool like that will give you good feedback so you try it and you suspect for some reason that you can do better.",
            "Well, it's very easy for you now to give the compiler some hints based on the loop sizes in your code and say, OK, I would like for you to have 100 gangs with a vector length of 128.",
            "It will take and it will interpret those numbers.",
            "Those suggestions and map those then to the hardware that underlies things.",
            "And it will implement that again.",
            "It will try its best, not make guarantees, it will try its best to do that.",
            "You can run your code and see what's going on.",
            "Here is one.",
            "We have a parallel region and we haven't got the parallel regions yet.",
            "We'll get to those in a moment.",
            "But in a parallel region we make some suggestions.",
            "It's very similar to what we have above and we run the code there an."
        ],
        [
            "See what how things improve now I'll give you an example of how effective or useless these things can be.",
            "I have a simple.",
            "The Saxby loop are very very simple.",
            "Add two vectors together, loop that we started with yesterday.",
            "And I run it with kernels.",
            "Well now I look at this and I go.",
            "I think I could do a little bit better.",
            "I know how long the loop is and what the architecture's machine is so I can give it a suggestion like this where I say I would like to have 100 gangs in this region and I vector length 128 and when I ran that I beat the compiler actually by about 20% on this loop.",
            "Now this was a good while ago as matter fact this was a couple of years ago and I'm reluctant to run it again because I think these days the compiler will probably beat me and my suggestion won't apply anymore.",
            "Because that's what compilers do.",
            "They get better and better at this simple minded kind of stuff.",
            "Generation to generation, and so my suggestions are less and less important.",
            "So that's a good thing.",
            "In general, it means you have to worry less and less about the little over details, but you shouldn't be put off from trying things because you can see how trivially easy it was for me to do this.",
            "I can just taken guess again based on the size, the amount of work that I'm asking a GPU to do, and some of it's magic numbers, which I'll show you in the next table and I can take it, guess and try it with very little effort.",
            "So the same approach the CUDA programmer takes, which is taking gas and then be willing to empirically search the parameter spaces of where a little bit I can do here with these open AC Direct."
        ],
        [
            "Here's the reason that this is an ongoing project, though you've got a very much a moving target and the general from generation to generation.",
            "Here all of these numbers which are important for you as a CUDA programmer have to be to be changed.",
            "They have to be accommodated in your code and you have to again do a little bit of empirical search each time around as an open ACC programmer, you hope that the compiler writers worry about these things and keep up with things, so these things change rapidly and again in the GPU world.",
            "Because it's such a new technology every 1416 months or so, there's a new generation of stuff coming out that substantially different, so you can kind of track this with the compute capability here.",
            "This is kind of a numbering of generations of GPU's, so if you're looking at what's in your laptop or what do you have sitting around your Department and you're trying to understand which generation of GPU you have?",
            "If you look at the compute capability, that kind of groups these families together and again as a programmer CUDA program."
        ],
        [
            "You really have to be aware of this, so these things are important because the capabilities again as the new technology is evolving so rapidly the capabilities have changed drastically.",
            "The performance capability here and this is on a matrix multiply we have right here, but it's really representative.",
            "General workloads that work well on GPU's capability and performance as a skyrocketing.",
            "So you do not want to say, yeah, I'll I'm not going to gain a whole lot by going, you know, tracking one generation to the next.",
            "Instead if you're going GPU programming."
        ],
        [
            "Later programming these days you are keeping up with the latest with the latest hardware.",
            "There's no point in getting involved with it to have a lasee Faire attitude like you might with serial code.",
            "It doesn't change much from 515 year period of the next, so let's look at another way that we can start to optimize our code and take a little more responsibility away from the Colonels command and start to take responsibility for exactly how the cores flow through our code is it will as you visit well, I'll demonstrate by example here.",
            "Uh.",
            "The open ACC Kernels Command has a history that comes from PGI.",
            "Again, there's a reason that I'm kind of favoring PGI compiler for a lot of stuff.",
            "We're doing PGI first came up with the idea of saying the compiler can do an awful lot of the work for us as a programmer, and this is at the same time that the open ACC Spec committee was starting to form.",
            "They were a big instigator in that it was actually the Open MP committee that sprung out of soap, e.g.",
            "I first had this thing called a region accelerator region in the code where they hope the compiler with lots of magic.",
            "For you, that is, since turned into the open ACC kernels we have today.",
            "On the other hand, the Open MP programmers amongst you and I know we have a substantial representation.",
            "This audience no that an open MP very much the same philosophy.",
            "You hope that an open MP parallel for loop or a do loop does a whole lot of magic for you and it very often will.",
            "If I'm writing an open MP program and I'll talk a little bit more about open MP here as we go, because even for those of you in art open, MP programmers.",
            "Open MP and Open ACC share an awful lot comedy.",
            "You've learned a good bit of what it takes to accelerate and open MP code as well just by learning open ACC.",
            "I'll make that even more dramatically clear when we compare open MP4 and open ACC later today.",
            "But as an open MP programmer you can also go from this magic command that's open MP parallel loop and very often does a great job of paralyzing a loop and just speeding your code up.",
            "There's not a lot of effort to put in there, you go after your big loops in your code, the same as we're going.",
            "You put one of these directives in front of it and it can hopefully substantially speed up a really important loop for you.",
            "But that might not be enough for you.",
            "You might have the idea.",
            "You can do a little bit better in that loop.",
            "You can eliminate some barriers or some synchronization, or you have a funky algorithm doesn't represent himself well, it's just a big for loop at an open MP you use a parallel region and in the parallel region you're going to take a lot more responsibility for managing things.",
            "Now is an open MP program, or you know when you jump into using parallel regions, you're making a big leap in responsibility of worry about synchronization barriers and race conditions.",
            "That's true, and open ACC as well, and open ACC when we go into a parallel region, which we're about to do right now.",
            "We use the pair."
        ],
        [
            "Well, construct we're going to take a lot of responsibility for worrying about where each core is up to will look at a race.",
            "Condition is probably the simplest way to explain that."
        ],
        [
            "But first I'll just show you the clause itself.",
            "The clauses basically say we're about to start a region in the code where you as a programmer taking responsibility now for doing a coordinating all of these cores that we've got available.",
            "So you can at the beginning of that region suggests how you'd like to break things up with gangs and workers in vector lengths."
        ],
        [
            "But the important thing to realize is that once you're in this region now, you have to take responsibility for thinking in your head about what every core is up to.",
            "What are they all doing?",
            "And this is the easiest thing I can show you.",
            "Here is a Saxby loop once again in a parallel region, so we've created a parallel region.",
            "We get to this part of the code.",
            "We are now telling all the cores on the device to go ahead and run as fast as you can.",
            "You're setting them free in hopes that you're going to get the maximum amount of work out of each core.",
            "In a parallel region here or a parallel region and open MP, the idea is that the biggest evil is idle time caused by barriers and synchronization points, and so you just said every core free, either an open MP or open ACC and say the beginning of parallel region.",
            "Go keep running, do all the work you can until I have to slow you down till I say hey stop here Chaltier synchronized with each other so you're setting all the course free.",
            "We set all the cores free.",
            "Well one of the ways that we can Marshall everybody together in synchronize them is to have a loop command here.",
            "In our code, and that's exactly what we do in this.",
            "In the Saxby example here is we put a loop command in the code and say hey this next block of code I want all the cores and you can think of several thousand cores here they're running away.",
            "I want you to cooperate on this loop and do what normally you'd expect the Kernel command to do here on this loop.",
            "Break this loop up into pieces and work on them independently and give me a huge speedup now."
        ],
        [
            "You might say, well, why not just do a colonels command, and in this particular case it would probably be silly not to, but the behavior is a little bit different here, so let's let's actually compare them.",
            "So this right here, this Colonels command over this for loop is exactly the same as doing a parallel region with a loop like that, so the same thing, no distinguishable difference in behavior, because as soon as we set all of our cores free we also focus them right back down in the loop.",
            "So it would be silly to do though."
        ],
        [
            "On the other hand, right here, this is an example, is pathologically wrong, right here?",
            "We've left out the Loop command if we have a parallel region like this, and we pointed out a loop what's going to happen here is that all of the cores.",
            "Each one of those cores is going to do this entire for loop itself, which is almost certainly wrong and useless.",
            "Each one of them is going to say N is equal to 1000.",
            "Here for this loop, each one of them is going to 1000 iterations of this loop, all of them redundantly, and that's meaningless and stupid and is going to give you incorrect."
        ],
        [
            "On the other hand, let's say we we do something a little more meaningful here.",
            "Now we've got a couple of for loops next to each other, and we want to to set our cores free in this.",
            "In this parallel region.",
            "Here in this kernels well.",
            "In this case we're not setting him free equation kernels here, so this this is the Safeway to do it here.",
            "So this first example here is using kernels where we're not setting the course free.",
            "Instead we say kernels.",
            "I want you to make kernels for this region of code and what it's going to do.",
            "Is it?",
            "Compiler is going to go in here and do the right nice logical thing on that first for loop.",
            "That it's going to be the right nice logic.",
            "Once a thing on that next for loop, so it's going to work right like you'd expect.",
            "But you, as a programmer trying to optimize this, might say that's good, but you know at the end the way kernels work is each kernel has.",
            "You know this entry point is exit point and it actually waits for all of the the cores to finish at the end of the loop before it moves on to the next part of the code.",
            "'cause kernels has pretty safe behavior.",
            "Kernels won't do anything behind the scenes.",
            "It's tricky, but I don't like that I'm going.",
            "I'm real serious optimization freak and I don't like the fact that some of my cores are waiting at the end of that.",
            "1st for Loop before they all continue on to the next part of the loop because some of them might have more work than another.",
            "You know, maybe when you've got a for loop, maybe some of them had 10 iterations of loop and some of them had eight because things that divide evenly, right?",
            "So some of them were getting done early and for optimization purposes I'd like to have the ones that are done early continue on to the second floor.",
            "But well, kernels won't allow that to happen, but we can."
        ],
        [
            "Let it happen.",
            "So we say we're going to parallel region, and we're going to set all of them loose and have them charge all the way through that first loop.",
            "They're going to.",
            "They're going to do the right thing on that first loop, because we gotta loop command there so they're not going to redundantly all do the loop.",
            "They're going to break that loop up and coordinate and do it in pieces.",
            "They go through that first loop, and then if someone get done early, they could charge ahead and start doing useful work in that 2nd loop, and that's great.",
            "Things are going to run faster, will have zero wasted time without a barrier in there where some are waiting for others to catch up.",
            "But the problem here that's open MP programmers.",
            "This is should be very familiar.",
            "It's the exact same situation when you start using parallel regions in an open MP code instead of using a parallel for a parallel do loop when we charge ahead here we better be paying attention.",
            "This code right here is probably going to break.",
            "Well, it's definitely prone to breakage, and the reason is that the 1st loop we're setting up a equal to some combination of BNC, right?",
            "Well, that's good and fine.",
            "However, some of the cores that get that early in that loop, we're going to go into the 2nd loop.",
            "And start setting going to the 2nd loop summer going to charge in the 2nd loop.",
            "Well, some of them might be finishing up in the 1st loop and they're going to use values of a here.",
            "That might not be set correctly yet in the 1st loop, some some cores doing threads in that first loop might still be working on values of A and we're here in the 2nd loop using values of a that might be set incorrectly.",
            "So we've introduced the possibility here of generating some incorrect results.",
            "So if you're going to start using parallel regions because you absolutely want to optimize the code silly, you have to pay attention again for an open MP programmer, this is exactly the same kind of problems that you need to be aware of.",
            "The same kind of race conditions you need to be aware of.",
            "In open MP4 open ACC programmers here this is the first you're seeing of the fact that if I'm not going to trust the Colonels command, or I don't think it's good enough, you're going to have to really pay attention to what you're actually doing.",
            "Asking the compiler to do so."
        ],
        [
            "Which is best parallel regions or currents?",
            "Well this is.",
            "This is an evolved this the answer to this question has changed over the years.",
            "When open ACC compilers first became available they operated just fine on obvious loops that did the most basic thing and so kernels was a way to go, but they couldn't see their way to optimize things and eliminate necessary barriers.",
            "Now they've gotten much better at that, so the number of places where kernels doesn't do pretty much exactly what you do have diminished and history kind.",
            "Of tends to favor compilers with this kind of stuff here.",
            "This very simple minded stuff of am I allowed to race ahead into the next set of four loops, because compilers pretty good at looking ahead and saying I was the same variable here and it there.",
            "Well then I need a barrier here or I don't need to Bury her.",
            "'cause that's not the case.",
            "There's no dependency between these two forms.",
            "Compilers are good at that kind of stuff, so for a lot of these cases here, an increasing number of them kernels does the trick.",
            "And again, this argument to the Open MP programmers out there.",
            "I hope somebody is presented that to you before parallel four loops in parallel do loops.",
            "Are fantastic, don't feel compelled to use parallel regions all over your code just cause it seems more sophisticated buys you nothing in many cases.",
            "On other hand, when you fire up a profiler or your experience and intimate knowledge of your algorithm, say, I know that some of the cores are wasting time waiting here.",
            "That's when you can say I've got parallel regions available to me.",
            "So again open ACC is not a handicap compared to CUDA in these cases right here, because if you say well if I use code I can make sure that all my cores are kept busy all the time.",
            "You have the ability to do that with parallel."
        ],
        [
            "Here as well.",
            "OK, let's talk about some things that you as new open ACC users didn't know were missing from the spec.",
            "The original Open Agency 1.0 spec.",
            "When things were still I won't say image sure, but because the spec itself was written with it in mind that compilers had some limitations and so did a lot of the early GPU's.",
            "But as GPU's have evolved rapidly again to be able to do a lot of things at a lower level, the spec upgraded to take advantage of that, so we can do a lot of fancy things with procedure calls and not worry about nested parallel."
        ],
        [
            "And whatnot.",
            "So here I give you example over here in open ACC 1.0, all of the procedures had to be inlined.",
            "If you wanted you could have subroutines.",
            "You couldn't have the GPU calling subroutines inside of itself just wasn't a physical reality, and so the compiler didn't give you that illusion these days.",
            "It's actually the case that the GPU itself can launch new kernels, and so the that's reflected in the standard.",
            "Here's an example of it right here.",
            "We have a routine called solver here that calls another subroutine.",
            "That does work as well, so solvers that actual GPU subroutine running on the GPU not on the CPU, and so is this other loop that it calls, which does more work.",
            "We can designate the fact that this is the situation to the compiler by saying in advance.",
            "Here we can tell it up.",
            "Here we can define this routine by saying this is a routine that runs on the GPU.",
            "It's a worker routine and it's called by a routine here.",
            "This running a number of gangs.",
            "So if we want to do things this way, we have to be aware of the hierarchy.",
            "'cause again can call workers, but a worker can't call gangs right?",
            "We've got gangs of workers.",
            "With vectors so we can do things, but we have to be aware of the high."
        ],
        [
            "Cherokee now, on the other hand, it's now possible to have kernels actually launch entirely new kernels within the code, so we don't have to worry about that hierarchy anymore, so we can have again a routine that we're saying this is this is a open ACC routine, so when it gets called later on, this is supposed to run on the GPU.",
            "And then here's a routine that is running will assume here on the GPU in a parallel region, and this loop itself then calls a subroutine within it.",
            "It's launching threads itself.",
            "Now it's not really a good idea to have thousands of threads launching thousands of new kernels.",
            "That's not the intent here.",
            "The intent is really to allow you to move all of the code onto the GPU.",
            "As I said, it's it's Nvidia's objective, it's it's the objective I guess of any program that really wants to get minimize any possibility of data management moving back and forth to put everything on the GPU so it will allow you to put everything on the GPU.",
            "Open.",
            "ACC now allows you to have routines actually calling other routines on the GPU.",
            "You can keep everything."
        ],
        [
            "Here's a more useful case about where we had originally something that quite you know, quite honestly, could be running on the front end, right here on the CPU that calls some GPU routines called solver.",
            "So solvers doing all the heavy lifting.",
            "Here, it's really the GPU routine, but for the sake of making sure everything's on the GPU, we can put the routine that calls solver itself on this GPU, so this makes everything run on the GPU, but on the GPU itself were not launching thousands and thousands of new new routines.",
            "Instead, we're calling a small.",
            "Of this solver and then solver is where all the thousands of cores are being launched and recruited.",
            "But again, the idea is open.",
            "ACC allows us to make sure we migrate all of the code to the GPU."
        ],
        [
            "I have a minimal amount running on the CPU.",
            "Now if you combine all these things together and you start doing things your again targeting the architecture, you're really targeting a specific architecture and optimizing perspective architecture.",
            "But you know there's there.",
            "You're trying to hide that as much as possible for code maintenance and everything else.",
            "So open ACS and very nice ways has a very nice way, in particular of saying do these parameters for this type of device, so you can say here if my device type is NVIDIA that I want 200 games.",
            "My advice type is radio on that I want 800 tanks, so you can do these optimizations and leave him in your code.",
            "Basically both open MP and open ACC try to eliminate the need for you have lots of ifdef senior code so I'm sure lots of you have seen lots of ifdef contaminating code out there, but modern specifications don't.",
            "Don't force you to do that, so if there is an open MP programmer you have lots of options like this as well.",
            "With open ACC you should not find yourself sticking if deaths in your code to support different devices or if open ACC is active is used in this part of the code, but you also have the serial code there you shouldn't need ifdef to separate those cases.",
            "Instead, the opening she directives have the ability to turn on and off pieces of code in different sections, or in this case here use different parameters.",
            "So don't litter your code in the modern era, you shouldn't find yourself writing lots of effects in your code, not for opening pier.",
            "Open ACC or not for too many."
        ],
        [
            "Other cases, either.",
            "You might ask yourself, you probably already asking yourself, what about if I have multiple devices plugged in?",
            "Because these days plugging in multiple GPU's to a single mode is a common thing on bridges.",
            "We have a couple of GPU's at least on every node.",
            "We actually have four on the nodes you guys have been using.",
            "Yes, you have to cause of the confusion early on.",
            "Yesterday is we had the wrong default parameter set up, so those of you were asking for a GPU.",
            "We're actually getting 4 GPS and that's that's why we ran out of GPU so fast.",
            "But but at any rate that's a common and useful configuration of multiple GPU's in a single node.",
            "At the same time as we've been saying this whole time, any modern CPU has multiple cores in it.",
            "So saying, how can I use all the cores along with multiple GPU's is a natural question to ask, and it's built into the spec, so it's not a big deal.",
            "Multiple threads in one device is fine, you don't really have to do anything, so if you've got a bunch of threads and you've got some nice open MP code there with all the threads using the GPU, that's fine.",
            "One thing it's a little bit less convenient or satisfactory is if you've got multiple threads in multiple devices, then you've got to break the data up to send to each device.",
            "I would prefer if this were a little more transparent, but the compilers haven't quite reached that point yet, so if you're doing that, you've got a slice your data up so that each thread is sending the data that corresponds to the right device.",
            "It's very straightforward to do with arrays shaping, so you know yesterday we discussed this kind of thing right here, but we've got slices of the data, so slicing the data up and sending it the GPS is straightforward.",
            "I just resent having to do bookkeeping, you know, I think this is compiler's job is to do bookkeeping where possible, so but in this case here, if you've got.",
            "A lot of GPU's plugged in and you want to do multi core programming at the same time it's up."
        ],
        [
            "You to coordinate it.",
            "And again, it's a straightforward task.",
            "It just I would prefer the compiler to do it for me to hide all of that.",
            "I want things again that I've mentioned continually is the motivation for optimizing using advanced features to eliminate barriers in your code to have all the cores racing ahead.",
            "Well, when you do that, you get into problems like our race condition that we saw a second ago.",
            "So then you need to put in barriers, so putting in and removing barriers is pretty much the main shore when you're doing this kind of threaded optimization, and so there's there are routines.",
            "The obvious routines are built into open AC delije deck.",
            "You have wait commands, you have a synchronous command to say don't wait in places where it would naturally wait.",
            "You have atomic clauses to say do this section atomically and as one piece."
        ],
        [
            "All those things are built in an in an obvious way.",
            "Data management.",
            "Yesterday I mentioned to us we for our data management.",
            "So far we've just been using data regions and then I threw an update there because it was necessary and you might.",
            "You might have and I know many of you did when we were doing the first exercise, worrying about the scope of the data region.",
            "And what about if I want to put the initialize routine?",
            "Several of you said I'd like to do the initialize routine that just sets the plate equal to 0.",
            "I'd like to do that on the GPU as well.",
            "Our final solution we have up there.",
            "I ignored initialized and my philosophy there in general was.",
            "Allies routines only done once.",
            "We don't need to worry about that.",
            "We we care about something that's done thousands of times, but on the other hand, those of you who are real perfectionist wanted to do the initial eyes on the GPU too.",
            "And it was a little awkward with our data region command to figure out how to make that initialize.",
            "How work on the GPU.",
            "At any rate, it's not at all awkward if we have a few additional commands which open ACC has to allow you to move data onto and off the GPU at will, not just in a region.",
            "So regions out very often a natural way to move data.",
            "You say.",
            "Here's the region where I'm doing some.",
            "I'm working on stuff on the GPU.",
            "This is where I'd like to move it onto and off the GPU.",
            "So region is probably the most useful one, but at any point in time you are free to just update things we've seen update already, but you can also have anywhere you want put and enter data and exit data region, so these commands allow you to just say to do instead of having to deal with the scope of big block.",
            "Instead, you can just say right now move all this stuff onto the GPU and then later on you exit data region, move all the stuff back off the GPU and that's very nice for something like initialization.",
            "If you've got a code where you've got initialization routine, it's living out there in its own place.",
            "Where you initialize stuff might be a good place to move everything on the GPU and then where you clean stuff up in your code might be a good time to pull stuff or where you need your results in the code might be a good time to pull everything back so you can just put those anywhere you want your code you want to fight with braces and figuring out what the scope should be.",
            "We also have some pointer commands it will look at when we talk about using CUDA and libraries with open ACC, the ability to just say hey some data is already on the GPU and here's a pointer to it is a nice thing to be able to do, so I'll save the.",
            "Detailed description of those till later, but you can absolutely do that so you don't have to worry about ever pulling the data back and forth at a time that you think is inappropriate.",
            "You can sit back and say this is what I want today to be on the GPU.",
            "This is when it needs to be back and these commands make it natural.",
            "You're not always fighting with a scope or a region, so we've seen."
        ],
        [
            "That profiling I'll mention quickly here.",
            "I'm always reluctant to talk about anything.",
            "It's platform dependent.",
            "You know I want to.",
            "So far everything we talked about an open standard open ACC is a standard and you can run on your laptop so you can run it on super computers.",
            "You can run it with compilers.",
            "You can get the PGI compiler for free with a student license and run it.",
            "You can go try the GCC compiler.",
            "You can do whatever you want is an open standard so I don't like to go into talks.",
            "Talk a whole lot about a particular tool or another, especially one that cost money.",
            "But in this case this ones free.",
            "So even though it's NVIDIA tool because it's free and you can just get it with the NVIDIA Toolkit.",
            "I'd be remiss if I didn't at least tell you that there's a great profiler."
        ],
        [
            "Out there and again profiling your code is important.",
            "I think people don't do it enough.",
            "It's always strikes me as silly that somebody will spend three or four months writing a code and then they won't spend half a day profiling and finding some way to squeeze a lot more performance out of it.",
            "Especially when these tools are so friendly so they all have GUI's these days and they will give you a pretty nice presentation of data.",
            "Here, for example, is a code that draws Mandelbrot graphs, and you look at it and you say, well, if you stare at this close enough here, you can see there's a region here where it does the communication.",
            "There well down there I really should get the stylus out.",
            "My fingers are too fat for this and then there's a region where it does the computation and you can see very well in this code that spends time spent sending data and it sends time computing and we might.",
            "It might be nice and a general nice thing to do in parallel computing is anytime you can overlap computation and communication you should do it.",
            "It's a big wit that applies to open it to MPI codes.",
            "It applies to open MP anytime you can overlap things.",
            "It's great, and in this case here, if you know what the Mandelbrot codes actually going, you might say, well, I'm drawing.",
            "Part of the mental block graph.",
            "I can import some more data and a way to do that is just a break thing."
        ],
        [
            "Up into pieces and so here's where I notice the problem.",
            "I see all these PCI transfers are taking all this time, and I'm sitting there waiting on them.",
            "What if I break?"
        ],
        [
            "The thing up into a little smaller pieces then, well, it's working on one piece.",
            "It can transfer data and they can overlap.",
            "So here's the thing blocked out and there's some very, very obvious ways and simple ways to block this particular algorithm.",
            "Other algorithms, communication and computation.",
            "It's hard to get him to overlap because they are really just separately separate things, but in this case here it was easy to break things up into."
        ],
        [
            "Cases and so.",
            "Here's kind of the optimization phase of this.",
            "You start out how fast this code runs on 16 cores it runs at.",
            "You know one is our baseline here.",
            "You paralyze it, which is the basic open ACC and you can get it up to about four times faster in this particular configuration here, turn it into blocks like I said, to allow it to overlap and we get up to 7 times faster, so that's a nice performance improvement.",
            "And then we can use some of synchronous commands.",
            "So I told.",
            "So if you look at this with the performance profiler here.",
            "You can go, oh, there's still some weights going on here, and I know that things don't have to wait because it doesn't really need that data.",
            "So you can stick in somebody's synchronous command and say don't wait here and poop.",
            "You can get it all the way up to hear that about 10 times faster.",
            "So this is the kind of optimization you can do starting with generic obvious open ACC things and with a profiler and trying some things you can you can get."
        ],
        [
            "Your performance now.",
            "What didn't we cover here?",
            "These are some of the miscellaneous things in the spec.",
            "We didn't talk about.",
            "Well, am I into our lunch break?",
            "So by the lunch break.",
            "OK well OK, sorry I didn't mean to run over 'cause we have plenty of time.",
            "I didn't need to.",
            "I could have stopped at any point, but we're on the last slide, so let me just say what I didn't cover here is a bunch of environment variables.",
            "Set stuff up.",
            "Make life easy.",
            "As I said, there are a bunch of things that make.",
            "If that's not important, so never find yourself writing effects to make this stuff work.",
            "Their API version instead of having directives.",
            "If you're willing to commit your code to being a GPU code instead of having these directive based things that look again like comments, you can stick in your code API versions, functions, subroutines that you call.",
            "That emulate all this behavior.",
            "You might find it more convenient and more natural way to code, but then we're jumping off of the keep keep serial code in parallel code in the same thing.",
            "Now we're going to have a GPU only code and many times it's perfectly acceptable.",
            "Lastly, hybrid programming, which I will talk about in great detail, so I don't need to right now.",
            "So OK. With that in mind, let's take our lunch break.",
            "I'll be glad to stand here and answer questions, question or two so it will not last long, but will also be returning to this right after lunch so.",
            "Don't know, don't feel too compelled to get your question right now, so can you, you, said Jesus, you can use can use GCC with open ACC or so.",
            "Just the PGI compiler, GCC.",
            "There are a number of compilers increasing number of compilers these days.",
            "If you go to open acc.org it will list all the compilers at work.",
            "GCC with five point X.",
            "They started working stuff in but with looks like with six point exit got more serious and the topic of where things are implemented well is a very important one.",
            "And we'll talk about that in great detail when we get into the open MP4.",
            "Versus open ACC later, so it's a good topic, but we'll get into it later.",
            "So thank you.",
            "We will return after lunch now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we have enough Slack in the schedule.",
                    "label": 0
                },
                {
                    "sent": "Fear not, we won't shortchange you on anything between between this morning and this afternoon that is.",
                    "label": 0
                },
                {
                    "sent": "We have we're going to switch gears I've I've given you kind of a slow paced, foundational introduction to open ACC.",
                    "label": 0
                },
                {
                    "sent": "We're going to switch gears and cover a lot more ground as I talk about some of the advanced stuff and how open ACC interacts with other other things that live out there in the world and will finish up with a hybrid challenge introduction, which I think will be interesting to you, not only if you're interested in the challenge, but also because it involves hybrid programming.",
                    "label": 0
                },
                {
                    "sent": "Again, how these things interact with the rest of the world.",
                    "label": 0
                },
                {
                    "sent": "But before we depart from the world of introductory open ACC.",
                    "label": 0
                },
                {
                    "sent": "I want to once again make sure everybody in the room is comfortable with either your solution or my solution, which in all likelihood looks a lot like yours to yesterday's exercise.",
                    "label": 0
                },
                {
                    "sent": "Does this make sense to everybody here?",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, this is a good time to answer some questions again, 'cause once we leave the station here, I'm going to take it as a given that you understand at least the basic components.",
                    "label": 0
                },
                {
                    "sent": "At least this solution.",
                    "label": 0
                },
                {
                    "sent": "Everybody good.",
                    "label": 0
                },
                {
                    "sent": "So little unbelievable, but I'll just be impressed instead of sceptical.",
                    "label": 0
                },
                {
                    "sent": "OK, well if that is the case then let's go on and talk about.",
                    "label": 0
                },
                {
                    "sent": "We'll start out with some.",
                    "label": 0
                },
                {
                    "sent": "This machine will come to life, will start out with some.",
                    "label": 0
                },
                {
                    "sent": "Advanced open ACC and then will.",
                    "label": 0
                },
                {
                    "sent": "Will get into all kinds of other.",
                    "label": 0
                },
                {
                    "sent": "Interesting pieces, but we'll start with this one.",
                    "label": 0
                },
                {
                    "sent": "Now what I want to do here?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is I won't do the typical approach to talking about advanced open ACC, or for that matter the way people live and talk about any of these MPI or open MP or any kind of library where they categorically go down the list or just go through the catalog the index as it were instead will group these things together into a couple of interesting pieces that may actually be relevant to you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll look at those.",
                    "label": 0
                },
                {
                    "sent": "And then I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you all the miscellaneous pieces that are out there.",
                    "label": 0
                },
                {
                    "sent": "We won't let him to let us let them distract us, but I'll make sure at the end that I mentioned anything that we overlooked intentionally.",
                    "label": 0
                },
                {
                    "sent": "But let's start out with a really interesting pieces of open ACC.",
                    "label": 0
                },
                {
                    "sent": "There advanced the first one here, will talk about is optimization using the advanced features because, as I mentioned to you yesterday, I am totally unwilling to trade off using a higher level approach if it costs us performance.",
                    "label": 0
                },
                {
                    "sent": "It's the whole point of high performance computing and using machines that cost 100 million dollars.",
                    "label": 0
                },
                {
                    "sent": "Is not to say it's acceptable for the sake of programmer productivity.",
                    "label": 0
                },
                {
                    "sent": "So for the sake of programmer laziness or lack of skill to let some inefficient code run on the machines that is not our philosophy here, so I am not introducing you to open ACC to enable you to write badly performing GPU codes, and I hope that kernels does the right thing.",
                    "label": 0
                },
                {
                    "sent": "The Kernels command that we've learned that puts an awful lot of responsibility for accelerating things on the compiler.",
                    "label": 0
                },
                {
                    "sent": "And every year that goes by every release of the compiler that comes along.",
                    "label": 0
                },
                {
                    "sent": "You hope it does a better and better job, but let's say that it's not going good enough job and you know that if you were a CUDA programmer working at a low level, you would be able to optimize things more well open ACC gives us the ability to make suggestions to the compiler that affect things at a lower level and allow us to.",
                    "label": 0
                },
                {
                    "sent": "We have lots of options and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we before I talk it to death while showing you some of those, let's jump right into how those options work.",
                    "label": 0
                },
                {
                    "sent": "The way that open ACC does it is, it targets the architecture.",
                    "label": 0
                },
                {
                    "sent": "But at an abstract way so that we don't end up saying, oh, we're working with well pieces.",
                    "label": 0
                },
                {
                    "sent": "I'm about to show you in a minute we're working with these low level hardware components instead.",
                    "label": 0
                },
                {
                    "sent": "It has an abstraction at a higher level, so we're target.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The architecture, but we're not really admitting it.",
                    "label": 0
                },
                {
                    "sent": "If I look at a piece of open ACC codes, it's using this stuff.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to see in there things that say we're using a K20 card here, or we're using as EON 5 instead.",
                    "label": 0
                },
                {
                    "sent": "We have an abstraction that allows us to map things to a lower level, and it's pretty straightforward and intuitive.",
                    "label": 0
                },
                {
                    "sent": "It basically just says that we have these.",
                    "label": 0
                },
                {
                    "sent": "Gangs of workers, each one of which has vectors of work to do.",
                    "label": 0
                },
                {
                    "sent": "And so by giving a granularity to our work with a couple of hierarchies to it, we can suggest that the compiler a good way to break up our work in distributed or lower level, which is the key to getting these GPU's to work well.",
                    "label": 0
                },
                {
                    "sent": "The key is basically saying that if the current if the compiler itself can't guess the right way to distribute work across the many thousands of cores that we have, then we can kind of massage.",
                    "label": 0
                },
                {
                    "sent": "That's guess a little bit with these things right here.",
                    "label": 0
                },
                {
                    "sent": "So we have a nice abstract.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hierarchy to work with that underneath the scenes targets hardware you won't see this in the spec.",
                    "label": 0
                },
                {
                    "sent": "The spec's job is to say hey, we're hiding this from you.",
                    "label": 0
                },
                {
                    "sent": "This is strictly suggestions to the compiler, but in reality this is what ends up happening in reality.",
                    "label": 0
                },
                {
                    "sent": "A worker or a gang sort of the top here gang targets at SM which is hardware component that will look at.",
                    "label": 0
                },
                {
                    "sent": "In a moment.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the largest chunk of of control in a GPU, or if we were targeting something like AZ on Fire device, which I'll talk about in some detail as well.",
                    "label": 0
                },
                {
                    "sent": "'cause this is a relevant thing in this day and age, if we're talking 'cause you don't fight.",
                    "label": 0
                },
                {
                    "sent": "It would target a CPU.",
                    "label": 0
                },
                {
                    "sent": "A worker targets what the GPU is called a warp, and again, I'll I'll define these things for you or on a CPU at Target score.",
                    "label": 0
                },
                {
                    "sent": "A vector looks an awful lot like your classical vector size, chunk of work, and any serial computer works with these days, and that's equivalent with red on a GPU.",
                    "label": 0
                },
                {
                    "sent": "So these things really do translate to something in the real hardware world, although the spec won't guarantee it, and the compiler is free to reinterpret it.",
                    "label": 0
                },
                {
                    "sent": "And as a matter of fact, sometimes does the compiler.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes thinks that it can do a better job even when you suggest these things to it, and it sometimes does.",
                    "label": 0
                },
                {
                    "sent": "Now I've been talking about all of this.",
                    "label": 0
                },
                {
                    "sent": "These hardware things right here.",
                    "label": 0
                },
                {
                    "sent": "I've started talking about hardware, which again I think it should impress you that we managed to not talk about hardware all yesterday, right?",
                    "label": 0
                },
                {
                    "sent": "So far as far as you're concerned, a GPU is a black box, and yet we were able to get a lot of capability on it under the right circumstances.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to dive into the hardware little bit and talk about how we these these commands here allow us to target hardware so.",
                    "label": 0
                },
                {
                    "sent": "Let's look at how GPS put together well.",
                    "label": 0
                },
                {
                    "sent": "GPU has threads at the lowest level of execution.",
                    "label": 0
                },
                {
                    "sent": "It executes a bunch of threads.",
                    "label": 0
                },
                {
                    "sent": "Thread is something that the many open MP programmers amongst you have heard of before.",
                    "label": 0
                },
                {
                    "sent": "A thread of control is something that anybody that does multicore programming on a CPU understands.",
                    "label": 0
                },
                {
                    "sent": "Only on a CPU.",
                    "label": 0
                },
                {
                    "sent": "A thread is something that you hope doesn't substantial amount of work.",
                    "label": 0
                },
                {
                    "sent": "A multi.",
                    "label": 0
                },
                {
                    "sent": "When you talk about multi threaded core program on a CPU, that very often corresponds to a real long lasting chunk of work like for example.",
                    "label": 0
                },
                {
                    "sent": "Your browsers are multi threaded.",
                    "label": 0
                },
                {
                    "sent": "Anna thread corresponds very often to a tab, so these days some of the newest browsers, each tab in your browser is actually a whole separate process.",
                    "label": 0
                },
                {
                    "sent": "But in some of them a tab is a threat of work at a tab in your browser, something that hangs around for a long time might do many, many millions of cycles of execution at joining.",
                    "label": 0
                },
                {
                    "sent": "Some little video ad in a corner.",
                    "label": 0
                },
                {
                    "sent": "It's running some animations somewhere else, so a thread on a CPU, substantial amount of work, and that's the way you think of a thread and open MP typically.",
                    "label": 0
                },
                {
                    "sent": "You don't launch a thread to do a couple of ads.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in a GPU, threads are very, very lightweight.",
                    "label": 0
                },
                {
                    "sent": "They have very little overhead, none compared to a CPU, so you will launch a thread to do just a handful of math operations on a GPU.",
                    "label": 0
                },
                {
                    "sent": "So we have lots of very lightweight threads flying around when we're using GPU's, and that's what corresponds to, and those get executed on an individual core of a GPU.",
                    "label": 0
                },
                {
                    "sent": "So you buy a current GPU card.",
                    "label": 0
                },
                {
                    "sent": "It's got a couple of 1000 cores on it or more, and each one of those cores.",
                    "label": 0
                },
                {
                    "sent": "Is very good at just absorbing these lightweight threads that have a relatively small amount of work, so that's different from a thread that you've seen in again in the CPU world of multiple cores, each one of which is running a thread.",
                    "label": 0
                },
                {
                    "sent": "Those threads get coalesce together into blocks.",
                    "label": 0
                },
                {
                    "sent": "Those blocks are controlled by streaming multiprocessors with the SME's are and a GPU has some handful some dozen.",
                    "label": 0
                },
                {
                    "sent": "Or in that order 10s of SMS.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the machine to do all the scheduling, let's look at particular card here.",
                    "label": 0
                },
                {
                    "sent": "I'll pick a Kepler card, which is a little bit long in the tooth these days and obsolete.",
                    "label": 0
                },
                {
                    "sent": "So I was tempted to update this to the very latest and greatest.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, this corresponds to what a lot of you are going to find sitting around in your laptop kind of class.",
                    "label": 0
                },
                {
                    "sent": "If you want to try some of these things, and more importantly, I'll throw up a chart showing some of the evolution of these cards and the numbers that correspond to it, and you can see that these things change a whole lot.",
                    "label": 0
                },
                {
                    "sent": "Everything about to show you right here changes generation generation the generation.",
                    "label": 0
                },
                {
                    "sent": "Come along every 14 months or so with the GPU world and so keeping track of this stuff becomes a real chore at the CUDA level, it's get.",
                    "label": 0
                },
                {
                    "sent": "It's another thing about open ACC.",
                    "label": 0
                },
                {
                    "sent": "So auto Kepler blocks are divided into 32 thread wide units called warps.",
                    "label": 1
                },
                {
                    "sent": "So this is the kind of a chunk of control you've got to be very aware of as a CUDA programmer.",
                    "label": 1
                },
                {
                    "sent": "The SM is managing these things at warp granularity, the cores are not completely independent.",
                    "label": 0
                },
                {
                    "sent": "Smart little CPUs running away instead their managed in groups.",
                    "label": 0
                },
                {
                    "sent": "The groups are slave together and have to have a lot of concurrency amongst them.",
                    "label": 0
                },
                {
                    "sent": "And so this is kind of where the the mental picture or model that you have a GPU as not being a CPU kind of happens.",
                    "label": 0
                },
                {
                    "sent": "You realize that the cores in a GPU are very simple minded and their slave together in these chunks and so your code can't diverge wildly from one thread to the next like you're welcome to do as an open MP programmer using cores in its CPU.",
                    "label": 0
                },
                {
                    "sent": "But part of the magic here of having all of these cores synchronized as they are, is not only can you make a lot more of them because they require less sophistication and less transistors to make, but they have the ability then to coordinate their memory access.",
                    "label": 0
                },
                {
                    "sent": "They can coalesce a lot of their memory accesses.",
                    "label": 0
                },
                {
                    "sent": "That's one of the keys to the fact that GPU's have this tremendous memory bandwidth.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is when you have simple minded cores with limited operations and divergent in her behavior.",
                    "label": 0
                },
                {
                    "sent": "Your memory system can do a lot more clever optimizations, so these are the kind of numbers that is a good programmer.",
                    "label": 0
                },
                {
                    "sent": "You've got to keep in mind and these are the kind of numbers that will with our advanced open ACC features kind of we can.",
                    "label": 0
                },
                {
                    "sent": "We can introduce this into our high level code if we think it's going to be helpful in optimization.",
                    "label": 0
                },
                {
                    "sent": "So you'll notice that there are lots of of.",
                    "label": 0
                },
                {
                    "sent": "In this case Kepler, or in a numbers here about how many threads in a block and a warp.",
                    "label": 1
                },
                {
                    "sent": "How many threads in a warp?",
                    "label": 1
                },
                {
                    "sent": "How many threads in a block?",
                    "label": 0
                },
                {
                    "sent": "And these are suggestions and a CUDA programmer.",
                    "label": 0
                },
                {
                    "sent": "When they're writing a code just because they know all of these details does not sit down.",
                    "label": 0
                },
                {
                    "sent": "Plug in the numbers into their algorithm and they're confident that they understand exactly how things are contracted.",
                    "label": 0
                },
                {
                    "sent": "Optimization instead even experienced good programmer takes a first guess at this stuff, then runs their code.",
                    "label": 0
                },
                {
                    "sent": "And they always you tweak things a little bit, and you empirically determine what the best numbers are.",
                    "label": 0
                },
                {
                    "sent": "It's impossible to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just sit down to be very confident with all the moving pieces inside of a GPU that you know what's going on, so most people will start out using a CUDA programmer using the knowledge of the architecture of the device in front of them, that Kepler or whatever version of car they have.",
                    "label": 0
                },
                {
                    "sent": "They'll take a good guess, an educated guess, and then they'll always tweak these numbers a little bit to see what's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the nice thing with open ACC is for you to tweak these numbers is trivially easy.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you in telling the compiler that you're going to guess.",
                    "label": 0
                },
                {
                    "sent": "So you start out with this first one up here, where you say we've got just a colonels command on this loop, and it does its thing.",
                    "label": 0
                },
                {
                    "sent": "And you think, I wonder if it's doing the best that can do or for some reason you're suspicious and you don't think it is.",
                    "label": 0
                },
                {
                    "sent": "And by the way, these days there are lots of performance profiling tools you can use to get some idea of what your performance is very easily.",
                    "label": 0
                },
                {
                    "sent": "NVIDIA has one called NB Purf which is just free and comes along with the ride and any of you that start doing a lot of this, you should use it.",
                    "label": 0
                },
                {
                    "sent": "It's a friendly tool with the Big GUI front end.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to deploy and get into your your testing cycle so you don't have to guess in time your code.",
                    "label": 0
                },
                {
                    "sent": "Silly a tool like that will give you good feedback so you try it and you suspect for some reason that you can do better.",
                    "label": 0
                },
                {
                    "sent": "Well, it's very easy for you now to give the compiler some hints based on the loop sizes in your code and say, OK, I would like for you to have 100 gangs with a vector length of 128.",
                    "label": 0
                },
                {
                    "sent": "It will take and it will interpret those numbers.",
                    "label": 0
                },
                {
                    "sent": "Those suggestions and map those then to the hardware that underlies things.",
                    "label": 0
                },
                {
                    "sent": "And it will implement that again.",
                    "label": 0
                },
                {
                    "sent": "It will try its best, not make guarantees, it will try its best to do that.",
                    "label": 0
                },
                {
                    "sent": "You can run your code and see what's going on.",
                    "label": 0
                },
                {
                    "sent": "Here is one.",
                    "label": 0
                },
                {
                    "sent": "We have a parallel region and we haven't got the parallel regions yet.",
                    "label": 0
                },
                {
                    "sent": "We'll get to those in a moment.",
                    "label": 0
                },
                {
                    "sent": "But in a parallel region we make some suggestions.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to what we have above and we run the code there an.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what how things improve now I'll give you an example of how effective or useless these things can be.",
                    "label": 0
                },
                {
                    "sent": "I have a simple.",
                    "label": 0
                },
                {
                    "sent": "The Saxby loop are very very simple.",
                    "label": 0
                },
                {
                    "sent": "Add two vectors together, loop that we started with yesterday.",
                    "label": 0
                },
                {
                    "sent": "And I run it with kernels.",
                    "label": 0
                },
                {
                    "sent": "Well now I look at this and I go.",
                    "label": 0
                },
                {
                    "sent": "I think I could do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "I know how long the loop is and what the architecture's machine is so I can give it a suggestion like this where I say I would like to have 100 gangs in this region and I vector length 128 and when I ran that I beat the compiler actually by about 20% on this loop.",
                    "label": 0
                },
                {
                    "sent": "Now this was a good while ago as matter fact this was a couple of years ago and I'm reluctant to run it again because I think these days the compiler will probably beat me and my suggestion won't apply anymore.",
                    "label": 0
                },
                {
                    "sent": "Because that's what compilers do.",
                    "label": 0
                },
                {
                    "sent": "They get better and better at this simple minded kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "Generation to generation, and so my suggestions are less and less important.",
                    "label": 0
                },
                {
                    "sent": "So that's a good thing.",
                    "label": 0
                },
                {
                    "sent": "In general, it means you have to worry less and less about the little over details, but you shouldn't be put off from trying things because you can see how trivially easy it was for me to do this.",
                    "label": 0
                },
                {
                    "sent": "I can just taken guess again based on the size, the amount of work that I'm asking a GPU to do, and some of it's magic numbers, which I'll show you in the next table and I can take it, guess and try it with very little effort.",
                    "label": 0
                },
                {
                    "sent": "So the same approach the CUDA programmer takes, which is taking gas and then be willing to empirically search the parameter spaces of where a little bit I can do here with these open AC Direct.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the reason that this is an ongoing project, though you've got a very much a moving target and the general from generation to generation.",
                    "label": 0
                },
                {
                    "sent": "Here all of these numbers which are important for you as a CUDA programmer have to be to be changed.",
                    "label": 1
                },
                {
                    "sent": "They have to be accommodated in your code and you have to again do a little bit of empirical search each time around as an open ACC programmer, you hope that the compiler writers worry about these things and keep up with things, so these things change rapidly and again in the GPU world.",
                    "label": 1
                },
                {
                    "sent": "Because it's such a new technology every 1416 months or so, there's a new generation of stuff coming out that substantially different, so you can kind of track this with the compute capability here.",
                    "label": 1
                },
                {
                    "sent": "This is kind of a numbering of generations of GPU's, so if you're looking at what's in your laptop or what do you have sitting around your Department and you're trying to understand which generation of GPU you have?",
                    "label": 0
                },
                {
                    "sent": "If you look at the compute capability, that kind of groups these families together and again as a programmer CUDA program.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You really have to be aware of this, so these things are important because the capabilities again as the new technology is evolving so rapidly the capabilities have changed drastically.",
                    "label": 0
                },
                {
                    "sent": "The performance capability here and this is on a matrix multiply we have right here, but it's really representative.",
                    "label": 0
                },
                {
                    "sent": "General workloads that work well on GPU's capability and performance as a skyrocketing.",
                    "label": 0
                },
                {
                    "sent": "So you do not want to say, yeah, I'll I'm not going to gain a whole lot by going, you know, tracking one generation to the next.",
                    "label": 0
                },
                {
                    "sent": "Instead if you're going GPU programming.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later programming these days you are keeping up with the latest with the latest hardware.",
                    "label": 0
                },
                {
                    "sent": "There's no point in getting involved with it to have a lasee Faire attitude like you might with serial code.",
                    "label": 0
                },
                {
                    "sent": "It doesn't change much from 515 year period of the next, so let's look at another way that we can start to optimize our code and take a little more responsibility away from the Colonels command and start to take responsibility for exactly how the cores flow through our code is it will as you visit well, I'll demonstrate by example here.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "The open ACC Kernels Command has a history that comes from PGI.",
                    "label": 0
                },
                {
                    "sent": "Again, there's a reason that I'm kind of favoring PGI compiler for a lot of stuff.",
                    "label": 0
                },
                {
                    "sent": "We're doing PGI first came up with the idea of saying the compiler can do an awful lot of the work for us as a programmer, and this is at the same time that the open ACC Spec committee was starting to form.",
                    "label": 0
                },
                {
                    "sent": "They were a big instigator in that it was actually the Open MP committee that sprung out of soap, e.g.",
                    "label": 0
                },
                {
                    "sent": "I first had this thing called a region accelerator region in the code where they hope the compiler with lots of magic.",
                    "label": 1
                },
                {
                    "sent": "For you, that is, since turned into the open ACC kernels we have today.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, the Open MP programmers amongst you and I know we have a substantial representation.",
                    "label": 0
                },
                {
                    "sent": "This audience no that an open MP very much the same philosophy.",
                    "label": 0
                },
                {
                    "sent": "You hope that an open MP parallel for loop or a do loop does a whole lot of magic for you and it very often will.",
                    "label": 0
                },
                {
                    "sent": "If I'm writing an open MP program and I'll talk a little bit more about open MP here as we go, because even for those of you in art open, MP programmers.",
                    "label": 0
                },
                {
                    "sent": "Open MP and Open ACC share an awful lot comedy.",
                    "label": 0
                },
                {
                    "sent": "You've learned a good bit of what it takes to accelerate and open MP code as well just by learning open ACC.",
                    "label": 0
                },
                {
                    "sent": "I'll make that even more dramatically clear when we compare open MP4 and open ACC later today.",
                    "label": 0
                },
                {
                    "sent": "But as an open MP programmer you can also go from this magic command that's open MP parallel loop and very often does a great job of paralyzing a loop and just speeding your code up.",
                    "label": 0
                },
                {
                    "sent": "There's not a lot of effort to put in there, you go after your big loops in your code, the same as we're going.",
                    "label": 0
                },
                {
                    "sent": "You put one of these directives in front of it and it can hopefully substantially speed up a really important loop for you.",
                    "label": 0
                },
                {
                    "sent": "But that might not be enough for you.",
                    "label": 0
                },
                {
                    "sent": "You might have the idea.",
                    "label": 1
                },
                {
                    "sent": "You can do a little bit better in that loop.",
                    "label": 0
                },
                {
                    "sent": "You can eliminate some barriers or some synchronization, or you have a funky algorithm doesn't represent himself well, it's just a big for loop at an open MP you use a parallel region and in the parallel region you're going to take a lot more responsibility for managing things.",
                    "label": 0
                },
                {
                    "sent": "Now is an open MP program, or you know when you jump into using parallel regions, you're making a big leap in responsibility of worry about synchronization barriers and race conditions.",
                    "label": 0
                },
                {
                    "sent": "That's true, and open ACC as well, and open ACC when we go into a parallel region, which we're about to do right now.",
                    "label": 0
                },
                {
                    "sent": "We use the pair.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, construct we're going to take a lot of responsibility for worrying about where each core is up to will look at a race.",
                    "label": 0
                },
                {
                    "sent": "Condition is probably the simplest way to explain that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But first I'll just show you the clause itself.",
                    "label": 0
                },
                {
                    "sent": "The clauses basically say we're about to start a region in the code where you as a programmer taking responsibility now for doing a coordinating all of these cores that we've got available.",
                    "label": 0
                },
                {
                    "sent": "So you can at the beginning of that region suggests how you'd like to break things up with gangs and workers in vector lengths.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the important thing to realize is that once you're in this region now, you have to take responsibility for thinking in your head about what every core is up to.",
                    "label": 0
                },
                {
                    "sent": "What are they all doing?",
                    "label": 0
                },
                {
                    "sent": "And this is the easiest thing I can show you.",
                    "label": 0
                },
                {
                    "sent": "Here is a Saxby loop once again in a parallel region, so we've created a parallel region.",
                    "label": 0
                },
                {
                    "sent": "We get to this part of the code.",
                    "label": 0
                },
                {
                    "sent": "We are now telling all the cores on the device to go ahead and run as fast as you can.",
                    "label": 0
                },
                {
                    "sent": "You're setting them free in hopes that you're going to get the maximum amount of work out of each core.",
                    "label": 0
                },
                {
                    "sent": "In a parallel region here or a parallel region and open MP, the idea is that the biggest evil is idle time caused by barriers and synchronization points, and so you just said every core free, either an open MP or open ACC and say the beginning of parallel region.",
                    "label": 1
                },
                {
                    "sent": "Go keep running, do all the work you can until I have to slow you down till I say hey stop here Chaltier synchronized with each other so you're setting all the course free.",
                    "label": 0
                },
                {
                    "sent": "We set all the cores free.",
                    "label": 0
                },
                {
                    "sent": "Well one of the ways that we can Marshall everybody together in synchronize them is to have a loop command here.",
                    "label": 0
                },
                {
                    "sent": "In our code, and that's exactly what we do in this.",
                    "label": 0
                },
                {
                    "sent": "In the Saxby example here is we put a loop command in the code and say hey this next block of code I want all the cores and you can think of several thousand cores here they're running away.",
                    "label": 0
                },
                {
                    "sent": "I want you to cooperate on this loop and do what normally you'd expect the Kernel command to do here on this loop.",
                    "label": 0
                },
                {
                    "sent": "Break this loop up into pieces and work on them independently and give me a huge speedup now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might say, well, why not just do a colonels command, and in this particular case it would probably be silly not to, but the behavior is a little bit different here, so let's let's actually compare them.",
                    "label": 0
                },
                {
                    "sent": "So this right here, this Colonels command over this for loop is exactly the same as doing a parallel region with a loop like that, so the same thing, no distinguishable difference in behavior, because as soon as we set all of our cores free we also focus them right back down in the loop.",
                    "label": 0
                },
                {
                    "sent": "So it would be silly to do though.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, right here, this is an example, is pathologically wrong, right here?",
                    "label": 0
                },
                {
                    "sent": "We've left out the Loop command if we have a parallel region like this, and we pointed out a loop what's going to happen here is that all of the cores.",
                    "label": 1
                },
                {
                    "sent": "Each one of those cores is going to do this entire for loop itself, which is almost certainly wrong and useless.",
                    "label": 0
                },
                {
                    "sent": "Each one of them is going to say N is equal to 1000.",
                    "label": 0
                },
                {
                    "sent": "Here for this loop, each one of them is going to 1000 iterations of this loop, all of them redundantly, and that's meaningless and stupid and is going to give you incorrect.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, let's say we we do something a little more meaningful here.",
                    "label": 0
                },
                {
                    "sent": "Now we've got a couple of for loops next to each other, and we want to to set our cores free in this.",
                    "label": 0
                },
                {
                    "sent": "In this parallel region.",
                    "label": 0
                },
                {
                    "sent": "Here in this kernels well.",
                    "label": 0
                },
                {
                    "sent": "In this case we're not setting him free equation kernels here, so this this is the Safeway to do it here.",
                    "label": 0
                },
                {
                    "sent": "So this first example here is using kernels where we're not setting the course free.",
                    "label": 0
                },
                {
                    "sent": "Instead we say kernels.",
                    "label": 0
                },
                {
                    "sent": "I want you to make kernels for this region of code and what it's going to do.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Compiler is going to go in here and do the right nice logical thing on that first for loop.",
                    "label": 0
                },
                {
                    "sent": "That it's going to be the right nice logic.",
                    "label": 0
                },
                {
                    "sent": "Once a thing on that next for loop, so it's going to work right like you'd expect.",
                    "label": 0
                },
                {
                    "sent": "But you, as a programmer trying to optimize this, might say that's good, but you know at the end the way kernels work is each kernel has.",
                    "label": 0
                },
                {
                    "sent": "You know this entry point is exit point and it actually waits for all of the the cores to finish at the end of the loop before it moves on to the next part of the code.",
                    "label": 0
                },
                {
                    "sent": "'cause kernels has pretty safe behavior.",
                    "label": 0
                },
                {
                    "sent": "Kernels won't do anything behind the scenes.",
                    "label": 0
                },
                {
                    "sent": "It's tricky, but I don't like that I'm going.",
                    "label": 0
                },
                {
                    "sent": "I'm real serious optimization freak and I don't like the fact that some of my cores are waiting at the end of that.",
                    "label": 0
                },
                {
                    "sent": "1st for Loop before they all continue on to the next part of the loop because some of them might have more work than another.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe when you've got a for loop, maybe some of them had 10 iterations of loop and some of them had eight because things that divide evenly, right?",
                    "label": 0
                },
                {
                    "sent": "So some of them were getting done early and for optimization purposes I'd like to have the ones that are done early continue on to the second floor.",
                    "label": 0
                },
                {
                    "sent": "But well, kernels won't allow that to happen, but we can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let it happen.",
                    "label": 0
                },
                {
                    "sent": "So we say we're going to parallel region, and we're going to set all of them loose and have them charge all the way through that first loop.",
                    "label": 1
                },
                {
                    "sent": "They're going to.",
                    "label": 0
                },
                {
                    "sent": "They're going to do the right thing on that first loop, because we gotta loop command there so they're not going to redundantly all do the loop.",
                    "label": 0
                },
                {
                    "sent": "They're going to break that loop up and coordinate and do it in pieces.",
                    "label": 0
                },
                {
                    "sent": "They go through that first loop, and then if someone get done early, they could charge ahead and start doing useful work in that 2nd loop, and that's great.",
                    "label": 0
                },
                {
                    "sent": "Things are going to run faster, will have zero wasted time without a barrier in there where some are waiting for others to catch up.",
                    "label": 0
                },
                {
                    "sent": "But the problem here that's open MP programmers.",
                    "label": 0
                },
                {
                    "sent": "This is should be very familiar.",
                    "label": 0
                },
                {
                    "sent": "It's the exact same situation when you start using parallel regions in an open MP code instead of using a parallel for a parallel do loop when we charge ahead here we better be paying attention.",
                    "label": 0
                },
                {
                    "sent": "This code right here is probably going to break.",
                    "label": 0
                },
                {
                    "sent": "Well, it's definitely prone to breakage, and the reason is that the 1st loop we're setting up a equal to some combination of BNC, right?",
                    "label": 0
                },
                {
                    "sent": "Well, that's good and fine.",
                    "label": 1
                },
                {
                    "sent": "However, some of the cores that get that early in that loop, we're going to go into the 2nd loop.",
                    "label": 0
                },
                {
                    "sent": "And start setting going to the 2nd loop summer going to charge in the 2nd loop.",
                    "label": 1
                },
                {
                    "sent": "Well, some of them might be finishing up in the 1st loop and they're going to use values of a here.",
                    "label": 1
                },
                {
                    "sent": "That might not be set correctly yet in the 1st loop, some some cores doing threads in that first loop might still be working on values of A and we're here in the 2nd loop using values of a that might be set incorrectly.",
                    "label": 0
                },
                {
                    "sent": "So we've introduced the possibility here of generating some incorrect results.",
                    "label": 1
                },
                {
                    "sent": "So if you're going to start using parallel regions because you absolutely want to optimize the code silly, you have to pay attention again for an open MP programmer, this is exactly the same kind of problems that you need to be aware of.",
                    "label": 0
                },
                {
                    "sent": "The same kind of race conditions you need to be aware of.",
                    "label": 1
                },
                {
                    "sent": "In open MP4 open ACC programmers here this is the first you're seeing of the fact that if I'm not going to trust the Colonels command, or I don't think it's good enough, you're going to have to really pay attention to what you're actually doing.",
                    "label": 0
                },
                {
                    "sent": "Asking the compiler to do so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is best parallel regions or currents?",
                    "label": 1
                },
                {
                    "sent": "Well this is.",
                    "label": 0
                },
                {
                    "sent": "This is an evolved this the answer to this question has changed over the years.",
                    "label": 0
                },
                {
                    "sent": "When open ACC compilers first became available they operated just fine on obvious loops that did the most basic thing and so kernels was a way to go, but they couldn't see their way to optimize things and eliminate necessary barriers.",
                    "label": 0
                },
                {
                    "sent": "Now they've gotten much better at that, so the number of places where kernels doesn't do pretty much exactly what you do have diminished and history kind.",
                    "label": 1
                },
                {
                    "sent": "Of tends to favor compilers with this kind of stuff here.",
                    "label": 0
                },
                {
                    "sent": "This very simple minded stuff of am I allowed to race ahead into the next set of four loops, because compilers pretty good at looking ahead and saying I was the same variable here and it there.",
                    "label": 0
                },
                {
                    "sent": "Well then I need a barrier here or I don't need to Bury her.",
                    "label": 0
                },
                {
                    "sent": "'cause that's not the case.",
                    "label": 0
                },
                {
                    "sent": "There's no dependency between these two forms.",
                    "label": 1
                },
                {
                    "sent": "Compilers are good at that kind of stuff, so for a lot of these cases here, an increasing number of them kernels does the trick.",
                    "label": 0
                },
                {
                    "sent": "And again, this argument to the Open MP programmers out there.",
                    "label": 0
                },
                {
                    "sent": "I hope somebody is presented that to you before parallel four loops in parallel do loops.",
                    "label": 1
                },
                {
                    "sent": "Are fantastic, don't feel compelled to use parallel regions all over your code just cause it seems more sophisticated buys you nothing in many cases.",
                    "label": 0
                },
                {
                    "sent": "On other hand, when you fire up a profiler or your experience and intimate knowledge of your algorithm, say, I know that some of the cores are wasting time waiting here.",
                    "label": 0
                },
                {
                    "sent": "That's when you can say I've got parallel regions available to me.",
                    "label": 0
                },
                {
                    "sent": "So again open ACC is not a handicap compared to CUDA in these cases right here, because if you say well if I use code I can make sure that all my cores are kept busy all the time.",
                    "label": 0
                },
                {
                    "sent": "You have the ability to do that with parallel.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here as well.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about some things that you as new open ACC users didn't know were missing from the spec.",
                    "label": 1
                },
                {
                    "sent": "The original Open Agency 1.0 spec.",
                    "label": 1
                },
                {
                    "sent": "When things were still I won't say image sure, but because the spec itself was written with it in mind that compilers had some limitations and so did a lot of the early GPU's.",
                    "label": 0
                },
                {
                    "sent": "But as GPU's have evolved rapidly again to be able to do a lot of things at a lower level, the spec upgraded to take advantage of that, so we can do a lot of fancy things with procedure calls and not worry about nested parallel.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And whatnot.",
                    "label": 0
                },
                {
                    "sent": "So here I give you example over here in open ACC 1.0, all of the procedures had to be inlined.",
                    "label": 1
                },
                {
                    "sent": "If you wanted you could have subroutines.",
                    "label": 0
                },
                {
                    "sent": "You couldn't have the GPU calling subroutines inside of itself just wasn't a physical reality, and so the compiler didn't give you that illusion these days.",
                    "label": 0
                },
                {
                    "sent": "It's actually the case that the GPU itself can launch new kernels, and so the that's reflected in the standard.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of it right here.",
                    "label": 0
                },
                {
                    "sent": "We have a routine called solver here that calls another subroutine.",
                    "label": 0
                },
                {
                    "sent": "That does work as well, so solvers that actual GPU subroutine running on the GPU not on the CPU, and so is this other loop that it calls, which does more work.",
                    "label": 0
                },
                {
                    "sent": "We can designate the fact that this is the situation to the compiler by saying in advance.",
                    "label": 0
                },
                {
                    "sent": "Here we can tell it up.",
                    "label": 0
                },
                {
                    "sent": "Here we can define this routine by saying this is a routine that runs on the GPU.",
                    "label": 0
                },
                {
                    "sent": "It's a worker routine and it's called by a routine here.",
                    "label": 1
                },
                {
                    "sent": "This running a number of gangs.",
                    "label": 0
                },
                {
                    "sent": "So if we want to do things this way, we have to be aware of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "'cause again can call workers, but a worker can't call gangs right?",
                    "label": 0
                },
                {
                    "sent": "We've got gangs of workers.",
                    "label": 0
                },
                {
                    "sent": "With vectors so we can do things, but we have to be aware of the high.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cherokee now, on the other hand, it's now possible to have kernels actually launch entirely new kernels within the code, so we don't have to worry about that hierarchy anymore, so we can have again a routine that we're saying this is this is a open ACC routine, so when it gets called later on, this is supposed to run on the GPU.",
                    "label": 1
                },
                {
                    "sent": "And then here's a routine that is running will assume here on the GPU in a parallel region, and this loop itself then calls a subroutine within it.",
                    "label": 0
                },
                {
                    "sent": "It's launching threads itself.",
                    "label": 1
                },
                {
                    "sent": "Now it's not really a good idea to have thousands of threads launching thousands of new kernels.",
                    "label": 0
                },
                {
                    "sent": "That's not the intent here.",
                    "label": 0
                },
                {
                    "sent": "The intent is really to allow you to move all of the code onto the GPU.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's it's Nvidia's objective, it's it's the objective I guess of any program that really wants to get minimize any possibility of data management moving back and forth to put everything on the GPU so it will allow you to put everything on the GPU.",
                    "label": 0
                },
                {
                    "sent": "Open.",
                    "label": 0
                },
                {
                    "sent": "ACC now allows you to have routines actually calling other routines on the GPU.",
                    "label": 0
                },
                {
                    "sent": "You can keep everything.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a more useful case about where we had originally something that quite you know, quite honestly, could be running on the front end, right here on the CPU that calls some GPU routines called solver.",
                    "label": 1
                },
                {
                    "sent": "So solvers doing all the heavy lifting.",
                    "label": 0
                },
                {
                    "sent": "Here, it's really the GPU routine, but for the sake of making sure everything's on the GPU, we can put the routine that calls solver itself on this GPU, so this makes everything run on the GPU, but on the GPU itself were not launching thousands and thousands of new new routines.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're calling a small.",
                    "label": 0
                },
                {
                    "sent": "Of this solver and then solver is where all the thousands of cores are being launched and recruited.",
                    "label": 0
                },
                {
                    "sent": "But again, the idea is open.",
                    "label": 1
                },
                {
                    "sent": "ACC allows us to make sure we migrate all of the code to the GPU.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a minimal amount running on the CPU.",
                    "label": 0
                },
                {
                    "sent": "Now if you combine all these things together and you start doing things your again targeting the architecture, you're really targeting a specific architecture and optimizing perspective architecture.",
                    "label": 0
                },
                {
                    "sent": "But you know there's there.",
                    "label": 0
                },
                {
                    "sent": "You're trying to hide that as much as possible for code maintenance and everything else.",
                    "label": 0
                },
                {
                    "sent": "So open ACS and very nice ways has a very nice way, in particular of saying do these parameters for this type of device, so you can say here if my device type is NVIDIA that I want 200 games.",
                    "label": 0
                },
                {
                    "sent": "My advice type is radio on that I want 800 tanks, so you can do these optimizations and leave him in your code.",
                    "label": 1
                },
                {
                    "sent": "Basically both open MP and open ACC try to eliminate the need for you have lots of ifdef senior code so I'm sure lots of you have seen lots of ifdef contaminating code out there, but modern specifications don't.",
                    "label": 0
                },
                {
                    "sent": "Don't force you to do that, so if there is an open MP programmer you have lots of options like this as well.",
                    "label": 1
                },
                {
                    "sent": "With open ACC you should not find yourself sticking if deaths in your code to support different devices or if open ACC is active is used in this part of the code, but you also have the serial code there you shouldn't need ifdef to separate those cases.",
                    "label": 0
                },
                {
                    "sent": "Instead, the opening she directives have the ability to turn on and off pieces of code in different sections, or in this case here use different parameters.",
                    "label": 0
                },
                {
                    "sent": "So don't litter your code in the modern era, you shouldn't find yourself writing lots of effects in your code, not for opening pier.",
                    "label": 0
                },
                {
                    "sent": "Open ACC or not for too many.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other cases, either.",
                    "label": 0
                },
                {
                    "sent": "You might ask yourself, you probably already asking yourself, what about if I have multiple devices plugged in?",
                    "label": 1
                },
                {
                    "sent": "Because these days plugging in multiple GPU's to a single mode is a common thing on bridges.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of GPU's at least on every node.",
                    "label": 0
                },
                {
                    "sent": "We actually have four on the nodes you guys have been using.",
                    "label": 1
                },
                {
                    "sent": "Yes, you have to cause of the confusion early on.",
                    "label": 1
                },
                {
                    "sent": "Yesterday is we had the wrong default parameter set up, so those of you were asking for a GPU.",
                    "label": 0
                },
                {
                    "sent": "We're actually getting 4 GPS and that's that's why we ran out of GPU so fast.",
                    "label": 0
                },
                {
                    "sent": "But but at any rate that's a common and useful configuration of multiple GPU's in a single node.",
                    "label": 0
                },
                {
                    "sent": "At the same time as we've been saying this whole time, any modern CPU has multiple cores in it.",
                    "label": 0
                },
                {
                    "sent": "So saying, how can I use all the cores along with multiple GPU's is a natural question to ask, and it's built into the spec, so it's not a big deal.",
                    "label": 1
                },
                {
                    "sent": "Multiple threads in one device is fine, you don't really have to do anything, so if you've got a bunch of threads and you've got some nice open MP code there with all the threads using the GPU, that's fine.",
                    "label": 1
                },
                {
                    "sent": "One thing it's a little bit less convenient or satisfactory is if you've got multiple threads in multiple devices, then you've got to break the data up to send to each device.",
                    "label": 0
                },
                {
                    "sent": "I would prefer if this were a little more transparent, but the compilers haven't quite reached that point yet, so if you're doing that, you've got a slice your data up so that each thread is sending the data that corresponds to the right device.",
                    "label": 0
                },
                {
                    "sent": "It's very straightforward to do with arrays shaping, so you know yesterday we discussed this kind of thing right here, but we've got slices of the data, so slicing the data up and sending it the GPS is straightforward.",
                    "label": 0
                },
                {
                    "sent": "I just resent having to do bookkeeping, you know, I think this is compiler's job is to do bookkeeping where possible, so but in this case here, if you've got.",
                    "label": 0
                },
                {
                    "sent": "A lot of GPU's plugged in and you want to do multi core programming at the same time it's up.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You to coordinate it.",
                    "label": 0
                },
                {
                    "sent": "And again, it's a straightforward task.",
                    "label": 0
                },
                {
                    "sent": "It just I would prefer the compiler to do it for me to hide all of that.",
                    "label": 0
                },
                {
                    "sent": "I want things again that I've mentioned continually is the motivation for optimizing using advanced features to eliminate barriers in your code to have all the cores racing ahead.",
                    "label": 0
                },
                {
                    "sent": "Well, when you do that, you get into problems like our race condition that we saw a second ago.",
                    "label": 0
                },
                {
                    "sent": "So then you need to put in barriers, so putting in and removing barriers is pretty much the main shore when you're doing this kind of threaded optimization, and so there's there are routines.",
                    "label": 0
                },
                {
                    "sent": "The obvious routines are built into open AC delije deck.",
                    "label": 0
                },
                {
                    "sent": "You have wait commands, you have a synchronous command to say don't wait in places where it would naturally wait.",
                    "label": 0
                },
                {
                    "sent": "You have atomic clauses to say do this section atomically and as one piece.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All those things are built in an in an obvious way.",
                    "label": 0
                },
                {
                    "sent": "Data management.",
                    "label": 0
                },
                {
                    "sent": "Yesterday I mentioned to us we for our data management.",
                    "label": 0
                },
                {
                    "sent": "So far we've just been using data regions and then I threw an update there because it was necessary and you might.",
                    "label": 0
                },
                {
                    "sent": "You might have and I know many of you did when we were doing the first exercise, worrying about the scope of the data region.",
                    "label": 0
                },
                {
                    "sent": "And what about if I want to put the initialize routine?",
                    "label": 0
                },
                {
                    "sent": "Several of you said I'd like to do the initialize routine that just sets the plate equal to 0.",
                    "label": 0
                },
                {
                    "sent": "I'd like to do that on the GPU as well.",
                    "label": 0
                },
                {
                    "sent": "Our final solution we have up there.",
                    "label": 0
                },
                {
                    "sent": "I ignored initialized and my philosophy there in general was.",
                    "label": 0
                },
                {
                    "sent": "Allies routines only done once.",
                    "label": 0
                },
                {
                    "sent": "We don't need to worry about that.",
                    "label": 0
                },
                {
                    "sent": "We we care about something that's done thousands of times, but on the other hand, those of you who are real perfectionist wanted to do the initial eyes on the GPU too.",
                    "label": 0
                },
                {
                    "sent": "And it was a little awkward with our data region command to figure out how to make that initialize.",
                    "label": 0
                },
                {
                    "sent": "How work on the GPU.",
                    "label": 0
                },
                {
                    "sent": "At any rate, it's not at all awkward if we have a few additional commands which open ACC has to allow you to move data onto and off the GPU at will, not just in a region.",
                    "label": 1
                },
                {
                    "sent": "So regions out very often a natural way to move data.",
                    "label": 0
                },
                {
                    "sent": "You say.",
                    "label": 0
                },
                {
                    "sent": "Here's the region where I'm doing some.",
                    "label": 0
                },
                {
                    "sent": "I'm working on stuff on the GPU.",
                    "label": 0
                },
                {
                    "sent": "This is where I'd like to move it onto and off the GPU.",
                    "label": 0
                },
                {
                    "sent": "So region is probably the most useful one, but at any point in time you are free to just update things we've seen update already, but you can also have anywhere you want put and enter data and exit data region, so these commands allow you to just say to do instead of having to deal with the scope of big block.",
                    "label": 1
                },
                {
                    "sent": "Instead, you can just say right now move all this stuff onto the GPU and then later on you exit data region, move all the stuff back off the GPU and that's very nice for something like initialization.",
                    "label": 0
                },
                {
                    "sent": "If you've got a code where you've got initialization routine, it's living out there in its own place.",
                    "label": 0
                },
                {
                    "sent": "Where you initialize stuff might be a good place to move everything on the GPU and then where you clean stuff up in your code might be a good time to pull stuff or where you need your results in the code might be a good time to pull everything back so you can just put those anywhere you want your code you want to fight with braces and figuring out what the scope should be.",
                    "label": 0
                },
                {
                    "sent": "We also have some pointer commands it will look at when we talk about using CUDA and libraries with open ACC, the ability to just say hey some data is already on the GPU and here's a pointer to it is a nice thing to be able to do, so I'll save the.",
                    "label": 1
                },
                {
                    "sent": "Detailed description of those till later, but you can absolutely do that so you don't have to worry about ever pulling the data back and forth at a time that you think is inappropriate.",
                    "label": 0
                },
                {
                    "sent": "You can sit back and say this is what I want today to be on the GPU.",
                    "label": 0
                },
                {
                    "sent": "This is when it needs to be back and these commands make it natural.",
                    "label": 0
                },
                {
                    "sent": "You're not always fighting with a scope or a region, so we've seen.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That profiling I'll mention quickly here.",
                    "label": 0
                },
                {
                    "sent": "I'm always reluctant to talk about anything.",
                    "label": 0
                },
                {
                    "sent": "It's platform dependent.",
                    "label": 0
                },
                {
                    "sent": "You know I want to.",
                    "label": 0
                },
                {
                    "sent": "So far everything we talked about an open standard open ACC is a standard and you can run on your laptop so you can run it on super computers.",
                    "label": 0
                },
                {
                    "sent": "You can run it with compilers.",
                    "label": 0
                },
                {
                    "sent": "You can get the PGI compiler for free with a student license and run it.",
                    "label": 0
                },
                {
                    "sent": "You can go try the GCC compiler.",
                    "label": 0
                },
                {
                    "sent": "You can do whatever you want is an open standard so I don't like to go into talks.",
                    "label": 0
                },
                {
                    "sent": "Talk a whole lot about a particular tool or another, especially one that cost money.",
                    "label": 0
                },
                {
                    "sent": "But in this case this ones free.",
                    "label": 0
                },
                {
                    "sent": "So even though it's NVIDIA tool because it's free and you can just get it with the NVIDIA Toolkit.",
                    "label": 0
                },
                {
                    "sent": "I'd be remiss if I didn't at least tell you that there's a great profiler.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out there and again profiling your code is important.",
                    "label": 0
                },
                {
                    "sent": "I think people don't do it enough.",
                    "label": 0
                },
                {
                    "sent": "It's always strikes me as silly that somebody will spend three or four months writing a code and then they won't spend half a day profiling and finding some way to squeeze a lot more performance out of it.",
                    "label": 0
                },
                {
                    "sent": "Especially when these tools are so friendly so they all have GUI's these days and they will give you a pretty nice presentation of data.",
                    "label": 0
                },
                {
                    "sent": "Here, for example, is a code that draws Mandelbrot graphs, and you look at it and you say, well, if you stare at this close enough here, you can see there's a region here where it does the communication.",
                    "label": 0
                },
                {
                    "sent": "There well down there I really should get the stylus out.",
                    "label": 0
                },
                {
                    "sent": "My fingers are too fat for this and then there's a region where it does the computation and you can see very well in this code that spends time spent sending data and it sends time computing and we might.",
                    "label": 0
                },
                {
                    "sent": "It might be nice and a general nice thing to do in parallel computing is anytime you can overlap computation and communication you should do it.",
                    "label": 0
                },
                {
                    "sent": "It's a big wit that applies to open it to MPI codes.",
                    "label": 0
                },
                {
                    "sent": "It applies to open MP anytime you can overlap things.",
                    "label": 1
                },
                {
                    "sent": "It's great, and in this case here, if you know what the Mandelbrot codes actually going, you might say, well, I'm drawing.",
                    "label": 0
                },
                {
                    "sent": "Part of the mental block graph.",
                    "label": 0
                },
                {
                    "sent": "I can import some more data and a way to do that is just a break thing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up into pieces and so here's where I notice the problem.",
                    "label": 0
                },
                {
                    "sent": "I see all these PCI transfers are taking all this time, and I'm sitting there waiting on them.",
                    "label": 0
                },
                {
                    "sent": "What if I break?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thing up into a little smaller pieces then, well, it's working on one piece.",
                    "label": 0
                },
                {
                    "sent": "It can transfer data and they can overlap.",
                    "label": 0
                },
                {
                    "sent": "So here's the thing blocked out and there's some very, very obvious ways and simple ways to block this particular algorithm.",
                    "label": 0
                },
                {
                    "sent": "Other algorithms, communication and computation.",
                    "label": 0
                },
                {
                    "sent": "It's hard to get him to overlap because they are really just separately separate things, but in this case here it was easy to break things up into.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cases and so.",
                    "label": 0
                },
                {
                    "sent": "Here's kind of the optimization phase of this.",
                    "label": 0
                },
                {
                    "sent": "You start out how fast this code runs on 16 cores it runs at.",
                    "label": 1
                },
                {
                    "sent": "You know one is our baseline here.",
                    "label": 0
                },
                {
                    "sent": "You paralyze it, which is the basic open ACC and you can get it up to about four times faster in this particular configuration here, turn it into blocks like I said, to allow it to overlap and we get up to 7 times faster, so that's a nice performance improvement.",
                    "label": 0
                },
                {
                    "sent": "And then we can use some of synchronous commands.",
                    "label": 0
                },
                {
                    "sent": "So I told.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this with the performance profiler here.",
                    "label": 0
                },
                {
                    "sent": "You can go, oh, there's still some weights going on here, and I know that things don't have to wait because it doesn't really need that data.",
                    "label": 0
                },
                {
                    "sent": "So you can stick in somebody's synchronous command and say don't wait here and poop.",
                    "label": 0
                },
                {
                    "sent": "You can get it all the way up to hear that about 10 times faster.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of optimization you can do starting with generic obvious open ACC things and with a profiler and trying some things you can you can get.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your performance now.",
                    "label": 0
                },
                {
                    "sent": "What didn't we cover here?",
                    "label": 0
                },
                {
                    "sent": "These are some of the miscellaneous things in the spec.",
                    "label": 0
                },
                {
                    "sent": "We didn't talk about.",
                    "label": 0
                },
                {
                    "sent": "Well, am I into our lunch break?",
                    "label": 0
                },
                {
                    "sent": "So by the lunch break.",
                    "label": 0
                },
                {
                    "sent": "OK well OK, sorry I didn't mean to run over 'cause we have plenty of time.",
                    "label": 0
                },
                {
                    "sent": "I didn't need to.",
                    "label": 0
                },
                {
                    "sent": "I could have stopped at any point, but we're on the last slide, so let me just say what I didn't cover here is a bunch of environment variables.",
                    "label": 0
                },
                {
                    "sent": "Set stuff up.",
                    "label": 0
                },
                {
                    "sent": "Make life easy.",
                    "label": 0
                },
                {
                    "sent": "As I said, there are a bunch of things that make.",
                    "label": 0
                },
                {
                    "sent": "If that's not important, so never find yourself writing effects to make this stuff work.",
                    "label": 0
                },
                {
                    "sent": "Their API version instead of having directives.",
                    "label": 0
                },
                {
                    "sent": "If you're willing to commit your code to being a GPU code instead of having these directive based things that look again like comments, you can stick in your code API versions, functions, subroutines that you call.",
                    "label": 0
                },
                {
                    "sent": "That emulate all this behavior.",
                    "label": 0
                },
                {
                    "sent": "You might find it more convenient and more natural way to code, but then we're jumping off of the keep keep serial code in parallel code in the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to have a GPU only code and many times it's perfectly acceptable.",
                    "label": 0
                },
                {
                    "sent": "Lastly, hybrid programming, which I will talk about in great detail, so I don't need to right now.",
                    "label": 0
                },
                {
                    "sent": "So OK. With that in mind, let's take our lunch break.",
                    "label": 0
                },
                {
                    "sent": "I'll be glad to stand here and answer questions, question or two so it will not last long, but will also be returning to this right after lunch so.",
                    "label": 0
                },
                {
                    "sent": "Don't know, don't feel too compelled to get your question right now, so can you, you, said Jesus, you can use can use GCC with open ACC or so.",
                    "label": 0
                },
                {
                    "sent": "Just the PGI compiler, GCC.",
                    "label": 0
                },
                {
                    "sent": "There are a number of compilers increasing number of compilers these days.",
                    "label": 0
                },
                {
                    "sent": "If you go to open acc.org it will list all the compilers at work.",
                    "label": 0
                },
                {
                    "sent": "GCC with five point X.",
                    "label": 0
                },
                {
                    "sent": "They started working stuff in but with looks like with six point exit got more serious and the topic of where things are implemented well is a very important one.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk about that in great detail when we get into the open MP4.",
                    "label": 0
                },
                {
                    "sent": "Versus open ACC later, so it's a good topic, but we'll get into it later.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "We will return after lunch now.",
                    "label": 0
                }
            ]
        }
    }
}