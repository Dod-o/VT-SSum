{
    "id": "l7u3a5sfvr3tjegery3fol4iikwgh72s",
    "title": "On Feature Selection, Bias-Variance, and Bagging",
    "info": {
        "author": [
            "M. Arthur Munson, Cornell University"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_munson_fsbvb/",
    "segmentation": [
        [
            "OK, I think everything fits on the screen, so I'm going to go ahead and start.",
            "And you're timing me, is that right?",
            "OK, so fantastic.",
            "So my name is Art Munson.",
            "I'm going to talk about feature selection, bias variance, and bagging.",
            "This is joint work with my advisor rich car.",
            "Wanna from Cornell?",
            "Raise your hand if you can hear me in the back?",
            "'cause I used to be a lifeguard and I can get louder."
        ],
        [
            "Alright, like a lot of research, this particular work starts with the story.",
            "We were collaborating with the lab of Ornithology at Cornell and we our task was to try and model the presence or absence of birds.",
            "More specifically, we're going to have a description of where you might go to see Birds description of the habitat, the population density in the area made with the typical climate or weather patterns, how long the person is going to look, how many cats the neighbors has.",
            "You know the list goes on and on and on right?",
            "So we have 200 features.",
            "And we want to predict the likelihood of seeing, for example, in this case a house Finch or not."
        ],
        [
            "We didn't really know where to start, so we tried all the standard machine learning methods, anything that we could basically take off the shelf, easily try and that we thought would work well and it turned out that the best model was a bad decision tree."
        ],
        [
            "Of course, this was really only beginning.",
            "Uh, the ecologists we were working with.",
            "They didn't really care about our predictions.",
            "I mean, they were important to ensure that we had a good model, but they what they really cared about was what had the model learned?",
            "Or maybe how was it making its prediction so accurately?",
            "So their goal was to understand avian population dynamics.",
            "And of course, if you're familiar with bagged decision trees, you know that this is not something you can just print out.",
            "Look at and understand, and in fact, even a single tree say we had used a single to shoot decision tree.",
            "There are so many features in this domain that the trees have something like 20,000 nodes, right?",
            "So at that point a single decision tree is no longer understandable.",
            "So our approach was, well, maybe we can do some black box analysis to understand what these models are doing.",
            "But before we do that we have to drastically cut down the number of inputs to the model.",
            "So obviously we tried to run feature selection and we were hoping to find a small compact set of features that kept the high performance that led us to use bag trees in the first place."
        ],
        [
            "Alright, here's a quick graph.",
            "This is the feature selection graph for this task.",
            "The X axis is the number of features.",
            "I don't know how legible the fonts are way back at the end of the room.",
            "The Y axis is root mean squared error because this is an error term.",
            "Down is good on this graph.",
            "And what we can see is that when we use all the features which is this horizontal dashed line, we're getting noticeably better performance than if we use as many as 30 features.",
            "If you've done feature selection, you know that often what you see is when you remove features, your performance gets better, but here it seems to be that.",
            "It's just hard to match this performance when we use all the features."
        ],
        [
            "This did not escape the notice of the reviewer, so when we went to sent this workout for review, it wasn't really on feature selection, was on something different.",
            "Nonetheless, a couple of reviewers commented on the fact that.",
            "Feature selection or using a smaller set of inputs was somehow hurting the performance of these models.",
            "And I'm a little short on time, so I'm going to skip reading these comments.",
            "If you're a fast reader, maybe you have a chance to read them before I change the slide."
        ],
        [
            "That brings me to this particular study.",
            "We are interested in the question of whether this is.",
            "A rare occurrence or whether this is something that happens frequently for bagging.",
            "So said another way, is this something that's particular to this kind of domain or this problem that we're trying to solve?",
            "Or is it the case that in bagging you are somehow immune to overfitting, extra irrelevant features?",
            "And if so, why?"
        ],
        [
            "The rest of the talk is going to be structured in the following way.",
            "I'm going to give some background and an overview of all the concepts in the talk, just to make sure that everyone's on the same page.",
            "And after that I'll talk about an experiment we ran where we're going to do feature selection with and without bagging, and we're going to look at the something called the bias variance decomposition of the air.",
            "If you don't know what that is, don't worry.",
            "I'm going to explain it in a minute.",
            "Based on the results from that experiment, we saw something interesting that caused us to run a second experiment, and I think for the sake of simplicity I'll postpone talking about that experiment until the end of the talk."
        ],
        [
            "Alright, quick show of hands who is familiar with bagging?",
            "OK, so it's about a little bit more than half of the room.",
            "You're with bagging what you're trying to do is build an ensemble or a committee of classifiers and you do it in the following way.",
            "You're going to take your training data and draw a random sample from it, and then train your model.",
            "It doesn't really matter what kind of model.",
            "You could use an SVM or neural network or decision tree.",
            "Whatever happens to be your favorite black box classification algorithm.",
            "And then you're going to repeat this some 25 times.",
            "And then you're going to average the predictions from your ensemble or committee of classifiers.",
            "Now on the surface of things, this might seem crazy.",
            "If you haven't seen this before, you're throwing away some of your training data so your individual models, it seems likely are going to be at least a little bit worse than if you had kept all the training data.",
            "However, when you combine them.",
            "The result is."
        ],
        [
            "Perhaps seemingly magically, you get a model that is surprisingly competitive and rarely overfits, so there's very risk of doing worse if you do bagging.",
            "And in many cases, if you have a noisy domain or a noisy task, this will be as competitive as a well trained neural network or support vector machine, or sometimes even boosting.",
            "The main benefit it's been shown is to reduce the variance of the classifier so this comes back to the fact that I was talking about this is something that works well on a noisy task.",
            "And finally, alien Pozzani and the early 90s showed that when you do bagging, that improves the base learners ability to ignore irrelevant features.",
            "So this is an interesting fact that will come back to later in the talk."
        ],
        [
            "Alright, a quick review of bias variance decomposition if you have a learning algorithm, your error can come from three sources.",
            "First, there might be some inherent uncertainty or stochastic city.",
            "In the domain, so that would mean that your Bayes optimal error rate would be non 0 right?",
            "So even if you had perfect knowledge, there's just some uncertainty that prevents you from doing perfectly.",
            "Second, you have a bias which comes from the inductive bias that's inherent in any particular learning algorithm.",
            "And you can think of this as how close on our average across all different possible training sets is the algorithms prediction from the Bayes optimal prediction.",
            "And finally you have variance which is.",
            "If you change the training set or perturb it, how much does that change the prediction of the algorithm, and in the case of squared error, which is what I'll be talking about in this talk.",
            "Air decomposes into noise plus bias plus variance in each term is non negative, so this is a pretty nice.",
            "Simple way to look at.",
            "Where your air is coming from.",
            "We're going to be estimating these terms on real data, and it turns out that we can't separate the bias from the noise because we don't know what the Bayes optimal prediction would be."
        ],
        [
            "Um?",
            "I I think it's interesting to look at how you actually measure this on a real data set.",
            "This is the method that's used by Baron Kohavi from a paper they did at the end of the 90s.",
            "And what you want to do is generate the empirical distribution of the algorithm algorithms predictions.",
            "Typically for a single example, I mean you're going to look across all of your test data, but you're going to look at a distribution for each example separately.",
            "So you're going to randomly sample your training data, fit a model to that sample, and then make predictions for each of your test data using that model, and then you repeat this maybe 20 times, for example.",
            "And it will be useful to think about also storing an average prediction.",
            "So imagine that you have your example X.",
            "You have 20 predictions.",
            "You know Y1Y2 all the way through Y 20.",
            "It's useful to think about also storing the average of those predictions, which I'll call Y sub M here."
        ],
        [
            "I.",
            "And I won't go through the derivation here, but it turns out that the bias for the example X is just going to be the squared error between the true label and the average prediction.",
            "And the variance of X is going to be the variance of all those Y one through Y 20 predictions.",
            "And then you do the the thing you would expect when you look at the whole problem or the whole test set.",
            "You just take an average over all your examples.",
            "Alright."
        ],
        [
            "I'm going to go through this really quickly.",
            "This is the feature selection session, so I think most people have probably seen this stuff.",
            "There are lots of ways to do feature selection.",
            "We're using two ways that have been in the literature for quite a long time.",
            "The first is forward stepwise feature selection.",
            "It's often called the wrapper approach and the ideas are going to greedy Hill climbing search each time adding the next feature that gives you the best benefit, and you're going to measure the best benefit by fitting a model.",
            "It turns out that."
        ],
        [
            "This greedy forward stepwise selection is too expensive if you have lots of features.",
            "If you're doing bioinformatics or natural language processing, you can't really afford to do something that's quadratic in the number of features.",
            "So instead what people will sometimes do is rank the features by their individual individual correlation with the class label, and then choose a cutoff point.",
            "It turns out that for this work we don't need to choose a cutoff point because we're going to be looking at Eric.",
            "Various feature set sizes."
        ],
        [
            "How my on time?",
            "OK. Alright, so here's the first experiment.",
            "We're going to be measuring doing feature selection both with a single decision tree and a bag decision tree, and we're going to look at 19 different datasets, and these datasets were chosen to have a wide range of sample size and number of features so they range from problems that have a few 100 samples all the way up to 90,000 samples an from a handful of features up to 400,000 features.",
            "We're going to order the features using feature selection and the kind of feature selection we use is dictated solely by the size of the problem.",
            "So if we could afford to do forward stepwise selection, that's what we did.",
            "And if we could not, we use correlation filtering.",
            "And then once we have this feature set order, we're going to estimate the bias and variance at multiple feature set sizes.",
            "So we'll estimate the bias and variance of models when you use one feature 5 features, 10 features, 15 features, etc.",
            "And then to get make sure that we have reliable estimates, we're going to do all of this five times inside 5 fold cross validation."
        ],
        [
            "All right, we're going to be looking at a couple of graphs like this, so I want to walk you through it to make sure that we all know how to read them on the X axis.",
            "We have the number of features.",
            "And on the Y axis again we have an error term.",
            "So this is mean squared error.",
            "And the height of each bar is the air of the corresponding models and you can see that there are pairs of bars at each feature set size.",
            "So the.",
            "The Left bar in each pair corresponds to a single decision tree, and the right bar to a bag decision tree.",
            "And based on you can see from the colors, which is sort of a yellowish green.",
            "There's the variance part of the air and then the blue part is the bias slash noise part of the air.",
            "Alright, So what can we see from this graph?",
            "Um?",
            "As you increase the number of features, it's pretty clear pattern that your bias goes down dramatically.",
            "We can understand that as simply being as you give more features to an algorithm, it has more information, so we would expect that the noise and uncertainty would go down.",
            "At the same time, the bias is going to go down because you are giving the algorithm more degrees of freedom.",
            "It has more ways that it can fit the data.",
            "On the Conversely, as you increase your features, your variance goes up and this is the same reason that your bias is going down right?",
            "You're giving the algorithm more degrees of freedom in it is better able to overfit your data.",
            "Alright, so jumping out to a little bit higher level.",
            "We had several datasets that fell into this category and I would categorize these datasets as.",
            "You didn't need to do feature selection to get the best performance.",
            "It didn't matter if you did bagging or not, you just did not need to do feature selection to get the best performance.",
            "On the 2nd."
        ],
        [
            "The category of problems.",
            "However, what we saw was that.",
            "After a certain point on this particular example, starting with 10 features, the single decision tree starts to overfit.",
            "Anne.",
            "And eventually.",
            "Starts to have really bad air.",
            "While bagging continues to go down and it's clear from this picture why this happens, why there is this difference?",
            "The variance component for bagging quickly asymptotes to a small value.",
            "And so it can continue to benefit from adding more features and decreasing the bias, whereas you know if you look at the single decision trees, its bias component is still going down all the way at the end.",
            "It's just that the variance is increasingly bad."
        ],
        [
            "Alright, so the key takeaway points here.",
            "There's a tradeoff between bias and variance.",
            "The feature selection doesn't seem to really improve the performance of bad decision trees, and here I'm purely talking about predictive performance.",
            "I'm not talking about.",
            "How long it takes to make predictions, etc.",
            "So there are lots of other reasons you might want to do feature selection, but if you only care about performance, feature selection with bagging seems to be redundant.",
            "And finally, it suggests that there's a new way to look at feature selection where.",
            "It's maybe not a way to separate relevant from irrelevant features, but instead you are.",
            "You can think of feature selection as a way to regularize a model.",
            "Alright."
        ],
        [
            "Going to have to speed up here a little.",
            "Bow is given the five minute warning 2 minutes ago.",
            "I want to point out this graph.",
            "I think this graph is really poor."
        ],
        [
            "Amazing, so this this would fall in category one here where you don't need feature selection to get the best performance with or without bagging.",
            "However it turns out you can look at this and say singletree its performance plateaus or levels off around 10 features.",
            "And I don't know how legible these numbers at the bottom are here, but if you look at the bagging, when you go from 800 features to."
        ],
        [
            "1300 features it's still improving.",
            "Right now I said at the beginning of talk that it's known in the literature that bagging helps you ignore purely garbage irrelevant features, right?",
            "But if that was what was happening here, we would expect bagging to level off somewhere around here, right?",
            "Yet it keeps going down, so it's doing something else in addition to ignoring irrelevant features."
        ],
        [
            "So our hypothesis was that bagging is somehow improving the base learners ability to extract useful information from noisy features.",
            "And that's exactly what our second experiment was designed to test."
        ],
        [
            "In the interest of time, I'm going to really go through this fast.",
            "We generate a synthetic data set so we had complete control over the experiment and we're going to take some of those features and make copy week copies of them that were going to corrupt an inject.",
            "A lot of noise into.",
            "And then we're going to train a model using.",
            "Half of the good features, and then the noisy copies of the other half.",
            "OK, so I think this is."
        ],
        [
            "Perhaps easiest to look at while I talk about the graph.",
            "So on the X axis, now we have the fraction of noise that's been added to those copied features.",
            "We can look at how well."
        ],
        [
            "So the algorithm could possibly do by saying, hey, let's go into a situation that will never have but just look at it.",
            "We're going to give you the exact features used to define the classification label, and they have zero noise.",
            "OK, and we can see that this problem is something that's hard for decision trees, because the error here is non 0.",
            "The other thing."
        ],
        [
            "We can compare about against IS.",
            "Imagine that we know which features are noisy.",
            "And we throw those away.",
            "So we're now only going to use three inputs to the model and measure how well we would do."
        ],
        [
            "And then everything else on this graph is what happens if you take the three non corrupted features and then add the 60 weak, noisy, corrupted, damaged features.",
            "And what we can see is that.",
            "At low levels of noise, both algorithms are able to get some benefit right?",
            "These bars are both lower than if you had thrown away all your noisy features.",
            "So there's some information being extracted from them.",
            "And in fact, the bagging bar is really, I think, remarkably close to the ideal.",
            "But as you continue to add noise, we can see that the noise part of the air increases and keeps climbing.",
            "And quickly, because the variance for the single tree is so large, pretty much as a constant across this whole experiment here.",
            "You end up doing worse than if you had thrown away your noisy features.",
            "Bagging on your head, I mean just to make a quick point here at 80% noise to 80% of the feature values are.",
            "Are just noise, pure garbage?",
            "Yeah, it's still able to do a little bit better than the baseline here, so it's still extracting some useful information."
        ],
        [
            "Alright conclusions.",
            "So after training some 9 million decision trees in a fit of craziness, I suppose what we've learned is that there's this tradeoff between bias and variance that's related to the amount of features you give to a model.",
            "And I guess I won't reiterate these.",
            "And then in our second experiment we saw that bag trees are surprisingly good at extracting useful information from noisy features an we think this is happening because different trees are using different weak features, but.",
            "I'm not really sure how to show this, so if anyone has any ideas I'd love to hear them.",
            "And with that, I'd be happy to take any questions."
        ],
        [
            "Yes.",
            "To come into question, the company is I. I'm not sure I quite agree with your reviewer in your initial point that you're expecting that feature selection will improve performance.",
            "So yeah, I think we have lots of domain and experimental evidence were feature extra session.",
            "Actually you are trying not to degrade too much of performance and basically the motivation is the one you showed in at the beginning of your true.",
            "Basically you want to tell something to the field expert you apply for your birthday wishes and that's.",
            "So basically you want to select feature, not use too much and then we are happy when we have.",
            "So I'm kind of surprised from from the beginning of the talk, especially that since you claim that it's good, like attendance.",
            "So I did agree with that.",
            "Now comes the question is you've been using.",
            "Back trees with with a simple feature selection or sometimes before.",
            "What selection and the next step which is very well known is to use random Perez and interpret your trees or doing embedded selection and that has been shown to be better and so that would completely change the picture.",
            "So have you tried that?",
            "I have not tried that.",
            "I think it would be very interesting to try an.",
            "I don't disagree with your comment.",
            "I think it's a matter of degrees, but I don't disagree with your comment.",
            "More questions, comments.",
            "Well, I have a little comment.",
            "Didn't take into account the number of instances because in many domains you have very small number of instances and by increasing the number of features so much you would even with bagging command applies to eventually get into troubles.",
            "So feature selection might happen that I I think that's true.",
            "That's something I want to follow up on is to do something like a learning curve where you can vary among the amount of training data.",
            "I was surprised actually in the domains that have hundreds of thousands of features that.",
            "We didn't see more overfitting, and possibly that's because we tuned the single decision tree to not overfit too badly, so we chose sort of a.",
            "You can think of it as a depth limit on training the trees to do well on those tasks.",
            "But I think it's an interesting Ave to follow up on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think everything fits on the screen, so I'm going to go ahead and start.",
                    "label": 0
                },
                {
                    "sent": "And you're timing me, is that right?",
                    "label": 0
                },
                {
                    "sent": "OK, so fantastic.",
                    "label": 0
                },
                {
                    "sent": "So my name is Art Munson.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about feature selection, bias variance, and bagging.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with my advisor rich car.",
                    "label": 0
                },
                {
                    "sent": "Wanna from Cornell?",
                    "label": 0
                },
                {
                    "sent": "Raise your hand if you can hear me in the back?",
                    "label": 0
                },
                {
                    "sent": "'cause I used to be a lifeguard and I can get louder.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, like a lot of research, this particular work starts with the story.",
                    "label": 0
                },
                {
                    "sent": "We were collaborating with the lab of Ornithology at Cornell and we our task was to try and model the presence or absence of birds.",
                    "label": 0
                },
                {
                    "sent": "More specifically, we're going to have a description of where you might go to see Birds description of the habitat, the population density in the area made with the typical climate or weather patterns, how long the person is going to look, how many cats the neighbors has.",
                    "label": 0
                },
                {
                    "sent": "You know the list goes on and on and on right?",
                    "label": 0
                },
                {
                    "sent": "So we have 200 features.",
                    "label": 0
                },
                {
                    "sent": "And we want to predict the likelihood of seeing, for example, in this case a house Finch or not.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We didn't really know where to start, so we tried all the standard machine learning methods, anything that we could basically take off the shelf, easily try and that we thought would work well and it turned out that the best model was a bad decision tree.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, this was really only beginning.",
                    "label": 0
                },
                {
                    "sent": "Uh, the ecologists we were working with.",
                    "label": 0
                },
                {
                    "sent": "They didn't really care about our predictions.",
                    "label": 0
                },
                {
                    "sent": "I mean, they were important to ensure that we had a good model, but they what they really cared about was what had the model learned?",
                    "label": 0
                },
                {
                    "sent": "Or maybe how was it making its prediction so accurately?",
                    "label": 0
                },
                {
                    "sent": "So their goal was to understand avian population dynamics.",
                    "label": 1
                },
                {
                    "sent": "And of course, if you're familiar with bagged decision trees, you know that this is not something you can just print out.",
                    "label": 0
                },
                {
                    "sent": "Look at and understand, and in fact, even a single tree say we had used a single to shoot decision tree.",
                    "label": 0
                },
                {
                    "sent": "There are so many features in this domain that the trees have something like 20,000 nodes, right?",
                    "label": 0
                },
                {
                    "sent": "So at that point a single decision tree is no longer understandable.",
                    "label": 0
                },
                {
                    "sent": "So our approach was, well, maybe we can do some black box analysis to understand what these models are doing.",
                    "label": 0
                },
                {
                    "sent": "But before we do that we have to drastically cut down the number of inputs to the model.",
                    "label": 0
                },
                {
                    "sent": "So obviously we tried to run feature selection and we were hoping to find a small compact set of features that kept the high performance that led us to use bag trees in the first place.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, here's a quick graph.",
                    "label": 0
                },
                {
                    "sent": "This is the feature selection graph for this task.",
                    "label": 0
                },
                {
                    "sent": "The X axis is the number of features.",
                    "label": 0
                },
                {
                    "sent": "I don't know how legible the fonts are way back at the end of the room.",
                    "label": 0
                },
                {
                    "sent": "The Y axis is root mean squared error because this is an error term.",
                    "label": 0
                },
                {
                    "sent": "Down is good on this graph.",
                    "label": 0
                },
                {
                    "sent": "And what we can see is that when we use all the features which is this horizontal dashed line, we're getting noticeably better performance than if we use as many as 30 features.",
                    "label": 0
                },
                {
                    "sent": "If you've done feature selection, you know that often what you see is when you remove features, your performance gets better, but here it seems to be that.",
                    "label": 0
                },
                {
                    "sent": "It's just hard to match this performance when we use all the features.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This did not escape the notice of the reviewer, so when we went to sent this workout for review, it wasn't really on feature selection, was on something different.",
                    "label": 1
                },
                {
                    "sent": "Nonetheless, a couple of reviewers commented on the fact that.",
                    "label": 0
                },
                {
                    "sent": "Feature selection or using a smaller set of inputs was somehow hurting the performance of these models.",
                    "label": 1
                },
                {
                    "sent": "And I'm a little short on time, so I'm going to skip reading these comments.",
                    "label": 0
                },
                {
                    "sent": "If you're a fast reader, maybe you have a chance to read them before I change the slide.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That brings me to this particular study.",
                    "label": 0
                },
                {
                    "sent": "We are interested in the question of whether this is.",
                    "label": 0
                },
                {
                    "sent": "A rare occurrence or whether this is something that happens frequently for bagging.",
                    "label": 0
                },
                {
                    "sent": "So said another way, is this something that's particular to this kind of domain or this problem that we're trying to solve?",
                    "label": 0
                },
                {
                    "sent": "Or is it the case that in bagging you are somehow immune to overfitting, extra irrelevant features?",
                    "label": 0
                },
                {
                    "sent": "And if so, why?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rest of the talk is going to be structured in the following way.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give some background and an overview of all the concepts in the talk, just to make sure that everyone's on the same page.",
                    "label": 0
                },
                {
                    "sent": "And after that I'll talk about an experiment we ran where we're going to do feature selection with and without bagging, and we're going to look at the something called the bias variance decomposition of the air.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what that is, don't worry.",
                    "label": 0
                },
                {
                    "sent": "I'm going to explain it in a minute.",
                    "label": 0
                },
                {
                    "sent": "Based on the results from that experiment, we saw something interesting that caused us to run a second experiment, and I think for the sake of simplicity I'll postpone talking about that experiment until the end of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, quick show of hands who is familiar with bagging?",
                    "label": 0
                },
                {
                    "sent": "OK, so it's about a little bit more than half of the room.",
                    "label": 0
                },
                {
                    "sent": "You're with bagging what you're trying to do is build an ensemble or a committee of classifiers and you do it in the following way.",
                    "label": 0
                },
                {
                    "sent": "You're going to take your training data and draw a random sample from it, and then train your model.",
                    "label": 1
                },
                {
                    "sent": "It doesn't really matter what kind of model.",
                    "label": 1
                },
                {
                    "sent": "You could use an SVM or neural network or decision tree.",
                    "label": 0
                },
                {
                    "sent": "Whatever happens to be your favorite black box classification algorithm.",
                    "label": 1
                },
                {
                    "sent": "And then you're going to repeat this some 25 times.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to average the predictions from your ensemble or committee of classifiers.",
                    "label": 0
                },
                {
                    "sent": "Now on the surface of things, this might seem crazy.",
                    "label": 0
                },
                {
                    "sent": "If you haven't seen this before, you're throwing away some of your training data so your individual models, it seems likely are going to be at least a little bit worse than if you had kept all the training data.",
                    "label": 0
                },
                {
                    "sent": "However, when you combine them.",
                    "label": 0
                },
                {
                    "sent": "The result is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perhaps seemingly magically, you get a model that is surprisingly competitive and rarely overfits, so there's very risk of doing worse if you do bagging.",
                    "label": 1
                },
                {
                    "sent": "And in many cases, if you have a noisy domain or a noisy task, this will be as competitive as a well trained neural network or support vector machine, or sometimes even boosting.",
                    "label": 0
                },
                {
                    "sent": "The main benefit it's been shown is to reduce the variance of the classifier so this comes back to the fact that I was talking about this is something that works well on a noisy task.",
                    "label": 0
                },
                {
                    "sent": "And finally, alien Pozzani and the early 90s showed that when you do bagging, that improves the base learners ability to ignore irrelevant features.",
                    "label": 1
                },
                {
                    "sent": "So this is an interesting fact that will come back to later in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, a quick review of bias variance decomposition if you have a learning algorithm, your error can come from three sources.",
                    "label": 0
                },
                {
                    "sent": "First, there might be some inherent uncertainty or stochastic city.",
                    "label": 0
                },
                {
                    "sent": "In the domain, so that would mean that your Bayes optimal error rate would be non 0 right?",
                    "label": 0
                },
                {
                    "sent": "So even if you had perfect knowledge, there's just some uncertainty that prevents you from doing perfectly.",
                    "label": 0
                },
                {
                    "sent": "Second, you have a bias which comes from the inductive bias that's inherent in any particular learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "And you can think of this as how close on our average across all different possible training sets is the algorithms prediction from the Bayes optimal prediction.",
                    "label": 1
                },
                {
                    "sent": "And finally you have variance which is.",
                    "label": 1
                },
                {
                    "sent": "If you change the training set or perturb it, how much does that change the prediction of the algorithm, and in the case of squared error, which is what I'll be talking about in this talk.",
                    "label": 0
                },
                {
                    "sent": "Air decomposes into noise plus bias plus variance in each term is non negative, so this is a pretty nice.",
                    "label": 0
                },
                {
                    "sent": "Simple way to look at.",
                    "label": 0
                },
                {
                    "sent": "Where your air is coming from.",
                    "label": 0
                },
                {
                    "sent": "We're going to be estimating these terms on real data, and it turns out that we can't separate the bias from the noise because we don't know what the Bayes optimal prediction would be.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I I think it's interesting to look at how you actually measure this on a real data set.",
                    "label": 0
                },
                {
                    "sent": "This is the method that's used by Baron Kohavi from a paper they did at the end of the 90s.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do is generate the empirical distribution of the algorithm algorithms predictions.",
                    "label": 1
                },
                {
                    "sent": "Typically for a single example, I mean you're going to look across all of your test data, but you're going to look at a distribution for each example separately.",
                    "label": 0
                },
                {
                    "sent": "So you're going to randomly sample your training data, fit a model to that sample, and then make predictions for each of your test data using that model, and then you repeat this maybe 20 times, for example.",
                    "label": 1
                },
                {
                    "sent": "And it will be useful to think about also storing an average prediction.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have your example X.",
                    "label": 0
                },
                {
                    "sent": "You have 20 predictions.",
                    "label": 0
                },
                {
                    "sent": "You know Y1Y2 all the way through Y 20.",
                    "label": 0
                },
                {
                    "sent": "It's useful to think about also storing the average of those predictions, which I'll call Y sub M here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And I won't go through the derivation here, but it turns out that the bias for the example X is just going to be the squared error between the true label and the average prediction.",
                    "label": 1
                },
                {
                    "sent": "And the variance of X is going to be the variance of all those Y one through Y 20 predictions.",
                    "label": 0
                },
                {
                    "sent": "And then you do the the thing you would expect when you look at the whole problem or the whole test set.",
                    "label": 0
                },
                {
                    "sent": "You just take an average over all your examples.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to go through this really quickly.",
                    "label": 0
                },
                {
                    "sent": "This is the feature selection session, so I think most people have probably seen this stuff.",
                    "label": 0
                },
                {
                    "sent": "There are lots of ways to do feature selection.",
                    "label": 1
                },
                {
                    "sent": "We're using two ways that have been in the literature for quite a long time.",
                    "label": 0
                },
                {
                    "sent": "The first is forward stepwise feature selection.",
                    "label": 1
                },
                {
                    "sent": "It's often called the wrapper approach and the ideas are going to greedy Hill climbing search each time adding the next feature that gives you the best benefit, and you're going to measure the best benefit by fitting a model.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This greedy forward stepwise selection is too expensive if you have lots of features.",
                    "label": 1
                },
                {
                    "sent": "If you're doing bioinformatics or natural language processing, you can't really afford to do something that's quadratic in the number of features.",
                    "label": 0
                },
                {
                    "sent": "So instead what people will sometimes do is rank the features by their individual individual correlation with the class label, and then choose a cutoff point.",
                    "label": 1
                },
                {
                    "sent": "It turns out that for this work we don't need to choose a cutoff point because we're going to be looking at Eric.",
                    "label": 0
                },
                {
                    "sent": "Various feature set sizes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How my on time?",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so here's the first experiment.",
                    "label": 0
                },
                {
                    "sent": "We're going to be measuring doing feature selection both with a single decision tree and a bag decision tree, and we're going to look at 19 different datasets, and these datasets were chosen to have a wide range of sample size and number of features so they range from problems that have a few 100 samples all the way up to 90,000 samples an from a handful of features up to 400,000 features.",
                    "label": 0
                },
                {
                    "sent": "We're going to order the features using feature selection and the kind of feature selection we use is dictated solely by the size of the problem.",
                    "label": 1
                },
                {
                    "sent": "So if we could afford to do forward stepwise selection, that's what we did.",
                    "label": 0
                },
                {
                    "sent": "And if we could not, we use correlation filtering.",
                    "label": 0
                },
                {
                    "sent": "And then once we have this feature set order, we're going to estimate the bias and variance at multiple feature set sizes.",
                    "label": 1
                },
                {
                    "sent": "So we'll estimate the bias and variance of models when you use one feature 5 features, 10 features, 15 features, etc.",
                    "label": 0
                },
                {
                    "sent": "And then to get make sure that we have reliable estimates, we're going to do all of this five times inside 5 fold cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All right, we're going to be looking at a couple of graphs like this, so I want to walk you through it to make sure that we all know how to read them on the X axis.",
                    "label": 0
                },
                {
                    "sent": "We have the number of features.",
                    "label": 0
                },
                {
                    "sent": "And on the Y axis again we have an error term.",
                    "label": 0
                },
                {
                    "sent": "So this is mean squared error.",
                    "label": 0
                },
                {
                    "sent": "And the height of each bar is the air of the corresponding models and you can see that there are pairs of bars at each feature set size.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The Left bar in each pair corresponds to a single decision tree, and the right bar to a bag decision tree.",
                    "label": 1
                },
                {
                    "sent": "And based on you can see from the colors, which is sort of a yellowish green.",
                    "label": 0
                },
                {
                    "sent": "There's the variance part of the air and then the blue part is the bias slash noise part of the air.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what can we see from this graph?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of features, it's pretty clear pattern that your bias goes down dramatically.",
                    "label": 0
                },
                {
                    "sent": "We can understand that as simply being as you give more features to an algorithm, it has more information, so we would expect that the noise and uncertainty would go down.",
                    "label": 0
                },
                {
                    "sent": "At the same time, the bias is going to go down because you are giving the algorithm more degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "It has more ways that it can fit the data.",
                    "label": 0
                },
                {
                    "sent": "On the Conversely, as you increase your features, your variance goes up and this is the same reason that your bias is going down right?",
                    "label": 0
                },
                {
                    "sent": "You're giving the algorithm more degrees of freedom in it is better able to overfit your data.",
                    "label": 0
                },
                {
                    "sent": "Alright, so jumping out to a little bit higher level.",
                    "label": 0
                },
                {
                    "sent": "We had several datasets that fell into this category and I would categorize these datasets as.",
                    "label": 1
                },
                {
                    "sent": "You didn't need to do feature selection to get the best performance.",
                    "label": 0
                },
                {
                    "sent": "It didn't matter if you did bagging or not, you just did not need to do feature selection to get the best performance.",
                    "label": 0
                },
                {
                    "sent": "On the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The category of problems.",
                    "label": 0
                },
                {
                    "sent": "However, what we saw was that.",
                    "label": 0
                },
                {
                    "sent": "After a certain point on this particular example, starting with 10 features, the single decision tree starts to overfit.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And eventually.",
                    "label": 0
                },
                {
                    "sent": "Starts to have really bad air.",
                    "label": 0
                },
                {
                    "sent": "While bagging continues to go down and it's clear from this picture why this happens, why there is this difference?",
                    "label": 0
                },
                {
                    "sent": "The variance component for bagging quickly asymptotes to a small value.",
                    "label": 0
                },
                {
                    "sent": "And so it can continue to benefit from adding more features and decreasing the bias, whereas you know if you look at the single decision trees, its bias component is still going down all the way at the end.",
                    "label": 0
                },
                {
                    "sent": "It's just that the variance is increasingly bad.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the key takeaway points here.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff between bias and variance.",
                    "label": 0
                },
                {
                    "sent": "The feature selection doesn't seem to really improve the performance of bad decision trees, and here I'm purely talking about predictive performance.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about.",
                    "label": 0
                },
                {
                    "sent": "How long it takes to make predictions, etc.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of other reasons you might want to do feature selection, but if you only care about performance, feature selection with bagging seems to be redundant.",
                    "label": 1
                },
                {
                    "sent": "And finally, it suggests that there's a new way to look at feature selection where.",
                    "label": 0
                },
                {
                    "sent": "It's maybe not a way to separate relevant from irrelevant features, but instead you are.",
                    "label": 0
                },
                {
                    "sent": "You can think of feature selection as a way to regularize a model.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to have to speed up here a little.",
                    "label": 0
                },
                {
                    "sent": "Bow is given the five minute warning 2 minutes ago.",
                    "label": 0
                },
                {
                    "sent": "I want to point out this graph.",
                    "label": 0
                },
                {
                    "sent": "I think this graph is really poor.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amazing, so this this would fall in category one here where you don't need feature selection to get the best performance with or without bagging.",
                    "label": 0
                },
                {
                    "sent": "However it turns out you can look at this and say singletree its performance plateaus or levels off around 10 features.",
                    "label": 0
                },
                {
                    "sent": "And I don't know how legible these numbers at the bottom are here, but if you look at the bagging, when you go from 800 features to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1300 features it's still improving.",
                    "label": 0
                },
                {
                    "sent": "Right now I said at the beginning of talk that it's known in the literature that bagging helps you ignore purely garbage irrelevant features, right?",
                    "label": 0
                },
                {
                    "sent": "But if that was what was happening here, we would expect bagging to level off somewhere around here, right?",
                    "label": 0
                },
                {
                    "sent": "Yet it keeps going down, so it's doing something else in addition to ignoring irrelevant features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our hypothesis was that bagging is somehow improving the base learners ability to extract useful information from noisy features.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what our second experiment was designed to test.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the interest of time, I'm going to really go through this fast.",
                    "label": 0
                },
                {
                    "sent": "We generate a synthetic data set so we had complete control over the experiment and we're going to take some of those features and make copy week copies of them that were going to corrupt an inject.",
                    "label": 0
                },
                {
                    "sent": "A lot of noise into.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to train a model using.",
                    "label": 0
                },
                {
                    "sent": "Half of the good features, and then the noisy copies of the other half.",
                    "label": 1
                },
                {
                    "sent": "OK, so I think this is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perhaps easiest to look at while I talk about the graph.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis, now we have the fraction of noise that's been added to those copied features.",
                    "label": 0
                },
                {
                    "sent": "We can look at how well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm could possibly do by saying, hey, let's go into a situation that will never have but just look at it.",
                    "label": 0
                },
                {
                    "sent": "We're going to give you the exact features used to define the classification label, and they have zero noise.",
                    "label": 0
                },
                {
                    "sent": "OK, and we can see that this problem is something that's hard for decision trees, because the error here is non 0.",
                    "label": 0
                },
                {
                    "sent": "The other thing.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can compare about against IS.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we know which features are noisy.",
                    "label": 0
                },
                {
                    "sent": "And we throw those away.",
                    "label": 0
                },
                {
                    "sent": "So we're now only going to use three inputs to the model and measure how well we would do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then everything else on this graph is what happens if you take the three non corrupted features and then add the 60 weak, noisy, corrupted, damaged features.",
                    "label": 1
                },
                {
                    "sent": "And what we can see is that.",
                    "label": 0
                },
                {
                    "sent": "At low levels of noise, both algorithms are able to get some benefit right?",
                    "label": 1
                },
                {
                    "sent": "These bars are both lower than if you had thrown away all your noisy features.",
                    "label": 0
                },
                {
                    "sent": "So there's some information being extracted from them.",
                    "label": 0
                },
                {
                    "sent": "And in fact, the bagging bar is really, I think, remarkably close to the ideal.",
                    "label": 0
                },
                {
                    "sent": "But as you continue to add noise, we can see that the noise part of the air increases and keeps climbing.",
                    "label": 1
                },
                {
                    "sent": "And quickly, because the variance for the single tree is so large, pretty much as a constant across this whole experiment here.",
                    "label": 1
                },
                {
                    "sent": "You end up doing worse than if you had thrown away your noisy features.",
                    "label": 0
                },
                {
                    "sent": "Bagging on your head, I mean just to make a quick point here at 80% noise to 80% of the feature values are.",
                    "label": 0
                },
                {
                    "sent": "Are just noise, pure garbage?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's still able to do a little bit better than the baseline here, so it's still extracting some useful information.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright conclusions.",
                    "label": 0
                },
                {
                    "sent": "So after training some 9 million decision trees in a fit of craziness, I suppose what we've learned is that there's this tradeoff between bias and variance that's related to the amount of features you give to a model.",
                    "label": 0
                },
                {
                    "sent": "And I guess I won't reiterate these.",
                    "label": 0
                },
                {
                    "sent": "And then in our second experiment we saw that bag trees are surprisingly good at extracting useful information from noisy features an we think this is happening because different trees are using different weak features, but.",
                    "label": 1
                },
                {
                    "sent": "I'm not really sure how to show this, so if anyone has any ideas I'd love to hear them.",
                    "label": 0
                },
                {
                    "sent": "And with that, I'd be happy to take any questions.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "To come into question, the company is I. I'm not sure I quite agree with your reviewer in your initial point that you're expecting that feature selection will improve performance.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I think we have lots of domain and experimental evidence were feature extra session.",
                    "label": 0
                },
                {
                    "sent": "Actually you are trying not to degrade too much of performance and basically the motivation is the one you showed in at the beginning of your true.",
                    "label": 0
                },
                {
                    "sent": "Basically you want to tell something to the field expert you apply for your birthday wishes and that's.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to select feature, not use too much and then we are happy when we have.",
                    "label": 0
                },
                {
                    "sent": "So I'm kind of surprised from from the beginning of the talk, especially that since you claim that it's good, like attendance.",
                    "label": 0
                },
                {
                    "sent": "So I did agree with that.",
                    "label": 0
                },
                {
                    "sent": "Now comes the question is you've been using.",
                    "label": 0
                },
                {
                    "sent": "Back trees with with a simple feature selection or sometimes before.",
                    "label": 1
                },
                {
                    "sent": "What selection and the next step which is very well known is to use random Perez and interpret your trees or doing embedded selection and that has been shown to be better and so that would completely change the picture.",
                    "label": 0
                },
                {
                    "sent": "So have you tried that?",
                    "label": 0
                },
                {
                    "sent": "I have not tried that.",
                    "label": 0
                },
                {
                    "sent": "I think it would be very interesting to try an.",
                    "label": 0
                },
                {
                    "sent": "I don't disagree with your comment.",
                    "label": 0
                },
                {
                    "sent": "I think it's a matter of degrees, but I don't disagree with your comment.",
                    "label": 0
                },
                {
                    "sent": "More questions, comments.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a little comment.",
                    "label": 0
                },
                {
                    "sent": "Didn't take into account the number of instances because in many domains you have very small number of instances and by increasing the number of features so much you would even with bagging command applies to eventually get into troubles.",
                    "label": 1
                },
                {
                    "sent": "So feature selection might happen that I I think that's true.",
                    "label": 0
                },
                {
                    "sent": "That's something I want to follow up on is to do something like a learning curve where you can vary among the amount of training data.",
                    "label": 0
                },
                {
                    "sent": "I was surprised actually in the domains that have hundreds of thousands of features that.",
                    "label": 0
                },
                {
                    "sent": "We didn't see more overfitting, and possibly that's because we tuned the single decision tree to not overfit too badly, so we chose sort of a.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a depth limit on training the trees to do well on those tasks.",
                    "label": 0
                },
                {
                    "sent": "But I think it's an interesting Ave to follow up on.",
                    "label": 0
                }
            ]
        }
    }
}