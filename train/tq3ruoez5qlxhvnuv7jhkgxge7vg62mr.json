{
    "id": "tq3ruoez5qlxhvnuv7jhkgxge7vg62mr",
    "title": "Joint Training for Open-domain Extraction on the Web: Exploiting Overlap when Supervision is Limited",
    "info": {
        "author": [
            "Rahul Gupta, Department of Computer Science and Engineering, Indian Institute of Technology Bombay"
        ],
        "published": "Aug. 9, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm2011_gupta_jto/",
    "segmentation": [
        [
            "I'll just begin with an example straight away as to what we do, and whatever contribution is."
        ],
        [
            "In this paper.",
            "So let's say there's a user who wants to compile a table of Clint Eastwood movies, and he's interested in a table with three columns specifically, so the columns are the movie name, the role name, and the year of release.",
            "And here's 2 examples already in mind which he knows and he wants to add more rows to this table of interest.",
            "So it goes to the web, and then the Web finds a few tables are tables and lists of interest which contains answer.",
            "So, for example, these are the three list from IMDb, Yahoo Movies and some fansite.",
            "An AC system is expected to extract.",
            "More couples of interest from these 3DS three or more list into the tables with the same schema as the user desires and combine them into a single table because the user wants to see a single table and then merge and duplicate and rank the tables and you give them to the user.",
            "So this is the task and this is this is an old piece of work which we did in VLDB 2009 and this talk is going to be concerned with the central step that you know.",
            "How do you convert these lists jointly into tables?",
            "And this is interesting 'cause you know the user provides only very very few.",
            "With supervision in this case, only two records and we expect it to extract or learn extractors which extract apples from these lists.",
            "And one thing to notice that in these three lists we have a huge amount of content overlap, which is there because these are all relevant to the same query.",
            "So we have the same movies the same here in different formats and different styles and different features.",
            "But we have content overlap.",
            "So how do you exploit this overlap to overcome the lack of supervision?",
            "Is the topic of the talk."
        ],
        [
            "So OK, so let's see how the content overlap appears in real life.",
            "So these are the three.",
            "Again, the three lists.",
            "So so the overlap happens in form of shared segments which are repeated across the sources so they can.",
            "They can be very long and they can repeat across two sources, maybe 3, maybe four sources, and so on and so forth.",
            "And they can appear anywhere in the text, so they can comprise of multiple labels, multiple entities, or they can even be noisy.",
            "For example, the word man, as I pointed out here.",
            "The man, so the man on the left refers to the movie name, sorry, the character name man with no name.",
            "And here it refers to hunky Tonk man which is the movie name.",
            "So these two have a different meaning in the two sources and therefore the shared segments can be arbitrary noisy.",
            "So this is the kind of overlap we're dealing with and we don't put any assumption on the kind of overlap we support."
        ],
        [
            "Now this is one more example, but with a much more verbose list.",
            "For example, the query is about.",
            "You know CS inventors and their contributions to computer science.",
            "It's a two column table and these are two 2 HTML lists and tables from Wikipedia and.",
            "One more site, and as you can see there are some texts which repeat across the two sources and so overlap happens here as well."
        ],
        [
            "OK, so So what is the what is the goal of this paper?",
            "So we are given very few supervision supervision in terms of only two or three records.",
            "We got multiple sources, usually 20 in our experiments and all these sources differ a lot in terms of their feature set, the features, the formatting styles, HTML templates which they use and so on.",
            "But they have a huge amount of content overlap and this is the saving grace that we have a huge amount of content overlap which can be potentially noisy.",
            "And our goal is to jointly train one extraction model per list so as to convert that list into a table of interest and to do it jointly such that the models agree on the labels of the shared content across the sources.",
            "So this is the key thing Ann.",
            "In this paper we focus on CRF extraction models which are shown to be very accurate and also very sound in terms of probabilities."
        ],
        [
            "So a very quick overview of CRF as to how they operate.",
            "So the most basic CRF works as follows that we were given a sentence X.",
            "My review of Fermat's Last theorem, and so on, and we have to assign a label to each token in the sentence.",
            "The label can be title, author, other, and so on.",
            "It depends on the label set, so the most naive way to assign labels is to assign label to each token independently, and obviously that is bad because we don't capture any correlation of labels across different words, and so therefore the basic CRF.",
            "What it does is that it actually makes a chain model out of these labels.",
            "Is says that let's say the label Wi-Fi depends on the labels Y4 and Y-6, and so you gotta change sequence dependency.",
            "These dependencies actually mathematically given by this log linear form.",
            "Below it says that it's a leader combination of the width W and the features which are given by the users F and the last time is the log that term which we call the lock partition.",
            "In our literature this is a.",
            "This is a very complex term which is easy in the case of chain models like this, but as we will see later on the talk, when the graph becomes more complex, logs becomes more and more intractable, so log that is actually just a sum of all possible labelings of.",
            "This particular sentence, that's all."
        ],
        [
            "Now back to our case.",
            "We want to learn jointly and multiple extraction models, 11 for each source.",
            "So what can we do apart from doing joint processing?",
            "Because that's expensive.",
            "So the most naive ways to club together all the sources into one giant source and just learn one CRF for the single source.",
            "And this is problematic because we don't have shared features across sources, so we cannot transfer information across sources, so this won't work.",
            "The other best ways to not do any joint training, but to couple the sources during inference time to assign the same labels to the shared content.",
            "But this only affects those records which have shared content and where there are many records which don't have shared content, the other cases are that we can do a stage training where we assign labels from a more confident source to a less confident source, and so on and so forth and so.",
            "Therefore, if you have any errors or you know upstream, they will cascade downstream as well.",
            "So this is error prone as well, and there are many methods in the literature like to view perceptrons, to view, regression and so on which work for two sources.",
            "But we have multiple sources and there are other join methods which we compare against another paper, so I will discuss them later on.",
            "So OK, so."
        ],
        [
            "What's the goal?",
            "So we are giving US data sources.",
            "Each source comes with its own set of labeled records, an unlabeled records the label record set is very small.",
            "As we saw, we were given a set of shared segments which are repeated across the sources and we have to train a CRF weight vector WI for each source I and we have this.",
            "Effective term which is given by the usual type of log likelihood of Li, which is the label set plus agreement likelihood over the set of shared segments.",
            "So the particular term which we use to denote likelihood of agreement."
        ],
        [
            "Is given by this.",
            "It's a log of summation and this kind of.",
            "This can be seen as follows that the inner term, which is Pi, matches the marginal probability that the RFI assigns are labeled YA to the set A and therefore the product term simply denotes the joint probability that all the models label a.",
            "With my A and since we do not know a priori what the label YA is going to be, just some overall possible why.",
            "So this is a lock, some formulation, and as anyone who's worked with lock solution formulation knows that this term is very intractable in the service, not convex or concave, so you can't optimize it exactly.",
            "And therefore this is a problem in our case, so I'll talk about you know how do you approximate the sacramentum attractively?"
        ],
        [
            "Now.",
            "Let's see a very small example as to how we can do do with this term.",
            "So for example, let's say we are given 2 Chainz Avian AC, which share a segment a just a single token A and their term of agreement is given by a solution over all possible levels of a marginal P1 of a into P2 of AYA.",
            "Sorry, and therefore I can just expand it into the joint probability P1 by AVP two eyc summed over by a by BYC, and now this can be seen as the sum of all possible scores of a three node graph, where the JCB is governed by P1.",
            "Yeah, baby and the edge AC is governed by P2Y AYC, so this graph can be composed from the probabilities which are known a priori and therefore this is just a partition function because as I mentioned before, the partition function is just the sum of scores of all labelings of the graph and in this case my graph is a three node graph, so it's a lot it's a partition function of this particular graph at the very bottom."
        ],
        [
            "Moving on to a more complex example, let's say we are given three sentences over here, which we have a huge amount of content overlap in terms of 4 shared segments.",
            "Matthew, Matt Groening, Matt Groening and so on.",
            "And So what I'll do is that I'll merge this almost the segments together.",
            "For example, here I actually merged on the shared segment a, so now I will merge on the shared 2nd."
        ],
        [
            "Which are longer.",
            "For example, I'll start merging on the 1st segment Matthew Matt Groening, so I get the graph at the very left top, and then I merge again.",
            "Further on the segment, Matt Groening and I'll get more more collapsed graph, which is shown at the bottom, bottom, left, and so on.",
            "So I'll keep on merging for every shared segment and at the very end."
        ],
        [
            "I'll get a completely fused graph, which I call fuse graph.",
            "It's achieved by collapsing the three sentences on all the shared segments together, so this is a giant graph which I get at the end and the same argument holds holds as I showed before.",
            "Short before that, the log, which is agreement term over here, which is given by.",
            "Will be equal to the sum of all labelings of this graph.",
            "This giant graph, which is which are denoted by logs at fused minus minus an easy term which is easy to compute, so we'll just ignore it.",
            "But the problem is that this graph is cycles and as I mentioned before, if the graph is complex, if the graph is not a tree, you can't really compute logs at its hash.",
            "Be hard to compute and therefore we have a problem again."
        ],
        [
            "So what have you done so far?",
            "We just we just took the log submission term we saw.",
            "It was hard and we just equate it to another term which is also hard.",
            "So we seem to have not achieved a lot, but the lock.",
            "Sometimes it's very hard to visualize, whereas the logs it feels term is easy to visualize in terms of a graph.",
            "That's all.",
            "That's advantage now.",
            "One way to approximate logs that fused in turn is to run belief propagation, and this is what people do in literature a lot.",
            "The belief propagation, but what we've seen in our experiments is that belief propagation on these kind of graphs don't converge very fast.",
            "If the converse, there is no guarantee that they will converge to the right answer.",
            "They won't give me the right log that fuse value, and therefore we have a problem, and there's actually one more problem that if we have noisy number agreement set, and if you know, collapse the graphs onto wrong segment, then we get it wrong graph.",
            "So there is no point running an expensive algorithm on a wrong graph.",
            "To begin with, and therefore we have a problem, So what?"
        ],
        [
            "We do is an alternate strategy we take about graph, which is which are cycles, which is hard to compute logs at four and the observation is that if you collapse in all the segments in the graph, then we have a graph with cycles which is hard.",
            "But if you collapse on only a few segments then then it's possible to get it.",
            "Re and trees are easy so."
        ],
        [
            "For example here, so my goal will be to to partition the set of segments a into disjoint sets able to AK, such that each each AI corresponds to a fuse tree instead of a fuse graph.",
            "In this case, I decompose over A1 and A2.",
            "These two sets Matt Groening and Matthew Matt Groening at the left, and the graph formed is a tree.",
            "Similarly, over here A2 comprises of Simpsons and Matt Groening, The Simpsons, and again the graph is a tree.",
            "So for each of these two graphs, I can compute the logs at individually.",
            "Easily and I can approximate the required log that's used by the sum of these two log sets, which are easy to compute.",
            "So this is the actual crux of it.",
            "Now, how do you partition so that you actually get trees in both the cases so?"
        ],
        [
            "For that we write, we write an IP, but before that I just point out very very extreme case of partitioning.",
            "So this partitioning says that if you put every segment into its own partition then we get trees trivially, because if you collapse on just one segment, you get a tree.",
            "You can't get cycles, so in this case we had four segments.",
            "So here 4 trees now.",
            "So we get trees trivially.",
            "But the problem is that we have too many nodes to process and we have four graphs to process.",
            "So this increases our runtime.",
            "So this is not desirable.",
            "The correct answer is somewhere in between.",
            "We want to partition not so fine, but not so coarse."
        ],
        [
            "Well, so this is the IP which we work with, so we want to partition a into able to AK where K is not known and such that the fuse graph for each AI has to be a tree for ease of use an we want to have low runtime in terms of number of nodes and we want to preserve correlation.",
            "We want to keep those shared segments which are correlated into the same partition.",
            "For example Matthew, Matt Groening and Matt Groening, and therefore it is easy to see that these two these two objectives are satisfied with the same criteria, which is that if we minimize the sum of nodes of all the fuse.",
            "Just found so this is this is Keith."
        ],
        [
            "To observe and this this IP we show in the paper it's NP hard and the size of agreement set.",
            "So again we have another NP hard problem at our hands.",
            "So we use a real Gotham to approximate this IP in paper and so I won't discuss the algorithm.",
            "It's fairly simple and straightforward."
        ],
        [
            "So what have you done so far?",
            "So we actually began with the first objective, which is given by the log likelihood of the labeled data plus the agreement likelihood.",
            "We saw that it was intractable to visualize or to see the agreement.",
            "Likelihood you know what it means, and so on.",
            "So we created to a much more visual log partition of a fuse graph, which is again intractable, and we decompose that by firing partitioning into fuse trees instead of fuse graph.",
            "So this is the final step in the algorithm, and we're done now, so let's see.",
            "So the same argument.",
            "Actually also holds for the gradient as well, so we can approximate the objective and the gradient using the similar mechanism, and we can optimize the objective at the top using this mechanism."
        ],
        [
            "So let's see how do we do if we if we model agreement likelihood in our experiments.",
            "So again we have the same kind of experiments we want to measure the F and accuracy of extraction of the tables that we have."
        ],
        [
            "Fact from the list and we have many such such queries.",
            "So for example, Clint Eastwood movies constitute one query and with 50 or such queries like oil spills, University Motors and so on, and each each query comes with about 2:20 HTML sources which are chosen by an index from a 500 million crawl an these these lists and tables they vary a lot in terms of columns, the number of sources, the number of records in the source is the number of overlap across sources and so on.",
            "And we measure refund using ground truth, which we corrected very painfully.",
            "Over a long period of time, and these datasets are because there are 50 datasets, I just bend them by the base model accuracy and the average number of shared segments.",
            "For ease of presentation."
        ],
        [
            "So just one last thing, which is that you know how do you compute the agreement set.",
            "So for people who are familiar with semi supervised learning, this is the same as this is equivalent to computing the graph before and before you do any processing.",
            "So how do you compute the agreement set?",
            "Because if you have noise in the agreement said then you're screwed.",
            "You cannot do anything else, so then, so the usual way to compute an agreement set is to see for uni grams that which unigrams repeat across sources and that is very noisy, because obviously unigrams can repeat arbitrarily often do not mean the same thing across sources and so on and.",
            "They don't transfer the weight of edges across sources.",
            "For that we need bigrams or trigrams and so on.",
            "So what we do is that we just we compute records record which are duplicates of each other or near duplicates of each other and we only restrict ourselves to finding shared segments across those records which are near duplicates.",
            "And this is done using approximate multipartite matching.",
            "In our paper.",
            "I won't go over the details, but this is what we do to compute the agreement set and having said."
        ],
        [
            "That, let's look at the results.",
            "So this is the comparison with two very simple baselines.",
            "So we are given in the blue bar over here, which is called joint.",
            "We compare against stage training, which is the multi stage training where we train one model at a time.",
            "The other is the green bar where we train where we don't train at all.",
            "But we do joint inference at the end by coupling over the shared segments and overall we have an accuracy boost over base from 83 to 87.5.",
            "This is better than corrective inference which can only benefit records which have overlap.",
            "And the surprising thing is that.",
            "The states training approach shows a wild fluctuation.",
            "It can go from minus 10% to plus 10% over base model and this is there because the error cascade phenomenon can happen very often in these kind of models."
        ],
        [
            "In terms of more detailed experiments, so here we compare against.",
            "Some more approaches, so we are given in the in the purple bar over here.",
            "I don't think this.",
            "Yeah, over here.",
            "So This Is Us.",
            "This is also as the orange bar.",
            "The purple and orange bar are us and we can see that all the joint approaches over here.",
            "We are the fastest over the base.",
            "We're still about five times lower than the base model, which doesn't do any coupling of training in a couple training, but we are the fastest among all the five approaches an over here we achieve about 27% reduction over the base model in terms of error and the other schemes.",
            "For example belief propagation, which are shown which are shown over here in the light blue and this.",
            "Weird blue bar.",
            "Over here, these two are either slow or not as accurate over here."
        ],
        [
            "These are some some more giant tables which contains a more numbers thing to notice that these two rows in blue are our approaches, and as you can see, we achieve up to 25%.",
            "Of error over the base model and the other approaches are are competitive, but they are slow.",
            "As we saw earlier."
        ],
        [
            "One final graph is that you know how do you choose agreement and how does it matter.",
            "So if you had magically a very clean agreement set with no noise, this is where you would be at 88% accuracy with zero percent noise.",
            "And if it shows unigrams as agreement set then we would be around here with about 17% noise and 86% F and accuracy.",
            "Where we are in the middle.",
            "This is where we are.",
            "We have 5% noise and about the same accuracy as the clean.",
            "As a clean case."
        ],
        [
            "So.",
            "This is all I have.",
            "So join further.",
            "Plenty of joint work in joint training across multiple models and there is work on multi task learning for a single source, multi multi view learning and so on.",
            "And there are two views methods which I mentioned earlier.",
            "These don't work in our case is because we have multiple sources and then there are these two methods which we compare against in our experiments and I showed that these are either not accurate or they're actually this low.",
            "And I can talk about this later offline, because these are very interesting methods which have interesting parallels to our work."
        ],
        [
            "To summarize, we join trading.",
            "Our aim was to figure out whether we can use text overlap to compensate for lack of supervision in our data, and indeed that is the case we found and we did that by rewarding agreement of the models on shared segments.",
            "Then we showed how to approximate the reward function and you know how to find low noise agreement sets to begin with.",
            "And then we did some experimental evaluation of our scheme as well and we found it the best accuracy and speed tradeoff in terms of both these parameters is given by.",
            "Agreement set over greedy tree partitions.",
            "This is still slow.",
            "It is about five times lower than the base model and this is a sequential processing which we've implemented right now, and we believe that there are some two or three very intuitive tricks which can be used to make it much faster in practice.",
            "So this is all future work for now."
        ],
        [
            "I'll take any questions.",
            "Questions please.",
            "OK.",
            "Thanks, I say with a system with limited supervision.",
            "Yeah, you know, could you say a bit more about the sensitivity to like the number of training examples and also whether it's you know there are certain types of examples that actually cause problems or you know these kind of issues to do with the choice of the supervision, right?",
            "So there are two things that there are two parts of the answer.",
            "One is that you know, in this kind of experiments over here.",
            "So with three records we got about 3% accuracy and without 7 records for trading we got about 87% accuracy.",
            "So with just one extra order two extra records, we got a huge boost, but we can't expect the user to input 5 records.",
            "It's too much supervision.",
            "So the goal was to just limit ourselves to two or three records and get the same performance as seven trading records.",
            "And this is what we actually ended up achievement, so I don't show it in the in the slides over here, But the paper shows experiments with truth or two sizes trading, three trading size of three and seven, and we have the same the same accuracy with trading size 3 plus.",
            "Plus overlap as taking sides with seven with no overlap, so we actually end up using overlap in a very good manner.",
            "The other part about the quality of the training examples which we provide.",
            "So what we notice is that each HTML risk usually comprises of two or three sub list in the sense that each sublist has its own style, so it has optional fields.",
            "It has some other extra extra formatting details and so on so forth.",
            "And given just two examples, one capture all the possible styles in a single list as well.",
            "So what overlap does is that it.",
            "Implicitly makes the other styles agree across unlabeled data of the other lists so that we are able to capture this variance in styles so the user doesn't have to be very careful about what he enters, so we're able to capture that using content overlap.",
            "Yeah.",
            "OK. OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just begin with an example straight away as to what we do, and whatever contribution is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this paper.",
                    "label": 0
                },
                {
                    "sent": "So let's say there's a user who wants to compile a table of Clint Eastwood movies, and he's interested in a table with three columns specifically, so the columns are the movie name, the role name, and the year of release.",
                    "label": 0
                },
                {
                    "sent": "And here's 2 examples already in mind which he knows and he wants to add more rows to this table of interest.",
                    "label": 0
                },
                {
                    "sent": "So it goes to the web, and then the Web finds a few tables are tables and lists of interest which contains answer.",
                    "label": 1
                },
                {
                    "sent": "So, for example, these are the three list from IMDb, Yahoo Movies and some fansite.",
                    "label": 0
                },
                {
                    "sent": "An AC system is expected to extract.",
                    "label": 0
                },
                {
                    "sent": "More couples of interest from these 3DS three or more list into the tables with the same schema as the user desires and combine them into a single table because the user wants to see a single table and then merge and duplicate and rank the tables and you give them to the user.",
                    "label": 0
                },
                {
                    "sent": "So this is the task and this is this is an old piece of work which we did in VLDB 2009 and this talk is going to be concerned with the central step that you know.",
                    "label": 0
                },
                {
                    "sent": "How do you convert these lists jointly into tables?",
                    "label": 1
                },
                {
                    "sent": "And this is interesting 'cause you know the user provides only very very few.",
                    "label": 0
                },
                {
                    "sent": "With supervision in this case, only two records and we expect it to extract or learn extractors which extract apples from these lists.",
                    "label": 0
                },
                {
                    "sent": "And one thing to notice that in these three lists we have a huge amount of content overlap, which is there because these are all relevant to the same query.",
                    "label": 0
                },
                {
                    "sent": "So we have the same movies the same here in different formats and different styles and different features.",
                    "label": 0
                },
                {
                    "sent": "But we have content overlap.",
                    "label": 0
                },
                {
                    "sent": "So how do you exploit this overlap to overcome the lack of supervision?",
                    "label": 0
                },
                {
                    "sent": "Is the topic of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so let's see how the content overlap appears in real life.",
                    "label": 1
                },
                {
                    "sent": "So these are the three.",
                    "label": 0
                },
                {
                    "sent": "Again, the three lists.",
                    "label": 0
                },
                {
                    "sent": "So so the overlap happens in form of shared segments which are repeated across the sources so they can.",
                    "label": 0
                },
                {
                    "sent": "They can be very long and they can repeat across two sources, maybe 3, maybe four sources, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And they can appear anywhere in the text, so they can comprise of multiple labels, multiple entities, or they can even be noisy.",
                    "label": 0
                },
                {
                    "sent": "For example, the word man, as I pointed out here.",
                    "label": 0
                },
                {
                    "sent": "The man, so the man on the left refers to the movie name, sorry, the character name man with no name.",
                    "label": 0
                },
                {
                    "sent": "And here it refers to hunky Tonk man which is the movie name.",
                    "label": 1
                },
                {
                    "sent": "So these two have a different meaning in the two sources and therefore the shared segments can be arbitrary noisy.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of overlap we're dealing with and we don't put any assumption on the kind of overlap we support.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this is one more example, but with a much more verbose list.",
                    "label": 0
                },
                {
                    "sent": "For example, the query is about.",
                    "label": 0
                },
                {
                    "sent": "You know CS inventors and their contributions to computer science.",
                    "label": 1
                },
                {
                    "sent": "It's a two column table and these are two 2 HTML lists and tables from Wikipedia and.",
                    "label": 0
                },
                {
                    "sent": "One more site, and as you can see there are some texts which repeat across the two sources and so overlap happens here as well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so So what is the what is the goal of this paper?",
                    "label": 0
                },
                {
                    "sent": "So we are given very few supervision supervision in terms of only two or three records.",
                    "label": 0
                },
                {
                    "sent": "We got multiple sources, usually 20 in our experiments and all these sources differ a lot in terms of their feature set, the features, the formatting styles, HTML templates which they use and so on.",
                    "label": 0
                },
                {
                    "sent": "But they have a huge amount of content overlap and this is the saving grace that we have a huge amount of content overlap which can be potentially noisy.",
                    "label": 0
                },
                {
                    "sent": "And our goal is to jointly train one extraction model per list so as to convert that list into a table of interest and to do it jointly such that the models agree on the labels of the shared content across the sources.",
                    "label": 1
                },
                {
                    "sent": "So this is the key thing Ann.",
                    "label": 0
                },
                {
                    "sent": "In this paper we focus on CRF extraction models which are shown to be very accurate and also very sound in terms of probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a very quick overview of CRF as to how they operate.",
                    "label": 0
                },
                {
                    "sent": "So the most basic CRF works as follows that we were given a sentence X.",
                    "label": 0
                },
                {
                    "sent": "My review of Fermat's Last theorem, and so on, and we have to assign a label to each token in the sentence.",
                    "label": 1
                },
                {
                    "sent": "The label can be title, author, other, and so on.",
                    "label": 0
                },
                {
                    "sent": "It depends on the label set, so the most naive way to assign labels is to assign label to each token independently, and obviously that is bad because we don't capture any correlation of labels across different words, and so therefore the basic CRF.",
                    "label": 0
                },
                {
                    "sent": "What it does is that it actually makes a chain model out of these labels.",
                    "label": 0
                },
                {
                    "sent": "Is says that let's say the label Wi-Fi depends on the labels Y4 and Y-6, and so you gotta change sequence dependency.",
                    "label": 0
                },
                {
                    "sent": "These dependencies actually mathematically given by this log linear form.",
                    "label": 0
                },
                {
                    "sent": "Below it says that it's a leader combination of the width W and the features which are given by the users F and the last time is the log that term which we call the lock partition.",
                    "label": 0
                },
                {
                    "sent": "In our literature this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a very complex term which is easy in the case of chain models like this, but as we will see later on the talk, when the graph becomes more complex, logs becomes more and more intractable, so log that is actually just a sum of all possible labelings of.",
                    "label": 0
                },
                {
                    "sent": "This particular sentence, that's all.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now back to our case.",
                    "label": 0
                },
                {
                    "sent": "We want to learn jointly and multiple extraction models, 11 for each source.",
                    "label": 0
                },
                {
                    "sent": "So what can we do apart from doing joint processing?",
                    "label": 0
                },
                {
                    "sent": "Because that's expensive.",
                    "label": 0
                },
                {
                    "sent": "So the most naive ways to club together all the sources into one giant source and just learn one CRF for the single source.",
                    "label": 1
                },
                {
                    "sent": "And this is problematic because we don't have shared features across sources, so we cannot transfer information across sources, so this won't work.",
                    "label": 0
                },
                {
                    "sent": "The other best ways to not do any joint training, but to couple the sources during inference time to assign the same labels to the shared content.",
                    "label": 0
                },
                {
                    "sent": "But this only affects those records which have shared content and where there are many records which don't have shared content, the other cases are that we can do a stage training where we assign labels from a more confident source to a less confident source, and so on and so forth and so.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if you have any errors or you know upstream, they will cascade downstream as well.",
                    "label": 0
                },
                {
                    "sent": "So this is error prone as well, and there are many methods in the literature like to view perceptrons, to view, regression and so on which work for two sources.",
                    "label": 0
                },
                {
                    "sent": "But we have multiple sources and there are other join methods which we compare against another paper, so I will discuss them later on.",
                    "label": 1
                },
                {
                    "sent": "So OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's the goal?",
                    "label": 0
                },
                {
                    "sent": "So we are giving US data sources.",
                    "label": 0
                },
                {
                    "sent": "Each source comes with its own set of labeled records, an unlabeled records the label record set is very small.",
                    "label": 0
                },
                {
                    "sent": "As we saw, we were given a set of shared segments which are repeated across the sources and we have to train a CRF weight vector WI for each source I and we have this.",
                    "label": 0
                },
                {
                    "sent": "Effective term which is given by the usual type of log likelihood of Li, which is the label set plus agreement likelihood over the set of shared segments.",
                    "label": 0
                },
                {
                    "sent": "So the particular term which we use to denote likelihood of agreement.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is given by this.",
                    "label": 0
                },
                {
                    "sent": "It's a log of summation and this kind of.",
                    "label": 0
                },
                {
                    "sent": "This can be seen as follows that the inner term, which is Pi, matches the marginal probability that the RFI assigns are labeled YA to the set A and therefore the product term simply denotes the joint probability that all the models label a.",
                    "label": 0
                },
                {
                    "sent": "With my A and since we do not know a priori what the label YA is going to be, just some overall possible why.",
                    "label": 0
                },
                {
                    "sent": "So this is a lock, some formulation, and as anyone who's worked with lock solution formulation knows that this term is very intractable in the service, not convex or concave, so you can't optimize it exactly.",
                    "label": 0
                },
                {
                    "sent": "And therefore this is a problem in our case, so I'll talk about you know how do you approximate the sacramentum attractively?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's see a very small example as to how we can do do with this term.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's say we are given 2 Chainz Avian AC, which share a segment a just a single token A and their term of agreement is given by a solution over all possible levels of a marginal P1 of a into P2 of AYA.",
                    "label": 0
                },
                {
                    "sent": "Sorry, and therefore I can just expand it into the joint probability P1 by AVP two eyc summed over by a by BYC, and now this can be seen as the sum of all possible scores of a three node graph, where the JCB is governed by P1.",
                    "label": 0
                },
                {
                    "sent": "Yeah, baby and the edge AC is governed by P2Y AYC, so this graph can be composed from the probabilities which are known a priori and therefore this is just a partition function because as I mentioned before, the partition function is just the sum of scores of all labelings of the graph and in this case my graph is a three node graph, so it's a lot it's a partition function of this particular graph at the very bottom.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving on to a more complex example, let's say we are given three sentences over here, which we have a huge amount of content overlap in terms of 4 shared segments.",
                    "label": 0
                },
                {
                    "sent": "Matthew, Matt Groening, Matt Groening and so on.",
                    "label": 1
                },
                {
                    "sent": "And So what I'll do is that I'll merge this almost the segments together.",
                    "label": 0
                },
                {
                    "sent": "For example, here I actually merged on the shared segment a, so now I will merge on the shared 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which are longer.",
                    "label": 0
                },
                {
                    "sent": "For example, I'll start merging on the 1st segment Matthew Matt Groening, so I get the graph at the very left top, and then I merge again.",
                    "label": 0
                },
                {
                    "sent": "Further on the segment, Matt Groening and I'll get more more collapsed graph, which is shown at the bottom, bottom, left, and so on.",
                    "label": 1
                },
                {
                    "sent": "So I'll keep on merging for every shared segment and at the very end.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll get a completely fused graph, which I call fuse graph.",
                    "label": 1
                },
                {
                    "sent": "It's achieved by collapsing the three sentences on all the shared segments together, so this is a giant graph which I get at the end and the same argument holds holds as I showed before.",
                    "label": 1
                },
                {
                    "sent": "Short before that, the log, which is agreement term over here, which is given by.",
                    "label": 0
                },
                {
                    "sent": "Will be equal to the sum of all labelings of this graph.",
                    "label": 0
                },
                {
                    "sent": "This giant graph, which is which are denoted by logs at fused minus minus an easy term which is easy to compute, so we'll just ignore it.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that this graph is cycles and as I mentioned before, if the graph is complex, if the graph is not a tree, you can't really compute logs at its hash.",
                    "label": 1
                },
                {
                    "sent": "Be hard to compute and therefore we have a problem again.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what have you done so far?",
                    "label": 0
                },
                {
                    "sent": "We just we just took the log submission term we saw.",
                    "label": 0
                },
                {
                    "sent": "It was hard and we just equate it to another term which is also hard.",
                    "label": 0
                },
                {
                    "sent": "So we seem to have not achieved a lot, but the lock.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's very hard to visualize, whereas the logs it feels term is easy to visualize in terms of a graph.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "That's advantage now.",
                    "label": 0
                },
                {
                    "sent": "One way to approximate logs that fused in turn is to run belief propagation, and this is what people do in literature a lot.",
                    "label": 0
                },
                {
                    "sent": "The belief propagation, but what we've seen in our experiments is that belief propagation on these kind of graphs don't converge very fast.",
                    "label": 0
                },
                {
                    "sent": "If the converse, there is no guarantee that they will converge to the right answer.",
                    "label": 0
                },
                {
                    "sent": "They won't give me the right log that fuse value, and therefore we have a problem, and there's actually one more problem that if we have noisy number agreement set, and if you know, collapse the graphs onto wrong segment, then we get it wrong graph.",
                    "label": 0
                },
                {
                    "sent": "So there is no point running an expensive algorithm on a wrong graph.",
                    "label": 0
                },
                {
                    "sent": "To begin with, and therefore we have a problem, So what?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do is an alternate strategy we take about graph, which is which are cycles, which is hard to compute logs at four and the observation is that if you collapse in all the segments in the graph, then we have a graph with cycles which is hard.",
                    "label": 0
                },
                {
                    "sent": "But if you collapse on only a few segments then then it's possible to get it.",
                    "label": 1
                },
                {
                    "sent": "Re and trees are easy so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example here, so my goal will be to to partition the set of segments a into disjoint sets able to AK, such that each each AI corresponds to a fuse tree instead of a fuse graph.",
                    "label": 0
                },
                {
                    "sent": "In this case, I decompose over A1 and A2.",
                    "label": 0
                },
                {
                    "sent": "These two sets Matt Groening and Matthew Matt Groening at the left, and the graph formed is a tree.",
                    "label": 1
                },
                {
                    "sent": "Similarly, over here A2 comprises of Simpsons and Matt Groening, The Simpsons, and again the graph is a tree.",
                    "label": 0
                },
                {
                    "sent": "So for each of these two graphs, I can compute the logs at individually.",
                    "label": 0
                },
                {
                    "sent": "Easily and I can approximate the required log that's used by the sum of these two log sets, which are easy to compute.",
                    "label": 0
                },
                {
                    "sent": "So this is the actual crux of it.",
                    "label": 0
                },
                {
                    "sent": "Now, how do you partition so that you actually get trees in both the cases so?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For that we write, we write an IP, but before that I just point out very very extreme case of partitioning.",
                    "label": 0
                },
                {
                    "sent": "So this partitioning says that if you put every segment into its own partition then we get trees trivially, because if you collapse on just one segment, you get a tree.",
                    "label": 0
                },
                {
                    "sent": "You can't get cycles, so in this case we had four segments.",
                    "label": 0
                },
                {
                    "sent": "So here 4 trees now.",
                    "label": 0
                },
                {
                    "sent": "So we get trees trivially.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that we have too many nodes to process and we have four graphs to process.",
                    "label": 0
                },
                {
                    "sent": "So this increases our runtime.",
                    "label": 0
                },
                {
                    "sent": "So this is not desirable.",
                    "label": 0
                },
                {
                    "sent": "The correct answer is somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "We want to partition not so fine, but not so coarse.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, so this is the IP which we work with, so we want to partition a into able to AK where K is not known and such that the fuse graph for each AI has to be a tree for ease of use an we want to have low runtime in terms of number of nodes and we want to preserve correlation.",
                    "label": 0
                },
                {
                    "sent": "We want to keep those shared segments which are correlated into the same partition.",
                    "label": 0
                },
                {
                    "sent": "For example Matthew, Matt Groening and Matt Groening, and therefore it is easy to see that these two these two objectives are satisfied with the same criteria, which is that if we minimize the sum of nodes of all the fuse.",
                    "label": 1
                },
                {
                    "sent": "Just found so this is this is Keith.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To observe and this this IP we show in the paper it's NP hard and the size of agreement set.",
                    "label": 1
                },
                {
                    "sent": "So again we have another NP hard problem at our hands.",
                    "label": 0
                },
                {
                    "sent": "So we use a real Gotham to approximate this IP in paper and so I won't discuss the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's fairly simple and straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what have you done so far?",
                    "label": 0
                },
                {
                    "sent": "So we actually began with the first objective, which is given by the log likelihood of the labeled data plus the agreement likelihood.",
                    "label": 1
                },
                {
                    "sent": "We saw that it was intractable to visualize or to see the agreement.",
                    "label": 0
                },
                {
                    "sent": "Likelihood you know what it means, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we created to a much more visual log partition of a fuse graph, which is again intractable, and we decompose that by firing partitioning into fuse trees instead of fuse graph.",
                    "label": 1
                },
                {
                    "sent": "So this is the final step in the algorithm, and we're done now, so let's see.",
                    "label": 0
                },
                {
                    "sent": "So the same argument.",
                    "label": 0
                },
                {
                    "sent": "Actually also holds for the gradient as well, so we can approximate the objective and the gradient using the similar mechanism, and we can optimize the objective at the top using this mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how do we do if we if we model agreement likelihood in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So again we have the same kind of experiments we want to measure the F and accuracy of extraction of the tables that we have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fact from the list and we have many such such queries.",
                    "label": 0
                },
                {
                    "sent": "So for example, Clint Eastwood movies constitute one query and with 50 or such queries like oil spills, University Motors and so on, and each each query comes with about 2:20 HTML sources which are chosen by an index from a 500 million crawl an these these lists and tables they vary a lot in terms of columns, the number of sources, the number of records in the source is the number of overlap across sources and so on.",
                    "label": 0
                },
                {
                    "sent": "And we measure refund using ground truth, which we corrected very painfully.",
                    "label": 1
                },
                {
                    "sent": "Over a long period of time, and these datasets are because there are 50 datasets, I just bend them by the base model accuracy and the average number of shared segments.",
                    "label": 1
                },
                {
                    "sent": "For ease of presentation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just one last thing, which is that you know how do you compute the agreement set.",
                    "label": 0
                },
                {
                    "sent": "So for people who are familiar with semi supervised learning, this is the same as this is equivalent to computing the graph before and before you do any processing.",
                    "label": 0
                },
                {
                    "sent": "So how do you compute the agreement set?",
                    "label": 1
                },
                {
                    "sent": "Because if you have noise in the agreement said then you're screwed.",
                    "label": 0
                },
                {
                    "sent": "You cannot do anything else, so then, so the usual way to compute an agreement set is to see for uni grams that which unigrams repeat across sources and that is very noisy, because obviously unigrams can repeat arbitrarily often do not mean the same thing across sources and so on and.",
                    "label": 0
                },
                {
                    "sent": "They don't transfer the weight of edges across sources.",
                    "label": 0
                },
                {
                    "sent": "For that we need bigrams or trigrams and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we just we compute records record which are duplicates of each other or near duplicates of each other and we only restrict ourselves to finding shared segments across those records which are near duplicates.",
                    "label": 1
                },
                {
                    "sent": "And this is done using approximate multipartite matching.",
                    "label": 0
                },
                {
                    "sent": "In our paper.",
                    "label": 0
                },
                {
                    "sent": "I won't go over the details, but this is what we do to compute the agreement set and having said.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That, let's look at the results.",
                    "label": 0
                },
                {
                    "sent": "So this is the comparison with two very simple baselines.",
                    "label": 0
                },
                {
                    "sent": "So we are given in the blue bar over here, which is called joint.",
                    "label": 0
                },
                {
                    "sent": "We compare against stage training, which is the multi stage training where we train one model at a time.",
                    "label": 0
                },
                {
                    "sent": "The other is the green bar where we train where we don't train at all.",
                    "label": 0
                },
                {
                    "sent": "But we do joint inference at the end by coupling over the shared segments and overall we have an accuracy boost over base from 83 to 87.5.",
                    "label": 0
                },
                {
                    "sent": "This is better than corrective inference which can only benefit records which have overlap.",
                    "label": 0
                },
                {
                    "sent": "And the surprising thing is that.",
                    "label": 0
                },
                {
                    "sent": "The states training approach shows a wild fluctuation.",
                    "label": 0
                },
                {
                    "sent": "It can go from minus 10% to plus 10% over base model and this is there because the error cascade phenomenon can happen very often in these kind of models.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of more detailed experiments, so here we compare against.",
                    "label": 0
                },
                {
                    "sent": "Some more approaches, so we are given in the in the purple bar over here.",
                    "label": 0
                },
                {
                    "sent": "I don't think this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, over here.",
                    "label": 0
                },
                {
                    "sent": "So This Is Us.",
                    "label": 0
                },
                {
                    "sent": "This is also as the orange bar.",
                    "label": 0
                },
                {
                    "sent": "The purple and orange bar are us and we can see that all the joint approaches over here.",
                    "label": 0
                },
                {
                    "sent": "We are the fastest over the base.",
                    "label": 0
                },
                {
                    "sent": "We're still about five times lower than the base model, which doesn't do any coupling of training in a couple training, but we are the fastest among all the five approaches an over here we achieve about 27% reduction over the base model in terms of error and the other schemes.",
                    "label": 0
                },
                {
                    "sent": "For example belief propagation, which are shown which are shown over here in the light blue and this.",
                    "label": 0
                },
                {
                    "sent": "Weird blue bar.",
                    "label": 0
                },
                {
                    "sent": "Over here, these two are either slow or not as accurate over here.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are some some more giant tables which contains a more numbers thing to notice that these two rows in blue are our approaches, and as you can see, we achieve up to 25%.",
                    "label": 0
                },
                {
                    "sent": "Of error over the base model and the other approaches are are competitive, but they are slow.",
                    "label": 0
                },
                {
                    "sent": "As we saw earlier.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One final graph is that you know how do you choose agreement and how does it matter.",
                    "label": 0
                },
                {
                    "sent": "So if you had magically a very clean agreement set with no noise, this is where you would be at 88% accuracy with zero percent noise.",
                    "label": 0
                },
                {
                    "sent": "And if it shows unigrams as agreement set then we would be around here with about 17% noise and 86% F and accuracy.",
                    "label": 0
                },
                {
                    "sent": "Where we are in the middle.",
                    "label": 0
                },
                {
                    "sent": "This is where we are.",
                    "label": 0
                },
                {
                    "sent": "We have 5% noise and about the same accuracy as the clean.",
                    "label": 0
                },
                {
                    "sent": "As a clean case.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is all I have.",
                    "label": 0
                },
                {
                    "sent": "So join further.",
                    "label": 0
                },
                {
                    "sent": "Plenty of joint work in joint training across multiple models and there is work on multi task learning for a single source, multi multi view learning and so on.",
                    "label": 0
                },
                {
                    "sent": "And there are two views methods which I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "These don't work in our case is because we have multiple sources and then there are these two methods which we compare against in our experiments and I showed that these are either not accurate or they're actually this low.",
                    "label": 0
                },
                {
                    "sent": "And I can talk about this later offline, because these are very interesting methods which have interesting parallels to our work.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize, we join trading.",
                    "label": 0
                },
                {
                    "sent": "Our aim was to figure out whether we can use text overlap to compensate for lack of supervision in our data, and indeed that is the case we found and we did that by rewarding agreement of the models on shared segments.",
                    "label": 1
                },
                {
                    "sent": "Then we showed how to approximate the reward function and you know how to find low noise agreement sets to begin with.",
                    "label": 1
                },
                {
                    "sent": "And then we did some experimental evaluation of our scheme as well and we found it the best accuracy and speed tradeoff in terms of both these parameters is given by.",
                    "label": 0
                },
                {
                    "sent": "Agreement set over greedy tree partitions.",
                    "label": 1
                },
                {
                    "sent": "This is still slow.",
                    "label": 1
                },
                {
                    "sent": "It is about five times lower than the base model and this is a sequential processing which we've implemented right now, and we believe that there are some two or three very intuitive tricks which can be used to make it much faster in practice.",
                    "label": 0
                },
                {
                    "sent": "So this is all future work for now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll take any questions.",
                    "label": 0
                },
                {
                    "sent": "Questions please.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks, I say with a system with limited supervision.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you know, could you say a bit more about the sensitivity to like the number of training examples and also whether it's you know there are certain types of examples that actually cause problems or you know these kind of issues to do with the choice of the supervision, right?",
                    "label": 0
                },
                {
                    "sent": "So there are two things that there are two parts of the answer.",
                    "label": 0
                },
                {
                    "sent": "One is that you know, in this kind of experiments over here.",
                    "label": 0
                },
                {
                    "sent": "So with three records we got about 3% accuracy and without 7 records for trading we got about 87% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So with just one extra order two extra records, we got a huge boost, but we can't expect the user to input 5 records.",
                    "label": 0
                },
                {
                    "sent": "It's too much supervision.",
                    "label": 0
                },
                {
                    "sent": "So the goal was to just limit ourselves to two or three records and get the same performance as seven trading records.",
                    "label": 0
                },
                {
                    "sent": "And this is what we actually ended up achievement, so I don't show it in the in the slides over here, But the paper shows experiments with truth or two sizes trading, three trading size of three and seven, and we have the same the same accuracy with trading size 3 plus.",
                    "label": 0
                },
                {
                    "sent": "Plus overlap as taking sides with seven with no overlap, so we actually end up using overlap in a very good manner.",
                    "label": 0
                },
                {
                    "sent": "The other part about the quality of the training examples which we provide.",
                    "label": 0
                },
                {
                    "sent": "So what we notice is that each HTML risk usually comprises of two or three sub list in the sense that each sublist has its own style, so it has optional fields.",
                    "label": 0
                },
                {
                    "sent": "It has some other extra extra formatting details and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And given just two examples, one capture all the possible styles in a single list as well.",
                    "label": 0
                },
                {
                    "sent": "So what overlap does is that it.",
                    "label": 0
                },
                {
                    "sent": "Implicitly makes the other styles agree across unlabeled data of the other lists so that we are able to capture this variance in styles so the user doesn't have to be very careful about what he enters, so we're able to capture that using content overlap.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}