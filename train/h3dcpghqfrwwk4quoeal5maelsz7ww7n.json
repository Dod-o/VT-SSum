{
    "id": "h3dcpghqfrwwk4quoeal5maelsz7ww7n",
    "title": "Hilbert Space Embedding for Dirichlet Process Mixtures",
    "info": {
        "author": [
            "Krikamol Muandet, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Jan. 18, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_muandet_dirichlet/",
    "segmentation": [
        [
            "Hello, everyone says.",
            "It's my pleasure to be here and.",
            "I have to apologize that what I'm going to talk about here is a bit provisional Ann, I hope, but I hope at least it will somehow.",
            "I mean have achieved the code this workshop in.",
            "I mean invoking an interesting discussion and the connection between kernel and graphical model.",
            "So basically what I'm going to talk about is the Hilbert space embedding for Richard Process mixture.",
            "So this morning here.",
            "You already heard about many talk in kernel stuff.",
            "Fan base in nonparametric model.",
            "So what is the motivation?"
        ],
        [
            "This talk so invasion nonparametric model.",
            "So you should have a flexible way of modeling your prior.",
            "Prior distribution so basically is allow you to sort of having the unbounded.",
            "Model complexity, so in other word, the complexity of the model is determined by the data that you observe.",
            "But unfortunately exact inference is often intractable, so people usually resorted to Markov chain Monte Carlo or like Gibbs sampling or variational Bayes approach.",
            "But on the other hand, the kernel method for sort of very efficient computation.",
            "So usually the problem you see it can be formulated as a convex optimization problem, But I mean the kernel method also suffer from this model selection and comparisons.",
            "For example, you need to do cross validation to specify the right model complexity."
        ],
        [
            "So in in in general, when people work with this and based in.",
            "Inference sofort they so they specify the prior distribution and then they use base theorem to sort of derive the post degree and then they do inference.",
            "And this is basically the variables and this morning candy already told you that how how you can achieve this using a kernel method but in basic non parameter you basically dealing with stochastic processes not.",
            "Not just a distribution, so so I'm wondering if we can use sort of kernel stuff to deal with this stochastic process.",
            "So in this in this talk we're going to sort of focus on basic.",
            "I think it's the simplest problem in Bayesian inference.",
            "Sort of clustering problem.",
            "So if you're given the data and you want to do some clustering and you ask so usually people assume that the."
        ],
        [
            "He did.",
            "The model can be written as a sort of a mixture of a simple distribution.",
            "Let's say you have a data.",
            "You assume that the data are drawn from some mixture model.",
            "For example, a mixture of Gaussians, something like that, and the key question in this in this problem is that I mean what is the right number of cluster.",
            "So this is so this problem is solved can be solved using this Bayesian nonparametric model where you can specify the prior for this number.",
            "Cluster so you don't need to specify the right number of cluster, but I mean you learn from."
        ],
        [
            "Radiator.",
            "OK, and in order to to go into the detail, I want to give you for the definition of division that process.",
            "So basically they do.",
            "It processes the distribution of the random probability measure over a measurable space such that for any finite partition of the your space.",
            "So you have if you consider the.",
            "The choice distribution of that partition you have additional distribution then for for any partition of your input space.",
            "Do you aside this distribution?",
            "D draw from this ritual process.",
            "Then you get digital distribution."
        ],
        [
            "OK, and then from that dishware process you can specify the ritual process.",
            "Make your model so first you do all the distribution from the repair process with some parameter, GS0 an Alpha where Alpha.",
            "Here's something like the.",
            "I mean so one can show that this parameter of correspond to some the expected number of clusters and then you draw some parameter.",
            "So here you would get some sort of discrete distribution and then you you do all this data I from this discrete distribution and that data is the latent variable that's parameterized.",
            "The distribution of the observed data point.",
            "For example this data I can can be like the parameter of the Gaussian distribution.",
            "OK."
        ],
        [
            "But I mean from from that part of you is not really clear what is the the division process and how we can prioritize is using kernel step.",
            "So in 1994 there is a paper that showed that you can also construct this distribution from the research process by consider what we call this stick breaking construction.",
            "So basically you have a unit length stick and then.",
            "$30 random variable from the data beta distribution.",
            "Then we specify the point in which you at which you break the stick, and then you repeat this process and then you get this.",
            "Pie I wish is mixing proportion of your mixture model and it can be shown that the stick breaking construction give the same probability measure.",
            "Overall random measure on the measurable space with Richard Process with the same parameter Alpha and this did not.",
            "OK from this."
        ],
        [
            "Construction then we proposed this Huber space embedding for digital process.",
            "Make your model so.",
            "So let's say this is the space or like Richard Process, make sure that you draw from this division process.",
            "Then you just embed it into the Huber space by this formulation that you saw earlier in this talk this morning.",
            "So it's just the expectation of the kernel under the distribution that you draw from from these reprocessed mixture.",
            "So backs basically just the infinite infinite sum of the the.",
            "I I, which correspond to this where each component is the density function that you specify in your model.",
            "For example, you can use Gaussian distribution.",
            "And we assume that this is a density function so that you have a verify distribution.",
            "OK, and one problem with this formulation is that I mean you have an infinite sum, so you can not sort of work with it directly, so."
        ],
        [
            "But Fortunately in 2001, there's a nice observation that if you sort of.",
            "Truncate the this stick breaking construction at sufficiently large truncation level.",
            "Then you have sort of an excellent approximation to the full model.",
            "So then we so, inspired by this observation, then we propose this truncated digitally process major embedding.",
            "So instead of considering the infinite sum, you just truncate the summer.",
            "Some truncation level T, then the next question is how much information we lose by using this content creation."
        ],
        [
            "And we can show that the.",
            "OK, the the different in the acacias norm of this two embedding.",
            "So this is the embedding of the full model and this is the embedding of the truncated version so you can show that this the information loss can be bounded by some exponential function that depends on this truncation level.",
            "So basically you say that the error go sort of reduced exponentially fast."
        ],
        [
            "So.",
            "Using this result, we say that OK if we use this truncated version of the embedding, then the error will be small.",
            "And then we can use this as a circuit 222 DPM model.",
            "And also the bio suggested how to choose this term casual level.",
            "So that you have the error to be small, smaller than some specified small values."
        ],
        [
            "Then because then, given a truncation version of the of the model, how do we learn this?",
            "Model.",
            "So we consider this similar problem that has been proposed by leasing in 2000 X.",
            "So basically what we want is that given a truncated DP, ME and some observation.",
            "So we want to minimize this different.",
            "So this is the basically empirical mean embedding of the data, so it gives you the access to this or the distribution of the data.",
            "And this is the embedding of the division process mixture model and then we can show that this is basically.",
            "If you, I mean, if you consider the parameter Pi, then this is Pis can be is the just the solution of this quadratic programming.",
            "Where is is the metric that depends on the inner product between the embedding of the mixture component and this is the vector that depends on the inner product between the empirical embedding and the embedding of the mixture component.",
            "But still you have to also learn this data parameter.",
            "So basically the opposite."
        ],
        [
            "Optimization problems go like this.",
            "So first you specify the parameter Alpha and the other is the error that you want to achieve and then you estimate the truncation level and then you optimize.",
            "These two 2 step until convergence.",
            "So you learn this pie using the quadratic programming on also optimized parameter data using constraint optimization and then after you get the the model then you can do clustering according to that model.",
            "Key observation here is that you don't need any sort of doing the optimization.",
            "You don't need to have the access to value up latent variable of the data.",
            "As opposed to like give sampling and variational inference."
        ],
        [
            "OK, so let look somebody.",
            "OK, so here you have.",
            "So the true number of component is 4 and then we choose.",
            "Truncation level to be 6.",
            "And the cause is a bit slow.",
            "But I think she should work.",
            "So basically, the point of this is that I mean I mean in order so so in K mean clustering problem you have to specify the right number of cluster, right?",
            "An in order to do clustering.",
            "But in this case you can see that I mean.",
            "Here we specify the bounded number of cluster and then we do optimization.",
            "And then.",
            "You don't need to specify the right number of cluster, and you can see that some of the mixing coefficient is.",
            "Here's go to zero and you can still have the right number of cluster by incorporating this into the.",
            "The optimization problem.",
            "Yeah."
        ],
        [
            "OK, so in conclusion I talked about this conjunction between base in nonparametric and kernel methods.",
            "So specifically I propose this to both read embedding for different way process mixture model and there's so many often question.",
            "For example how to avoid this truncation and also the solution of this I mean optimization problem related to the maximum likelihood or the map solution of the standard approach.",
            "Ann and I also did.",
            "The important problem is that how to choose the kernel K because this solution is also very sensitive to this type of kernel.",
            "And last but not least, I think it would be interesting to look into these kernel method and also random measure thing.",
            "So it's so in the previous talk you see this kernel random measure.",
            "But in this sense is a bit different because I mean for example.",
            "Instead of considering random variable and now you can have random measure and for example you can do like 2 sample tests for random measure which has a lot of application in academic changepoint detection.",
            "Something like that and OK. Lastly I want to thankfully panic Anne Francis Path for like food, food, conduct, discussion.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, everyone says.",
                    "label": 0
                },
                {
                    "sent": "It's my pleasure to be here and.",
                    "label": 0
                },
                {
                    "sent": "I have to apologize that what I'm going to talk about here is a bit provisional Ann, I hope, but I hope at least it will somehow.",
                    "label": 0
                },
                {
                    "sent": "I mean have achieved the code this workshop in.",
                    "label": 0
                },
                {
                    "sent": "I mean invoking an interesting discussion and the connection between kernel and graphical model.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm going to talk about is the Hilbert space embedding for Richard Process mixture.",
                    "label": 1
                },
                {
                    "sent": "So this morning here.",
                    "label": 0
                },
                {
                    "sent": "You already heard about many talk in kernel stuff.",
                    "label": 0
                },
                {
                    "sent": "Fan base in nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "So what is the motivation?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk so invasion nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "So you should have a flexible way of modeling your prior.",
                    "label": 0
                },
                {
                    "sent": "Prior distribution so basically is allow you to sort of having the unbounded.",
                    "label": 0
                },
                {
                    "sent": "Model complexity, so in other word, the complexity of the model is determined by the data that you observe.",
                    "label": 1
                },
                {
                    "sent": "But unfortunately exact inference is often intractable, so people usually resorted to Markov chain Monte Carlo or like Gibbs sampling or variational Bayes approach.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, the kernel method for sort of very efficient computation.",
                    "label": 0
                },
                {
                    "sent": "So usually the problem you see it can be formulated as a convex optimization problem, But I mean the kernel method also suffer from this model selection and comparisons.",
                    "label": 1
                },
                {
                    "sent": "For example, you need to do cross validation to specify the right model complexity.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in in in general, when people work with this and based in.",
                    "label": 0
                },
                {
                    "sent": "Inference sofort they so they specify the prior distribution and then they use base theorem to sort of derive the post degree and then they do inference.",
                    "label": 0
                },
                {
                    "sent": "And this is basically the variables and this morning candy already told you that how how you can achieve this using a kernel method but in basic non parameter you basically dealing with stochastic processes not.",
                    "label": 0
                },
                {
                    "sent": "Not just a distribution, so so I'm wondering if we can use sort of kernel stuff to deal with this stochastic process.",
                    "label": 0
                },
                {
                    "sent": "So in this in this talk we're going to sort of focus on basic.",
                    "label": 0
                },
                {
                    "sent": "I think it's the simplest problem in Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Sort of clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So if you're given the data and you want to do some clustering and you ask so usually people assume that the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He did.",
                    "label": 0
                },
                {
                    "sent": "The model can be written as a sort of a mixture of a simple distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a data.",
                    "label": 0
                },
                {
                    "sent": "You assume that the data are drawn from some mixture model.",
                    "label": 0
                },
                {
                    "sent": "For example, a mixture of Gaussians, something like that, and the key question in this in this problem is that I mean what is the right number of cluster.",
                    "label": 1
                },
                {
                    "sent": "So this is so this problem is solved can be solved using this Bayesian nonparametric model where you can specify the prior for this number.",
                    "label": 0
                },
                {
                    "sent": "Cluster so you don't need to specify the right number of cluster, but I mean you learn from.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Radiator.",
                    "label": 0
                },
                {
                    "sent": "OK, and in order to to go into the detail, I want to give you for the definition of division that process.",
                    "label": 0
                },
                {
                    "sent": "So basically they do.",
                    "label": 0
                },
                {
                    "sent": "It processes the distribution of the random probability measure over a measurable space such that for any finite partition of the your space.",
                    "label": 1
                },
                {
                    "sent": "So you have if you consider the.",
                    "label": 0
                },
                {
                    "sent": "The choice distribution of that partition you have additional distribution then for for any partition of your input space.",
                    "label": 0
                },
                {
                    "sent": "Do you aside this distribution?",
                    "label": 0
                },
                {
                    "sent": "D draw from this ritual process.",
                    "label": 0
                },
                {
                    "sent": "Then you get digital distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then from that dishware process you can specify the ritual process.",
                    "label": 0
                },
                {
                    "sent": "Make your model so first you do all the distribution from the repair process with some parameter, GS0 an Alpha where Alpha.",
                    "label": 0
                },
                {
                    "sent": "Here's something like the.",
                    "label": 0
                },
                {
                    "sent": "I mean so one can show that this parameter of correspond to some the expected number of clusters and then you draw some parameter.",
                    "label": 0
                },
                {
                    "sent": "So here you would get some sort of discrete distribution and then you you do all this data I from this discrete distribution and that data is the latent variable that's parameterized.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the observed data point.",
                    "label": 1
                },
                {
                    "sent": "For example this data I can can be like the parameter of the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I mean from from that part of you is not really clear what is the the division process and how we can prioritize is using kernel step.",
                    "label": 0
                },
                {
                    "sent": "So in 1994 there is a paper that showed that you can also construct this distribution from the research process by consider what we call this stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "So basically you have a unit length stick and then.",
                    "label": 0
                },
                {
                    "sent": "$30 random variable from the data beta distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we specify the point in which you at which you break the stick, and then you repeat this process and then you get this.",
                    "label": 0
                },
                {
                    "sent": "Pie I wish is mixing proportion of your mixture model and it can be shown that the stick breaking construction give the same probability measure.",
                    "label": 1
                },
                {
                    "sent": "Overall random measure on the measurable space with Richard Process with the same parameter Alpha and this did not.",
                    "label": 1
                },
                {
                    "sent": "OK from this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Construction then we proposed this Huber space embedding for digital process.",
                    "label": 1
                },
                {
                    "sent": "Make your model so.",
                    "label": 0
                },
                {
                    "sent": "So let's say this is the space or like Richard Process, make sure that you draw from this division process.",
                    "label": 0
                },
                {
                    "sent": "Then you just embed it into the Huber space by this formulation that you saw earlier in this talk this morning.",
                    "label": 0
                },
                {
                    "sent": "So it's just the expectation of the kernel under the distribution that you draw from from these reprocessed mixture.",
                    "label": 0
                },
                {
                    "sent": "So backs basically just the infinite infinite sum of the the.",
                    "label": 0
                },
                {
                    "sent": "I I, which correspond to this where each component is the density function that you specify in your model.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "And we assume that this is a density function so that you have a verify distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and one problem with this formulation is that I mean you have an infinite sum, so you can not sort of work with it directly, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But Fortunately in 2001, there's a nice observation that if you sort of.",
                    "label": 1
                },
                {
                    "sent": "Truncate the this stick breaking construction at sufficiently large truncation level.",
                    "label": 1
                },
                {
                    "sent": "Then you have sort of an excellent approximation to the full model.",
                    "label": 1
                },
                {
                    "sent": "So then we so, inspired by this observation, then we propose this truncated digitally process major embedding.",
                    "label": 0
                },
                {
                    "sent": "So instead of considering the infinite sum, you just truncate the summer.",
                    "label": 0
                },
                {
                    "sent": "Some truncation level T, then the next question is how much information we lose by using this content creation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can show that the.",
                    "label": 0
                },
                {
                    "sent": "OK, the the different in the acacias norm of this two embedding.",
                    "label": 0
                },
                {
                    "sent": "So this is the embedding of the full model and this is the embedding of the truncated version so you can show that this the information loss can be bounded by some exponential function that depends on this truncation level.",
                    "label": 0
                },
                {
                    "sent": "So basically you say that the error go sort of reduced exponentially fast.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Using this result, we say that OK if we use this truncated version of the embedding, then the error will be small.",
                    "label": 0
                },
                {
                    "sent": "And then we can use this as a circuit 222 DPM model.",
                    "label": 1
                },
                {
                    "sent": "And also the bio suggested how to choose this term casual level.",
                    "label": 1
                },
                {
                    "sent": "So that you have the error to be small, smaller than some specified small values.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then because then, given a truncation version of the of the model, how do we learn this?",
                    "label": 1
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So we consider this similar problem that has been proposed by leasing in 2000 X.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want is that given a truncated DP, ME and some observation.",
                    "label": 1
                },
                {
                    "sent": "So we want to minimize this different.",
                    "label": 0
                },
                {
                    "sent": "So this is the basically empirical mean embedding of the data, so it gives you the access to this or the distribution of the data.",
                    "label": 1
                },
                {
                    "sent": "And this is the embedding of the division process mixture model and then we can show that this is basically.",
                    "label": 0
                },
                {
                    "sent": "If you, I mean, if you consider the parameter Pi, then this is Pis can be is the just the solution of this quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "Where is is the metric that depends on the inner product between the embedding of the mixture component and this is the vector that depends on the inner product between the empirical embedding and the embedding of the mixture component.",
                    "label": 0
                },
                {
                    "sent": "But still you have to also learn this data parameter.",
                    "label": 0
                },
                {
                    "sent": "So basically the opposite.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization problems go like this.",
                    "label": 0
                },
                {
                    "sent": "So first you specify the parameter Alpha and the other is the error that you want to achieve and then you estimate the truncation level and then you optimize.",
                    "label": 0
                },
                {
                    "sent": "These two 2 step until convergence.",
                    "label": 1
                },
                {
                    "sent": "So you learn this pie using the quadratic programming on also optimized parameter data using constraint optimization and then after you get the the model then you can do clustering according to that model.",
                    "label": 1
                },
                {
                    "sent": "Key observation here is that you don't need any sort of doing the optimization.",
                    "label": 0
                },
                {
                    "sent": "You don't need to have the access to value up latent variable of the data.",
                    "label": 0
                },
                {
                    "sent": "As opposed to like give sampling and variational inference.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let look somebody.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you have.",
                    "label": 0
                },
                {
                    "sent": "So the true number of component is 4 and then we choose.",
                    "label": 0
                },
                {
                    "sent": "Truncation level to be 6.",
                    "label": 0
                },
                {
                    "sent": "And the cause is a bit slow.",
                    "label": 0
                },
                {
                    "sent": "But I think she should work.",
                    "label": 0
                },
                {
                    "sent": "So basically, the point of this is that I mean I mean in order so so in K mean clustering problem you have to specify the right number of cluster, right?",
                    "label": 0
                },
                {
                    "sent": "An in order to do clustering.",
                    "label": 0
                },
                {
                    "sent": "But in this case you can see that I mean.",
                    "label": 0
                },
                {
                    "sent": "Here we specify the bounded number of cluster and then we do optimization.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "You don't need to specify the right number of cluster, and you can see that some of the mixing coefficient is.",
                    "label": 0
                },
                {
                    "sent": "Here's go to zero and you can still have the right number of cluster by incorporating this into the.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion I talked about this conjunction between base in nonparametric and kernel methods.",
                    "label": 1
                },
                {
                    "sent": "So specifically I propose this to both read embedding for different way process mixture model and there's so many often question.",
                    "label": 0
                },
                {
                    "sent": "For example how to avoid this truncation and also the solution of this I mean optimization problem related to the maximum likelihood or the map solution of the standard approach.",
                    "label": 1
                },
                {
                    "sent": "Ann and I also did.",
                    "label": 1
                },
                {
                    "sent": "The important problem is that how to choose the kernel K because this solution is also very sensitive to this type of kernel.",
                    "label": 0
                },
                {
                    "sent": "And last but not least, I think it would be interesting to look into these kernel method and also random measure thing.",
                    "label": 0
                },
                {
                    "sent": "So it's so in the previous talk you see this kernel random measure.",
                    "label": 0
                },
                {
                    "sent": "But in this sense is a bit different because I mean for example.",
                    "label": 0
                },
                {
                    "sent": "Instead of considering random variable and now you can have random measure and for example you can do like 2 sample tests for random measure which has a lot of application in academic changepoint detection.",
                    "label": 0
                },
                {
                    "sent": "Something like that and OK. Lastly I want to thankfully panic Anne Francis Path for like food, food, conduct, discussion.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}