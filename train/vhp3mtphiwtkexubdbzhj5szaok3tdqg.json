{
    "id": "vhp3mtphiwtkexubdbzhj5szaok3tdqg",
    "title": "Speech Recognition and Deep Learning",
    "info": {
        "author": [
            "Adam Coates, Baidu, Inc."
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_coates_speech_recognition/",
    "segmentation": [
        [
            "I'm saying, as you mentioned, I actually spent most of my time in school working on computer vision problems, which actually lend themselves to much better demos.",
            "It turns out so one of the things about speech recognition, though a lot of the work that has come out of Baidu on speech recognition, is actually the work of the team.",
            "And so I sat down to work on this talk.",
            "Like Russell saying earlier, I spent a lot of time thinking about, like what's the most useful thing to tell you guys about?",
            "And so much of speech recognition as I think it will be done in deep learning is about just training very, very powerful neural networks to solve the problem end to end.",
            "I really felt like a lot of the stuff that makes that tick.",
            "You guys are seeing in all of the lectures preceding this one.",
            "So what I wanted to do is actually spend some time telling you about speech recognition as it is widely practiced today.",
            "The sort of dominant form of speech system.",
            "Because if you dive into the literature, this is the kind of thing that you're going to see, so I'll tell you a little bit about that.",
            "So just in."
        ],
        [
            "Case you needed any motivation in case you felt that speech was not exciting enough.",
            "I think speech recognition is kind of one of these things, like computer vision.",
            "It's really like a fundamental AI goal.",
            "In my mind.",
            "You know, as a practical matter, there are lots and lots of applications, so you can imagine trying to do things like video or voice transcription.",
            "Things that are very easy for humans and practice to do very accurately or maybe even building very natural interfaces to services and devices.",
            "Make it as easy.",
            "To talk to the devices that you interact with as it is to talk to people next to you.",
            "So this would really help because I hate typing on my phone, I was trying to search for a very long paper name on my phone before I got on the airplane.",
            "I was desperate to to have a mobile connection that could do this.",
            "So even though speech recognition in the past I think is always been a high goal.",
            "It's one of these things is deceptively easy for people to do, but has historically been incredibly hard for machines to do.",
            "Think this is sort of the consonant AI problem, something that humans do with very little effort But machines have really just not been able to do well, and as one of my little like stock photo examples over here, you can see this hasn't stopped us from trying to deploy these things anyway.",
            "So I have no idea what this person is actually saying, but I'm going to guess that it's not.",
            "It's not this."
        ],
        [
            "So the high level goal right is that we want to take some speech audio which is usually coming to us is just a raw waveform so.",
            "Someone's going to talk to their phone, talk to my computer and I'm just going to record the raw audio waves.",
            "So usually it's like a sequence of say 8 bit or 16 bit numbers.",
            "But you can just think of this as a big array of floating point values.",
            "It's like a 1 dimensional vector.",
            "And then we're going to have some system in here.",
            "That's a speech recognizer an our end goal is just to spit out a transcription to figure out that this is actually, you know me, sitting in my living room, saying Hello World.",
            "So."
        ],
        [
            "The reason that this is really hard is because solving this problem, figuring out what's being said in this audio wave is confounded by a crazy number of factors, just like in computer vision, we always talking about occlusion and shadows and shading and all the optical effects and things that really confuse computers.",
            "Speech has its own giant stack of problems that it's gotta deal with.",
            "So for example, there's a big range of types of speech.",
            "People speak differently in different contexts, so when I'm talking to you right now, I'm speaking kind of spontaneously sort of thinking of what I want to say, but then just telling you and I have a very different intonation from if I'm just reading something to you on paper.",
            "So if I sit down and do something more dictation ollh, then that speech sounds very different.",
            "And it turns out that, like Red speech, is actually far easier than something like conversational speech, especially in conversational speech, you get huge variations in tempo in volume, you'll hear people's voices kind of trail off.",
            "At times, you have natural speaker variation.",
            "You have pronunciation and accent differences.",
            "So even though 10 different people will say the same word, they all sound completely different in the audio waves look completely different.",
            "You know things like disfluency, which is almost like a problem in translation.",
            "You sort of don't know how to handle things, and maybe you want to transcribe them.",
            "Maybe you don't.",
            "If someone repeats a word a couple of times like they're stuttering, should you transcribe that?",
            "Or should you try to skip over it when people say halves of words, what do you do and how do you recognize that this is all really, really hard to deal with, and that doesn't even account for the very basic environmental problems like noise.",
            "Something really cool that I only learned about since joining by Jews called the Lombard effect.",
            "So it turns out if you're standing in a noisy room, I always notice that now this now when I'm standing in a noisy room and someone calls to you and you want to talk to them, actually kind of raise your voice like hey in the back and this is an almost involuntary kind of reflexive change that you make to sort of break out of that noise channel so that people can understand you.",
            "But if you do this to a speech engine, it is very confused.",
            "It's never heard someone raising their voice like that before an editor actually breakdown for this sort of trivial thing that you and I handle without thinking about it.",
            "And then another one that is also pretty interesting from a practical standpoint is that we often want speech engines to deal with a very large vocabulary.",
            "In fact, one of the biggest problems is that the vocabulary needs to be superhuman.",
            "That for me to understand, sort of my friends who who use very similar vocabulary and slang and technical jargon.",
            "To me it's really easy.",
            "Maybe I use 50,000 words or something at best, but if I'm trying to build a system that lots of people are going to use that I can just deploy out there.",
            "I might need to know like millions of words because I just don't know where you're from and what you're going to say necessarily, and so I actually have to be superhuman in terms of my my intellect and ability to guess what you've just said.",
            "So these are really huge problems.",
            "I think this is part of the reason that this is a really, really great place for deep learning.",
            "Great place where deep learning can make a big difference.",
            "The deep learning is very good at dealing with these sorts of intuitive, noisy problems when we have a lot of data where in the past we've been trying to kind of engineer around this stuff.",
            "So."
        ],
        [
            "For the rest of the time here, I'm actually going to talk about speech recognition in two pieces.",
            "So as I mentioned, kind of at the outset I'm going to spend quite a good chunk talking about traditional speech models, and that's not because I think that you should all go home and try to build a speech engine based on this traditional approach.",
            "Like, please, please do not.",
            "You will be much happier, so there are great tools out there like kaldi which is an open source engine that can kind of get you started.",
            "But the real reason I want to talk about is that this is still sort of the dominant architecture behind state of the art systems, and it's just commonly assumed throughout the literature.",
            "So if you pick pick up even a deep learning research paper today on speech recognition, you'll hear people talking about things like triphone States and acoustic models and stuff like that, and it's because they're plugging deep learning into this existing system that has a great deal of wisdom.",
            "And expert knowledge built in so you can kind of think of this as being like the deep Learning Survival School for speech recognition as it's practiced today.",
            "And then with the last bit of the time, I'll talk about deep learning for speech recognition.",
            "What are the things you can do with deep learning in the traditional model to make it work much better and also talk to you about something called CTC, which is actually one of the very cool pieces of technology we have in the deep learning world that enables us to do things more like end to end learning and throw away a lot of those traditional components."
        ],
        [
            "So let's get started talking about traditional speech MoD."
        ],
        [
            "Here so.",
            "The basic pipeline that I'm going to tell you about represents a pretty wide range of common practice, but the speech literature is so large that you're going to see a ton of variations on this when you go looking for them in the wild, so I'm going to gloss over some of the algorithm IK details of these pieces, but try to give you a picture of water.",
            "All the moving parts and how these things are put together.",
            "So just as a caveat, this may have a short shelf life, so if deep learning researchers do our jobs, we might well be able to jettison a lot of this over the next couple of years, but for the moment, if you want to understand what's out there now and a lot of the research that's happening today, some of this is still quite relevant.",
            "So."
        ],
        [
            "Our basic goal, like I said, is to start out with an audio wave and generate this transcription through some kind of speech recognition engine.",
            "And I'll tell you what the pieces are in a second and just as a notational thing, I'm going to kind of use capital letters here to talk about a sequence like a vector of a whole bunch of samples, and then I'll use lowercase letters to represent the sort of internal parts of that array.",
            "So here for an audio wave, as I said, it's like a sequence of a 16 bit numbers or floating point numbers.",
            "X1 would be like the first sample and X2 would be the next sample.",
            "And if I were to plot all these I would just see some audio wave so it's just a notational thing.",
            "So.",
            "Similarly, once we get to the end, our goal formally is that we want to find a sentence or a word.",
            "I'm going to call Capital W here, which is just a sequence of little word tokens.",
            "That is the most likely word given given the input is straightforward."
        ],
        [
            "So.",
            "To actually solve this the way that most people proceed is to break this into a few different problems.",
            "I actually have several components that we're going to pull together to give the final solution, so I'm going to start out with my audio wave here.",
            "And then I'm going to compute some sort of feature representations common in machine learning.",
            "We are really unhappy working with raw data, usually like 16 bit numbers, and so on, and so we're going to compute some feature representation that's more convenient for doing recognition.",
            "We're also going to use something called an acoustic model, so the idea behind the acoustic model and this is where deep learning turns out to play a big role.",
            "Is that what I want to model is the probability of the observations or the probability of my features given a word.",
            "So if I tell you the word in advance, this is a model that tells you roughly how it might sound.",
            "All the different variations of how it might sound, so it's a very complicated distribution, right?",
            "'cause it has all the speaker variation and so on baked into it.",
            "But, but that's what the acoustic model is.",
            "And then we're going to have a language model, which is just the prior probability over all of the sentences that I could say.",
            "So you could train this off of the web, for example, just look at what people are saying out there and be able to say for any given sentence, how likely is it that this is a reasonable English.",
            "And then the component there really hard piece actually that pulls all of this together is often called the decoder, and the decoder is going to try to take this acoustic model and the language model and the feature representation and solve this maximization problem.",
            "So our goal is to find the word that is the most likely or the most likely sequence of words given the input.",
            "And that means we need to maximize this expression, which you can breakdown using Bayes rule into the product between this acoustic model and the language model.",
            "So many questions so far.",
            "OK, cool."
        ],
        [
            "So.",
            "There's a bit of a wrinkle in this and that.",
            "Representing words as strings of letters is really complicated, because letters we know just they don't necessarily correlate with how we pronounce things.",
            "So the common transformation that gets made is to turn a word like the word hello into phonemes.",
            "So phonemes are sort of an invention to represent the sort of smallest distinct units of sound you can think of.",
            "These as like the building blocks or the atoms of speech.",
            "So for something like hello.",
            "I have a noise and I have an uh oh sound.",
            "And if you string them together and you kind of slow them then it makes the sound hello and I think of each of these is like a symbol.",
            "So when I go to try to recognize a word what I'm really trying to do is recognize this sequence of phonemes.",
            "Now, yeah.",
            "So there are lots of choices for our how to represent phonemes, right?",
            "These in some sense in engineered representation.",
            "And actually this is very approximate.",
            "So what is exactly a phoneme is not entirely clear, and so there are lots of different choices for how to do this, so I'm using one here that, for example, is taken from the TIMIT data set.",
            "That's kind of standard amongst researchers, but for example, if you go looking at like a dictionary.",
            "They they will have a different kind of pronunciation key, and you'll often see speech papers using like the IPA alphabet to to represent the phones or the sounds.",
            "Yeah.",
            "Right so previously I was using this notation W, right?",
            "It's like a sequence of words, and if I knew that everyone pronounced the words exactly the same way, then I could just kind of translate back and forth and say that for a given word it's pronounced exactly like this.",
            "It's a sequence of phonemes as opposed to a sequence of characters and vice versa.",
            "Yes, so so this is hard right to do the mapping back and forth is nontrivial, so this is exactly one of the little things I'll wind up glossing over a bit, but but the mechanics here will handle that situation if you really unroll it.",
            "Cool, were there any more questions about that?",
            "OK, cool so so this is just sort of a trick to kind of get out of the character world and into a world that's a little more related to sound.",
            "And if you go online and use something like the TIMIT data set will sort of tell you what the phones are.",
            "The phonemes are that correspond to the sounds in the data set, and they'll actually even give you a small labeled corpus to say, here's a here's someone saying ha and hear someone saying and so on.",
            "So."
        ],
        [
            "The traditional systems try to model the sequences as phonemes instead of words, and this means of course as we were just talking about that.",
            "We need some way to go back and forth between words and phonemes, right?",
            "So in practice what what a lot of systems have is a pronunciation model.",
            "This is another statistical model that says if you tell me the word W, what are the distribution of all the different sequences of phonemes that correspond to that word.",
            "And it could be ambiguous.",
            "For example, it could be that for saying the word hello, I have a bunch of pronunciations and maybe there's something a word like Halo that can sound kind of similarly, and it would be the job of inference in this model to kind of disambiguate using, say, the language model.",
            "So what I've done here, basic?"
        ],
        [
            "He is just replaced W in a couple of spots with Q, which is the sequence of phonemes and for simplicity I'm actually only going to consider the case where we have this one to one mapping.",
            "I'm going to sort of drop this distribution here and say, you know we have some dictionary Q, so if you give me a W I'm just going to spit out a sequence of phonemes.",
            "I say everyone pronounces it this way and similarly with the acoustic model I'm now going to say for every sequence of phonemes.",
            "There's some distribution over the way that people might sort of say that that it might sound in practice, and then when I go back to my language model here, I'm going to have to affect some sort of mapping to make sure that I can look at a sequence of phonemes and guess how likely is it that that that is a legitimate sequence of words.",
            "So again, if you have ambiguous pronunciations, this doesn't work very well, but we're just going to simplify it that way.",
            "So then when we go to solve this problem right, we just want to find the most likely word and so that basically means now taking our word, searching for a word, an it for every choice that I might have, I'm going to convert it to phonemes first so I can plug it into my little acoustic model and then solve the same problem."
        ],
        [
            "So.",
            "The first stage in all this is feature representations only going to spend a Slider 2 on this, but I'll say that definitely in traditional systems, the feature representation can make a big difference, and so there are lots and lots of different choices in the literature so."
        ],
        [
            "Very common, one is like spectrograms.",
            "This might be like the first thing you would try in a sense.",
            "Sort of 1st principles.",
            "How do I convert a raw audio signal into something a little more compact and informative?",
            "There's something called MFC C, which is Mel frequency Kestrel coefficients.",
            "I'm not going to go into how these things are computed because it's a little tricky, but I'll tell you that there are libraries out there that will do it for you.",
            "Writing it yourself is hard, and it's an incredibly successful feature in audio processing, so if you're doing sort of classic machine learning, you're playing around with like recognizing music or something like that.",
            "You'll see MCCS alot.",
            "And then, as with all machine learning applications, there's a whole bunch of literature on other crazy kinds of features that turn out to be useful in different contexts.",
            "For example, you can also generate new features like, take all of your, say spectrogram bins, and then take the differences over little time slices, and now add the differences in his features as a way of kind of getting some sort of temporal information in there.",
            "So."
        ],
        [
            "I gotta use the spectrogram as an example.",
            "So in all this stuff I'm talking about here, we're just using a spectrogram as a Canonical instance of water feature representation is.",
            "So so here's me again, saying Hello World and the way you computer spectrogram is, you take a little window.",
            "It's about say 10 milliseconds or 20 milliseconds wide and I clip out this little waveform.",
            "Alright, so so here's actually what it looks like when you zoom in, so that's about 20 milliseconds of audio.",
            "And then what we're going to do is we're going to run a fast Fourier transform.",
            "That's going to give us basically the frequency domain representation.",
            "We're going to sort of break this wave down into a composition of sine waves, and then we're going to take the squared modulus of this, and it's basically like asking how much power is in each of the frequencies, and I'm going to ignore the phase information.",
            "I don't care if the sine waves that make this thing up or shifted a little bit.",
            "So this basically describes like the frequency or say, pitch content that's in one of these little windows.",
            "So if you want to sort of visualize what this means, you can take the FFT and plot it on two axes here.",
            "So this is the frequency axis, so this would be like low frequencies over here.",
            "Like really low pitch, and these are high frequencies over frequencies over here that are high pitch and then the height or color, same same representation is how much power there is.",
            "So this is sort of a low sound.",
            "With with less high frequency information and then in all the future slides you'll see me just referring to this with colors, so these are the low frequencies.",
            "These high frequencies and this is basically a vector that we're going to call a frame so.",
            "Basically, once we know how to compute the this for like a small window, we can just compute these frames, right?",
            "So I've decided that the representation of this little 20 millisecond slice is going to be this vector of power.",
            "Basically, this vector of frequencies, and so if I want to represent this entire utterance as a spectrogram, then I'm just going to concatenate all those vectors together into this Big 2 dimensional array.",
            "So that makes sense.",
            "Yeah.",
            "No, not really.",
            "This is in some sense more informative.",
            "So if you go back and look at kind of a lot of the intuitions behind, like why people use MFC sees.",
            "For example, if you look.",
            "So this is me.",
            "Saying Hello World, if you look at the bands here, you can actually make out patterns that correspond sensibly well to sounds.",
            "It's very noisy.",
            "It's not exact, but you can actually see things there, whereas if you're just looking at the temporal domain, it's really hard.",
            "And historically, it's just been very hard to find patterns in that, yeah.",
            "Yes so.",
            "Yes, and in the deep learning world we are spoiled by this.",
            "So in the past when you haven't had a lot of data, you've had much less powerful models.",
            "You've had to sort of engineers some knowledge into the system to be able to get the machine learning algorithms to work well.",
            "You've got to do a little bit of extra work, but if you have a very powerful acoustic model, let's say then, then really you don't care.",
            "You could try to model it directly from the audio wave, and there's actually recent work where people are doing that.",
            "Yep.",
            "Yes.",
            "Yeah, so so another justification for why to do this is maybe not the engineering aspect.",
            "Even in fact one of the motivations for MFC sees one of the transformations that's done there is to set these frequency bins on a scale that kind of mimics perceptual behaviors of humans.",
            "So if you go and test people and check what frequencies they can distinguish between, you can figure out how to bend the frequencies so that you kind of get the best representation for speech.",
            "So people are using exactly this kind of intuition about how humans hear things in the hardware.",
            "We have to inform how to build these features and tune them.",
            "So another way we're kind of baking our knowledge into the system.",
            "Yeah.",
            "Yeah Yep alot.",
            "This window, oh, so they can be disjoint or overlapping.",
            "You can pick.",
            "So in practice there's like a little windowing function, almost like a little Gaussian.",
            "So if you did it sort of the perfect way instead of actually cropping the window, your support is the whole signal 'cause the Gaussian never really goes to 0, so they're always overlapping.",
            "But in practice, when we compute these things, you're running FPS on little windows."
        ],
        [
            "Cool, so let's go back to modeling."
        ],
        [
            "So what we need to do is come up with a model of the observations of these features given this sequence of phonemes and what we'd like to create is some generative model of these features.",
            "And so to kind of get at this, I'm going to start with a similar case.",
            "Let's just assume that our whole job in life is to model observations of 1 phony.",
            "So the way that we're going to do this is to imagine that someone is saying how which actually covers, say, more than 20 milliseconds, and so that's represented in audio by several frames of speech.",
            "Several of these frequency vectors.",
            "And so we're going to model this sequence of observations that are generated while you're speaking using a hidden Markov models, like a standard off the shelf sequence modeling tool."
        ],
        [
            "So.",
            "Let me see here how many people are used to thinking of HMMS is like a directed graph with hidden variables and observations hanging off of them.",
            "OK, and how many people are more familiar with HMMS?",
            "Thinking of them is like state machines.",
            "Select few people.",
            "OK, this seems kind of normal.",
            "I think we've changed in the machine learning world.",
            "We've changed how we teach Hmm's so so kind of walk through what this means so.",
            "What we can think of you can think of and.",
            "Hmm is being like a little machine, so here's the generative process that in HMM is running to create those observation vectors.",
            "So I'm going to start out here in State 0, right?",
            "That's where I'm going to begin, and then I imagine that there's a little machine that's going to come through here and at every time step it's going to hop to some new state, let's say state J from state state I with the probability determined by some parameter that I'm going to call AJ.",
            "So we can represent this graphically by saying I'm in State Zero.",
            "I can hop over this edge with this probability, which is going to be 1 here 'cause I don't have any other choices or if I'm in state one, I can either sort of self loop and come back to state one with probability A1 one.",
            "Or I can jump to state two with probability A12 and I'm going to keep running through this.",
            "And after each jump I'm going to generate one of these frames.",
            "One of these vectors of all the frequencies based on whichever state I'm in, and so as a first pass of this is a very simple model.",
            "You can imagine that this observation how I generate my features given the state is just some Gaussian.",
            "So for each of these I'm going to have a little Gaussian that generates my features."
        ],
        [
            "So you see how this works going to start in State Zero, I'm going to."
        ],
        [
            "To state one generating observation, maybe I decide well, I randomly decided that I'm going to self."
        ],
        [
            "Repair I get another observation."
        ],
        [
            "Maybe jump to State 2."
        ],
        [
            "Jump to state three and generate another observation is another loop."
        ],
        [
            "And then."
        ],
        [
            "And at random I might get to the end of this state machine that says I'm done with my utterance.",
            "So now I have all of the frames that represent whichever version of the phoneme I just sampled.",
            "So this is like the mechanical process of how the HMM is going to generate these observations."
        ],
        [
            "So.",
            "The challenge here now is to do inference.",
            "So what we really want is not to be able to generate more utterances.",
            "What we want to be able to do is recognize the ones we've already got.",
            "So.",
            "So we have been hmm, and let's just suppose for a minute that I've already trained up my parameters, so I know the transition probabilities between all these States and I have the say the Gaussian meaning covariance parameters that define how my observations are generated.",
            "So if you give me the observation sequence, if you give me all these vectors, like the big speech spectrogram.",
            "Then I can find the most likely sequence of internal states.",
            "I can find the path that actually got followed through here to generate these using the Viterbi algorithm.",
            "So if you just go back to your your AI textbook or your statistical models textbook Viterbi is a simple dynamic programming programming algorithm that will find the most likely path that generated these observations.",
            "So in this case, maybe it's decides you know what the most likely path is to be in State Zero, then one, then one, then two, then three, then three, then three, and then and then four.",
            "And the intuition behind what Viterbi is doing for you is that it's actually deciding how to link up these observations with the various states it's performing, like an alignment of figuring out which state generated which observations with the baked in constraint.",
            "That is kind of gotta go left to right.",
            "I can't have this guy generate something over here after.",
            "State Two has generated something in the middle, so we've kind of got some knowledge about how audio gets generated, baked in, and Viterbi is helping us go backward to figure out what is the sequence of states that could have generated the audio we see."
        ],
        [
            "So that's how we can take a given observation and find one state sequence that could have generated it.",
            "But a more important thing to be able to do actually, especially for learning, is that.",
            "We'd like to be able to compute the likelihood of our observations, 'cause if I go to optimize this model right, I want to maximize the log likelihood.",
            "I've got to be able to compute it first, and so the way we compute this is by summing over every possible sequence of states that we could possibly make inside that little machine.",
            "That would give us a plausable replication of our observations, and so this is just marginalizing I'm multiplying my.",
            "Observation model here by the probability of a particular state sequence which is conditioned on the fact that I'm talking about a single phoneme.",
            "So this is just marginalizing out all those hidden states to give us the probability of this observation for this specific phony.",
            "So the good news is that we have efficient algorithms for this.",
            "So Iturbi is sort of your off the shelf algorithm to find the best sequence.",
            "And then there's the forward backward algorithm that just kind of your off the shelf system for doing these marginalization's in computing this some very efficiently.",
            "So the good news is this is the acoustic model that we wanted, right?",
            "We started out saying we wanted to know what's the probability of the observations given a sequence of phonemes.",
            "So this is the guy we're really interested in."
        ],
        [
            "So that's how we Model 1 phoneme and now we want to model whole word, let's say.",
            "So.",
            "Here's a simple example of a word that has two phonemes.",
            "So I'm going to say my sequence of phonemes is Q1 and Q2 and the nice thing about having this little stamped hmm to represent the phonemes is that I can take the model that I use for Q1.",
            "That's the state machine that generates the sound for Q1, and I can just wire it into the end of another model.",
            "That generates the sound for Q 2.",
            "So now if I want to generate observations.",
            "For what this little word would sound like, I can start out in State Zero and I can hop around to this model and at some point the HMM will hop to the next phoneme and it'll generate some more observations until it gets to the end and now we'll have a whole word.",
            "So mechanically, nothing has really changed here.",
            "You can use all the same algorithms and so on.",
            "This is just a way of kind of stretching out a little model to make a bigger one for words that make sense."
        ],
        [
            "So likewise, if you want to build a whole sentence right, if I give you a sentence in advance, you can wire up all of these HMMS to build a single single model that is basically the generative model for that one sentence.",
            "So the good news is that we can for any given sentence, define a hidden Markov model that is the generative model for just that sentence.",
            "So the way that we do training is pretty straightforward in the sense that we have the sentence that we want to.",
            "We want to maximize the likelihood, and then we know how to build the model, and so if we have this fixed HMM structure, observations that were generated by the HMM we have off the shelf algorithms.",
            "Again, that that can solve this training process for us.",
            "So I'm not going to dig into TM in the context of Hmm's, it's called Baum Welch.",
            "But but just as a refresher EMH involves two key steps, so the eastep is inference.",
            "So we talked about the Viterbi algorithm, which is kind of a simplified version.",
            "But basically if I tell you the observations, I want to know what's the distribution over all of this state sequences that could generate it?",
            "And you would compute this, usually with the forward backward algorithm.",
            "And then there's the M step, which says go through all my parameters for my hmm, those aij's, the Gaussians and so on and try to update them to maximize the likelihood of my observations.",
            "And I'm just going to keep iterating these guys to get the best model."
        ],
        [
            "Low.",
            "This is difficult to do in practice, and the very abstract reason is that EM is not guaranteed you to get guaranteed to get you the global minimum.",
            "This is a sort of complicated process, and so there are a bunch of things that can go wrong, but some intuition for what makes this tough.",
            "You can see by simplifying EM.",
            "So instead of thinking of the step is giving me the distribution over all of the phony sequences that could have generated my observations, let's just replace that with Viterbi again, let's just assume that I'm going to take the most likely sequence to generate my phonemes, and then I'm just going to say all of the probability in the posterior goes on that one hypothesis, and I'm going to ignore all all of the other explanations.",
            "Then what you can see is that if I do that herbion, I generate this sequence of states that I think corresponds to my observations.",
            "If my model isn't trained yet.",
            "If I'm just starting out, this could go very badly, and instead of getting this nice alignment that corresponds to how this thing was generated to begin with, I could end up, say, putting all of these observations in the wrong state.",
            "I could sort of misinterpret the audio, and this is very painful.",
            "Because it means that if we get the wrong alignment, then when we go in and we update all the observation models were sort of updating them with bad data.",
            "We're updating them with a bad alignment and this whole thing will go South.",
            "So this in practice makes training these systems really hard, and there's a lot of wisdom that goes into kind of fiddling with your pipeline to make this work."
        ],
        [
            "And so if you think about what are the big problems with traditional systems and where a lot of the work is going, it goes into sort of bootstrapping these things to get them to all play nicely together.",
            "And so one really common trick, for instance, is go download timid or go download like a phoneme corpus that's already been labeled for you and try to train up your observation models in advance so that they'll kind of give you a good alignment from the beginning before you actually start PM.",
            "So if you just implement this training algorithm from scratch, and you throw it in with a random initialization, it's probably not going to work well.",
            "You've got to do a little bit of work upfront to get it to play nicely.",
            "So."
        ],
        [
            "I'm not going to dive in deeply on language modeling, but I want to say a few things that are sort of speech specific about language modeling.",
            "So remember, in addition to the acoustic model right, we want a language model because when we multiply these things together that will give us the likelihood of our of the words given the observations.",
            "And so we need this probability of W, the probability of some sequence of words.",
            "So there are a whole bunch of options for doing this.",
            "There's a whole literature of course on just modeling language, but there are a few designer at a for building language models for speech that are kind of important.",
            "One is that we want it to be really fast to query this thing because it's going to turn out in a little bit that you query this language model inside your decoder so your decoder is constantly asking the language model.",
            "How good is this sentence?",
            "How good is this sentence?",
            "How good is this sentence and the language model needs to be able to run very quickly, so if you have like a giant neural network language model.",
            "That needs to gobble up lots of context to give you an answer that this will slow you down.",
            "Another important aspect is that we want to be able to train on really huge corpora, right?",
            "We want to be able to train on billions of words ideally, and this comes from the fact that we're looking for this sort of superhuman vocabulary, so there are just lots of words, technical jargon, and things that you're never going to hear when you listen to people speak.",
            "So if you just use speech data, you're never going to hear people use a deep learning technical terms.",
            "If you're just gathering data off the web, it's very unlikely statistically and so.",
            "You really want to be able to gobble up text, because that's how you're going to get all those rare words that people use.",
            "And finally we want to be able to train them quickly.",
            "This is a real problem for production systems, so if you want to use this to back up your search engine, for example one day people are searching for Madonna and another day people are searching for Taylor Swift and your language model needs to get updated to sort of account for the fact that some things are suddenly much more likely than they used to be.",
            "So, so those are some of the things that we want, and as a result, one of the things you're going to see all the time is N gram models, where we basically have a gigantic table that just counts how many times we see a particular string of words.",
            "So you can think of a bigram model, which is to say a 2 gram model of just being a table where I count up all the times that I see one word, followed by another, and then for a larger NI, count up the number of times I see a particular word preceded by some longer context.",
            "And obviously for really large N there will be lots of sequences.",
            "I just never see, even though they're reasonable, and so you have to play a lot of smoothing tricks and so on to make this work well.",
            "So so there's good literature's behind this.",
            "There's lots of kind of established schemes for doing that."
        ],
        [
            "So I'm going to assume for the most part, that you've got a working language model here.",
            "OK so so now for the big fish.",
            "The really hard part of all of these speech engines is usually the decoder, because what the decoder has to do is solve this maximization problem.",
            "Remember the forward backward algorithm in the Viterbi algorithm could tell us what's the probability of our observations when I fix these.",
            "If I give you the sentence, it's not too bad to compute this number.",
            "It's not that the objective can't be computed, it's that the number of combinations of words is enormous, and so I have to search over this space somehow very efficiently.",
            "So."
        ],
        [
            "The basic problem is to to somehow maximize over all of the sequences we could.",
            "We could possibly choose to make this as likely as possible.",
            "And again we have this wrinkle here, which is that we've gotta sum out all of these.",
            "All of these states."
        ],
        [
            "So there are a whole bunch of different strategies to do this.",
            "If you pick up a speech textbook, you'll probably see two or three different kinds of decoders there with different kinds of search strategies, so I'm not going to dig into a state of the art version because they can be really complicated and hard to get working, so I'm going to try to simplify this a little bit to just give you the idea for what this system does and what its job is."
        ],
        [
            "So so to do that?",
            "I'm going to try to get rid of this little summation, which makes things more complicated and say, well, you know what, we're just going to look for the most likely S right?",
            "The most likely sequence of states which won't give us the absolute maximum.",
            "Here we could wind up sort of missing some cases, but it's actually not a bad approximation.",
            "And the nice part is that if you fix the choice of S, if I told you what I thought the most likely sequence of states was.",
            "I could go back to my picture of the hmm and just map out all of the words so it's really easy to go back to the transcription.",
            "So let's talk about a simplified problem here.",
            "Let's assume that I've got a two word vocabulary, so so all of the work that I'm going to do, I'm going to assume that people are only allowed to ever say 2 words and I've chosen Hyun guy because they both have two syllables, and they happen to share a syllable.",
            "So so I'm only allowed to say hi and guy.",
            "And we're going to have a very simple language model.",
            "These numbers aren't important, but basically your language model says what's the probability of saying guy after I've said hi?",
            "What's the probability of saying high, high and so on?",
            "So every combination and you also have some prior probability.",
            "What's the probability that I start a sentence with high versus probability that I start a sentence with guy?",
            "And then we're going to also have our little HMM acoustic models like like we had earlier.",
            "So for each of these phonemes, there are three of them.",
            "I'll have a little template for how that phoneme is allowed to sound."
        ],
        [
            "OK, so we can actually build the entire HMM for this world basically.",
            "And hmm, that can generate all of the sentences or utterances.",
            "Allegedly that I can generate in this world.",
            "So the first is I'm going to create two buckets, one for the word high and one for the word guy.",
            "And within each of these I'm going to create a bucket for the different phonemes that make it up, and I'm going to fill these in with the little HMM templates that represent each phoneme.",
            "So if you say the word high like we have before, if I'm modeling a word, I can kind of bounce around in these states to generate with the first phoneme sounds like, then I can hop to the next one and then eventually I'm going to leave this state and I have to decide what word I'm going to say next.",
            "And so for this site, power point lines aren't great.",
            "With this, when I leave the box, I can circle around and say hi again, or I can run back here and I can say guy and vice versa.",
            "So when we leave this, we gotta remember that there's basically a state transition that comes from my my little hmm that says how things sound.",
            "And then there's also this language model term that says how likely it is to jump to either of the words.",
            "So this is just an hmm, just like the little one we started with, it's a gigantic state machine and so all of the basic pieces we talked about still work here.",
            "So our goal as I was talking about on the last slide, is to try to find what is the sequence of states here that maximizes the observation.",
            "Basically, what's the transcription of my observations?"
        ],
        [
            "So it's really easy to do this with maturity, right?",
            "We just talked about this, but there are some practical problems with doing this.",
            "The first of all, if your problem is really big, Viterbi is still not going to be fast enough.",
            "It's pretty fast.",
            "It's pretty efficient, but if you have an enormous vocabulary right that diagram on the last page, that gets really big.",
            "If you have 10,000 words and lots of different pronunciations.",
            "And also I I kind of cheated in my example I used a bigram model and the bigram model is nice because it satisfies the Markov assumption, meaning I don't care about all of the previous context.",
            "I only care about the last word, and so this kind of fits into Viterbi nicely.",
            "But unfortunately, if you try to use an ngram model that's got a long context, you are completely violating all the assumptions that make Viterbi work, and so you can't use dynamic programming anymore to get a good solution.",
            "So This is why, in practice, when you look at state of the art decoders, you go looking through the literature how like a real decoder solves this problem.",
            "You're going to see a lot of more general search formalisms.",
            "Things like a star, for example, might be apart of your decoder, so I'm going to actually talk about an example here that's just based on beam search, partly because it's sort of representative of how the more complex decoders work, and also because even in current deep learning practice, if you build even an end to end system.",
            "You'll see beam search like algorithms being used for decoding.",
            "OK."
        ],
        [
            "So the idea behind Beam search is that I'm just going to keep a list of my top an hypothesis, like the top end candidates or state sequences, and at every step I'm going to propose an extension to each of those candidates, and then I'm going to re rank the man, come up with the N best.",
            "So if we had the entire state sequence in advance, so I just gave you the whole transcription, then we could break out the log likelihood.",
            "With this right I just took my product that I've had before and.",
            "Took the log and so now it turns into a sum and so this is now the sum over all of the observations given the states, it's the sum over all of the state transition probabilities that I make.",
            "And then it's the sum over all the word transitions that I end up making in the transcription.",
            "And so the cheat.",
            "The trick that we're going to use to make beam search work is that while we're searching, we're just going to keep a partial sum as we're hunting between States and we're sort of greedily trying to find the best next state.",
            "I'm going to keep a running sum of just the first terms, so if I've made T prime steps right, I've jumped through T prime states.",
            "I'm going to keep track of the log probability of all the observations I've seen so far.",
            "And I'm going to keep track of the log probability of all the states I've hopped through so far and every time I hop between words, I'm going to add on a little log probability for the words that I used.",
            "And the nice thing about using beam search is that this is very generic.",
            "If I want to add something like a word insertion penalty or word insertion bonus, which people often do, I can tack that onto the end for example, so I can actually add other little objective functions in here to try to make things work better.",
            "So."
        ],
        [
            "Basically.",
            "I'm going to walk through a visualization of how very simple beam search might work, so we're going to keep the top two candidates, and so to see how we accumulate this score.",
            "These slides took forever to make, so I hope you enjoy them.",
            "So starting from State 0 right, we don't have anywhere to go.",
            "So if we pick what are the possible successors, there are only two choices, so I'm going to end up over here."
        ],
        [
            "And so if I keep the top two, this is now my my end best list.",
            "This is my top two list.",
            "And so."
        ],
        [
            "When I try to create a new set of candidates when I'm first going to start at each of these States and generate all the places I could possibly end up so.",
            "Oh apologies, so starting from this state after I make the jump I have to add up.",
            "What is the probability of seeing the first observation given that I'm in one of these two states.",
            "So for state one I'm going to accumulate the log probability of this observation given that I'm in state one and I'm going to store that number here with this candidate.",
            "And then I'm going to do the same thing for State number 9 and I'm going to store that probability with this candidate.",
            "I'm going to keep track of them as we go.",
            "So then I generate all the places that I could end up next.",
            "So from this state I could loop."
        ],
        [
            "And stay here.",
            "Or I could jump to state two and for each of these candidates I'm going to add on the probability the log probability of making whichever transition I actually."
        ],
        [
            "So I'm just keeping a running sum of all of those.",
            "I'm going to do the same thing down here.",
            "And then again, I'm going to look at the second observation, and given that I'm in this state or this state, this state, this state, I'm going to add the log probability of that.",
            "So now I've got four different candidates.",
            "Each candidate has its own running sum for how likely the observations are, and also how likely the state transitions are that it's seen.",
            "So now I've got 4 numbers and I can rank them to just take the top two so."
        ],
        [
            "You know the Gus sounding has sound or not very ambiguous, so hopefully my top two are up here.",
            "And so I can."
        ],
        [
            "Kind of keep doing this can add the probability of the transition add in."
        ],
        [
            "The observations again."
        ],
        [
            "Just keep on going and take the top two."
        ],
        [
            "Take the top two."
        ],
        [
            "Up to."
        ],
        [
            "2.",
            "Great, so now that we got to the end."
        ],
        [
            "When I generate the next set of candidates, right?",
            "I can now hop through these big word arcs here so I can go from State 8."
        ],
        [
            "And I can actually end up over here again and say the word high a second time where I could jump around here and I could end up saying guy so I can say hi guy afterward.",
            "And so, in addition to adding on the log probability of whichever of these transitions I'm going to make.",
            "I'm also going to add in the term that corresponds to the language model, so every time I actually see a new word start, I'm going to attack on the probability associated with it, yeah?",
            "Oh these two yeah yeah so.",
            "So I skipped a little step at the very beginning before I was kind of assuming that the words are equally probable so it wouldn't matter.",
            "But normally if you want to know what is the probability of a word, you can use the prior probability.",
            "And in fact what people often do is they'll add a special beginning of sentence token to their language model so that it's not just the prior probability.",
            "I'll actually say what's the probability that my sentence or utterance starts with this specific word.",
            "So good observation, thanks.",
            "So after."
        ],
        [
            "Maybe I keep the top two and I end up with these and I can keep on going and at the end maybe I decide that these are the."
        ],
        [
            "Top two and that this is the best one.",
            "So if I really said hi guy, this kind of the ideal situation that my beam search is figured out that by the time I'm done walking through these transitions I'm in the last state for the word guy.",
            "News about how we run this beam search is that this guy, this little candidate here, has a history of all the transition transitions that it's made and it has the sum that tells me how likely my observations."
        ],
        [
            "Which actually is taking into account the language model taking into account the observations and everything.",
            "So what we can actually do is look at the history."
        ],
        [
            "Three that this state has accumulated, and then I can actually just read off what the transcription is, so I know that we went through this phoneme.",
            "This phoneme, this phoneme.",
            "This phoneme on that corresponds to me saying hi guy.",
            "Any questions about how this works?",
            "No, so yeah, so if you want to, so I assume that we're going to find the most likely state.",
            "That's what we're looking for.",
            "If I wanted to do a much better job, what I should really be doing is summing over all of the different ways that I could possibly get here.",
            "That's what I'd like to do, but that requires running forward, backward, and it makes the decoder a lot more complicated.",
            "But if you go hunting through a textbook or research papers, you will find decoders that try to do this.",
            "Yeah, it's great.",
            "OK.",
            "So you might be."
        ],
        [
            "So is that all?",
            "This is all it takes to build a speech engine and then the short answer is definitely no.",
            "So this is highly simplified so so people have sharply been pointing out places where we could have done better where we left something out to make it really simple.",
            "But this is actually all of the big moving pieces, so we see someone talk about a decoder.",
            "Now they're talking about a search algorithm that's going to hunt through this hmm and look for a good solution when they're talking about an acoustic model.",
            "You've kind of seen how that works, even though we've been using.",
            "A really simple one.",
            "So these are the big pieces.",
            "It's a lot of the vocabulary, but to get it to work well, if you're going to build this from scratch, there's a raft of other things to do to try to make it work.",
            "So one really common one that you'll see in the literature is that instead of using either simple states like I've used that are just kind of black box, part of the HMM, or just using individual phonemes to say each phoneme has its own hmm, they'll actually break that space up into things called triphones.",
            "So it's almost like a trigram in a language model.",
            "I find all of the triplets of phonemes that occur in my data, and I might have a separate HMM for each possible triplet so that I can kind of capture the ways in which these sounds slower together when I put different phonemes with each other.",
            "And then you might figure out that, wow, that's an awful lot of triphones, which means an awful lot of Hmm's and a lot of parameters that are tough to train.",
            "And So what people will do is hunt for Tri phones that are kind of similar anti their parameters together.",
            "So there's lots and lots of tricks to try to make this work well.",
            "Similarly, we haven't talked at all about like normal noise filtering, for example, which is really important if you use something like MFC sees.",
            "There's also just really cool stuff.",
            "This is like my favorite part of speech.",
            "Literature is the really cool things that people do to adapt to like speakers.",
            "So it turns out that one of the things you can try to do is listen to the pitch of someone's voice and guess hyperparameter like the vocal tract length of the person that's talking, and then use this to apply like a nonlinear.",
            "Bend to their pitches to kind of move them into like a generic range so that your speech engine doesn't have to handle such a wide range of people, so there's lots of really neat tricks out there that you actually need to make a bleeding edge system.",
            "I'm not going to go into them 'cause I hope the deep learning will mean we don't need so many of these things anymore.",
            "Alright."
        ],
        [
            "Who's tired of traditional speech models?",
            "Yeah OK, I am alright.",
            "Let's go back to the Bunny."
        ],
        [
            "So we can deep learning helping this, so maybe a few years back if he was talking like 2000 eight 2010 something like that.",
            "If you walked into an existing speech group they would have a very powerful, very well built speech engine based on a lot of concepts I've been talking about.",
            "And more recently, one of this sort of low hanging fruits has been to figure out alright which of these components is sort of the bottleneck.",
            "Which of these things do we really not like?",
            "'cause it's constraining performance and let's just replace them with deep learning, which has much more powerful estimation abilities.",
            "So."
        ],
        [
            "The basic pipeline I started with at the beginning right has all of these different components, and so one of the things that we can do is start poking through these and figure out which ones deep learning might help with.",
            "So for instance, with the acoustic model, maybe we can use deep learning to somehow compute this probability more accurately somehow make a better model there.",
            "With language modeling.",
            "I don't sound like maybe it hasn't been talked about this week, but you could also try to use a neural network to do language modeling.",
            "And the hope is that if you do really well with deep learning, maybe some of these other things can go away, right?",
            "People are asking like why do you want a spectrogram?",
            "Why not just use an audio wave?",
            "We kind of want that because these components aren't very good at dealing with raw audio, and maybe if we use deep learning for some of these, right to have a much more powerful model of our observations, maybe we wouldn't even need this anymore, and in fact there's this current work people actually trying to model directly from audio, so they could just jettison this whole piece.",
            "And finally, this pronunciation dictionary, right that I kind of assumed away 'cause it makes things really complicated.",
            "We really, really wish that we could just figure this out from data, right?",
            "The idea that I'm going to somehow brute force write down all possible pronunciations or engineer a model that covers all the pronunciations.",
            "That's a tough, tough job.",
            "That's a tall order, and so with any luck deep learning, a lot of data, we won't have to touch these, so I'm going to focus on these two things."
        ],
        [
            "So one classic improvement.",
            "In some ways, one that's been around for a while but is also responsible for a few recent big successes in improving speech engine's, is this idea of trying to enhance the acoustic model, making a much better model model of the observations given my sequence of phonemes.",
            "So we're still going to reuse all of the HMM machinery we still have this decoder.",
            "We still have these Markov models, but we're just going to try to replace the probability of the observations given given the states.",
            "So at the very beginning I assume this thing was like Gaussian, like a really really weak model, and now I'm going to try to replace it with something much more powerful.",
            "So the one wrinkle here that you've got to think about is that if I start from my observations.",
            "With deep learning, normally I'm trying to predict something right.",
            "My all my super supervised learning algorithms are made to bottom up.",
            "So maybe maybe Ruskin build us a generative model that goes top down and tells us this probability, but when we'd really like to use discriminative training to somehow make a prediction, that's going to help us 'cause we know how to do that really really well.",
            "So.",
            "We'd like to take in the observations and then and then predict some kind of label here because the technology for that is is, really, we've got down Pat."
        ],
        [
            "So if we knew the target for S, right?",
            "If we knew the state sequence that we wanted, it seems like it should be useful if the neural network could just predict it for us for a new utterance, right?",
            "Like if I just gave you the audio and you had a brilliant neural network that you could just say, oh, it's this state, it's this state it's this state, then you would be done.",
            "You wouldn't need the HMMS at all.",
            "So what we'd really love to have is something that takes in a speech frame, or maybe a small group of speech frames, plows it through a bunch of sigmoid or or rectified linear units in our neural network, and then spits out a softmax vector, a vector of probabilities for how likely are all the different states in my model.",
            "This is what we really like to do."
        ],
        [
            "The problem is we need to get the targets for this.",
            "Right now no one really bothers to furnish us with these states in advance.",
            "So we can do a whole bunch of different things here to try to make it work.",
            "One is to take a standard pipeline that we've already got.",
            "Like if you went to the trouble of building a traditional system and now you're trying to super charge it by plugging in deep learning, you can actually use that to generate this sequence of states.",
            "Go find, use our beam search decoder there to find the most likely sequence of states for this utterance for these observations, oh.",
            "And then once we have the word labels, this is actually much easier.",
            "We can actually just align them.",
            "So if we have if we have labeled data right, we have other words we can generate the sequence of phonemes that I heard, and then I can use one of these algorithms to find what is the most likely sequence of states that corresponds to my input.",
            "So it's just an alignment problem.",
            "And then once I have this state sequence, I can just chop up all of these state and observation pairs and now I have a training set that I can use for supervised learning.",
            "So that's one approach.",
            "Other way to do it which you might have to do if you're building a system from scratch, is to go download a data set where someone has labeled this by hand.",
            "So for instance, if you download TIMIT, someone has very, very mercifully gone through and labeled a lot of phonemes for you based on speech frames, and so you can actually train an observation model with a deep neural network from that and kind of get yourself started.",
            "And then once you've done it, and you've got kind of a sort of weak, deep, deep neural net, you can rerun the alignment, retrain, and so on.",
            "So the last option is to just try to train to predict phonemes directly.",
            "So I've been talking about this hidden State S right, which is kind of the state within the Markov model for a particular phoneme.",
            "But another option, since we happen to have lots of labeled sets of phonemes sitting around, is to just predict the phoneme directly and try to use that as my observations, and so there are different tricks you can play with trying to rephrase the problem to make this work."
        ],
        [
            "So.",
            "The little thing you've got to be aware of though, is that we don't really want the probability of the state given the observation right, but that's not what our acoustic model wants in order to work, we really wants is the probability observation given the state, and that's how we can integrate this nicely into the HMM framework.",
            "So that's OK. Bayes rule tells us what to do if we want the probability of the observations given the state, we can apply Bayes rule.",
            "And then this term right here.",
            "The probability of the observations is just a constant, right?",
            "I know the observation, so this is just a constant and it turns out that if I drop this, it's just harmless.",
            "There will be a constant that shows up in my recognizer and in my beam search, but it doesn't actually affect anything.",
            "And so here's the thing I get from my neural network and I just have to remember to divide it by the prior probability of the state.",
            "If I don't do this, my neural network could be amazing and I could wind up getting really bad.",
            "Really bad performance of the end to end system.",
            "So in practice, you can compute this by just taking a whole bunch of aligned data and taking the empirical probability of how often that state occurs.",
            "For things like phonemes, some people will actually take.",
            "Take very careful measurements of how often this phoneme actually occurs in human speech and just use a hard hard coded parameter.",
            "So anyway, if you're trying to debug and you're finding that wow you've got this amazing neural net, but the pipeline just doesn't work, don't forget this step and it's a little strange that sometimes this is buried in papers or it's buried in books, so don't forget that this is there.",
            "Yeah.",
            "So that's one example of what you could do that's kind of like the first option.",
            "So if I have a pipeline that kind of works, I could run Viterbi to find the most likely States and use that for a training set.",
            "In practice, regardless of how you start out, people sort of do this iteratively, like I start with a weak model, I do my alignment train up, minor all net, and then I re run it again and I align it, retrain my neural net, and so on, almost like its own little EM procedure of going back and forth between aligning and retraining.",
            "Great so."
        ],
        [
            "So the reason I talk about this is really cool.",
            "This is kind of like one of the first ways to get a big win in speech recognition, so this is actually a clip out of George dolls Paper from 2011 and where you can see here is that this combination of a deep belief network and in HMM system get about 69.6% accuracy and this is compared to a whole bunch of GMM HMM baselines.",
            "So these are basically.",
            "Hmm, systems like I was telling you about, but where the observation model is a mixture of Gaussians.",
            "So you can see this is about a 10% relative improvement over the best one down here in the mix and he went up even more once you add things like Rolo and Dropout.",
            "And of course much more recent state of the art papers are well beyond that, so this is really neat that this very simple idea using deep learning.",
            "Plug it into the speech engine, actually give you a relative improvement that would be equivalent to enormous amounts of engineering.",
            "If you go through the speech literature.",
            "So you quit."
        ],
        [
            "And of course, replace this acoustic model these DNS with something else the framework doesn't especially care what lives down there.",
            "It's kind of a black box, so I was kind of assuming when I started that you guys have been seeing recurrent neural networks all week long and then roll include me in and said no, no, no we haven't seen seen that many of them yet, so I'm going to kind of run through this quickly.",
            "But instead of just having deep neural Nets where we try to predict the probability of this state given observations, one of the things that turns out to be really useful is if these guys can fold in context.",
            "So if this neuron up here, that's making a prediction, can somehow see the past and the future in the audio to make a better prediction.",
            "So there are a couple of ways you can do that.",
            "Why is that?",
            "You could have each of these neurons pull in a sort of big window of acoustic vectors of these observations that might help you a bit, but what's really popular to do now is to use a recurrent network, and So what?",
            "A recurrent network is is really one DNN that you can think of as having like little self loops.",
            "But once we unroll it over time, we sort of clone the deep neural network with these little recurrent loops being unrolled, and so now I have this grid.",
            "But don't get, don't worry too much about it.",
            "Once it's unrolled, it's just another deep neural network, right?",
            "You can see that this is just a directed graph, so if I want to run backpropagation from my training algorithm, I can just run the same back properties like propagation algorithm that I would use for any other deep neural network.",
            "Yeah.",
            "So this is the beauty of the RNN is that this guy over here has effectively unlimited prior context, right?",
            "If the neural net really wants to, and assuming I can get the optimization algorithm to workout, this neuron can actually get information transferred all the way from the beginning of the utterance, and something that I haven't drawn in the picture here is that's popular for systems.",
            "In the research literature is I can also have a bidirectional RNN.",
            "I can actually take future context and feed it back, and even though it seems like that might break it, you will be able to see it's actually still a deep neural network and you can run forward and backward propagation the same way.",
            "Yeah, and so if you use a bidirectional RNN, basically every single neuron can essentially see the entire utterance.",
            "It has all the context it could want.",
            "OK, so."
        ],
        [
            "I want to make sure I get through some of the stuff on more modern deep learning things, so I'm just going to say about something about rescoring very quickly because it's a useful idea.",
            "So a lot of these systems produce like an N best list, so you take your amazing speech engine, you crank some data through it, and for every piece of audio it'll generate a list of things that think you soup that you could have said along with a score.",
            "And so one of the things that we can do is look at all those scores and try to use a more powerful model to go back for just that list.",
            "Just that list of N Anri rank them basically so it was much cheaper than trying to run the decoder again, right?",
            "Which has to search over all possible word sequences.",
            "If I just have N word sequences, it's much cheaper to just check them all and see which one is the best so I can use a much slower or much more powerful algorithm.",
            "So one exam."
        ],
        [
            "Full of place where this is kind of interesting is in the language model, so we played with the acoustic model and there there's some reasons that you might not use a neural language model in the system.",
            "One of them is speed, but you can also use the language model for rescoring.",
            "So for example.",
            "If I have a giant neural language model that takes in a lot of context if I only have 1000 sentances, it's very fast to evaluate them all here.",
            "And so if I give you a couple of options from a speech transcription, right?",
            "If someone saying I'm a connoisseur or looking for wine and pork chops.",
            "That's not entirely unreasonable.",
            "Where is this?",
            "So it says I'm a connoisseur looking for wine and pork chops.",
            "This sounds very similar and because of the sort of long range in context here, we figure kind of sewers are probably looking for wine and port shops, not wine and pork chops, but depending on the audio, maybe your recognizer has scored them in the wrong order, so something that you could use a neural language model for is to actually gobble up these words and try to predict how likely is the word pork chops to end this sentence versus the word port, followed by the word shops and so.",
            "If your neural language model is much more powerful and unlike an ngram model that tends to have short context, if the neural language model can pull in this long range in context, it has a chance that it could reorder these things and get you the answer you want.",
            "So this idea of rescoring is really useful 'cause you've got a big fat slow algorithm that you really wish you could use.",
            "This gives you a place to plug it in where the speed doesn't hurt you so much."
        ],
        [
            "OK, so with the last little bit of."
        ],
        [
            "I'm here, I want to talk about what is a very cool technology that I think is going to help make all of this other stuff obsolete.",
            "Overtime, this is actually one approach.",
            "There are others in the deep learning literature now, but I'm just going to talk about this one as an example.",
            "So one of the problems with this whole pipeline up to now is that it's really complex.",
            "We were chatting earlier about how to get the things started.",
            "You kind of need labels to train your acoustic model and then you need to realign all your data and then you need to retrain it and realign and so on.",
            "And if the alignment goes wrong then everything gets screwed up and you've got to go get hand labeled data.",
            "It's a big mess.",
            "There are all these different steps and a lot of them can breakdown.",
            "So what we'd really love to do is to be able to train at least the acoustic model without worrying about this alignment problem.",
            "To just say I don't care what the alignment is, I just want you to give me the transcription.",
            "So in some sense, finding the alignment makes the problem harder.",
            "So one of the proposals by Alex Graves from at the time in your group is called Connectionstring connectionist temporal classification.",
            "The tongue twister so everybody says CTC 'cause we don't like saying that.",
            "And so I'm going to."
        ],
        [
            "To tell you a little bit about how CTC works intuitively, I'm not going to go through the algorithm mathematics.",
            "You can find them in the paper, it's pretty clear.",
            "So basically what we want to do is to create a neural network that's going to output a sequence of probability vectors.",
            "So the idea is that we start with a sequence of feature vectors going in, and we're going to output a sequence of probability vectors with the same number of items.",
            "So it's going to be the same length in the end as our observation.",
            "And we're going to assume somewhat naively, that the probability of any given sequence of phonemes is just the product of the probability of those phonemes for every single time step taken independently.",
            "So I'm just going to take a product.",
            "So in order to make this kind of workout right, because the sequences are the same length, I can't just take my 5 phonemes and try to spread them out over the whole axis, right?",
            "So what CTC does is it introduces this blank character that you can kind of use to fill space in your predictions so that the.",
            "The output sequence can be the same length as the input sequence, so to kind of show you how this would work is if we start with our spectrogram here, which is just a sequence of observation vectors.",
            "We can crank this through a neural network or recurrent neural network, and then what we want to have out is a big grid of probabilities.",
            "So each of these columns is one of the probability vectors.",
            "And the.",
            "And each of these little boxes represents the probability of this symbol on the left, which right now are either phonemes or blanks.",
            "So most of the time we sort of expect this thing to say, be saying blank, blank, blank, blank, blank, 'cause I don't know what's being said or I'm waiting for the next phoneme to show up.",
            "And then as time goes by, hopefully the neural network says, oh I heard a phoneme.",
            "I think I know it's been said, and it turns on the unit that represents that phoneme.",
            "And it turns off the blank.",
            "And then it goes back to predicting blank, blank, blank blank.",
            "So, so this is kind of the basic idea for what we want the network to do.",
            "So the reason for this?",
            "Is that we don't know the phoneme alignment, advance, right?",
            "So we can't just train this directly, so we want to somehow figure out a way to train this neural network that's generating these probability vectors without knowing when those spikes are supposed to happen.",
            "We know that those phonemes should show up somewhere, but we don't care where they are.",
            "We want to somehow be agnostic to this.",
            "So pre."
        ],
        [
            "Basically, the way we did this, where was that we let the EM algorithm guess we just said pick some alignments, tell me what you think is reasonable, and then I'm just going to retrain.",
            "Assuming that you got the right answer and that makes it complicated.",
            "But what we really want to have happen is for the alignment to be irrelevant, so that doesn't bother us.",
            "So the solution idea that the CTC paper introduces is an operation that makes it so that transcribing from from a given utterance or from a given observation to the phonemes that represent your words is invariant to all these shifts and changes.",
            "So if I change the alignment, I want to get the same transcription in the end, I just want to take all of that out and be invariant to it."
        ],
        [
            "So what they do is they introduce this operator and so let's.",
            "Let's assume that we start out with a network that's already trained up to do the right thing, right?",
            "And let's think about drawing a string from that distribution.",
            "So I look at the first set of probability vectors and I sample one of the symbols.",
            "When I move over to the next probability vector and I sample assemble and I just keep on going.",
            "What you might see is something like this.",
            "So for the word hello maybe I say blank, blank, blank, blank.",
            "When I say ha ha ha ha ha for a little while to represent the the H noise and blank blank blank for awhile until I see another phoneme and so on.",
            "The collapsing operator.",
            "What it does is it goes through and it removes all of the duplicates.",
            "All of the repeated phonemes because it figures it well adjacent predictions that are seeing the same thing.",
            "And then I'm going to wipe out all the blanks.",
            "I don't care about that that's not part of my transcription.",
            "And so this would actually just collapse to hello.",
            "And the important aspect of this is that under this observation, all of these different potential transcriptions of blanks and phonemes, all mapped to the same thing.",
            "So these are all perfectly legitimate outputs.",
            "If the CTC Network says that this is a good one, that's OK with me.",
            "If it says that this is a good one, that's also OK with me.",
            "So I'm kind of now agnostic to the alignment, right?",
            "I can move these things around and I just don't care."
        ],
        [
            "Alright.",
            "So in order to actually do training or to do anything of value, what we need to be able to compute is the likelihood of one of the label sequences.",
            "So for example, if I say hello is my, is my training label.",
            "What I want to do is sum over all of the possible alignments, right?",
            "I want to do this marginalization.",
            "Basically, that was really hard to do in the previous settings.",
            "All the possible transcriptions from my network that could collapse to give me this result, 'cause I don't care which one of you pick, I'm just going to take them all and assign them to the same word.",
            "So basically if I take this probability here, I'm going to break it out as the sum of all of the different transcriptions.",
            "So here's the three from the last page and you can imagine going through all of the combinatorially many choices.",
            "And so the nice thing here is that graves and coauthors gave a forward backward algorithm, a modification of the forward backward algorithm you would use for in HMM, for example, to actually compute this summation efficiently."
        ],
        [
            "So to train this thing, what we really want to do is do gradient gradient descent to maximize the probabilities."
        ],
        [
            "Again, here's this kind of gnarly summation that has combinatorially many utterances that could collapse to give me the right transcription or combinatorially many sequences that could give me the right transcription and the good news is that I can actually compute this thing using this forward backward algorithm.",
            "The forward backward algorithm will have a bunch of intermediate's that it generates to do its work, and if you just take those things, you can actually compute this gradient and it's just in the paper."
        ],
        [
            "Great.",
            "So if we actually run this training algorithm, it's really cruel in my opinion to see how this thing does its work.",
            "So the way to interpret this graph, let's start with the first one here is that on the vertical axis is the probability of a particular symbol, say like the first, say the blank or the probability of the huh sound plotted overtime.",
            "So you can think of this is like watching one neuron overtime and looking at when it turns on and turns off.",
            "And that's the same for all of these.",
            "Horizontal axis is time.",
            "So when the system starts out, training the actual outputs are just going to be 0 or garbage, right?",
            "Just say I I don't know what's being said.",
            "Everything is just not going to be outputing very much, but if you look at the gradient, which is like the error signal, it's what your label is telling you to change in your probability vectors.",
            "You'll see that the algorithm is trying to sneak everything blank, so please do something just just output blanks and then it's going to take a very rough partitioning of all the phonemes in your transcription and just kind of spread them out.",
            "It's going to say you know what?",
            "The H sound that's somewhere over here.",
            "You should try to say H. The sound is somewhere here.",
            "You should try to make more probable and overtime it kind of lines itself up so that you're starting to get little peaks where the sounds are, and by the time you're finished, CTC tends to be actually very confident that it knows exactly what phonemes are being said and exactly when they've been said, and your error will slowly go down to 0.",
            "So this is a pretty cool result because it frees us basically from having to deal with all of these alignment problems, which is a big challenge for like traditional systems, but is also a big enabler for new systems.",
            "So."
        ],
        [
            "We still have to do decoding with this.",
            "We have a sequence of probability vectors, so it turns out that if we want to compute this."
        ],
        [
            "We have a couple of options.",
            "One is the quick and dirty solution.",
            "If I just say you know what?",
            "I just want to find the Mac.",
            "The most probable sequence and then I'm going to collapse it.",
            "Kind of makes sense, right?",
            "But it turns out this is not going to give you the best answer, right?",
            "This is not accounting for all of the ambiguities that can come up, but it will actually give you something reasonable.",
            "But it's a useful sanity check.",
            "So the."
        ],
        [
            "Order though, and This is why I went through beam search earlier is that you can use a lot of the same ideas, the same search algorithms to go hunting through sequences of characters or phonemes in these probability vectors, you can incorporate a language model if you'd like.",
            "And this will allow you to basically generate a transcription of phonemes.",
            "So.",
            "Another way to do it, and in fact Alex Graves himself, has used this in a couple of papers for results so that you can take those probability vectors.",
            "And again running beam search might be slow, but every time we have a slow algorithm, we can always kind of punt and use it for rescoring.",
            "So if I take a list of phonemes transcribed from my really good speech engine and then re score them with this, I might be able to do better."
        ],
        [
            "So there's no fundamental reason why we have to use phonemes.",
            "There's no reason why we have to do this.",
            "It's partly because it meshes well with all of the existing systems, so a really cool future direction of deep learning, and which is happening right now at Baidu in many places is trying to get rid of all of this infrastructure.",
            "And instead of using phonemes, why not just have CTC generate characters?",
            "Just have it out, put it all on its own.",
            "So Grayson gently try to do this in acnl.",
            "We still probably want a language model in here even though we're outputting characters, but the high level point here is that once you have CTC working and you're really good at working with unaligned transcriptions, you don't need big changes to the algorithm.",
            "The same algorithm will actually work and yet it's much much much simpler to build.",
            "So."
        ],
        [
            "So this is actually a snapshot from Alex is last paper on this where you can actually see that CTC is doing the same thing it did for phonemes.",
            "You can actually see it a lot if I say his friends will actually say, well, that sounds like an H. Sounds like an eye and it's folding in context from all of the all of the observation vectors to just make the right prediction.",
            "And we don't care about the alignment anymore because of this CTC loss.",
            "One sort of funny thing to be aware of is that this introduces a new problem, which is that CTC kind of learns how to spell like a like a 5 year old.",
            "If it is not like if you say to illustrate a point so it's like 2 hours straight, the point.",
            "This is an example from Alex is paper and these are really hard to fix because your language model doesn't really get to see what's going on underneath here.",
            "And so the fact that these sound exactly the same is a little tough to deal with after the fact, and finally."
        ],
        [
            "So here are some examples from from paper actually from our group recently where if you look at the Max decoding you can kind of see what the neural net wants to to output on its own.",
            "So sounds like Boston with an I.",
            "We were trying to figure this out.",
            "Prime Minister Narendra Modi.",
            "But if you use a language model on top of this with the language model, decoder actually knows who Narendra Modi is.",
            "And similarly, Arthur, any tickets for the game sort of fanatically works out, but you need a language model to really find the proper spellings, and one of the things that really vexed us for a long time.",
            "We started calling this the Tchaikovsky problem, because if someone says, oh, I'd like to hear something by Tchaikovsky.",
            "The phonetic you know transcription of this that CTC outputs is actually completely reasonable.",
            "But unless you happen to have heard this before and have seen someone write it down, you have no idea why Tchaikovsky would start with a T. And so this is one of the reasons, actually, that they sort of phoneme models from previous work still kind of have some things to like because they're a little bit robust to these sorts of spelling errors that the neural net might make."
        ],
        [
            "So high level conclusion is that these traditional HMM and DM Ed DN hybrids are still really common out there.",
            "If you go do an internship in your work on a speech engine somewhere, you may well see a whole bunch of this stuff, and so I hope you've seen a few places where deep learning has plugged in in the past, and certainly there are a lot of others, so so it's sort of a way to supercharge the existing systems we've got and more recently, CTC, especially, but also now, is like attentional models, and some of the more powerful recurrent Nets.",
            "We're starting to see.",
            "There's there's a chance that we're going to be able to replace all of this stuff soon with sort of the next wave of deep learning systems that won't won't have a lot of these caveats and can just learn from data and do a much better job, and at least for the Tchaikovsky problem, I think the system at Baidu Now gets this right 'cause it's actually heard enough audio and seen enough things that it understands that there's a chance that it starts with a T. And so I think the next wave of deep learning system should be even better.",
            "So."
        ],
        [
            "You a lot and I do just want to say really quickly thanks to to anyone who's actually the real speech expert at Baidu for a lot of his help on this, I really appreciate it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm saying, as you mentioned, I actually spent most of my time in school working on computer vision problems, which actually lend themselves to much better demos.",
                    "label": 0
                },
                {
                    "sent": "It turns out so one of the things about speech recognition, though a lot of the work that has come out of Baidu on speech recognition, is actually the work of the team.",
                    "label": 0
                },
                {
                    "sent": "And so I sat down to work on this talk.",
                    "label": 0
                },
                {
                    "sent": "Like Russell saying earlier, I spent a lot of time thinking about, like what's the most useful thing to tell you guys about?",
                    "label": 0
                },
                {
                    "sent": "And so much of speech recognition as I think it will be done in deep learning is about just training very, very powerful neural networks to solve the problem end to end.",
                    "label": 0
                },
                {
                    "sent": "I really felt like a lot of the stuff that makes that tick.",
                    "label": 0
                },
                {
                    "sent": "You guys are seeing in all of the lectures preceding this one.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to do is actually spend some time telling you about speech recognition as it is widely practiced today.",
                    "label": 0
                },
                {
                    "sent": "The sort of dominant form of speech system.",
                    "label": 0
                },
                {
                    "sent": "Because if you dive into the literature, this is the kind of thing that you're going to see, so I'll tell you a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "So just in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case you needed any motivation in case you felt that speech was not exciting enough.",
                    "label": 0
                },
                {
                    "sent": "I think speech recognition is kind of one of these things, like computer vision.",
                    "label": 0
                },
                {
                    "sent": "It's really like a fundamental AI goal.",
                    "label": 0
                },
                {
                    "sent": "In my mind.",
                    "label": 0
                },
                {
                    "sent": "You know, as a practical matter, there are lots and lots of applications, so you can imagine trying to do things like video or voice transcription.",
                    "label": 0
                },
                {
                    "sent": "Things that are very easy for humans and practice to do very accurately or maybe even building very natural interfaces to services and devices.",
                    "label": 1
                },
                {
                    "sent": "Make it as easy.",
                    "label": 0
                },
                {
                    "sent": "To talk to the devices that you interact with as it is to talk to people next to you.",
                    "label": 0
                },
                {
                    "sent": "So this would really help because I hate typing on my phone, I was trying to search for a very long paper name on my phone before I got on the airplane.",
                    "label": 0
                },
                {
                    "sent": "I was desperate to to have a mobile connection that could do this.",
                    "label": 0
                },
                {
                    "sent": "So even though speech recognition in the past I think is always been a high goal.",
                    "label": 0
                },
                {
                    "sent": "It's one of these things is deceptively easy for people to do, but has historically been incredibly hard for machines to do.",
                    "label": 1
                },
                {
                    "sent": "Think this is sort of the consonant AI problem, something that humans do with very little effort But machines have really just not been able to do well, and as one of my little like stock photo examples over here, you can see this hasn't stopped us from trying to deploy these things anyway.",
                    "label": 0
                },
                {
                    "sent": "So I have no idea what this person is actually saying, but I'm going to guess that it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the high level goal right is that we want to take some speech audio which is usually coming to us is just a raw waveform so.",
                    "label": 0
                },
                {
                    "sent": "Someone's going to talk to their phone, talk to my computer and I'm just going to record the raw audio waves.",
                    "label": 0
                },
                {
                    "sent": "So usually it's like a sequence of say 8 bit or 16 bit numbers.",
                    "label": 0
                },
                {
                    "sent": "But you can just think of this as a big array of floating point values.",
                    "label": 0
                },
                {
                    "sent": "It's like a 1 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have some system in here.",
                    "label": 0
                },
                {
                    "sent": "That's a speech recognizer an our end goal is just to spit out a transcription to figure out that this is actually, you know me, sitting in my living room, saying Hello World.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason that this is really hard is because solving this problem, figuring out what's being said in this audio wave is confounded by a crazy number of factors, just like in computer vision, we always talking about occlusion and shadows and shading and all the optical effects and things that really confuse computers.",
                    "label": 0
                },
                {
                    "sent": "Speech has its own giant stack of problems that it's gotta deal with.",
                    "label": 0
                },
                {
                    "sent": "So for example, there's a big range of types of speech.",
                    "label": 1
                },
                {
                    "sent": "People speak differently in different contexts, so when I'm talking to you right now, I'm speaking kind of spontaneously sort of thinking of what I want to say, but then just telling you and I have a very different intonation from if I'm just reading something to you on paper.",
                    "label": 0
                },
                {
                    "sent": "So if I sit down and do something more dictation ollh, then that speech sounds very different.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that, like Red speech, is actually far easier than something like conversational speech, especially in conversational speech, you get huge variations in tempo in volume, you'll hear people's voices kind of trail off.",
                    "label": 0
                },
                {
                    "sent": "At times, you have natural speaker variation.",
                    "label": 1
                },
                {
                    "sent": "You have pronunciation and accent differences.",
                    "label": 0
                },
                {
                    "sent": "So even though 10 different people will say the same word, they all sound completely different in the audio waves look completely different.",
                    "label": 0
                },
                {
                    "sent": "You know things like disfluency, which is almost like a problem in translation.",
                    "label": 0
                },
                {
                    "sent": "You sort of don't know how to handle things, and maybe you want to transcribe them.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't.",
                    "label": 0
                },
                {
                    "sent": "If someone repeats a word a couple of times like they're stuttering, should you transcribe that?",
                    "label": 1
                },
                {
                    "sent": "Or should you try to skip over it when people say halves of words, what do you do and how do you recognize that this is all really, really hard to deal with, and that doesn't even account for the very basic environmental problems like noise.",
                    "label": 0
                },
                {
                    "sent": "Something really cool that I only learned about since joining by Jews called the Lombard effect.",
                    "label": 0
                },
                {
                    "sent": "So it turns out if you're standing in a noisy room, I always notice that now this now when I'm standing in a noisy room and someone calls to you and you want to talk to them, actually kind of raise your voice like hey in the back and this is an almost involuntary kind of reflexive change that you make to sort of break out of that noise channel so that people can understand you.",
                    "label": 0
                },
                {
                    "sent": "But if you do this to a speech engine, it is very confused.",
                    "label": 0
                },
                {
                    "sent": "It's never heard someone raising their voice like that before an editor actually breakdown for this sort of trivial thing that you and I handle without thinking about it.",
                    "label": 0
                },
                {
                    "sent": "And then another one that is also pretty interesting from a practical standpoint is that we often want speech engines to deal with a very large vocabulary.",
                    "label": 0
                },
                {
                    "sent": "In fact, one of the biggest problems is that the vocabulary needs to be superhuman.",
                    "label": 0
                },
                {
                    "sent": "That for me to understand, sort of my friends who who use very similar vocabulary and slang and technical jargon.",
                    "label": 0
                },
                {
                    "sent": "To me it's really easy.",
                    "label": 0
                },
                {
                    "sent": "Maybe I use 50,000 words or something at best, but if I'm trying to build a system that lots of people are going to use that I can just deploy out there.",
                    "label": 0
                },
                {
                    "sent": "I might need to know like millions of words because I just don't know where you're from and what you're going to say necessarily, and so I actually have to be superhuman in terms of my my intellect and ability to guess what you've just said.",
                    "label": 0
                },
                {
                    "sent": "So these are really huge problems.",
                    "label": 0
                },
                {
                    "sent": "I think this is part of the reason that this is a really, really great place for deep learning.",
                    "label": 1
                },
                {
                    "sent": "Great place where deep learning can make a big difference.",
                    "label": 1
                },
                {
                    "sent": "The deep learning is very good at dealing with these sorts of intuitive, noisy problems when we have a lot of data where in the past we've been trying to kind of engineer around this stuff.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the rest of the time here, I'm actually going to talk about speech recognition in two pieces.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, kind of at the outset I'm going to spend quite a good chunk talking about traditional speech models, and that's not because I think that you should all go home and try to build a speech engine based on this traditional approach.",
                    "label": 0
                },
                {
                    "sent": "Like, please, please do not.",
                    "label": 0
                },
                {
                    "sent": "You will be much happier, so there are great tools out there like kaldi which is an open source engine that can kind of get you started.",
                    "label": 0
                },
                {
                    "sent": "But the real reason I want to talk about is that this is still sort of the dominant architecture behind state of the art systems, and it's just commonly assumed throughout the literature.",
                    "label": 0
                },
                {
                    "sent": "So if you pick pick up even a deep learning research paper today on speech recognition, you'll hear people talking about things like triphone States and acoustic models and stuff like that, and it's because they're plugging deep learning into this existing system that has a great deal of wisdom.",
                    "label": 0
                },
                {
                    "sent": "And expert knowledge built in so you can kind of think of this as being like the deep Learning Survival School for speech recognition as it's practiced today.",
                    "label": 1
                },
                {
                    "sent": "And then with the last bit of the time, I'll talk about deep learning for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "What are the things you can do with deep learning in the traditional model to make it work much better and also talk to you about something called CTC, which is actually one of the very cool pieces of technology we have in the deep learning world that enables us to do things more like end to end learning and throw away a lot of those traditional components.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's get started talking about traditional speech MoD.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here so.",
                    "label": 0
                },
                {
                    "sent": "The basic pipeline that I'm going to tell you about represents a pretty wide range of common practice, but the speech literature is so large that you're going to see a ton of variations on this when you go looking for them in the wild, so I'm going to gloss over some of the algorithm IK details of these pieces, but try to give you a picture of water.",
                    "label": 1
                },
                {
                    "sent": "All the moving parts and how these things are put together.",
                    "label": 0
                },
                {
                    "sent": "So just as a caveat, this may have a short shelf life, so if deep learning researchers do our jobs, we might well be able to jettison a lot of this over the next couple of years, but for the moment, if you want to understand what's out there now and a lot of the research that's happening today, some of this is still quite relevant.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our basic goal, like I said, is to start out with an audio wave and generate this transcription through some kind of speech recognition engine.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you what the pieces are in a second and just as a notational thing, I'm going to kind of use capital letters here to talk about a sequence like a vector of a whole bunch of samples, and then I'll use lowercase letters to represent the sort of internal parts of that array.",
                    "label": 0
                },
                {
                    "sent": "So here for an audio wave, as I said, it's like a sequence of a 16 bit numbers or floating point numbers.",
                    "label": 1
                },
                {
                    "sent": "X1 would be like the first sample and X2 would be the next sample.",
                    "label": 0
                },
                {
                    "sent": "And if I were to plot all these I would just see some audio wave so it's just a notational thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Similarly, once we get to the end, our goal formally is that we want to find a sentence or a word.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call Capital W here, which is just a sequence of little word tokens.",
                    "label": 0
                },
                {
                    "sent": "That is the most likely word given given the input is straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To actually solve this the way that most people proceed is to break this into a few different problems.",
                    "label": 0
                },
                {
                    "sent": "I actually have several components that we're going to pull together to give the final solution, so I'm going to start out with my audio wave here.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to compute some sort of feature representations common in machine learning.",
                    "label": 0
                },
                {
                    "sent": "We are really unhappy working with raw data, usually like 16 bit numbers, and so on, and so we're going to compute some feature representation that's more convenient for doing recognition.",
                    "label": 0
                },
                {
                    "sent": "We're also going to use something called an acoustic model, so the idea behind the acoustic model and this is where deep learning turns out to play a big role.",
                    "label": 0
                },
                {
                    "sent": "Is that what I want to model is the probability of the observations or the probability of my features given a word.",
                    "label": 0
                },
                {
                    "sent": "So if I tell you the word in advance, this is a model that tells you roughly how it might sound.",
                    "label": 0
                },
                {
                    "sent": "All the different variations of how it might sound, so it's a very complicated distribution, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it has all the speaker variation and so on baked into it.",
                    "label": 0
                },
                {
                    "sent": "But, but that's what the acoustic model is.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have a language model, which is just the prior probability over all of the sentences that I could say.",
                    "label": 0
                },
                {
                    "sent": "So you could train this off of the web, for example, just look at what people are saying out there and be able to say for any given sentence, how likely is it that this is a reasonable English.",
                    "label": 0
                },
                {
                    "sent": "And then the component there really hard piece actually that pulls all of this together is often called the decoder, and the decoder is going to try to take this acoustic model and the language model and the feature representation and solve this maximization problem.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to find the word that is the most likely or the most likely sequence of words given the input.",
                    "label": 0
                },
                {
                    "sent": "And that means we need to maximize this expression, which you can breakdown using Bayes rule into the product between this acoustic model and the language model.",
                    "label": 0
                },
                {
                    "sent": "So many questions so far.",
                    "label": 0
                },
                {
                    "sent": "OK, cool.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of a wrinkle in this and that.",
                    "label": 0
                },
                {
                    "sent": "Representing words as strings of letters is really complicated, because letters we know just they don't necessarily correlate with how we pronounce things.",
                    "label": 0
                },
                {
                    "sent": "So the common transformation that gets made is to turn a word like the word hello into phonemes.",
                    "label": 0
                },
                {
                    "sent": "So phonemes are sort of an invention to represent the sort of smallest distinct units of sound you can think of.",
                    "label": 1
                },
                {
                    "sent": "These as like the building blocks or the atoms of speech.",
                    "label": 0
                },
                {
                    "sent": "So for something like hello.",
                    "label": 0
                },
                {
                    "sent": "I have a noise and I have an uh oh sound.",
                    "label": 0
                },
                {
                    "sent": "And if you string them together and you kind of slow them then it makes the sound hello and I think of each of these is like a symbol.",
                    "label": 0
                },
                {
                    "sent": "So when I go to try to recognize a word what I'm really trying to do is recognize this sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "Now, yeah.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of choices for our how to represent phonemes, right?",
                    "label": 0
                },
                {
                    "sent": "These in some sense in engineered representation.",
                    "label": 0
                },
                {
                    "sent": "And actually this is very approximate.",
                    "label": 0
                },
                {
                    "sent": "So what is exactly a phoneme is not entirely clear, and so there are lots of different choices for how to do this, so I'm using one here that, for example, is taken from the TIMIT data set.",
                    "label": 0
                },
                {
                    "sent": "That's kind of standard amongst researchers, but for example, if you go looking at like a dictionary.",
                    "label": 0
                },
                {
                    "sent": "They they will have a different kind of pronunciation key, and you'll often see speech papers using like the IPA alphabet to to represent the phones or the sounds.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right so previously I was using this notation W, right?",
                    "label": 0
                },
                {
                    "sent": "It's like a sequence of words, and if I knew that everyone pronounced the words exactly the same way, then I could just kind of translate back and forth and say that for a given word it's pronounced exactly like this.",
                    "label": 1
                },
                {
                    "sent": "It's a sequence of phonemes as opposed to a sequence of characters and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Yes, so so this is hard right to do the mapping back and forth is nontrivial, so this is exactly one of the little things I'll wind up glossing over a bit, but but the mechanics here will handle that situation if you really unroll it.",
                    "label": 0
                },
                {
                    "sent": "Cool, were there any more questions about that?",
                    "label": 0
                },
                {
                    "sent": "OK, cool so so this is just sort of a trick to kind of get out of the character world and into a world that's a little more related to sound.",
                    "label": 0
                },
                {
                    "sent": "And if you go online and use something like the TIMIT data set will sort of tell you what the phones are.",
                    "label": 0
                },
                {
                    "sent": "The phonemes are that correspond to the sounds in the data set, and they'll actually even give you a small labeled corpus to say, here's a here's someone saying ha and hear someone saying and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The traditional systems try to model the sequences as phonemes instead of words, and this means of course as we were just talking about that.",
                    "label": 1
                },
                {
                    "sent": "We need some way to go back and forth between words and phonemes, right?",
                    "label": 0
                },
                {
                    "sent": "So in practice what what a lot of systems have is a pronunciation model.",
                    "label": 0
                },
                {
                    "sent": "This is another statistical model that says if you tell me the word W, what are the distribution of all the different sequences of phonemes that correspond to that word.",
                    "label": 0
                },
                {
                    "sent": "And it could be ambiguous.",
                    "label": 1
                },
                {
                    "sent": "For example, it could be that for saying the word hello, I have a bunch of pronunciations and maybe there's something a word like Halo that can sound kind of similarly, and it would be the job of inference in this model to kind of disambiguate using, say, the language model.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here, basic?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He is just replaced W in a couple of spots with Q, which is the sequence of phonemes and for simplicity I'm actually only going to consider the case where we have this one to one mapping.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of drop this distribution here and say, you know we have some dictionary Q, so if you give me a W I'm just going to spit out a sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "I say everyone pronounces it this way and similarly with the acoustic model I'm now going to say for every sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "There's some distribution over the way that people might sort of say that that it might sound in practice, and then when I go back to my language model here, I'm going to have to affect some sort of mapping to make sure that I can look at a sequence of phonemes and guess how likely is it that that that is a legitimate sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So again, if you have ambiguous pronunciations, this doesn't work very well, but we're just going to simplify it that way.",
                    "label": 0
                },
                {
                    "sent": "So then when we go to solve this problem right, we just want to find the most likely word and so that basically means now taking our word, searching for a word, an it for every choice that I might have, I'm going to convert it to phonemes first so I can plug it into my little acoustic model and then solve the same problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first stage in all this is feature representations only going to spend a Slider 2 on this, but I'll say that definitely in traditional systems, the feature representation can make a big difference, and so there are lots and lots of different choices in the literature so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very common, one is like spectrograms.",
                    "label": 0
                },
                {
                    "sent": "This might be like the first thing you would try in a sense.",
                    "label": 0
                },
                {
                    "sent": "Sort of 1st principles.",
                    "label": 0
                },
                {
                    "sent": "How do I convert a raw audio signal into something a little more compact and informative?",
                    "label": 0
                },
                {
                    "sent": "There's something called MFC C, which is Mel frequency Kestrel coefficients.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into how these things are computed because it's a little tricky, but I'll tell you that there are libraries out there that will do it for you.",
                    "label": 0
                },
                {
                    "sent": "Writing it yourself is hard, and it's an incredibly successful feature in audio processing, so if you're doing sort of classic machine learning, you're playing around with like recognizing music or something like that.",
                    "label": 0
                },
                {
                    "sent": "You'll see MCCS alot.",
                    "label": 0
                },
                {
                    "sent": "And then, as with all machine learning applications, there's a whole bunch of literature on other crazy kinds of features that turn out to be useful in different contexts.",
                    "label": 0
                },
                {
                    "sent": "For example, you can also generate new features like, take all of your, say spectrogram bins, and then take the differences over little time slices, and now add the differences in his features as a way of kind of getting some sort of temporal information in there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I gotta use the spectrogram as an example.",
                    "label": 0
                },
                {
                    "sent": "So in all this stuff I'm talking about here, we're just using a spectrogram as a Canonical instance of water feature representation is.",
                    "label": 0
                },
                {
                    "sent": "So so here's me again, saying Hello World and the way you computer spectrogram is, you take a little window.",
                    "label": 0
                },
                {
                    "sent": "It's about say 10 milliseconds or 20 milliseconds wide and I clip out this little waveform.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so here's actually what it looks like when you zoom in, so that's about 20 milliseconds of audio.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is we're going to run a fast Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "That's going to give us basically the frequency domain representation.",
                    "label": 0
                },
                {
                    "sent": "We're going to sort of break this wave down into a composition of sine waves, and then we're going to take the squared modulus of this, and it's basically like asking how much power is in each of the frequencies, and I'm going to ignore the phase information.",
                    "label": 0
                },
                {
                    "sent": "I don't care if the sine waves that make this thing up or shifted a little bit.",
                    "label": 0
                },
                {
                    "sent": "So this basically describes like the frequency or say, pitch content that's in one of these little windows.",
                    "label": 0
                },
                {
                    "sent": "So if you want to sort of visualize what this means, you can take the FFT and plot it on two axes here.",
                    "label": 0
                },
                {
                    "sent": "So this is the frequency axis, so this would be like low frequencies over here.",
                    "label": 0
                },
                {
                    "sent": "Like really low pitch, and these are high frequencies over frequencies over here that are high pitch and then the height or color, same same representation is how much power there is.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a low sound.",
                    "label": 0
                },
                {
                    "sent": "With with less high frequency information and then in all the future slides you'll see me just referring to this with colors, so these are the low frequencies.",
                    "label": 0
                },
                {
                    "sent": "These high frequencies and this is basically a vector that we're going to call a frame so.",
                    "label": 0
                },
                {
                    "sent": "Basically, once we know how to compute the this for like a small window, we can just compute these frames, right?",
                    "label": 0
                },
                {
                    "sent": "So I've decided that the representation of this little 20 millisecond slice is going to be this vector of power.",
                    "label": 0
                },
                {
                    "sent": "Basically, this vector of frequencies, and so if I want to represent this entire utterance as a spectrogram, then I'm just going to concatenate all those vectors together into this Big 2 dimensional array.",
                    "label": 0
                },
                {
                    "sent": "So that makes sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, not really.",
                    "label": 0
                },
                {
                    "sent": "This is in some sense more informative.",
                    "label": 0
                },
                {
                    "sent": "So if you go back and look at kind of a lot of the intuitions behind, like why people use MFC sees.",
                    "label": 0
                },
                {
                    "sent": "For example, if you look.",
                    "label": 0
                },
                {
                    "sent": "So this is me.",
                    "label": 0
                },
                {
                    "sent": "Saying Hello World, if you look at the bands here, you can actually make out patterns that correspond sensibly well to sounds.",
                    "label": 0
                },
                {
                    "sent": "It's very noisy.",
                    "label": 0
                },
                {
                    "sent": "It's not exact, but you can actually see things there, whereas if you're just looking at the temporal domain, it's really hard.",
                    "label": 0
                },
                {
                    "sent": "And historically, it's just been very hard to find patterns in that, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "Yes, and in the deep learning world we are spoiled by this.",
                    "label": 0
                },
                {
                    "sent": "So in the past when you haven't had a lot of data, you've had much less powerful models.",
                    "label": 0
                },
                {
                    "sent": "You've had to sort of engineers some knowledge into the system to be able to get the machine learning algorithms to work well.",
                    "label": 0
                },
                {
                    "sent": "You've got to do a little bit of extra work, but if you have a very powerful acoustic model, let's say then, then really you don't care.",
                    "label": 0
                },
                {
                    "sent": "You could try to model it directly from the audio wave, and there's actually recent work where people are doing that.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so another justification for why to do this is maybe not the engineering aspect.",
                    "label": 0
                },
                {
                    "sent": "Even in fact one of the motivations for MFC sees one of the transformations that's done there is to set these frequency bins on a scale that kind of mimics perceptual behaviors of humans.",
                    "label": 0
                },
                {
                    "sent": "So if you go and test people and check what frequencies they can distinguish between, you can figure out how to bend the frequencies so that you kind of get the best representation for speech.",
                    "label": 0
                },
                {
                    "sent": "So people are using exactly this kind of intuition about how humans hear things in the hardware.",
                    "label": 0
                },
                {
                    "sent": "We have to inform how to build these features and tune them.",
                    "label": 0
                },
                {
                    "sent": "So another way we're kind of baking our knowledge into the system.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah Yep alot.",
                    "label": 0
                },
                {
                    "sent": "This window, oh, so they can be disjoint or overlapping.",
                    "label": 0
                },
                {
                    "sent": "You can pick.",
                    "label": 0
                },
                {
                    "sent": "So in practice there's like a little windowing function, almost like a little Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So if you did it sort of the perfect way instead of actually cropping the window, your support is the whole signal 'cause the Gaussian never really goes to 0, so they're always overlapping.",
                    "label": 0
                },
                {
                    "sent": "But in practice, when we compute these things, you're running FPS on little windows.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool, so let's go back to modeling.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we need to do is come up with a model of the observations of these features given this sequence of phonemes and what we'd like to create is some generative model of these features.",
                    "label": 1
                },
                {
                    "sent": "And so to kind of get at this, I'm going to start with a similar case.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume that our whole job in life is to model observations of 1 phony.",
                    "label": 0
                },
                {
                    "sent": "So the way that we're going to do this is to imagine that someone is saying how which actually covers, say, more than 20 milliseconds, and so that's represented in audio by several frames of speech.",
                    "label": 0
                },
                {
                    "sent": "Several of these frequency vectors.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to model this sequence of observations that are generated while you're speaking using a hidden Markov models, like a standard off the shelf sequence modeling tool.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me see here how many people are used to thinking of HMMS is like a directed graph with hidden variables and observations hanging off of them.",
                    "label": 0
                },
                {
                    "sent": "OK, and how many people are more familiar with HMMS?",
                    "label": 0
                },
                {
                    "sent": "Thinking of them is like state machines.",
                    "label": 0
                },
                {
                    "sent": "Select few people.",
                    "label": 0
                },
                {
                    "sent": "OK, this seems kind of normal.",
                    "label": 0
                },
                {
                    "sent": "I think we've changed in the machine learning world.",
                    "label": 0
                },
                {
                    "sent": "We've changed how we teach Hmm's so so kind of walk through what this means so.",
                    "label": 0
                },
                {
                    "sent": "What we can think of you can think of and.",
                    "label": 1
                },
                {
                    "sent": "Hmm is being like a little machine, so here's the generative process that in HMM is running to create those observation vectors.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to start out here in State 0, right?",
                    "label": 0
                },
                {
                    "sent": "That's where I'm going to begin, and then I imagine that there's a little machine that's going to come through here and at every time step it's going to hop to some new state, let's say state J from state state I with the probability determined by some parameter that I'm going to call AJ.",
                    "label": 0
                },
                {
                    "sent": "So we can represent this graphically by saying I'm in State Zero.",
                    "label": 0
                },
                {
                    "sent": "I can hop over this edge with this probability, which is going to be 1 here 'cause I don't have any other choices or if I'm in state one, I can either sort of self loop and come back to state one with probability A1 one.",
                    "label": 1
                },
                {
                    "sent": "Or I can jump to state two with probability A12 and I'm going to keep running through this.",
                    "label": 0
                },
                {
                    "sent": "And after each jump I'm going to generate one of these frames.",
                    "label": 0
                },
                {
                    "sent": "One of these vectors of all the frequencies based on whichever state I'm in, and so as a first pass of this is a very simple model.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that this observation how I generate my features given the state is just some Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So for each of these I'm going to have a little Gaussian that generates my features.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you see how this works going to start in State Zero, I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To state one generating observation, maybe I decide well, I randomly decided that I'm going to self.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repair I get another observation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe jump to State 2.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jump to state three and generate another observation is another loop.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at random I might get to the end of this state machine that says I'm done with my utterance.",
                    "label": 0
                },
                {
                    "sent": "So now I have all of the frames that represent whichever version of the phoneme I just sampled.",
                    "label": 0
                },
                {
                    "sent": "So this is like the mechanical process of how the HMM is going to generate these observations.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The challenge here now is to do inference.",
                    "label": 0
                },
                {
                    "sent": "So what we really want is not to be able to generate more utterances.",
                    "label": 0
                },
                {
                    "sent": "What we want to be able to do is recognize the ones we've already got.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we have been hmm, and let's just suppose for a minute that I've already trained up my parameters, so I know the transition probabilities between all these States and I have the say the Gaussian meaning covariance parameters that define how my observations are generated.",
                    "label": 0
                },
                {
                    "sent": "So if you give me the observation sequence, if you give me all these vectors, like the big speech spectrogram.",
                    "label": 0
                },
                {
                    "sent": "Then I can find the most likely sequence of internal states.",
                    "label": 1
                },
                {
                    "sent": "I can find the path that actually got followed through here to generate these using the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if you just go back to your your AI textbook or your statistical models textbook Viterbi is a simple dynamic programming programming algorithm that will find the most likely path that generated these observations.",
                    "label": 0
                },
                {
                    "sent": "So in this case, maybe it's decides you know what the most likely path is to be in State Zero, then one, then one, then two, then three, then three, then three, and then and then four.",
                    "label": 0
                },
                {
                    "sent": "And the intuition behind what Viterbi is doing for you is that it's actually deciding how to link up these observations with the various states it's performing, like an alignment of figuring out which state generated which observations with the baked in constraint.",
                    "label": 0
                },
                {
                    "sent": "That is kind of gotta go left to right.",
                    "label": 0
                },
                {
                    "sent": "I can't have this guy generate something over here after.",
                    "label": 0
                },
                {
                    "sent": "State Two has generated something in the middle, so we've kind of got some knowledge about how audio gets generated, baked in, and Viterbi is helping us go backward to figure out what is the sequence of states that could have generated the audio we see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's how we can take a given observation and find one state sequence that could have generated it.",
                    "label": 1
                },
                {
                    "sent": "But a more important thing to be able to do actually, especially for learning, is that.",
                    "label": 0
                },
                {
                    "sent": "We'd like to be able to compute the likelihood of our observations, 'cause if I go to optimize this model right, I want to maximize the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "I've got to be able to compute it first, and so the way we compute this is by summing over every possible sequence of states that we could possibly make inside that little machine.",
                    "label": 1
                },
                {
                    "sent": "That would give us a plausable replication of our observations, and so this is just marginalizing I'm multiplying my.",
                    "label": 0
                },
                {
                    "sent": "Observation model here by the probability of a particular state sequence which is conditioned on the fact that I'm talking about a single phoneme.",
                    "label": 0
                },
                {
                    "sent": "So this is just marginalizing out all those hidden states to give us the probability of this observation for this specific phony.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that we have efficient algorithms for this.",
                    "label": 0
                },
                {
                    "sent": "So Iturbi is sort of your off the shelf algorithm to find the best sequence.",
                    "label": 0
                },
                {
                    "sent": "And then there's the forward backward algorithm that just kind of your off the shelf system for doing these marginalization's in computing this some very efficiently.",
                    "label": 1
                },
                {
                    "sent": "So the good news is this is the acoustic model that we wanted, right?",
                    "label": 0
                },
                {
                    "sent": "We started out saying we wanted to know what's the probability of the observations given a sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "So this is the guy we're really interested in.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's how we Model 1 phoneme and now we want to model whole word, let's say.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's a simple example of a word that has two phonemes.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say my sequence of phonemes is Q1 and Q2 and the nice thing about having this little stamped hmm to represent the phonemes is that I can take the model that I use for Q1.",
                    "label": 0
                },
                {
                    "sent": "That's the state machine that generates the sound for Q1, and I can just wire it into the end of another model.",
                    "label": 0
                },
                {
                    "sent": "That generates the sound for Q 2.",
                    "label": 0
                },
                {
                    "sent": "So now if I want to generate observations.",
                    "label": 0
                },
                {
                    "sent": "For what this little word would sound like, I can start out in State Zero and I can hop around to this model and at some point the HMM will hop to the next phoneme and it'll generate some more observations until it gets to the end and now we'll have a whole word.",
                    "label": 0
                },
                {
                    "sent": "So mechanically, nothing has really changed here.",
                    "label": 0
                },
                {
                    "sent": "You can use all the same algorithms and so on.",
                    "label": 0
                },
                {
                    "sent": "This is just a way of kind of stretching out a little model to make a bigger one for words that make sense.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So likewise, if you want to build a whole sentence right, if I give you a sentence in advance, you can wire up all of these HMMS to build a single single model that is basically the generative model for that one sentence.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that we can for any given sentence, define a hidden Markov model that is the generative model for just that sentence.",
                    "label": 0
                },
                {
                    "sent": "So the way that we do training is pretty straightforward in the sense that we have the sentence that we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize the likelihood, and then we know how to build the model, and so if we have this fixed HMM structure, observations that were generated by the HMM we have off the shelf algorithms.",
                    "label": 1
                },
                {
                    "sent": "Again, that that can solve this training process for us.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to dig into TM in the context of Hmm's, it's called Baum Welch.",
                    "label": 0
                },
                {
                    "sent": "But but just as a refresher EMH involves two key steps, so the eastep is inference.",
                    "label": 0
                },
                {
                    "sent": "So we talked about the Viterbi algorithm, which is kind of a simplified version.",
                    "label": 0
                },
                {
                    "sent": "But basically if I tell you the observations, I want to know what's the distribution over all of this state sequences that could generate it?",
                    "label": 0
                },
                {
                    "sent": "And you would compute this, usually with the forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then there's the M step, which says go through all my parameters for my hmm, those aij's, the Gaussians and so on and try to update them to maximize the likelihood of my observations.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to keep iterating these guys to get the best model.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Low.",
                    "label": 0
                },
                {
                    "sent": "This is difficult to do in practice, and the very abstract reason is that EM is not guaranteed you to get guaranteed to get you the global minimum.",
                    "label": 1
                },
                {
                    "sent": "This is a sort of complicated process, and so there are a bunch of things that can go wrong, but some intuition for what makes this tough.",
                    "label": 0
                },
                {
                    "sent": "You can see by simplifying EM.",
                    "label": 1
                },
                {
                    "sent": "So instead of thinking of the step is giving me the distribution over all of the phony sequences that could have generated my observations, let's just replace that with Viterbi again, let's just assume that I'm going to take the most likely sequence to generate my phonemes, and then I'm just going to say all of the probability in the posterior goes on that one hypothesis, and I'm going to ignore all all of the other explanations.",
                    "label": 0
                },
                {
                    "sent": "Then what you can see is that if I do that herbion, I generate this sequence of states that I think corresponds to my observations.",
                    "label": 0
                },
                {
                    "sent": "If my model isn't trained yet.",
                    "label": 0
                },
                {
                    "sent": "If I'm just starting out, this could go very badly, and instead of getting this nice alignment that corresponds to how this thing was generated to begin with, I could end up, say, putting all of these observations in the wrong state.",
                    "label": 1
                },
                {
                    "sent": "I could sort of misinterpret the audio, and this is very painful.",
                    "label": 0
                },
                {
                    "sent": "Because it means that if we get the wrong alignment, then when we go in and we update all the observation models were sort of updating them with bad data.",
                    "label": 0
                },
                {
                    "sent": "We're updating them with a bad alignment and this whole thing will go South.",
                    "label": 0
                },
                {
                    "sent": "So this in practice makes training these systems really hard, and there's a lot of wisdom that goes into kind of fiddling with your pipeline to make this work.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so if you think about what are the big problems with traditional systems and where a lot of the work is going, it goes into sort of bootstrapping these things to get them to all play nicely together.",
                    "label": 0
                },
                {
                    "sent": "And so one really common trick, for instance, is go download timid or go download like a phoneme corpus that's already been labeled for you and try to train up your observation models in advance so that they'll kind of give you a good alignment from the beginning before you actually start PM.",
                    "label": 0
                },
                {
                    "sent": "So if you just implement this training algorithm from scratch, and you throw it in with a random initialization, it's probably not going to work well.",
                    "label": 1
                },
                {
                    "sent": "You've got to do a little bit of work upfront to get it to play nicely.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not going to dive in deeply on language modeling, but I want to say a few things that are sort of speech specific about language modeling.",
                    "label": 1
                },
                {
                    "sent": "So remember, in addition to the acoustic model right, we want a language model because when we multiply these things together that will give us the likelihood of our of the words given the observations.",
                    "label": 0
                },
                {
                    "sent": "And so we need this probability of W, the probability of some sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So there are a whole bunch of options for doing this.",
                    "label": 0
                },
                {
                    "sent": "There's a whole literature of course on just modeling language, but there are a few designer at a for building language models for speech that are kind of important.",
                    "label": 0
                },
                {
                    "sent": "One is that we want it to be really fast to query this thing because it's going to turn out in a little bit that you query this language model inside your decoder so your decoder is constantly asking the language model.",
                    "label": 0
                },
                {
                    "sent": "How good is this sentence?",
                    "label": 0
                },
                {
                    "sent": "How good is this sentence?",
                    "label": 0
                },
                {
                    "sent": "How good is this sentence and the language model needs to be able to run very quickly, so if you have like a giant neural network language model.",
                    "label": 0
                },
                {
                    "sent": "That needs to gobble up lots of context to give you an answer that this will slow you down.",
                    "label": 0
                },
                {
                    "sent": "Another important aspect is that we want to be able to train on really huge corpora, right?",
                    "label": 1
                },
                {
                    "sent": "We want to be able to train on billions of words ideally, and this comes from the fact that we're looking for this sort of superhuman vocabulary, so there are just lots of words, technical jargon, and things that you're never going to hear when you listen to people speak.",
                    "label": 0
                },
                {
                    "sent": "So if you just use speech data, you're never going to hear people use a deep learning technical terms.",
                    "label": 0
                },
                {
                    "sent": "If you're just gathering data off the web, it's very unlikely statistically and so.",
                    "label": 1
                },
                {
                    "sent": "You really want to be able to gobble up text, because that's how you're going to get all those rare words that people use.",
                    "label": 0
                },
                {
                    "sent": "And finally we want to be able to train them quickly.",
                    "label": 0
                },
                {
                    "sent": "This is a real problem for production systems, so if you want to use this to back up your search engine, for example one day people are searching for Madonna and another day people are searching for Taylor Swift and your language model needs to get updated to sort of account for the fact that some things are suddenly much more likely than they used to be.",
                    "label": 1
                },
                {
                    "sent": "So, so those are some of the things that we want, and as a result, one of the things you're going to see all the time is N gram models, where we basically have a gigantic table that just counts how many times we see a particular string of words.",
                    "label": 0
                },
                {
                    "sent": "So you can think of a bigram model, which is to say a 2 gram model of just being a table where I count up all the times that I see one word, followed by another, and then for a larger NI, count up the number of times I see a particular word preceded by some longer context.",
                    "label": 1
                },
                {
                    "sent": "And obviously for really large N there will be lots of sequences.",
                    "label": 0
                },
                {
                    "sent": "I just never see, even though they're reasonable, and so you have to play a lot of smoothing tricks and so on to make this work well.",
                    "label": 0
                },
                {
                    "sent": "So so there's good literature's behind this.",
                    "label": 0
                },
                {
                    "sent": "There's lots of kind of established schemes for doing that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to assume for the most part, that you've got a working language model here.",
                    "label": 1
                },
                {
                    "sent": "OK so so now for the big fish.",
                    "label": 0
                },
                {
                    "sent": "The really hard part of all of these speech engines is usually the decoder, because what the decoder has to do is solve this maximization problem.",
                    "label": 0
                },
                {
                    "sent": "Remember the forward backward algorithm in the Viterbi algorithm could tell us what's the probability of our observations when I fix these.",
                    "label": 0
                },
                {
                    "sent": "If I give you the sentence, it's not too bad to compute this number.",
                    "label": 0
                },
                {
                    "sent": "It's not that the objective can't be computed, it's that the number of combinations of words is enormous, and so I have to search over this space somehow very efficiently.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic problem is to to somehow maximize over all of the sequences we could.",
                    "label": 0
                },
                {
                    "sent": "We could possibly choose to make this as likely as possible.",
                    "label": 0
                },
                {
                    "sent": "And again we have this wrinkle here, which is that we've gotta sum out all of these.",
                    "label": 0
                },
                {
                    "sent": "All of these states.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are a whole bunch of different strategies to do this.",
                    "label": 0
                },
                {
                    "sent": "If you pick up a speech textbook, you'll probably see two or three different kinds of decoders there with different kinds of search strategies, so I'm not going to dig into a state of the art version because they can be really complicated and hard to get working, so I'm going to try to simplify this a little bit to just give you the idea for what this system does and what its job is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so to do that?",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to get rid of this little summation, which makes things more complicated and say, well, you know what, we're just going to look for the most likely S right?",
                    "label": 0
                },
                {
                    "sent": "The most likely sequence of states which won't give us the absolute maximum.",
                    "label": 0
                },
                {
                    "sent": "Here we could wind up sort of missing some cases, but it's actually not a bad approximation.",
                    "label": 0
                },
                {
                    "sent": "And the nice part is that if you fix the choice of S, if I told you what I thought the most likely sequence of states was.",
                    "label": 0
                },
                {
                    "sent": "I could go back to my picture of the hmm and just map out all of the words so it's really easy to go back to the transcription.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about a simplified problem here.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that I've got a two word vocabulary, so so all of the work that I'm going to do, I'm going to assume that people are only allowed to ever say 2 words and I've chosen Hyun guy because they both have two syllables, and they happen to share a syllable.",
                    "label": 1
                },
                {
                    "sent": "So so I'm only allowed to say hi and guy.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have a very simple language model.",
                    "label": 1
                },
                {
                    "sent": "These numbers aren't important, but basically your language model says what's the probability of saying guy after I've said hi?",
                    "label": 0
                },
                {
                    "sent": "What's the probability of saying high, high and so on?",
                    "label": 0
                },
                {
                    "sent": "So every combination and you also have some prior probability.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that I start a sentence with high versus probability that I start a sentence with guy?",
                    "label": 1
                },
                {
                    "sent": "And then we're going to also have our little HMM acoustic models like like we had earlier.",
                    "label": 0
                },
                {
                    "sent": "So for each of these phonemes, there are three of them.",
                    "label": 0
                },
                {
                    "sent": "I'll have a little template for how that phoneme is allowed to sound.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can actually build the entire HMM for this world basically.",
                    "label": 0
                },
                {
                    "sent": "And hmm, that can generate all of the sentences or utterances.",
                    "label": 0
                },
                {
                    "sent": "Allegedly that I can generate in this world.",
                    "label": 0
                },
                {
                    "sent": "So the first is I'm going to create two buckets, one for the word high and one for the word guy.",
                    "label": 0
                },
                {
                    "sent": "And within each of these I'm going to create a bucket for the different phonemes that make it up, and I'm going to fill these in with the little HMM templates that represent each phoneme.",
                    "label": 0
                },
                {
                    "sent": "So if you say the word high like we have before, if I'm modeling a word, I can kind of bounce around in these states to generate with the first phoneme sounds like, then I can hop to the next one and then eventually I'm going to leave this state and I have to decide what word I'm going to say next.",
                    "label": 0
                },
                {
                    "sent": "And so for this site, power point lines aren't great.",
                    "label": 0
                },
                {
                    "sent": "With this, when I leave the box, I can circle around and say hi again, or I can run back here and I can say guy and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So when we leave this, we gotta remember that there's basically a state transition that comes from my my little hmm that says how things sound.",
                    "label": 0
                },
                {
                    "sent": "And then there's also this language model term that says how likely it is to jump to either of the words.",
                    "label": 0
                },
                {
                    "sent": "So this is just an hmm, just like the little one we started with, it's a gigantic state machine and so all of the basic pieces we talked about still work here.",
                    "label": 0
                },
                {
                    "sent": "So our goal as I was talking about on the last slide, is to try to find what is the sequence of states here that maximizes the observation.",
                    "label": 1
                },
                {
                    "sent": "Basically, what's the transcription of my observations?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's really easy to do this with maturity, right?",
                    "label": 0
                },
                {
                    "sent": "We just talked about this, but there are some practical problems with doing this.",
                    "label": 0
                },
                {
                    "sent": "The first of all, if your problem is really big, Viterbi is still not going to be fast enough.",
                    "label": 0
                },
                {
                    "sent": "It's pretty fast.",
                    "label": 0
                },
                {
                    "sent": "It's pretty efficient, but if you have an enormous vocabulary right that diagram on the last page, that gets really big.",
                    "label": 0
                },
                {
                    "sent": "If you have 10,000 words and lots of different pronunciations.",
                    "label": 0
                },
                {
                    "sent": "And also I I kind of cheated in my example I used a bigram model and the bigram model is nice because it satisfies the Markov assumption, meaning I don't care about all of the previous context.",
                    "label": 1
                },
                {
                    "sent": "I only care about the last word, and so this kind of fits into Viterbi nicely.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately, if you try to use an ngram model that's got a long context, you are completely violating all the assumptions that make Viterbi work, and so you can't use dynamic programming anymore to get a good solution.",
                    "label": 0
                },
                {
                    "sent": "So This is why, in practice, when you look at state of the art decoders, you go looking through the literature how like a real decoder solves this problem.",
                    "label": 0
                },
                {
                    "sent": "You're going to see a lot of more general search formalisms.",
                    "label": 0
                },
                {
                    "sent": "Things like a star, for example, might be apart of your decoder, so I'm going to actually talk about an example here that's just based on beam search, partly because it's sort of representative of how the more complex decoders work, and also because even in current deep learning practice, if you build even an end to end system.",
                    "label": 0
                },
                {
                    "sent": "You'll see beam search like algorithms being used for decoding.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea behind Beam search is that I'm just going to keep a list of my top an hypothesis, like the top end candidates or state sequences, and at every step I'm going to propose an extension to each of those candidates, and then I'm going to re rank the man, come up with the N best.",
                    "label": 1
                },
                {
                    "sent": "So if we had the entire state sequence in advance, so I just gave you the whole transcription, then we could break out the log likelihood.",
                    "label": 1
                },
                {
                    "sent": "With this right I just took my product that I've had before and.",
                    "label": 0
                },
                {
                    "sent": "Took the log and so now it turns into a sum and so this is now the sum over all of the observations given the states, it's the sum over all of the state transition probabilities that I make.",
                    "label": 0
                },
                {
                    "sent": "And then it's the sum over all the word transitions that I end up making in the transcription.",
                    "label": 0
                },
                {
                    "sent": "And so the cheat.",
                    "label": 0
                },
                {
                    "sent": "The trick that we're going to use to make beam search work is that while we're searching, we're just going to keep a partial sum as we're hunting between States and we're sort of greedily trying to find the best next state.",
                    "label": 0
                },
                {
                    "sent": "I'm going to keep a running sum of just the first terms, so if I've made T prime steps right, I've jumped through T prime states.",
                    "label": 0
                },
                {
                    "sent": "I'm going to keep track of the log probability of all the observations I've seen so far.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to keep track of the log probability of all the states I've hopped through so far and every time I hop between words, I'm going to add on a little log probability for the words that I used.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about using beam search is that this is very generic.",
                    "label": 0
                },
                {
                    "sent": "If I want to add something like a word insertion penalty or word insertion bonus, which people often do, I can tack that onto the end for example, so I can actually add other little objective functions in here to try to make things work better.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "I'm going to walk through a visualization of how very simple beam search might work, so we're going to keep the top two candidates, and so to see how we accumulate this score.",
                    "label": 0
                },
                {
                    "sent": "These slides took forever to make, so I hope you enjoy them.",
                    "label": 0
                },
                {
                    "sent": "So starting from State 0 right, we don't have anywhere to go.",
                    "label": 0
                },
                {
                    "sent": "So if we pick what are the possible successors, there are only two choices, so I'm going to end up over here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if I keep the top two, this is now my my end best list.",
                    "label": 0
                },
                {
                    "sent": "This is my top two list.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I try to create a new set of candidates when I'm first going to start at each of these States and generate all the places I could possibly end up so.",
                    "label": 0
                },
                {
                    "sent": "Oh apologies, so starting from this state after I make the jump I have to add up.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of seeing the first observation given that I'm in one of these two states.",
                    "label": 0
                },
                {
                    "sent": "So for state one I'm going to accumulate the log probability of this observation given that I'm in state one and I'm going to store that number here with this candidate.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to do the same thing for State number 9 and I'm going to store that probability with this candidate.",
                    "label": 0
                },
                {
                    "sent": "I'm going to keep track of them as we go.",
                    "label": 1
                },
                {
                    "sent": "So then I generate all the places that I could end up next.",
                    "label": 0
                },
                {
                    "sent": "So from this state I could loop.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And stay here.",
                    "label": 0
                },
                {
                    "sent": "Or I could jump to state two and for each of these candidates I'm going to add on the probability the log probability of making whichever transition I actually.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just keeping a running sum of all of those.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do the same thing down here.",
                    "label": 0
                },
                {
                    "sent": "And then again, I'm going to look at the second observation, and given that I'm in this state or this state, this state, this state, I'm going to add the log probability of that.",
                    "label": 0
                },
                {
                    "sent": "So now I've got four different candidates.",
                    "label": 0
                },
                {
                    "sent": "Each candidate has its own running sum for how likely the observations are, and also how likely the state transitions are that it's seen.",
                    "label": 0
                },
                {
                    "sent": "So now I've got 4 numbers and I can rank them to just take the top two so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know the Gus sounding has sound or not very ambiguous, so hopefully my top two are up here.",
                    "label": 0
                },
                {
                    "sent": "And so I can.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of keep doing this can add the probability of the transition add in.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The observations again.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just keep on going and take the top two.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the top two.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up to.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Great, so now that we got to the end.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I generate the next set of candidates, right?",
                    "label": 0
                },
                {
                    "sent": "I can now hop through these big word arcs here so I can go from State 8.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can actually end up over here again and say the word high a second time where I could jump around here and I could end up saying guy so I can say hi guy afterward.",
                    "label": 0
                },
                {
                    "sent": "And so, in addition to adding on the log probability of whichever of these transitions I'm going to make.",
                    "label": 0
                },
                {
                    "sent": "I'm also going to add in the term that corresponds to the language model, so every time I actually see a new word start, I'm going to attack on the probability associated with it, yeah?",
                    "label": 0
                },
                {
                    "sent": "Oh these two yeah yeah so.",
                    "label": 0
                },
                {
                    "sent": "So I skipped a little step at the very beginning before I was kind of assuming that the words are equally probable so it wouldn't matter.",
                    "label": 0
                },
                {
                    "sent": "But normally if you want to know what is the probability of a word, you can use the prior probability.",
                    "label": 0
                },
                {
                    "sent": "And in fact what people often do is they'll add a special beginning of sentence token to their language model so that it's not just the prior probability.",
                    "label": 0
                },
                {
                    "sent": "I'll actually say what's the probability that my sentence or utterance starts with this specific word.",
                    "label": 0
                },
                {
                    "sent": "So good observation, thanks.",
                    "label": 0
                },
                {
                    "sent": "So after.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe I keep the top two and I end up with these and I can keep on going and at the end maybe I decide that these are the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Top two and that this is the best one.",
                    "label": 0
                },
                {
                    "sent": "So if I really said hi guy, this kind of the ideal situation that my beam search is figured out that by the time I'm done walking through these transitions I'm in the last state for the word guy.",
                    "label": 1
                },
                {
                    "sent": "News about how we run this beam search is that this guy, this little candidate here, has a history of all the transition transitions that it's made and it has the sum that tells me how likely my observations.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which actually is taking into account the language model taking into account the observations and everything.",
                    "label": 0
                },
                {
                    "sent": "So what we can actually do is look at the history.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three that this state has accumulated, and then I can actually just read off what the transcription is, so I know that we went through this phoneme.",
                    "label": 0
                },
                {
                    "sent": "This phoneme, this phoneme.",
                    "label": 0
                },
                {
                    "sent": "This phoneme on that corresponds to me saying hi guy.",
                    "label": 0
                },
                {
                    "sent": "Any questions about how this works?",
                    "label": 0
                },
                {
                    "sent": "No, so yeah, so if you want to, so I assume that we're going to find the most likely state.",
                    "label": 0
                },
                {
                    "sent": "That's what we're looking for.",
                    "label": 0
                },
                {
                    "sent": "If I wanted to do a much better job, what I should really be doing is summing over all of the different ways that I could possibly get here.",
                    "label": 0
                },
                {
                    "sent": "That's what I'd like to do, but that requires running forward, backward, and it makes the decoder a lot more complicated.",
                    "label": 0
                },
                {
                    "sent": "But if you go hunting through a textbook or research papers, you will find decoders that try to do this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's great.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you might be.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is that all?",
                    "label": 0
                },
                {
                    "sent": "This is all it takes to build a speech engine and then the short answer is definitely no.",
                    "label": 0
                },
                {
                    "sent": "So this is highly simplified so so people have sharply been pointing out places where we could have done better where we left something out to make it really simple.",
                    "label": 0
                },
                {
                    "sent": "But this is actually all of the big moving pieces, so we see someone talk about a decoder.",
                    "label": 1
                },
                {
                    "sent": "Now they're talking about a search algorithm that's going to hunt through this hmm and look for a good solution when they're talking about an acoustic model.",
                    "label": 0
                },
                {
                    "sent": "You've kind of seen how that works, even though we've been using.",
                    "label": 0
                },
                {
                    "sent": "A really simple one.",
                    "label": 0
                },
                {
                    "sent": "So these are the big pieces.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of the vocabulary, but to get it to work well, if you're going to build this from scratch, there's a raft of other things to do to try to make it work.",
                    "label": 0
                },
                {
                    "sent": "So one really common one that you'll see in the literature is that instead of using either simple states like I've used that are just kind of black box, part of the HMM, or just using individual phonemes to say each phoneme has its own hmm, they'll actually break that space up into things called triphones.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like a trigram in a language model.",
                    "label": 0
                },
                {
                    "sent": "I find all of the triplets of phonemes that occur in my data, and I might have a separate HMM for each possible triplet so that I can kind of capture the ways in which these sounds slower together when I put different phonemes with each other.",
                    "label": 0
                },
                {
                    "sent": "And then you might figure out that, wow, that's an awful lot of triphones, which means an awful lot of Hmm's and a lot of parameters that are tough to train.",
                    "label": 0
                },
                {
                    "sent": "And So what people will do is hunt for Tri phones that are kind of similar anti their parameters together.",
                    "label": 0
                },
                {
                    "sent": "So there's lots and lots of tricks to try to make this work well.",
                    "label": 1
                },
                {
                    "sent": "Similarly, we haven't talked at all about like normal noise filtering, for example, which is really important if you use something like MFC sees.",
                    "label": 0
                },
                {
                    "sent": "There's also just really cool stuff.",
                    "label": 0
                },
                {
                    "sent": "This is like my favorite part of speech.",
                    "label": 0
                },
                {
                    "sent": "Literature is the really cool things that people do to adapt to like speakers.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that one of the things you can try to do is listen to the pitch of someone's voice and guess hyperparameter like the vocal tract length of the person that's talking, and then use this to apply like a nonlinear.",
                    "label": 1
                },
                {
                    "sent": "Bend to their pitches to kind of move them into like a generic range so that your speech engine doesn't have to handle such a wide range of people, so there's lots of really neat tricks out there that you actually need to make a bleeding edge system.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into them 'cause I hope the deep learning will mean we don't need so many of these things anymore.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's tired of traditional speech models?",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, I am alright.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to the Bunny.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can deep learning helping this, so maybe a few years back if he was talking like 2000 eight 2010 something like that.",
                    "label": 0
                },
                {
                    "sent": "If you walked into an existing speech group they would have a very powerful, very well built speech engine based on a lot of concepts I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "And more recently, one of this sort of low hanging fruits has been to figure out alright which of these components is sort of the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "Which of these things do we really not like?",
                    "label": 0
                },
                {
                    "sent": "'cause it's constraining performance and let's just replace them with deep learning, which has much more powerful estimation abilities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic pipeline I started with at the beginning right has all of these different components, and so one of the things that we can do is start poking through these and figure out which ones deep learning might help with.",
                    "label": 0
                },
                {
                    "sent": "So for instance, with the acoustic model, maybe we can use deep learning to somehow compute this probability more accurately somehow make a better model there.",
                    "label": 0
                },
                {
                    "sent": "With language modeling.",
                    "label": 0
                },
                {
                    "sent": "I don't sound like maybe it hasn't been talked about this week, but you could also try to use a neural network to do language modeling.",
                    "label": 0
                },
                {
                    "sent": "And the hope is that if you do really well with deep learning, maybe some of these other things can go away, right?",
                    "label": 0
                },
                {
                    "sent": "People are asking like why do you want a spectrogram?",
                    "label": 0
                },
                {
                    "sent": "Why not just use an audio wave?",
                    "label": 1
                },
                {
                    "sent": "We kind of want that because these components aren't very good at dealing with raw audio, and maybe if we use deep learning for some of these, right to have a much more powerful model of our observations, maybe we wouldn't even need this anymore, and in fact there's this current work people actually trying to model directly from audio, so they could just jettison this whole piece.",
                    "label": 0
                },
                {
                    "sent": "And finally, this pronunciation dictionary, right that I kind of assumed away 'cause it makes things really complicated.",
                    "label": 0
                },
                {
                    "sent": "We really, really wish that we could just figure this out from data, right?",
                    "label": 0
                },
                {
                    "sent": "The idea that I'm going to somehow brute force write down all possible pronunciations or engineer a model that covers all the pronunciations.",
                    "label": 0
                },
                {
                    "sent": "That's a tough, tough job.",
                    "label": 0
                },
                {
                    "sent": "That's a tall order, and so with any luck deep learning, a lot of data, we won't have to touch these, so I'm going to focus on these two things.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one classic improvement.",
                    "label": 0
                },
                {
                    "sent": "In some ways, one that's been around for a while but is also responsible for a few recent big successes in improving speech engine's, is this idea of trying to enhance the acoustic model, making a much better model model of the observations given my sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "So we're still going to reuse all of the HMM machinery we still have this decoder.",
                    "label": 0
                },
                {
                    "sent": "We still have these Markov models, but we're just going to try to replace the probability of the observations given given the states.",
                    "label": 0
                },
                {
                    "sent": "So at the very beginning I assume this thing was like Gaussian, like a really really weak model, and now I'm going to try to replace it with something much more powerful.",
                    "label": 0
                },
                {
                    "sent": "So the one wrinkle here that you've got to think about is that if I start from my observations.",
                    "label": 0
                },
                {
                    "sent": "With deep learning, normally I'm trying to predict something right.",
                    "label": 0
                },
                {
                    "sent": "My all my super supervised learning algorithms are made to bottom up.",
                    "label": 0
                },
                {
                    "sent": "So maybe maybe Ruskin build us a generative model that goes top down and tells us this probability, but when we'd really like to use discriminative training to somehow make a prediction, that's going to help us 'cause we know how to do that really really well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We'd like to take in the observations and then and then predict some kind of label here because the technology for that is is, really, we've got down Pat.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we knew the target for S, right?",
                    "label": 1
                },
                {
                    "sent": "If we knew the state sequence that we wanted, it seems like it should be useful if the neural network could just predict it for us for a new utterance, right?",
                    "label": 0
                },
                {
                    "sent": "Like if I just gave you the audio and you had a brilliant neural network that you could just say, oh, it's this state, it's this state it's this state, then you would be done.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't need the HMMS at all.",
                    "label": 0
                },
                {
                    "sent": "So what we'd really love to have is something that takes in a speech frame, or maybe a small group of speech frames, plows it through a bunch of sigmoid or or rectified linear units in our neural network, and then spits out a softmax vector, a vector of probabilities for how likely are all the different states in my model.",
                    "label": 0
                },
                {
                    "sent": "This is what we really like to do.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is we need to get the targets for this.",
                    "label": 1
                },
                {
                    "sent": "Right now no one really bothers to furnish us with these states in advance.",
                    "label": 0
                },
                {
                    "sent": "So we can do a whole bunch of different things here to try to make it work.",
                    "label": 1
                },
                {
                    "sent": "One is to take a standard pipeline that we've already got.",
                    "label": 0
                },
                {
                    "sent": "Like if you went to the trouble of building a traditional system and now you're trying to super charge it by plugging in deep learning, you can actually use that to generate this sequence of states.",
                    "label": 0
                },
                {
                    "sent": "Go find, use our beam search decoder there to find the most likely sequence of states for this utterance for these observations, oh.",
                    "label": 1
                },
                {
                    "sent": "And then once we have the word labels, this is actually much easier.",
                    "label": 1
                },
                {
                    "sent": "We can actually just align them.",
                    "label": 0
                },
                {
                    "sent": "So if we have if we have labeled data right, we have other words we can generate the sequence of phonemes that I heard, and then I can use one of these algorithms to find what is the most likely sequence of states that corresponds to my input.",
                    "label": 0
                },
                {
                    "sent": "So it's just an alignment problem.",
                    "label": 0
                },
                {
                    "sent": "And then once I have this state sequence, I can just chop up all of these state and observation pairs and now I have a training set that I can use for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So that's one approach.",
                    "label": 0
                },
                {
                    "sent": "Other way to do it which you might have to do if you're building a system from scratch, is to go download a data set where someone has labeled this by hand.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you download TIMIT, someone has very, very mercifully gone through and labeled a lot of phonemes for you based on speech frames, and so you can actually train an observation model with a deep neural network from that and kind of get yourself started.",
                    "label": 0
                },
                {
                    "sent": "And then once you've done it, and you've got kind of a sort of weak, deep, deep neural net, you can rerun the alignment, retrain, and so on.",
                    "label": 0
                },
                {
                    "sent": "So the last option is to just try to train to predict phonemes directly.",
                    "label": 1
                },
                {
                    "sent": "So I've been talking about this hidden State S right, which is kind of the state within the Markov model for a particular phoneme.",
                    "label": 0
                },
                {
                    "sent": "But another option, since we happen to have lots of labeled sets of phonemes sitting around, is to just predict the phoneme directly and try to use that as my observations, and so there are different tricks you can play with trying to rephrase the problem to make this work.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The little thing you've got to be aware of though, is that we don't really want the probability of the state given the observation right, but that's not what our acoustic model wants in order to work, we really wants is the probability observation given the state, and that's how we can integrate this nicely into the HMM framework.",
                    "label": 0
                },
                {
                    "sent": "So that's OK. Bayes rule tells us what to do if we want the probability of the observations given the state, we can apply Bayes rule.",
                    "label": 1
                },
                {
                    "sent": "And then this term right here.",
                    "label": 0
                },
                {
                    "sent": "The probability of the observations is just a constant, right?",
                    "label": 0
                },
                {
                    "sent": "I know the observation, so this is just a constant and it turns out that if I drop this, it's just harmless.",
                    "label": 0
                },
                {
                    "sent": "There will be a constant that shows up in my recognizer and in my beam search, but it doesn't actually affect anything.",
                    "label": 0
                },
                {
                    "sent": "And so here's the thing I get from my neural network and I just have to remember to divide it by the prior probability of the state.",
                    "label": 0
                },
                {
                    "sent": "If I don't do this, my neural network could be amazing and I could wind up getting really bad.",
                    "label": 0
                },
                {
                    "sent": "Really bad performance of the end to end system.",
                    "label": 0
                },
                {
                    "sent": "So in practice, you can compute this by just taking a whole bunch of aligned data and taking the empirical probability of how often that state occurs.",
                    "label": 0
                },
                {
                    "sent": "For things like phonemes, some people will actually take.",
                    "label": 0
                },
                {
                    "sent": "Take very careful measurements of how often this phoneme actually occurs in human speech and just use a hard hard coded parameter.",
                    "label": 0
                },
                {
                    "sent": "So anyway, if you're trying to debug and you're finding that wow you've got this amazing neural net, but the pipeline just doesn't work, don't forget this step and it's a little strange that sometimes this is buried in papers or it's buried in books, so don't forget that this is there.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's one example of what you could do that's kind of like the first option.",
                    "label": 0
                },
                {
                    "sent": "So if I have a pipeline that kind of works, I could run Viterbi to find the most likely States and use that for a training set.",
                    "label": 0
                },
                {
                    "sent": "In practice, regardless of how you start out, people sort of do this iteratively, like I start with a weak model, I do my alignment train up, minor all net, and then I re run it again and I align it, retrain my neural net, and so on, almost like its own little EM procedure of going back and forth between aligning and retraining.",
                    "label": 0
                },
                {
                    "sent": "Great so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the reason I talk about this is really cool.",
                    "label": 0
                },
                {
                    "sent": "This is kind of like one of the first ways to get a big win in speech recognition, so this is actually a clip out of George dolls Paper from 2011 and where you can see here is that this combination of a deep belief network and in HMM system get about 69.6% accuracy and this is compared to a whole bunch of GMM HMM baselines.",
                    "label": 0
                },
                {
                    "sent": "So these are basically.",
                    "label": 0
                },
                {
                    "sent": "Hmm, systems like I was telling you about, but where the observation model is a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So you can see this is about a 10% relative improvement over the best one down here in the mix and he went up even more once you add things like Rolo and Dropout.",
                    "label": 0
                },
                {
                    "sent": "And of course much more recent state of the art papers are well beyond that, so this is really neat that this very simple idea using deep learning.",
                    "label": 0
                },
                {
                    "sent": "Plug it into the speech engine, actually give you a relative improvement that would be equivalent to enormous amounts of engineering.",
                    "label": 0
                },
                {
                    "sent": "If you go through the speech literature.",
                    "label": 0
                },
                {
                    "sent": "So you quit.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, replace this acoustic model these DNS with something else the framework doesn't especially care what lives down there.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a black box, so I was kind of assuming when I started that you guys have been seeing recurrent neural networks all week long and then roll include me in and said no, no, no we haven't seen seen that many of them yet, so I'm going to kind of run through this quickly.",
                    "label": 0
                },
                {
                    "sent": "But instead of just having deep neural Nets where we try to predict the probability of this state given observations, one of the things that turns out to be really useful is if these guys can fold in context.",
                    "label": 0
                },
                {
                    "sent": "So if this neuron up here, that's making a prediction, can somehow see the past and the future in the audio to make a better prediction.",
                    "label": 0
                },
                {
                    "sent": "So there are a couple of ways you can do that.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "You could have each of these neurons pull in a sort of big window of acoustic vectors of these observations that might help you a bit, but what's really popular to do now is to use a recurrent network, and So what?",
                    "label": 0
                },
                {
                    "sent": "A recurrent network is is really one DNN that you can think of as having like little self loops.",
                    "label": 0
                },
                {
                    "sent": "But once we unroll it over time, we sort of clone the deep neural network with these little recurrent loops being unrolled, and so now I have this grid.",
                    "label": 0
                },
                {
                    "sent": "But don't get, don't worry too much about it.",
                    "label": 0
                },
                {
                    "sent": "Once it's unrolled, it's just another deep neural network, right?",
                    "label": 0
                },
                {
                    "sent": "You can see that this is just a directed graph, so if I want to run backpropagation from my training algorithm, I can just run the same back properties like propagation algorithm that I would use for any other deep neural network.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is the beauty of the RNN is that this guy over here has effectively unlimited prior context, right?",
                    "label": 0
                },
                {
                    "sent": "If the neural net really wants to, and assuming I can get the optimization algorithm to workout, this neuron can actually get information transferred all the way from the beginning of the utterance, and something that I haven't drawn in the picture here is that's popular for systems.",
                    "label": 0
                },
                {
                    "sent": "In the research literature is I can also have a bidirectional RNN.",
                    "label": 0
                },
                {
                    "sent": "I can actually take future context and feed it back, and even though it seems like that might break it, you will be able to see it's actually still a deep neural network and you can run forward and backward propagation the same way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so if you use a bidirectional RNN, basically every single neuron can essentially see the entire utterance.",
                    "label": 0
                },
                {
                    "sent": "It has all the context it could want.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to make sure I get through some of the stuff on more modern deep learning things, so I'm just going to say about something about rescoring very quickly because it's a useful idea.",
                    "label": 0
                },
                {
                    "sent": "So a lot of these systems produce like an N best list, so you take your amazing speech engine, you crank some data through it, and for every piece of audio it'll generate a list of things that think you soup that you could have said along with a score.",
                    "label": 0
                },
                {
                    "sent": "And so one of the things that we can do is look at all those scores and try to use a more powerful model to go back for just that list.",
                    "label": 0
                },
                {
                    "sent": "Just that list of N Anri rank them basically so it was much cheaper than trying to run the decoder again, right?",
                    "label": 0
                },
                {
                    "sent": "Which has to search over all possible word sequences.",
                    "label": 0
                },
                {
                    "sent": "If I just have N word sequences, it's much cheaper to just check them all and see which one is the best so I can use a much slower or much more powerful algorithm.",
                    "label": 0
                },
                {
                    "sent": "So one exam.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full of place where this is kind of interesting is in the language model, so we played with the acoustic model and there there's some reasons that you might not use a neural language model in the system.",
                    "label": 0
                },
                {
                    "sent": "One of them is speed, but you can also use the language model for rescoring.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If I have a giant neural language model that takes in a lot of context if I only have 1000 sentances, it's very fast to evaluate them all here.",
                    "label": 0
                },
                {
                    "sent": "And so if I give you a couple of options from a speech transcription, right?",
                    "label": 0
                },
                {
                    "sent": "If someone saying I'm a connoisseur or looking for wine and pork chops.",
                    "label": 1
                },
                {
                    "sent": "That's not entirely unreasonable.",
                    "label": 0
                },
                {
                    "sent": "Where is this?",
                    "label": 0
                },
                {
                    "sent": "So it says I'm a connoisseur looking for wine and pork chops.",
                    "label": 1
                },
                {
                    "sent": "This sounds very similar and because of the sort of long range in context here, we figure kind of sewers are probably looking for wine and port shops, not wine and pork chops, but depending on the audio, maybe your recognizer has scored them in the wrong order, so something that you could use a neural language model for is to actually gobble up these words and try to predict how likely is the word pork chops to end this sentence versus the word port, followed by the word shops and so.",
                    "label": 0
                },
                {
                    "sent": "If your neural language model is much more powerful and unlike an ngram model that tends to have short context, if the neural language model can pull in this long range in context, it has a chance that it could reorder these things and get you the answer you want.",
                    "label": 0
                },
                {
                    "sent": "So this idea of rescoring is really useful 'cause you've got a big fat slow algorithm that you really wish you could use.",
                    "label": 0
                },
                {
                    "sent": "This gives you a place to plug it in where the speed doesn't hurt you so much.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so with the last little bit of.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm here, I want to talk about what is a very cool technology that I think is going to help make all of this other stuff obsolete.",
                    "label": 0
                },
                {
                    "sent": "Overtime, this is actually one approach.",
                    "label": 0
                },
                {
                    "sent": "There are others in the deep learning literature now, but I'm just going to talk about this one as an example.",
                    "label": 0
                },
                {
                    "sent": "So one of the problems with this whole pipeline up to now is that it's really complex.",
                    "label": 0
                },
                {
                    "sent": "We were chatting earlier about how to get the things started.",
                    "label": 0
                },
                {
                    "sent": "You kind of need labels to train your acoustic model and then you need to realign all your data and then you need to retrain it and realign and so on.",
                    "label": 0
                },
                {
                    "sent": "And if the alignment goes wrong then everything gets screwed up and you've got to go get hand labeled data.",
                    "label": 0
                },
                {
                    "sent": "It's a big mess.",
                    "label": 0
                },
                {
                    "sent": "There are all these different steps and a lot of them can breakdown.",
                    "label": 0
                },
                {
                    "sent": "So what we'd really love to do is to be able to train at least the acoustic model without worrying about this alignment problem.",
                    "label": 0
                },
                {
                    "sent": "To just say I don't care what the alignment is, I just want you to give me the transcription.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, finding the alignment makes the problem harder.",
                    "label": 0
                },
                {
                    "sent": "So one of the proposals by Alex Graves from at the time in your group is called Connectionstring connectionist temporal classification.",
                    "label": 0
                },
                {
                    "sent": "The tongue twister so everybody says CTC 'cause we don't like saying that.",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To tell you a little bit about how CTC works intuitively, I'm not going to go through the algorithm mathematics.",
                    "label": 0
                },
                {
                    "sent": "You can find them in the paper, it's pretty clear.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to do is to create a neural network that's going to output a sequence of probability vectors.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that we start with a sequence of feature vectors going in, and we're going to output a sequence of probability vectors with the same number of items.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be the same length in the end as our observation.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume somewhat naively, that the probability of any given sequence of phonemes is just the product of the probability of those phonemes for every single time step taken independently.",
                    "label": 1
                },
                {
                    "sent": "So I'm just going to take a product.",
                    "label": 0
                },
                {
                    "sent": "So in order to make this kind of workout right, because the sequences are the same length, I can't just take my 5 phonemes and try to spread them out over the whole axis, right?",
                    "label": 0
                },
                {
                    "sent": "So what CTC does is it introduces this blank character that you can kind of use to fill space in your predictions so that the.",
                    "label": 0
                },
                {
                    "sent": "The output sequence can be the same length as the input sequence, so to kind of show you how this would work is if we start with our spectrogram here, which is just a sequence of observation vectors.",
                    "label": 0
                },
                {
                    "sent": "We can crank this through a neural network or recurrent neural network, and then what we want to have out is a big grid of probabilities.",
                    "label": 0
                },
                {
                    "sent": "So each of these columns is one of the probability vectors.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "And each of these little boxes represents the probability of this symbol on the left, which right now are either phonemes or blanks.",
                    "label": 0
                },
                {
                    "sent": "So most of the time we sort of expect this thing to say, be saying blank, blank, blank, blank, blank, 'cause I don't know what's being said or I'm waiting for the next phoneme to show up.",
                    "label": 0
                },
                {
                    "sent": "And then as time goes by, hopefully the neural network says, oh I heard a phoneme.",
                    "label": 0
                },
                {
                    "sent": "I think I know it's been said, and it turns on the unit that represents that phoneme.",
                    "label": 0
                },
                {
                    "sent": "And it turns off the blank.",
                    "label": 0
                },
                {
                    "sent": "And then it goes back to predicting blank, blank, blank blank.",
                    "label": 0
                },
                {
                    "sent": "So, so this is kind of the basic idea for what we want the network to do.",
                    "label": 0
                },
                {
                    "sent": "So the reason for this?",
                    "label": 0
                },
                {
                    "sent": "Is that we don't know the phoneme alignment, advance, right?",
                    "label": 0
                },
                {
                    "sent": "So we can't just train this directly, so we want to somehow figure out a way to train this neural network that's generating these probability vectors without knowing when those spikes are supposed to happen.",
                    "label": 0
                },
                {
                    "sent": "We know that those phonemes should show up somewhere, but we don't care where they are.",
                    "label": 0
                },
                {
                    "sent": "We want to somehow be agnostic to this.",
                    "label": 0
                },
                {
                    "sent": "So pre.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the way we did this, where was that we let the EM algorithm guess we just said pick some alignments, tell me what you think is reasonable, and then I'm just going to retrain.",
                    "label": 0
                },
                {
                    "sent": "Assuming that you got the right answer and that makes it complicated.",
                    "label": 1
                },
                {
                    "sent": "But what we really want to have happen is for the alignment to be irrelevant, so that doesn't bother us.",
                    "label": 1
                },
                {
                    "sent": "So the solution idea that the CTC paper introduces is an operation that makes it so that transcribing from from a given utterance or from a given observation to the phonemes that represent your words is invariant to all these shifts and changes.",
                    "label": 0
                },
                {
                    "sent": "So if I change the alignment, I want to get the same transcription in the end, I just want to take all of that out and be invariant to it.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what they do is they introduce this operator and so let's.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we start out with a network that's already trained up to do the right thing, right?",
                    "label": 1
                },
                {
                    "sent": "And let's think about drawing a string from that distribution.",
                    "label": 1
                },
                {
                    "sent": "So I look at the first set of probability vectors and I sample one of the symbols.",
                    "label": 0
                },
                {
                    "sent": "When I move over to the next probability vector and I sample assemble and I just keep on going.",
                    "label": 0
                },
                {
                    "sent": "What you might see is something like this.",
                    "label": 0
                },
                {
                    "sent": "So for the word hello maybe I say blank, blank, blank, blank.",
                    "label": 0
                },
                {
                    "sent": "When I say ha ha ha ha ha for a little while to represent the the H noise and blank blank blank for awhile until I see another phoneme and so on.",
                    "label": 0
                },
                {
                    "sent": "The collapsing operator.",
                    "label": 0
                },
                {
                    "sent": "What it does is it goes through and it removes all of the duplicates.",
                    "label": 0
                },
                {
                    "sent": "All of the repeated phonemes because it figures it well adjacent predictions that are seeing the same thing.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to wipe out all the blanks.",
                    "label": 1
                },
                {
                    "sent": "I don't care about that that's not part of my transcription.",
                    "label": 0
                },
                {
                    "sent": "And so this would actually just collapse to hello.",
                    "label": 1
                },
                {
                    "sent": "And the important aspect of this is that under this observation, all of these different potential transcriptions of blanks and phonemes, all mapped to the same thing.",
                    "label": 0
                },
                {
                    "sent": "So these are all perfectly legitimate outputs.",
                    "label": 0
                },
                {
                    "sent": "If the CTC Network says that this is a good one, that's OK with me.",
                    "label": 0
                },
                {
                    "sent": "If it says that this is a good one, that's also OK with me.",
                    "label": 0
                },
                {
                    "sent": "So I'm kind of now agnostic to the alignment, right?",
                    "label": 0
                },
                {
                    "sent": "I can move these things around and I just don't care.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So in order to actually do training or to do anything of value, what we need to be able to compute is the likelihood of one of the label sequences.",
                    "label": 1
                },
                {
                    "sent": "So for example, if I say hello is my, is my training label.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is sum over all of the possible alignments, right?",
                    "label": 1
                },
                {
                    "sent": "I want to do this marginalization.",
                    "label": 1
                },
                {
                    "sent": "Basically, that was really hard to do in the previous settings.",
                    "label": 0
                },
                {
                    "sent": "All the possible transcriptions from my network that could collapse to give me this result, 'cause I don't care which one of you pick, I'm just going to take them all and assign them to the same word.",
                    "label": 0
                },
                {
                    "sent": "So basically if I take this probability here, I'm going to break it out as the sum of all of the different transcriptions.",
                    "label": 0
                },
                {
                    "sent": "So here's the three from the last page and you can imagine going through all of the combinatorially many choices.",
                    "label": 0
                },
                {
                    "sent": "And so the nice thing here is that graves and coauthors gave a forward backward algorithm, a modification of the forward backward algorithm you would use for in HMM, for example, to actually compute this summation efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to train this thing, what we really want to do is do gradient gradient descent to maximize the probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, here's this kind of gnarly summation that has combinatorially many utterances that could collapse to give me the right transcription or combinatorially many sequences that could give me the right transcription and the good news is that I can actually compute this thing using this forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "The forward backward algorithm will have a bunch of intermediate's that it generates to do its work, and if you just take those things, you can actually compute this gradient and it's just in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So if we actually run this training algorithm, it's really cruel in my opinion to see how this thing does its work.",
                    "label": 0
                },
                {
                    "sent": "So the way to interpret this graph, let's start with the first one here is that on the vertical axis is the probability of a particular symbol, say like the first, say the blank or the probability of the huh sound plotted overtime.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this is like watching one neuron overtime and looking at when it turns on and turns off.",
                    "label": 0
                },
                {
                    "sent": "And that's the same for all of these.",
                    "label": 0
                },
                {
                    "sent": "Horizontal axis is time.",
                    "label": 0
                },
                {
                    "sent": "So when the system starts out, training the actual outputs are just going to be 0 or garbage, right?",
                    "label": 0
                },
                {
                    "sent": "Just say I I don't know what's being said.",
                    "label": 0
                },
                {
                    "sent": "Everything is just not going to be outputing very much, but if you look at the gradient, which is like the error signal, it's what your label is telling you to change in your probability vectors.",
                    "label": 0
                },
                {
                    "sent": "You'll see that the algorithm is trying to sneak everything blank, so please do something just just output blanks and then it's going to take a very rough partitioning of all the phonemes in your transcription and just kind of spread them out.",
                    "label": 0
                },
                {
                    "sent": "It's going to say you know what?",
                    "label": 0
                },
                {
                    "sent": "The H sound that's somewhere over here.",
                    "label": 0
                },
                {
                    "sent": "You should try to say H. The sound is somewhere here.",
                    "label": 0
                },
                {
                    "sent": "You should try to make more probable and overtime it kind of lines itself up so that you're starting to get little peaks where the sounds are, and by the time you're finished, CTC tends to be actually very confident that it knows exactly what phonemes are being said and exactly when they've been said, and your error will slowly go down to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty cool result because it frees us basically from having to deal with all of these alignment problems, which is a big challenge for like traditional systems, but is also a big enabler for new systems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We still have to do decoding with this.",
                    "label": 0
                },
                {
                    "sent": "We have a sequence of probability vectors, so it turns out that if we want to compute this.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a couple of options.",
                    "label": 0
                },
                {
                    "sent": "One is the quick and dirty solution.",
                    "label": 1
                },
                {
                    "sent": "If I just say you know what?",
                    "label": 0
                },
                {
                    "sent": "I just want to find the Mac.",
                    "label": 0
                },
                {
                    "sent": "The most probable sequence and then I'm going to collapse it.",
                    "label": 0
                },
                {
                    "sent": "Kind of makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "But it turns out this is not going to give you the best answer, right?",
                    "label": 0
                },
                {
                    "sent": "This is not accounting for all of the ambiguities that can come up, but it will actually give you something reasonable.",
                    "label": 0
                },
                {
                    "sent": "But it's a useful sanity check.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order though, and This is why I went through beam search earlier is that you can use a lot of the same ideas, the same search algorithms to go hunting through sequences of characters or phonemes in these probability vectors, you can incorporate a language model if you'd like.",
                    "label": 0
                },
                {
                    "sent": "And this will allow you to basically generate a transcription of phonemes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another way to do it, and in fact Alex Graves himself, has used this in a couple of papers for results so that you can take those probability vectors.",
                    "label": 0
                },
                {
                    "sent": "And again running beam search might be slow, but every time we have a slow algorithm, we can always kind of punt and use it for rescoring.",
                    "label": 0
                },
                {
                    "sent": "So if I take a list of phonemes transcribed from my really good speech engine and then re score them with this, I might be able to do better.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's no fundamental reason why we have to use phonemes.",
                    "label": 0
                },
                {
                    "sent": "There's no reason why we have to do this.",
                    "label": 0
                },
                {
                    "sent": "It's partly because it meshes well with all of the existing systems, so a really cool future direction of deep learning, and which is happening right now at Baidu in many places is trying to get rid of all of this infrastructure.",
                    "label": 0
                },
                {
                    "sent": "And instead of using phonemes, why not just have CTC generate characters?",
                    "label": 0
                },
                {
                    "sent": "Just have it out, put it all on its own.",
                    "label": 0
                },
                {
                    "sent": "So Grayson gently try to do this in acnl.",
                    "label": 0
                },
                {
                    "sent": "We still probably want a language model in here even though we're outputting characters, but the high level point here is that once you have CTC working and you're really good at working with unaligned transcriptions, you don't need big changes to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The same algorithm will actually work and yet it's much much much simpler to build.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is actually a snapshot from Alex is last paper on this where you can actually see that CTC is doing the same thing it did for phonemes.",
                    "label": 0
                },
                {
                    "sent": "You can actually see it a lot if I say his friends will actually say, well, that sounds like an H. Sounds like an eye and it's folding in context from all of the all of the observation vectors to just make the right prediction.",
                    "label": 0
                },
                {
                    "sent": "And we don't care about the alignment anymore because of this CTC loss.",
                    "label": 0
                },
                {
                    "sent": "One sort of funny thing to be aware of is that this introduces a new problem, which is that CTC kind of learns how to spell like a like a 5 year old.",
                    "label": 0
                },
                {
                    "sent": "If it is not like if you say to illustrate a point so it's like 2 hours straight, the point.",
                    "label": 1
                },
                {
                    "sent": "This is an example from Alex is paper and these are really hard to fix because your language model doesn't really get to see what's going on underneath here.",
                    "label": 1
                },
                {
                    "sent": "And so the fact that these sound exactly the same is a little tough to deal with after the fact, and finally.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples from from paper actually from our group recently where if you look at the Max decoding you can kind of see what the neural net wants to to output on its own.",
                    "label": 0
                },
                {
                    "sent": "So sounds like Boston with an I.",
                    "label": 0
                },
                {
                    "sent": "We were trying to figure this out.",
                    "label": 0
                },
                {
                    "sent": "Prime Minister Narendra Modi.",
                    "label": 0
                },
                {
                    "sent": "But if you use a language model on top of this with the language model, decoder actually knows who Narendra Modi is.",
                    "label": 0
                },
                {
                    "sent": "And similarly, Arthur, any tickets for the game sort of fanatically works out, but you need a language model to really find the proper spellings, and one of the things that really vexed us for a long time.",
                    "label": 0
                },
                {
                    "sent": "We started calling this the Tchaikovsky problem, because if someone says, oh, I'd like to hear something by Tchaikovsky.",
                    "label": 0
                },
                {
                    "sent": "The phonetic you know transcription of this that CTC outputs is actually completely reasonable.",
                    "label": 0
                },
                {
                    "sent": "But unless you happen to have heard this before and have seen someone write it down, you have no idea why Tchaikovsky would start with a T. And so this is one of the reasons, actually, that they sort of phoneme models from previous work still kind of have some things to like because they're a little bit robust to these sorts of spelling errors that the neural net might make.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So high level conclusion is that these traditional HMM and DM Ed DN hybrids are still really common out there.",
                    "label": 0
                },
                {
                    "sent": "If you go do an internship in your work on a speech engine somewhere, you may well see a whole bunch of this stuff, and so I hope you've seen a few places where deep learning has plugged in in the past, and certainly there are a lot of others, so so it's sort of a way to supercharge the existing systems we've got and more recently, CTC, especially, but also now, is like attentional models, and some of the more powerful recurrent Nets.",
                    "label": 0
                },
                {
                    "sent": "We're starting to see.",
                    "label": 0
                },
                {
                    "sent": "There's there's a chance that we're going to be able to replace all of this stuff soon with sort of the next wave of deep learning systems that won't won't have a lot of these caveats and can just learn from data and do a much better job, and at least for the Tchaikovsky problem, I think the system at Baidu Now gets this right 'cause it's actually heard enough audio and seen enough things that it understands that there's a chance that it starts with a T. And so I think the next wave of deep learning system should be even better.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You a lot and I do just want to say really quickly thanks to to anyone who's actually the real speech expert at Baidu for a lot of his help on this, I really appreciate it.",
                    "label": 0
                }
            ]
        }
    }
}