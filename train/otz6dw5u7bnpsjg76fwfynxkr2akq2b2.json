{
    "id": "otz6dw5u7bnpsjg76fwfynxkr2akq2b2",
    "title": "Latent Force Models with Gaussian Processes",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/bark08_lawrence_lfmwgp/",
    "segmentation": [
        [
            "OK.",
            "So welcome back after the break.",
            "I'll now introduce myself.",
            "I'm going to talk about something that we sort of calling latent force models with Gaussian process, but it relates nicely to some of the stuff that some of the concepts that Chris talked about in the last talk, and I think it will relate to future stuff that other people will talk about as well.",
            "So I was sort of a later entry when someone dropped out, so I don't really have nice philosophical things to say in the talk, it's."
        ],
        [
            "More like.",
            "Well, some of its philosophical, but it's I don't know.",
            "I think I don't think it's controversial.",
            "So."
        ],
        [
            "So."
        ],
        [
            "I'm going to start with an introduction.",
            "And what I'm going to do is I'm just going to look at sort of standard approach to dealing with data that we all use, and I want to extend it as we go along.",
            "So let's say I've got.",
            "I've got some data X and I've got some latent variables F. Normally I use Wi-Fi data and X for latent variables.",
            "But the reason I've done this will become clear.",
            "So one thing we can do with our data is just come up with a reduced dimensionality reduction of our data, which is to say that add a tourism, some latent variables, times where the number of latent variables that dimensionality Q is less than the dimensionality of our data D times some weighting matrix plus some noise.",
            "And we know this to be a model behind factor analysis, principal component analysis, independent component analysis, various models conform to this.",
            "So we integrate out or X very often, and we optimize with respect to W. Now, for temporal data and a particular Gaussian prior in the latent space.",
            "So the common filter or smoother.",
            "You can make this a common filter, so if I have prior over F that is a particular Gaussian form that is temporal, so it's got the same dimension N of the prior has a full set of work.",
            "It's gotta try diagonal covariance.",
            "I can get this common filter.",
            "More generally, though, we could consider, say, Gaussian process prior over F. So this model Chris mentioned it briefly, but this is actually already been proposed that you've got several F. You take each column of F to be independent, and you place a Gaussian process prior independently over each F separately.",
            "If you allow each of these covariances to be different, then I believe this is just the semi parametric latent factor model of TE, Zager and Jordan.",
            "It was discussed before.",
            "So this sort of this sort of model you can see what I'm trying to do is take a model that we used to seeing and by putting into temporal prior.",
            "OK.",
            "So in that case you could have any input you don't just have to have time.",
            "You could have any inputs, so I'm sort of saying this F here could either be time based, or it could have some inputs.",
            "It could be a Gaussian pro."
        ],
        [
            "Assess so.",
            "Traditionally, though, we prefer."
        ],
        [
            "Not to do Gaussian process is oh, in the time case over this sort of model Huawei because of cubic complexity in the length of."
        ],
        [
            "Time series so a common filter smoother approach has been preferred and that gives us a sort of.",
            "Here's my chair.",
            "That gives us a complexity which is linear in the amount of time and quadratic in the number of states.",
            "So we normally do this sort of thing, but.",
            "Well, I'd like to say is the advances in sparse approximations, in particular ones.",
            "I'm thinking of it's Nelson and Zubin's approach to ask parametric Gaussian process is and we're keen and Carl's.",
            "Generalization of that to the what they call the partial independent training conditional and also Michalis will talk about another sparse variant which I think is also useful.",
            "This is meant we've got these approximate inference techniques that actually have the same complexity as the common filter, so I think in some sense we can now think of looking beyond those, filter methods to these Gaussian process based methods to have a richer model at the expense of some approximations in the inference.",
            "But I think as Michalis will show some of those approximations can be really very good.",
            "Without"
        ],
        [
            "Great computational cost.",
            "OK so but I want to extend those sort of models again.",
            "I want to use that sort of way of thinking about these models to extend further so the models rely on a latent variables to provide dynamic information here.",
            "So in the common filter you've gotta sort of Gaussian random walk in the latent space and in the Gaussian process example you've got a temporal function.",
            "You could view the common filter as a particular Gaussian process, but what I want to do is.",
            "Push this model little bit further by using a mechanistic inspiration so.",
            "We talked a bit about physics and we talked a bit about the sort of models people using physics and what we sort of.",
            "So I think I I read this book on the history of stats recently and it had a lot of stuff.",
            "A lot of material on why it was that it took, let's say, 150 years to go while less than that, maybe over 100 years to go from Gauss's application of least squares in an astronomical system to application of least squares to the data that they were looking at just around the turn of the last century.",
            "So it took about 100 years to say, oh, I can do regression on this data.",
            "That's pretty amazing when you think about.",
            "What a well used technique regression is and the explanation given was that.",
            "Because when they were looking at statistics about 100 years ago from now, they want they didn't actually have a model in mind about the way things happened.",
            "They didn't have a Newtonian view of the world that the argument was that because of mutant people had this view that Amos was talking referring to earlier as an example of a parametric model that you believe in and that you could fit the parameters.",
            "This parametric model within the context of a theory of errors, but 100 years ago for SoC data where you don't know the underlying generating thing, you couldn't do that.",
            "You couldn't say what your parametric model was.",
            "And the argument was with.",
            "That was why they took a long time to look to methods that have been used in astronomy for 100 years.",
            "But what I'd like to do is take some of the ideas that are used in those Newtonian models and try and pull them back into machine learning, not because we know that their present in exactly the way we're going to model, but because we can incorporate those things and they have certain characteristics, including things like resonance or inertia, which are actually quite difficult to model in the base case, so I don't think it quite worked exactly for Chris is.",
            "Robot kinematics example in the way I'll introduce it, but he sort of did that there.",
            "He sort of said I have knowledge about the Coulomb friction and I'm going to include a covariance function in my model that handles that knowledge.",
            "And then I expect my machine learning part to just deal with the rest of the model on its own, and this is the this is the same sort of idea with these sort of models.",
            "So physical interpretation of the thing we saw before the latent functions, RQ forces and we observe the displacement of D Springs.",
            "To these forces, we interpret the system as the force balance equation X D = F S. So S is a matrix of sensitivities, which says that we've got a limited number of Q forces, but they may be operating through some set of levers on the Springs, so they could increase the value of their force, or decrease the value of their force, change the sign so and so forth.",
            "So we've got some.",
            "Or they could be pulleys or whatever your favorite thought.",
            "So we've got some sort of simple force balance equation there, and then the original system is recovered just by taking that matrix sensitivities and multiplying by the inverse of the spring constants.",
            "So that is a non.",
            "It's a mechanical system, but there's no dynamics.",
            "It just moves because."
        ],
        [
            "Notes for differential equation.",
            "So the suggestion is that what you can do is do things like Abadam PO and give the system mass.",
            "So here's a first derivative of the system.",
            "Multiply by damping coefficient and the system's mass.",
            "The acceleration is multiplied by the mass, and now we've got a second order mechanical system.",
            "Which we are interested in.",
            "So it's actually exactly the same it's.",
            "I mean it's a generalization of PCA or factor analysis, or any of those models you'd like to mention, or common filter.",
            "As long as you put the right prior over F and then the right values of these MC ends, and then you also have some decision about your noise.",
            "But the nice thing about this system is it will exhibit inertia and resonance.",
            "So these are characteristics that are quite difficult to just put into, say, a standard neural network.",
            "And there are many systems that can be represented by differential equations, so this is a temporal one.",
            "But other things we've been looking at includes partial differential equations, where you may be driving in a sort of heat equation.",
            "The temperature at a certain point, and you're looking at the temperature or the response of the system across plate.",
            "So there's various sort of systems you can think of as being like this, and I want to call those systems in general latent force models, so their models where there's some latent.",
            "Or switch we're actually going to Gaussian process prior for in this talk, but you could consider at your favorite prior.",
            "It's just easier if you're using a Gaussian process and those latent forces are driving a system which has some dynamics and physical characteristics which we may or may not believe.",
            "Our existence in the actual system, but the point is, we've got parameters that we can learn."
        ],
        [
            "To try and determine if they're there.",
            "OK, the nice thing is, for Gaussian process we can compute covariance matrices for the output displacement.",
            "So for one displacement that should say for one displacement the model looks like this you've got this summer forces will actually look at one force for the moment because the sum of forces just means you can sum the covariance functions in the end.",
            "So it's trivial to extend it to multiple forces 'cause it's just adding up the covariance functions.",
            "So it turns out nicely for this covariance function.",
            "You can solve this differential equation for X and the result is a convolution and you can do."
        ],
        [
            "Convolution tractably you can't only do that convolution tractably the colors are a bit funny, but you can actually do the double convolution.",
            "You need to compute the covariance.",
            "So Chris Williams in his very nice stochastic process introduction which I didn't see live but saw on videolectures.net talks about the."
        ],
        [
            "Case where you've got a system like this and you're driving it by Gaussian noise as your force.",
            "So that's sort of like PCA example and then you get out a particular covariance functions.",
            "I'm considering the case where you have an RBF covariance here, so there's something going on."
        ],
        [
            "In the latent space.",
            "Overtime.",
            "Now, this is a visualization of the covariance function you get, so the top corner here F of T is the auto covariance for the latent function.",
            "So it's just an RBF covariance.",
            "Colors are slightly different.",
            "On my hope we don't lose color later on.",
            "But what you're seeing, then, is you're seeing some different displacements, which are the result of the application of this force and each different displacement has a different mass, spring and damper associated with it, and you can summarize it in a better way by talking about what the damping ratio is.",
            "So the first one here, which I've denoted why it should say X, is underdamped.",
            "So this one's got resonance.",
            "So actually it has an input and then you can see you get these bars.",
            "He's got resonant frequency which is dependent on the selection of those parameters.",
            "This one here is critically damped.",
            "So actually he tries to follow that as best as possible, so that's critically damped, and this one here is over damped, so it's a damped version of the original signal.",
            "So."
        ],
        [
            "I had a line which I've given this talk once before and I've used it."
        ],
        [
            "Once before so.",
            "When you first sample from Gaussian process covariance, I have this analogy that, like it's like the first time you meet your wife and you fall in love because it's amazing that you get these functions coming out of this thing that you've just written down.",
            "It just looks beautiful.",
            "I think when you sample from this covariance.",
            "An example from this covariance.",
            "It's kind of like when your wife does something and you remember why you fell in love all over again."
        ],
        [
            "For these these are the samples from this covariance.",
            "So what you're seeing here?"
        ],
        [
            "There is a joint sample, so we've got 1234 functions, so this is like Chris Williams is stuff in the previous talk.",
            "This multi outputs right?",
            "So you've got one output here which is F. In fact it would be more similar to think of marginalizing with respect to F and thinking of three outputs because they're sort of then similar.",
            "But we've got four outputs and each is the sample, then gives you function one, function 2, function 3, function 4."
        ],
        [
            "For what you're interested in, you can pull up these and what I've done here is plotted that for the scion being the driving function.",
            "The red is the.",
            "Under damped system.",
            "So that's got a natural frequency, but because it's not getting hit at the right times by the driving function here, that depends on the variance between the length scale and the.",
            "Well, it depends on coincidence as well, but it's also it depends on when the length scale, what the length scale is as well.",
            "You don't see a lot of the resonance in the red.",
            "Then you've got the critically damped.",
            "Is the blue and the green is overdamped, so it's a more."
        ],
        [
            "Version of the original signal.",
            "K in this case you start to see some of the resonance because it gets banged at the right time.",
            "So it happens to be bang there and there.",
            "So the red actually goes quite large because it's starting to exhibit its natural frequency, doesn't have the same frequency as the latent function, but it's getting banged at the right time, so it starts to exhibit some of its underlying properties.",
            "Again."
        ],
        [
            "Greed, overdamped and so and so forth.",
            "So what you're actually doing here when you sample this system?",
            "I mean you solve the differential equation.",
            "By doing that you need to do a double integral over your covariance function, which is a form of two convolutions.",
            "But you're actually solving the differential equation for each sample F simultaneously with the X is, which is, which is quite fun."
        ],
        [
            "OK, and there's another example.",
            "So now I wanted to talk.",
            "We haven't had this nasty thing about the double convolution to get.",
            "This is actually you end up using the complex Earth function to compute the covariance.",
            "So I've got two guys Maricio who's here and David Lang Garcia been working on this and it's quite slow to compute, so getting results on seriously large data is not something we have yet.",
            "So I'm actually going to drop back to, but we believe it's possible we're getting fast enough that we think it will do something nice."
        ],
        [
            "The complex Earth appears because you re expressed that as a complex exponential and then you combine that with this, which is your Gaussian process covariance when you do the try and compute the actual covariance and then you're going to integrate from nought to T. So it turns out to be a complex Gaussian.",
            "So you get the complex Earth as a result and some other nasty terms.",
            "And there's a few.",
            "It's just one of those you know.",
            "Sometimes when you need a particular function and you look up when the Fortran version of it was first published efficiently.",
            "It's always like in the 1960s, these ones are done in like the 1980s, and their ongoing most efficient version.",
            "So it's not.",
            "You know, it's it's we're getting there in terms of improving our Matlab code really to get."
        ],
        [
            "To speed out, so we expect results on that soon.",
            "But so I'm going to drop back to something that's easier, which just involves because there's no sign in this convolution.",
            "This is the first order differential equation, and it's just involves the Earth function in the actual covariance you get, so this is quicker to compute because it's already built into Matlab, etc."
        ],
        [
            "So we've used this one as a model of gene transcription.",
            "Now, in some cases we think it's an accurate model of gene transcription, but the sort of things we hope to do with it is.",
            "We also know it's not accurate.",
            "Because of this linear response here is genuinely a nonlinear response.",
            "In most cases.",
            "Combine it with other Gaussian process.",
            "Regressor is to try and pick up when you're in the nonlinear response regime.",
            "So you sort of pick a broken model which has some of the characteristics that you want, and then you try and combine it with a model that emits a simpler model, but it describes well.",
            "It's a more it can describe more datasets so.",
            "This model here is quite constrained.",
            "In the datasets it can describe for the multiple outputs, so it will fail to describe a nonlinear response if the nonlinearity is being used.",
            "So that's the sort of idea with the."
        ],
        [
            "Sensitive MoD."
        ],
        [
            "So this is a."
        ],
        [
            "Example that's being used for gene transcription by Branco at all, and that's the."
        ],
        [
            "The model that they used.",
            "You can see this is the covariance for this model, so it's the same thing we saw before, but now we don't have any resonance 'cause there's no mass in this system.",
            "It's effectively spring damper system."
        ],
        [
            "There's just sort of some damping.",
            "OK, so I want to show you now now."
        ],
        [
            "Is a toy example that use realistic parameters from brain."
        ],
        [
            "Those data, but I want to show you is how you can do inference in this sort of system, and we're actually know in this case that the system doesn't start until time 0.",
            "So at times zero, the system starts operating, but.",
            "We know that we've had.",
            "That was an observation on F, so we know that at times zero, the function is doing nothing.",
            "It just happens.",
            "That's the sensible thing to do.",
            "In this case you can have it, but you haven't observed what's going on at time 0, so we're going to see observations appearing here these."
        ],
        [
            "Output jeans, so this is gene transcription.",
            "So in Gene Tran."
        ],
        [
            "Scription, what you have is a rate of production of a particular gene.",
            "Given an underlying protein that is called."
        ],
        [
            "For it to be produced."
        ],
        [
            "So what we're going to observe is some artificially generated genes from a known F. So this is a known protein.",
            "And as we observe them coming in, what we'll see is that as we observe, more genes will see that we start to get a better idea of what the latent F function was.",
            "This model you're not seeing the learning here, but this was learned in the 1st place, so the parameters I'm using here, I learned from the toy data."
        ],
        [
            "Sample Daniel."
        ],
        [
            "There's not a lot of data."
        ],
        [
            "And it's quite high now."
        ],
        [
            "So."
        ],
        [
            "Even the genuine under."
        ],
        [
            "Buying jeans."
        ],
        [
            "As I add the."
        ],
        [
            "Teens in"
        ],
        [
            "You start to see."
        ],
        [
            "Need the."
        ],
        [
            "Stereo Rover."
        ],
        [
            "Imp."
        ],
        [
            "Cruise."
        ],
        [
            "So they've got."
        ],
        [
            "Two bumps"
        ],
        [
            "Roll."
        ],
        [
            "Quite high sensitive."
        ],
        [
            "In quite high decay."
        ],
        [
            "So in some way."
        ],
        [
            "Is there quite an easy?"
        ],
        [
            "Example."
        ],
        [
            "To do by clustering."
        ],
        [
            "Well, but."
        ],
        [
            "I'll show you an example."
        ],
        [
            "Later that isn't."
        ],
        [
            "So as you go."
        ],
        [
            "Can you gain?"
        ],
        [
            "The inference."
        ],
        [
            "What's going on?"
        ],
        [
            "And even though."
        ],
        [
            "So here in."
        ],
        [
            "These cases, the noise sometimes outweighs the actual bumps in the genes have added the noise on.",
            "You still managed to recover and learn the parameters in this case of the actual system, so these are parameters that are difficult to measure typically, and so that's sort of interest.",
            "I should add that this is right on the edge of not being able to find the parameters 'cause I did it once and it got the parameters and I thought I better check for robustness and I did it again and it didn't get the parameters so you can learn a system of this sort of high level noise, but you."
        ],
        [
            "On the edge of what you might expect to do.",
            "OK, so."
        ],
        [
            "This is a P53 system.",
            "These are so cool."
        ],
        [
            "You can find these pics."
        ],
        [
            "On the web of what molecules look like.",
            "So in P53 you've got this 14 function.",
            "Is this protein which is driving the transcription of several genes?",
            "So these are actual observations and this is the system that Martina Franca."
        ],
        [
            "So looked at and we sort of then looked at it again with this model and this is.",
            "We've looked at it in the NIPS paper, but this is updated results with improvements to the system, which is an ECB paper coming this year and the work was done in collaboration with Magnus, and the Post was paid out.",
            "So there's.",
            "Observations these crosses with error bars or Arab are based observations of the gene expression from microarray data and this is the inference of the protein from the linear system.",
            "An insert a nice result, it can.",
            "It's similar to what Barranco derived, but the error bars are sort of slightly lower and the parameters make sense.",
            "I mean, it's difficult to validate some of these things.",
            "One thing you might complain about that he said, well, these look all the same.",
            "You could have just on PCA.",
            "You could have just done something linear.",
            "The decay insensitivity you know what was the use of your differential equation, and you're right, because actually in this case the decays and sensitivities are relatively high, so that means that the genes tend to track what how the protein does, and this must happen.",
            "Otherwise clustering of microarray data just wouldn't tell you."
        ],
        [
            "Anything interesting?"
        ],
        [
            "Here's an example where it doesn't happen.",
            "So this is a bunch of results from systems work by Jennifer Weathers an it's elk one is the transcription factor in this case and these are some of the targets so you can see these are the training jeans here.",
            "Now a lot of them just go up and down so they're high sensitivity, high decay, but this one goes up and stays up and you can't explain that you won't cluster that with that.",
            "But you can't explain it with a diff."
        ],
        [
            "Equation model.",
            "So in this case you can then use your model of what's going on based on these six jeans that you know a targets to search for other targets.",
            "And here's, for example, are predicted target gene.",
            "You can't quite see the Red Cross apologize, but there, within the prediction of the model.",
            "And then there's other genes in this side which aren't within the prediction of the model.",
            "How about dinner time?",
            "OK."
        ],
        [
            "So that's nice.",
            "This was really so.",
            "I guess originally it was Magnus myself, and Greedo Sanguineti when he was a postdoc at the start."
        ],
        [
            "Work on these sort of systems and it was inspired by this talk by Martino.",
            "That was saying I have this simple linear model so we can do Gaussian processes on that."
        ],
        [
            "I think the."
        ],
        [
            "Thinking."
        ],
        [
            "Now on the SSD."
        ],
        [
            "In terms of machine learning is well, why not use these sort of models more?",
            "In general, when you don't necessarily believe the system so much now we don't have all the answers for how to do that."
        ],
        [
            "But one of the questions that immediately comes up is this one.",
            "So this is work by Mauricio Alvarez.",
            "And this is going to appear at NIPS this year.",
            "I can say now.",
            "So the solutions to these differential equations and others are all typically forms of convolution.",
            "So what you get is the result of your displacement in the mechanical case, but it could be a sort of temperature response.",
            "Could be anything that's in your physical model is typically the integral of your driving latent force times some kernel function plus some other function.",
            "Now this integral, if it's a Fourier transform type integral, could be from minus Infinity to Infinity, but if it's a Laplace transform, or if there are boundary conditions, it could be a finite integral.",
            "So whether it's tractable or not will depend on the sort of system you look at.",
            "But for example, if you were looking at the heat equation, there's an approximation to the Greens function of the heat equation, which is a Gaussian kernel here, and if you got RBF kernel here and a Gaussian approximation to the equation here, then it's just Gaussian Gaussian integral from the limits of minus Infinity to Infinity, so that's a valid thing.",
            "Now, this sort of system has been talked about as a convolution process without necessarily worrying about what the physical interpretation is.",
            "Personally, I find it helpful to think of what the physical interpretation is when putting the system together, but Dave Higdon in 2002 sort of talked about how you could use this for multi output Gaussian processes, so this is very general way of doing multi output Gaussian processes and in the paper we describe how several other ways can be seen as specific cases where you're using Delta functions or white noise in here.",
            "Of doing a multi output Gaussian process, of course, the nasty thing about it is these convolutions mean you get this full covariance matrix with off diagonal blocks.",
            "And you don't have the chronica structure that Chris described in his previous talk, so the inverse of this covariance matrix is going to be order N ^3 ^3.",
            "So it's clearly not going to be very useful.",
            "I mean, we've gone from common filters with I guess N. An Q squared I suppose there too N cubed cubed which isn't going to attract many people, but what we've sort of showing in this paper is that we can do something about it.",
            "So the model is conditionally independent given F of T. So if you know what this function is, you know what all these axes are exactly.",
            "You can do that convolution.",
            "It comes out exactly, so the assumption we made is that you can assume conditionally independent given some observations of F of T. So time based observations.",
            "Now these are a little bit like this is inspired by the idea of pseudo inputs that was in at work and is also being used in previous thing, but we were particularly thinking sort of pseudo inputs when we did this that the timing of these observations of this latent function are like pseudo inputs to the system.",
            "And the nice thing, although we didn't quite know."
        ],
        [
            "But the result of doing this making this assumption leads to a very similar approximation to the Pizzi approximation.",
            "the PC approximation is block diagonal approximation plus an out a special type of outer product that we're keen and Carl talked about.",
            "One of the problems with that as Ed sort of pointed out an AI stats paper is that you don't know what you should block off these different observations, so you've got this block diagonal structure and you don't know who to put in which block.",
            "Here the block diagonal structure, the blocks of the different outputs.",
            "So it's completely obvious what you should put in each block and the result of doing this is that the complexity goes from order N ^3 D ^3 to order N ^3 K ^2, where K is the number of pseudo inputs you choose to keep.",
            "And the storage is the same, it's decay times two order and squared.",
            "So this I mean apart from this sort of K factor, this is taking us back to the regime of independent Gaussian processes, which is your main competitor in this domain that you just trained on each output separately.",
            "So we're getting back to that sort of complexity now.",
            "It can also do a Fitzy style approximation which isn't so natural.",
            "The Pizzi stuff came out of the natural thing that we felt you could do, because we can see this conditional independence given the whole of F. So we've got a very good idea of.",
            "When it's going to work, it works well.",
            "If F is very smooth and if the points are closer together than the length scale of the latent F. Here it's less clear, but you can do a fit C style approximation which then takes you down to the NDK squared complexity in NDK storage."
        ],
        [
            "So this is some tide sensor network now and this is a multi output GP case where there's some sensors in the Solent of what tide height is and what you're interested in is how can you deal with the fact that one sensor may get knocked out so you lose a sensor and you want to still make predictions during that time but all the other sensors are on Now if you've just got temporal predictions so in this case we're using fairly dumb model were ignoring the sort of tidal frequency that exists in the soul and we're just using effectively RBF.",
            "A large multi output RBF built."
        ],
        [
            "These convolutions, if you do that, and you if you don't do that, and you have an independent Gaussian process prediction, then when you lose the sensor, the model doesn't know anything about what's going on.",
            "'cause it doesn't know about this oscillating structure.",
            "But in this case, when you've got this partially independent training, conditional stuff going on, you actually can see that you recover because you've seen what goes on other sensors.",
            "You recover the correct answer so that pretty much sits exactly over the truth, this is.",
            "Same example, but different set of sensors.",
            "So that's sort of quite nice."
        ],
        [
            "We can use these things effectively.",
            "So another data set that Mauricio looked at was.",
            "We've heard about Co creaking a bit and there's an entire book about this data set called the juror data set, which is data set of heavy metal concentration in the Swiss Jura.",
            "Is that the one with the River or was it the other one with the River?",
            "The other ones with the River.",
            "Yeah, so you've just got some input locations and measurements at each location of some heavy metal concentrations.",
            "You're interested in cabin and copper, but you also have nickel and zinc.",
            "As well, so the idea is you use these easy, cheap to measure nickel and zinc.",
            "They're cheap to measure to try and predict what the cadmium and copper is as well.",
            "So using a multi output GP this is a classical Geo stats data set an I think large section of the book is about how you do Coke rigging.",
            "Specifying the right kernel for Coke rigging.",
            "So very hand design kernel which incorporates a lot of their Geo statistical knowledge.",
            "So we're going to be machine learning and black box about it and just see how good our algorithm, which takes none of this knowledge in.",
            "But has an underlying physical interpretation, which is heat."
        ],
        [
            "Fusion works, so these plots show for the cadmean predictions and the copper predictions on the right you see Coke rigging results taken from that book.",
            "So these are the results that they get on mean absolute error.",
            "What you see on the left is this is the full Gaussian process, so this is without the approximation because the datasets fairly small, so it destroys Coke rigging for this one, and it's a little bit better than Coke rigging for this one, and then this is Pizzi with multiple runs of Pizzi to get different.",
            "With different numbers of.",
            "Inducing points from 5000.",
            "Two hundred 500.",
            "So that's the improving performance and we don't get quite down to the full Gaussian process here and here.",
            "We actually don't get anywhere near the full Gaussian process.",
            "And what's quite interesting about this one?",
            "If you're familiar with these methods, you'll know that in these Pizzi and Fitzy methods, there's never any point in having more inducing variables and data points.",
            "If you want to recover the exact thing, and these are fixed location inputs, so we're not optimizing with respect to these inducing inputs, But here we've actually got 500 inducing inputs.",
            "259 observations that I won't go into the details of why that you can still keep doing that, but it's because your assumption only becomes correct once you've observed the entire function.",
            "So the assumption of this petsay becomes correct.",
            "Once you've got infinite data points here, so it's conceivable that you can have far more that K can be larger than N, and you still don't get it."
        ],
        [
            "Is all town.",
            "OK, so.",
            "That sort of covers the complexity.",
            "Hey.",
            "Larger than any situation, you don't have to pay points on any occasion, no?",
            "We assumption is going back to the convolution, so where the."
        ],
        [
            "On the what the assumption is is that this is independent given F of you, which is true.",
            "So if I observe the whole of F of you, it's true, so the difference between Pizzi and this is that you're integrating across those observations, so the whole function is important, not just the value of the function at the particular time, so it's certainly conceivable.",
            "OK, to do this approximation you have to have a smooth F. You don't have a smooth F. If you do what Chris did and have a white noise F, you're not going to get anything serious.",
            "We're not going to get approximate the true model with these sort of things.",
            "You might get something out.",
            "So you can in that case you would have to have infinite inducing points.",
            "Here, it's perhaps indicative of the latent fun."
        ],
        [
            "Ocean is moving fairly quickly, I guess in this case, so you're not getting a good approximation for your conditional independence to be true, sorry.",
            "Sorry yeah I this Pixie Pixie approximation.",
            "Your model, will you have the physical model in the physical model is very simple, one in this it's just a heat equation.",
            "And no boundaries approximation to the heat equation.",
            "Cokriging they put they put loads of stuff there, Geo statisticians.",
            "I could say the whole it's right.",
            "The whole book is about this data set, right?",
            "So they have the same.",
            "As you would with no Coke rigging the way they do it is well Dan would know better than life, so I'm not gonna.",
            "That is really comparison between and.",
            "You can really see the effect of your physical model.",
            "Yeah, so this is.",
            "Yeah, this is a.",
            "This is a case where dumb black box.",
            "Beats intelligent statistician.",
            "So in this case we've replaced a statistician.",
            "He can now be removed from the Earth.",
            "I think the book's quite old, so maybe they do better today.",
            "This is this is really encouraging result, yes, but in this case here with our approximations I mean the size of the data set is small.",
            "That's why we can do the full GP.",
            "We're really looking for larger datasets in 259 data points, but certainly we can do the full GP in these sort of smallish datasets like this.",
            "And maybe there are computational.",
            "I have the reference."
        ],
        [
            "The date of the book that's 97, was that a second printing, maybe multiple?"
        ],
        [
            "So it may have originally been older than that, and they may have not had the computational power to do the thing we're doing here.",
            "So we have to be a bit careful.",
            "But yeah, it's nice that that model is getting there, but it's really simple one, it's just the two colonels.",
            "The Gaussian process is RBF covariance, and the kernel is a Gaussian, so you just end up with an RBF in the end."
        ],
        [
            "Well.",
            "But you also estimated counter answer the question.",
            "Well, what there is yeah in there.",
            "Yeah, it's but actually we're biased towards wanting the latent function to be smooth to make our approximation better.",
            "I don't think we're explicitly doing anything to cause that to happen, but there is a. Yeah, I mean, I think we haven't got the ideal parameterization of that yet.",
            "I didn't show it.",
            "I was just saying that your convolution.",
            "The."
        ],
        [
            "Salt of your convolution here could be the result of the heat equation."
        ],
        [
            "And that's the model we're using here.",
            "That's the closest physical model behind it."
        ],
        [
            "OK."
        ],
        [
            "So, um."
        ],
        [
            "So the next thing I just wanted to briefly talk."
        ],
        [
            "With.",
            "What you can do if you've got nonlinear regulation in the system, so nonlinear activation is something that happens, for example in the.",
            "So I showed you earlier, the P 53 gene transcriptional system where your FT isn't a linear response.",
            "So this is a linear differential equation, but there's a nonlinear function of your Gaussian process, so this is bound to be the case in a chemical system like that because the concentration has to be positive and a Gaussian process could be negative, so you always get this sort of thing turning."
        ],
        [
            "But what's quite nice about these sort of systems, so this is being used by Rogers and Girolami who did something similar to what I talked about before, but it didn't use a GP prior over FT. And then also, there's it's being used in a repression case, so you can get these nonlinearities still having linear differential equations, you can solve the differential equation, but the result is not a Gaussian process jointly across your ex."
        ],
        [
            "Is an F. So, um.",
            "I love this bit because what you can do is you can do Laplace's approximation and you use Laplace's method, Laplace 1774.",
            "Start.",
            "Just a point.",
            "So I read this translation of this paper recently.",
            "Did you know that is the first introduction of the Gaussian in the entire academic history?",
            "The Gaussian was invented by Laplace to do the Laplace approximation.",
            "He was doing a beta binomial system and he was looking for a beta posterior and he couldn't compute.",
            "We didn't call it that he couldn't compute the beat posterior 'cause he didn't have all these beta functions and whatnot, and so he did.",
            "They go to the class approximation, which is a Gaussian approximation, and in the limit he did it in.",
            "It was valid as well.",
            "Here it isn't so valid, so you can do it Laplace approximation.",
            "So class was ahead of us in 1774.",
            "Well it took them 40 years after that.",
            "Come up with least squares or something.",
            "Amazing.",
            "So, but here this was about gambling, right?",
            "So that was important.",
            "Then this is about loaded dice, or apparently calls them in the paper English dice."
        ],
        [
            "OK, so the Michaelis Menten activation has this particular form.",
            "Now this is a linear system now, and the linear system you see we infer a negative protein.",
            "But with this constraint, which keeps things positive in terms of E to the F of T, which is now the protein, then we get this different form and the question is."
        ],
        [
            "Is little class approximation good here in the car list?",
            "As I may have a little time to show, you has done a sampling approach MCMC approach which is really fast and really good and we can show that it is pretty good in this case.",
            "But there are going to be other cases where it's not very good."
        ],
        [
            "So you can do a lot less approximation.",
            "This means you get to reference 1774 papers.",
            "So Michalis, as I say, that those samples were from an approach that Michalis is used to sampling Gaussian processes, and I'll just briefly say that this involves what we call using control points for sampling.",
            "And again, it's inspired by these sparse Gaussian process techniques.",
            "The idea is you select a bunch of inducing variables as they call in these sparse Gaussian process techniques, and given those you sample functions from the system.",
            "And you can then sample what the point is by sampling over the control point separately.",
            "You can move around the space very quickly, but you always get samples that are consistent with your Gaussian process prior when you sample the full function and we found we initially thought that this sampling.",
            "Would be too slow to be practical, that's why we did the last approximation.",
            "But actually the way mechanics has implemented it's extremely fast and very practical for these small."
        ],
        [
            "All data sets here so."
        ],
        [
            "I'll skip through the."
        ],
        [
            "Details of that and just show you these are results of.",
            "The P53 system again, but these are."
        ],
        [
            "Are the.",
            "I think these are yeah, so these results you've seen before and this is the inferred protein levels.",
            "This is with barrancos results superimposed, and this is with Nicholas is results from the HMC sampling overall parameters of the system as well.",
            "So it's full Bayesian sampling and how long did that take to run Michalis?",
            "4 hours so I mean it's only a few data points.",
            "6 genes and seven time points, Regina.",
            "Manuel said 1 million iterations of.",
            "So basically that's really quite fast compared to the last approximation."
        ],
        [
            "Um, OK those are."
        ],
        [
            "So in the final sort of four minutes, just sort of say another extension we've looked at, which was work with Auntie Hunkeler, who visited us in Manchester.",
            "So actually you could start cascading these differential equations as well.",
            "So in this case it's again in."
        ],
        [
            "Powered by transcriptional regulation, the data is from the."
        ],
        [
            "Furlong lab in Heidelberg.",
            "The idea here is that you observe the transcription factors gene.",
            "And that is producing protein, so this is a model of translation that you see the gene and it produces protein from the gene and then from the M RNA.",
            "You then produce other genes.",
            "So you're getting a constraint above and below, so you're observing.",
            "You can't always do this because of things like phosphorylation which make proteins active in biology.",
            "But this is development system.",
            "It's reasonable to think that phosphorylation is less involved in these sort of systems that it is in, say, signaling network.",
            "So we've got this simple OD model of.",
            "Translation and a simple ODI model of transcription.",
            "And it turns out with a lot of integrating.",
            "It's auntie did.",
            "You can find the solution for this."
        ],
        [
            "Both systems and get the joint covariance over the input transcription factor.",
            "The input M RNA to the transcription factor.",
            "The driven latent force.",
            "So you're forcing two things you're forcing from there to force that to force that.",
            "So it's like a hierarchy or a cascade of differential equations.",
            "You could do a third level, but how many terms were in the two levels?",
            "20 or something.",
            "You've got about 20 double integrals to do today.",
            "At the third level out, but we think we may be able to.",
            "I mean, we hope we could do things to maybe."
        ],
        [
            "Provide and look at the higher level things.",
            "So in this case you got the input M RNA and then you've got targets.",
            "Two targets of the M RNA, and then we infer the latent missing function of the pro."
        ],
        [
            "Change between the two."
        ],
        [
            "So I hope to sort of show you as well, which we don't have results on yet, but I think we'll have them soon.",
            "Is how you can use these sort of 2nd order differential equations to model things like motion capture data where you have these physical things going on and you have a time based input.",
            "Other thing I'm very excited about and I can mention this because it's also a NIPS paper is.",
            "Ben called ahead.",
            "A student marcher also means they came up with this idea of using Gaussian process is to help solve nonlinear differential equations and placing Gaussian process priors over what the solution should be.",
            "And one of the things I'm very excited about is enhancing those Gaussian process priors with linear, solvable physical models of the system, which captures some idea of what's going on in the system.",
            "But they're not nonlinear, so you can do this joint sampling to improve the speed of which you can solve these nonlinear differential equations.",
            "And given that a, I think what's this?",
            "What's the fastest sampling speed for those MCMC tests about a week for getting a base factors out?",
            "Something like that.",
            "Say it takes a long time to get Bayes factors out, and we're looking to get larger systems done spatial systems, so we need these speedups to try and get the Bayes factors to do science.",
            "In the actual real world systems, so we're looking to do that sort of thing as well.",
            "And then there's also lots of work on stochastic differential equations.",
            "We haven't had a stochastic component here, we're just still trying to understand whatever."
        ],
        [
            "Else is doing there.",
            "OK."
        ],
        [
            "And so the acknowledgements.",
            "A lot of this work is joint with Magnus, who's my copii on two grants that have gone into this post docs, Pai Gow Auntie who visited us.",
            "Michalis is posted with this maricio.",
            "I thought it should be said is a PhD student.",
            "Me and Jennifer, PhD student with Magnus and then data from these sources and two.",
            "2M Research Council awards.",
            "That's it.",
            "Can there be no.",
            "I worry about the SG so.",
            "I think we are no.",
            "I mean, you could obviously do this if you've got Gaussian noise driving our system, and you could obviously enhance what we've talked about with the Gaussian.",
            "I mean, there's Chris is shown.",
            "That's just adding an extra covariance term on.",
            "But I would say that we don't know enough about.",
            "I think we're kind of interested in things like whether the MCMC method can be applied in some form to SDS, but I think we're not sure if it can be.",
            "And yeah, we're still trying to understand your work.",
            "And Andrew and Darren's work in this area try and get a better handle on that.",
            "We're leaving that to you for the moment.",
            "We don't have it yet.",
            "Big thing missing data.",
            "Benefit yet.",
            "Things like that can be done by computer algebra.",
            "Yeah, well, actually what I think.",
            "My hope is that we when these Earths come out you know it's a covariance which is based on earth surface into symmetric function.",
            "They always come out in pairs.",
            "I'm.",
            "And my vague hope is that manually we might you should you can pair these things up and see that a pair of them is a kernel in itself.",
            "So you're actually getting a sum of kernels out of the system when you do the integrals, so I'm curious if we haven't, we haven't done yet is sit down and pair up all those kernels and see if you can apply the cover.",
            "You can then apply independently to each of those, and it may be that you can automatically compute computer algebra is another option, but that sort of manual alternative.",
            "The problem is that you know Earth isn't as quick as exponential to compute.",
            "In you know, in the base case, so already when you're using this Earth, which is not complex, I think the speed of computation versus the Gaussian kernel is maybe 10 times slower.",
            "And then when you go to the complex Earth, you're in more trouble.",
            "So it's but a lot of our problems are more about things that we didn't have to worry about before 'cause the kernel computation was relatively quick.",
            "Now we have to worry and think about and so we hope we can tidy a lot of them up and still get it reasonably efficient.",
            "But you'll see my abstract.",
            "It says given time I'll talk about.",
            "Motion capture.",
            "Well I didn't say is I mean given the computation time rather than the given time in the actual talk.",
            "Laplace approximation is exactly yeah so.",
            "Yeah, that's that's true.",
            "You can do these, yes, but it's not as.",
            "For the what Magnus is saying is that if it's for these cases here.",
            "So it turns out."
        ],
        [
            "The bit that Magnus knows that you won't have heard is that this is the interesting thing about when you do this Laplace approximation.",
            "To solve this Laplace approximation, you only need a 1 dimensional.",
            "Integral numerical integral rather than the two dimensional double integral you get if you're computing the covariance.",
            "So this is a really good thing to do if you can't compute this convolution for your kernel, because if you can't compute that convolution for your kernel, you have to do a double numerical integral over the covariance.",
            "But it turns out by using the Laplace approximation you can lose that double numerical integral and just do a single numerical integral, which is obviously much nicer.",
            "So the scope to do this for funkier covariances, and indeed we have done with what I call them.",
            "MLP covariance, but it's the one Chris Williams derived is an infinite neural network.",
            "Do it.",
            "Magnus may be better placed to answer that than I, but you do.",
            "Yeah, you basically are getting a.",
            "You compute the error bars through a.",
            "We do have it, we don't get it in continuous space.",
            "We have a discretization of the integral and we do the integral by Simpson's rule.",
            "Now.",
            "Um, so we don't have a continuous, but can you write down a continuous differential equation?",
            "For that.",
            "It is for the diagonal, yeah?",
            "Here already.",
            "Then the other thing about this technique, as Magnus did in his post Doc with David side I think is you can do nonlinear differential equations and this still works.",
            "So you can put another.",
            "You have a nonlinear term on the decay.",
            "You can still do this Laplace approximation.",
            "Yeah.",
            "We don't have the data we've got loads of brilliant stuff without any application.",
            "OK.",
            "Celebrate darling.",
            "King you've shown us.",
            "Gaussian symmetric predicted distributions over concentrations, yeah?",
            "Well.",
            "Albayzin are we being well?",
            "That's the point.",
            "We're not that Black box now, but maybe you know, maybe it doesn't matter, because in this case you're assigning."
        ],
        [
            "Things that are impossible.",
            "Rather than assigning zero match things that you know are possible.",
            "This is the.",
            "This is the one where the posterior actually an error bar goes negative.",
            "Tony Ohagan talked about this eliciting prior distributions.",
            "You remember that where he was saying and he was basically running a prior Gaussian process prior over a probability distribution function, and those were going to negative two occasions when the rejecting samples.",
            "I thought maybe we could try that rejection samples technique, but that doesn't mean I think it's not that great, but it's something you could do, but this is really pointless.",
            "Approximation doesn't suffer that problem because now you're put a GP over the log concentration.",
            "And then Nicholas is sampling approach again.",
            "Eliminates that is a problem.",
            "I, I think that this is so.",
            "I've sort of.",
            "I'm mixing because I didn't have the results on some of the things we've done where we just know we've got the model wrong.",
            "I'm mixing two concepts in here.",
            "This is this split between science and engineering.",
            "I think that the linear system as an engineering approach and add in what we call a crud catcher where you add in a block diagonal independent GPS to try and deal with anything you haven't modeled correctly.",
            "I think that's really sensible as a black box type approach to systems you think this thing might be going on 'cause it's not going on.",
            "The crud catcher will pick it up.",
            "That's the beauty of blades.",
            "But the crowd catcher is regression on its own.",
            "Where is Nicholas is stuff about is MCMC and a lot of what Mark's done in this domain is science and then you have to do your MCMC and estimate your base factors and worry a lot about those things.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So welcome back after the break.",
                    "label": 0
                },
                {
                    "sent": "I'll now introduce myself.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about something that we sort of calling latent force models with Gaussian process, but it relates nicely to some of the stuff that some of the concepts that Chris talked about in the last talk, and I think it will relate to future stuff that other people will talk about as well.",
                    "label": 1
                },
                {
                    "sent": "So I was sort of a later entry when someone dropped out, so I don't really have nice philosophical things to say in the talk, it's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More like.",
                    "label": 0
                },
                {
                    "sent": "Well, some of its philosophical, but it's I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think I don't think it's controversial.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to start with an introduction.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to do is I'm just going to look at sort of standard approach to dealing with data that we all use, and I want to extend it as we go along.",
                    "label": 0
                },
                {
                    "sent": "So let's say I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got some data X and I've got some latent variables F. Normally I use Wi-Fi data and X for latent variables.",
                    "label": 0
                },
                {
                    "sent": "But the reason I've done this will become clear.",
                    "label": 0
                },
                {
                    "sent": "So one thing we can do with our data is just come up with a reduced dimensionality reduction of our data, which is to say that add a tourism, some latent variables, times where the number of latent variables that dimensionality Q is less than the dimensionality of our data D times some weighting matrix plus some noise.",
                    "label": 0
                },
                {
                    "sent": "And we know this to be a model behind factor analysis, principal component analysis, independent component analysis, various models conform to this.",
                    "label": 0
                },
                {
                    "sent": "So we integrate out or X very often, and we optimize with respect to W. Now, for temporal data and a particular Gaussian prior in the latent space.",
                    "label": 1
                },
                {
                    "sent": "So the common filter or smoother.",
                    "label": 0
                },
                {
                    "sent": "You can make this a common filter, so if I have prior over F that is a particular Gaussian form that is temporal, so it's got the same dimension N of the prior has a full set of work.",
                    "label": 0
                },
                {
                    "sent": "It's gotta try diagonal covariance.",
                    "label": 0
                },
                {
                    "sent": "I can get this common filter.",
                    "label": 0
                },
                {
                    "sent": "More generally, though, we could consider, say, Gaussian process prior over F. So this model Chris mentioned it briefly, but this is actually already been proposed that you've got several F. You take each column of F to be independent, and you place a Gaussian process prior independently over each F separately.",
                    "label": 0
                },
                {
                    "sent": "If you allow each of these covariances to be different, then I believe this is just the semi parametric latent factor model of TE, Zager and Jordan.",
                    "label": 0
                },
                {
                    "sent": "It was discussed before.",
                    "label": 0
                },
                {
                    "sent": "So this sort of this sort of model you can see what I'm trying to do is take a model that we used to seeing and by putting into temporal prior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in that case you could have any input you don't just have to have time.",
                    "label": 0
                },
                {
                    "sent": "You could have any inputs, so I'm sort of saying this F here could either be time based, or it could have some inputs.",
                    "label": 0
                },
                {
                    "sent": "It could be a Gaussian pro.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assess so.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, though, we prefer.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not to do Gaussian process is oh, in the time case over this sort of model Huawei because of cubic complexity in the length of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time series so a common filter smoother approach has been preferred and that gives us a sort of.",
                    "label": 1
                },
                {
                    "sent": "Here's my chair.",
                    "label": 0
                },
                {
                    "sent": "That gives us a complexity which is linear in the amount of time and quadratic in the number of states.",
                    "label": 0
                },
                {
                    "sent": "So we normally do this sort of thing, but.",
                    "label": 0
                },
                {
                    "sent": "Well, I'd like to say is the advances in sparse approximations, in particular ones.",
                    "label": 1
                },
                {
                    "sent": "I'm thinking of it's Nelson and Zubin's approach to ask parametric Gaussian process is and we're keen and Carl's.",
                    "label": 0
                },
                {
                    "sent": "Generalization of that to the what they call the partial independent training conditional and also Michalis will talk about another sparse variant which I think is also useful.",
                    "label": 0
                },
                {
                    "sent": "This is meant we've got these approximate inference techniques that actually have the same complexity as the common filter, so I think in some sense we can now think of looking beyond those, filter methods to these Gaussian process based methods to have a richer model at the expense of some approximations in the inference.",
                    "label": 0
                },
                {
                    "sent": "But I think as Michalis will show some of those approximations can be really very good.",
                    "label": 0
                },
                {
                    "sent": "Without",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great computational cost.",
                    "label": 0
                },
                {
                    "sent": "OK so but I want to extend those sort of models again.",
                    "label": 0
                },
                {
                    "sent": "I want to use that sort of way of thinking about these models to extend further so the models rely on a latent variables to provide dynamic information here.",
                    "label": 1
                },
                {
                    "sent": "So in the common filter you've gotta sort of Gaussian random walk in the latent space and in the Gaussian process example you've got a temporal function.",
                    "label": 0
                },
                {
                    "sent": "You could view the common filter as a particular Gaussian process, but what I want to do is.",
                    "label": 0
                },
                {
                    "sent": "Push this model little bit further by using a mechanistic inspiration so.",
                    "label": 0
                },
                {
                    "sent": "We talked a bit about physics and we talked a bit about the sort of models people using physics and what we sort of.",
                    "label": 0
                },
                {
                    "sent": "So I think I I read this book on the history of stats recently and it had a lot of stuff.",
                    "label": 0
                },
                {
                    "sent": "A lot of material on why it was that it took, let's say, 150 years to go while less than that, maybe over 100 years to go from Gauss's application of least squares in an astronomical system to application of least squares to the data that they were looking at just around the turn of the last century.",
                    "label": 0
                },
                {
                    "sent": "So it took about 100 years to say, oh, I can do regression on this data.",
                    "label": 0
                },
                {
                    "sent": "That's pretty amazing when you think about.",
                    "label": 0
                },
                {
                    "sent": "What a well used technique regression is and the explanation given was that.",
                    "label": 0
                },
                {
                    "sent": "Because when they were looking at statistics about 100 years ago from now, they want they didn't actually have a model in mind about the way things happened.",
                    "label": 0
                },
                {
                    "sent": "They didn't have a Newtonian view of the world that the argument was that because of mutant people had this view that Amos was talking referring to earlier as an example of a parametric model that you believe in and that you could fit the parameters.",
                    "label": 0
                },
                {
                    "sent": "This parametric model within the context of a theory of errors, but 100 years ago for SoC data where you don't know the underlying generating thing, you couldn't do that.",
                    "label": 0
                },
                {
                    "sent": "You couldn't say what your parametric model was.",
                    "label": 0
                },
                {
                    "sent": "And the argument was with.",
                    "label": 0
                },
                {
                    "sent": "That was why they took a long time to look to methods that have been used in astronomy for 100 years.",
                    "label": 0
                },
                {
                    "sent": "But what I'd like to do is take some of the ideas that are used in those Newtonian models and try and pull them back into machine learning, not because we know that their present in exactly the way we're going to model, but because we can incorporate those things and they have certain characteristics, including things like resonance or inertia, which are actually quite difficult to model in the base case, so I don't think it quite worked exactly for Chris is.",
                    "label": 0
                },
                {
                    "sent": "Robot kinematics example in the way I'll introduce it, but he sort of did that there.",
                    "label": 0
                },
                {
                    "sent": "He sort of said I have knowledge about the Coulomb friction and I'm going to include a covariance function in my model that handles that knowledge.",
                    "label": 0
                },
                {
                    "sent": "And then I expect my machine learning part to just deal with the rest of the model on its own, and this is the this is the same sort of idea with these sort of models.",
                    "label": 1
                },
                {
                    "sent": "So physical interpretation of the thing we saw before the latent functions, RQ forces and we observe the displacement of D Springs.",
                    "label": 1
                },
                {
                    "sent": "To these forces, we interpret the system as the force balance equation X D = F S. So S is a matrix of sensitivities, which says that we've got a limited number of Q forces, but they may be operating through some set of levers on the Springs, so they could increase the value of their force, or decrease the value of their force, change the sign so and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we've got some.",
                    "label": 0
                },
                {
                    "sent": "Or they could be pulleys or whatever your favorite thought.",
                    "label": 0
                },
                {
                    "sent": "So we've got some sort of simple force balance equation there, and then the original system is recovered just by taking that matrix sensitivities and multiplying by the inverse of the spring constants.",
                    "label": 0
                },
                {
                    "sent": "So that is a non.",
                    "label": 0
                },
                {
                    "sent": "It's a mechanical system, but there's no dynamics.",
                    "label": 0
                },
                {
                    "sent": "It just moves because.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notes for differential equation.",
                    "label": 0
                },
                {
                    "sent": "So the suggestion is that what you can do is do things like Abadam PO and give the system mass.",
                    "label": 1
                },
                {
                    "sent": "So here's a first derivative of the system.",
                    "label": 0
                },
                {
                    "sent": "Multiply by damping coefficient and the system's mass.",
                    "label": 0
                },
                {
                    "sent": "The acceleration is multiplied by the mass, and now we've got a second order mechanical system.",
                    "label": 1
                },
                {
                    "sent": "Which we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So it's actually exactly the same it's.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a generalization of PCA or factor analysis, or any of those models you'd like to mention, or common filter.",
                    "label": 0
                },
                {
                    "sent": "As long as you put the right prior over F and then the right values of these MC ends, and then you also have some decision about your noise.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about this system is it will exhibit inertia and resonance.",
                    "label": 1
                },
                {
                    "sent": "So these are characteristics that are quite difficult to just put into, say, a standard neural network.",
                    "label": 0
                },
                {
                    "sent": "And there are many systems that can be represented by differential equations, so this is a temporal one.",
                    "label": 1
                },
                {
                    "sent": "But other things we've been looking at includes partial differential equations, where you may be driving in a sort of heat equation.",
                    "label": 0
                },
                {
                    "sent": "The temperature at a certain point, and you're looking at the temperature or the response of the system across plate.",
                    "label": 0
                },
                {
                    "sent": "So there's various sort of systems you can think of as being like this, and I want to call those systems in general latent force models, so their models where there's some latent.",
                    "label": 0
                },
                {
                    "sent": "Or switch we're actually going to Gaussian process prior for in this talk, but you could consider at your favorite prior.",
                    "label": 0
                },
                {
                    "sent": "It's just easier if you're using a Gaussian process and those latent forces are driving a system which has some dynamics and physical characteristics which we may or may not believe.",
                    "label": 0
                },
                {
                    "sent": "Our existence in the actual system, but the point is, we've got parameters that we can learn.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To try and determine if they're there.",
                    "label": 0
                },
                {
                    "sent": "OK, the nice thing is, for Gaussian process we can compute covariance matrices for the output displacement.",
                    "label": 1
                },
                {
                    "sent": "So for one displacement that should say for one displacement the model looks like this you've got this summer forces will actually look at one force for the moment because the sum of forces just means you can sum the covariance functions in the end.",
                    "label": 0
                },
                {
                    "sent": "So it's trivial to extend it to multiple forces 'cause it's just adding up the covariance functions.",
                    "label": 0
                },
                {
                    "sent": "So it turns out nicely for this covariance function.",
                    "label": 0
                },
                {
                    "sent": "You can solve this differential equation for X and the result is a convolution and you can do.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convolution tractably you can't only do that convolution tractably the colors are a bit funny, but you can actually do the double convolution.",
                    "label": 0
                },
                {
                    "sent": "You need to compute the covariance.",
                    "label": 0
                },
                {
                    "sent": "So Chris Williams in his very nice stochastic process introduction which I didn't see live but saw on videolectures.net talks about the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case where you've got a system like this and you're driving it by Gaussian noise as your force.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of like PCA example and then you get out a particular covariance functions.",
                    "label": 0
                },
                {
                    "sent": "I'm considering the case where you have an RBF covariance here, so there's something going on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the latent space.",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 0
                },
                {
                    "sent": "Now, this is a visualization of the covariance function you get, so the top corner here F of T is the auto covariance for the latent function.",
                    "label": 0
                },
                {
                    "sent": "So it's just an RBF covariance.",
                    "label": 0
                },
                {
                    "sent": "Colors are slightly different.",
                    "label": 0
                },
                {
                    "sent": "On my hope we don't lose color later on.",
                    "label": 0
                },
                {
                    "sent": "But what you're seeing, then, is you're seeing some different displacements, which are the result of the application of this force and each different displacement has a different mass, spring and damper associated with it, and you can summarize it in a better way by talking about what the damping ratio is.",
                    "label": 0
                },
                {
                    "sent": "So the first one here, which I've denoted why it should say X, is underdamped.",
                    "label": 0
                },
                {
                    "sent": "So this one's got resonance.",
                    "label": 0
                },
                {
                    "sent": "So actually it has an input and then you can see you get these bars.",
                    "label": 0
                },
                {
                    "sent": "He's got resonant frequency which is dependent on the selection of those parameters.",
                    "label": 0
                },
                {
                    "sent": "This one here is critically damped.",
                    "label": 0
                },
                {
                    "sent": "So actually he tries to follow that as best as possible, so that's critically damped, and this one here is over damped, so it's a damped version of the original signal.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had a line which I've given this talk once before and I've used it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once before so.",
                    "label": 0
                },
                {
                    "sent": "When you first sample from Gaussian process covariance, I have this analogy that, like it's like the first time you meet your wife and you fall in love because it's amazing that you get these functions coming out of this thing that you've just written down.",
                    "label": 0
                },
                {
                    "sent": "It just looks beautiful.",
                    "label": 0
                },
                {
                    "sent": "I think when you sample from this covariance.",
                    "label": 0
                },
                {
                    "sent": "An example from this covariance.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like when your wife does something and you remember why you fell in love all over again.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For these these are the samples from this covariance.",
                    "label": 0
                },
                {
                    "sent": "So what you're seeing here?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a joint sample, so we've got 1234 functions, so this is like Chris Williams is stuff in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "This multi outputs right?",
                    "label": 0
                },
                {
                    "sent": "So you've got one output here which is F. In fact it would be more similar to think of marginalizing with respect to F and thinking of three outputs because they're sort of then similar.",
                    "label": 0
                },
                {
                    "sent": "But we've got four outputs and each is the sample, then gives you function one, function 2, function 3, function 4.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For what you're interested in, you can pull up these and what I've done here is plotted that for the scion being the driving function.",
                    "label": 0
                },
                {
                    "sent": "The red is the.",
                    "label": 0
                },
                {
                    "sent": "Under damped system.",
                    "label": 0
                },
                {
                    "sent": "So that's got a natural frequency, but because it's not getting hit at the right times by the driving function here, that depends on the variance between the length scale and the.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends on coincidence as well, but it's also it depends on when the length scale, what the length scale is as well.",
                    "label": 0
                },
                {
                    "sent": "You don't see a lot of the resonance in the red.",
                    "label": 0
                },
                {
                    "sent": "Then you've got the critically damped.",
                    "label": 0
                },
                {
                    "sent": "Is the blue and the green is overdamped, so it's a more.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version of the original signal.",
                    "label": 0
                },
                {
                    "sent": "K in this case you start to see some of the resonance because it gets banged at the right time.",
                    "label": 0
                },
                {
                    "sent": "So it happens to be bang there and there.",
                    "label": 0
                },
                {
                    "sent": "So the red actually goes quite large because it's starting to exhibit its natural frequency, doesn't have the same frequency as the latent function, but it's getting banged at the right time, so it starts to exhibit some of its underlying properties.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greed, overdamped and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "So what you're actually doing here when you sample this system?",
                    "label": 0
                },
                {
                    "sent": "I mean you solve the differential equation.",
                    "label": 0
                },
                {
                    "sent": "By doing that you need to do a double integral over your covariance function, which is a form of two convolutions.",
                    "label": 0
                },
                {
                    "sent": "But you're actually solving the differential equation for each sample F simultaneously with the X is, which is, which is quite fun.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and there's another example.",
                    "label": 0
                },
                {
                    "sent": "So now I wanted to talk.",
                    "label": 0
                },
                {
                    "sent": "We haven't had this nasty thing about the double convolution to get.",
                    "label": 0
                },
                {
                    "sent": "This is actually you end up using the complex Earth function to compute the covariance.",
                    "label": 0
                },
                {
                    "sent": "So I've got two guys Maricio who's here and David Lang Garcia been working on this and it's quite slow to compute, so getting results on seriously large data is not something we have yet.",
                    "label": 0
                },
                {
                    "sent": "So I'm actually going to drop back to, but we believe it's possible we're getting fast enough that we think it will do something nice.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The complex Earth appears because you re expressed that as a complex exponential and then you combine that with this, which is your Gaussian process covariance when you do the try and compute the actual covariance and then you're going to integrate from nought to T. So it turns out to be a complex Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So you get the complex Earth as a result and some other nasty terms.",
                    "label": 0
                },
                {
                    "sent": "And there's a few.",
                    "label": 0
                },
                {
                    "sent": "It's just one of those you know.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when you need a particular function and you look up when the Fortran version of it was first published efficiently.",
                    "label": 0
                },
                {
                    "sent": "It's always like in the 1960s, these ones are done in like the 1980s, and their ongoing most efficient version.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "You know, it's it's we're getting there in terms of improving our Matlab code really to get.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To speed out, so we expect results on that soon.",
                    "label": 0
                },
                {
                    "sent": "But so I'm going to drop back to something that's easier, which just involves because there's no sign in this convolution.",
                    "label": 0
                },
                {
                    "sent": "This is the first order differential equation, and it's just involves the Earth function in the actual covariance you get, so this is quicker to compute because it's already built into Matlab, etc.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've used this one as a model of gene transcription.",
                    "label": 1
                },
                {
                    "sent": "Now, in some cases we think it's an accurate model of gene transcription, but the sort of things we hope to do with it is.",
                    "label": 0
                },
                {
                    "sent": "We also know it's not accurate.",
                    "label": 0
                },
                {
                    "sent": "Because of this linear response here is genuinely a nonlinear response.",
                    "label": 0
                },
                {
                    "sent": "In most cases.",
                    "label": 0
                },
                {
                    "sent": "Combine it with other Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Regressor is to try and pick up when you're in the nonlinear response regime.",
                    "label": 0
                },
                {
                    "sent": "So you sort of pick a broken model which has some of the characteristics that you want, and then you try and combine it with a model that emits a simpler model, but it describes well.",
                    "label": 0
                },
                {
                    "sent": "It's a more it can describe more datasets so.",
                    "label": 0
                },
                {
                    "sent": "This model here is quite constrained.",
                    "label": 0
                },
                {
                    "sent": "In the datasets it can describe for the multiple outputs, so it will fail to describe a nonlinear response if the nonlinearity is being used.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of idea with the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensitive MoD.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example that's being used for gene transcription by Branco at all, and that's the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model that they used.",
                    "label": 0
                },
                {
                    "sent": "You can see this is the covariance for this model, so it's the same thing we saw before, but now we don't have any resonance 'cause there's no mass in this system.",
                    "label": 0
                },
                {
                    "sent": "It's effectively spring damper system.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's just sort of some damping.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to show you now now.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a toy example that use realistic parameters from brain.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those data, but I want to show you is how you can do inference in this sort of system, and we're actually know in this case that the system doesn't start until time 0.",
                    "label": 0
                },
                {
                    "sent": "So at times zero, the system starts operating, but.",
                    "label": 0
                },
                {
                    "sent": "We know that we've had.",
                    "label": 0
                },
                {
                    "sent": "That was an observation on F, so we know that at times zero, the function is doing nothing.",
                    "label": 0
                },
                {
                    "sent": "It just happens.",
                    "label": 0
                },
                {
                    "sent": "That's the sensible thing to do.",
                    "label": 0
                },
                {
                    "sent": "In this case you can have it, but you haven't observed what's going on at time 0, so we're going to see observations appearing here these.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Output jeans, so this is gene transcription.",
                    "label": 0
                },
                {
                    "sent": "So in Gene Tran.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scription, what you have is a rate of production of a particular gene.",
                    "label": 0
                },
                {
                    "sent": "Given an underlying protein that is called.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For it to be produced.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to observe is some artificially generated genes from a known F. So this is a known protein.",
                    "label": 0
                },
                {
                    "sent": "And as we observe them coming in, what we'll see is that as we observe, more genes will see that we start to get a better idea of what the latent F function was.",
                    "label": 0
                },
                {
                    "sent": "This model you're not seeing the learning here, but this was learned in the 1st place, so the parameters I'm using here, I learned from the toy data.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample Daniel.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's not a lot of data.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's quite high now.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even the genuine under.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buying jeans.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I add the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teens in",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You start to see.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stereo Rover.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imp.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cruise.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they've got.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two bumps",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roll.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite high sensitive.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In quite high decay.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in some way.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is there quite an easy?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do by clustering.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, but.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you an example.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later that isn't.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you go.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you gain?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inference.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's going on?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even though.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here in.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These cases, the noise sometimes outweighs the actual bumps in the genes have added the noise on.",
                    "label": 0
                },
                {
                    "sent": "You still managed to recover and learn the parameters in this case of the actual system, so these are parameters that are difficult to measure typically, and so that's sort of interest.",
                    "label": 0
                },
                {
                    "sent": "I should add that this is right on the edge of not being able to find the parameters 'cause I did it once and it got the parameters and I thought I better check for robustness and I did it again and it didn't get the parameters so you can learn a system of this sort of high level noise, but you.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the edge of what you might expect to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a P53 system.",
                    "label": 0
                },
                {
                    "sent": "These are so cool.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can find these pics.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the web of what molecules look like.",
                    "label": 0
                },
                {
                    "sent": "So in P53 you've got this 14 function.",
                    "label": 0
                },
                {
                    "sent": "Is this protein which is driving the transcription of several genes?",
                    "label": 0
                },
                {
                    "sent": "So these are actual observations and this is the system that Martina Franca.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looked at and we sort of then looked at it again with this model and this is.",
                    "label": 0
                },
                {
                    "sent": "We've looked at it in the NIPS paper, but this is updated results with improvements to the system, which is an ECB paper coming this year and the work was done in collaboration with Magnus, and the Post was paid out.",
                    "label": 0
                },
                {
                    "sent": "So there's.",
                    "label": 0
                },
                {
                    "sent": "Observations these crosses with error bars or Arab are based observations of the gene expression from microarray data and this is the inference of the protein from the linear system.",
                    "label": 0
                },
                {
                    "sent": "An insert a nice result, it can.",
                    "label": 0
                },
                {
                    "sent": "It's similar to what Barranco derived, but the error bars are sort of slightly lower and the parameters make sense.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's difficult to validate some of these things.",
                    "label": 0
                },
                {
                    "sent": "One thing you might complain about that he said, well, these look all the same.",
                    "label": 0
                },
                {
                    "sent": "You could have just on PCA.",
                    "label": 0
                },
                {
                    "sent": "You could have just done something linear.",
                    "label": 0
                },
                {
                    "sent": "The decay insensitivity you know what was the use of your differential equation, and you're right, because actually in this case the decays and sensitivities are relatively high, so that means that the genes tend to track what how the protein does, and this must happen.",
                    "label": 0
                },
                {
                    "sent": "Otherwise clustering of microarray data just wouldn't tell you.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anything interesting?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example where it doesn't happen.",
                    "label": 0
                },
                {
                    "sent": "So this is a bunch of results from systems work by Jennifer Weathers an it's elk one is the transcription factor in this case and these are some of the targets so you can see these are the training jeans here.",
                    "label": 0
                },
                {
                    "sent": "Now a lot of them just go up and down so they're high sensitivity, high decay, but this one goes up and stays up and you can't explain that you won't cluster that with that.",
                    "label": 0
                },
                {
                    "sent": "But you can't explain it with a diff.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equation model.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can then use your model of what's going on based on these six jeans that you know a targets to search for other targets.",
                    "label": 0
                },
                {
                    "sent": "And here's, for example, are predicted target gene.",
                    "label": 0
                },
                {
                    "sent": "You can't quite see the Red Cross apologize, but there, within the prediction of the model.",
                    "label": 0
                },
                {
                    "sent": "And then there's other genes in this side which aren't within the prediction of the model.",
                    "label": 0
                },
                {
                    "sent": "How about dinner time?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "This was really so.",
                    "label": 0
                },
                {
                    "sent": "I guess originally it was Magnus myself, and Greedo Sanguineti when he was a postdoc at the start.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work on these sort of systems and it was inspired by this talk by Martino.",
                    "label": 0
                },
                {
                    "sent": "That was saying I have this simple linear model so we can do Gaussian processes on that.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thinking.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now on the SSD.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of machine learning is well, why not use these sort of models more?",
                    "label": 0
                },
                {
                    "sent": "In general, when you don't necessarily believe the system so much now we don't have all the answers for how to do that.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But one of the questions that immediately comes up is this one.",
                    "label": 0
                },
                {
                    "sent": "So this is work by Mauricio Alvarez.",
                    "label": 0
                },
                {
                    "sent": "And this is going to appear at NIPS this year.",
                    "label": 0
                },
                {
                    "sent": "I can say now.",
                    "label": 0
                },
                {
                    "sent": "So the solutions to these differential equations and others are all typically forms of convolution.",
                    "label": 0
                },
                {
                    "sent": "So what you get is the result of your displacement in the mechanical case, but it could be a sort of temperature response.",
                    "label": 0
                },
                {
                    "sent": "Could be anything that's in your physical model is typically the integral of your driving latent force times some kernel function plus some other function.",
                    "label": 0
                },
                {
                    "sent": "Now this integral, if it's a Fourier transform type integral, could be from minus Infinity to Infinity, but if it's a Laplace transform, or if there are boundary conditions, it could be a finite integral.",
                    "label": 0
                },
                {
                    "sent": "So whether it's tractable or not will depend on the sort of system you look at.",
                    "label": 0
                },
                {
                    "sent": "But for example, if you were looking at the heat equation, there's an approximation to the Greens function of the heat equation, which is a Gaussian kernel here, and if you got RBF kernel here and a Gaussian approximation to the equation here, then it's just Gaussian Gaussian integral from the limits of minus Infinity to Infinity, so that's a valid thing.",
                    "label": 0
                },
                {
                    "sent": "Now, this sort of system has been talked about as a convolution process without necessarily worrying about what the physical interpretation is.",
                    "label": 0
                },
                {
                    "sent": "Personally, I find it helpful to think of what the physical interpretation is when putting the system together, but Dave Higdon in 2002 sort of talked about how you could use this for multi output Gaussian processes, so this is very general way of doing multi output Gaussian processes and in the paper we describe how several other ways can be seen as specific cases where you're using Delta functions or white noise in here.",
                    "label": 0
                },
                {
                    "sent": "Of doing a multi output Gaussian process, of course, the nasty thing about it is these convolutions mean you get this full covariance matrix with off diagonal blocks.",
                    "label": 0
                },
                {
                    "sent": "And you don't have the chronica structure that Chris described in his previous talk, so the inverse of this covariance matrix is going to be order N ^3 ^3.",
                    "label": 0
                },
                {
                    "sent": "So it's clearly not going to be very useful.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've gone from common filters with I guess N. An Q squared I suppose there too N cubed cubed which isn't going to attract many people, but what we've sort of showing in this paper is that we can do something about it.",
                    "label": 0
                },
                {
                    "sent": "So the model is conditionally independent given F of T. So if you know what this function is, you know what all these axes are exactly.",
                    "label": 0
                },
                {
                    "sent": "You can do that convolution.",
                    "label": 0
                },
                {
                    "sent": "It comes out exactly, so the assumption we made is that you can assume conditionally independent given some observations of F of T. So time based observations.",
                    "label": 0
                },
                {
                    "sent": "Now these are a little bit like this is inspired by the idea of pseudo inputs that was in at work and is also being used in previous thing, but we were particularly thinking sort of pseudo inputs when we did this that the timing of these observations of this latent function are like pseudo inputs to the system.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing, although we didn't quite know.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the result of doing this making this assumption leads to a very similar approximation to the Pizzi approximation.",
                    "label": 0
                },
                {
                    "sent": "the PC approximation is block diagonal approximation plus an out a special type of outer product that we're keen and Carl talked about.",
                    "label": 0
                },
                {
                    "sent": "One of the problems with that as Ed sort of pointed out an AI stats paper is that you don't know what you should block off these different observations, so you've got this block diagonal structure and you don't know who to put in which block.",
                    "label": 0
                },
                {
                    "sent": "Here the block diagonal structure, the blocks of the different outputs.",
                    "label": 0
                },
                {
                    "sent": "So it's completely obvious what you should put in each block and the result of doing this is that the complexity goes from order N ^3 D ^3 to order N ^3 K ^2, where K is the number of pseudo inputs you choose to keep.",
                    "label": 0
                },
                {
                    "sent": "And the storage is the same, it's decay times two order and squared.",
                    "label": 0
                },
                {
                    "sent": "So this I mean apart from this sort of K factor, this is taking us back to the regime of independent Gaussian processes, which is your main competitor in this domain that you just trained on each output separately.",
                    "label": 0
                },
                {
                    "sent": "So we're getting back to that sort of complexity now.",
                    "label": 0
                },
                {
                    "sent": "It can also do a Fitzy style approximation which isn't so natural.",
                    "label": 0
                },
                {
                    "sent": "The Pizzi stuff came out of the natural thing that we felt you could do, because we can see this conditional independence given the whole of F. So we've got a very good idea of.",
                    "label": 0
                },
                {
                    "sent": "When it's going to work, it works well.",
                    "label": 0
                },
                {
                    "sent": "If F is very smooth and if the points are closer together than the length scale of the latent F. Here it's less clear, but you can do a fit C style approximation which then takes you down to the NDK squared complexity in NDK storage.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is some tide sensor network now and this is a multi output GP case where there's some sensors in the Solent of what tide height is and what you're interested in is how can you deal with the fact that one sensor may get knocked out so you lose a sensor and you want to still make predictions during that time but all the other sensors are on Now if you've just got temporal predictions so in this case we're using fairly dumb model were ignoring the sort of tidal frequency that exists in the soul and we're just using effectively RBF.",
                    "label": 0
                },
                {
                    "sent": "A large multi output RBF built.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These convolutions, if you do that, and you if you don't do that, and you have an independent Gaussian process prediction, then when you lose the sensor, the model doesn't know anything about what's going on.",
                    "label": 0
                },
                {
                    "sent": "'cause it doesn't know about this oscillating structure.",
                    "label": 0
                },
                {
                    "sent": "But in this case, when you've got this partially independent training, conditional stuff going on, you actually can see that you recover because you've seen what goes on other sensors.",
                    "label": 0
                },
                {
                    "sent": "You recover the correct answer so that pretty much sits exactly over the truth, this is.",
                    "label": 0
                },
                {
                    "sent": "Same example, but different set of sensors.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of quite nice.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can use these things effectively.",
                    "label": 0
                },
                {
                    "sent": "So another data set that Mauricio looked at was.",
                    "label": 0
                },
                {
                    "sent": "We've heard about Co creaking a bit and there's an entire book about this data set called the juror data set, which is data set of heavy metal concentration in the Swiss Jura.",
                    "label": 0
                },
                {
                    "sent": "Is that the one with the River or was it the other one with the River?",
                    "label": 0
                },
                {
                    "sent": "The other ones with the River.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you've just got some input locations and measurements at each location of some heavy metal concentrations.",
                    "label": 0
                },
                {
                    "sent": "You're interested in cabin and copper, but you also have nickel and zinc.",
                    "label": 0
                },
                {
                    "sent": "As well, so the idea is you use these easy, cheap to measure nickel and zinc.",
                    "label": 0
                },
                {
                    "sent": "They're cheap to measure to try and predict what the cadmium and copper is as well.",
                    "label": 0
                },
                {
                    "sent": "So using a multi output GP this is a classical Geo stats data set an I think large section of the book is about how you do Coke rigging.",
                    "label": 0
                },
                {
                    "sent": "Specifying the right kernel for Coke rigging.",
                    "label": 0
                },
                {
                    "sent": "So very hand design kernel which incorporates a lot of their Geo statistical knowledge.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be machine learning and black box about it and just see how good our algorithm, which takes none of this knowledge in.",
                    "label": 0
                },
                {
                    "sent": "But has an underlying physical interpretation, which is heat.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fusion works, so these plots show for the cadmean predictions and the copper predictions on the right you see Coke rigging results taken from that book.",
                    "label": 0
                },
                {
                    "sent": "So these are the results that they get on mean absolute error.",
                    "label": 0
                },
                {
                    "sent": "What you see on the left is this is the full Gaussian process, so this is without the approximation because the datasets fairly small, so it destroys Coke rigging for this one, and it's a little bit better than Coke rigging for this one, and then this is Pizzi with multiple runs of Pizzi to get different.",
                    "label": 0
                },
                {
                    "sent": "With different numbers of.",
                    "label": 0
                },
                {
                    "sent": "Inducing points from 5000.",
                    "label": 0
                },
                {
                    "sent": "Two hundred 500.",
                    "label": 0
                },
                {
                    "sent": "So that's the improving performance and we don't get quite down to the full Gaussian process here and here.",
                    "label": 0
                },
                {
                    "sent": "We actually don't get anywhere near the full Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And what's quite interesting about this one?",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with these methods, you'll know that in these Pizzi and Fitzy methods, there's never any point in having more inducing variables and data points.",
                    "label": 0
                },
                {
                    "sent": "If you want to recover the exact thing, and these are fixed location inputs, so we're not optimizing with respect to these inducing inputs, But here we've actually got 500 inducing inputs.",
                    "label": 0
                },
                {
                    "sent": "259 observations that I won't go into the details of why that you can still keep doing that, but it's because your assumption only becomes correct once you've observed the entire function.",
                    "label": 0
                },
                {
                    "sent": "So the assumption of this petsay becomes correct.",
                    "label": 0
                },
                {
                    "sent": "Once you've got infinite data points here, so it's conceivable that you can have far more that K can be larger than N, and you still don't get it.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is all town.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That sort of covers the complexity.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "Larger than any situation, you don't have to pay points on any occasion, no?",
                    "label": 0
                },
                {
                    "sent": "We assumption is going back to the convolution, so where the.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the what the assumption is is that this is independent given F of you, which is true.",
                    "label": 0
                },
                {
                    "sent": "So if I observe the whole of F of you, it's true, so the difference between Pizzi and this is that you're integrating across those observations, so the whole function is important, not just the value of the function at the particular time, so it's certainly conceivable.",
                    "label": 0
                },
                {
                    "sent": "OK, to do this approximation you have to have a smooth F. You don't have a smooth F. If you do what Chris did and have a white noise F, you're not going to get anything serious.",
                    "label": 0
                },
                {
                    "sent": "We're not going to get approximate the true model with these sort of things.",
                    "label": 0
                },
                {
                    "sent": "You might get something out.",
                    "label": 0
                },
                {
                    "sent": "So you can in that case you would have to have infinite inducing points.",
                    "label": 0
                },
                {
                    "sent": "Here, it's perhaps indicative of the latent fun.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean is moving fairly quickly, I guess in this case, so you're not getting a good approximation for your conditional independence to be true, sorry.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah I this Pixie Pixie approximation.",
                    "label": 0
                },
                {
                    "sent": "Your model, will you have the physical model in the physical model is very simple, one in this it's just a heat equation.",
                    "label": 0
                },
                {
                    "sent": "And no boundaries approximation to the heat equation.",
                    "label": 0
                },
                {
                    "sent": "Cokriging they put they put loads of stuff there, Geo statisticians.",
                    "label": 0
                },
                {
                    "sent": "I could say the whole it's right.",
                    "label": 0
                },
                {
                    "sent": "The whole book is about this data set, right?",
                    "label": 0
                },
                {
                    "sent": "So they have the same.",
                    "label": 0
                },
                {
                    "sent": "As you would with no Coke rigging the way they do it is well Dan would know better than life, so I'm not gonna.",
                    "label": 0
                },
                {
                    "sent": "That is really comparison between and.",
                    "label": 0
                },
                {
                    "sent": "You can really see the effect of your physical model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a case where dumb black box.",
                    "label": 0
                },
                {
                    "sent": "Beats intelligent statistician.",
                    "label": 0
                },
                {
                    "sent": "So in this case we've replaced a statistician.",
                    "label": 0
                },
                {
                    "sent": "He can now be removed from the Earth.",
                    "label": 0
                },
                {
                    "sent": "I think the book's quite old, so maybe they do better today.",
                    "label": 0
                },
                {
                    "sent": "This is this is really encouraging result, yes, but in this case here with our approximations I mean the size of the data set is small.",
                    "label": 0
                },
                {
                    "sent": "That's why we can do the full GP.",
                    "label": 0
                },
                {
                    "sent": "We're really looking for larger datasets in 259 data points, but certainly we can do the full GP in these sort of smallish datasets like this.",
                    "label": 0
                },
                {
                    "sent": "And maybe there are computational.",
                    "label": 0
                },
                {
                    "sent": "I have the reference.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The date of the book that's 97, was that a second printing, maybe multiple?",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it may have originally been older than that, and they may have not had the computational power to do the thing we're doing here.",
                    "label": 0
                },
                {
                    "sent": "So we have to be a bit careful.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's nice that that model is getting there, but it's really simple one, it's just the two colonels.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian process is RBF covariance, and the kernel is a Gaussian, so you just end up with an RBF in the end.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "But you also estimated counter answer the question.",
                    "label": 0
                },
                {
                    "sent": "Well, what there is yeah in there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's but actually we're biased towards wanting the latent function to be smooth to make our approximation better.",
                    "label": 0
                },
                {
                    "sent": "I don't think we're explicitly doing anything to cause that to happen, but there is a. Yeah, I mean, I think we haven't got the ideal parameterization of that yet.",
                    "label": 0
                },
                {
                    "sent": "I didn't show it.",
                    "label": 0
                },
                {
                    "sent": "I was just saying that your convolution.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Salt of your convolution here could be the result of the heat equation.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's the model we're using here.",
                    "label": 0
                },
                {
                    "sent": "That's the closest physical model behind it.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next thing I just wanted to briefly talk.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "What you can do if you've got nonlinear regulation in the system, so nonlinear activation is something that happens, for example in the.",
                    "label": 0
                },
                {
                    "sent": "So I showed you earlier, the P 53 gene transcriptional system where your FT isn't a linear response.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear differential equation, but there's a nonlinear function of your Gaussian process, so this is bound to be the case in a chemical system like that because the concentration has to be positive and a Gaussian process could be negative, so you always get this sort of thing turning.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what's quite nice about these sort of systems, so this is being used by Rogers and Girolami who did something similar to what I talked about before, but it didn't use a GP prior over FT. And then also, there's it's being used in a repression case, so you can get these nonlinearities still having linear differential equations, you can solve the differential equation, but the result is not a Gaussian process jointly across your ex.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an F. So, um.",
                    "label": 0
                },
                {
                    "sent": "I love this bit because what you can do is you can do Laplace's approximation and you use Laplace's method, Laplace 1774.",
                    "label": 0
                },
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "Just a point.",
                    "label": 0
                },
                {
                    "sent": "So I read this translation of this paper recently.",
                    "label": 0
                },
                {
                    "sent": "Did you know that is the first introduction of the Gaussian in the entire academic history?",
                    "label": 0
                },
                {
                    "sent": "The Gaussian was invented by Laplace to do the Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "He was doing a beta binomial system and he was looking for a beta posterior and he couldn't compute.",
                    "label": 0
                },
                {
                    "sent": "We didn't call it that he couldn't compute the beat posterior 'cause he didn't have all these beta functions and whatnot, and so he did.",
                    "label": 0
                },
                {
                    "sent": "They go to the class approximation, which is a Gaussian approximation, and in the limit he did it in.",
                    "label": 0
                },
                {
                    "sent": "It was valid as well.",
                    "label": 0
                },
                {
                    "sent": "Here it isn't so valid, so you can do it Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "So class was ahead of us in 1774.",
                    "label": 0
                },
                {
                    "sent": "Well it took them 40 years after that.",
                    "label": 0
                },
                {
                    "sent": "Come up with least squares or something.",
                    "label": 0
                },
                {
                    "sent": "Amazing.",
                    "label": 0
                },
                {
                    "sent": "So, but here this was about gambling, right?",
                    "label": 0
                },
                {
                    "sent": "So that was important.",
                    "label": 0
                },
                {
                    "sent": "Then this is about loaded dice, or apparently calls them in the paper English dice.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the Michaelis Menten activation has this particular form.",
                    "label": 0
                },
                {
                    "sent": "Now this is a linear system now, and the linear system you see we infer a negative protein.",
                    "label": 0
                },
                {
                    "sent": "But with this constraint, which keeps things positive in terms of E to the F of T, which is now the protein, then we get this different form and the question is.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is little class approximation good here in the car list?",
                    "label": 0
                },
                {
                    "sent": "As I may have a little time to show, you has done a sampling approach MCMC approach which is really fast and really good and we can show that it is pretty good in this case.",
                    "label": 0
                },
                {
                    "sent": "But there are going to be other cases where it's not very good.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can do a lot less approximation.",
                    "label": 0
                },
                {
                    "sent": "This means you get to reference 1774 papers.",
                    "label": 0
                },
                {
                    "sent": "So Michalis, as I say, that those samples were from an approach that Michalis is used to sampling Gaussian processes, and I'll just briefly say that this involves what we call using control points for sampling.",
                    "label": 0
                },
                {
                    "sent": "And again, it's inspired by these sparse Gaussian process techniques.",
                    "label": 0
                },
                {
                    "sent": "The idea is you select a bunch of inducing variables as they call in these sparse Gaussian process techniques, and given those you sample functions from the system.",
                    "label": 0
                },
                {
                    "sent": "And you can then sample what the point is by sampling over the control point separately.",
                    "label": 0
                },
                {
                    "sent": "You can move around the space very quickly, but you always get samples that are consistent with your Gaussian process prior when you sample the full function and we found we initially thought that this sampling.",
                    "label": 0
                },
                {
                    "sent": "Would be too slow to be practical, that's why we did the last approximation.",
                    "label": 0
                },
                {
                    "sent": "But actually the way mechanics has implemented it's extremely fast and very practical for these small.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All data sets here so.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll skip through the.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Details of that and just show you these are results of.",
                    "label": 0
                },
                {
                    "sent": "The P53 system again, but these are.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are the.",
                    "label": 0
                },
                {
                    "sent": "I think these are yeah, so these results you've seen before and this is the inferred protein levels.",
                    "label": 0
                },
                {
                    "sent": "This is with barrancos results superimposed, and this is with Nicholas is results from the HMC sampling overall parameters of the system as well.",
                    "label": 0
                },
                {
                    "sent": "So it's full Bayesian sampling and how long did that take to run Michalis?",
                    "label": 0
                },
                {
                    "sent": "4 hours so I mean it's only a few data points.",
                    "label": 0
                },
                {
                    "sent": "6 genes and seven time points, Regina.",
                    "label": 0
                },
                {
                    "sent": "Manuel said 1 million iterations of.",
                    "label": 0
                },
                {
                    "sent": "So basically that's really quite fast compared to the last approximation.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, OK those are.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the final sort of four minutes, just sort of say another extension we've looked at, which was work with Auntie Hunkeler, who visited us in Manchester.",
                    "label": 0
                },
                {
                    "sent": "So actually you could start cascading these differential equations as well.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's again in.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Powered by transcriptional regulation, the data is from the.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furlong lab in Heidelberg.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that you observe the transcription factors gene.",
                    "label": 0
                },
                {
                    "sent": "And that is producing protein, so this is a model of translation that you see the gene and it produces protein from the gene and then from the M RNA.",
                    "label": 0
                },
                {
                    "sent": "You then produce other genes.",
                    "label": 0
                },
                {
                    "sent": "So you're getting a constraint above and below, so you're observing.",
                    "label": 0
                },
                {
                    "sent": "You can't always do this because of things like phosphorylation which make proteins active in biology.",
                    "label": 0
                },
                {
                    "sent": "But this is development system.",
                    "label": 0
                },
                {
                    "sent": "It's reasonable to think that phosphorylation is less involved in these sort of systems that it is in, say, signaling network.",
                    "label": 0
                },
                {
                    "sent": "So we've got this simple OD model of.",
                    "label": 0
                },
                {
                    "sent": "Translation and a simple ODI model of transcription.",
                    "label": 0
                },
                {
                    "sent": "And it turns out with a lot of integrating.",
                    "label": 0
                },
                {
                    "sent": "It's auntie did.",
                    "label": 0
                },
                {
                    "sent": "You can find the solution for this.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both systems and get the joint covariance over the input transcription factor.",
                    "label": 0
                },
                {
                    "sent": "The input M RNA to the transcription factor.",
                    "label": 0
                },
                {
                    "sent": "The driven latent force.",
                    "label": 0
                },
                {
                    "sent": "So you're forcing two things you're forcing from there to force that to force that.",
                    "label": 0
                },
                {
                    "sent": "So it's like a hierarchy or a cascade of differential equations.",
                    "label": 0
                },
                {
                    "sent": "You could do a third level, but how many terms were in the two levels?",
                    "label": 0
                },
                {
                    "sent": "20 or something.",
                    "label": 0
                },
                {
                    "sent": "You've got about 20 double integrals to do today.",
                    "label": 0
                },
                {
                    "sent": "At the third level out, but we think we may be able to.",
                    "label": 0
                },
                {
                    "sent": "I mean, we hope we could do things to maybe.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Provide and look at the higher level things.",
                    "label": 0
                },
                {
                    "sent": "So in this case you got the input M RNA and then you've got targets.",
                    "label": 0
                },
                {
                    "sent": "Two targets of the M RNA, and then we infer the latent missing function of the pro.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change between the two.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I hope to sort of show you as well, which we don't have results on yet, but I think we'll have them soon.",
                    "label": 0
                },
                {
                    "sent": "Is how you can use these sort of 2nd order differential equations to model things like motion capture data where you have these physical things going on and you have a time based input.",
                    "label": 0
                },
                {
                    "sent": "Other thing I'm very excited about and I can mention this because it's also a NIPS paper is.",
                    "label": 0
                },
                {
                    "sent": "Ben called ahead.",
                    "label": 0
                },
                {
                    "sent": "A student marcher also means they came up with this idea of using Gaussian process is to help solve nonlinear differential equations and placing Gaussian process priors over what the solution should be.",
                    "label": 0
                },
                {
                    "sent": "And one of the things I'm very excited about is enhancing those Gaussian process priors with linear, solvable physical models of the system, which captures some idea of what's going on in the system.",
                    "label": 0
                },
                {
                    "sent": "But they're not nonlinear, so you can do this joint sampling to improve the speed of which you can solve these nonlinear differential equations.",
                    "label": 0
                },
                {
                    "sent": "And given that a, I think what's this?",
                    "label": 0
                },
                {
                    "sent": "What's the fastest sampling speed for those MCMC tests about a week for getting a base factors out?",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "Say it takes a long time to get Bayes factors out, and we're looking to get larger systems done spatial systems, so we need these speedups to try and get the Bayes factors to do science.",
                    "label": 0
                },
                {
                    "sent": "In the actual real world systems, so we're looking to do that sort of thing as well.",
                    "label": 0
                },
                {
                    "sent": "And then there's also lots of work on stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "We haven't had a stochastic component here, we're just still trying to understand whatever.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Else is doing there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the acknowledgements.",
                    "label": 0
                },
                {
                    "sent": "A lot of this work is joint with Magnus, who's my copii on two grants that have gone into this post docs, Pai Gow Auntie who visited us.",
                    "label": 0
                },
                {
                    "sent": "Michalis is posted with this maricio.",
                    "label": 0
                },
                {
                    "sent": "I thought it should be said is a PhD student.",
                    "label": 0
                },
                {
                    "sent": "Me and Jennifer, PhD student with Magnus and then data from these sources and two.",
                    "label": 0
                },
                {
                    "sent": "2M Research Council awards.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Can there be no.",
                    "label": 0
                },
                {
                    "sent": "I worry about the SG so.",
                    "label": 0
                },
                {
                    "sent": "I think we are no.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could obviously do this if you've got Gaussian noise driving our system, and you could obviously enhance what we've talked about with the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's Chris is shown.",
                    "label": 0
                },
                {
                    "sent": "That's just adding an extra covariance term on.",
                    "label": 0
                },
                {
                    "sent": "But I would say that we don't know enough about.",
                    "label": 0
                },
                {
                    "sent": "I think we're kind of interested in things like whether the MCMC method can be applied in some form to SDS, but I think we're not sure if it can be.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we're still trying to understand your work.",
                    "label": 0
                },
                {
                    "sent": "And Andrew and Darren's work in this area try and get a better handle on that.",
                    "label": 0
                },
                {
                    "sent": "We're leaving that to you for the moment.",
                    "label": 0
                },
                {
                    "sent": "We don't have it yet.",
                    "label": 0
                },
                {
                    "sent": "Big thing missing data.",
                    "label": 0
                },
                {
                    "sent": "Benefit yet.",
                    "label": 0
                },
                {
                    "sent": "Things like that can be done by computer algebra.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, actually what I think.",
                    "label": 0
                },
                {
                    "sent": "My hope is that we when these Earths come out you know it's a covariance which is based on earth surface into symmetric function.",
                    "label": 0
                },
                {
                    "sent": "They always come out in pairs.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And my vague hope is that manually we might you should you can pair these things up and see that a pair of them is a kernel in itself.",
                    "label": 0
                },
                {
                    "sent": "So you're actually getting a sum of kernels out of the system when you do the integrals, so I'm curious if we haven't, we haven't done yet is sit down and pair up all those kernels and see if you can apply the cover.",
                    "label": 0
                },
                {
                    "sent": "You can then apply independently to each of those, and it may be that you can automatically compute computer algebra is another option, but that sort of manual alternative.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you know Earth isn't as quick as exponential to compute.",
                    "label": 0
                },
                {
                    "sent": "In you know, in the base case, so already when you're using this Earth, which is not complex, I think the speed of computation versus the Gaussian kernel is maybe 10 times slower.",
                    "label": 0
                },
                {
                    "sent": "And then when you go to the complex Earth, you're in more trouble.",
                    "label": 0
                },
                {
                    "sent": "So it's but a lot of our problems are more about things that we didn't have to worry about before 'cause the kernel computation was relatively quick.",
                    "label": 0
                },
                {
                    "sent": "Now we have to worry and think about and so we hope we can tidy a lot of them up and still get it reasonably efficient.",
                    "label": 0
                },
                {
                    "sent": "But you'll see my abstract.",
                    "label": 0
                },
                {
                    "sent": "It says given time I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Motion capture.",
                    "label": 0
                },
                {
                    "sent": "Well I didn't say is I mean given the computation time rather than the given time in the actual talk.",
                    "label": 0
                },
                {
                    "sent": "Laplace approximation is exactly yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's true.",
                    "label": 0
                },
                {
                    "sent": "You can do these, yes, but it's not as.",
                    "label": 0
                },
                {
                    "sent": "For the what Magnus is saying is that if it's for these cases here.",
                    "label": 0
                },
                {
                    "sent": "So it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bit that Magnus knows that you won't have heard is that this is the interesting thing about when you do this Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "To solve this Laplace approximation, you only need a 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Integral numerical integral rather than the two dimensional double integral you get if you're computing the covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is a really good thing to do if you can't compute this convolution for your kernel, because if you can't compute that convolution for your kernel, you have to do a double numerical integral over the covariance.",
                    "label": 0
                },
                {
                    "sent": "But it turns out by using the Laplace approximation you can lose that double numerical integral and just do a single numerical integral, which is obviously much nicer.",
                    "label": 0
                },
                {
                    "sent": "So the scope to do this for funkier covariances, and indeed we have done with what I call them.",
                    "label": 0
                },
                {
                    "sent": "MLP covariance, but it's the one Chris Williams derived is an infinite neural network.",
                    "label": 0
                },
                {
                    "sent": "Do it.",
                    "label": 0
                },
                {
                    "sent": "Magnus may be better placed to answer that than I, but you do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you basically are getting a.",
                    "label": 0
                },
                {
                    "sent": "You compute the error bars through a.",
                    "label": 0
                },
                {
                    "sent": "We do have it, we don't get it in continuous space.",
                    "label": 0
                },
                {
                    "sent": "We have a discretization of the integral and we do the integral by Simpson's rule.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Um, so we don't have a continuous, but can you write down a continuous differential equation?",
                    "label": 0
                },
                {
                    "sent": "For that.",
                    "label": 0
                },
                {
                    "sent": "It is for the diagonal, yeah?",
                    "label": 0
                },
                {
                    "sent": "Here already.",
                    "label": 0
                },
                {
                    "sent": "Then the other thing about this technique, as Magnus did in his post Doc with David side I think is you can do nonlinear differential equations and this still works.",
                    "label": 0
                },
                {
                    "sent": "So you can put another.",
                    "label": 0
                },
                {
                    "sent": "You have a nonlinear term on the decay.",
                    "label": 0
                },
                {
                    "sent": "You can still do this Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We don't have the data we've got loads of brilliant stuff without any application.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Celebrate darling.",
                    "label": 0
                },
                {
                    "sent": "King you've shown us.",
                    "label": 0
                },
                {
                    "sent": "Gaussian symmetric predicted distributions over concentrations, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Albayzin are we being well?",
                    "label": 0
                },
                {
                    "sent": "That's the point.",
                    "label": 0
                },
                {
                    "sent": "We're not that Black box now, but maybe you know, maybe it doesn't matter, because in this case you're assigning.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things that are impossible.",
                    "label": 0
                },
                {
                    "sent": "Rather than assigning zero match things that you know are possible.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the one where the posterior actually an error bar goes negative.",
                    "label": 0
                },
                {
                    "sent": "Tony Ohagan talked about this eliciting prior distributions.",
                    "label": 0
                },
                {
                    "sent": "You remember that where he was saying and he was basically running a prior Gaussian process prior over a probability distribution function, and those were going to negative two occasions when the rejecting samples.",
                    "label": 0
                },
                {
                    "sent": "I thought maybe we could try that rejection samples technique, but that doesn't mean I think it's not that great, but it's something you could do, but this is really pointless.",
                    "label": 0
                },
                {
                    "sent": "Approximation doesn't suffer that problem because now you're put a GP over the log concentration.",
                    "label": 0
                },
                {
                    "sent": "And then Nicholas is sampling approach again.",
                    "label": 0
                },
                {
                    "sent": "Eliminates that is a problem.",
                    "label": 0
                },
                {
                    "sent": "I, I think that this is so.",
                    "label": 0
                },
                {
                    "sent": "I've sort of.",
                    "label": 0
                },
                {
                    "sent": "I'm mixing because I didn't have the results on some of the things we've done where we just know we've got the model wrong.",
                    "label": 0
                },
                {
                    "sent": "I'm mixing two concepts in here.",
                    "label": 0
                },
                {
                    "sent": "This is this split between science and engineering.",
                    "label": 0
                },
                {
                    "sent": "I think that the linear system as an engineering approach and add in what we call a crud catcher where you add in a block diagonal independent GPS to try and deal with anything you haven't modeled correctly.",
                    "label": 0
                },
                {
                    "sent": "I think that's really sensible as a black box type approach to systems you think this thing might be going on 'cause it's not going on.",
                    "label": 0
                },
                {
                    "sent": "The crud catcher will pick it up.",
                    "label": 0
                },
                {
                    "sent": "That's the beauty of blades.",
                    "label": 0
                },
                {
                    "sent": "But the crowd catcher is regression on its own.",
                    "label": 0
                },
                {
                    "sent": "Where is Nicholas is stuff about is MCMC and a lot of what Mark's done in this domain is science and then you have to do your MCMC and estimate your base factors and worry a lot about those things.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}