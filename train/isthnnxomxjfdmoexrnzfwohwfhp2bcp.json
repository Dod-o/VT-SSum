{
    "id": "isthnnxomxjfdmoexrnzfwohwfhp2bcp",
    "title": "Concentration-Based Guarantees for Low-Rank Matrix Reconstruction",
    "info": {
        "author": [
            "Rina Foygel, Department of Statistics, University of Chicago"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/colt2011_foygel_matrix/",
    "segmentation": [
        [
            "So like a no hot stock will be looking at a setting where were partially observing a matrix, but in our work the settings a bit different and we're looking at the problem of reconstructing a low rank matrix.",
            "Someone just asked how to Hods results compared to compressed sensing, and that's that's one of our main questions.",
            "How the results we have from learning theory will compare to."
        ],
        [
            "Compressed sensing, so I'll start by defining the setting.",
            "We're looking at a matrix why that's partially observed, and it comes from a low rank signal that might or might not be corrupted by some noise.",
            "The Matrix is an N by N matrix an I'll just be assuming without loss of generality that N is greater than or equal to M and the goal here is simple, it's just to try to estimate the entries of the matrix that we haven't seen in the training sample."
        ],
        [
            "So, since we're assuming that the matrix is approximately low rank, the most natural thing to do would be to look for a low rank approximation where we're trying to measure where we're trying to minimize the error on the training sample, and hear us is that just gives the locations of the training sample.",
            "Unfortunately, even though this would be the most natural thing to do, it's a non non convex constraint and it would actually be NP hard to solve this problem.",
            "So what's most commonly done is to use a convex constraint such as the trace norm or the Max norm which I'll be looking at in detail.",
            "And the trace norm is what's used most commonly in the compressed sensing community.",
            "For this problem, a third approach is to do a local search with the rank constraint rather than looking for."
        ],
        [
            "Global minimum.",
            "So let's take a look quickly at some of the results that exist from the compressed sensing community.",
            "These are results that give guarantees on either exact or approximate reconstruction of the underlying low rank signal M and the setting that they work in has to assume some kinds of assumptions on that low rank matrix.",
            "Typically that's either saying that the entries of that matrix are uniformly bounded or that there are incoherence conditions, and this is a strict condition that basically means that information in the matrix is spread fairly evenly across the rows and the columns, rather than being concentrated.",
            "There's some pretty specialized assumptions on the noise as well, so it's typically assumed to be IID normal.",
            "They're looking at squared error and the bounds that they give are proved by looking at conditions under which the correct matrix, or roughly the correct matrix is optimal for the optimization problem, and overall, most of these results use the trace norm, and we can more or less conclude here that in this specialized setting for the estimation problem, meaning that we're trying to recover the values in the matrix M trace norm gives a good surrogate for rank.",
            "And we can use it to recover a low rank matrix."
        ],
        [
            "In contrast, we can look at the problem of binary classification.",
            "So the reference by Cerebro and Schreibman, called 2005, looks at this binary classification setting where instead of trying to recover the values of the matrix, they just want to recover the matrix up to the correct sign and another difference in this setting is that it's agnostic noise rather than a more specialized assumption, an independent noise.",
            "The bounds here are given with concentration guarantees rather than looking at optimality conditions.",
            "And overall, what they give here is learning guarantees with the Max norm in the trace norm, but they also show that in the classification problem that race Norman the Max norm do not always do a good job of recovering a low rank matrix, because in this setting there might be a low rank sign pattern that isn't recovered by a low trace, norm or Lomax."
        ],
        [
            "Sign pattern, so here's what we're interested in, what we'd like to do is take these concentration guarantees and use them to apply them to the estimation setting that's been approached with the compressed sensing point of view, and see how far we can get there so."
        ],
        [
            "We're interested in.",
            "More specifically, we'd like to know what guarantees we can get for estimation with the trace norm in the Max norm and how they're going to compare to the more specialized guarantees given with optimality conditions in the references I showed, we want to know whether Trace Norman Max Norm do give good surrogates for rank in this setting, like Cerebro and Treibmann showed for the classification setting.",
            "They don't, but what we're interested in is the estimation or regression setting.",
            "And finally, we'd like to know whether the Max norm could perhaps offer some benefit over.",
            "Excuse me over the trace norm for this."
        ],
        [
            "So let's start the first thing I want to do is just give a bit more details on the Max Norman the Trace Norm.",
            "So the trace norm is generally defined as the sum of singular values of a matrix.",
            "But we can also define it in a different way.",
            "We can look at factorizations of the Matrix X into a product U times V transpose, and then measure the sum of the row norms of yuan.",
            "The sum of the row norms of V. So what I'm going to do for this talk is actually look at the rescaled trace norm.",
            "So now will be looking at the average row Norman Yuan, the average row Norman V. And this makes the trace norm very comparable to the Max norm, which also looks at factorizations of the matrix.",
            "But now takes the high istro Norman you and the high istro Norman V. So what we can see here is basically where the trace norm looks at an average.",
            "The Max norm looks at a maximum, and also this shows us that the rescaled trace norm is bounded by the Max norm."
        ],
        [
            "An important property of these two norms is that if we take the unit balls for the rescaled trace norm in the Max norm, we can express them as convex hulls of similar classes for the trace norm, the unit ball is actually exactly equal to the convex Hull of all rank one matrices that have unit where the average entry is bounded by one and for the Max norm up to a constant.",
            "We have it as the convex Hull of all rank one matrices where all of the entries are plus or minus one.",
            "So where for the trace norm that average entry is 1 for the Max norm.",
            "The maximum entry is 1."
        ],
        [
            "So the most important thing about the trace norm in the Max norm though, is their relationship to rank, which we're using for all of our results today.",
            "So to give a bit of background on this for the trace norm, the trace norm is the L1 norm of the singular values of a matrix, and we can compare that to the Frobenius norm, which is the L2 norm of the singular values of the matrix.",
            "If we also take into account the fact that rank just tells us how many nonzero singular values there are, we can just use the relationship between L1 norm and L2 norm to get the inequality that we have.",
            "With some rescaling here, so another way to look at this is that if we have a matrix where this term is 1, meaning that the average entry has magnitude one, then the rescaled trace norm squared is bounded by the rank.",
            "So that's the relationship that we're going to use today, and for the maximum we have the same inequality, but instead of looking at the average magnitude of an entry, we use what I'm writing as X Infinity, which is the largest magnitude of an entry in the matrix.",
            "So I just want to stop and point out why this is so important to the estimation problem.",
            "When we're looking at estimating a matrix in practical settings, it's very reasonable, like oh, how was saying to assume that we're in a bounded setting, and so that the entries are bounded in in the matrix that we're trying to estimate.",
            "So in a practical setting, the trace norm and the Max norm will be bounded by."
        ],
        [
            "Drink.",
            "So next, let's look at the Rademacher complexities for these two norms, so Cerebro and Schreibman computed the Rademacher complexities, using the convex Hull properties for the Max norm, the Max norm ball lies inside the convex Hull of a finite set of matrices, and so they bounded the empirical Rademacher complexity on the order of square root ANOVA rest.",
            "So here N is the size of the matrix, an S is the number of samples for the trace norm.",
            "The situation is a little bit different, so again the convex Hull can be used to give a Rademacher complexity.",
            "And what they saw is that the Rademacher complexity, empirical Rademacher complexity for the trace norm ball is related to the expected spectral norm of a random matrix Sigma, where Sigma has random plus one and minus one entries at the locations where the samples were observed.",
            "So unfortunately, if the samples are observed in a degenerate way, meaning that they are all in one column, for example, an aren't uniformly spread across the matrix.",
            "What this will give us is that the Rademacher complexity is on the order of square root N M / S. So this is, uh, this means that if we want to get a non trivial bound on the Rademacher complexity, we'd need a sample size that's at least as large as N * M, which is basically like sampling the whole matrix, so we can't get a meaningful bound here."
        ],
        [
            "The good news is that we can bound the Rademacher complexity in expectation, so we were able to give a slightly tighter bounds than Cerebro, and Schreibman had, and this is using a very nice result from Joel Tropp and last year for the spectral Norm, and we have that the expected Rademacher complexity is now bounded, on the order of square root N log N over us, so this is now a meaningful bound, and this expectation is taken where the sample is drawn at random uniformly from the matrix.",
            "So now that we have these two bounds, we can use them to directly give a learning guarantee.",
            "In the situation of absolute."
        ],
        [
            "Error, so we recall that for absolute error the average error of the ERM is bounded by the average error of the best predictor plus a term that comes from the expected Rademacher complexity.",
            "So we can just directly plug in these terms for the Max norm when we're learning with absolute error, we have that the average absolute error of the fitted matrix is bounded by the size of the noise plus this term.",
            "Here coming from the Rademacher complexity so."
        ],
        [
            "In the low rank setting, the maximum ball we're looking at is has a nice bound be, so if we're looking at a low rank signal."
        ],
        [
            "Break are where the entries are bounded uniformly.",
            "Then the Max norm of the true signal is bounded by square root R. So if we fit inside of that Max norm ball and use this property up here, we can see that we get an excess error.",
            "Excess absolute error bound of epsilon as soon as the sample size is on the order of RN over epsilon squared.",
            "So this sample size inequality is up to a constant."
        ],
        [
            "For the trace norm, we can do the same thing and the only difference is that there's a log term in the Rademacher complexity, and now in the low rank set."
        ],
        [
            "All we need to assume about the entries of the low rank signal M is that they are bounded on average rather than bounded uniformly, so we have a weaker assumption on the underlying matrix, but then the sample complexity we need has an extra log factor in order to bound the excess absolute error."
        ],
        [
            "So here's what we have.",
            "We have absolute error excess bounds for the trace norm in the Max norm under a weaker or stronger assumption on the entries of M. But the existing results in compressed sensing give it for squared error.",
            "So that's what we did."
        ],
        [
            "To look at next.",
            "So we used a recent result from Cerebro St Herranen Tewari, which says that when we're looking at a smooth error with a mild bound, the excess average is an excess error when comparing to the best predictor is bounded by this here, which uses now the squared Rademacher complexity.",
            "So for the Max norm, if we plug in what?"
        ],
        [
            "We know we have that the squared error averaged over the matrix of our ERM, inside the Max norm ball is bounded by the equation up there and in the."
        ],
        [
            "Rank setting here's what we got.",
            "We got excess squared bounded by Epsilon as soon as our sample size is up to a constant this big.",
            "So let's just look at this.",
            "The first term, if it was by itself, would be the best possible dependence on rank, matrix size and epsilon that we could hope for.",
            "But we also have the second term here, which is very interesting if the noise is 0 or if the excess error we're willing to allow is not very small relative to the noise, then the second term behaves like a constant and our sample size grows with one over epsilon.",
            "But if we want a small excess error in the noisy setting.",
            "Then the second term behaves as one over epsilon and the sample size grows as one over epsilon squared.",
            "So we call this the optimistic rate, meaning that depending on the noise level and the excess error, we want, the sample size can grow as either one over epsilon or one over epsilon."
        ],
        [
            "Weird.",
            "So here's what we have so far.",
            "For absolute error, we can give results when either we assume a weaker average bound on the entries or a stronger uniform bound on the entries of the underlying signal for squared errors.",
            "So far we only have a result for this stronger assumption on.",
            "The entries are bounded uniformly, so could we get a result when the unit when the entries are bounded only on?"
        ],
        [
            "Average.",
            "So for the absolute case, the bound that we used was based on the expected Rademacher complexity, which is fine for both the trace norm and for the Max norm, but the Cerebro CERT.",
            "Haran and Tewari result we used to produce are bound for the squared error case uses this worst case Rademacher complexity, and it's not possible to get a meaningful bound on that for the trace norm, so we can't use the same proof technique.",
            "But the question is, is it still possible to have the weaker assumption of an average bound on the entries of M and get a result on squared error learning?"
        ],
        [
            "So it turns out that this is actually not possible.",
            "We can construct examples that are low rank and that have bounded average error, but we're squared.",
            "Error learning does not occur, so there's no way to get to use this weaker assumption and get a squared error learning guarantee.",
            "Of course, there is something we can do with the trace norm if we only consider bounded matrices.",
            "So with this stronger assumption, squared loss becomes a Lipschitz function, and so we can use the same techniques.",
            "We used an absolute error to get a bound for excess squared error with.",
            "Now this sample size RN, Logn over epsilon squared."
        ],
        [
            "So let's see what we have for the absolute error case, we can get results under either assumption on M. For the squared error case, we need to assume that the entries of EM are bounded uniformly and what and?",
            "So looking at trace Norm versus Max Norm we see that the trace norm doesn't actually offer any theoretical benefit over the Max norm.",
            "Even if we could find a way of showing an optimistic rate dependence on epsilon for the trace norm, because even in that case we'd still have this extra log term here.",
            "So it looks like for squared error the Max norm is giving us the stronger theoretical result here."
        ],
        [
            "So so far I've only looked at the problem of bounding the excess error in the reconstruction of the matrix Y.",
            "So now I need to point out a couple of subtleties in this.",
            "The first thing is that we might want to instead guarantee an error in reconstructing the low rank signal M, which is what's done in the compressed sensing literature, and another issue is what is our sampling model?",
            "In particular, are we sampling with replacement or without RIP?"
        ],
        [
            "Basement.",
            "So what we've done so far is sampling with replacement, because the concentration guarantees use IID sampling, and the noise is per entry, which basically means that if we observe the same location multiple times, we see the same value there each time, and this is this is what gives us the results on the excess error for Y.",
            "If we want to try to recover the low rank Cigna."
        ],
        [
            "M then in order to do that, we're going to have to assume that the noises in dependently generated and at each observation Anna Zero mean and we also need in both these cases of very mild bound on the size of the noise.",
            "If we're sampling with replacement and we want to recover M, what this means is the noise has to be generated per observation rather than per entry.",
            "So if we see the same entry multiple Times Now, the noise has to be generated independently each time that we see that entry.",
            "And so if we assume these things then we get the same results we had before.",
            "With those same sample complexities, we now have a result that bounds the error in the reconstruction of the low rank signal M rather than bounding the excess error in reconstructing the matrix."
        ],
        [
            "Alternately, we can think about sampling without replacement, so in this case the concentration guarantees don't directly apply, because sampling without replacement is not IID.",
            "But we have a general result that says that in the sampling without replacement setting, we can get the same learning guarantees and in the sampling with replacement settings that we considered.",
            "So, in particular, if we want weaker assumptions on the noise, so agnostic noise that only has to satisfy very mild bound, then sampling with or without replacement we can get this excess error bound.",
            "On recovering why with the same sample complexity we had before up to a constant, and if we want to have stronger assumptions on the noise that it's independently generated at each observation and zero mean, then we have the same result as we had before on reconstructing the matrix M and so we have the same results in this setting up to possibly a constant factor cost as we had for sampling with replacement."
        ],
        [
            "So now we've covered all of our results.",
            "What I'd like to do now is compare a bit between what we have and what the results that exist in the compressed sensing community.",
            "So the results that exist.",
            "There's very many of them, just in the last few years, and they all use very different settings and give the results in a somewhat different way.",
            "So it's a bit tricky to compare between them and to compare to ours, but let me try to give sort of a general sense of the results that are out there."
        ],
        [
            "So the first type of result we have is results that give a guarantee of exactly recovering the low rank signal M. In this case, the matrix M has to satisfy strict conditions like in coherence.",
            "In order for this to be possible.",
            "And of course there's no noise in this setting.",
            "The sample complexity that these results give is RN Logn over epsilon, so the dependence on epsilon is very nice.",
            "It's only one over Epsilon, which is better than the optimistic rate we saw before and the guarantee of course is that the fitted matrix gives the low rank signal exactly."
        ],
        [
            "Now let's look at results that give recovery of M, but only up to some error.",
            "We've actually split these into two categories because there's two different flavors that are out there for this type of result.",
            "The first type we're going to call near exact recovery of M because it's very similar to the exact recovery setting.",
            "In this setting, the same incoherence conditions are assumed, but the noise is now allowed to be iid normal or sub Gaussian, and the type of guarantee that's being given is instead of exact reconstruction, we have squared error in the reconstruction bounded.",
            "In a way that scales with the size of the noise.",
            "So in particular, if the noise goes to 0, then that bound goes to 0 as well and we would get exact reconstruction.",
            "So in a sense we can look at this as a generalization of the exact reconstruction results that are out there and the sample size here is the same."
        ],
        [
            "The second category of results giving reconstruction up to an error.",
            "We've called approximate recovery, and this has a somewhat different setting.",
            "So now instead of the strong incoherence condition on M, we just require that the entries of EM are bounded uniformly.",
            "The sample size is the same, and then the result is then a little bit of a weaker guarantee because there were weaker assumptions.",
            "So now there's a bound on error which no longer scales down with the size of the noise, so even in a noiseless setting with less assumptions on EM, we no longer get.",
            "Exact reconstruction.",
            "So looking at this, this is actually very close to the setting that we're working in, so our results.",
            "First, let's look at the ones that look at recovery of M."
        ],
        [
            "Our results use the same assumptions on M and have the same guarantee, which is that we're trying to bound the excess squared error in recovering M. The differences are in our sample size and in our noise model for the noise model we have a much more general setting because we have independent and zero mean noise with only a mild bound condition which we could satisfy with, for example, a finite 5th moment rather than assuming that the noise has to be sub Gaussian.",
            "Another difference is looking at the sample size, so a benefit of our result is that we've lost this log factor here, and that's because of.",
            "Using the Max norm, but this comes at a cost, which is that we now have instead of the one over epsilon dependency.",
            "We have an optimistic rate dependency on epsilon and one another aspect of our results is that we can look at excess error in recovering why?"
        ],
        [
            "So now we're in a more general setting with agnostic noise, and the only assumption about the noise is that it still satisfies a mild bound, but we no longer look at independent noise, and in this case with the same assumptions on the signal an that same optimistic rate sample size.",
            "What we now have is we have a bound on the excess error in recovering Y rather than abound on reconstructing M. So this shows us what was going on with our results as compared to the compressed sensing literature.",
            "So now I'll just summarize a couple of quick points."
        ],
        [
            "What we've shown in how they compare.",
            "So first of all, we can conclude that trace Norman Max Norm in this general setting do give a good surrogate for low rank because we can get reconstruction guarantees using those properties be comparing the trace Norman Max norm to the rank for absolute error.",
            "We've shown results with both the trace norm and the Max norm.",
            "If the entries of EM are assumed to be bounded on average, we can use the trace norm with that weaker assumption that comes at the cost of a log factor in the sample size for squared error.",
            "We've shown that the Max norm gives us recovery at an optimistic rate, but only if we assume the entries of EM are all.",
            "Founded an if we don't assume that if we only bound them on average, then recovery is not."
        ],
        [
            "Possible and finally we can compare to the existing results and see that we give a more general noise model, but at the cost of an optimistic rate in the sample size.",
            "Using concentration guarantees rather than the optimality conditions.",
            "Thank you.",
            "How do they compare in practice the trace norm versus the Max norm?",
            "Yeah naughty.",
            "Can you help me out?",
            "So yeah, there's some purely empirical results from the snip, so the Max on most datasets we tried, the maximum is better than trace norm.",
            "This is in several different types of problems.",
            "The there is a correction to the trace on the way to trace Norm, which in some situations slightly outperforms the maximum but the straight up maximum is typically is in practice better than the maximum.",
            "Much will depend on what you define by much.",
            "You know, so I mean, you can look at this when there's a paper, but with the remember who the first thing they think it's a lirik.",
            "Celle konicov myself from less, nips looking at empirical comparison, you look at numbers there.",
            "So I have a question and so can your result sort of separate the effect of sort of the approximation error versus, say if you had zero mean noise an approximation error.",
            "Can you give sort of concentration guarantees index setting so so so?",
            "One thing to look at is if we have zero noise than the optimistic rate actually turns into what we can call the fast rate.",
            "So right here if we have zero noise, the second term is a constant and we actually got like the same dependence on epsilon.",
            "Is the compressed sensing right?",
            "But I guess here you're taking noise to be both.",
            "Proximation, Erran, and possibly also zero mean sub Gaussian noise.",
            "Have you looked at the case where you sort of have both of these effects and see if there's a difference by approximation error?",
            "Do you mean that the signal is not exactly equal to M, right?",
            "So when we're looking at the recovery of M, then we just have to assume that that we're looking at independent zero mean zero mean noise, and that sort of comparing to compressed sensing.",
            "I'm not sure if that answers answers your question.",
            "Are there other questions?",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So like a no hot stock will be looking at a setting where were partially observing a matrix, but in our work the settings a bit different and we're looking at the problem of reconstructing a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Someone just asked how to Hods results compared to compressed sensing, and that's that's one of our main questions.",
                    "label": 0
                },
                {
                    "sent": "How the results we have from learning theory will compare to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compressed sensing, so I'll start by defining the setting.",
                    "label": 0
                },
                {
                    "sent": "We're looking at a matrix why that's partially observed, and it comes from a low rank signal that might or might not be corrupted by some noise.",
                    "label": 1
                },
                {
                    "sent": "The Matrix is an N by N matrix an I'll just be assuming without loss of generality that N is greater than or equal to M and the goal here is simple, it's just to try to estimate the entries of the matrix that we haven't seen in the training sample.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, since we're assuming that the matrix is approximately low rank, the most natural thing to do would be to look for a low rank approximation where we're trying to measure where we're trying to minimize the error on the training sample, and hear us is that just gives the locations of the training sample.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, even though this would be the most natural thing to do, it's a non non convex constraint and it would actually be NP hard to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So what's most commonly done is to use a convex constraint such as the trace norm or the Max norm which I'll be looking at in detail.",
                    "label": 0
                },
                {
                    "sent": "And the trace norm is what's used most commonly in the compressed sensing community.",
                    "label": 0
                },
                {
                    "sent": "For this problem, a third approach is to do a local search with the rank constraint rather than looking for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Global minimum.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look quickly at some of the results that exist from the compressed sensing community.",
                    "label": 0
                },
                {
                    "sent": "These are results that give guarantees on either exact or approximate reconstruction of the underlying low rank signal M and the setting that they work in has to assume some kinds of assumptions on that low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Typically that's either saying that the entries of that matrix are uniformly bounded or that there are incoherence conditions, and this is a strict condition that basically means that information in the matrix is spread fairly evenly across the rows and the columns, rather than being concentrated.",
                    "label": 1
                },
                {
                    "sent": "There's some pretty specialized assumptions on the noise as well, so it's typically assumed to be IID normal.",
                    "label": 1
                },
                {
                    "sent": "They're looking at squared error and the bounds that they give are proved by looking at conditions under which the correct matrix, or roughly the correct matrix is optimal for the optimization problem, and overall, most of these results use the trace norm, and we can more or less conclude here that in this specialized setting for the estimation problem, meaning that we're trying to recover the values in the matrix M trace norm gives a good surrogate for rank.",
                    "label": 1
                },
                {
                    "sent": "And we can use it to recover a low rank matrix.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In contrast, we can look at the problem of binary classification.",
                    "label": 1
                },
                {
                    "sent": "So the reference by Cerebro and Schreibman, called 2005, looks at this binary classification setting where instead of trying to recover the values of the matrix, they just want to recover the matrix up to the correct sign and another difference in this setting is that it's agnostic noise rather than a more specialized assumption, an independent noise.",
                    "label": 0
                },
                {
                    "sent": "The bounds here are given with concentration guarantees rather than looking at optimality conditions.",
                    "label": 1
                },
                {
                    "sent": "And overall, what they give here is learning guarantees with the Max norm in the trace norm, but they also show that in the classification problem that race Norman the Max norm do not always do a good job of recovering a low rank matrix, because in this setting there might be a low rank sign pattern that isn't recovered by a low trace, norm or Lomax.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sign pattern, so here's what we're interested in, what we'd like to do is take these concentration guarantees and use them to apply them to the estimation setting that's been approached with the compressed sensing point of view, and see how far we can get there so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're interested in.",
                    "label": 0
                },
                {
                    "sent": "More specifically, we'd like to know what guarantees we can get for estimation with the trace norm in the Max norm and how they're going to compare to the more specialized guarantees given with optimality conditions in the references I showed, we want to know whether Trace Norman Max Norm do give good surrogates for rank in this setting, like Cerebro and Treibmann showed for the classification setting.",
                    "label": 1
                },
                {
                    "sent": "They don't, but what we're interested in is the estimation or regression setting.",
                    "label": 0
                },
                {
                    "sent": "And finally, we'd like to know whether the Max norm could perhaps offer some benefit over.",
                    "label": 0
                },
                {
                    "sent": "Excuse me over the trace norm for this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start the first thing I want to do is just give a bit more details on the Max Norman the Trace Norm.",
                    "label": 0
                },
                {
                    "sent": "So the trace norm is generally defined as the sum of singular values of a matrix.",
                    "label": 0
                },
                {
                    "sent": "But we can also define it in a different way.",
                    "label": 0
                },
                {
                    "sent": "We can look at factorizations of the Matrix X into a product U times V transpose, and then measure the sum of the row norms of yuan.",
                    "label": 0
                },
                {
                    "sent": "The sum of the row norms of V. So what I'm going to do for this talk is actually look at the rescaled trace norm.",
                    "label": 0
                },
                {
                    "sent": "So now will be looking at the average row Norman Yuan, the average row Norman V. And this makes the trace norm very comparable to the Max norm, which also looks at factorizations of the matrix.",
                    "label": 0
                },
                {
                    "sent": "But now takes the high istro Norman you and the high istro Norman V. So what we can see here is basically where the trace norm looks at an average.",
                    "label": 0
                },
                {
                    "sent": "The Max norm looks at a maximum, and also this shows us that the rescaled trace norm is bounded by the Max norm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An important property of these two norms is that if we take the unit balls for the rescaled trace norm in the Max norm, we can express them as convex hulls of similar classes for the trace norm, the unit ball is actually exactly equal to the convex Hull of all rank one matrices that have unit where the average entry is bounded by one and for the Max norm up to a constant.",
                    "label": 0
                },
                {
                    "sent": "We have it as the convex Hull of all rank one matrices where all of the entries are plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So where for the trace norm that average entry is 1 for the Max norm.",
                    "label": 0
                },
                {
                    "sent": "The maximum entry is 1.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the most important thing about the trace norm in the Max norm though, is their relationship to rank, which we're using for all of our results today.",
                    "label": 0
                },
                {
                    "sent": "So to give a bit of background on this for the trace norm, the trace norm is the L1 norm of the singular values of a matrix, and we can compare that to the Frobenius norm, which is the L2 norm of the singular values of the matrix.",
                    "label": 0
                },
                {
                    "sent": "If we also take into account the fact that rank just tells us how many nonzero singular values there are, we can just use the relationship between L1 norm and L2 norm to get the inequality that we have.",
                    "label": 0
                },
                {
                    "sent": "With some rescaling here, so another way to look at this is that if we have a matrix where this term is 1, meaning that the average entry has magnitude one, then the rescaled trace norm squared is bounded by the rank.",
                    "label": 0
                },
                {
                    "sent": "So that's the relationship that we're going to use today, and for the maximum we have the same inequality, but instead of looking at the average magnitude of an entry, we use what I'm writing as X Infinity, which is the largest magnitude of an entry in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So I just want to stop and point out why this is so important to the estimation problem.",
                    "label": 0
                },
                {
                    "sent": "When we're looking at estimating a matrix in practical settings, it's very reasonable, like oh, how was saying to assume that we're in a bounded setting, and so that the entries are bounded in in the matrix that we're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "So in a practical setting, the trace norm and the Max norm will be bounded by.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Drink.",
                    "label": 0
                },
                {
                    "sent": "So next, let's look at the Rademacher complexities for these two norms, so Cerebro and Schreibman computed the Rademacher complexities, using the convex Hull properties for the Max norm, the Max norm ball lies inside the convex Hull of a finite set of matrices, and so they bounded the empirical Rademacher complexity on the order of square root ANOVA rest.",
                    "label": 1
                },
                {
                    "sent": "So here N is the size of the matrix, an S is the number of samples for the trace norm.",
                    "label": 0
                },
                {
                    "sent": "The situation is a little bit different, so again the convex Hull can be used to give a Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And what they saw is that the Rademacher complexity, empirical Rademacher complexity for the trace norm ball is related to the expected spectral norm of a random matrix Sigma, where Sigma has random plus one and minus one entries at the locations where the samples were observed.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately, if the samples are observed in a degenerate way, meaning that they are all in one column, for example, an aren't uniformly spread across the matrix.",
                    "label": 0
                },
                {
                    "sent": "What this will give us is that the Rademacher complexity is on the order of square root N M / S. So this is, uh, this means that if we want to get a non trivial bound on the Rademacher complexity, we'd need a sample size that's at least as large as N * M, which is basically like sampling the whole matrix, so we can't get a meaningful bound here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The good news is that we can bound the Rademacher complexity in expectation, so we were able to give a slightly tighter bounds than Cerebro, and Schreibman had, and this is using a very nice result from Joel Tropp and last year for the spectral Norm, and we have that the expected Rademacher complexity is now bounded, on the order of square root N log N over us, so this is now a meaningful bound, and this expectation is taken where the sample is drawn at random uniformly from the matrix.",
                    "label": 0
                },
                {
                    "sent": "So now that we have these two bounds, we can use them to directly give a learning guarantee.",
                    "label": 0
                },
                {
                    "sent": "In the situation of absolute.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error, so we recall that for absolute error the average error of the ERM is bounded by the average error of the best predictor plus a term that comes from the expected Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "So we can just directly plug in these terms for the Max norm when we're learning with absolute error, we have that the average absolute error of the fitted matrix is bounded by the size of the noise plus this term.",
                    "label": 0
                },
                {
                    "sent": "Here coming from the Rademacher complexity so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the low rank setting, the maximum ball we're looking at is has a nice bound be, so if we're looking at a low rank signal.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Break are where the entries are bounded uniformly.",
                    "label": 0
                },
                {
                    "sent": "Then the Max norm of the true signal is bounded by square root R. So if we fit inside of that Max norm ball and use this property up here, we can see that we get an excess error.",
                    "label": 0
                },
                {
                    "sent": "Excess absolute error bound of epsilon as soon as the sample size is on the order of RN over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So this sample size inequality is up to a constant.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the trace norm, we can do the same thing and the only difference is that there's a log term in the Rademacher complexity, and now in the low rank set.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All we need to assume about the entries of the low rank signal M is that they are bounded on average rather than bounded uniformly, so we have a weaker assumption on the underlying matrix, but then the sample complexity we need has an extra log factor in order to bound the excess absolute error.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what we have.",
                    "label": 0
                },
                {
                    "sent": "We have absolute error excess bounds for the trace norm in the Max norm under a weaker or stronger assumption on the entries of M. But the existing results in compressed sensing give it for squared error.",
                    "label": 0
                },
                {
                    "sent": "So that's what we did.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To look at next.",
                    "label": 0
                },
                {
                    "sent": "So we used a recent result from Cerebro St Herranen Tewari, which says that when we're looking at a smooth error with a mild bound, the excess average is an excess error when comparing to the best predictor is bounded by this here, which uses now the squared Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "So for the Max norm, if we plug in what?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know we have that the squared error averaged over the matrix of our ERM, inside the Max norm ball is bounded by the equation up there and in the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rank setting here's what we got.",
                    "label": 0
                },
                {
                    "sent": "We got excess squared bounded by Epsilon as soon as our sample size is up to a constant this big.",
                    "label": 0
                },
                {
                    "sent": "So let's just look at this.",
                    "label": 0
                },
                {
                    "sent": "The first term, if it was by itself, would be the best possible dependence on rank, matrix size and epsilon that we could hope for.",
                    "label": 0
                },
                {
                    "sent": "But we also have the second term here, which is very interesting if the noise is 0 or if the excess error we're willing to allow is not very small relative to the noise, then the second term behaves like a constant and our sample size grows with one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "But if we want a small excess error in the noisy setting.",
                    "label": 0
                },
                {
                    "sent": "Then the second term behaves as one over epsilon and the sample size grows as one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So we call this the optimistic rate, meaning that depending on the noise level and the excess error, we want, the sample size can grow as either one over epsilon or one over epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weird.",
                    "label": 0
                },
                {
                    "sent": "So here's what we have so far.",
                    "label": 0
                },
                {
                    "sent": "For absolute error, we can give results when either we assume a weaker average bound on the entries or a stronger uniform bound on the entries of the underlying signal for squared errors.",
                    "label": 0
                },
                {
                    "sent": "So far we only have a result for this stronger assumption on.",
                    "label": 0
                },
                {
                    "sent": "The entries are bounded uniformly, so could we get a result when the unit when the entries are bounded only on?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Average.",
                    "label": 0
                },
                {
                    "sent": "So for the absolute case, the bound that we used was based on the expected Rademacher complexity, which is fine for both the trace norm and for the Max norm, but the Cerebro CERT.",
                    "label": 0
                },
                {
                    "sent": "Haran and Tewari result we used to produce are bound for the squared error case uses this worst case Rademacher complexity, and it's not possible to get a meaningful bound on that for the trace norm, so we can't use the same proof technique.",
                    "label": 1
                },
                {
                    "sent": "But the question is, is it still possible to have the weaker assumption of an average bound on the entries of M and get a result on squared error learning?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that this is actually not possible.",
                    "label": 0
                },
                {
                    "sent": "We can construct examples that are low rank and that have bounded average error, but we're squared.",
                    "label": 0
                },
                {
                    "sent": "Error learning does not occur, so there's no way to get to use this weaker assumption and get a squared error learning guarantee.",
                    "label": 0
                },
                {
                    "sent": "Of course, there is something we can do with the trace norm if we only consider bounded matrices.",
                    "label": 0
                },
                {
                    "sent": "So with this stronger assumption, squared loss becomes a Lipschitz function, and so we can use the same techniques.",
                    "label": 1
                },
                {
                    "sent": "We used an absolute error to get a bound for excess squared error with.",
                    "label": 1
                },
                {
                    "sent": "Now this sample size RN, Logn over epsilon squared.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what we have for the absolute error case, we can get results under either assumption on M. For the squared error case, we need to assume that the entries of EM are bounded uniformly and what and?",
                    "label": 0
                },
                {
                    "sent": "So looking at trace Norm versus Max Norm we see that the trace norm doesn't actually offer any theoretical benefit over the Max norm.",
                    "label": 0
                },
                {
                    "sent": "Even if we could find a way of showing an optimistic rate dependence on epsilon for the trace norm, because even in that case we'd still have this extra log term here.",
                    "label": 0
                },
                {
                    "sent": "So it looks like for squared error the Max norm is giving us the stronger theoretical result here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so far I've only looked at the problem of bounding the excess error in the reconstruction of the matrix Y.",
                    "label": 0
                },
                {
                    "sent": "So now I need to point out a couple of subtleties in this.",
                    "label": 0
                },
                {
                    "sent": "The first thing is that we might want to instead guarantee an error in reconstructing the low rank signal M, which is what's done in the compressed sensing literature, and another issue is what is our sampling model?",
                    "label": 0
                },
                {
                    "sent": "In particular, are we sampling with replacement or without RIP?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basement.",
                    "label": 0
                },
                {
                    "sent": "So what we've done so far is sampling with replacement, because the concentration guarantees use IID sampling, and the noise is per entry, which basically means that if we observe the same location multiple times, we see the same value there each time, and this is this is what gives us the results on the excess error for Y.",
                    "label": 1
                },
                {
                    "sent": "If we want to try to recover the low rank Cigna.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "M then in order to do that, we're going to have to assume that the noises in dependently generated and at each observation Anna Zero mean and we also need in both these cases of very mild bound on the size of the noise.",
                    "label": 0
                },
                {
                    "sent": "If we're sampling with replacement and we want to recover M, what this means is the noise has to be generated per observation rather than per entry.",
                    "label": 0
                },
                {
                    "sent": "So if we see the same entry multiple Times Now, the noise has to be generated independently each time that we see that entry.",
                    "label": 0
                },
                {
                    "sent": "And so if we assume these things then we get the same results we had before.",
                    "label": 0
                },
                {
                    "sent": "With those same sample complexities, we now have a result that bounds the error in the reconstruction of the low rank signal M rather than bounding the excess error in reconstructing the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alternately, we can think about sampling without replacement, so in this case the concentration guarantees don't directly apply, because sampling without replacement is not IID.",
                    "label": 0
                },
                {
                    "sent": "But we have a general result that says that in the sampling without replacement setting, we can get the same learning guarantees and in the sampling with replacement settings that we considered.",
                    "label": 1
                },
                {
                    "sent": "So, in particular, if we want weaker assumptions on the noise, so agnostic noise that only has to satisfy very mild bound, then sampling with or without replacement we can get this excess error bound.",
                    "label": 1
                },
                {
                    "sent": "On recovering why with the same sample complexity we had before up to a constant, and if we want to have stronger assumptions on the noise that it's independently generated at each observation and zero mean, then we have the same result as we had before on reconstructing the matrix M and so we have the same results in this setting up to possibly a constant factor cost as we had for sampling with replacement.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we've covered all of our results.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do now is compare a bit between what we have and what the results that exist in the compressed sensing community.",
                    "label": 0
                },
                {
                    "sent": "So the results that exist.",
                    "label": 0
                },
                {
                    "sent": "There's very many of them, just in the last few years, and they all use very different settings and give the results in a somewhat different way.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit tricky to compare between them and to compare to ours, but let me try to give sort of a general sense of the results that are out there.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first type of result we have is results that give a guarantee of exactly recovering the low rank signal M. In this case, the matrix M has to satisfy strict conditions like in coherence.",
                    "label": 0
                },
                {
                    "sent": "In order for this to be possible.",
                    "label": 0
                },
                {
                    "sent": "And of course there's no noise in this setting.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity that these results give is RN Logn over epsilon, so the dependence on epsilon is very nice.",
                    "label": 0
                },
                {
                    "sent": "It's only one over Epsilon, which is better than the optimistic rate we saw before and the guarantee of course is that the fitted matrix gives the low rank signal exactly.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at results that give recovery of M, but only up to some error.",
                    "label": 0
                },
                {
                    "sent": "We've actually split these into two categories because there's two different flavors that are out there for this type of result.",
                    "label": 0
                },
                {
                    "sent": "The first type we're going to call near exact recovery of M because it's very similar to the exact recovery setting.",
                    "label": 0
                },
                {
                    "sent": "In this setting, the same incoherence conditions are assumed, but the noise is now allowed to be iid normal or sub Gaussian, and the type of guarantee that's being given is instead of exact reconstruction, we have squared error in the reconstruction bounded.",
                    "label": 0
                },
                {
                    "sent": "In a way that scales with the size of the noise.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if the noise goes to 0, then that bound goes to 0 as well and we would get exact reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So in a sense we can look at this as a generalization of the exact reconstruction results that are out there and the sample size here is the same.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second category of results giving reconstruction up to an error.",
                    "label": 0
                },
                {
                    "sent": "We've called approximate recovery, and this has a somewhat different setting.",
                    "label": 0
                },
                {
                    "sent": "So now instead of the strong incoherence condition on M, we just require that the entries of EM are bounded uniformly.",
                    "label": 0
                },
                {
                    "sent": "The sample size is the same, and then the result is then a little bit of a weaker guarantee because there were weaker assumptions.",
                    "label": 0
                },
                {
                    "sent": "So now there's a bound on error which no longer scales down with the size of the noise, so even in a noiseless setting with less assumptions on EM, we no longer get.",
                    "label": 0
                },
                {
                    "sent": "Exact reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So looking at this, this is actually very close to the setting that we're working in, so our results.",
                    "label": 0
                },
                {
                    "sent": "First, let's look at the ones that look at recovery of M.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our results use the same assumptions on M and have the same guarantee, which is that we're trying to bound the excess squared error in recovering M. The differences are in our sample size and in our noise model for the noise model we have a much more general setting because we have independent and zero mean noise with only a mild bound condition which we could satisfy with, for example, a finite 5th moment rather than assuming that the noise has to be sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Another difference is looking at the sample size, so a benefit of our result is that we've lost this log factor here, and that's because of.",
                    "label": 0
                },
                {
                    "sent": "Using the Max norm, but this comes at a cost, which is that we now have instead of the one over epsilon dependency.",
                    "label": 0
                },
                {
                    "sent": "We have an optimistic rate dependency on epsilon and one another aspect of our results is that we can look at excess error in recovering why?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're in a more general setting with agnostic noise, and the only assumption about the noise is that it still satisfies a mild bound, but we no longer look at independent noise, and in this case with the same assumptions on the signal an that same optimistic rate sample size.",
                    "label": 0
                },
                {
                    "sent": "What we now have is we have a bound on the excess error in recovering Y rather than abound on reconstructing M. So this shows us what was going on with our results as compared to the compressed sensing literature.",
                    "label": 0
                },
                {
                    "sent": "So now I'll just summarize a couple of quick points.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we've shown in how they compare.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we can conclude that trace Norman Max Norm in this general setting do give a good surrogate for low rank because we can get reconstruction guarantees using those properties be comparing the trace Norman Max norm to the rank for absolute error.",
                    "label": 0
                },
                {
                    "sent": "We've shown results with both the trace norm and the Max norm.",
                    "label": 0
                },
                {
                    "sent": "If the entries of EM are assumed to be bounded on average, we can use the trace norm with that weaker assumption that comes at the cost of a log factor in the sample size for squared error.",
                    "label": 1
                },
                {
                    "sent": "We've shown that the Max norm gives us recovery at an optimistic rate, but only if we assume the entries of EM are all.",
                    "label": 0
                },
                {
                    "sent": "Founded an if we don't assume that if we only bound them on average, then recovery is not.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possible and finally we can compare to the existing results and see that we give a more general noise model, but at the cost of an optimistic rate in the sample size.",
                    "label": 1
                },
                {
                    "sent": "Using concentration guarantees rather than the optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "How do they compare in practice the trace norm versus the Max norm?",
                    "label": 0
                },
                {
                    "sent": "Yeah naughty.",
                    "label": 0
                },
                {
                    "sent": "Can you help me out?",
                    "label": 0
                },
                {
                    "sent": "So yeah, there's some purely empirical results from the snip, so the Max on most datasets we tried, the maximum is better than trace norm.",
                    "label": 0
                },
                {
                    "sent": "This is in several different types of problems.",
                    "label": 0
                },
                {
                    "sent": "The there is a correction to the trace on the way to trace Norm, which in some situations slightly outperforms the maximum but the straight up maximum is typically is in practice better than the maximum.",
                    "label": 0
                },
                {
                    "sent": "Much will depend on what you define by much.",
                    "label": 0
                },
                {
                    "sent": "You know, so I mean, you can look at this when there's a paper, but with the remember who the first thing they think it's a lirik.",
                    "label": 0
                },
                {
                    "sent": "Celle konicov myself from less, nips looking at empirical comparison, you look at numbers there.",
                    "label": 0
                },
                {
                    "sent": "So I have a question and so can your result sort of separate the effect of sort of the approximation error versus, say if you had zero mean noise an approximation error.",
                    "label": 0
                },
                {
                    "sent": "Can you give sort of concentration guarantees index setting so so so?",
                    "label": 0
                },
                {
                    "sent": "One thing to look at is if we have zero noise than the optimistic rate actually turns into what we can call the fast rate.",
                    "label": 0
                },
                {
                    "sent": "So right here if we have zero noise, the second term is a constant and we actually got like the same dependence on epsilon.",
                    "label": 0
                },
                {
                    "sent": "Is the compressed sensing right?",
                    "label": 0
                },
                {
                    "sent": "But I guess here you're taking noise to be both.",
                    "label": 0
                },
                {
                    "sent": "Proximation, Erran, and possibly also zero mean sub Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Have you looked at the case where you sort of have both of these effects and see if there's a difference by approximation error?",
                    "label": 0
                },
                {
                    "sent": "Do you mean that the signal is not exactly equal to M, right?",
                    "label": 1
                },
                {
                    "sent": "So when we're looking at the recovery of M, then we just have to assume that that we're looking at independent zero mean zero mean noise, and that sort of comparing to compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if that answers answers your question.",
                    "label": 0
                },
                {
                    "sent": "Are there other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}