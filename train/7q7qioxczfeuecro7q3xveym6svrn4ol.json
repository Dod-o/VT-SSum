{
    "id": "7q7qioxczfeuecro7q3xveym6svrn4ol",
    "title": "Deep Natural Language Understanding",
    "info": {
        "author": [
            "Kyunghyun Cho, Courant Institute of Mathematical Sciences, New York University (NYU)"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_cho_language_understanding/",
    "segmentation": [
        [
            "Let me try to think about the language understanding a bit.",
            "So what does it mean to build an agent?",
            "That's going to understand the natural language?",
            "And then you know, like the immediate thing that I come up with that say, OK, so how does it actually differ from language modeling?"
        ],
        [
            "Anne.",
            "At that point, you know there are two sides, right?",
            "So one is to view it as a statistical phenomenon and the other way is to view it as a.",
            "You know, like some kind of linguistics.",
            "Really, there is some theory and some structure that's going to generate the actual language.",
            "So the next question is should we?"
        ],
        [
            "Start reading linguistics and already from the late 80s people have realized that they maybe that's not necessarily the way to go.",
            "In fact, I heard that this is not really the right quote, but the Fred Jelinek at IBM back then in 1988 said that every time I fire a linguist, the performance of the speech recognizer goes up.",
            "Whenever I show this quote, everyone starts to hate me in another community.",
            "OK, not here.",
            "So then you know like, OK, so it seems like the linguistics doesn't have to be the way to go.",
            "Then you know what will be the next possibility."
        ],
        [
            "The next possibility of the language understanding might be to build an agent that is able to tell how likely a given sentence is.",
            "Think about the question and answering.",
            "So how likely is a sentence given sentence as an answer to the question.",
            "So, for instance, question, who is the president of the United States?",
            "A likely answer in 2016 is going to be Obama.",
            "Is the president of the United States, and that's going to be the answer, whereas the all the other sentences, for instance chip process the President of America is not going to be an answer.",
            "So by just being able to tell which sentence is likely under the current context.",
            "Might be a one way to view what the language understanding should be.",
            "And similarly, we can find a lot of different examples so."
        ],
        [
            "For instance, given this kind of image of two Dolphins, how likely is a given sentence describing this view?",
            "So likely one would be 2 Dolphins or diving.",
            "Two men are flying will be unlikely one, so in some sense, if we can tell a given the context why the given sentence?",
            "How were given sentences likely?",
            "We might be able to say at least superficially, that that agent or machine has an understanding up to certain extent of the natural language, so I'm going to make a bold claim for now in this lecture that the language."
        ],
        [
            "Understanding or the natural language understanding is all about telling how likely a sentence is.",
            "I believe that in the second lecture this morning, we're going to hear a bit different story, but for now we're going to just say that it's all about putting a right probability to a very likely sentence, and then any machine that can do it is going to have an understanding of natural languages.",
            "So in order to do so."
        ],
        [
            "We're going to talk about the language modeling.",
            "So language modeling is like."
        ],
        [
            "Really super simple, so we're going to say that we are given a sentence and what is sentence?",
            "Sentence is a sequence of symbols and the symbols can be worse characters, bytes or bits or phrases the other way around as well.",
            "And then the question is, what is the probability of that given sentence or a sequence of symbols?",
            "And that's going to be written as the probability of X one X2 and until XT.",
            "And then we have to talk about a bit of probability."
        ],
        [
            "So it's going to be the single slide where we talk about probability and how it actually works.",
            "That is, does everyone remember what the joint probability, conditional probability and marginal probabilities so anybody who hasn't taken probability one please raise your hand?",
            "I'm going to spend about 3 minutes here, otherwise I can just skip.",
            "So you have selected a great set of students.",
            "Everyone has taken it OK, so the joint probability.",
            "Let's say we are given two variables X&Y joint probability is going to be the probability of.",
            "Both of them happening at this certain configuration and conditional probability is given certain value or the event.",
            "What is the probability of another, let's say variables happening at certain configuration and marginal probability is after removing the effect of another variable.",
            "What is the probability of the remaining variable and they're all related by this Bayes rule where the joint probability is the product of the conditional probability and the marginal probability of the condition variable."
        ],
        [
            "Now using that rule we can rewrite the probability over a sequence of variable into a product of the conditionals, where each conditional tries to predict the following symbol given all the previous symbols.",
            "Now, because there are like more more than two variables, is like, it might be a bit unclear.",
            "But if you look at it graphically is quite clear, so we're going to have the T different variables that correspond to the different symbols in a sentence.",
            "We're going to say that OK.",
            "Given any of the given all the previous symbols, what is the probability distribution of the coming symbol, right?",
            "So in this case, what is the probability of the first symbol is going to be defined as it is and then probability of the second symbol is going to be defined with respect to the first symbol and probability of the third symbol is going to be conditioned on the 1st two symbols, and so on, and then we see that this is actually exactly, say equation, so we lose really almost nothing except for the fact that we have imposed this kind of search.",
            "This kind of let's say conditional structure on the sentence.",
            "And now since we have the probability, we can write the probability of a sentence.",
            "Now it becomes a straightforward maximum likelihood estimation."
        ],
        [
            "So, given a sentence given a corpus that consists of many, many sentences, we want to make sure that the probability the model assigns to each of the training sentences is going to be as high as possible.",
            "And there can be simply ritualized Max over the expectation over the corpus of the low probability of all the sentences in the corpus.",
            "Of course, you're looking at it now.",
            "It's pretty obvious that you want to do it, but apparently it was not that obvious that you know we have to approach it from the statistical point of view.",
            "So for instance, in 1990 the people at IBM proposed to do this machine translation in a statistical way, and then they submit a paper.",
            "And before that, everything was writing about the rules and everything in order to make the translation system work and the reviewers back then said that the crude force of compute computers is not science.",
            "Of course, the first author got really mad at no.",
            "The last author got really mad at it, but essentially this view of looking at the language, understanding of language modeling from the perspective of statistics is really important idea.",
            "But it's not really obvious as we think we are as we think it is these days.",
            "So how do we actually?",
            "The probability and then trying to maximize or the assign a good probability to training sentences.",
            "So the one of the last sentence.",
            "This is not science, right?",
            "So back then, as far as I know, they used the largest corpus that anybody has ever used before, and then in order to do so, they had to use amazing amount of computing resources at IBM unlike all the other people who are feeding the translation model exactly.",
            "Yes, so which is kind of like.",
            "Yeah.",
            "But it turns out that this is a model that actually works, unlike the other existing models that we're using less computers and doing more science."
        ],
        [
            "OK, so the first approach we can think of is called Ngram language model.",
            "So."
        ],
        [
            "So when we looked at the factorization of the sentence probability, we saw that the every conditional distribution of a teeth variable or the symbol was conditioned on every every single.",
            "Let's say previous word, and that seems like too much, so we're going to make assumption or the NTH order Markov assumption saying that the any of the word or the symbol in the sentence is going to only depends on the end previous sentences.",
            "Not the sentence.",
            "Sorry bout that and previous symbols.",
            "So it's written here, so the probability of XT is going to be conditioned on the variables from XT minus North to the XT minus one.",
            "There might be actually an minus one of them, but OK. And then now, since we made this kind of assumption, what we can simply do is to for each of the conditional probabilities, conditional distribution.",
            "Here, we're going to collect the statistics from very large corpus and the collecting statistics sounds fancy, but all you're going to do is that they essentially count how many times X T -- N to X T -- 1.",
            "This phrase.",
            "Of course, in the corpus and then also to count a slightly larger phrases and then you know, like just.",
            "Divide the letter with the former.",
            "Then we get exact maximum likelihood solution."
        ],
        [
            "And this is very straightforward, But let's take an example.",
            "So my sentence is going to be out.",
            "I would like to comment the aperture on his work.",
            "By the way, this example is from the field lonesome at Google in mind.",
            "I shamelessly two kids slide, but it's a very nice example I believe.",
            "And then we're going to compute how many times each of the phrase, so we start from a single symbol and that we call the unigram modeling, so we can make a very extreme assumption that the probability of a sequence of symbols is going to equal to the product of probability of each symbol independently from each other.",
            "In that case we get the low probability or the negative log probability of.",
            "Actually, average low probability of the 8.051, so it's higher the worse because we look at the negative value of the probability and then of course it's not that great, so we're going to make the assumption slightly less by modeling bigrams or the two words phrases at a time.",
            "And then we look at the look at the probability of I.",
            "An probability of good given I probability of light given note and then so on until the probability of the end of the sentence symbol, which is a special symbol that we're going to just introduce in order to make our life easier given the punctuation mark at the end.",
            "And then we see that the probability of this very likely sentence went down by the order of magnitude.",
            "And then of course we can make it even better by considering two previous words instead of one for every single symbol you get that is called trigram modeling, where we're going to start with the probability of I, the first word.",
            "Probability of would give 9, which is the second word, and then when we went to the probability of the Third world, we're going to condition it on both.",
            "I would and then we see that the probability assigned by the model.",
            "This is estimated model to the sentence has got even better and then we can continue going on.",
            "All the way as much as you want, but obviously there is some issue.",
            "Do you see some issue here?",
            "Do you see this kind of, let's say, the issue with this kind of formulation.",
            "There was a like hand, right?",
            "Right, OK, so this is too good an answer right away.",
            "OK, that actually makes it difficult to do some kind of discussion.",
            "So you see that you know, like we multiply the probability or estimate of the conditional probabilities and what's the kind of, let's say property of multiplication.",
            "If you multiply anything with zero, you're going to get zero.",
            "In other words, if any of these phrases never occur, however likely it is.",
            "But in the corpus, then you're going to get the."
        ],
        [
            "Zero probability.",
            "So let's say this is an example, so I was in Edinburgh when I was making this example.",
            "So the probability of a tenured professor likes drinking whiskey is going to be decomposed into a probability of a probability of tenured given a probability of professor given a tenured, and so on, and we can see that the drinking whiskey, for instance, is a very very likely phrase, especially in Edinburgh an A is a very frequent words, but let's assume that the.",
            "A tenured professor just never occurred in our corpus, and suddenly the probability of the whole sentence goes to 0.",
            "And this is the issue often refer that to as data sparsity or data scarcity.",
            "So when the data just cannot cover every phrases that we're going to expect to have in our test sentence, and as soon as there is a single phrase that does not occur in the training set, we're kind of doing.",
            "Our model is going to Simply put the zero probability to every sentence that contains that kind of phrase.",
            "And obviously people have notice.",
            "Notice this already from the very early days.",
            "And then there are few.",
            "So."
        ],
        [
            "I've been thinking about this issue of the data sparsity and then the.",
            "You know there can be like quite straightforward, obvious solutions.",
            "First of all, you can think of just smoothing it out.",
            "You're going to say that the any phrase you can think of is going to be assigned a very small probability that does not have anything to do with the actual corpus.",
            "How it is, how many times it occurs in the corpus, and that's called the smoothing or the add Alpha smoothing.",
            "In this case, by having certain amount of.",
            "Constant added in the denominator and numerator.",
            "We make sure that we are never going to run into a phrase that is assigned zero probability, but you can see that it's kind of like OK. Dumb way to do it, right?",
            "So if you want to be slightly smarter, you can back off to a lower order ngram.",
            "So let's say you want to compute the conditional probability or you want to estimate the count statistics over a very long phrase, but you see that that phrase never happened, and what you're going to do is that your case, then I'm going to back off and then for this phrase I'm going to forget about the first few symbols until I find the phrase that actually occurs in the training corpus and use test statistics with some transformations.",
            "Instead of the original statistics, which is going to be essentially the zero probability estimate.",
            "So I'm not going to go into detail of this because I have like so many slides to talk about anyway.",
            "But even then, even even if we use back off and then all these smoothing techniques together.",
            "Amazing ngram model.",
            "That's going to avoid the issue with the data sparsity.",
            "This kind of N gram language model is pretty unsatisfactory in another aspect that is."
        ],
        [
            "There is no generalization.",
            "So let's say we, in the corpus we had a very many occurrences of chases, adult chases, a cat chases a rabbit.",
            "And then suddenly in our test sentence, there is a phrase.",
            "That is, Chase is a llama.",
            "And of course, we all know that the llama is a an animal.",
            "I hope everyone knows.",
            "I think it's from South America.",
            "An because we know that Llama is an animal just like dog, cat or rabbit.",
            "Even without looking at the corpus or anything, we can immediately tell that OK Chase is a llama is very likely.",
            "But now let's say your training set didn't have.",
            "In the case of Ngram language modeling.",
            "Let's assume that in the training set there was no mention of llama at all.",
            "Then suddenly our N gram language model is going to tell us that the OK, the probability or conditional probability of llama given chases all is going to be 0.",
            "On the other hand, you should have the model should figure out that your Kalama is a dog or cat or something similar.",
            "So we should put certain type of probability.",
            "Sorry.",
            "Great present.",
            "It's enough that it's not in that context.",
            "Ono?",
            "OK, exactly, exactly, yes.",
            "So you can back off.",
            "But if this context was not there, then you know we can go to a very dumb estimate or just a zero estimate.",
            "So this is really problematic an I check the Google book, Google Books Ngram and really Chase is a llama.",
            "Never occurs in the gigantic corpus or the engram that they have built is the probability zero phrase we have here.",
            "Now, in order to avoid that, we can think of, let's say, going into the neural net length, right?",
            "So let's talk about."
        ],
        [
            "How we can avoid that issue?",
            "So instead of having a count based table based approach now?"
        ],
        [
            "Going to say that, OK, let's forget about it and we're going to go into the approach where we're going to simply replace this complex statistics with a function approximator that is parameterized by a.",
            "Sorry bout that.",
            "There should be a theater parameterized by Theatre and the function is going to take us to input the context or the variables on which the current conditional distribution is conditioned.",
            "And returns the probability of the next word XD in this case.",
            "So is it clear so the transition from the N gram language model to the parametric function approximator?",
            "But let's say you know it's clear for now and then we're going to use a feedforward neural net to replace this.",
            "Proper function approximator."
        ],
        [
            "This work, it was done initially in 2000 or was in 1999 by Yoshua Bengio who is organizing this summer school.",
            "So it's a quite it's 2000 OK apparently yes.",
            "So first of all, we have to decide on how to encode or how to represent a given sequence so that given sequence is going to be the context where the conditional probability is conditional distribution is going to be conditioned on and we're going to take the dumbest possible encoding ever, that is.",
            "111 K encoding of each word, and we'll say that the OK, so we're going to build a vocabulary, so we're going to build a vocabulary, all the distinct symbols we find in a corpus.",
            "And we'll say that each word or the symbol is a vector that is binary and has as many elements as there are words or the symbols in the vocabulary.",
            "And we're going to set only one of them.",
            "One of the elements whose index corresponds to the index of the symbol in the vocabulary to one.",
            "Now, for instance.",
            "In this case, we're thinking about the vocabulary of six symbols, and then X T -- 1 is the word or the symbol that corresponds to the second symbol in the vocabulary.",
            "X T -- 2 to the one that corresponds to the 4th symbol in the vocabulary, and so on.",
            "Now, why is it like the dumbest?",
            "Because every single symbol in the vocabulary.",
            "Is equally distant away from every other symbol, so it does not encode really any kind of prior knowledge about the what each symbol means and how each symbol relates to the all the other symbols in the vocabulary.",
            "And then each of these.",
            "What are one of vectors or the one of K coded vectors are going to be projected into the continuous space an when we say that and then you get this a fancy continuous vector word representation.",
            "But obviously that's really offensive term of just multiplying that vector from left with a weight matrix W. And when we build a W, we're going to build up W such that there are as many rows as there are symbol in the vocabulary, and the columns will correspond to the how large the continuous.",
            "So how was the dimensionality of the continuous spaces?",
            "And once we have those word vectors, we will concatenate them to form a vector that represents the whole sequence of the context symbols.",
            "So once we have this, this vector is going to be a representation of the context words XT minus one to X T -- 3 here.",
            "And that vector is going to go through the nonlinear hidden layer once, twice, or some number of times, and then eventually we get a nonlinear the nonlinear projection of the context vector.",
            "That is a just a simple continuous vector.",
            "Now I hope that there was a lecture about the usual MLP and so on, right?"
        ],
        [
            "Just to be sure, just to be sure.",
            "And now we have this vector H. I'm going to call it H. That summarizes the whole context or the input to the neural net.",
            "Now what we want is that the afterward we want this neural net to return the probability over the all possible next symbol, so that once we have the probabilities of all of them, we get effectively distribution, and that's exactly what we wanted.",
            "So what we will we will do is to project that H vector that summarizes the whole context context into a vector.",
            "That has as many elements as there are symbols in a vocabulary.",
            "But of course, once we do the projection like this, like the linear projection, we're not going to get probability is going to be like sometimes negative, sometimes positive.",
            "They will never sent one, so we simply use a softmax normalization to make sure that they're going to be positive first of all, and then when we sum over every single symbol, the probability of every single symbol that some is going to be one, and then we get a probability over all possible words.",
            "In the vocabulary.",
            "Now at this point.",
            "You gotta remember that I was talking about the lack of generalization in the N gram language model and how and why I suddenly move on to the neural language model and saying that you know, like, OK, so may so this kind of implies that this is a solution to the lack of generalization in ngram model.",
            "So perhaps if anybody has some idea of why that would be.",
            "Anybody know?",
            "OK, OK that's great."
        ],
        [
            "Alright, so how does the generalization actually happen if we use this kind of neural language model compared to the N gram language model?",
            "Now let's take 3 examples sentences which we're going to assume that they are in the training corpus.",
            "First sentence says that there are three teams left for the qualification, second sentence, four teams have passed the first round.",
            "The last sentence is four groups are playing in the field.",
            "Now that is our training corpus.",
            "And then someone asked me how likely is the symbol groups followed by three and we see that in the training corpus there is no such occurrence, so in usual engram modeling either is going to be 0 probability, or you're going to back off onto the very dumb, let's say, probability estimate.",
            "But instead, what's going to happen with the neural language model is that the?",
            "The neural language model will learn to project three and four which are going to be 2 distinct inputs into a nearby space.",
            "Nearby points in the continuous space.",
            "In order to put a good probability that the groups will follow one of those words.",
            "Because the probability of groups the output is going to be similar, the two distinct inputs will have to be mapped into a similar points in the continuous space.",
            "And what that means is that the when we are asking for three teams which never really occur.",
            "By going near where the four is going to be projected.",
            "Which is it the other way around?",
            "OK, sorry, sorry about that.",
            "OK, let me try again, alright?",
            "So yes, so three and four are followed by.",
            "Both of them are followed by teams.",
            "So the neural language model needs to project the two different inputs, tored the similar space in the continuous vector space inside the neural language model, because only from their own you can say that the OK.",
            "So the teams has a similar probability when follow following either 3 or 4.",
            "Now what it means is that even though.",
            "Three groups was never in the training corpus.",
            "The neural language model is going to project the three near where four is going to be projected and from their own neural language model can tell that the probability of groups following three is going to be something similar to the probability of groups following four.",
            "So this is a natural natural phenomena you get by training neural language model that exploits the continuous vector space.",
            "Continuous vector space, yes.",
            "And number happens in a phrase that we usually discover in the sentence is approximately right.",
            "Two sides to every point.",
            "The only number that works there is to say, of course.",
            "Right, right?",
            "Right, so that's a good question.",
            "So the there are actually two parts of the question.",
            "First part, first part is that the OK.",
            "So because.",
            "Yes, I'm going to get there.",
            "Yes, the first question is.",
            "You know, like because the three and four, both of them are numbers.",
            "If you just, let's say, replace it with a special symbol denoting that is a number, then it's going to be all fine even with engram language model.",
            "And that is definitely true.",
            "And in practice, in natural language technologies we always do certain type of so-called classing in order to replace numbers that we know are just numbers were going to replace with a special symbols an other named entities as well.",
            "But this is just because of the example that, but that's a good point.",
            "I'll try to revise the example where you cannot really do that kind of special treatment.",
            "And second question, can you repeat the second question?",
            "Special cases where only.",
            "Right right, so exactly?",
            "Yeah, so two sides of the coin, right?",
            "Yeah, sorry about that.",
            "Alright two sides of the coin, so even then.",
            "So if you think about it, let's say these two words, let's say three and four are going to be projected into similar points.",
            "So similar region in the space.",
            "But that doesn't necessarily mean that they are going to collapse onto each other, so there will be always some signal that neural machine neural language model can exploit in order to assign different probabilities to different context.",
            "However similar they are.",
            "So does that answer your question, and in particular, if you have a frequent phrase, then we could learn something specific about that frequently.",
            "So any other questions?",
            "Now, OK, that was just saying hi right?",
            "Right?",
            "Good thing is that since their presentations are very high dimensional, learn that similarities in one dimension are like OK, this number.",
            "This number they may in this context he similar but another dimension that can be completely different than model learn that relates to other things where you can only play store right right?",
            "That's precise feature.",
            "So because we're using a very high dimensional vectors in the neural language model, what's esentially happening is that it can encode the multiple degrees of similarities.",
            "So in some sense, it's going to encode that these are numbers in some other dimensions.",
            "You can exploit other dimensions to encode other properties of that word, so it's always possible, and in reality this feedforward neural language models are extremely powerful, especially when used in combination with the N gram language model now.",
            "So I told you about this example and then of course I completely trust in this.",
            "Let's say interpretation as well as a lot of other people do as well.",
            "But how can you be sure?",
            "Can we actually inspect whether that happens or not?",
            "And one way people have realized and then we have been using a lot is."
        ],
        [
            "We effectively visualize the vectors you get by training this neural language model, and of course you're going to get a 300 dimensional vectors.",
            "And how do we visualize them?",
            "You know, like with the 300 dimensional vectors that there is no way we can look at it as it as they are, but we can reduce the dimensionality by using certain type of dimensionality reduction algorithms such as Tiffany, which is very famous.",
            "Yes please.",
            "Joseph trains yes, right?",
            "You didn't publish a paper on this, right?",
            "There's only a figure right that you used once awhile, right?",
            "And?",
            "Then use these ideas in their papers I see.",
            "Right, right?",
            "Yes?",
            "So this kind of like visualization technique for the neural language model in general was first developed in Montreal, by the Joseph Turian Anusha Banjo and then since then everyone is essentially using it to view it.",
            "And if you go to NLP community until only this year, everyone was going crazy about visualizing and see you know which words are like related to other words, But what you see is that once you train this kind of model, if you visualize just the very first projection with matrix.",
            "We see that the similar words are close to each other and this similar words are often far away from each other.",
            "And this really made this kind of figure, regardless of what kind of perplexity or the performance of the language model you get.",
            "But this figure is the thing that really like made people go crazy about these continuous vector language models."
        ],
        [
            "Before moving on to the next lesson, part of my talk, if you have any questions about the feet forward in your language model, please go ahead.",
            "Yes.",
            "The singular values of this W matrix.",
            "How many significant single values are going?",
            "What's the actual functionality, right, right?",
            "That's actually a really good point.",
            "And it actually."
        ],
        [
            "Depends on the task that you train your model on.",
            "So for instance, if you train your neural language model but not to predict the probability of the next word, But let's say to predict the sentiment of the given sentence, then what you get is that you get a very low low dimensional.",
            "So the rank of the, let's say word embedding or the first projection matrix is going to be really low.",
            "And then if you visualize them you see that the all the words are essentially effectively separated along one dimension according to positive and negative signs.",
            "So it actually depends on the tasks.",
            "So how much it actually matters.",
            "So that is the kind of the answer that I really like most, and then second answer is that it turned out that the weight vector a matrix here encodes certain.",
            "Let's say very shallow properties such as the frequencies as well.",
            "So if you look at the vectors of a very rare words, you see that they rarely change starting from the random initialization and what people have recently found out is that you can essentially prune out most of the elements.",
            "In the very rare words vectors, and then I guess that means that the rank is going to be actually much smaller than the full rank.",
            "I wanna add something so yes.",
            "Then rare words.",
            "Have more data to figure out what they mean.",
            "So if you were just before we cut the dimension it wouldn't work as well and now it is the best models house.",
            "I don't know 1000 dimensions or things like that so it's pretty large and we did our experiments.",
            "We started with 20 and then 50 right tip right?",
            "Yes yes please.",
            "Words that you want to consider right, sorry.",
            "Total #4 so the."
        ],
        [
            "Capital K. Oh yes, in the one of K encoding, K is going to the number of symbols in the vocabulary, yes.",
            "If you want to add more, Oh yeah, that's actually a good point.",
            "So let's say we train this model and then we have.",
            "We just realized that if we got another set of training sentences and there are symbols that didn't occur in the previous set of training sentences, we one way you can do is to simply add few more in the vocabulary and the re tune the whole thing.",
            "Or tune only the word vector part.",
            "But clearly that's not the thing that you want to do, but I'm going to talk about whether we want to even use words in this neural language model later on.",
            "Today, yes.",
            "Any yes?",
            "Words that have multiple meanings that you might have one to one.",
            "Right, so yeah, that's a good point.",
            "So because we're going to project down the very high dimensional vector as high as like 1000 dimensional vector in 2D space, we actually cannot preserve every single similarity properties that have been captured by the word vectors word matrix here.",
            "So in 2011 or 12 I forgot exactly which here, Lawrence, Fundament and propose a way to do a multiple dimensionality reduction simultaneously.",
            "While making sure that the properties that are captured by each map or the projection operator is going to be as different as possible, and in that paper if you look at the visualization, you see that the each map corresponds to different type of similarities and that kind of that could be the one way to essentially visualize a better.",
            "This way matrices better.",
            "OK, so then now we kind of like OK address the issue with the lack of generalization, that's great."
        ],
        [
            "But now the one issue I see is that."
        ],
        [
            "We had to make this kind of assumption on the what is going to be the maximum length of the context words and we don't want to, you know, like set some kind of constraint like that.",
            "And the reason why we had to go into this direction initially, not even before the neural language model, is because of the data sparsity issue.",
            "And then you know, now that we've moved onto the neural net land.",
            "Is it possible that we can just forget about or abandon this whole assumption on the.",
            "Whole assumption of Markov property and why do we want to avoid it?",
            "So in English actually, you know like the N gram language model or the Markov language model works pretty well.",
            "But even in English there are so many corner cases where this kind of short context will just definitely kill our language models performance.",
            "Let's take an example by the Steven Clark.",
            "The same the same stuff which had impeded the car of manual guests in the past 30 years, in which he refused to have removed.",
            "Let's say we want to model.",
            "Probability of this sentence using the ngram language model.",
            "When we compute the probability of removed.",
            "We actually need to know the stump was there in order to put the high probability, because it is the stump that he's refusing to remove.",
            "But The thing is that any kind of ngram model, however, in practice where we go up to, let's say 7 words in the context, will never be able to capture that the probability of this remove is high because of stump, because it never actually sees stump in the way."
        ],
        [
            "Now let's try to model the origonal conditional probabilities without having that kind of Markov assumption.",
            "Now the issue here is that the our input to the neural language model is going to have a different length, so any words any conditional probability or the distribution of the words in the later part of the sentence will have a context that is very long.",
            "So you're going to have a very many symbols that the neural language model needs to take in, whereas if you consider, let's say the symbol in the earlier stage of the sentence is going to have a very small of them.",
            "So one way to deal with it is to simply set that.",
            "Say that the OK our maximum number of context symbols is going to be gigantic 3264, and then whenever we have less than that, we're going to just pad it with all zeros or the symbol saying that OK, those are non symbols.",
            "But there's slightly less satisfying.",
            "Instead, we're going to think of it from the computer science perspective, so if you took, let's say, computer science 101, first thing you learn is that the OK. How do you make the recursive function and the recursive function can handle a very long sequence with a single function that applies over and over to the symbol and the state of the recursive function itself.",
            "So.",
            "And we're going to."
        ],
        [
            "Sorry about that.",
            "OK, so yes, so we're going to let's say resorts of recursive construction of the neural net by defining first the memory state or the hidden state.",
            "That's going to be set to all zeros and then we'll have a function that applies to the one of the symbols and the current hidden state or memory state of the function, and we apply this recursion over and over until the sentence finishes.",
            "Oh, you did?",
            "OK, that's actually great.",
            "Yes, OK so."
        ],
        [
            "So what it means?",
            "Because now you all know already RNS.",
            "What we're doing with this kind of conditional distribution is that we're going to apply this recurrent neural net activation function starting from the first word in the context, and then get update the memory state and based on that memory state, we're going to run that recurrent activation function again with the second word, and so on until they all the context has been consumed.",
            "Now at that point, what we get is a memory, so the memory state of the recurrent recursive function should summarize.",
            "What the context is and based on this summary, we're going to compute the probability of the next word.",
            "And clearly it's going to work for any number of context words, and using this kind of idea by replacing the function F with the parametric neuron."
        ],
        [
            "That we get the recurrent neural net language model."
        ],
        [
            "Recursion."
        ],
        [
            "The language model is very straightforward idea, so we had that idea of the recursive construction of a function that's going to read any number of context symbols.",
            "We're going to apply that to the full sequence.",
            "One word at a time, and as we apply that function, we get the summary of the context symbols so far, and based on which we compute the probability of the next word.",
            "And we do that over and over and all we need to do at the end of the day is to multiply the probability you get at every time step in order to get the probability of the full sequence.",
            "So we let the model read the word.",
            "An update is hidden state and based on that we let the model predict what the probability of the next word is and we continue doing on and then at the end of the sentence we get the probability of the full sentence right away.",
            "And if I write it like this, that doesn't really look intuitive, but if you."
        ],
        [
            "Throw it in a graphical way.",
            "Then it becomes suddenly so much easier.",
            "So we we have an initial value of the memory state of the recurrent neural net from which we compute the probability of the very first word.",
            "And next time we're going to read the first word.",
            "The update is memory state, predicted probability of the next word and we continue doing so until the end of the sentence and we just multiply all the probabilities we have gotten and to get the probability of a given sentence.",
            "So we do read, update and predict, read, update, and predict until we predict that last end of the sentence symbol or the last punctuation mark.",
            "So is it super clear?",
            "Right now, OK, probably not."
        ],
        [
            "Yes.",
            "Probability.",
            "Moving.",
            "What?",
            "Sorry, so the probability of East.",
            "Knowing that.",
            "Instead of cats, or is it really the world pass?",
            "Another page, one H1?",
            "OK, yes.",
            "OK.",
            "Right, the H1 is to bring the Oh yes yes.",
            "So if you if you view those H is as random variables that will be the right way to go.",
            "But at this moment these H is are just simple placeholder for the output or the computation done by the F multiple times.",
            "So H is not really the random variable, is just a discrete function.",
            "So effectively by using H1 and CAD what we get is the conditional probability of East.",
            "That depends on dog cat, not only the H1.",
            "Yes.",
            "Context.",
            "What, why should it be less reliable?",
            "What do you mean by less reliable?",
            "More and more.",
            "Right, so that question actually, you know, like has a lot of different, let's say parts in it, but essentially the one thing that we cannot really say for sure whether it is going to be.",
            "Less reliable estimate as we move on to the later parts of the sentence is difficult to tell because of at least two reasons.",
            "First reason actually heavily depends on the actual data, so it's likely that the, let's say, every single sentence finishes with some kind of punctuation mark, let's say.",
            "Then you know predicting the punctuation mark is going to be actually pretty straightforward, and that's going to be very reliable estimate.",
            "And then we can think of it.",
            "We can generalize it into other concepts, so there may be a construction where the verbs always come at the end of a sentence, like in Korean and German.",
            "German I guess, but yeah in that case and then let's say there are very frequent verbs, many frequent words and then predicting those frequent verbs is going to be actually more reliable by looking at the very first first symbol, which is going to be subject right.",
            "So it actually depends heavily on the structure of it, but if we do not assume any kind of those, this kind of recurrent language model does have an inductive bias that prefers the better estimate of the earlier parts.",
            "But of course you know like that one is kind of very general statement.",
            "That may not work well, depending on what kind of underlying structure in the training corpus you see.",
            "Alright, so then I'll continue."
        ],
        [
            "OK, so."
        ],
        [
            "Continue by OK constructing the recurrent neural net.",
            "Which yeah sure did, but because this is super important, I'll just try to quickly recap one more time.",
            "So now input again is going to be symbols that are represented as a Walnut vectors or one of K coded vectors, and in this case now the each time we're going to look at single time step at a time, because the same function is going to be applied over and over.",
            "So at time step T we're going to consider a symbol at time step T -- 1, which is going to be.",
            "Vector that corresponds to the symbol at T -- 1 step and we have the hidden state or the memory state of the recurrent neural net from the previous time step T -- 1.",
            "So we have X T -- 1 H, T -- 1 parameters will be the input way matrix.",
            "Which is often called word embedding.",
            "So we looked at the visual logic 2D visualization of that way matrix and we have transition weight matrix which is going to be a square matrix that is used to transform the previous hidden state and have a bias vector."
        ],
        [
            "And we let us define a knife transition function, which is simply going to be the affine transformation of concatenation of the input vectors, which is the T -- 1 symbol and the previous city and state, followed by a elementwise hyperbolic tangent function.",
            "Now we get the continuous state representation, so we read it, and then we update the hidden state of the recurrent neural net and based recurrent neural net to get the new hidden state, and we apply the pointwise nonlinear transformation.",
            "And now."
        ],
        [
            "Now, because we have the hidden state that summarizes all the context symbols up to the T -- 1 symbol, what we're going to do is simply use the very same trick from the neural language model or the feedforward language model where we transform the current state of the recurrent neural net."
        ],
        [
            "Two, the probability distribution over the all the all possible symbols in the next time step, which is going to be simply defined.",
            "Transformation of HT minus one into a vector that has as many elements as there are symbols in the vocabulary and we use the softmax.",
            "Normalization to make them positive and sum to one.",
            "And we get the probability distribution.",
            "Now, this part is exactly same as the usual feedforward neural language model and we see that the only thing that really changes from the feed."
        ],
        [
            "In your language model, is that by introducing this kind of recurrent connection, that's going to be used to read as many symbols as possible.",
            "We have ability now to handle variable length input and then tries to compute the probability of the next word given the potentially infinite infinitely long context work.",
            "So."
        ],
        [
            "We built it and then I'm pretty sure you learned it already, but again, this is really important, so I'll go over it one more."
        ],
        [
            "Time.",
            "How do we train the language model is to maximize the making sure that the model is going to put as high probability as possible to training sentences or likely sentences.",
            "And we can write it down as the maximizing the likelihood of the parameters given the training sentences.",
            "So we assume that there are N sentences given, and each sentence is a sequence with a variable number of symbols, and we can write, write the log likelihood function as the sum over the all the training sentences sum over some over the all the training sentences.",
            "And then we sum the low probability of each sentence.",
            "Given by the current model, so effectively what we're saying is that the OK.",
            "So we want to maximize the probability or the low probability assigned by the model to every single training sentence we have available.",
            "And because it's the kind of optimization problem, usually people just write it as a negative log likelihood and say that you minimize it.",
            "But it's the same thing.",
            "Just put the negative sign or not."
        ],
        [
            "And we usually use the mini batch stochastic gradient descent.",
            "Really nothing changes from the feed forward neural net.",
            "We randomly select the mini batch of very small number of training sentences and then compute the gradient or estimate the gradient of per sample costs with respect to the theater for each one of them in the mini batch and we just average them to get the mini batch gradient.",
            "And we follow the direction of that mini batch gradient in order to maximize the log likelihood.",
            "Or if you follow the opposite direction of the gradient, you're going to minimize the negative log likelihood and you repeat it until the convergence.",
            "But of course when I say convergence convergence in terms of the performance on the validation sentences that were not used on the training used to compute the gradient.",
            "Now this should be pretty clear and then we stop and then we call it all this stopping.",
            "Now the question is how do we compute?"
        ],
        [
            "The gradient of the cost function with respect to the parameters in this kind of recurrent neural net language model.",
            "And as you have already learned, we use the algorithm called backpropagation through time, which is 2.",
            "We're going to enforce the recurrent neural net.",
            "That had a loop in time.",
            "So given the sentence we are going to say that we have a very deep recurrent neural net very difficult for neural net that has as many hidden layers as there are symbols in the given sentence.",
            "Now this shows a single step of that unfolding, and you can see that the starting from the very beginning we're going to have a very similar nonlinear layer here and then that layers activation is going to be fed into the nonlinear layer at the next time step, and so on until the end of the sentence symbol.",
            "The probability of the end of sentence symbol is predicted.",
            "And once you unroll this recurrent neural net language model overtime, what you see there is just a fit for a neural net.",
            "And then with the feed for neural net, what we can do, we can simply use the piano tutorial.",
            "As well, so you do the import Theano that tensor and Theano tensor that grad and then cost function.",
            "And then it's going to compute the gradient of the cost function with respect to all the parameters automatically.",
            "But one thing that differs slightly is that the now, unlike the usual feedforward net, we build, the parameters are all shared across the layers.",
            "Now in I don't know why."
        ],
        [
            "Sorry OK OK yes.",
            "So what I mean is that the essentially when you do the backpropagation through time, as in, you're going to compute the gradient of the of the cost function with respect to the parameters that let's say time step T, and then we're going to send it back to the T -- 1, send it back to T -- 2 T minus three.",
            "And as you do the back propagation, what you need to do is.",
            "Accumulate that derivative of the cost function with respect to the parameters at each time step into a shared story storage.",
            "Because we share the parameters, we only get the gradient of the cost function with respect to each parameter only once.",
            "And in order to compute that, we compute the derivative at each time step separately.",
            "But as you compute it, you're going to sum them up in order to get the correct derivative.",
            "And once we come all the way to the first symbol, we have now accumulated the gradient of every time step, and that is going to be the gradient of the full cost function, which is going to be this sum of all these low probabilities with respect to the parameters, and we use that to update the parameters stored.",
            "Maximizing the low probability likelihood or to minimize the negative log likelihood.",
            "Is this like super clear, right?"
        ],
        [
            "Yes.",
            "OK, so I'll continue on and the."
        ],
        [
            "Get over issue with this kind of neural net, or they're doing the backpropagation through time."
        ],
        [
            "Using the naive transition function.",
            "Is that the is super difficult to train the model in this way without a lot of let's say trickery is there?",
            "Now let's see what actually happens and then what we are trying to compute.",
            "When we do the backpropagation through time.",
            "So when we compute the derivative, what we see is that effectively at every time step we're going to compute the derivative of the cost function or the per step cost function in the future with respect to the current time steps hidden activation.",
            "So JT plus North.",
            "Is going to be the cost function cost coming from the T plus end step and HT is the current activation at time step T, so we tried to figure out what kind of influence the cost is going to have.",
            "If we perturb or if we change a bit at time step T like in the past.",
            "So if we change something Now, what kind of influence is it going to be?",
            "Is it going to have in the future?",
            "And by computing that we can essentially adjust the parameters to make sure that the this adjustment is going to have an influence in the future to minimize the cost or the maximizer look like clear.",
            "And if you look at this kind of temporal derivative so the derivative of the.",
            "Next time steps activation, given the current time steps activation, we see that there is a transition matrix coming out of the nonlinear function we had, which was the hyperbolic tangent.",
            "Following the affine transformation of the input, and if we look at the long time step.",
            "So if you rewrite the full influence derivative, then we see that we're going to multiply those transition matrix over and over with each other as many times as possible into the future.",
            "And that brings into."
        ],
        [
            "A bit of an issue where the normal of the derivative.",
            "Is going to either shrink or explode depending on the configuration of the transition matrix, if the.",
            "If we assume that if we use the hyperbolic tangents if the largest eigenvalue of the transition matrix is smaller than one is very likely that the norm is going to shrink to 0.",
            "Because if you compute, let's say, zero point 900 times, you're going to effectively get 0.",
            "But if the largest eigenvalue is going to be larger than one, it's very likely that it's going to explode.",
            "As in, if you multiply 1.100 times, then you're going to get a very large number.",
            "Which means that in either case there is going to be effectively no learning signal that we can use to tune the parameters so as to maximize the low probability of a given training sentence.",
            "But turned out."
        ],
        [
            "Note that the exploding gradient, so when the norm of the gradient is exposed to the Infinity is not really a problem.",
            "At the end of the day.",
            "What people have figured out, including past canoan L from 2013 who are here as a PhD student and then who is now at the gold in mind.",
            "What he and Joshua and Thomas Michael showed is that you can simply.",
            "Look at the norm of the gradient and then if the norm is too large, as in if it's larger than one or five, then you're going to simply shrink the magnitude to that threshold, yes?",
            "Oh, you did?",
            "OK, that's great.",
            "Yes, did you talk about the STM as well, OK?"
        ],
        [
            "So you learned almost everything alright, but that's good because I was thinking that you.",
            "Spend a lot of time alright, yes, so yes.",
            "So you know like you know about the vanishing gradient exploring gradient and then those are problems right now.",
            "Let's think about the exploding gradient turned out to be a bit easy to address, but the vanishing gradient is a bit problematic.",
            "There were some solutions proposed earlier, but now let's think about why that happens and why that actually calls very naturally.",
            "Calls for the LTM or the gated recurrent units."
        ],
        [
            "So when we look at the gradient vanishing of the gradient, what we see is that.",
            "Gradient vanish is because when you do the back propagation, it has to go through every single time step.",
            "So as the error is back, propagated is going to be multiplied by the transition rate matrix at every time step and that is the part where the normal of the error derivative is going to shrink.",
            "Now then, you know the natural like the solution is to.",
            "OK, what if we can send the aeroderivative?",
            "By passing a lot of steps right away, that will be the solution, because if we just go from here all the way to the time step T without going through all these nodes effectively, we don't have to do the multiplication of the transition matrix and we can just send it right away without sacrificing the without banishing the norm of the error derivative.",
            "So we can think of adding the temporal shortcut connections, so we're going to Add all those."
        ],
        [
            "Short connections that bypasses some number of time steps and then when we have those connections the error derivatives will be backpropagated through these connections, bypassing all those steps addressing or the avoiding.",
            "The issue with the vanishing gradient.",
            "But the obvious issue is that we cannot really put all those shortcut connections as many as we can because first of all, the number of parameters will explode and as soon as we fix the number of the shortcuts that we're going to add, we effectively put the bound on the.",
            "What is the longest context?",
            "Words are context length.",
            "We're going to handle what is the longest context we're going to handle, so we're going to instead applied."
        ],
        [
            "The idea of the leaky integration and that leaky integration is going to be based on the adaptive coefficients, so we're going to say that the at every time step the deactivation or the new memory state value is going to be the combination or the convex some of the previous activation and the current candidate activation.",
            "And how do we combine them that is based on what the input says, so we're going to let the neural net this size.",
            "So based on the current input.",
            "And the previous hidden activation based on those neural net is going to automatically decide how much of the previous activation is going to be carried over, effectively creating a shortcut or how much is going to be replaced with a new value.",
            "So that's going to be this adaptively integration.",
            "And now once we do that, we get all those shortcut connections that are adaptively created on the fly based on the input.",
            "So that's great.",
            "But what happens is that that can have a potential downside of.",
            "Diluting the credit assignment process of the back propagation.",
            "So what backpropagation is trying to do is to figure out which hidden neuron at which time step is most responsible for the cost function at in the future, and then if there are so many paths, then the back propagation gets, let's say confused and then it tries to, let's say, sends out the signal in a very smooth way.",
            "So instead what we want to do is that we will."
        ],
        [
            "To be able to let them network prune automatically.",
            "The unnecessary connections and that is by introducing a reset gate, which again is based on the input and previous activation.",
            "The neural net is going to automatically decide whether we're going to prune out certain connections by masking out the influence from the previous activation.",
            "And combining this reset and update gate together with the adaptive link integration, what?"
        ],
        [
            "It is a gated recurrent unit.",
            "And you can see that this one is going to have a less problem with the vanishing gradient because we create a shortcut on the fly as needed and the aeroderivatives can bypass multiple steps if the network has deemed that that is needed.",
            "And this gated recurrent unit is essentially a variance, a simpler variance of the long short term."
        ],
        [
            "Memory units which are proposed already in the 1999 and 2001 by the group in Switzerland are led by the Organism it over and the idea is very similar.",
            "Essentially, you want to make sure that the network is able to carry over the activation forward in time without overriding it, thereby making the aeroderivatives to be able to bypass many of the steps and that effectively avoids the issue with the vanishing gradient.",
            "So I'm pretty sure this is the second time you heard, so you're like completely.",
            "Completely like let's say sure about all these things, right?"
        ],
        [
            "Yes.",
            "While you were that.",
            "When?",
            "All the way to 220.",
            "Grease the gradients also exponentially, because essentially you would have if you will.",
            "Yes, yes."
        ],
        [
            "In fact, that's the thing that I didn't mention is that that is precisely the reason why we want to make it adaptive.",
            "So The thing is that what we want is that the not, too, let's say, make sure that the gradient is not going to vanish because gradient might be banishing because there is no such signal in the data itself.",
            "So we make sure that the unit is going to be close to either one or zero.",
            "If you if you set the UTI to certain value ourselves and then use the constant, what happens is exactly what you said is going to always vanish, whereas UTI can be either close to zero or one and then when it's close to one is effectively creating a very good shortcut and then when it's zero is going to just ignore it.",
            "So that's the idea.",
            "So it doesn't of course solve the finishing radiation perfectly.",
            "I'm not even sure if that is even possible by the definition."
        ],
        [
            "But yes, OK, now finally to the machine translation that actually I do really like.",
            "OK it took a long time to come here.",
            "Now, one thing that I want to talk about a bit of the recurrent language model is that you know we learned how to generate.",
            "We observe that we can generate an amazing sentence out of the trained recurrent language.",
            "Neural net language model, which was not really true with the N gram language model or any of the Markovian language model, whereas without any Markov Markov assumption and with the power of those gated recurrent units or the LCMS effectively, what we have now is a language model that.",
            "Can not only score a given sentence, but to generate a sentence that is likely under the trained models distribution and we're going to use that idea in a slightly more interest."
        ],
        [
            "In application of the machine translation.",
            "So what is machine translation?",
            "Machine translation is infected, probably one of the most elegant problem you can find where your goal is to try to maximize the probability of a correct translation given a source sentence.",
            "It's like the very less a straightforward application of simple supervised learning, and then that probability can be decomposed.",
            "Or, you know, using the base rule you can rewrite it as the probability of going back so they're doing the back translation.",
            "So what is the translation probability of the source sentence given the correct translation and the probability of the correct translation itself?",
            "And why is it really amazing?",
            "Is that we can essentially train 1/2 of the model?",
            "Using the parallel corpus where we have these set of sentence pairs, where each pair consists of the source sentence, Ann is correct translation and the other half with the monolingual corpus, as in like unsupervised data, right?",
            "So you have no annotation, you just saw the text and we can use that to tune this model as well as possible using as much resources you have.",
            "And then you just maximize the probability.",
            "So this is a very elegant."
        ],
        [
            "Except that in reality it has not been that elegant because first of all we don't know one thing we're sure is that from the source sentence to his translation is going to be super nonlinear.",
            "But The thing is that we didn't really know how to tune those.",
            "Let's say no nonlinear models in a very large scale data, So what people have resorted to is to make a lot of feature functions.",
            "So which words correspond to which word in the translation you know, like how they are distorted, how they're ordered, all those different features, and then put it into the log linear model.",
            "So it's effectively a linear model.",
            "And what happens is that the they ignore the normalization constant, thereby they just kind of ignored the whole, let's say.",
            "Elegance of the probabilistic view of the problem itself.",
            "And what happens is that because this model is so weak, if you get the translations, you get a lot of garbage, so you need to have a way to filter them out.",
            "And of course filtering we know how to do it right.",
            "We already learned I spent like an hour an hour to talk about how to select a very likely sentence, and then you know people and we can use the very strong external, let's say neural language model or RNN language model or N gram language model.",
            "And that has.",
            "This has been the kind of usual way.",
            "But"
        ],
        [
            "It turned out because we know how to train recurrent language model now and learn how to use the recurrent neural net to summarize a input that is of the variable length we can.",
            "Construct a single model that's going to maximize the probability of the correct translation given the source sentence.",
            "As is this instead of going into all those feature engineering with a very weak model followed by a strong language model.",
            "And is there a new thing?",
            "It's not really a new thing, so apparently in 1991, in fact, before the year before that, in 1989 and 90 and 91 people at CMU and some other places have proposed a very same model to do the machine translation.",
            "And of course they had a data set of about 2000 to 3000 sentence pairs.",
            "So it was impressive back then, but it was not impressive enough.",
            "And then in 1997 the idea was revived in Spain where they wanted to build essentially this model.",
            "Which has a recurrent neural net encoder.",
            "The reason source sentence one similar time summarize it into the hidden vector and based on that hidden vector, we're going to have another record year in the language model that's going to generate one symbol error time.",
            "How can we generate it?",
            "Because we compute the distribution, we simply sample one at a time and this idea was revived in 1997.",
            "But that idea kind of went away because they just realize that they don't have enough computing resource nor data set to play around with and only in 2013 and 14 the group at Oxford.",
            "Group at Google and group.",
            "Here in Montreal we have developed and proposed to use this and now we have all those GPU's and we know how to train this recurrent neural net using LST emoji Arias and it turned out that we can in this train a single model that's going to approximate the correct conditional distribution of the translations given the source sentence.",
            "Now I will."
        ],
        [
            "Need to go into this detail, but I'm pretty sure that the Joshua Sumet and add who is going to give a lecture later on.",
            "I have covered a lot of things so I want to spend the next 10 minutes, which is just the last 10 minutes to talk about what this neural net community or the techniques from the neural Nets.",
            "Is bringing to the NLP because I just realized that the lecture title is deep NLP one and I wanted to say that the OK how this deep learning is changing the NLP so?"
        ],
        [
            "Pasta machine."
        ],
        [
            "Station I have a blog."
        ],
        [
            "So you can read about it so.",
            "So what is deep natural language processing?",
            "So I'm going to talk about what my view is, and then you know, like how we should approach NLP, which is again going to be a bit different from what Ed is going to say, so."
        ],
        [
            "1st is that if we can actually forget about words.",
            "Because neural Nets don't really care about words."
        ],
        [
            "So in the machine translation system or a lot of the systems until like January this year.",
            "We're using words.",
            "And this model has a very little structure in it, but when I see it, you know like I see that the there are just so much explicit structure at the very very roll over the input and output level.",
            "Why are we going into a sequence of words when the neural net doesn't really know whether there's a worse characters phrases, it only sees the one at vectors.",
            "It's a sequence of one of vectors."
        ],
        [
            "And a lot of people with."
        ],
        [
            "Sorry about that.",
            "Oh OK yes."
        ],
        [
            "And there are legitimate reasons why you know you.",
            "Probably you might want to use the words.",
            "The first thing is that you know we kind of have a belief that the world is a basic unit of meaning, which is a bit debatable.",
            "And the second thing is that we have an inherent fear of data sparsity, and data sparsity gets worse and worse as the length of the sequence gets longer and longer because the state space grows exponentially large with respect to the length.",
            "And what happens is that if you put it into the sequence of characters instead of sequence of words, suddenly the length of the sentence.",
            "Cause growth spiral 5 to sixfold.",
            "In the case of English and 3rd, we are worried that we cannot really train the recurrent neural net on those long sequences.",
            "We were right."
        ],
        [
            "And it turned out that the these techniques from the deep learning in the natural language processing effectively has address those two most important issues, reasons that we were working with the words first Yoshua Bengio already in 2003 and 2000 showed that we don't really have to fear the data sparsity.",
            "If you go into the continuous representations."
        ],
        [
            "And then starting from the.",
            "Yeah 2011, we have already learned that in terms of language modeling we can train a recurrent neural net language model on the characters and then can generate an amazing text that is just one character at a time generated by the recurrent neural net language model.",
            "And 3rd."
        ],
        [
            "We don't have to worry about training recurrent neural net, unlike in 1994 when your shows is that yeah, it's a very difficult problem, but thanks to the gated recurrent unit in LCMS, which we learned about it today and yesterday we know that we can up to certain point, which is actually very long.",
            "We can train a recurrent neural net language model very nicely without too much issue, as long as you have some patients.",
            "Now what?"
        ],
        [
            "Means is that the in this new territory of deep NLP we don't have to be bound by the constraints of the having to have a sequence of words, which is a big constraint, especially if you go beyond English.",
            "If you go into the, say, Chinese, you don't have blank space that you can use to separate the sequence in towards, you have to run certain other algorithm to get a sequence of words and then you know who knows how optimal that is.",
            "Then maybe very suboptimal.",
            "And then it turned out."
        ],
        [
            "That the.",
            "Some of the languages, such as Arabic, has a very interesting morphology that you see one word here that it corresponds to N. 2 his vehicle, and it's not even clear whether we can segment it out clearly into the separate pits.",
            "And there are few more weird languages where you know you get very interesting phenomena of the compounds of the 8, two, 12 words that is combined into a single word, such as in finish.",
            "And then as Joshua said earlier, what happens is that the word based models are going to assign the same amount of parameters to a word.",
            "That is, that means 3000 kilowatt per minute an hour and just three.",
            "So it's going to spend the same amount of capacity on these two.",
            "Tokens and that just doesn't make any sense, whereas."
        ],
        [
            "What we're learning is that the we don't have to do it anymore.",
            "In deep NLP, we're going to go into the part where we're going to just go directly into the characters, just like we have done in computer vision where we just went all the way down to the pixels."
        ],
        [
            "We'll just plug in the characters.",
            "Instead of a word and then based on that, the based on the recurrent neural Nets processing of the character sequence, we're going to consider the output of that as a vector that corresponds to word, and we're going to let the next level recurrent neural net to read the vectors that are result of the lower level recurrent neural net and it turned out that it actually just works.",
            "A lot of reason works by from the Stanford Google, Deep Mind, CMU, NYU, you and all these places are pointing to the fact that the.",
            "In fact, with this deep learning now, NLP gets slightly easier because we don't have to do the segmentation in advance."
        ],
        [
            "So yeah, I'm running out of time.",
            "I had some."
        ],
        [
            "Anything to say?"
        ],
        [
            "OK."
        ],
        [
            "And Furthermore, what we have observed is that we don't even have to have a hierarchical structure, and we can actually go directly into the character level sequence with just a number of layers without having an explicit separation.",
            "We just stack the recurrent neural Nets number of them and then let it figure out how to segment it automatically inside the network by consuming a sequence of character without any kind of separation symbol.",
            "Saying that, hey, this is one word, there is another word.",
            "And it turns out that if you do that, the new recurrent."
        ],
        [
            "That figures out how to say."
        ],
        [
            "Operated automatically, so this is an example of machine translation system with the attention mechanism you learned from Summit yesterday.",
            "So the source sentence is given in a word.",
            "And target this is the generated translation in German that was generated one character at a time and we see the alignment here and we see that the recurrent neural net has implicitly learned how to learn the learn to segment the character sequence into words like units.",
            "It doesn't have to be and then correctly align to the corresponding words in the source side English.",
            "So it seems like we have a amazing tool that even can learn implicitly the structure of this segmentation automatically from the sequence of characters, just like what the computer vision has figured out four years ago that you can just go into the pixel level and then convolutional neural net is going to figure out what kind of objects are there, and then it's going to do the implicit segmentation.",
            "And this is exactly happening in NLP as well."
        ],
        [
            "And then second thing I wanted to want to say is that in the deep NLP now we can do the multi lingual processing so we can build a single model that's going to handle multiple languages.",
            "All simultaneously.",
            "Without any issue.",
            "Because why is that?",
            "Because we can go into the continuous space and then in that continuous space all we need is a neural Nets that are going to project different languages.",
            "So the sentences in different languages into a common point.",
            "And it turned out that in terms of New York machine translation."
        ],
        [
            "It is indeed possible to build a single model that's going to handle, let's say, six different source languages and six different target languages, and we train one model jointly on the data from all those languages simultaneously.",
            "And it turns out that the model can actually do the translation better.",
            "Then training, let's say 10 different single pair models while having much smaller number of parameters."
        ],
        [
            "Ha."
        ],
        [
            "Unfortunately, I'll have to skip, but."
        ],
        [
            "The person that you want to talk with."
        ],
        [
            "If you're interested in it is or hand.",
            "Who is sitting somewhere here?",
            "Yes there.",
            "So he has built this awesome multilingual translation system that actually works.",
            "And then if you're interested, you can talk to him about it.",
            "And then the."
        ],
        [
            "Last thing I want to talk about.",
            "I have like 1."
        ],
        [
            "It is now the these neural Ness or the continuous vector representing yes."
        ],
        [
            "Oh yes, so I was.",
            "So these are all different, let's say languages and I wanted to give a fair chance to all the different territories and states that use the same language.",
            "So you probably all you should recognize that this is a Qu\u00e9bec flit and that stands for French Canadian for English, Mexican for Spanish, and you know, like it's a quite interesting thing to try it out and you know this is Spanish.",
            "Yeah, I want you to be as fair as possible and promoting diversity and so on, yes."
        ],
        [
            "So the last thing that I want to say this is the one thing that everyone is agreeing that you know we should really think now in the NLP community is that the discontinuous piece representation finally gives us a means to go above or beyond the sentences.",
            "We can make a neural net that can take a look at much larger context without fearing about the data sparsity.",
            "So we can really build a model that's going to look at the much larger context to solve the problem."
        ],
        [
            "Better and then context definitely matters, because context tells us what is a theme or topic of a document or the dialogue.",
            "An, in practice, it actually tells us precisely or better what the following word is going to be.",
            "So for instance in this case.",
            "If we have only this sentence, it's not going to be easy to fill in this empty boxes, but as soon as we have access to the context, it becomes quite trivial to fill in these boxes an with the neural net.",
            "We can naturally train one model that's going to read the whole context and then tries to model the following sentence in order to fill in the missing gaps and."
        ],
        [
            "Already starting from few years back, people have realized it and we're building a model that is doing less eight language modeling where you model the current sentence while explicitly conditioned on the previous sentences, which is going to be context.",
            "And it turned out that doing that."
        ],
        [
            "Guess us always.",
            "A benefit in terms of the perplexity.",
            "So perplexity means how predictable next word is, so it directly reflects how good a language model is.",
            "And as we put more and more context, which is the X axis, we see that the perplexity goes down, which corresponds to a better and better predictability, meaning that we get a better and better language model an.",
            "How do?",
            "How do I know that the context actually tells us about the topic and theme?"
        ],
        [
            "It turned out that the types of words that gets better modeled or better predicted are the words that are also called open class words, which are nouns, verbs, adjectives.",
            "Essentially, they are the ones that are going to be influenced most by the topic of the documents, whereas the functional words such as the.",
            "What we have here?",
            "OK, so coordinator determiner, preposition.",
            "Those things are, you know, it has a grammatical function and then top it doesn't really matter and this graph the improvements on these open class words essentially tells us that context given the context, neural net is able to automatically figure out the topic.",
            "And now this tells.",
            "This gives us amazing opportunity to do, for instance Q&A in a very."
        ],
        [
            "Large word order."
        ],
        [
            "In the dialogue mode."
        ],
        [
            "Playing and then eventually making a translation or Q&A or dialogue system that's going to have access to the word knowledge.",
            "So think about building a neural net where input is going to be the question.",
            "And the whole Wikipedia or input is going to be the question and whole Internet, and that's going to that.",
            "Neural net is going to try to answer the question based on the whole world knowledge.",
            "There is all possibility and this means that the we have so many interesting problems that we can solve in NLP with this deep learning.",
            "And."
        ],
        [
            "I said thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me try to think about the language understanding a bit.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean to build an agent?",
                    "label": 0
                },
                {
                    "sent": "That's going to understand the natural language?",
                    "label": 1
                },
                {
                    "sent": "And then you know, like the immediate thing that I come up with that say, OK, so how does it actually differ from language modeling?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "At that point, you know there are two sides, right?",
                    "label": 0
                },
                {
                    "sent": "So one is to view it as a statistical phenomenon and the other way is to view it as a.",
                    "label": 0
                },
                {
                    "sent": "You know, like some kind of linguistics.",
                    "label": 0
                },
                {
                    "sent": "Really, there is some theory and some structure that's going to generate the actual language.",
                    "label": 0
                },
                {
                    "sent": "So the next question is should we?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start reading linguistics and already from the late 80s people have realized that they maybe that's not necessarily the way to go.",
                    "label": 0
                },
                {
                    "sent": "In fact, I heard that this is not really the right quote, but the Fred Jelinek at IBM back then in 1988 said that every time I fire a linguist, the performance of the speech recognizer goes up.",
                    "label": 1
                },
                {
                    "sent": "Whenever I show this quote, everyone starts to hate me in another community.",
                    "label": 0
                },
                {
                    "sent": "OK, not here.",
                    "label": 0
                },
                {
                    "sent": "So then you know like, OK, so it seems like the linguistics doesn't have to be the way to go.",
                    "label": 0
                },
                {
                    "sent": "Then you know what will be the next possibility.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next possibility of the language understanding might be to build an agent that is able to tell how likely a given sentence is.",
                    "label": 1
                },
                {
                    "sent": "Think about the question and answering.",
                    "label": 0
                },
                {
                    "sent": "So how likely is a sentence given sentence as an answer to the question.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, question, who is the president of the United States?",
                    "label": 0
                },
                {
                    "sent": "A likely answer in 2016 is going to be Obama.",
                    "label": 0
                },
                {
                    "sent": "Is the president of the United States, and that's going to be the answer, whereas the all the other sentences, for instance chip process the President of America is not going to be an answer.",
                    "label": 0
                },
                {
                    "sent": "So by just being able to tell which sentence is likely under the current context.",
                    "label": 0
                },
                {
                    "sent": "Might be a one way to view what the language understanding should be.",
                    "label": 0
                },
                {
                    "sent": "And similarly, we can find a lot of different examples so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance, given this kind of image of two Dolphins, how likely is a given sentence describing this view?",
                    "label": 1
                },
                {
                    "sent": "So likely one would be 2 Dolphins or diving.",
                    "label": 1
                },
                {
                    "sent": "Two men are flying will be unlikely one, so in some sense, if we can tell a given the context why the given sentence?",
                    "label": 0
                },
                {
                    "sent": "How were given sentences likely?",
                    "label": 0
                },
                {
                    "sent": "We might be able to say at least superficially, that that agent or machine has an understanding up to certain extent of the natural language, so I'm going to make a bold claim for now in this lecture that the language.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Understanding or the natural language understanding is all about telling how likely a sentence is.",
                    "label": 1
                },
                {
                    "sent": "I believe that in the second lecture this morning, we're going to hear a bit different story, but for now we're going to just say that it's all about putting a right probability to a very likely sentence, and then any machine that can do it is going to have an understanding of natural languages.",
                    "label": 0
                },
                {
                    "sent": "So in order to do so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to talk about the language modeling.",
                    "label": 0
                },
                {
                    "sent": "So language modeling is like.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really super simple, so we're going to say that we are given a sentence and what is sentence?",
                    "label": 1
                },
                {
                    "sent": "Sentence is a sequence of symbols and the symbols can be worse characters, bytes or bits or phrases the other way around as well.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, what is the probability of that given sentence or a sequence of symbols?",
                    "label": 1
                },
                {
                    "sent": "And that's going to be written as the probability of X one X2 and until XT.",
                    "label": 0
                },
                {
                    "sent": "And then we have to talk about a bit of probability.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's going to be the single slide where we talk about probability and how it actually works.",
                    "label": 0
                },
                {
                    "sent": "That is, does everyone remember what the joint probability, conditional probability and marginal probabilities so anybody who hasn't taken probability one please raise your hand?",
                    "label": 0
                },
                {
                    "sent": "I'm going to spend about 3 minutes here, otherwise I can just skip.",
                    "label": 0
                },
                {
                    "sent": "So you have selected a great set of students.",
                    "label": 0
                },
                {
                    "sent": "Everyone has taken it OK, so the joint probability.",
                    "label": 0
                },
                {
                    "sent": "Let's say we are given two variables X&Y joint probability is going to be the probability of.",
                    "label": 0
                },
                {
                    "sent": "Both of them happening at this certain configuration and conditional probability is given certain value or the event.",
                    "label": 1
                },
                {
                    "sent": "What is the probability of another, let's say variables happening at certain configuration and marginal probability is after removing the effect of another variable.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of the remaining variable and they're all related by this Bayes rule where the joint probability is the product of the conditional probability and the marginal probability of the condition variable.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now using that rule we can rewrite the probability over a sequence of variable into a product of the conditionals, where each conditional tries to predict the following symbol given all the previous symbols.",
                    "label": 1
                },
                {
                    "sent": "Now, because there are like more more than two variables, is like, it might be a bit unclear.",
                    "label": 0
                },
                {
                    "sent": "But if you look at it graphically is quite clear, so we're going to have the T different variables that correspond to the different symbols in a sentence.",
                    "label": 0
                },
                {
                    "sent": "We're going to say that OK.",
                    "label": 0
                },
                {
                    "sent": "Given any of the given all the previous symbols, what is the probability distribution of the coming symbol, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case, what is the probability of the first symbol is going to be defined as it is and then probability of the second symbol is going to be defined with respect to the first symbol and probability of the third symbol is going to be conditioned on the 1st two symbols, and so on, and then we see that this is actually exactly, say equation, so we lose really almost nothing except for the fact that we have imposed this kind of search.",
                    "label": 0
                },
                {
                    "sent": "This kind of let's say conditional structure on the sentence.",
                    "label": 0
                },
                {
                    "sent": "And now since we have the probability, we can write the probability of a sentence.",
                    "label": 0
                },
                {
                    "sent": "Now it becomes a straightforward maximum likelihood estimation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given a sentence given a corpus that consists of many, many sentences, we want to make sure that the probability the model assigns to each of the training sentences is going to be as high as possible.",
                    "label": 0
                },
                {
                    "sent": "And there can be simply ritualized Max over the expectation over the corpus of the low probability of all the sentences in the corpus.",
                    "label": 0
                },
                {
                    "sent": "Of course, you're looking at it now.",
                    "label": 0
                },
                {
                    "sent": "It's pretty obvious that you want to do it, but apparently it was not that obvious that you know we have to approach it from the statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in 1990 the people at IBM proposed to do this machine translation in a statistical way, and then they submit a paper.",
                    "label": 0
                },
                {
                    "sent": "And before that, everything was writing about the rules and everything in order to make the translation system work and the reviewers back then said that the crude force of compute computers is not science.",
                    "label": 1
                },
                {
                    "sent": "Of course, the first author got really mad at no.",
                    "label": 0
                },
                {
                    "sent": "The last author got really mad at it, but essentially this view of looking at the language, understanding of language modeling from the perspective of statistics is really important idea.",
                    "label": 0
                },
                {
                    "sent": "But it's not really obvious as we think we are as we think it is these days.",
                    "label": 0
                },
                {
                    "sent": "So how do we actually?",
                    "label": 0
                },
                {
                    "sent": "The probability and then trying to maximize or the assign a good probability to training sentences.",
                    "label": 0
                },
                {
                    "sent": "So the one of the last sentence.",
                    "label": 0
                },
                {
                    "sent": "This is not science, right?",
                    "label": 0
                },
                {
                    "sent": "So back then, as far as I know, they used the largest corpus that anybody has ever used before, and then in order to do so, they had to use amazing amount of computing resources at IBM unlike all the other people who are feeding the translation model exactly.",
                    "label": 0
                },
                {
                    "sent": "Yes, so which is kind of like.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that this is a model that actually works, unlike the other existing models that we're using less computers and doing more science.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first approach we can think of is called Ngram language model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we looked at the factorization of the sentence probability, we saw that the every conditional distribution of a teeth variable or the symbol was conditioned on every every single.",
                    "label": 0
                },
                {
                    "sent": "Let's say previous word, and that seems like too much, so we're going to make assumption or the NTH order Markov assumption saying that the any of the word or the symbol in the sentence is going to only depends on the end previous sentences.",
                    "label": 1
                },
                {
                    "sent": "Not the sentence.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that and previous symbols.",
                    "label": 0
                },
                {
                    "sent": "So it's written here, so the probability of XT is going to be conditioned on the variables from XT minus North to the XT minus one.",
                    "label": 1
                },
                {
                    "sent": "There might be actually an minus one of them, but OK. And then now, since we made this kind of assumption, what we can simply do is to for each of the conditional probabilities, conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Here, we're going to collect the statistics from very large corpus and the collecting statistics sounds fancy, but all you're going to do is that they essentially count how many times X T -- N to X T -- 1.",
                    "label": 0
                },
                {
                    "sent": "This phrase.",
                    "label": 0
                },
                {
                    "sent": "Of course, in the corpus and then also to count a slightly larger phrases and then you know, like just.",
                    "label": 0
                },
                {
                    "sent": "Divide the letter with the former.",
                    "label": 0
                },
                {
                    "sent": "Then we get exact maximum likelihood solution.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is very straightforward, But let's take an example.",
                    "label": 0
                },
                {
                    "sent": "So my sentence is going to be out.",
                    "label": 0
                },
                {
                    "sent": "I would like to comment the aperture on his work.",
                    "label": 0
                },
                {
                    "sent": "By the way, this example is from the field lonesome at Google in mind.",
                    "label": 0
                },
                {
                    "sent": "I shamelessly two kids slide, but it's a very nice example I believe.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to compute how many times each of the phrase, so we start from a single symbol and that we call the unigram modeling, so we can make a very extreme assumption that the probability of a sequence of symbols is going to equal to the product of probability of each symbol independently from each other.",
                    "label": 0
                },
                {
                    "sent": "In that case we get the low probability or the negative log probability of.",
                    "label": 0
                },
                {
                    "sent": "Actually, average low probability of the 8.051, so it's higher the worse because we look at the negative value of the probability and then of course it's not that great, so we're going to make the assumption slightly less by modeling bigrams or the two words phrases at a time.",
                    "label": 0
                },
                {
                    "sent": "And then we look at the look at the probability of I.",
                    "label": 0
                },
                {
                    "sent": "An probability of good given I probability of light given note and then so on until the probability of the end of the sentence symbol, which is a special symbol that we're going to just introduce in order to make our life easier given the punctuation mark at the end.",
                    "label": 0
                },
                {
                    "sent": "And then we see that the probability of this very likely sentence went down by the order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "And then of course we can make it even better by considering two previous words instead of one for every single symbol you get that is called trigram modeling, where we're going to start with the probability of I, the first word.",
                    "label": 0
                },
                {
                    "sent": "Probability of would give 9, which is the second word, and then when we went to the probability of the Third world, we're going to condition it on both.",
                    "label": 0
                },
                {
                    "sent": "I would and then we see that the probability assigned by the model.",
                    "label": 0
                },
                {
                    "sent": "This is estimated model to the sentence has got even better and then we can continue going on.",
                    "label": 0
                },
                {
                    "sent": "All the way as much as you want, but obviously there is some issue.",
                    "label": 0
                },
                {
                    "sent": "Do you see some issue here?",
                    "label": 0
                },
                {
                    "sent": "Do you see this kind of, let's say, the issue with this kind of formulation.",
                    "label": 0
                },
                {
                    "sent": "There was a like hand, right?",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so this is too good an answer right away.",
                    "label": 0
                },
                {
                    "sent": "OK, that actually makes it difficult to do some kind of discussion.",
                    "label": 0
                },
                {
                    "sent": "So you see that you know, like we multiply the probability or estimate of the conditional probabilities and what's the kind of, let's say property of multiplication.",
                    "label": 0
                },
                {
                    "sent": "If you multiply anything with zero, you're going to get zero.",
                    "label": 0
                },
                {
                    "sent": "In other words, if any of these phrases never occur, however likely it is.",
                    "label": 0
                },
                {
                    "sent": "But in the corpus, then you're going to get the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zero probability.",
                    "label": 0
                },
                {
                    "sent": "So let's say this is an example, so I was in Edinburgh when I was making this example.",
                    "label": 0
                },
                {
                    "sent": "So the probability of a tenured professor likes drinking whiskey is going to be decomposed into a probability of a probability of tenured given a probability of professor given a tenured, and so on, and we can see that the drinking whiskey, for instance, is a very very likely phrase, especially in Edinburgh an A is a very frequent words, but let's assume that the.",
                    "label": 1
                },
                {
                    "sent": "A tenured professor just never occurred in our corpus, and suddenly the probability of the whole sentence goes to 0.",
                    "label": 1
                },
                {
                    "sent": "And this is the issue often refer that to as data sparsity or data scarcity.",
                    "label": 0
                },
                {
                    "sent": "So when the data just cannot cover every phrases that we're going to expect to have in our test sentence, and as soon as there is a single phrase that does not occur in the training set, we're kind of doing.",
                    "label": 0
                },
                {
                    "sent": "Our model is going to Simply put the zero probability to every sentence that contains that kind of phrase.",
                    "label": 0
                },
                {
                    "sent": "And obviously people have notice.",
                    "label": 0
                },
                {
                    "sent": "Notice this already from the very early days.",
                    "label": 0
                },
                {
                    "sent": "And then there are few.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've been thinking about this issue of the data sparsity and then the.",
                    "label": 0
                },
                {
                    "sent": "You know there can be like quite straightforward, obvious solutions.",
                    "label": 0
                },
                {
                    "sent": "First of all, you can think of just smoothing it out.",
                    "label": 0
                },
                {
                    "sent": "You're going to say that the any phrase you can think of is going to be assigned a very small probability that does not have anything to do with the actual corpus.",
                    "label": 0
                },
                {
                    "sent": "How it is, how many times it occurs in the corpus, and that's called the smoothing or the add Alpha smoothing.",
                    "label": 0
                },
                {
                    "sent": "In this case, by having certain amount of.",
                    "label": 0
                },
                {
                    "sent": "Constant added in the denominator and numerator.",
                    "label": 0
                },
                {
                    "sent": "We make sure that we are never going to run into a phrase that is assigned zero probability, but you can see that it's kind of like OK. Dumb way to do it, right?",
                    "label": 0
                },
                {
                    "sent": "So if you want to be slightly smarter, you can back off to a lower order ngram.",
                    "label": 0
                },
                {
                    "sent": "So let's say you want to compute the conditional probability or you want to estimate the count statistics over a very long phrase, but you see that that phrase never happened, and what you're going to do is that your case, then I'm going to back off and then for this phrase I'm going to forget about the first few symbols until I find the phrase that actually occurs in the training corpus and use test statistics with some transformations.",
                    "label": 0
                },
                {
                    "sent": "Instead of the original statistics, which is going to be essentially the zero probability estimate.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go into detail of this because I have like so many slides to talk about anyway.",
                    "label": 0
                },
                {
                    "sent": "But even then, even even if we use back off and then all these smoothing techniques together.",
                    "label": 0
                },
                {
                    "sent": "Amazing ngram model.",
                    "label": 0
                },
                {
                    "sent": "That's going to avoid the issue with the data sparsity.",
                    "label": 0
                },
                {
                    "sent": "This kind of N gram language model is pretty unsatisfactory in another aspect that is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is no generalization.",
                    "label": 0
                },
                {
                    "sent": "So let's say we, in the corpus we had a very many occurrences of chases, adult chases, a cat chases a rabbit.",
                    "label": 1
                },
                {
                    "sent": "And then suddenly in our test sentence, there is a phrase.",
                    "label": 0
                },
                {
                    "sent": "That is, Chase is a llama.",
                    "label": 0
                },
                {
                    "sent": "And of course, we all know that the llama is a an animal.",
                    "label": 0
                },
                {
                    "sent": "I hope everyone knows.",
                    "label": 0
                },
                {
                    "sent": "I think it's from South America.",
                    "label": 1
                },
                {
                    "sent": "An because we know that Llama is an animal just like dog, cat or rabbit.",
                    "label": 0
                },
                {
                    "sent": "Even without looking at the corpus or anything, we can immediately tell that OK Chase is a llama is very likely.",
                    "label": 0
                },
                {
                    "sent": "But now let's say your training set didn't have.",
                    "label": 0
                },
                {
                    "sent": "In the case of Ngram language modeling.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that in the training set there was no mention of llama at all.",
                    "label": 0
                },
                {
                    "sent": "Then suddenly our N gram language model is going to tell us that the OK, the probability or conditional probability of llama given chases all is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you should have the model should figure out that your Kalama is a dog or cat or something similar.",
                    "label": 0
                },
                {
                    "sent": "So we should put certain type of probability.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Great present.",
                    "label": 0
                },
                {
                    "sent": "It's enough that it's not in that context.",
                    "label": 0
                },
                {
                    "sent": "Ono?",
                    "label": 0
                },
                {
                    "sent": "OK, exactly, exactly, yes.",
                    "label": 0
                },
                {
                    "sent": "So you can back off.",
                    "label": 0
                },
                {
                    "sent": "But if this context was not there, then you know we can go to a very dumb estimate or just a zero estimate.",
                    "label": 0
                },
                {
                    "sent": "So this is really problematic an I check the Google book, Google Books Ngram and really Chase is a llama.",
                    "label": 0
                },
                {
                    "sent": "Never occurs in the gigantic corpus or the engram that they have built is the probability zero phrase we have here.",
                    "label": 0
                },
                {
                    "sent": "Now, in order to avoid that, we can think of, let's say, going into the neural net length, right?",
                    "label": 0
                },
                {
                    "sent": "So let's talk about.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we can avoid that issue?",
                    "label": 0
                },
                {
                    "sent": "So instead of having a count based table based approach now?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to say that, OK, let's forget about it and we're going to go into the approach where we're going to simply replace this complex statistics with a function approximator that is parameterized by a.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "There should be a theater parameterized by Theatre and the function is going to take us to input the context or the variables on which the current conditional distribution is conditioned.",
                    "label": 0
                },
                {
                    "sent": "And returns the probability of the next word XD in this case.",
                    "label": 0
                },
                {
                    "sent": "So is it clear so the transition from the N gram language model to the parametric function approximator?",
                    "label": 0
                },
                {
                    "sent": "But let's say you know it's clear for now and then we're going to use a feedforward neural net to replace this.",
                    "label": 0
                },
                {
                    "sent": "Proper function approximator.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work, it was done initially in 2000 or was in 1999 by Yoshua Bengio who is organizing this summer school.",
                    "label": 1
                },
                {
                    "sent": "So it's a quite it's 2000 OK apparently yes.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we have to decide on how to encode or how to represent a given sequence so that given sequence is going to be the context where the conditional probability is conditional distribution is going to be conditioned on and we're going to take the dumbest possible encoding ever, that is.",
                    "label": 0
                },
                {
                    "sent": "111 K encoding of each word, and we'll say that the OK, so we're going to build a vocabulary, so we're going to build a vocabulary, all the distinct symbols we find in a corpus.",
                    "label": 1
                },
                {
                    "sent": "And we'll say that each word or the symbol is a vector that is binary and has as many elements as there are words or the symbols in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "And we're going to set only one of them.",
                    "label": 0
                },
                {
                    "sent": "One of the elements whose index corresponds to the index of the symbol in the vocabulary to one.",
                    "label": 0
                },
                {
                    "sent": "Now, for instance.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're thinking about the vocabulary of six symbols, and then X T -- 1 is the word or the symbol that corresponds to the second symbol in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "X T -- 2 to the one that corresponds to the 4th symbol in the vocabulary, and so on.",
                    "label": 0
                },
                {
                    "sent": "Now, why is it like the dumbest?",
                    "label": 0
                },
                {
                    "sent": "Because every single symbol in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Is equally distant away from every other symbol, so it does not encode really any kind of prior knowledge about the what each symbol means and how each symbol relates to the all the other symbols in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "And then each of these.",
                    "label": 1
                },
                {
                    "sent": "What are one of vectors or the one of K coded vectors are going to be projected into the continuous space an when we say that and then you get this a fancy continuous vector word representation.",
                    "label": 0
                },
                {
                    "sent": "But obviously that's really offensive term of just multiplying that vector from left with a weight matrix W. And when we build a W, we're going to build up W such that there are as many rows as there are symbol in the vocabulary, and the columns will correspond to the how large the continuous.",
                    "label": 0
                },
                {
                    "sent": "So how was the dimensionality of the continuous spaces?",
                    "label": 0
                },
                {
                    "sent": "And once we have those word vectors, we will concatenate them to form a vector that represents the whole sequence of the context symbols.",
                    "label": 0
                },
                {
                    "sent": "So once we have this, this vector is going to be a representation of the context words XT minus one to X T -- 3 here.",
                    "label": 1
                },
                {
                    "sent": "And that vector is going to go through the nonlinear hidden layer once, twice, or some number of times, and then eventually we get a nonlinear the nonlinear projection of the context vector.",
                    "label": 0
                },
                {
                    "sent": "That is a just a simple continuous vector.",
                    "label": 0
                },
                {
                    "sent": "Now I hope that there was a lecture about the usual MLP and so on, right?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to be sure, just to be sure.",
                    "label": 0
                },
                {
                    "sent": "And now we have this vector H. I'm going to call it H. That summarizes the whole context or the input to the neural net.",
                    "label": 0
                },
                {
                    "sent": "Now what we want is that the afterward we want this neural net to return the probability over the all possible next symbol, so that once we have the probabilities of all of them, we get effectively distribution, and that's exactly what we wanted.",
                    "label": 0
                },
                {
                    "sent": "So what we will we will do is to project that H vector that summarizes the whole context context into a vector.",
                    "label": 0
                },
                {
                    "sent": "That has as many elements as there are symbols in a vocabulary.",
                    "label": 0
                },
                {
                    "sent": "But of course, once we do the projection like this, like the linear projection, we're not going to get probability is going to be like sometimes negative, sometimes positive.",
                    "label": 0
                },
                {
                    "sent": "They will never sent one, so we simply use a softmax normalization to make sure that they're going to be positive first of all, and then when we sum over every single symbol, the probability of every single symbol that some is going to be one, and then we get a probability over all possible words.",
                    "label": 0
                },
                {
                    "sent": "In the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Now at this point.",
                    "label": 0
                },
                {
                    "sent": "You gotta remember that I was talking about the lack of generalization in the N gram language model and how and why I suddenly move on to the neural language model and saying that you know, like, OK, so may so this kind of implies that this is a solution to the lack of generalization in ngram model.",
                    "label": 0
                },
                {
                    "sent": "So perhaps if anybody has some idea of why that would be.",
                    "label": 0
                },
                {
                    "sent": "Anybody know?",
                    "label": 0
                },
                {
                    "sent": "OK, OK that's great.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so how does the generalization actually happen if we use this kind of neural language model compared to the N gram language model?",
                    "label": 0
                },
                {
                    "sent": "Now let's take 3 examples sentences which we're going to assume that they are in the training corpus.",
                    "label": 0
                },
                {
                    "sent": "First sentence says that there are three teams left for the qualification, second sentence, four teams have passed the first round.",
                    "label": 1
                },
                {
                    "sent": "The last sentence is four groups are playing in the field.",
                    "label": 0
                },
                {
                    "sent": "Now that is our training corpus.",
                    "label": 0
                },
                {
                    "sent": "And then someone asked me how likely is the symbol groups followed by three and we see that in the training corpus there is no such occurrence, so in usual engram modeling either is going to be 0 probability, or you're going to back off onto the very dumb, let's say, probability estimate.",
                    "label": 0
                },
                {
                    "sent": "But instead, what's going to happen with the neural language model is that the?",
                    "label": 1
                },
                {
                    "sent": "The neural language model will learn to project three and four which are going to be 2 distinct inputs into a nearby space.",
                    "label": 0
                },
                {
                    "sent": "Nearby points in the continuous space.",
                    "label": 0
                },
                {
                    "sent": "In order to put a good probability that the groups will follow one of those words.",
                    "label": 0
                },
                {
                    "sent": "Because the probability of groups the output is going to be similar, the two distinct inputs will have to be mapped into a similar points in the continuous space.",
                    "label": 0
                },
                {
                    "sent": "And what that means is that the when we are asking for three teams which never really occur.",
                    "label": 0
                },
                {
                    "sent": "By going near where the four is going to be projected.",
                    "label": 0
                },
                {
                    "sent": "Which is it the other way around?",
                    "label": 0
                },
                {
                    "sent": "OK, sorry, sorry about that.",
                    "label": 0
                },
                {
                    "sent": "OK, let me try again, alright?",
                    "label": 0
                },
                {
                    "sent": "So yes, so three and four are followed by.",
                    "label": 0
                },
                {
                    "sent": "Both of them are followed by teams.",
                    "label": 0
                },
                {
                    "sent": "So the neural language model needs to project the two different inputs, tored the similar space in the continuous vector space inside the neural language model, because only from their own you can say that the OK.",
                    "label": 0
                },
                {
                    "sent": "So the teams has a similar probability when follow following either 3 or 4.",
                    "label": 0
                },
                {
                    "sent": "Now what it means is that even though.",
                    "label": 0
                },
                {
                    "sent": "Three groups was never in the training corpus.",
                    "label": 0
                },
                {
                    "sent": "The neural language model is going to project the three near where four is going to be projected and from their own neural language model can tell that the probability of groups following three is going to be something similar to the probability of groups following four.",
                    "label": 0
                },
                {
                    "sent": "So this is a natural natural phenomena you get by training neural language model that exploits the continuous vector space.",
                    "label": 0
                },
                {
                    "sent": "Continuous vector space, yes.",
                    "label": 0
                },
                {
                    "sent": "And number happens in a phrase that we usually discover in the sentence is approximately right.",
                    "label": 0
                },
                {
                    "sent": "Two sides to every point.",
                    "label": 0
                },
                {
                    "sent": "The only number that works there is to say, of course.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So the there are actually two parts of the question.",
                    "label": 0
                },
                {
                    "sent": "First part, first part is that the OK.",
                    "label": 1
                },
                {
                    "sent": "So because.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm going to get there.",
                    "label": 0
                },
                {
                    "sent": "Yes, the first question is.",
                    "label": 0
                },
                {
                    "sent": "You know, like because the three and four, both of them are numbers.",
                    "label": 0
                },
                {
                    "sent": "If you just, let's say, replace it with a special symbol denoting that is a number, then it's going to be all fine even with engram language model.",
                    "label": 0
                },
                {
                    "sent": "And that is definitely true.",
                    "label": 0
                },
                {
                    "sent": "And in practice, in natural language technologies we always do certain type of so-called classing in order to replace numbers that we know are just numbers were going to replace with a special symbols an other named entities as well.",
                    "label": 0
                },
                {
                    "sent": "But this is just because of the example that, but that's a good point.",
                    "label": 0
                },
                {
                    "sent": "I'll try to revise the example where you cannot really do that kind of special treatment.",
                    "label": 0
                },
                {
                    "sent": "And second question, can you repeat the second question?",
                    "label": 0
                },
                {
                    "sent": "Special cases where only.",
                    "label": 0
                },
                {
                    "sent": "Right right, so exactly?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so two sides of the coin, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Alright two sides of the coin, so even then.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it, let's say these two words, let's say three and four are going to be projected into similar points.",
                    "label": 0
                },
                {
                    "sent": "So similar region in the space.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't necessarily mean that they are going to collapse onto each other, so there will be always some signal that neural machine neural language model can exploit in order to assign different probabilities to different context.",
                    "label": 0
                },
                {
                    "sent": "However similar they are.",
                    "label": 0
                },
                {
                    "sent": "So does that answer your question, and in particular, if you have a frequent phrase, then we could learn something specific about that frequently.",
                    "label": 0
                },
                {
                    "sent": "So any other questions?",
                    "label": 0
                },
                {
                    "sent": "Now, OK, that was just saying hi right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Good thing is that since their presentations are very high dimensional, learn that similarities in one dimension are like OK, this number.",
                    "label": 0
                },
                {
                    "sent": "This number they may in this context he similar but another dimension that can be completely different than model learn that relates to other things where you can only play store right right?",
                    "label": 0
                },
                {
                    "sent": "That's precise feature.",
                    "label": 0
                },
                {
                    "sent": "So because we're using a very high dimensional vectors in the neural language model, what's esentially happening is that it can encode the multiple degrees of similarities.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, it's going to encode that these are numbers in some other dimensions.",
                    "label": 0
                },
                {
                    "sent": "You can exploit other dimensions to encode other properties of that word, so it's always possible, and in reality this feedforward neural language models are extremely powerful, especially when used in combination with the N gram language model now.",
                    "label": 0
                },
                {
                    "sent": "So I told you about this example and then of course I completely trust in this.",
                    "label": 0
                },
                {
                    "sent": "Let's say interpretation as well as a lot of other people do as well.",
                    "label": 0
                },
                {
                    "sent": "But how can you be sure?",
                    "label": 0
                },
                {
                    "sent": "Can we actually inspect whether that happens or not?",
                    "label": 0
                },
                {
                    "sent": "And one way people have realized and then we have been using a lot is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We effectively visualize the vectors you get by training this neural language model, and of course you're going to get a 300 dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "And how do we visualize them?",
                    "label": 0
                },
                {
                    "sent": "You know, like with the 300 dimensional vectors that there is no way we can look at it as it as they are, but we can reduce the dimensionality by using certain type of dimensionality reduction algorithms such as Tiffany, which is very famous.",
                    "label": 0
                },
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "Joseph trains yes, right?",
                    "label": 0
                },
                {
                    "sent": "You didn't publish a paper on this, right?",
                    "label": 0
                },
                {
                    "sent": "There's only a figure right that you used once awhile, right?",
                    "label": 0
                },
                {
                    "sent": "And?",
                    "label": 0
                },
                {
                    "sent": "Then use these ideas in their papers I see.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                },
                {
                    "sent": "So this kind of like visualization technique for the neural language model in general was first developed in Montreal, by the Joseph Turian Anusha Banjo and then since then everyone is essentially using it to view it.",
                    "label": 0
                },
                {
                    "sent": "And if you go to NLP community until only this year, everyone was going crazy about visualizing and see you know which words are like related to other words, But what you see is that once you train this kind of model, if you visualize just the very first projection with matrix.",
                    "label": 0
                },
                {
                    "sent": "We see that the similar words are close to each other and this similar words are often far away from each other.",
                    "label": 0
                },
                {
                    "sent": "And this really made this kind of figure, regardless of what kind of perplexity or the performance of the language model you get.",
                    "label": 0
                },
                {
                    "sent": "But this figure is the thing that really like made people go crazy about these continuous vector language models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before moving on to the next lesson, part of my talk, if you have any questions about the feet forward in your language model, please go ahead.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The singular values of this W matrix.",
                    "label": 0
                },
                {
                    "sent": "How many significant single values are going?",
                    "label": 0
                },
                {
                    "sent": "What's the actual functionality, right, right?",
                    "label": 0
                },
                {
                    "sent": "That's actually a really good point.",
                    "label": 0
                },
                {
                    "sent": "And it actually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Depends on the task that you train your model on.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you train your neural language model but not to predict the probability of the next word, But let's say to predict the sentiment of the given sentence, then what you get is that you get a very low low dimensional.",
                    "label": 0
                },
                {
                    "sent": "So the rank of the, let's say word embedding or the first projection matrix is going to be really low.",
                    "label": 1
                },
                {
                    "sent": "And then if you visualize them you see that the all the words are essentially effectively separated along one dimension according to positive and negative signs.",
                    "label": 0
                },
                {
                    "sent": "So it actually depends on the tasks.",
                    "label": 0
                },
                {
                    "sent": "So how much it actually matters.",
                    "label": 0
                },
                {
                    "sent": "So that is the kind of the answer that I really like most, and then second answer is that it turned out that the weight vector a matrix here encodes certain.",
                    "label": 0
                },
                {
                    "sent": "Let's say very shallow properties such as the frequencies as well.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the vectors of a very rare words, you see that they rarely change starting from the random initialization and what people have recently found out is that you can essentially prune out most of the elements.",
                    "label": 0
                },
                {
                    "sent": "In the very rare words vectors, and then I guess that means that the rank is going to be actually much smaller than the full rank.",
                    "label": 0
                },
                {
                    "sent": "I wanna add something so yes.",
                    "label": 0
                },
                {
                    "sent": "Then rare words.",
                    "label": 0
                },
                {
                    "sent": "Have more data to figure out what they mean.",
                    "label": 0
                },
                {
                    "sent": "So if you were just before we cut the dimension it wouldn't work as well and now it is the best models house.",
                    "label": 0
                },
                {
                    "sent": "I don't know 1000 dimensions or things like that so it's pretty large and we did our experiments.",
                    "label": 0
                },
                {
                    "sent": "We started with 20 and then 50 right tip right?",
                    "label": 0
                },
                {
                    "sent": "Yes yes please.",
                    "label": 0
                },
                {
                    "sent": "Words that you want to consider right, sorry.",
                    "label": 0
                },
                {
                    "sent": "Total #4 so the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Capital K. Oh yes, in the one of K encoding, K is going to the number of symbols in the vocabulary, yes.",
                    "label": 0
                },
                {
                    "sent": "If you want to add more, Oh yeah, that's actually a good point.",
                    "label": 0
                },
                {
                    "sent": "So let's say we train this model and then we have.",
                    "label": 0
                },
                {
                    "sent": "We just realized that if we got another set of training sentences and there are symbols that didn't occur in the previous set of training sentences, we one way you can do is to simply add few more in the vocabulary and the re tune the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Or tune only the word vector part.",
                    "label": 0
                },
                {
                    "sent": "But clearly that's not the thing that you want to do, but I'm going to talk about whether we want to even use words in this neural language model later on.",
                    "label": 0
                },
                {
                    "sent": "Today, yes.",
                    "label": 0
                },
                {
                    "sent": "Any yes?",
                    "label": 0
                },
                {
                    "sent": "Words that have multiple meanings that you might have one to one.",
                    "label": 0
                },
                {
                    "sent": "Right, so yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "So because we're going to project down the very high dimensional vector as high as like 1000 dimensional vector in 2D space, we actually cannot preserve every single similarity properties that have been captured by the word vectors word matrix here.",
                    "label": 0
                },
                {
                    "sent": "So in 2011 or 12 I forgot exactly which here, Lawrence, Fundament and propose a way to do a multiple dimensionality reduction simultaneously.",
                    "label": 0
                },
                {
                    "sent": "While making sure that the properties that are captured by each map or the projection operator is going to be as different as possible, and in that paper if you look at the visualization, you see that the each map corresponds to different type of similarities and that kind of that could be the one way to essentially visualize a better.",
                    "label": 0
                },
                {
                    "sent": "This way matrices better.",
                    "label": 0
                },
                {
                    "sent": "OK, so then now we kind of like OK address the issue with the lack of generalization, that's great.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now the one issue I see is that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We had to make this kind of assumption on the what is going to be the maximum length of the context words and we don't want to, you know, like set some kind of constraint like that.",
                    "label": 0
                },
                {
                    "sent": "And the reason why we had to go into this direction initially, not even before the neural language model, is because of the data sparsity issue.",
                    "label": 0
                },
                {
                    "sent": "And then you know, now that we've moved onto the neural net land.",
                    "label": 0
                },
                {
                    "sent": "Is it possible that we can just forget about or abandon this whole assumption on the.",
                    "label": 0
                },
                {
                    "sent": "Whole assumption of Markov property and why do we want to avoid it?",
                    "label": 0
                },
                {
                    "sent": "So in English actually, you know like the N gram language model or the Markov language model works pretty well.",
                    "label": 0
                },
                {
                    "sent": "But even in English there are so many corner cases where this kind of short context will just definitely kill our language models performance.",
                    "label": 0
                },
                {
                    "sent": "Let's take an example by the Steven Clark.",
                    "label": 0
                },
                {
                    "sent": "The same the same stuff which had impeded the car of manual guests in the past 30 years, in which he refused to have removed.",
                    "label": 1
                },
                {
                    "sent": "Let's say we want to model.",
                    "label": 0
                },
                {
                    "sent": "Probability of this sentence using the ngram language model.",
                    "label": 0
                },
                {
                    "sent": "When we compute the probability of removed.",
                    "label": 0
                },
                {
                    "sent": "We actually need to know the stump was there in order to put the high probability, because it is the stump that he's refusing to remove.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that any kind of ngram model, however, in practice where we go up to, let's say 7 words in the context, will never be able to capture that the probability of this remove is high because of stump, because it never actually sees stump in the way.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's try to model the origonal conditional probabilities without having that kind of Markov assumption.",
                    "label": 1
                },
                {
                    "sent": "Now the issue here is that the our input to the neural language model is going to have a different length, so any words any conditional probability or the distribution of the words in the later part of the sentence will have a context that is very long.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have a very many symbols that the neural language model needs to take in, whereas if you consider, let's say the symbol in the earlier stage of the sentence is going to have a very small of them.",
                    "label": 0
                },
                {
                    "sent": "So one way to deal with it is to simply set that.",
                    "label": 0
                },
                {
                    "sent": "Say that the OK our maximum number of context symbols is going to be gigantic 3264, and then whenever we have less than that, we're going to just pad it with all zeros or the symbol saying that OK, those are non symbols.",
                    "label": 0
                },
                {
                    "sent": "But there's slightly less satisfying.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're going to think of it from the computer science perspective, so if you took, let's say, computer science 101, first thing you learn is that the OK. How do you make the recursive function and the recursive function can handle a very long sequence with a single function that applies over and over to the symbol and the state of the recursive function itself.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes, so we're going to let's say resorts of recursive construction of the neural net by defining first the memory state or the hidden state.",
                    "label": 1
                },
                {
                    "sent": "That's going to be set to all zeros and then we'll have a function that applies to the one of the symbols and the current hidden state or memory state of the function, and we apply this recursion over and over until the sentence finishes.",
                    "label": 0
                },
                {
                    "sent": "Oh, you did?",
                    "label": 0
                },
                {
                    "sent": "OK, that's actually great.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what it means?",
                    "label": 0
                },
                {
                    "sent": "Because now you all know already RNS.",
                    "label": 0
                },
                {
                    "sent": "What we're doing with this kind of conditional distribution is that we're going to apply this recurrent neural net activation function starting from the first word in the context, and then get update the memory state and based on that memory state, we're going to run that recurrent activation function again with the second word, and so on until they all the context has been consumed.",
                    "label": 0
                },
                {
                    "sent": "Now at that point, what we get is a memory, so the memory state of the recurrent recursive function should summarize.",
                    "label": 0
                },
                {
                    "sent": "What the context is and based on this summary, we're going to compute the probability of the next word.",
                    "label": 0
                },
                {
                    "sent": "And clearly it's going to work for any number of context words, and using this kind of idea by replacing the function F with the parametric neuron.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we get the recurrent neural net language model.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recursion.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The language model is very straightforward idea, so we had that idea of the recursive construction of a function that's going to read any number of context symbols.",
                    "label": 0
                },
                {
                    "sent": "We're going to apply that to the full sequence.",
                    "label": 0
                },
                {
                    "sent": "One word at a time, and as we apply that function, we get the summary of the context symbols so far, and based on which we compute the probability of the next word.",
                    "label": 0
                },
                {
                    "sent": "And we do that over and over and all we need to do at the end of the day is to multiply the probability you get at every time step in order to get the probability of the full sequence.",
                    "label": 0
                },
                {
                    "sent": "So we let the model read the word.",
                    "label": 0
                },
                {
                    "sent": "An update is hidden state and based on that we let the model predict what the probability of the next word is and we continue doing on and then at the end of the sentence we get the probability of the full sentence right away.",
                    "label": 0
                },
                {
                    "sent": "And if I write it like this, that doesn't really look intuitive, but if you.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Throw it in a graphical way.",
                    "label": 0
                },
                {
                    "sent": "Then it becomes suddenly so much easier.",
                    "label": 0
                },
                {
                    "sent": "So we we have an initial value of the memory state of the recurrent neural net from which we compute the probability of the very first word.",
                    "label": 1
                },
                {
                    "sent": "And next time we're going to read the first word.",
                    "label": 0
                },
                {
                    "sent": "The update is memory state, predicted probability of the next word and we continue doing so until the end of the sentence and we just multiply all the probabilities we have gotten and to get the probability of a given sentence.",
                    "label": 0
                },
                {
                    "sent": "So we do read, update and predict, read, update, and predict until we predict that last end of the sentence symbol or the last punctuation mark.",
                    "label": 1
                },
                {
                    "sent": "So is it super clear?",
                    "label": 0
                },
                {
                    "sent": "Right now, OK, probably not.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Probability.",
                    "label": 0
                },
                {
                    "sent": "Moving.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Sorry, so the probability of East.",
                    "label": 0
                },
                {
                    "sent": "Knowing that.",
                    "label": 0
                },
                {
                    "sent": "Instead of cats, or is it really the world pass?",
                    "label": 0
                },
                {
                    "sent": "Another page, one H1?",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, the H1 is to bring the Oh yes yes.",
                    "label": 1
                },
                {
                    "sent": "So if you if you view those H is as random variables that will be the right way to go.",
                    "label": 0
                },
                {
                    "sent": "But at this moment these H is are just simple placeholder for the output or the computation done by the F multiple times.",
                    "label": 1
                },
                {
                    "sent": "So H is not really the random variable, is just a discrete function.",
                    "label": 0
                },
                {
                    "sent": "So effectively by using H1 and CAD what we get is the conditional probability of East.",
                    "label": 0
                },
                {
                    "sent": "That depends on dog cat, not only the H1.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "What, why should it be less reliable?",
                    "label": 0
                },
                {
                    "sent": "What do you mean by less reliable?",
                    "label": 0
                },
                {
                    "sent": "More and more.",
                    "label": 0
                },
                {
                    "sent": "Right, so that question actually, you know, like has a lot of different, let's say parts in it, but essentially the one thing that we cannot really say for sure whether it is going to be.",
                    "label": 0
                },
                {
                    "sent": "Less reliable estimate as we move on to the later parts of the sentence is difficult to tell because of at least two reasons.",
                    "label": 0
                },
                {
                    "sent": "First reason actually heavily depends on the actual data, so it's likely that the, let's say, every single sentence finishes with some kind of punctuation mark, let's say.",
                    "label": 0
                },
                {
                    "sent": "Then you know predicting the punctuation mark is going to be actually pretty straightforward, and that's going to be very reliable estimate.",
                    "label": 0
                },
                {
                    "sent": "And then we can think of it.",
                    "label": 0
                },
                {
                    "sent": "We can generalize it into other concepts, so there may be a construction where the verbs always come at the end of a sentence, like in Korean and German.",
                    "label": 1
                },
                {
                    "sent": "German I guess, but yeah in that case and then let's say there are very frequent verbs, many frequent words and then predicting those frequent verbs is going to be actually more reliable by looking at the very first first symbol, which is going to be subject right.",
                    "label": 0
                },
                {
                    "sent": "So it actually depends heavily on the structure of it, but if we do not assume any kind of those, this kind of recurrent language model does have an inductive bias that prefers the better estimate of the earlier parts.",
                    "label": 0
                },
                {
                    "sent": "But of course you know like that one is kind of very general statement.",
                    "label": 0
                },
                {
                    "sent": "That may not work well, depending on what kind of underlying structure in the training corpus you see.",
                    "label": 0
                },
                {
                    "sent": "Alright, so then I'll continue.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Continue by OK constructing the recurrent neural net.",
                    "label": 1
                },
                {
                    "sent": "Which yeah sure did, but because this is super important, I'll just try to quickly recap one more time.",
                    "label": 0
                },
                {
                    "sent": "So now input again is going to be symbols that are represented as a Walnut vectors or one of K coded vectors, and in this case now the each time we're going to look at single time step at a time, because the same function is going to be applied over and over.",
                    "label": 0
                },
                {
                    "sent": "So at time step T we're going to consider a symbol at time step T -- 1, which is going to be.",
                    "label": 0
                },
                {
                    "sent": "Vector that corresponds to the symbol at T -- 1 step and we have the hidden state or the memory state of the recurrent neural net from the previous time step T -- 1.",
                    "label": 0
                },
                {
                    "sent": "So we have X T -- 1 H, T -- 1 parameters will be the input way matrix.",
                    "label": 0
                },
                {
                    "sent": "Which is often called word embedding.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the visual logic 2D visualization of that way matrix and we have transition weight matrix which is going to be a square matrix that is used to transform the previous hidden state and have a bias vector.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we let us define a knife transition function, which is simply going to be the affine transformation of concatenation of the input vectors, which is the T -- 1 symbol and the previous city and state, followed by a elementwise hyperbolic tangent function.",
                    "label": 1
                },
                {
                    "sent": "Now we get the continuous state representation, so we read it, and then we update the hidden state of the recurrent neural net and based recurrent neural net to get the new hidden state, and we apply the pointwise nonlinear transformation.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, because we have the hidden state that summarizes all the context symbols up to the T -- 1 symbol, what we're going to do is simply use the very same trick from the neural language model or the feedforward language model where we transform the current state of the recurrent neural net.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two, the probability distribution over the all the all possible symbols in the next time step, which is going to be simply defined.",
                    "label": 0
                },
                {
                    "sent": "Transformation of HT minus one into a vector that has as many elements as there are symbols in the vocabulary and we use the softmax.",
                    "label": 0
                },
                {
                    "sent": "Normalization to make them positive and sum to one.",
                    "label": 0
                },
                {
                    "sent": "And we get the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Now, this part is exactly same as the usual feedforward neural language model and we see that the only thing that really changes from the feed.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In your language model, is that by introducing this kind of recurrent connection, that's going to be used to read as many symbols as possible.",
                    "label": 0
                },
                {
                    "sent": "We have ability now to handle variable length input and then tries to compute the probability of the next word given the potentially infinite infinitely long context work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We built it and then I'm pretty sure you learned it already, but again, this is really important, so I'll go over it one more.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "How do we train the language model is to maximize the making sure that the model is going to put as high probability as possible to training sentences or likely sentences.",
                    "label": 0
                },
                {
                    "sent": "And we can write it down as the maximizing the likelihood of the parameters given the training sentences.",
                    "label": 0
                },
                {
                    "sent": "So we assume that there are N sentences given, and each sentence is a sequence with a variable number of symbols, and we can write, write the log likelihood function as the sum over the all the training sentences sum over some over the all the training sentences.",
                    "label": 0
                },
                {
                    "sent": "And then we sum the low probability of each sentence.",
                    "label": 0
                },
                {
                    "sent": "Given by the current model, so effectively what we're saying is that the OK.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize the probability or the low probability assigned by the model to every single training sentence we have available.",
                    "label": 0
                },
                {
                    "sent": "And because it's the kind of optimization problem, usually people just write it as a negative log likelihood and say that you minimize it.",
                    "label": 0
                },
                {
                    "sent": "But it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "Just put the negative sign or not.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we usually use the mini batch stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "Really nothing changes from the feed forward neural net.",
                    "label": 1
                },
                {
                    "sent": "We randomly select the mini batch of very small number of training sentences and then compute the gradient or estimate the gradient of per sample costs with respect to the theater for each one of them in the mini batch and we just average them to get the mini batch gradient.",
                    "label": 0
                },
                {
                    "sent": "And we follow the direction of that mini batch gradient in order to maximize the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Or if you follow the opposite direction of the gradient, you're going to minimize the negative log likelihood and you repeat it until the convergence.",
                    "label": 0
                },
                {
                    "sent": "But of course when I say convergence convergence in terms of the performance on the validation sentences that were not used on the training used to compute the gradient.",
                    "label": 0
                },
                {
                    "sent": "Now this should be pretty clear and then we stop and then we call it all this stopping.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how do we compute?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The gradient of the cost function with respect to the parameters in this kind of recurrent neural net language model.",
                    "label": 0
                },
                {
                    "sent": "And as you have already learned, we use the algorithm called backpropagation through time, which is 2.",
                    "label": 1
                },
                {
                    "sent": "We're going to enforce the recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "That had a loop in time.",
                    "label": 0
                },
                {
                    "sent": "So given the sentence we are going to say that we have a very deep recurrent neural net very difficult for neural net that has as many hidden layers as there are symbols in the given sentence.",
                    "label": 0
                },
                {
                    "sent": "Now this shows a single step of that unfolding, and you can see that the starting from the very beginning we're going to have a very similar nonlinear layer here and then that layers activation is going to be fed into the nonlinear layer at the next time step, and so on until the end of the sentence symbol.",
                    "label": 0
                },
                {
                    "sent": "The probability of the end of sentence symbol is predicted.",
                    "label": 0
                },
                {
                    "sent": "And once you unroll this recurrent neural net language model overtime, what you see there is just a fit for a neural net.",
                    "label": 0
                },
                {
                    "sent": "And then with the feed for neural net, what we can do, we can simply use the piano tutorial.",
                    "label": 0
                },
                {
                    "sent": "As well, so you do the import Theano that tensor and Theano tensor that grad and then cost function.",
                    "label": 0
                },
                {
                    "sent": "And then it's going to compute the gradient of the cost function with respect to all the parameters automatically.",
                    "label": 0
                },
                {
                    "sent": "But one thing that differs slightly is that the now, unlike the usual feedforward net, we build, the parameters are all shared across the layers.",
                    "label": 0
                },
                {
                    "sent": "Now in I don't know why.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry OK OK yes.",
                    "label": 0
                },
                {
                    "sent": "So what I mean is that the essentially when you do the backpropagation through time, as in, you're going to compute the gradient of the of the cost function with respect to the parameters that let's say time step T, and then we're going to send it back to the T -- 1, send it back to T -- 2 T minus three.",
                    "label": 0
                },
                {
                    "sent": "And as you do the back propagation, what you need to do is.",
                    "label": 0
                },
                {
                    "sent": "Accumulate that derivative of the cost function with respect to the parameters at each time step into a shared story storage.",
                    "label": 0
                },
                {
                    "sent": "Because we share the parameters, we only get the gradient of the cost function with respect to each parameter only once.",
                    "label": 0
                },
                {
                    "sent": "And in order to compute that, we compute the derivative at each time step separately.",
                    "label": 0
                },
                {
                    "sent": "But as you compute it, you're going to sum them up in order to get the correct derivative.",
                    "label": 0
                },
                {
                    "sent": "And once we come all the way to the first symbol, we have now accumulated the gradient of every time step, and that is going to be the gradient of the full cost function, which is going to be this sum of all these low probabilities with respect to the parameters, and we use that to update the parameters stored.",
                    "label": 0
                },
                {
                    "sent": "Maximizing the low probability likelihood or to minimize the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Is this like super clear, right?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll continue on and the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get over issue with this kind of neural net, or they're doing the backpropagation through time.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the naive transition function.",
                    "label": 0
                },
                {
                    "sent": "Is that the is super difficult to train the model in this way without a lot of let's say trickery is there?",
                    "label": 0
                },
                {
                    "sent": "Now let's see what actually happens and then what we are trying to compute.",
                    "label": 0
                },
                {
                    "sent": "When we do the backpropagation through time.",
                    "label": 0
                },
                {
                    "sent": "So when we compute the derivative, what we see is that effectively at every time step we're going to compute the derivative of the cost function or the per step cost function in the future with respect to the current time steps hidden activation.",
                    "label": 0
                },
                {
                    "sent": "So JT plus North.",
                    "label": 0
                },
                {
                    "sent": "Is going to be the cost function cost coming from the T plus end step and HT is the current activation at time step T, so we tried to figure out what kind of influence the cost is going to have.",
                    "label": 0
                },
                {
                    "sent": "If we perturb or if we change a bit at time step T like in the past.",
                    "label": 0
                },
                {
                    "sent": "So if we change something Now, what kind of influence is it going to be?",
                    "label": 0
                },
                {
                    "sent": "Is it going to have in the future?",
                    "label": 0
                },
                {
                    "sent": "And by computing that we can essentially adjust the parameters to make sure that the this adjustment is going to have an influence in the future to minimize the cost or the maximizer look like clear.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this kind of temporal derivative so the derivative of the.",
                    "label": 0
                },
                {
                    "sent": "Next time steps activation, given the current time steps activation, we see that there is a transition matrix coming out of the nonlinear function we had, which was the hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Following the affine transformation of the input, and if we look at the long time step.",
                    "label": 0
                },
                {
                    "sent": "So if you rewrite the full influence derivative, then we see that we're going to multiply those transition matrix over and over with each other as many times as possible into the future.",
                    "label": 0
                },
                {
                    "sent": "And that brings into.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit of an issue where the normal of the derivative.",
                    "label": 0
                },
                {
                    "sent": "Is going to either shrink or explode depending on the configuration of the transition matrix, if the.",
                    "label": 0
                },
                {
                    "sent": "If we assume that if we use the hyperbolic tangents if the largest eigenvalue of the transition matrix is smaller than one is very likely that the norm is going to shrink to 0.",
                    "label": 0
                },
                {
                    "sent": "Because if you compute, let's say, zero point 900 times, you're going to effectively get 0.",
                    "label": 0
                },
                {
                    "sent": "But if the largest eigenvalue is going to be larger than one, it's very likely that it's going to explode.",
                    "label": 0
                },
                {
                    "sent": "As in, if you multiply 1.100 times, then you're going to get a very large number.",
                    "label": 0
                },
                {
                    "sent": "Which means that in either case there is going to be effectively no learning signal that we can use to tune the parameters so as to maximize the low probability of a given training sentence.",
                    "label": 0
                },
                {
                    "sent": "But turned out.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note that the exploding gradient, so when the norm of the gradient is exposed to the Infinity is not really a problem.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day.",
                    "label": 0
                },
                {
                    "sent": "What people have figured out, including past canoan L from 2013 who are here as a PhD student and then who is now at the gold in mind.",
                    "label": 0
                },
                {
                    "sent": "What he and Joshua and Thomas Michael showed is that you can simply.",
                    "label": 0
                },
                {
                    "sent": "Look at the norm of the gradient and then if the norm is too large, as in if it's larger than one or five, then you're going to simply shrink the magnitude to that threshold, yes?",
                    "label": 0
                },
                {
                    "sent": "Oh, you did?",
                    "label": 0
                },
                {
                    "sent": "OK, that's great.",
                    "label": 0
                },
                {
                    "sent": "Yes, did you talk about the STM as well, OK?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you learned almost everything alright, but that's good because I was thinking that you.",
                    "label": 0
                },
                {
                    "sent": "Spend a lot of time alright, yes, so yes.",
                    "label": 0
                },
                {
                    "sent": "So you know like you know about the vanishing gradient exploring gradient and then those are problems right now.",
                    "label": 0
                },
                {
                    "sent": "Let's think about the exploding gradient turned out to be a bit easy to address, but the vanishing gradient is a bit problematic.",
                    "label": 0
                },
                {
                    "sent": "There were some solutions proposed earlier, but now let's think about why that happens and why that actually calls very naturally.",
                    "label": 0
                },
                {
                    "sent": "Calls for the LTM or the gated recurrent units.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we look at the gradient vanishing of the gradient, what we see is that.",
                    "label": 1
                },
                {
                    "sent": "Gradient vanish is because when you do the back propagation, it has to go through every single time step.",
                    "label": 1
                },
                {
                    "sent": "So as the error is back, propagated is going to be multiplied by the transition rate matrix at every time step and that is the part where the normal of the error derivative is going to shrink.",
                    "label": 0
                },
                {
                    "sent": "Now then, you know the natural like the solution is to.",
                    "label": 0
                },
                {
                    "sent": "OK, what if we can send the aeroderivative?",
                    "label": 0
                },
                {
                    "sent": "By passing a lot of steps right away, that will be the solution, because if we just go from here all the way to the time step T without going through all these nodes effectively, we don't have to do the multiplication of the transition matrix and we can just send it right away without sacrificing the without banishing the norm of the error derivative.",
                    "label": 0
                },
                {
                    "sent": "So we can think of adding the temporal shortcut connections, so we're going to Add all those.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Short connections that bypasses some number of time steps and then when we have those connections the error derivatives will be backpropagated through these connections, bypassing all those steps addressing or the avoiding.",
                    "label": 1
                },
                {
                    "sent": "The issue with the vanishing gradient.",
                    "label": 0
                },
                {
                    "sent": "But the obvious issue is that we cannot really put all those shortcut connections as many as we can because first of all, the number of parameters will explode and as soon as we fix the number of the shortcuts that we're going to add, we effectively put the bound on the.",
                    "label": 0
                },
                {
                    "sent": "What is the longest context?",
                    "label": 0
                },
                {
                    "sent": "Words are context length.",
                    "label": 0
                },
                {
                    "sent": "We're going to handle what is the longest context we're going to handle, so we're going to instead applied.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea of the leaky integration and that leaky integration is going to be based on the adaptive coefficients, so we're going to say that the at every time step the deactivation or the new memory state value is going to be the combination or the convex some of the previous activation and the current candidate activation.",
                    "label": 0
                },
                {
                    "sent": "And how do we combine them that is based on what the input says, so we're going to let the neural net this size.",
                    "label": 0
                },
                {
                    "sent": "So based on the current input.",
                    "label": 0
                },
                {
                    "sent": "And the previous hidden activation based on those neural net is going to automatically decide how much of the previous activation is going to be carried over, effectively creating a shortcut or how much is going to be replaced with a new value.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be this adaptively integration.",
                    "label": 0
                },
                {
                    "sent": "And now once we do that, we get all those shortcut connections that are adaptively created on the fly based on the input.",
                    "label": 0
                },
                {
                    "sent": "So that's great.",
                    "label": 0
                },
                {
                    "sent": "But what happens is that that can have a potential downside of.",
                    "label": 0
                },
                {
                    "sent": "Diluting the credit assignment process of the back propagation.",
                    "label": 0
                },
                {
                    "sent": "So what backpropagation is trying to do is to figure out which hidden neuron at which time step is most responsible for the cost function at in the future, and then if there are so many paths, then the back propagation gets, let's say confused and then it tries to, let's say, sends out the signal in a very smooth way.",
                    "label": 0
                },
                {
                    "sent": "So instead what we want to do is that we will.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be able to let them network prune automatically.",
                    "label": 0
                },
                {
                    "sent": "The unnecessary connections and that is by introducing a reset gate, which again is based on the input and previous activation.",
                    "label": 0
                },
                {
                    "sent": "The neural net is going to automatically decide whether we're going to prune out certain connections by masking out the influence from the previous activation.",
                    "label": 0
                },
                {
                    "sent": "And combining this reset and update gate together with the adaptive link integration, what?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is a gated recurrent unit.",
                    "label": 1
                },
                {
                    "sent": "And you can see that this one is going to have a less problem with the vanishing gradient because we create a shortcut on the fly as needed and the aeroderivatives can bypass multiple steps if the network has deemed that that is needed.",
                    "label": 1
                },
                {
                    "sent": "And this gated recurrent unit is essentially a variance, a simpler variance of the long short term.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory units which are proposed already in the 1999 and 2001 by the group in Switzerland are led by the Organism it over and the idea is very similar.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you want to make sure that the network is able to carry over the activation forward in time without overriding it, thereby making the aeroderivatives to be able to bypass many of the steps and that effectively avoids the issue with the vanishing gradient.",
                    "label": 0
                },
                {
                    "sent": "So I'm pretty sure this is the second time you heard, so you're like completely.",
                    "label": 0
                },
                {
                    "sent": "Completely like let's say sure about all these things, right?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "While you were that.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "All the way to 220.",
                    "label": 0
                },
                {
                    "sent": "Grease the gradients also exponentially, because essentially you would have if you will.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, that's the thing that I didn't mention is that that is precisely the reason why we want to make it adaptive.",
                    "label": 0
                },
                {
                    "sent": "So The thing is that what we want is that the not, too, let's say, make sure that the gradient is not going to vanish because gradient might be banishing because there is no such signal in the data itself.",
                    "label": 0
                },
                {
                    "sent": "So we make sure that the unit is going to be close to either one or zero.",
                    "label": 0
                },
                {
                    "sent": "If you if you set the UTI to certain value ourselves and then use the constant, what happens is exactly what you said is going to always vanish, whereas UTI can be either close to zero or one and then when it's close to one is effectively creating a very good shortcut and then when it's zero is going to just ignore it.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't of course solve the finishing radiation perfectly.",
                    "label": 0
                },
                {
                    "sent": "I'm not even sure if that is even possible by the definition.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But yes, OK, now finally to the machine translation that actually I do really like.",
                    "label": 0
                },
                {
                    "sent": "OK it took a long time to come here.",
                    "label": 0
                },
                {
                    "sent": "Now, one thing that I want to talk about a bit of the recurrent language model is that you know we learned how to generate.",
                    "label": 0
                },
                {
                    "sent": "We observe that we can generate an amazing sentence out of the trained recurrent language.",
                    "label": 0
                },
                {
                    "sent": "Neural net language model, which was not really true with the N gram language model or any of the Markovian language model, whereas without any Markov Markov assumption and with the power of those gated recurrent units or the LCMS effectively, what we have now is a language model that.",
                    "label": 0
                },
                {
                    "sent": "Can not only score a given sentence, but to generate a sentence that is likely under the trained models distribution and we're going to use that idea in a slightly more interest.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In application of the machine translation.",
                    "label": 0
                },
                {
                    "sent": "So what is machine translation?",
                    "label": 0
                },
                {
                    "sent": "Machine translation is infected, probably one of the most elegant problem you can find where your goal is to try to maximize the probability of a correct translation given a source sentence.",
                    "label": 0
                },
                {
                    "sent": "It's like the very less a straightforward application of simple supervised learning, and then that probability can be decomposed.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, using the base rule you can rewrite it as the probability of going back so they're doing the back translation.",
                    "label": 0
                },
                {
                    "sent": "So what is the translation probability of the source sentence given the correct translation and the probability of the correct translation itself?",
                    "label": 0
                },
                {
                    "sent": "And why is it really amazing?",
                    "label": 0
                },
                {
                    "sent": "Is that we can essentially train 1/2 of the model?",
                    "label": 0
                },
                {
                    "sent": "Using the parallel corpus where we have these set of sentence pairs, where each pair consists of the source sentence, Ann is correct translation and the other half with the monolingual corpus, as in like unsupervised data, right?",
                    "label": 0
                },
                {
                    "sent": "So you have no annotation, you just saw the text and we can use that to tune this model as well as possible using as much resources you have.",
                    "label": 0
                },
                {
                    "sent": "And then you just maximize the probability.",
                    "label": 0
                },
                {
                    "sent": "So this is a very elegant.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Except that in reality it has not been that elegant because first of all we don't know one thing we're sure is that from the source sentence to his translation is going to be super nonlinear.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that we didn't really know how to tune those.",
                    "label": 0
                },
                {
                    "sent": "Let's say no nonlinear models in a very large scale data, So what people have resorted to is to make a lot of feature functions.",
                    "label": 0
                },
                {
                    "sent": "So which words correspond to which word in the translation you know, like how they are distorted, how they're ordered, all those different features, and then put it into the log linear model.",
                    "label": 0
                },
                {
                    "sent": "So it's effectively a linear model.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that the they ignore the normalization constant, thereby they just kind of ignored the whole, let's say.",
                    "label": 0
                },
                {
                    "sent": "Elegance of the probabilistic view of the problem itself.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that because this model is so weak, if you get the translations, you get a lot of garbage, so you need to have a way to filter them out.",
                    "label": 0
                },
                {
                    "sent": "And of course filtering we know how to do it right.",
                    "label": 0
                },
                {
                    "sent": "We already learned I spent like an hour an hour to talk about how to select a very likely sentence, and then you know people and we can use the very strong external, let's say neural language model or RNN language model or N gram language model.",
                    "label": 0
                },
                {
                    "sent": "And that has.",
                    "label": 0
                },
                {
                    "sent": "This has been the kind of usual way.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turned out because we know how to train recurrent language model now and learn how to use the recurrent neural net to summarize a input that is of the variable length we can.",
                    "label": 1
                },
                {
                    "sent": "Construct a single model that's going to maximize the probability of the correct translation given the source sentence.",
                    "label": 0
                },
                {
                    "sent": "As is this instead of going into all those feature engineering with a very weak model followed by a strong language model.",
                    "label": 0
                },
                {
                    "sent": "And is there a new thing?",
                    "label": 0
                },
                {
                    "sent": "It's not really a new thing, so apparently in 1991, in fact, before the year before that, in 1989 and 90 and 91 people at CMU and some other places have proposed a very same model to do the machine translation.",
                    "label": 1
                },
                {
                    "sent": "And of course they had a data set of about 2000 to 3000 sentence pairs.",
                    "label": 0
                },
                {
                    "sent": "So it was impressive back then, but it was not impressive enough.",
                    "label": 0
                },
                {
                    "sent": "And then in 1997 the idea was revived in Spain where they wanted to build essentially this model.",
                    "label": 0
                },
                {
                    "sent": "Which has a recurrent neural net encoder.",
                    "label": 1
                },
                {
                    "sent": "The reason source sentence one similar time summarize it into the hidden vector and based on that hidden vector, we're going to have another record year in the language model that's going to generate one symbol error time.",
                    "label": 0
                },
                {
                    "sent": "How can we generate it?",
                    "label": 0
                },
                {
                    "sent": "Because we compute the distribution, we simply sample one at a time and this idea was revived in 1997.",
                    "label": 0
                },
                {
                    "sent": "But that idea kind of went away because they just realize that they don't have enough computing resource nor data set to play around with and only in 2013 and 14 the group at Oxford.",
                    "label": 0
                },
                {
                    "sent": "Group at Google and group.",
                    "label": 0
                },
                {
                    "sent": "Here in Montreal we have developed and proposed to use this and now we have all those GPU's and we know how to train this recurrent neural net using LST emoji Arias and it turned out that we can in this train a single model that's going to approximate the correct conditional distribution of the translations given the source sentence.",
                    "label": 0
                },
                {
                    "sent": "Now I will.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to go into this detail, but I'm pretty sure that the Joshua Sumet and add who is going to give a lecture later on.",
                    "label": 0
                },
                {
                    "sent": "I have covered a lot of things so I want to spend the next 10 minutes, which is just the last 10 minutes to talk about what this neural net community or the techniques from the neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Is bringing to the NLP because I just realized that the lecture title is deep NLP one and I wanted to say that the OK how this deep learning is changing the NLP so?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pasta machine.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station I have a blog.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can read about it so.",
                    "label": 0
                },
                {
                    "sent": "So what is deep natural language processing?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about what my view is, and then you know, like how we should approach NLP, which is again going to be a bit different from what Ed is going to say, so.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st is that if we can actually forget about words.",
                    "label": 0
                },
                {
                    "sent": "Because neural Nets don't really care about words.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the machine translation system or a lot of the systems until like January this year.",
                    "label": 0
                },
                {
                    "sent": "We're using words.",
                    "label": 0
                },
                {
                    "sent": "And this model has a very little structure in it, but when I see it, you know like I see that the there are just so much explicit structure at the very very roll over the input and output level.",
                    "label": 0
                },
                {
                    "sent": "Why are we going into a sequence of words when the neural net doesn't really know whether there's a worse characters phrases, it only sees the one at vectors.",
                    "label": 0
                },
                {
                    "sent": "It's a sequence of one of vectors.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a lot of people with.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Oh OK yes.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are legitimate reasons why you know you.",
                    "label": 0
                },
                {
                    "sent": "Probably you might want to use the words.",
                    "label": 0
                },
                {
                    "sent": "The first thing is that you know we kind of have a belief that the world is a basic unit of meaning, which is a bit debatable.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is that we have an inherent fear of data sparsity, and data sparsity gets worse and worse as the length of the sequence gets longer and longer because the state space grows exponentially large with respect to the length.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that if you put it into the sequence of characters instead of sequence of words, suddenly the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Cause growth spiral 5 to sixfold.",
                    "label": 0
                },
                {
                    "sent": "In the case of English and 3rd, we are worried that we cannot really train the recurrent neural net on those long sequences.",
                    "label": 0
                },
                {
                    "sent": "We were right.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turned out that the these techniques from the deep learning in the natural language processing effectively has address those two most important issues, reasons that we were working with the words first Yoshua Bengio already in 2003 and 2000 showed that we don't really have to fear the data sparsity.",
                    "label": 0
                },
                {
                    "sent": "If you go into the continuous representations.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then starting from the.",
                    "label": 0
                },
                {
                    "sent": "Yeah 2011, we have already learned that in terms of language modeling we can train a recurrent neural net language model on the characters and then can generate an amazing text that is just one character at a time generated by the recurrent neural net language model.",
                    "label": 0
                },
                {
                    "sent": "And 3rd.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't have to worry about training recurrent neural net, unlike in 1994 when your shows is that yeah, it's a very difficult problem, but thanks to the gated recurrent unit in LCMS, which we learned about it today and yesterday we know that we can up to certain point, which is actually very long.",
                    "label": 0
                },
                {
                    "sent": "We can train a recurrent neural net language model very nicely without too much issue, as long as you have some patients.",
                    "label": 0
                },
                {
                    "sent": "Now what?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means is that the in this new territory of deep NLP we don't have to be bound by the constraints of the having to have a sequence of words, which is a big constraint, especially if you go beyond English.",
                    "label": 0
                },
                {
                    "sent": "If you go into the, say, Chinese, you don't have blank space that you can use to separate the sequence in towards, you have to run certain other algorithm to get a sequence of words and then you know who knows how optimal that is.",
                    "label": 0
                },
                {
                    "sent": "Then maybe very suboptimal.",
                    "label": 0
                },
                {
                    "sent": "And then it turned out.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the.",
                    "label": 0
                },
                {
                    "sent": "Some of the languages, such as Arabic, has a very interesting morphology that you see one word here that it corresponds to N. 2 his vehicle, and it's not even clear whether we can segment it out clearly into the separate pits.",
                    "label": 0
                },
                {
                    "sent": "And there are few more weird languages where you know you get very interesting phenomena of the compounds of the 8, two, 12 words that is combined into a single word, such as in finish.",
                    "label": 0
                },
                {
                    "sent": "And then as Joshua said earlier, what happens is that the word based models are going to assign the same amount of parameters to a word.",
                    "label": 0
                },
                {
                    "sent": "That is, that means 3000 kilowatt per minute an hour and just three.",
                    "label": 0
                },
                {
                    "sent": "So it's going to spend the same amount of capacity on these two.",
                    "label": 0
                },
                {
                    "sent": "Tokens and that just doesn't make any sense, whereas.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're learning is that the we don't have to do it anymore.",
                    "label": 0
                },
                {
                    "sent": "In deep NLP, we're going to go into the part where we're going to just go directly into the characters, just like we have done in computer vision where we just went all the way down to the pixels.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll just plug in the characters.",
                    "label": 0
                },
                {
                    "sent": "Instead of a word and then based on that, the based on the recurrent neural Nets processing of the character sequence, we're going to consider the output of that as a vector that corresponds to word, and we're going to let the next level recurrent neural net to read the vectors that are result of the lower level recurrent neural net and it turned out that it actually just works.",
                    "label": 0
                },
                {
                    "sent": "A lot of reason works by from the Stanford Google, Deep Mind, CMU, NYU, you and all these places are pointing to the fact that the.",
                    "label": 0
                },
                {
                    "sent": "In fact, with this deep learning now, NLP gets slightly easier because we don't have to do the segmentation in advance.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, I'm running out of time.",
                    "label": 0
                },
                {
                    "sent": "I had some.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anything to say?",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Furthermore, what we have observed is that we don't even have to have a hierarchical structure, and we can actually go directly into the character level sequence with just a number of layers without having an explicit separation.",
                    "label": 0
                },
                {
                    "sent": "We just stack the recurrent neural Nets number of them and then let it figure out how to segment it automatically inside the network by consuming a sequence of character without any kind of separation symbol.",
                    "label": 0
                },
                {
                    "sent": "Saying that, hey, this is one word, there is another word.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you do that, the new recurrent.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That figures out how to say.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operated automatically, so this is an example of machine translation system with the attention mechanism you learned from Summit yesterday.",
                    "label": 0
                },
                {
                    "sent": "So the source sentence is given in a word.",
                    "label": 0
                },
                {
                    "sent": "And target this is the generated translation in German that was generated one character at a time and we see the alignment here and we see that the recurrent neural net has implicitly learned how to learn the learn to segment the character sequence into words like units.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be and then correctly align to the corresponding words in the source side English.",
                    "label": 0
                },
                {
                    "sent": "So it seems like we have a amazing tool that even can learn implicitly the structure of this segmentation automatically from the sequence of characters, just like what the computer vision has figured out four years ago that you can just go into the pixel level and then convolutional neural net is going to figure out what kind of objects are there, and then it's going to do the implicit segmentation.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly happening in NLP as well.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then second thing I wanted to want to say is that in the deep NLP now we can do the multi lingual processing so we can build a single model that's going to handle multiple languages.",
                    "label": 0
                },
                {
                    "sent": "All simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Without any issue.",
                    "label": 0
                },
                {
                    "sent": "Because why is that?",
                    "label": 0
                },
                {
                    "sent": "Because we can go into the continuous space and then in that continuous space all we need is a neural Nets that are going to project different languages.",
                    "label": 0
                },
                {
                    "sent": "So the sentences in different languages into a common point.",
                    "label": 0
                },
                {
                    "sent": "And it turned out that in terms of New York machine translation.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is indeed possible to build a single model that's going to handle, let's say, six different source languages and six different target languages, and we train one model jointly on the data from all those languages simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the model can actually do the translation better.",
                    "label": 0
                },
                {
                    "sent": "Then training, let's say 10 different single pair models while having much smaller number of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ha.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately, I'll have to skip, but.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The person that you want to talk with.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're interested in it is or hand.",
                    "label": 0
                },
                {
                    "sent": "Who is sitting somewhere here?",
                    "label": 0
                },
                {
                    "sent": "Yes there.",
                    "label": 0
                },
                {
                    "sent": "So he has built this awesome multilingual translation system that actually works.",
                    "label": 0
                },
                {
                    "sent": "And then if you're interested, you can talk to him about it.",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last thing I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "I have like 1.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is now the these neural Ness or the continuous vector representing yes.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yes, so I was.",
                    "label": 0
                },
                {
                    "sent": "So these are all different, let's say languages and I wanted to give a fair chance to all the different territories and states that use the same language.",
                    "label": 0
                },
                {
                    "sent": "So you probably all you should recognize that this is a Qu\u00e9bec flit and that stands for French Canadian for English, Mexican for Spanish, and you know, like it's a quite interesting thing to try it out and you know this is Spanish.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I want you to be as fair as possible and promoting diversity and so on, yes.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the last thing that I want to say this is the one thing that everyone is agreeing that you know we should really think now in the NLP community is that the discontinuous piece representation finally gives us a means to go above or beyond the sentences.",
                    "label": 0
                },
                {
                    "sent": "We can make a neural net that can take a look at much larger context without fearing about the data sparsity.",
                    "label": 0
                },
                {
                    "sent": "So we can really build a model that's going to look at the much larger context to solve the problem.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better and then context definitely matters, because context tells us what is a theme or topic of a document or the dialogue.",
                    "label": 0
                },
                {
                    "sent": "An, in practice, it actually tells us precisely or better what the following word is going to be.",
                    "label": 0
                },
                {
                    "sent": "So for instance in this case.",
                    "label": 0
                },
                {
                    "sent": "If we have only this sentence, it's not going to be easy to fill in this empty boxes, but as soon as we have access to the context, it becomes quite trivial to fill in these boxes an with the neural net.",
                    "label": 0
                },
                {
                    "sent": "We can naturally train one model that's going to read the whole context and then tries to model the following sentence in order to fill in the missing gaps and.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already starting from few years back, people have realized it and we're building a model that is doing less eight language modeling where you model the current sentence while explicitly conditioned on the previous sentences, which is going to be context.",
                    "label": 0
                },
                {
                    "sent": "And it turned out that doing that.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guess us always.",
                    "label": 0
                },
                {
                    "sent": "A benefit in terms of the perplexity.",
                    "label": 0
                },
                {
                    "sent": "So perplexity means how predictable next word is, so it directly reflects how good a language model is.",
                    "label": 0
                },
                {
                    "sent": "And as we put more and more context, which is the X axis, we see that the perplexity goes down, which corresponds to a better and better predictability, meaning that we get a better and better language model an.",
                    "label": 0
                },
                {
                    "sent": "How do?",
                    "label": 0
                },
                {
                    "sent": "How do I know that the context actually tells us about the topic and theme?",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turned out that the types of words that gets better modeled or better predicted are the words that are also called open class words, which are nouns, verbs, adjectives.",
                    "label": 0
                },
                {
                    "sent": "Essentially, they are the ones that are going to be influenced most by the topic of the documents, whereas the functional words such as the.",
                    "label": 0
                },
                {
                    "sent": "What we have here?",
                    "label": 0
                },
                {
                    "sent": "OK, so coordinator determiner, preposition.",
                    "label": 0
                },
                {
                    "sent": "Those things are, you know, it has a grammatical function and then top it doesn't really matter and this graph the improvements on these open class words essentially tells us that context given the context, neural net is able to automatically figure out the topic.",
                    "label": 0
                },
                {
                    "sent": "And now this tells.",
                    "label": 0
                },
                {
                    "sent": "This gives us amazing opportunity to do, for instance Q&A in a very.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large word order.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the dialogue mode.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Playing and then eventually making a translation or Q&A or dialogue system that's going to have access to the word knowledge.",
                    "label": 0
                },
                {
                    "sent": "So think about building a neural net where input is going to be the question.",
                    "label": 0
                },
                {
                    "sent": "And the whole Wikipedia or input is going to be the question and whole Internet, and that's going to that.",
                    "label": 0
                },
                {
                    "sent": "Neural net is going to try to answer the question based on the whole world knowledge.",
                    "label": 0
                },
                {
                    "sent": "There is all possibility and this means that the we have so many interesting problems that we can solve in NLP with this deep learning.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said thank you.",
                    "label": 0
                }
            ]
        }
    }
}