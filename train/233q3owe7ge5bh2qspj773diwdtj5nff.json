{
    "id": "233q3owe7ge5bh2qspj773diwdtj5nff",
    "title": "A note on the evaluation of generative models",
    "info": {
        "author": [
            "Lucas Theis, University of T\u00fcbingen"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_theis_generative_models/",
    "segmentation": [
        [
            "I'm going to focus a little bit on image models, but I think much of what I'm going to say should also apply to other modalities such as sound or text."
        ],
        [
            "Just to give you a little bit of context for why we want to write this paper.",
            "There's many different ways that you could evaluate a generative model that you could look at the generative model.",
            "Um?",
            "One of one of the reasons for that is that there are certainly many applications of generative models.",
            "You could use them for compression.",
            "You could use them to generate new content texture synthesis.",
            "You can use them for all kinds of image reconstruction applications in computational photography.",
            "And of course we would also like to use them to learn representations in an unsupervised way for other tasks.",
            "Another reason why there's such a diversity for evaluating Jennifer models is that look likely is often hard to evaluate.",
            "And so people have come up with.",
            "I ternative's and approximations to to the log likelihood.",
            "And maybe a third reason for this diversity is that many people were kind of unsatisfied with the with the progress in general modeling, in particular for unsupervised representation learning, but maybe also for content generation, and so that again they've come up with alternatives to the log likelihood.",
            "But I think the existence of these different ways to value the general model raises an important question, and that is how do these methods?",
            "How do these evaluation methods relate to each other?",
            "For example, how does my ability to generate nice looking samples relate to density estimation, performance, compression, performance, or the quality of the learned representations?",
            "I think this question wasn't asked quite often enough.",
            "Becausw maybe maybe because of this elegant idea that we have in general modeling that you can first fit a giant model and then later worry about applications and that works very well in low dimensions.",
            "But it turns out it doesn't work very well.",
            "In high dimensions.",
            "So I guess."
        ],
        [
            "What we wanted people to take away from our paper is that just because a general model works well for one application doesn't necessarily mean that it works.",
            "Well, in any of these other applications, and so you really have to think about your applications.",
            "And so in the rest of the talk I'm going to.",
            "Look at some of some examples of this where where this can fail and also look at some other metrics that people have come up with to try to evaluate Journal models in a more general general and application independent way.",
            "But"
        ],
        [
            "Let's start at the beginning and talk a talk briefly about training where of course you also have a similar problem of choosing a metric or or an objective function.",
            "And also here can have a big.",
            "This choice can have a big effect on the result."
        ],
        [
            "That you get in the kind of conclusions that you that you draw from your results.",
            "To illustrate this.",
            "Here we have a little toy, example 2 dimensional data by motor and we fit an isotropic Gaussian to this data using various commonly used methods.",
            "In the top right we have maximum likelihood learning, then maximum in discrepancy gains and gender virgins, and adversarial networks, and I think all these methods, have the property that under certain assumptions such as having the right model and in certain limits such as infinite amounts of data.",
            "The data distribution will become optimal under these objective functions, so you might think that it doesn't really matter which of these I choose, as long as my model is flexible enough and have enough data.",
            "In the end I will get the same answer, but of course in practice, especially if we're talking about such complicated distributions as natural images.",
            "For example, the general model can be very far away from the true distribution, and in that case we get quite different tradeoffs using these methods.",
            "So for example, using maximum likelihood learning we get something which has quite large entropy, much larger entropy than using these other methods, and this is actually what you what you want when you want to do compassion, for example, because then you really want to assign positive density to all your data points.",
            "But of course, if you want to generate nice looking samples then this would not be the right approach, because you're going to generate a lot of samples which don't look like data.",
            "Using one of these other methods, we do get samples which look just like the data, but of course this comes at the cost of completely completely ignoring one of the modes, and this wouldn't work very well for compression.",
            "Um?"
        ],
        [
            "Onto evaluating general models.",
            "Perhaps the Aurora?"
        ],
        [
            "Easy and perhaps intuitive way to evaluate kind of models is to simply draw samples from it and to look at it.",
            "And this can be very useful diagnostic tool to develop intuition for model or to see where it could be improved or we find there is also often used to reason about a model density estimation performance or to reason about a model.",
            "Representations that the model has learned and to see why this is problematic, I think it's helpful to keep in mind but that.",
            "Just generating nice looking samples is quite easy.",
            "You just store a bunch of training images and at Test time you draw samples from your from your database of images.",
            "Is a perfectly fine general model, it's.",
            "But of course it hasn't learned anything about objects, even those able to generate objects is of course a very terrible density model.",
            "And even when we just want to generate content new content, then obviously this is also not what we want, so we have to do a bit more work to evaluate general models.",
            "And one thing that.",
            "But many papers have done is to look at nearest neighbors, so here the idea is that we take our sample and we compute the distance to all the images in the training set and then we find the nearest neighbors and we look at them.",
            "And I guess the idea here is that if your model overfits strongly to the data then.",
            "You will get samples which look very similar to the training images.",
            "The."
        ],
        [
            "Problem with this approach is that already very small changes to your images can lead to very different nearest neighbors.",
            "So to illustrate this, we have looked at what happens when you shift images slightly by taking different crops of from cipher training images.",
            "So if we.",
            "Look at the first column in.",
            "For these images, those are slightly shifted versions of the same image and the second column is other nearest neighbors.",
            "The plot the plot on the X axis shows the.",
            "On the shift in pixels, the Y axis OST the Euclidean distance to training images.",
            "So each line corresponds to.",
            "One training image, the Black line corresponds to the reference image.",
            "The corresponding image that the shifted image was based on.",
            "Yes, you can see already for a small shift on only two pixels, we get a completely different nearest neighbor.",
            "Um?",
            "Even though the perceptually, the image has hardly changed, not.",
            "Not only that, but the nearest neighbor looks completely different.",
            "It's not only slightly different, it's not only just a slightly different track, but it's a completely different image, and that is because the.",
            "Euclidean distance doesn't agree with very well with the perception.",
            "Images.",
            "Which is not to say that if we had a perception metric that this test would be useful, but at least with the way that we do it now, the only thing that this test does is basically detect a look up table and as soon as you model slightly more complicated, this test will not find anything.",
            "Um?"
        ],
        [
            "So another thing that people have done is to use passing window estimates where the basic idea is again we draw samples from the model and then we build a tractable model using these samples.",
            "For example, a mixture of Gaussians.",
            "And we evaluate the log likelihood under this tractable model.",
            "One problem with this approach is that.",
            "You know, as an approximation to the log likelihood, this works very poorly.",
            "So here we fit in a Gaussian distribution to small image patches 6 by 6 image patches.",
            "And on the Y axis we have to log likelihood of the Gaussian in the, that's the black line and then the pass window estimate is the red line.",
            "And even though the.",
            "Data is quite low dimensional models.",
            "Model is quite simple and we use a lot of samples.",
            "The approximation is very far away from the true log likelihood.",
            "This would still be fine if you know at least the ranking that pass member estimates assigned to models in somewhere meaningful.",
            "For example, if the ranking assigned bypass window estimates was the same as the ranking assigned by log likelihoods.",
            "That would still be a very very useful measure, but unfortunately.",
            "This is also not the case."
        ],
        [
            "So here we have past midnight estimates for a bunch of models for M nest, and we've also included the passing window estimate for the true distribution where you draw samples from the data distribution.",
            "And as you can see, the.",
            "True distribution performs worse than some other models.",
            "In fact, that K means based model is a very simple model just based on K means, and it performs very very well.",
            "Thinking decision theory, such a measure which assigns you know.",
            "The real distribution poor value is called an improper scoring rule and should be avoided because you're basically optimizing for the wrong thing."
        ],
        [
            "Then it seems that if we if we care about things like look, likely that we really have to evaluate the log likelihoods, or at least properly approximated.",
            "But also there.",
            "But there are some caveats if you, especially if we're talking about fitting densities to discretize data, as we often do, you can get easily infinite log likelihood not only on the training data, but also on the test data.",
            "If your model takes these.",
            "Discretizations in your data, but there's an easy fix to this, and that is to simply add uniform noise to your discretized data.",
            "This fixes it because then they look likely with this bounded by the log likelihood of a discrete model.",
            "So if we define a discrete Model S, this integral over the density.",
            "Then it's easy to show that the log likelihood of your continuous data.",
            "So why is the continuous data with the noise?",
            "Is bounded by the log likelihood.",
            "Of the discrete model on the discrete data, which is in itself bounded by the negative entropy of the discrete data.",
            "And.",
            "This discrete log likelihood is also directly related to compression performance of the discrete model.",
            "For the discrete data.",
            "So if you use.",
            "This kid model for compression of discrete data, then the negative log likelihood directly corresponds to the number of bits that you would use on average.",
            "And that makes this look like a very meaningful becausw.",
            "As soon as you get an improvement in this continuous log likelihood, you know how to better compress the discrete data.",
            "So then we can ask how?"
        ],
        [
            "Plus, how do samples and log likelihood relate?",
            "And.",
            "And it turns out they are quite different features of objective model.",
            "You can see this by looking at this mixture model.",
            "So let's say P is any model that you like.",
            "Could be extremely good model in terms of likelihood, could be the perfect distribution of the data.",
            "And we mix it with some Q where Q could be just white noise for example and we mix it so that 99% of the time we draw white noise instead of images.",
            "Or instead of from our model.",
            "But you can show that the log likelihood of this mixture model still at most 4.61 Nets worse than the log likelihood of P and in high dimensions 4.61 Nets is really not a lot if.",
            "You know?",
            "For cipher block Key will be in the thousands, so 4.61 Nats is really not a lot."
        ],
        [
            "Make this a little more concrete what this means.",
            "What this result means?",
            "Let's look at some examples.",
            "So here we may."
        ],
        [
            "Isotropic here we mix isotopically since Gaussians and look up table.",
            "Um?",
            "With again 1% and 99% weights.",
            "And we sampled from them 100 samples and below the images below the samples to see the compression performance of the of the mixture model.",
            "And if we focus on the 1st row, we see that these mixture models all have almost identical compression performance, but extremely different samples.",
            "And if we focus on the last two columns, we can see that the model can have almost identical samples, but very different compression performance.",
            "So what this means is you can basically mix and match compression performance and samples arbitrarily."
        ],
        [
            "You can make a similar argument for samples applications.",
            "So.",
            "For example, classification, let's, let's let's, let's say classification.",
            "Let's say why is a class label.",
            "Again X is image.",
            "And again we have a mixture over of P&Q so that 99% of the time.",
            "We will draw samples from Q.",
            "And we also assume that the log likelihood of peeing is much larger than the log likelihood of Q, and in that case if lock P is much larger larger than lock you the posterior over Y will be dominated by the posterior over by the posterior of P. So what this means against you can have you can mix and match arbitrary samples with arbitrary classification performance for example.",
            "So too."
        ],
        [
            "Arise based on these observations, we think that it's important to evaluate gender models on the on the right applications on the applications that you care about.",
            "For example, if you care about compression, not likely to, certainly the right measure.",
            "On the other hand, if you want to, if you care about content generation looking at samples, it's fine.",
            "Of course, same goes for an image reconstructions.",
            "There are daily.",
            "We used psychophysics, Hawley, Gerhard, immature speaker have a nice paper on how to evaluate samples from models using psychophysics.",
            "If we, if we care about representation learning, then I think we should really try to use those representations for some task.",
            "And further recommendations based on these observations are try to avoid using parsing window estimates.",
            "Do not rely on nearest neighbor tests for overfitting.",
            "And you know, use samples only where it's relevant for the application.",
            "Or as a diagnostic tool, but not necessarily as a proxy for other tasks.",
            "And that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to focus a little bit on image models, but I think much of what I'm going to say should also apply to other modalities such as sound or text.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to give you a little bit of context for why we want to write this paper.",
                    "label": 0
                },
                {
                    "sent": "There's many different ways that you could evaluate a generative model that you could look at the generative model.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One of one of the reasons for that is that there are certainly many applications of generative models.",
                    "label": 0
                },
                {
                    "sent": "You could use them for compression.",
                    "label": 0
                },
                {
                    "sent": "You could use them to generate new content texture synthesis.",
                    "label": 0
                },
                {
                    "sent": "You can use them for all kinds of image reconstruction applications in computational photography.",
                    "label": 1
                },
                {
                    "sent": "And of course we would also like to use them to learn representations in an unsupervised way for other tasks.",
                    "label": 0
                },
                {
                    "sent": "Another reason why there's such a diversity for evaluating Jennifer models is that look likely is often hard to evaluate.",
                    "label": 0
                },
                {
                    "sent": "And so people have come up with.",
                    "label": 0
                },
                {
                    "sent": "I ternative's and approximations to to the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And maybe a third reason for this diversity is that many people were kind of unsatisfied with the with the progress in general modeling, in particular for unsupervised representation learning, but maybe also for content generation, and so that again they've come up with alternatives to the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "But I think the existence of these different ways to value the general model raises an important question, and that is how do these methods?",
                    "label": 0
                },
                {
                    "sent": "How do these evaluation methods relate to each other?",
                    "label": 0
                },
                {
                    "sent": "For example, how does my ability to generate nice looking samples relate to density estimation, performance, compression, performance, or the quality of the learned representations?",
                    "label": 0
                },
                {
                    "sent": "I think this question wasn't asked quite often enough.",
                    "label": 0
                },
                {
                    "sent": "Becausw maybe maybe because of this elegant idea that we have in general modeling that you can first fit a giant model and then later worry about applications and that works very well in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "But it turns out it doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "In high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So I guess.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we wanted people to take away from our paper is that just because a general model works well for one application doesn't necessarily mean that it works.",
                    "label": 0
                },
                {
                    "sent": "Well, in any of these other applications, and so you really have to think about your applications.",
                    "label": 0
                },
                {
                    "sent": "And so in the rest of the talk I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Look at some of some examples of this where where this can fail and also look at some other metrics that people have come up with to try to evaluate Journal models in a more general general and application independent way.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start at the beginning and talk a talk briefly about training where of course you also have a similar problem of choosing a metric or or an objective function.",
                    "label": 0
                },
                {
                    "sent": "And also here can have a big.",
                    "label": 0
                },
                {
                    "sent": "This choice can have a big effect on the result.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you get in the kind of conclusions that you that you draw from your results.",
                    "label": 0
                },
                {
                    "sent": "To illustrate this.",
                    "label": 0
                },
                {
                    "sent": "Here we have a little toy, example 2 dimensional data by motor and we fit an isotropic Gaussian to this data using various commonly used methods.",
                    "label": 0
                },
                {
                    "sent": "In the top right we have maximum likelihood learning, then maximum in discrepancy gains and gender virgins, and adversarial networks, and I think all these methods, have the property that under certain assumptions such as having the right model and in certain limits such as infinite amounts of data.",
                    "label": 0
                },
                {
                    "sent": "The data distribution will become optimal under these objective functions, so you might think that it doesn't really matter which of these I choose, as long as my model is flexible enough and have enough data.",
                    "label": 0
                },
                {
                    "sent": "In the end I will get the same answer, but of course in practice, especially if we're talking about such complicated distributions as natural images.",
                    "label": 0
                },
                {
                    "sent": "For example, the general model can be very far away from the true distribution, and in that case we get quite different tradeoffs using these methods.",
                    "label": 0
                },
                {
                    "sent": "So for example, using maximum likelihood learning we get something which has quite large entropy, much larger entropy than using these other methods, and this is actually what you what you want when you want to do compassion, for example, because then you really want to assign positive density to all your data points.",
                    "label": 0
                },
                {
                    "sent": "But of course, if you want to generate nice looking samples then this would not be the right approach, because you're going to generate a lot of samples which don't look like data.",
                    "label": 0
                },
                {
                    "sent": "Using one of these other methods, we do get samples which look just like the data, but of course this comes at the cost of completely completely ignoring one of the modes, and this wouldn't work very well for compression.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Onto evaluating general models.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the Aurora?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy and perhaps intuitive way to evaluate kind of models is to simply draw samples from it and to look at it.",
                    "label": 0
                },
                {
                    "sent": "And this can be very useful diagnostic tool to develop intuition for model or to see where it could be improved or we find there is also often used to reason about a model density estimation performance or to reason about a model.",
                    "label": 0
                },
                {
                    "sent": "Representations that the model has learned and to see why this is problematic, I think it's helpful to keep in mind but that.",
                    "label": 0
                },
                {
                    "sent": "Just generating nice looking samples is quite easy.",
                    "label": 0
                },
                {
                    "sent": "You just store a bunch of training images and at Test time you draw samples from your from your database of images.",
                    "label": 0
                },
                {
                    "sent": "Is a perfectly fine general model, it's.",
                    "label": 0
                },
                {
                    "sent": "But of course it hasn't learned anything about objects, even those able to generate objects is of course a very terrible density model.",
                    "label": 0
                },
                {
                    "sent": "And even when we just want to generate content new content, then obviously this is also not what we want, so we have to do a bit more work to evaluate general models.",
                    "label": 0
                },
                {
                    "sent": "And one thing that.",
                    "label": 0
                },
                {
                    "sent": "But many papers have done is to look at nearest neighbors, so here the idea is that we take our sample and we compute the distance to all the images in the training set and then we find the nearest neighbors and we look at them.",
                    "label": 0
                },
                {
                    "sent": "And I guess the idea here is that if your model overfits strongly to the data then.",
                    "label": 0
                },
                {
                    "sent": "You will get samples which look very similar to the training images.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem with this approach is that already very small changes to your images can lead to very different nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So to illustrate this, we have looked at what happens when you shift images slightly by taking different crops of from cipher training images.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 0
                },
                {
                    "sent": "Look at the first column in.",
                    "label": 0
                },
                {
                    "sent": "For these images, those are slightly shifted versions of the same image and the second column is other nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "The plot the plot on the X axis shows the.",
                    "label": 0
                },
                {
                    "sent": "On the shift in pixels, the Y axis OST the Euclidean distance to training images.",
                    "label": 0
                },
                {
                    "sent": "So each line corresponds to.",
                    "label": 0
                },
                {
                    "sent": "One training image, the Black line corresponds to the reference image.",
                    "label": 0
                },
                {
                    "sent": "The corresponding image that the shifted image was based on.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can see already for a small shift on only two pixels, we get a completely different nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Even though the perceptually, the image has hardly changed, not.",
                    "label": 0
                },
                {
                    "sent": "Not only that, but the nearest neighbor looks completely different.",
                    "label": 0
                },
                {
                    "sent": "It's not only slightly different, it's not only just a slightly different track, but it's a completely different image, and that is because the.",
                    "label": 0
                },
                {
                    "sent": "Euclidean distance doesn't agree with very well with the perception.",
                    "label": 0
                },
                {
                    "sent": "Images.",
                    "label": 0
                },
                {
                    "sent": "Which is not to say that if we had a perception metric that this test would be useful, but at least with the way that we do it now, the only thing that this test does is basically detect a look up table and as soon as you model slightly more complicated, this test will not find anything.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another thing that people have done is to use passing window estimates where the basic idea is again we draw samples from the model and then we build a tractable model using these samples.",
                    "label": 0
                },
                {
                    "sent": "For example, a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And we evaluate the log likelihood under this tractable model.",
                    "label": 0
                },
                {
                    "sent": "One problem with this approach is that.",
                    "label": 0
                },
                {
                    "sent": "You know, as an approximation to the log likelihood, this works very poorly.",
                    "label": 0
                },
                {
                    "sent": "So here we fit in a Gaussian distribution to small image patches 6 by 6 image patches.",
                    "label": 0
                },
                {
                    "sent": "And on the Y axis we have to log likelihood of the Gaussian in the, that's the black line and then the pass window estimate is the red line.",
                    "label": 0
                },
                {
                    "sent": "And even though the.",
                    "label": 0
                },
                {
                    "sent": "Data is quite low dimensional models.",
                    "label": 0
                },
                {
                    "sent": "Model is quite simple and we use a lot of samples.",
                    "label": 0
                },
                {
                    "sent": "The approximation is very far away from the true log likelihood.",
                    "label": 0
                },
                {
                    "sent": "This would still be fine if you know at least the ranking that pass member estimates assigned to models in somewhere meaningful.",
                    "label": 0
                },
                {
                    "sent": "For example, if the ranking assigned bypass window estimates was the same as the ranking assigned by log likelihoods.",
                    "label": 1
                },
                {
                    "sent": "That would still be a very very useful measure, but unfortunately.",
                    "label": 0
                },
                {
                    "sent": "This is also not the case.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have past midnight estimates for a bunch of models for M nest, and we've also included the passing window estimate for the true distribution where you draw samples from the data distribution.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the.",
                    "label": 0
                },
                {
                    "sent": "True distribution performs worse than some other models.",
                    "label": 0
                },
                {
                    "sent": "In fact, that K means based model is a very simple model just based on K means, and it performs very very well.",
                    "label": 0
                },
                {
                    "sent": "Thinking decision theory, such a measure which assigns you know.",
                    "label": 0
                },
                {
                    "sent": "The real distribution poor value is called an improper scoring rule and should be avoided because you're basically optimizing for the wrong thing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then it seems that if we if we care about things like look, likely that we really have to evaluate the log likelihoods, or at least properly approximated.",
                    "label": 0
                },
                {
                    "sent": "But also there.",
                    "label": 0
                },
                {
                    "sent": "But there are some caveats if you, especially if we're talking about fitting densities to discretize data, as we often do, you can get easily infinite log likelihood not only on the training data, but also on the test data.",
                    "label": 0
                },
                {
                    "sent": "If your model takes these.",
                    "label": 0
                },
                {
                    "sent": "Discretizations in your data, but there's an easy fix to this, and that is to simply add uniform noise to your discretized data.",
                    "label": 0
                },
                {
                    "sent": "This fixes it because then they look likely with this bounded by the log likelihood of a discrete model.",
                    "label": 0
                },
                {
                    "sent": "So if we define a discrete Model S, this integral over the density.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy to show that the log likelihood of your continuous data.",
                    "label": 0
                },
                {
                    "sent": "So why is the continuous data with the noise?",
                    "label": 0
                },
                {
                    "sent": "Is bounded by the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Of the discrete model on the discrete data, which is in itself bounded by the negative entropy of the discrete data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This discrete log likelihood is also directly related to compression performance of the discrete model.",
                    "label": 0
                },
                {
                    "sent": "For the discrete data.",
                    "label": 0
                },
                {
                    "sent": "So if you use.",
                    "label": 0
                },
                {
                    "sent": "This kid model for compression of discrete data, then the negative log likelihood directly corresponds to the number of bits that you would use on average.",
                    "label": 0
                },
                {
                    "sent": "And that makes this look like a very meaningful becausw.",
                    "label": 0
                },
                {
                    "sent": "As soon as you get an improvement in this continuous log likelihood, you know how to better compress the discrete data.",
                    "label": 0
                },
                {
                    "sent": "So then we can ask how?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plus, how do samples and log likelihood relate?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And it turns out they are quite different features of objective model.",
                    "label": 0
                },
                {
                    "sent": "You can see this by looking at this mixture model.",
                    "label": 0
                },
                {
                    "sent": "So let's say P is any model that you like.",
                    "label": 0
                },
                {
                    "sent": "Could be extremely good model in terms of likelihood, could be the perfect distribution of the data.",
                    "label": 0
                },
                {
                    "sent": "And we mix it with some Q where Q could be just white noise for example and we mix it so that 99% of the time we draw white noise instead of images.",
                    "label": 0
                },
                {
                    "sent": "Or instead of from our model.",
                    "label": 0
                },
                {
                    "sent": "But you can show that the log likelihood of this mixture model still at most 4.61 Nets worse than the log likelihood of P and in high dimensions 4.61 Nets is really not a lot if.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "For cipher block Key will be in the thousands, so 4.61 Nats is really not a lot.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make this a little more concrete what this means.",
                    "label": 0
                },
                {
                    "sent": "What this result means?",
                    "label": 0
                },
                {
                    "sent": "Let's look at some examples.",
                    "label": 0
                },
                {
                    "sent": "So here we may.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Isotropic here we mix isotopically since Gaussians and look up table.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "With again 1% and 99% weights.",
                    "label": 0
                },
                {
                    "sent": "And we sampled from them 100 samples and below the images below the samples to see the compression performance of the of the mixture model.",
                    "label": 0
                },
                {
                    "sent": "And if we focus on the 1st row, we see that these mixture models all have almost identical compression performance, but extremely different samples.",
                    "label": 0
                },
                {
                    "sent": "And if we focus on the last two columns, we can see that the model can have almost identical samples, but very different compression performance.",
                    "label": 0
                },
                {
                    "sent": "So what this means is you can basically mix and match compression performance and samples arbitrarily.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can make a similar argument for samples applications.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example, classification, let's, let's let's, let's say classification.",
                    "label": 0
                },
                {
                    "sent": "Let's say why is a class label.",
                    "label": 0
                },
                {
                    "sent": "Again X is image.",
                    "label": 0
                },
                {
                    "sent": "And again we have a mixture over of P&Q so that 99% of the time.",
                    "label": 0
                },
                {
                    "sent": "We will draw samples from Q.",
                    "label": 0
                },
                {
                    "sent": "And we also assume that the log likelihood of peeing is much larger than the log likelihood of Q, and in that case if lock P is much larger larger than lock you the posterior over Y will be dominated by the posterior over by the posterior of P. So what this means against you can have you can mix and match arbitrary samples with arbitrary classification performance for example.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arise based on these observations, we think that it's important to evaluate gender models on the on the right applications on the applications that you care about.",
                    "label": 0
                },
                {
                    "sent": "For example, if you care about compression, not likely to, certainly the right measure.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you want to, if you care about content generation looking at samples, it's fine.",
                    "label": 0
                },
                {
                    "sent": "Of course, same goes for an image reconstructions.",
                    "label": 0
                },
                {
                    "sent": "There are daily.",
                    "label": 0
                },
                {
                    "sent": "We used psychophysics, Hawley, Gerhard, immature speaker have a nice paper on how to evaluate samples from models using psychophysics.",
                    "label": 0
                },
                {
                    "sent": "If we, if we care about representation learning, then I think we should really try to use those representations for some task.",
                    "label": 0
                },
                {
                    "sent": "And further recommendations based on these observations are try to avoid using parsing window estimates.",
                    "label": 0
                },
                {
                    "sent": "Do not rely on nearest neighbor tests for overfitting.",
                    "label": 1
                },
                {
                    "sent": "And you know, use samples only where it's relevant for the application.",
                    "label": 0
                },
                {
                    "sent": "Or as a diagnostic tool, but not necessarily as a proxy for other tasks.",
                    "label": 1
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                }
            ]
        }
    }
}