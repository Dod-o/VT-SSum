{
    "id": "wlh4kaqbz3ds46ih7oi6ycgquxzaea63",
    "title": "Combining Shallow and Deep NLP Methods for Recognizing Textual Entailment",
    "info": {
        "author": [
            "Katja Markert, University of Leeds"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/pcw05_markert_csdnm/",
    "segmentation": [
        [
            "Operating based on data analysis, continuous event volunteers gather baseline data so volunteers is still we only consider equality here.",
            "So once you completed and had a idea of five on so that's added to the way gather is not repeated.",
            "So there's no similarity measure or any similarity.",
            "Therefore, and so on and so on.",
            "And then you Add all the dates of the head."
        ],
        [
            "Abide by that, yeah, it's just simple as that.",
            "And I told a little bit about modifications shortly in a moment.",
            "So if you just look at the decision tree on the development side that trained you get a very simple symmetry, namely, the overlap is smaller than specific numbers, 'cause otherwise it's true.",
            "You will see that that contrast is what I said during the panel.",
            "If you train the same thing on the test set to get this effect, that very high overlap means it's actually force.",
            "So it looks different.",
            "Well, we used for our decision trees and so on.",
            "The custody,"
        ],
        [
            "Confidence values, so the results for this very simple method was already significantly greater.",
            "The baselines again accuracy of 55 and actually compensating 58%.",
            "Ann Louise always.",
            "Everything that is Golden Freddy is always bad, although they could look like for eight on the small screen.",
            "But everybody else is performed by bad on the CD task.",
            "It actually also made some impact."
        ],
        [
            "On the task, but all the other part expecting machine translation was.",
            "Um?",
            "Probably expected your data.",
            "So in summary, it is only good for the baseline and the city United Cos it was especially bad at handling paraphrases if you only looked at quality negation, any cases of background knowledge is needed now for the paraphrases we thought, OK.",
            "I mean, even Vernon similarity that people that collect gather to make some form of difference.",
            "Um, so the.",
            "Back this year so we actually try to also incorporate, like gathering, collecting similar and therefore you would add 2.877 instead of subtracting, but we did not weigh, for example, the connections in orbit you just used.",
            "Synonyms and then what happens is that actually you ever get lots of false positive."
        ],
        [
            "We emphasize, wasn't better than just using equality or we would get better at measuring the true examples.",
            "You get locks versus F measurement on the neck.",
            "So that's also how we would expect because it makes equation can be pointed out.",
            "What made some difference?",
            "But we always have tried is instead of subtracting to just have zero for everything that doesn't match, like for example months into right he did and it is actually better to penalize.",
            "Just ignore it.",
            "OK.",
            "So we have a feeling that going further down that Rd over there, obviously other things you can do with shallow methods and lots more features you can add was maybe not particularly interesting.",
            "So as a completely different approach, we went into deep semantic analysis.",
            "So what we did is.",
            "Muse divide coverage.",
            "The part that was originally developed by Stephen Clark and James Curran.",
            "That is tokenizing part.",
            "Plymouth Ising cars in tagging.",
            "And also as very similar approach to the name."
        ],
        [
            "Lebanon.",
            "And for that I developed the CCG part, transformed the CCG parser representations into semantic representations for the text hypothesis pair and the semantic formalism.",
            "User 1st order fragment of physical representation theory.",
            "And I basically start from there and don't know much about the passing car because there's any very similar to what we've heard from other people.",
            "So as an example, if you have that hypothesis, then the DRT, the final output would have.",
            "For discord entities expense to explore and then several predicate one place predicates like that volunteers to explore that gather is experiment that."
        ],
        [
            "So that's just standard discuss representation theories.",
            "And then here also some two place predicates, mainly that.",
            "Next one is the agent explores the agent of its body and the dissipation relationship and the noun compound relationship.",
            "Regarding like Asian patients, subjects and so on, there was a partial semantic relations match there which so didn't give you full frames.",
            "So for example, we didn't analyze compounds any further, it would only useful.",
            "Um?",
            "Then for implementing the inference that the races were translated into just first order logic, as you know, the RT is.",
            "Equivalent 1st order logic.",
            "So you can translate back and forth without meaning loss and use parallel into parallel inference engines.",
            "Use the theorem prover for 1st order logic called Vampire which is publicly available as well as a finite model builder called Paradox and for people who are more interested into in this kind.",
            "So you have sandpaper Blackburn School district.",
            "Come out under under serializable."
        ],
        [
            "Right, so as an example for for a model.",
            "Damage can be seen by the motivator.",
            "This would be.",
            "The model for the same sentence SEC.",
            "Again, you have a domain of foreign the domain sizes in just four and the motor size."
        ],
        [
            "And the domain size multiplied by the number of or two place predicate.",
            "So we have to place predicates the agent patient in an ounce of the motor size before time speaking with each other.",
            "We need that later.",
            "So now you might ask.",
            "I mean this isn't different.",
            "Almost a dollar from the from the DRT representation so you might add my last by this representation instead.",
            "So what it does is it's a flat representation, so those of you know DRT better we know that it can be heretical because of negation operators or disjunction, and you won't have that bear, so it's already mapped into a flat representation, which is what the theorem provers for itself.",
            "Um?"
        ],
        [
            "Right then they had certain background knowledge so they were generic axioms, lexicon, origin, geographical knowledge, and then go through them 1 by 1.",
            "So the Canary accents were.",
            "Your hand melted, that's why there at the moment only about 20 and you don't really want to hand hold them any further, but preferably learn them automatically in the future.",
            "Are typically examples are active, passive ordinations, or genitive versus off genitives.",
            "Equivalences answer on here.",
            "Another example is that if you have an event and."
        ],
        [
            "It's an open event and you know where the event took place.",
            "When you can normally infer that the agent was also in that place.",
            "That campus location.",
            "Let's say the knowledge was derived automatically from Bernard.",
            "What is important here?",
            "We didn't look at multi word collocations at all.",
            "So we didn't catch them, which gave us quite a bit of a. Locking information.",
            "And that was computed on the fly and included synonyms, like here for all X, if you have together as of late.",
            "And they were translated then into first order logic or also hyponymy like a volunteer person.",
            "Almost five important because of the expensive runtimes of theorem proving is that the only computed the lexical knowledge relevant to the immediate Texan hypothesis.",
            "So we only looked at the items occurring in various states.",
            "And the geographical knowledge was automatically extracted from the CIA World Fact book.",
            "And included capitals and other cities and countries relations.",
            "As well as languages in countries like the Filipino Philippines example.",
            "OK.",
            "So how it works for the exact containment is that the theorem prover tries to prove that the.",
            "Text in Tennessee hypothesis just completely standard and what that can handle from our representations.",
            "For example, positions, relative clauses, active, passive, possessive.",
            "And simple background knowledge like that case that we have just seen visible up together and created by including that background knowledge however.",
            "It found only further proves out of 100 of the test set, of which 20 three were really entailment, so it didn't make many errors.",
            "Most of the errors it had.",
            "They are due to parsing or something like that.",
            "And so it has high precision.",
            "But however this is muscle or record and you can compute that inaccuracy over the whole test said to get to kind of 52% similar to the minds of our system.",
            "And the main reason was that we had a lack of carnal knowledge, especially of paraphrases, and my people again.",
            "So if you have something like this I can software seems to on sales in Europe and then strong says for items in Europe we would have no information that to seize Tom says.",
            "Can be the same as a for iTunes.",
            "For example, so it wouldn't be able to handle that over there.",
            "Such very similar sentences.",
            "So one more which will go down in the future is to automatically acquire more background knowledge.",
            "But we have gone from now is you have to decide to format.",
            "We have up to now to make sure.",
            "So for the proximate attainment by comparing the motor sizes for the text and the text, if you have the hypothesis, which means if the difference in these modern size is small then there is little new information introduced.",
            "So if you build a model for both the text and had positive and you don't get much more domain entities.",
            "Much more new relations then.",
            "It is likely to be in attainment.",
            "Obviously, for sentences which are entailed in the theorem prover of India.",
            "0 difference.",
            "And we use all the domain size and the motor size and you combine these features with the decision tree learning algorithm.",
            "So the features that were used were positively affected by theorem prover.",
            "Then the inconsistent is a feature which measures better level inconsistency with the background knowledge.",
            "Then the D is just the domain size of T plus age and then the absolute difference between DDT plus H and the text and the audience relative difference and the same for the motor sizes.",
            "Yeah, so that approximate containment, so if one little piece of background knowledge is missing, lighting the items in the has seen that piece is more dangerous.",
            "Other things that were used which were.",
            "Hello there, the text includes a negation factor somehow which was immediately derived from the DRS.",
            "So didn't go wild theorem prover.",
            "Whether the hypothesis?",
            "Had a negation or whether either wasn't allocation, so negation is just a Boolean or between the other two and then the overlap which we had from the word overlap, which wasn't very feature.",
            "So what I actually would like to do is to look a bit at the decision tree because it had some interesting factors.",
            "So it started with the fact that divide violent aid at the top level, which is fine, parrigin 'cause it was a higher position feature.",
            "So if entailed was proved.",
            "Then it's true.",
            "If not though, and that showed some artificiality in the data set, it went first to the negation.",
            "Feature we just said.",
            "If there's indication in the hypothesis, and it is likely to be forward instead of any kind of comparison between Texan hypothesis is that is not the case, and it went mostly to overlap to shallow overlap features first.",
            "Um?",
            "And only.",
            "Oops.",
            "Only actually before these.",
            "You know, if you have a relatively high overlaps, no mediation, then it went into this part where we actually then really used the left on the next slide.",
            "Married action really used in the domain and motor sizes.",
            "The domains that difference between the two domain sizes smaller then it goes to true and the same for motor sizes.",
            "So if the motor sizes actually did add.",
            "But we wanted him to act.",
            "Given the results, you see that there again better than the baseline.",
            "There is slightly better, but not significant result in Chelsea Shadow feature alone however.",
            "And although this is not encouraging that it's not much bigger than the shadow feature along the main reason for this is that it again misses the background knowledge.",
            "So in many cases, if you only again.",
            "Have a.",
            "That being able to infer from inequality or maybe synonymy in Word net.",
            "And we're not really there.",
            "Is there for very much improved over the overlap.",
            "However, if you look at the tasks in detail, you see some differences, so again, it performs well on the CD, and I do, but it actually performs worse on CD then the shell overlap.",
            "So in a way it's a shadow features.",
            "S Today was constructed to maximizing overlap.",
            "You may be off.",
            "Figure out just using this.",
            "It performs overall, although not valid on the other tasks, better than the shadow overlap and especially more even these, so there are none of these really strange outliers, like 30% for machine translation, and there is at least for the confidence weighted average score.",
            "Definitely some hope for the paraphrasing and reading comprehension leverage significantly better than the baseline there.",
            "So I do think this is the way forward over in the overall scores.",
            "You don't really see it.",
            "See it properly.",
            "The other issue is if you look at the F measure of the true and force that it doesn't overestimate the true so much as the lexical overlap does, but is much more uniform in being able to handle foreign shore as well.",
            "Um?",
            "Yeah, OK, so that's the summary below really just said.",
            "That actually before I come to the conclusions was not our best system that is submitted because we submitted only the result over the.",
            "Whole set testing training over the whole state.",
            "If you include a task feature in the decision tree.",
            "If you just say OK. At the task in the task is a feature which is extensively used by the decision tree.",
            "You get about 4% more in both accuracy and the confidence weighted average, so it's about 60 to 64.",
            "Then the fire.",
            "Major.",
            "So to conclude, I presented both Shadow system based on word overlap and by I think it's not the way forward over at the moment present performed similar to deep system.",
            "Then a deep semantic analysis which did well space on modern building in theorem proving.",
            "And to expand on this, another notion of approximate attainment via comparing motor size is the bottleneck is still background knowledge and that Secondly populated focus on in future work.",
            "I both shared on the hybrid model significantly outperform the baseline, but the hybrid model has more potential for different kinds of tasks.",
            "Question related to the theorem.",
            "And in many cases for so that logic is insufficient to represent what's in the texts.",
            "Problems.",
            "2nd order.",
            "Predicates.",
            "I don't know God.",
            "Opportunity to share all this.",
            "I think all these cases you know they.",
            "The fact that.",
            "It's good to have you back.",
            "They need to be accommodated in which way?",
            "I know that like I, I think I think for the for the current task.",
            "We had this feeling that all of these problems exist like this functionality and so on that these were not the major problems for performing well on the current data settings, or at least for getting to a reasonable number.",
            "Or it might matter once we get over 90%.",
            "Everybody is really far away from at the moment that, but that really, the specially the paraphrases enter background knowledge are at the moment the main issues.",
            "But I can see if you see the point, yes.",
            "So, so it's interesting that the hybrid system and the conclusion right?",
            "If you take this to an extreme.",
            "Then you know you could build 7 different engines for the different tasks and then the decision tree, which basically is dispatching which strategy to choose could then choose the appropriate engine.",
            "So that way we didn't train the decision trees separately.",
            "Then we took incorporated as a feature.",
            "So the feature was task and was just a string CDIR and it shows them automatically in their entirety.",
            "So it was entailed and then the next thing was the task feature.",
            "Basically entailed was still the best, but then the task and then the split up in the task came next.",
            "So that's definitely relevant.",
            "Yeah overall by about 4%.",
            "No, no, no drops this event up again because custody just only the overlap and ignored all the models and everything else.",
            "No, nothing that dumb.",
            "It wasn't general overall improvement far as I can remember.",
            "Some of the tasks performance improved partners regarding an overall increase didn't really change that.",
            "No, I was changed quite a bit, changed quite a bit.",
            "Don't know why that is.",
            "Example, so if you both of you use decision trees, so it might get to start overfitting, and that's sort of the test.",
            "Yeah, we didn't use the bathroom.",
            "100 hundred examples.",
            "We didn't want to train them separately, but just have it as a feature.",
            "I don't know.",
            "Did you have it as a feature or did you?",
            "Also I don't know.",
            "Ways of incorporating existing background knowledge in the in the reasoning.",
            "It's the amount, I mean, lots of the background knowledge is relative, is relatively easily translated into first order logic, although obviously not.",
            "It's just.",
            "For example, there are many resources which we didn't look at yet.",
            "For example, we didn't use the motion yet, or like the embarrassing lies automatic paraphrase at positions.",
            "So we just haven't really gotten around to incorporating them all yet.",
            "One minor very there, although I'm pretty sure that this will improve the system in the end, is all the running time, because the theorem proving is not very fast.",
            "If you, I mean, at least it would be yours if you go if you have all this background knowledge, because it has to play or the model building in parallel."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operating based on data analysis, continuous event volunteers gather baseline data so volunteers is still we only consider equality here.",
                    "label": 0
                },
                {
                    "sent": "So once you completed and had a idea of five on so that's added to the way gather is not repeated.",
                    "label": 0
                },
                {
                    "sent": "So there's no similarity measure or any similarity.",
                    "label": 0
                },
                {
                    "sent": "Therefore, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And then you Add all the dates of the head.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abide by that, yeah, it's just simple as that.",
                    "label": 0
                },
                {
                    "sent": "And I told a little bit about modifications shortly in a moment.",
                    "label": 0
                },
                {
                    "sent": "So if you just look at the decision tree on the development side that trained you get a very simple symmetry, namely, the overlap is smaller than specific numbers, 'cause otherwise it's true.",
                    "label": 0
                },
                {
                    "sent": "You will see that that contrast is what I said during the panel.",
                    "label": 0
                },
                {
                    "sent": "If you train the same thing on the test set to get this effect, that very high overlap means it's actually force.",
                    "label": 0
                },
                {
                    "sent": "So it looks different.",
                    "label": 0
                },
                {
                    "sent": "Well, we used for our decision trees and so on.",
                    "label": 0
                },
                {
                    "sent": "The custody,",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Confidence values, so the results for this very simple method was already significantly greater.",
                    "label": 0
                },
                {
                    "sent": "The baselines again accuracy of 55 and actually compensating 58%.",
                    "label": 0
                },
                {
                    "sent": "Ann Louise always.",
                    "label": 0
                },
                {
                    "sent": "Everything that is Golden Freddy is always bad, although they could look like for eight on the small screen.",
                    "label": 0
                },
                {
                    "sent": "But everybody else is performed by bad on the CD task.",
                    "label": 0
                },
                {
                    "sent": "It actually also made some impact.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the task, but all the other part expecting machine translation was.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Probably expected your data.",
                    "label": 0
                },
                {
                    "sent": "So in summary, it is only good for the baseline and the city United Cos it was especially bad at handling paraphrases if you only looked at quality negation, any cases of background knowledge is needed now for the paraphrases we thought, OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, even Vernon similarity that people that collect gather to make some form of difference.",
                    "label": 0
                },
                {
                    "sent": "Um, so the.",
                    "label": 0
                },
                {
                    "sent": "Back this year so we actually try to also incorporate, like gathering, collecting similar and therefore you would add 2.877 instead of subtracting, but we did not weigh, for example, the connections in orbit you just used.",
                    "label": 0
                },
                {
                    "sent": "Synonyms and then what happens is that actually you ever get lots of false positive.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We emphasize, wasn't better than just using equality or we would get better at measuring the true examples.",
                    "label": 0
                },
                {
                    "sent": "You get locks versus F measurement on the neck.",
                    "label": 0
                },
                {
                    "sent": "So that's also how we would expect because it makes equation can be pointed out.",
                    "label": 0
                },
                {
                    "sent": "What made some difference?",
                    "label": 0
                },
                {
                    "sent": "But we always have tried is instead of subtracting to just have zero for everything that doesn't match, like for example months into right he did and it is actually better to penalize.",
                    "label": 0
                },
                {
                    "sent": "Just ignore it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have a feeling that going further down that Rd over there, obviously other things you can do with shallow methods and lots more features you can add was maybe not particularly interesting.",
                    "label": 0
                },
                {
                    "sent": "So as a completely different approach, we went into deep semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "So what we did is.",
                    "label": 0
                },
                {
                    "sent": "Muse divide coverage.",
                    "label": 0
                },
                {
                    "sent": "The part that was originally developed by Stephen Clark and James Curran.",
                    "label": 0
                },
                {
                    "sent": "That is tokenizing part.",
                    "label": 0
                },
                {
                    "sent": "Plymouth Ising cars in tagging.",
                    "label": 0
                },
                {
                    "sent": "And also as very similar approach to the name.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lebanon.",
                    "label": 0
                },
                {
                    "sent": "And for that I developed the CCG part, transformed the CCG parser representations into semantic representations for the text hypothesis pair and the semantic formalism.",
                    "label": 0
                },
                {
                    "sent": "User 1st order fragment of physical representation theory.",
                    "label": 0
                },
                {
                    "sent": "And I basically start from there and don't know much about the passing car because there's any very similar to what we've heard from other people.",
                    "label": 0
                },
                {
                    "sent": "So as an example, if you have that hypothesis, then the DRT, the final output would have.",
                    "label": 0
                },
                {
                    "sent": "For discord entities expense to explore and then several predicate one place predicates like that volunteers to explore that gather is experiment that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's just standard discuss representation theories.",
                    "label": 0
                },
                {
                    "sent": "And then here also some two place predicates, mainly that.",
                    "label": 0
                },
                {
                    "sent": "Next one is the agent explores the agent of its body and the dissipation relationship and the noun compound relationship.",
                    "label": 0
                },
                {
                    "sent": "Regarding like Asian patients, subjects and so on, there was a partial semantic relations match there which so didn't give you full frames.",
                    "label": 0
                },
                {
                    "sent": "So for example, we didn't analyze compounds any further, it would only useful.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then for implementing the inference that the races were translated into just first order logic, as you know, the RT is.",
                    "label": 0
                },
                {
                    "sent": "Equivalent 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "So you can translate back and forth without meaning loss and use parallel into parallel inference engines.",
                    "label": 0
                },
                {
                    "sent": "Use the theorem prover for 1st order logic called Vampire which is publicly available as well as a finite model builder called Paradox and for people who are more interested into in this kind.",
                    "label": 0
                },
                {
                    "sent": "So you have sandpaper Blackburn School district.",
                    "label": 0
                },
                {
                    "sent": "Come out under under serializable.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so as an example for for a model.",
                    "label": 0
                },
                {
                    "sent": "Damage can be seen by the motivator.",
                    "label": 0
                },
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "The model for the same sentence SEC.",
                    "label": 0
                },
                {
                    "sent": "Again, you have a domain of foreign the domain sizes in just four and the motor size.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the domain size multiplied by the number of or two place predicate.",
                    "label": 0
                },
                {
                    "sent": "So we have to place predicates the agent patient in an ounce of the motor size before time speaking with each other.",
                    "label": 0
                },
                {
                    "sent": "We need that later.",
                    "label": 0
                },
                {
                    "sent": "So now you might ask.",
                    "label": 0
                },
                {
                    "sent": "I mean this isn't different.",
                    "label": 0
                },
                {
                    "sent": "Almost a dollar from the from the DRT representation so you might add my last by this representation instead.",
                    "label": 0
                },
                {
                    "sent": "So what it does is it's a flat representation, so those of you know DRT better we know that it can be heretical because of negation operators or disjunction, and you won't have that bear, so it's already mapped into a flat representation, which is what the theorem provers for itself.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right then they had certain background knowledge so they were generic axioms, lexicon, origin, geographical knowledge, and then go through them 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "So the Canary accents were.",
                    "label": 0
                },
                {
                    "sent": "Your hand melted, that's why there at the moment only about 20 and you don't really want to hand hold them any further, but preferably learn them automatically in the future.",
                    "label": 0
                },
                {
                    "sent": "Are typically examples are active, passive ordinations, or genitive versus off genitives.",
                    "label": 0
                },
                {
                    "sent": "Equivalences answer on here.",
                    "label": 0
                },
                {
                    "sent": "Another example is that if you have an event and.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's an open event and you know where the event took place.",
                    "label": 0
                },
                {
                    "sent": "When you can normally infer that the agent was also in that place.",
                    "label": 0
                },
                {
                    "sent": "That campus location.",
                    "label": 0
                },
                {
                    "sent": "Let's say the knowledge was derived automatically from Bernard.",
                    "label": 0
                },
                {
                    "sent": "What is important here?",
                    "label": 0
                },
                {
                    "sent": "We didn't look at multi word collocations at all.",
                    "label": 0
                },
                {
                    "sent": "So we didn't catch them, which gave us quite a bit of a. Locking information.",
                    "label": 0
                },
                {
                    "sent": "And that was computed on the fly and included synonyms, like here for all X, if you have together as of late.",
                    "label": 0
                },
                {
                    "sent": "And they were translated then into first order logic or also hyponymy like a volunteer person.",
                    "label": 0
                },
                {
                    "sent": "Almost five important because of the expensive runtimes of theorem proving is that the only computed the lexical knowledge relevant to the immediate Texan hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So we only looked at the items occurring in various states.",
                    "label": 0
                },
                {
                    "sent": "And the geographical knowledge was automatically extracted from the CIA World Fact book.",
                    "label": 0
                },
                {
                    "sent": "And included capitals and other cities and countries relations.",
                    "label": 0
                },
                {
                    "sent": "As well as languages in countries like the Filipino Philippines example.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how it works for the exact containment is that the theorem prover tries to prove that the.",
                    "label": 0
                },
                {
                    "sent": "Text in Tennessee hypothesis just completely standard and what that can handle from our representations.",
                    "label": 0
                },
                {
                    "sent": "For example, positions, relative clauses, active, passive, possessive.",
                    "label": 0
                },
                {
                    "sent": "And simple background knowledge like that case that we have just seen visible up together and created by including that background knowledge however.",
                    "label": 0
                },
                {
                    "sent": "It found only further proves out of 100 of the test set, of which 20 three were really entailment, so it didn't make many errors.",
                    "label": 0
                },
                {
                    "sent": "Most of the errors it had.",
                    "label": 0
                },
                {
                    "sent": "They are due to parsing or something like that.",
                    "label": 0
                },
                {
                    "sent": "And so it has high precision.",
                    "label": 0
                },
                {
                    "sent": "But however this is muscle or record and you can compute that inaccuracy over the whole test said to get to kind of 52% similar to the minds of our system.",
                    "label": 0
                },
                {
                    "sent": "And the main reason was that we had a lack of carnal knowledge, especially of paraphrases, and my people again.",
                    "label": 0
                },
                {
                    "sent": "So if you have something like this I can software seems to on sales in Europe and then strong says for items in Europe we would have no information that to seize Tom says.",
                    "label": 0
                },
                {
                    "sent": "Can be the same as a for iTunes.",
                    "label": 0
                },
                {
                    "sent": "For example, so it wouldn't be able to handle that over there.",
                    "label": 0
                },
                {
                    "sent": "Such very similar sentences.",
                    "label": 0
                },
                {
                    "sent": "So one more which will go down in the future is to automatically acquire more background knowledge.",
                    "label": 0
                },
                {
                    "sent": "But we have gone from now is you have to decide to format.",
                    "label": 0
                },
                {
                    "sent": "We have up to now to make sure.",
                    "label": 0
                },
                {
                    "sent": "So for the proximate attainment by comparing the motor sizes for the text and the text, if you have the hypothesis, which means if the difference in these modern size is small then there is little new information introduced.",
                    "label": 0
                },
                {
                    "sent": "So if you build a model for both the text and had positive and you don't get much more domain entities.",
                    "label": 0
                },
                {
                    "sent": "Much more new relations then.",
                    "label": 0
                },
                {
                    "sent": "It is likely to be in attainment.",
                    "label": 0
                },
                {
                    "sent": "Obviously, for sentences which are entailed in the theorem prover of India.",
                    "label": 0
                },
                {
                    "sent": "0 difference.",
                    "label": 0
                },
                {
                    "sent": "And we use all the domain size and the motor size and you combine these features with the decision tree learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the features that were used were positively affected by theorem prover.",
                    "label": 0
                },
                {
                    "sent": "Then the inconsistent is a feature which measures better level inconsistency with the background knowledge.",
                    "label": 0
                },
                {
                    "sent": "Then the D is just the domain size of T plus age and then the absolute difference between DDT plus H and the text and the audience relative difference and the same for the motor sizes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that approximate containment, so if one little piece of background knowledge is missing, lighting the items in the has seen that piece is more dangerous.",
                    "label": 0
                },
                {
                    "sent": "Other things that were used which were.",
                    "label": 0
                },
                {
                    "sent": "Hello there, the text includes a negation factor somehow which was immediately derived from the DRS.",
                    "label": 0
                },
                {
                    "sent": "So didn't go wild theorem prover.",
                    "label": 0
                },
                {
                    "sent": "Whether the hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Had a negation or whether either wasn't allocation, so negation is just a Boolean or between the other two and then the overlap which we had from the word overlap, which wasn't very feature.",
                    "label": 0
                },
                {
                    "sent": "So what I actually would like to do is to look a bit at the decision tree because it had some interesting factors.",
                    "label": 0
                },
                {
                    "sent": "So it started with the fact that divide violent aid at the top level, which is fine, parrigin 'cause it was a higher position feature.",
                    "label": 0
                },
                {
                    "sent": "So if entailed was proved.",
                    "label": 0
                },
                {
                    "sent": "Then it's true.",
                    "label": 0
                },
                {
                    "sent": "If not though, and that showed some artificiality in the data set, it went first to the negation.",
                    "label": 0
                },
                {
                    "sent": "Feature we just said.",
                    "label": 0
                },
                {
                    "sent": "If there's indication in the hypothesis, and it is likely to be forward instead of any kind of comparison between Texan hypothesis is that is not the case, and it went mostly to overlap to shallow overlap features first.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And only.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Only actually before these.",
                    "label": 0
                },
                {
                    "sent": "You know, if you have a relatively high overlaps, no mediation, then it went into this part where we actually then really used the left on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Married action really used in the domain and motor sizes.",
                    "label": 0
                },
                {
                    "sent": "The domains that difference between the two domain sizes smaller then it goes to true and the same for motor sizes.",
                    "label": 0
                },
                {
                    "sent": "So if the motor sizes actually did add.",
                    "label": 0
                },
                {
                    "sent": "But we wanted him to act.",
                    "label": 0
                },
                {
                    "sent": "Given the results, you see that there again better than the baseline.",
                    "label": 0
                },
                {
                    "sent": "There is slightly better, but not significant result in Chelsea Shadow feature alone however.",
                    "label": 0
                },
                {
                    "sent": "And although this is not encouraging that it's not much bigger than the shadow feature along the main reason for this is that it again misses the background knowledge.",
                    "label": 0
                },
                {
                    "sent": "So in many cases, if you only again.",
                    "label": 0
                },
                {
                    "sent": "Have a.",
                    "label": 0
                },
                {
                    "sent": "That being able to infer from inequality or maybe synonymy in Word net.",
                    "label": 0
                },
                {
                    "sent": "And we're not really there.",
                    "label": 0
                },
                {
                    "sent": "Is there for very much improved over the overlap.",
                    "label": 0
                },
                {
                    "sent": "However, if you look at the tasks in detail, you see some differences, so again, it performs well on the CD, and I do, but it actually performs worse on CD then the shell overlap.",
                    "label": 0
                },
                {
                    "sent": "So in a way it's a shadow features.",
                    "label": 0
                },
                {
                    "sent": "S Today was constructed to maximizing overlap.",
                    "label": 0
                },
                {
                    "sent": "You may be off.",
                    "label": 0
                },
                {
                    "sent": "Figure out just using this.",
                    "label": 0
                },
                {
                    "sent": "It performs overall, although not valid on the other tasks, better than the shadow overlap and especially more even these, so there are none of these really strange outliers, like 30% for machine translation, and there is at least for the confidence weighted average score.",
                    "label": 0
                },
                {
                    "sent": "Definitely some hope for the paraphrasing and reading comprehension leverage significantly better than the baseline there.",
                    "label": 0
                },
                {
                    "sent": "So I do think this is the way forward over in the overall scores.",
                    "label": 0
                },
                {
                    "sent": "You don't really see it.",
                    "label": 0
                },
                {
                    "sent": "See it properly.",
                    "label": 0
                },
                {
                    "sent": "The other issue is if you look at the F measure of the true and force that it doesn't overestimate the true so much as the lexical overlap does, but is much more uniform in being able to handle foreign shore as well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that's the summary below really just said.",
                    "label": 0
                },
                {
                    "sent": "That actually before I come to the conclusions was not our best system that is submitted because we submitted only the result over the.",
                    "label": 0
                },
                {
                    "sent": "Whole set testing training over the whole state.",
                    "label": 0
                },
                {
                    "sent": "If you include a task feature in the decision tree.",
                    "label": 0
                },
                {
                    "sent": "If you just say OK. At the task in the task is a feature which is extensively used by the decision tree.",
                    "label": 0
                },
                {
                    "sent": "You get about 4% more in both accuracy and the confidence weighted average, so it's about 60 to 64.",
                    "label": 0
                },
                {
                    "sent": "Then the fire.",
                    "label": 0
                },
                {
                    "sent": "Major.",
                    "label": 1
                },
                {
                    "sent": "So to conclude, I presented both Shadow system based on word overlap and by I think it's not the way forward over at the moment present performed similar to deep system.",
                    "label": 0
                },
                {
                    "sent": "Then a deep semantic analysis which did well space on modern building in theorem proving.",
                    "label": 1
                },
                {
                    "sent": "And to expand on this, another notion of approximate attainment via comparing motor size is the bottleneck is still background knowledge and that Secondly populated focus on in future work.",
                    "label": 0
                },
                {
                    "sent": "I both shared on the hybrid model significantly outperform the baseline, but the hybrid model has more potential for different kinds of tasks.",
                    "label": 0
                },
                {
                    "sent": "Question related to the theorem.",
                    "label": 0
                },
                {
                    "sent": "And in many cases for so that logic is insufficient to represent what's in the texts.",
                    "label": 0
                },
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "2nd order.",
                    "label": 0
                },
                {
                    "sent": "Predicates.",
                    "label": 0
                },
                {
                    "sent": "I don't know God.",
                    "label": 0
                },
                {
                    "sent": "Opportunity to share all this.",
                    "label": 0
                },
                {
                    "sent": "I think all these cases you know they.",
                    "label": 0
                },
                {
                    "sent": "The fact that.",
                    "label": 0
                },
                {
                    "sent": "It's good to have you back.",
                    "label": 0
                },
                {
                    "sent": "They need to be accommodated in which way?",
                    "label": 0
                },
                {
                    "sent": "I know that like I, I think I think for the for the current task.",
                    "label": 0
                },
                {
                    "sent": "We had this feeling that all of these problems exist like this functionality and so on that these were not the major problems for performing well on the current data settings, or at least for getting to a reasonable number.",
                    "label": 0
                },
                {
                    "sent": "Or it might matter once we get over 90%.",
                    "label": 0
                },
                {
                    "sent": "Everybody is really far away from at the moment that, but that really, the specially the paraphrases enter background knowledge are at the moment the main issues.",
                    "label": 1
                },
                {
                    "sent": "But I can see if you see the point, yes.",
                    "label": 0
                },
                {
                    "sent": "So, so it's interesting that the hybrid system and the conclusion right?",
                    "label": 0
                },
                {
                    "sent": "If you take this to an extreme.",
                    "label": 0
                },
                {
                    "sent": "Then you know you could build 7 different engines for the different tasks and then the decision tree, which basically is dispatching which strategy to choose could then choose the appropriate engine.",
                    "label": 0
                },
                {
                    "sent": "So that way we didn't train the decision trees separately.",
                    "label": 0
                },
                {
                    "sent": "Then we took incorporated as a feature.",
                    "label": 0
                },
                {
                    "sent": "So the feature was task and was just a string CDIR and it shows them automatically in their entirety.",
                    "label": 0
                },
                {
                    "sent": "So it was entailed and then the next thing was the task feature.",
                    "label": 0
                },
                {
                    "sent": "Basically entailed was still the best, but then the task and then the split up in the task came next.",
                    "label": 0
                },
                {
                    "sent": "So that's definitely relevant.",
                    "label": 0
                },
                {
                    "sent": "Yeah overall by about 4%.",
                    "label": 0
                },
                {
                    "sent": "No, no, no drops this event up again because custody just only the overlap and ignored all the models and everything else.",
                    "label": 0
                },
                {
                    "sent": "No, nothing that dumb.",
                    "label": 0
                },
                {
                    "sent": "It wasn't general overall improvement far as I can remember.",
                    "label": 0
                },
                {
                    "sent": "Some of the tasks performance improved partners regarding an overall increase didn't really change that.",
                    "label": 0
                },
                {
                    "sent": "No, I was changed quite a bit, changed quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Don't know why that is.",
                    "label": 0
                },
                {
                    "sent": "Example, so if you both of you use decision trees, so it might get to start overfitting, and that's sort of the test.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we didn't use the bathroom.",
                    "label": 0
                },
                {
                    "sent": "100 hundred examples.",
                    "label": 0
                },
                {
                    "sent": "We didn't want to train them separately, but just have it as a feature.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Did you have it as a feature or did you?",
                    "label": 0
                },
                {
                    "sent": "Also I don't know.",
                    "label": 0
                },
                {
                    "sent": "Ways of incorporating existing background knowledge in the in the reasoning.",
                    "label": 0
                },
                {
                    "sent": "It's the amount, I mean, lots of the background knowledge is relative, is relatively easily translated into first order logic, although obviously not.",
                    "label": 0
                },
                {
                    "sent": "It's just.",
                    "label": 0
                },
                {
                    "sent": "For example, there are many resources which we didn't look at yet.",
                    "label": 0
                },
                {
                    "sent": "For example, we didn't use the motion yet, or like the embarrassing lies automatic paraphrase at positions.",
                    "label": 0
                },
                {
                    "sent": "So we just haven't really gotten around to incorporating them all yet.",
                    "label": 0
                },
                {
                    "sent": "One minor very there, although I'm pretty sure that this will improve the system in the end, is all the running time, because the theorem proving is not very fast.",
                    "label": 0
                },
                {
                    "sent": "If you, I mean, at least it would be yours if you go if you have all this background knowledge, because it has to play or the model building in parallel.",
                    "label": 0
                }
            ]
        }
    }
}