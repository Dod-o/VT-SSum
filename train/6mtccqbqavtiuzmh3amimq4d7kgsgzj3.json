{
    "id": "6mtccqbqavtiuzmh3amimq4d7kgsgzj3",
    "title": "A Comparison Of AUC-Estimators In Small-Sample Studies",
    "info": {
        "author": [
            "Antti Airola, Department of Information Technology, University of Turku"
        ],
        "published": "Oct. 5, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlsb09_airola_acoauce/",
    "segmentation": [
        [
            "So good morning everybody, my name is on the Ireland I am giving you a presentation about a comparison of AUC estimators in small sample studies.",
            "This is a joint work together with my colleagues to be a bicolor villain.",
            "Bargmann and Bernard about Santa Pearsall, across key about the half of our group are from Universe Captor Queen Finland and the other half is from the University of Ghent in Belgium.",
            "And I might have as well included the word cross validation in the title, because that's what we are going to be talking about."
        ],
        [
            "More specific column, so very quick overview.",
            "So are you saying the area under a curve is popular classification mess around?",
            "Cross validation is typically used to measure your classifiers.",
            "AUC performance.",
            "If you have very little."
        ],
        [
            "Data available and you cannot afford having a separate test set, but also do you do it right because test?",
            "Some issues which do not even arise when using more standard univariate performance measures like accuracy or squared error.",
            "Best choice of pulling versus averaging which we will be will be talking about.",
            "There's some choices.",
            "Select the tenfold like the leave one note like the Leaper out cross validation and.",
            "I will first talk about this is serious and then I will present some simulation results and study the bias and variance of different estimators.",
            "So I start with preliminaries.",
            "I talked with cross validation and I got the simulation study."
        ],
        [
            "So it was something is binary classification.",
            "We have some training set of examples where we have the inputs and their corresponding labels for binary classification to possible labels.",
            "The positive and the negative class.",
            "And we want to learn some prediction function with a new.",
            "Examples will hopefully well predict whether they belong to the positive one to the negative class.",
            "And we assume it's a real valued function.",
            "So AUC has this many ways to define AUC and."
        ],
        [
            "Some of you might be more familiar with different types of definitions, but.",
            "Sorry girl, but basically it's the way I like to think about this.",
            "It's.",
            "Ranking based method of classification performance.",
            "It's probability that you are given one positive and one negative example drawn from your distribution and you are able.",
            "Are Mesa, are you able to the probability that you are able to rank them in the correct order?",
            "And this is insensitive to relative class distributions and class specific error costs, which says while it has gained popularity and it's.",
            "Much better than, for example there.",
            "Accuracy in settings where you let, let's say you have 99% of your examples from one class and one person from the other.",
            "So very imbalanced data and.",
            "It's popular gained popularity in machine learning.",
            "It's been used forever in medical decision making and microalgae studies well.",
            "We have seen already quite a few presentations where they are using measure has been used.",
            "So."
        ],
        [
            "Conditional expected AUC.",
            "So here we have.",
            "So what is this?",
            "We want to measure how well our.",
            "Classifier function F that how well it generalizes to new examples, so we are taking an expectation.",
            "Expectation or this distribution where we are drawing a positive and a negative example?",
            "And this Heaviside step function here?",
            "Will we see value one if we rank this in the correct order?",
            "That is, the positive example receives a higher value from the function F set than the negative value.",
            "If we make a tight Predix and we get value 1/2 here and we get the value 0 if we make the incorrect ordering here.",
            "And if your classifier works.",
            "Perfectly will be getting here value one under random baseline would be 0.5.",
            "If you would just throw, flip, flip a coin, or predict the same value for always.",
            "So when we are, this is conditional because we are assuming the conditioning dials on having a fixed training set that we have no some fixed training set of.",
            "50 examples, Let's say we use this to train the classifier.",
            "And we want to evaluate how well this particular classifier now works.",
            "And we of course we can almost never die again."
        ],
        [
            "Calculate this so we have to use some estimator instead and in our case we will be using cross validation based ones and how to measure whether this estimator is any good.",
            "Does it really?",
            "Approximate this conditional expected AUC well well, we use like treatments on previous studies have used the deviates another Mr Obvious.",
            "So there's tonight minus.",
            "The throw performance.",
            "And.",
            "We studied the expected value that is, the bias of estimator.",
            "So if this receives a positive value, we are being optimistic if it receives a negative value.",
            "We are being pessimistic and then the variance how stable.",
            "Our estimator is."
        ],
        [
            "And I just mentioned that the reason I'm talking about conditional expected AUC is because there's a thing called unconditional expected AUC and it's just easy to mix days.",
            "Because there's a different question we might be asking.",
            "It's not whether there is one particular prediction function I happen to have, whether it works well, but how well my learning algorithm works in general.",
            "Let's say I have this.",
            "Let's say I want to know whether the support vector machine learning algorithm works well on some domain, then I might be one more interested in taking the expectation over all possible.",
            "Training sites and wanting to know how well on average I will be doing rather than how well I happen to do with this one particular training set that happened to God.",
            "And I just want to mention that in machine learning, literate sand in quite a few statistics papers also quite common that they're talking about the unconditional performance.",
            "So if it's just.",
            "Sample one of these papers and it says something like, you know, we know this that tenfold or leave one out.",
            "Had particularly high or low bar ions.",
            "It's important to check which one they are talking about and whether for your problem.",
            "With the same problems, but.",
            "So how S?"
        ],
        [
            "Night this conditional expected a, you say, well we are using.",
            "One definition of AUC is the Wilcoxon Mann Whitney statistic that.",
            "I want to measure how good if that my prediction function nice and I have something it set of examples so I like this.",
            "Some here over all the positive and negative examples and I count how many times I rank them in the correct order.",
            "And then I normalize to get it between zero and one.",
            "There's only one problem we have here.",
            "So where do we get these examples?",
            "If I have very little data available?",
            "I have 50 examples.",
            "I have 80.",
            "I have 100.",
            "I can't really maybe afford to have a test set I could use always Terry substitutes and performance that training performance, but it's very well known that due to overfitting this will be.",
            "In many cases really over optimistic and with linear models for even with linear models for high dimensional data you will pretty much be separating your classes in the training set anyway perfectly, and seeing AUC so one.",
            "For your training set, so one way to go about this very commonly is this cross validation.",
            "So no, becomes maybe the most central part of this."
        ],
        [
            "Introduction.",
            "So about this customer Dyson when we find it here is that they are split in some way.",
            "Your training set to pieces to hold out sets.",
            "This might be overlapping or you not might allow this to overlap depending on the same you are using.",
            "But anyway somehow you split your training set into pieces.",
            "I don't need cross validation round.",
            "You use all the you took one.",
            "Take one of the holdout set, set it aside.",
            "You sold the rest of the training examples to train your classifier and use that to make predictions on that holdout set.",
            "Then you put it back.",
            "You take the next holdouts at repeat and again and again, and then you have the fold by spreadex and which you want to somehow compared to the true labels.",
            "To somehow get an estimate whether your classifier was any good or not.",
            "Unless here 2 approaches which lead to different results.",
            "To everything informally.",
            "Defined here that you just one at the time.",
            "Pick the holdout sets and the corresponding through labels.",
            "You calculate that you save, let's say, using the Wilcoxon Mann Whitney statistic for each of these, and finally you sum this all together.",
            "So you consider all the only the pairs within the folds, or you might pull them together, but this you just mix all the predictions together and calculate the AUC over all of this.",
            "Or more formally.",
            "Advocates AUC formal."
        ],
        [
            "Looks like this, so here's the normalizer and.",
            "Just like I said, we are taking a sum over all the holdout sets and it dates holdout set.",
            "We compare the Predix and for the positive and the negative examples compared to the polling, this approach has one advantage typically and one disorder bandits.",
            "The advantage would be that we are not comparing predictions from different rounds of cross validation advice this an advantage.",
            "Well, basically becausw.",
            "Those might not be strictly speaking, compareable.",
            "So easy to imagine an example of like some.",
            "But let's say I have this very well working AUC maximizing learner.",
            "Which is 1 one round of cross validation.",
            "It would always predict let's say value 0 for the negative class and value one for the positive and it works perfectly.",
            "Ranks higher the positive examples and the negative examples on the second fold.",
            "It always predicted value 27 for the negative class and 32 for the positive.",
            "And again it worked perfectly.",
            "But when you start mixing this pairs, you will get quite a bit more pessimistic view of your performance.",
            "And this approach has one clear disadvantages.",
            "For example, if I do 10 fold cross validation, I will only be using quite a small subset of all the pairs available in my training set.",
            "Which can and will lead to higher ions.",
            "The estimator."
        ],
        [
            "Then the pulled up routes.",
            "Test test 1 difference.",
            "Now there's some goes also.",
            "Overall, the holdout set pairs, so I'm mixing all the predictions together and comparing them.",
            "And this has one advantage now compared to this average formulized that I'm now able to use all the pairs and I will lower my varients, but I will be in some settings introducing bias.",
            "As we will saying, the same license and then arises the question would there be?",
            "Way to combine the good things in this two approaches and then space.",
            "Yes, there is the leaper row."
        ],
        [
            "Cross Validation is an average strategy.",
            "Which also has some similarities to pooling in the sense that we are now going through all the positive.",
            "Example negative example appears in the training set.",
            "So we define that each such pair is one of the holdout sets.",
            "So we can use all the pairs, but we never compare predictions from different cross validation rounds.",
            "But of course there is one cuts to it and it's the computational complexity.",
            "You need to train your method and to off the order of N to the power of two times.",
            "Which can be affordable if you have very little data about four bit larger datasets.",
            "You can't do it.",
            "Maybe unless you have very efficient cross validation methods about which side will say a few words at the end of presentation.",
            "If I had time.",
            "So some common cross validation methods.",
            "The end fold cross valid."
        ],
        [
            "My son most common choice for an would be 10, so I would split my data set to 10 nonoverlapping parts and do the cross validation and I can use here both averaging and pooling.",
            "Leave one note I hold use its individual example as a holdout set.",
            "I cannot do everything here.",
            "Becausw to calculate AUC you have to have.",
            "Any set of examples, both positive and negative examples, and here I have only one example, so obviously it's not possible.",
            "And leave it there out.",
            "I talked already about.",
            "And then the words."
        ],
        [
            "Our simulation studies, so we compare several different cross validation strategies.",
            "We use high dimensional low dimensional data later where there is signal present data where there is no correlation whatsoever between the training inputs and labels.",
            "So you cannot do better than random of actually not about either for AUC.",
            "To get stable result, we repeat older runs 10,000 times, where straining sets of 30 examples.",
            "And when calculating performance is best sets of 10,000 examples we study today via SMS or it's mean and variance.",
            "Are you still learning algorithms, regularised least squares and it's ranking bus on?",
            "If these are not familiar with you, I might say that this very similar to the support vector machine and the ranking support vector machine.",
            "The difference being that you replace the hinge loss with the least squares placed loss.",
            "There is one reason we are using this is that these have a closed form solution which has allowed us well Fargo list.",
            "Some of these have already previously been known for our ground cargo less we have derived.",
            "FSM cross validation methods, which means that the computational cost of evil leaper out is no worse than training one learner.",
            "We used a linear kernel and waste at the regularization parameter to one.",
            "We tried a few other parameters but it didn't really affect much the conclusion so.",
            "Somewhat arbitrary choice, of course."
        ],
        [
            "'cause there's many others we could have studied, but we wanted some polling approaches.",
            "Some average step process.",
            "The labor out and so on.",
            "And I just mentioned, by the way, that tests one.",
            "Previous related work which has done similar thing, it's the Parkers paper stratification bias in low single micro grade studies so I think it was potentially bioinformatics and.",
            "They studied on low dimensional data, pooled and average cross validation methods.",
            "And so that the pooling approach tends to have negative bias."
        ],
        [
            "Didn't study by Ryan, so some results here.",
            "10 dimensional data, no signal whatsoever.",
            "All the pulled up Rd says oh, by the way, the X&Y axis is, so an X axis, we are changing the.",
            "I mean nowadays that the relative class distribution so.",
            "This would be a balanced distribution where there are as many positive as negative examples.",
            "These are very imbalanced distributions where there's only 10% from one of the classes and everything in between.",
            "The Y axis is the mean of deviation, so if it's zero we are basically unbiased.",
            "If it's below 0, we have some negative.",
            "Bias, we're being pessimistic then.",
            "So all the polled, by the way, the balanced leave one out.",
            "I forgot to say that Parker, in his paper proposed the balancing approach.",
            "To elevate some of the problems in polling where you basically delete if you leave positive example of positive examples in your holdout set, then you will delete from the training set some negatives and vice versa.",
            "And it seems to be working becausw it has clearly less biased than the pooled.",
            "For normal leave one out the full 10 fold is still negatively biased.",
            "Abella Slaven out somewhat, and all the average strategy is it's hard to tell them apart because all the lines are overlapping.",
            "My name, this experiment.",
            "There was no signal in the present, so the true performance was also 0.5."
        ],
        [
            "Then we introduce something null and nothing really changes other than the five fold is now starting to have some pessimism, probably due to the fact that you are using always using 1/5.",
            "Not using 1/5 of your training data.",
            "And this is the interesting part where we do not know quite why this is happening, but are consistently seeing this happening.",
            "'cause now we raise the number of dimensions to 1000 and."
        ],
        [
            "Bias of the pooled estimators is basically disappearing.",
            "Consistently everywhere.",
            "So we're not quite sure why this is happening, and this has not been previously reported, as the Parker didn't really use high dimensional datasets in this study."
        ],
        [
            "And then the ranking.",
            "No, this is the signal was and actually on the five fold cross validation has no some pessimism Duda.",
            "Leaving 1/5 of the training data on you."
        ],
        [
            "Kristen, it's found the Conqueror less.",
            "The ranking method, which maximizes more directly for AUC metric.",
            "Is seems to have less bias present here."
        ],
        [
            "And finally.",
            "With single day, though, it's quite similar.",
            "And still the pulled up roads.",
            "Sale here the fold up routes is well below the average ones.",
            "And I think I'll just go to."
        ],
        [
            "A guy and then I have only one picture here, not because where I answer is less important.",
            "Quite the contrary, but becausw all the pictures look very much the same for brians.",
            "So.",
            "Basically what's happening here is that the average 5 old and average 10 fold approach have quite a lot of more Barry and stand up old ones.",
            "And then I should probably just mention that you might maybe ignore these parts.",
            "Four in balance distributions for the average tenfold, for example becausw.",
            "If you do not have enough examples from both classes, you can't really do tenfold cross validation for really imbalanced.",
            "Dataset if you do not, even with stratification you can find examples from both of the classes to each.",
            "So we are folds.",
            "So here we actually in this case.",
            "Skipped those faults where we did not have examples from both of the classes and this makes this various artificially higher.",
            "You can get a bit lower variance by doing something more sensible and such doing repeated holdout.",
            "But still it will have lots of our iron stand.",
            "Apolda proud says or the laborer."
        ],
        [
            "Out.",
            "So some conclusions.",
            "The pooled estimators were negatively biased on low dimensional data, the average tenfold and fivefold have higher brians pelipper out.",
            "Cross validation was almost unbiased and had quite competitive variance.",
            "So some recommendations for this setting where you have very little data available, paper out cross validation seems to be most robust.",
            "So maybe it should be used if it can be afforded.",
            "And pulling in our case seemed to be reliable on high dimensional data, but it showed some quite worrying behavior in the low dimensional case.",
            "And for the learning algorithms and the cross Validation Method series test is open source, software packets are garlasco.",
            "Available at this.",
            "Address and we are developing it.",
            "And.",
            "OK, thank you.",
            "How much this comparison would change if you don't graduate?",
            "REM where their offer for estimate makers?",
            "Well, good question becaused.",
            "They of course the averaging and polling is you doesn't really arise with accuracy becausw, they're basically equivale and it's only for this multivariate type performance measure value, like AUC or F score.",
            "Some days it's at the information retrieval.",
            "So I think it would be maybe quite different than I think they are out there quite a lot.",
            "Of articles about estimation for accuracy.",
            "Actually, I think it.",
            "Think their focus paper also compared accuracy and so that there is can be bias severe bias also for accuracy.",
            "And.",
            "Find on doing them with the exact date dial soap.",
            "That's right now right now.",
            "What changes can be expected?",
            "So yes, this is about AUC and also sometimes it can be expected for.",
            "Lots of data sets we have done since then.",
            "Some small.",
            "They can run Suebi increased for example to 300 training examples and we started to see that variance of the average method was not really so much of a problem anyway, but the bias did not disappear.",
            "This slowdown in some other guys.",
            "Kermit comment here.",
            "I think that the major issue is whether the loss function is edited across datasets or not.",
            "So if you if you have a classifier and then you have two test sets and then you assess the error on one and on the other and the error on the Union of these is the sum of.",
            "You know, be able see you don't have that in the more general case.",
            "For example, if you would be talking about regression.",
            "Squared error would care for that, but correlation would not have that.",
            "Yes exactly.",
            "I think that has to be studied and I was wondering whether you are approaching general enough to actually allow full.",
            "So please let's regression case and correlation rather than just Pacific Asia.",
            "I would not dare to guess at this point what I would say if I would start to look at these measures, but similar issues arise with then certainly.",
            "Strikes me that the example you gave out the classifier 01 verses 2732 is.",
            "I think in some settings almost more important than your variance on your estimates.",
            "In other words, if you're pooling versus combining all them into a single seeker, you're making different claims about whether or not the scores things coming out of your classifier are calibrated, whether they have an absolute measurement or whether they're just relative to that one training set.",
            "So in a sense, I would say yes, it's important to try to keep your head good.",
            "Theoretical properties of your score, but it's also very important to say what is it that the end user wants to accomplish that they really care if these are calibrated, and if they don't then you know then maybe just averaging across different cross validations is fine.",
            "Or if the calibration matters you know the difference is that you're going to get a less rosy picture of your performance.",
            "You try merge everything together and we actually don't care about calibrate my thought so.",
            "Relative to calibration matters a lot, maybe the AUC is not the measure.",
            "You should have chosen also possibly.",
            "Try to optimize.",
            "Not exactly big 'cause they.",
            "Oculus is actually, let's say, optimizing least squares approximation of accuracy.",
            "Which I think quite a lot of machine learning algorithms to under Anchorless is optimizing.",
            "Least squares approximation of AUC in this case.",
            "So they are different.",
            "Yeah, so they would be then closer to this are less what you would maybe expect to see with them but.",
            "In some places exactly, especially for very small and imbalanced datasets, you can get sometimes quite a bit better performance.",
            "With this I use a maximizing method selector anchorless or around Caspian working boost.",
            "I think maybe others."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good morning everybody, my name is on the Ireland I am giving you a presentation about a comparison of AUC estimators in small sample studies.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work together with my colleagues to be a bicolor villain.",
                    "label": 1
                },
                {
                    "sent": "Bargmann and Bernard about Santa Pearsall, across key about the half of our group are from Universe Captor Queen Finland and the other half is from the University of Ghent in Belgium.",
                    "label": 0
                },
                {
                    "sent": "And I might have as well included the word cross validation in the title, because that's what we are going to be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More specific column, so very quick overview.",
                    "label": 0
                },
                {
                    "sent": "So are you saying the area under a curve is popular classification mess around?",
                    "label": 1
                },
                {
                    "sent": "Cross validation is typically used to measure your classifiers.",
                    "label": 1
                },
                {
                    "sent": "AUC performance.",
                    "label": 0
                },
                {
                    "sent": "If you have very little.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data available and you cannot afford having a separate test set, but also do you do it right because test?",
                    "label": 0
                },
                {
                    "sent": "Some issues which do not even arise when using more standard univariate performance measures like accuracy or squared error.",
                    "label": 0
                },
                {
                    "sent": "Best choice of pulling versus averaging which we will be will be talking about.",
                    "label": 0
                },
                {
                    "sent": "There's some choices.",
                    "label": 0
                },
                {
                    "sent": "Select the tenfold like the leave one note like the Leaper out cross validation and.",
                    "label": 0
                },
                {
                    "sent": "I will first talk about this is serious and then I will present some simulation results and study the bias and variance of different estimators.",
                    "label": 0
                },
                {
                    "sent": "So I start with preliminaries.",
                    "label": 0
                },
                {
                    "sent": "I talked with cross validation and I got the simulation study.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it was something is binary classification.",
                    "label": 0
                },
                {
                    "sent": "We have some training set of examples where we have the inputs and their corresponding labels for binary classification to possible labels.",
                    "label": 1
                },
                {
                    "sent": "The positive and the negative class.",
                    "label": 0
                },
                {
                    "sent": "And we want to learn some prediction function with a new.",
                    "label": 1
                },
                {
                    "sent": "Examples will hopefully well predict whether they belong to the positive one to the negative class.",
                    "label": 1
                },
                {
                    "sent": "And we assume it's a real valued function.",
                    "label": 0
                },
                {
                    "sent": "So AUC has this many ways to define AUC and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of you might be more familiar with different types of definitions, but.",
                    "label": 0
                },
                {
                    "sent": "Sorry girl, but basically it's the way I like to think about this.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Ranking based method of classification performance.",
                    "label": 1
                },
                {
                    "sent": "It's probability that you are given one positive and one negative example drawn from your distribution and you are able.",
                    "label": 0
                },
                {
                    "sent": "Are Mesa, are you able to the probability that you are able to rank them in the correct order?",
                    "label": 0
                },
                {
                    "sent": "And this is insensitive to relative class distributions and class specific error costs, which says while it has gained popularity and it's.",
                    "label": 1
                },
                {
                    "sent": "Much better than, for example there.",
                    "label": 0
                },
                {
                    "sent": "Accuracy in settings where you let, let's say you have 99% of your examples from one class and one person from the other.",
                    "label": 1
                },
                {
                    "sent": "So very imbalanced data and.",
                    "label": 0
                },
                {
                    "sent": "It's popular gained popularity in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's been used forever in medical decision making and microalgae studies well.",
                    "label": 0
                },
                {
                    "sent": "We have seen already quite a few presentations where they are using measure has been used.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditional expected AUC.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "So what is this?",
                    "label": 0
                },
                {
                    "sent": "We want to measure how well our.",
                    "label": 0
                },
                {
                    "sent": "Classifier function F that how well it generalizes to new examples, so we are taking an expectation.",
                    "label": 0
                },
                {
                    "sent": "Expectation or this distribution where we are drawing a positive and a negative example?",
                    "label": 0
                },
                {
                    "sent": "And this Heaviside step function here?",
                    "label": 0
                },
                {
                    "sent": "Will we see value one if we rank this in the correct order?",
                    "label": 0
                },
                {
                    "sent": "That is, the positive example receives a higher value from the function F set than the negative value.",
                    "label": 0
                },
                {
                    "sent": "If we make a tight Predix and we get value 1/2 here and we get the value 0 if we make the incorrect ordering here.",
                    "label": 0
                },
                {
                    "sent": "And if your classifier works.",
                    "label": 0
                },
                {
                    "sent": "Perfectly will be getting here value one under random baseline would be 0.5.",
                    "label": 0
                },
                {
                    "sent": "If you would just throw, flip, flip a coin, or predict the same value for always.",
                    "label": 0
                },
                {
                    "sent": "So when we are, this is conditional because we are assuming the conditioning dials on having a fixed training set that we have no some fixed training set of.",
                    "label": 1
                },
                {
                    "sent": "50 examples, Let's say we use this to train the classifier.",
                    "label": 0
                },
                {
                    "sent": "And we want to evaluate how well this particular classifier now works.",
                    "label": 0
                },
                {
                    "sent": "And we of course we can almost never die again.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculate this so we have to use some estimator instead and in our case we will be using cross validation based ones and how to measure whether this estimator is any good.",
                    "label": 0
                },
                {
                    "sent": "Does it really?",
                    "label": 0
                },
                {
                    "sent": "Approximate this conditional expected AUC well well, we use like treatments on previous studies have used the deviates another Mr Obvious.",
                    "label": 0
                },
                {
                    "sent": "So there's tonight minus.",
                    "label": 0
                },
                {
                    "sent": "The throw performance.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We studied the expected value that is, the bias of estimator.",
                    "label": 0
                },
                {
                    "sent": "So if this receives a positive value, we are being optimistic if it receives a negative value.",
                    "label": 0
                },
                {
                    "sent": "We are being pessimistic and then the variance how stable.",
                    "label": 0
                },
                {
                    "sent": "Our estimator is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I just mentioned that the reason I'm talking about conditional expected AUC is because there's a thing called unconditional expected AUC and it's just easy to mix days.",
                    "label": 1
                },
                {
                    "sent": "Because there's a different question we might be asking.",
                    "label": 0
                },
                {
                    "sent": "It's not whether there is one particular prediction function I happen to have, whether it works well, but how well my learning algorithm works in general.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have this.",
                    "label": 0
                },
                {
                    "sent": "Let's say I want to know whether the support vector machine learning algorithm works well on some domain, then I might be one more interested in taking the expectation over all possible.",
                    "label": 0
                },
                {
                    "sent": "Training sites and wanting to know how well on average I will be doing rather than how well I happen to do with this one particular training set that happened to God.",
                    "label": 1
                },
                {
                    "sent": "And I just want to mention that in machine learning, literate sand in quite a few statistics papers also quite common that they're talking about the unconditional performance.",
                    "label": 1
                },
                {
                    "sent": "So if it's just.",
                    "label": 0
                },
                {
                    "sent": "Sample one of these papers and it says something like, you know, we know this that tenfold or leave one out.",
                    "label": 0
                },
                {
                    "sent": "Had particularly high or low bar ions.",
                    "label": 0
                },
                {
                    "sent": "It's important to check which one they are talking about and whether for your problem.",
                    "label": 0
                },
                {
                    "sent": "With the same problems, but.",
                    "label": 0
                },
                {
                    "sent": "So how S?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Night this conditional expected a, you say, well we are using.",
                    "label": 0
                },
                {
                    "sent": "One definition of AUC is the Wilcoxon Mann Whitney statistic that.",
                    "label": 0
                },
                {
                    "sent": "I want to measure how good if that my prediction function nice and I have something it set of examples so I like this.",
                    "label": 0
                },
                {
                    "sent": "Some here over all the positive and negative examples and I count how many times I rank them in the correct order.",
                    "label": 1
                },
                {
                    "sent": "And then I normalize to get it between zero and one.",
                    "label": 0
                },
                {
                    "sent": "There's only one problem we have here.",
                    "label": 0
                },
                {
                    "sent": "So where do we get these examples?",
                    "label": 0
                },
                {
                    "sent": "If I have very little data available?",
                    "label": 0
                },
                {
                    "sent": "I have 50 examples.",
                    "label": 0
                },
                {
                    "sent": "I have 80.",
                    "label": 0
                },
                {
                    "sent": "I have 100.",
                    "label": 1
                },
                {
                    "sent": "I can't really maybe afford to have a test set I could use always Terry substitutes and performance that training performance, but it's very well known that due to overfitting this will be.",
                    "label": 0
                },
                {
                    "sent": "In many cases really over optimistic and with linear models for even with linear models for high dimensional data you will pretty much be separating your classes in the training set anyway perfectly, and seeing AUC so one.",
                    "label": 0
                },
                {
                    "sent": "For your training set, so one way to go about this very commonly is this cross validation.",
                    "label": 0
                },
                {
                    "sent": "So no, becomes maybe the most central part of this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduction.",
                    "label": 0
                },
                {
                    "sent": "So about this customer Dyson when we find it here is that they are split in some way.",
                    "label": 0
                },
                {
                    "sent": "Your training set to pieces to hold out sets.",
                    "label": 0
                },
                {
                    "sent": "This might be overlapping or you not might allow this to overlap depending on the same you are using.",
                    "label": 0
                },
                {
                    "sent": "But anyway somehow you split your training set into pieces.",
                    "label": 0
                },
                {
                    "sent": "I don't need cross validation round.",
                    "label": 0
                },
                {
                    "sent": "You use all the you took one.",
                    "label": 0
                },
                {
                    "sent": "Take one of the holdout set, set it aside.",
                    "label": 0
                },
                {
                    "sent": "You sold the rest of the training examples to train your classifier and use that to make predictions on that holdout set.",
                    "label": 0
                },
                {
                    "sent": "Then you put it back.",
                    "label": 0
                },
                {
                    "sent": "You take the next holdouts at repeat and again and again, and then you have the fold by spreadex and which you want to somehow compared to the true labels.",
                    "label": 0
                },
                {
                    "sent": "To somehow get an estimate whether your classifier was any good or not.",
                    "label": 0
                },
                {
                    "sent": "Unless here 2 approaches which lead to different results.",
                    "label": 0
                },
                {
                    "sent": "To everything informally.",
                    "label": 0
                },
                {
                    "sent": "Defined here that you just one at the time.",
                    "label": 0
                },
                {
                    "sent": "Pick the holdout sets and the corresponding through labels.",
                    "label": 0
                },
                {
                    "sent": "You calculate that you save, let's say, using the Wilcoxon Mann Whitney statistic for each of these, and finally you sum this all together.",
                    "label": 0
                },
                {
                    "sent": "So you consider all the only the pairs within the folds, or you might pull them together, but this you just mix all the predictions together and calculate the AUC over all of this.",
                    "label": 0
                },
                {
                    "sent": "Or more formally.",
                    "label": 0
                },
                {
                    "sent": "Advocates AUC formal.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks like this, so here's the normalizer and.",
                    "label": 0
                },
                {
                    "sent": "Just like I said, we are taking a sum over all the holdout sets and it dates holdout set.",
                    "label": 1
                },
                {
                    "sent": "We compare the Predix and for the positive and the negative examples compared to the polling, this approach has one advantage typically and one disorder bandits.",
                    "label": 0
                },
                {
                    "sent": "The advantage would be that we are not comparing predictions from different rounds of cross validation advice this an advantage.",
                    "label": 0
                },
                {
                    "sent": "Well, basically becausw.",
                    "label": 0
                },
                {
                    "sent": "Those might not be strictly speaking, compareable.",
                    "label": 0
                },
                {
                    "sent": "So easy to imagine an example of like some.",
                    "label": 0
                },
                {
                    "sent": "But let's say I have this very well working AUC maximizing learner.",
                    "label": 0
                },
                {
                    "sent": "Which is 1 one round of cross validation.",
                    "label": 0
                },
                {
                    "sent": "It would always predict let's say value 0 for the negative class and value one for the positive and it works perfectly.",
                    "label": 0
                },
                {
                    "sent": "Ranks higher the positive examples and the negative examples on the second fold.",
                    "label": 1
                },
                {
                    "sent": "It always predicted value 27 for the negative class and 32 for the positive.",
                    "label": 0
                },
                {
                    "sent": "And again it worked perfectly.",
                    "label": 0
                },
                {
                    "sent": "But when you start mixing this pairs, you will get quite a bit more pessimistic view of your performance.",
                    "label": 0
                },
                {
                    "sent": "And this approach has one clear disadvantages.",
                    "label": 0
                },
                {
                    "sent": "For example, if I do 10 fold cross validation, I will only be using quite a small subset of all the pairs available in my training set.",
                    "label": 0
                },
                {
                    "sent": "Which can and will lead to higher ions.",
                    "label": 0
                },
                {
                    "sent": "The estimator.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the pulled up routes.",
                    "label": 0
                },
                {
                    "sent": "Test test 1 difference.",
                    "label": 0
                },
                {
                    "sent": "Now there's some goes also.",
                    "label": 0
                },
                {
                    "sent": "Overall, the holdout set pairs, so I'm mixing all the predictions together and comparing them.",
                    "label": 1
                },
                {
                    "sent": "And this has one advantage now compared to this average formulized that I'm now able to use all the pairs and I will lower my varients, but I will be in some settings introducing bias.",
                    "label": 0
                },
                {
                    "sent": "As we will saying, the same license and then arises the question would there be?",
                    "label": 0
                },
                {
                    "sent": "Way to combine the good things in this two approaches and then space.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is the leaper row.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cross Validation is an average strategy.",
                    "label": 0
                },
                {
                    "sent": "Which also has some similarities to pooling in the sense that we are now going through all the positive.",
                    "label": 1
                },
                {
                    "sent": "Example negative example appears in the training set.",
                    "label": 0
                },
                {
                    "sent": "So we define that each such pair is one of the holdout sets.",
                    "label": 0
                },
                {
                    "sent": "So we can use all the pairs, but we never compare predictions from different cross validation rounds.",
                    "label": 0
                },
                {
                    "sent": "But of course there is one cuts to it and it's the computational complexity.",
                    "label": 0
                },
                {
                    "sent": "You need to train your method and to off the order of N to the power of two times.",
                    "label": 0
                },
                {
                    "sent": "Which can be affordable if you have very little data about four bit larger datasets.",
                    "label": 0
                },
                {
                    "sent": "You can't do it.",
                    "label": 0
                },
                {
                    "sent": "Maybe unless you have very efficient cross validation methods about which side will say a few words at the end of presentation.",
                    "label": 0
                },
                {
                    "sent": "If I had time.",
                    "label": 0
                },
                {
                    "sent": "So some common cross validation methods.",
                    "label": 0
                },
                {
                    "sent": "The end fold cross valid.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My son most common choice for an would be 10, so I would split my data set to 10 nonoverlapping parts and do the cross validation and I can use here both averaging and pooling.",
                    "label": 1
                },
                {
                    "sent": "Leave one note I hold use its individual example as a holdout set.",
                    "label": 0
                },
                {
                    "sent": "I cannot do everything here.",
                    "label": 0
                },
                {
                    "sent": "Becausw to calculate AUC you have to have.",
                    "label": 1
                },
                {
                    "sent": "Any set of examples, both positive and negative examples, and here I have only one example, so obviously it's not possible.",
                    "label": 0
                },
                {
                    "sent": "And leave it there out.",
                    "label": 0
                },
                {
                    "sent": "I talked already about.",
                    "label": 0
                },
                {
                    "sent": "And then the words.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our simulation studies, so we compare several different cross validation strategies.",
                    "label": 1
                },
                {
                    "sent": "We use high dimensional low dimensional data later where there is signal present data where there is no correlation whatsoever between the training inputs and labels.",
                    "label": 0
                },
                {
                    "sent": "So you cannot do better than random of actually not about either for AUC.",
                    "label": 0
                },
                {
                    "sent": "To get stable result, we repeat older runs 10,000 times, where straining sets of 30 examples.",
                    "label": 1
                },
                {
                    "sent": "And when calculating performance is best sets of 10,000 examples we study today via SMS or it's mean and variance.",
                    "label": 1
                },
                {
                    "sent": "Are you still learning algorithms, regularised least squares and it's ranking bus on?",
                    "label": 0
                },
                {
                    "sent": "If these are not familiar with you, I might say that this very similar to the support vector machine and the ranking support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The difference being that you replace the hinge loss with the least squares placed loss.",
                    "label": 0
                },
                {
                    "sent": "There is one reason we are using this is that these have a closed form solution which has allowed us well Fargo list.",
                    "label": 0
                },
                {
                    "sent": "Some of these have already previously been known for our ground cargo less we have derived.",
                    "label": 0
                },
                {
                    "sent": "FSM cross validation methods, which means that the computational cost of evil leaper out is no worse than training one learner.",
                    "label": 0
                },
                {
                    "sent": "We used a linear kernel and waste at the regularization parameter to one.",
                    "label": 0
                },
                {
                    "sent": "We tried a few other parameters but it didn't really affect much the conclusion so.",
                    "label": 0
                },
                {
                    "sent": "Somewhat arbitrary choice, of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause there's many others we could have studied, but we wanted some polling approaches.",
                    "label": 0
                },
                {
                    "sent": "Some average step process.",
                    "label": 0
                },
                {
                    "sent": "The labor out and so on.",
                    "label": 0
                },
                {
                    "sent": "And I just mentioned, by the way, that tests one.",
                    "label": 0
                },
                {
                    "sent": "Previous related work which has done similar thing, it's the Parkers paper stratification bias in low single micro grade studies so I think it was potentially bioinformatics and.",
                    "label": 0
                },
                {
                    "sent": "They studied on low dimensional data, pooled and average cross validation methods.",
                    "label": 0
                },
                {
                    "sent": "And so that the pooling approach tends to have negative bias.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Didn't study by Ryan, so some results here.",
                    "label": 0
                },
                {
                    "sent": "10 dimensional data, no signal whatsoever.",
                    "label": 0
                },
                {
                    "sent": "All the pulled up Rd says oh, by the way, the X&Y axis is, so an X axis, we are changing the.",
                    "label": 0
                },
                {
                    "sent": "I mean nowadays that the relative class distribution so.",
                    "label": 0
                },
                {
                    "sent": "This would be a balanced distribution where there are as many positive as negative examples.",
                    "label": 0
                },
                {
                    "sent": "These are very imbalanced distributions where there's only 10% from one of the classes and everything in between.",
                    "label": 0
                },
                {
                    "sent": "The Y axis is the mean of deviation, so if it's zero we are basically unbiased.",
                    "label": 1
                },
                {
                    "sent": "If it's below 0, we have some negative.",
                    "label": 0
                },
                {
                    "sent": "Bias, we're being pessimistic then.",
                    "label": 0
                },
                {
                    "sent": "So all the polled, by the way, the balanced leave one out.",
                    "label": 0
                },
                {
                    "sent": "I forgot to say that Parker, in his paper proposed the balancing approach.",
                    "label": 1
                },
                {
                    "sent": "To elevate some of the problems in polling where you basically delete if you leave positive example of positive examples in your holdout set, then you will delete from the training set some negatives and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And it seems to be working becausw it has clearly less biased than the pooled.",
                    "label": 0
                },
                {
                    "sent": "For normal leave one out the full 10 fold is still negatively biased.",
                    "label": 0
                },
                {
                    "sent": "Abella Slaven out somewhat, and all the average strategy is it's hard to tell them apart because all the lines are overlapping.",
                    "label": 0
                },
                {
                    "sent": "My name, this experiment.",
                    "label": 0
                },
                {
                    "sent": "There was no signal in the present, so the true performance was also 0.5.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we introduce something null and nothing really changes other than the five fold is now starting to have some pessimism, probably due to the fact that you are using always using 1/5.",
                    "label": 0
                },
                {
                    "sent": "Not using 1/5 of your training data.",
                    "label": 0
                },
                {
                    "sent": "And this is the interesting part where we do not know quite why this is happening, but are consistently seeing this happening.",
                    "label": 0
                },
                {
                    "sent": "'cause now we raise the number of dimensions to 1000 and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bias of the pooled estimators is basically disappearing.",
                    "label": 0
                },
                {
                    "sent": "Consistently everywhere.",
                    "label": 0
                },
                {
                    "sent": "So we're not quite sure why this is happening, and this has not been previously reported, as the Parker didn't really use high dimensional datasets in this study.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the ranking.",
                    "label": 0
                },
                {
                    "sent": "No, this is the signal was and actually on the five fold cross validation has no some pessimism Duda.",
                    "label": 0
                },
                {
                    "sent": "Leaving 1/5 of the training data on you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kristen, it's found the Conqueror less.",
                    "label": 0
                },
                {
                    "sent": "The ranking method, which maximizes more directly for AUC metric.",
                    "label": 0
                },
                {
                    "sent": "Is seems to have less bias present here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "With single day, though, it's quite similar.",
                    "label": 0
                },
                {
                    "sent": "And still the pulled up roads.",
                    "label": 0
                },
                {
                    "sent": "Sale here the fold up routes is well below the average ones.",
                    "label": 0
                },
                {
                    "sent": "And I think I'll just go to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A guy and then I have only one picture here, not because where I answer is less important.",
                    "label": 0
                },
                {
                    "sent": "Quite the contrary, but becausw all the pictures look very much the same for brians.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically what's happening here is that the average 5 old and average 10 fold approach have quite a lot of more Barry and stand up old ones.",
                    "label": 0
                },
                {
                    "sent": "And then I should probably just mention that you might maybe ignore these parts.",
                    "label": 0
                },
                {
                    "sent": "Four in balance distributions for the average tenfold, for example becausw.",
                    "label": 0
                },
                {
                    "sent": "If you do not have enough examples from both classes, you can't really do tenfold cross validation for really imbalanced.",
                    "label": 0
                },
                {
                    "sent": "Dataset if you do not, even with stratification you can find examples from both of the classes to each.",
                    "label": 0
                },
                {
                    "sent": "So we are folds.",
                    "label": 0
                },
                {
                    "sent": "So here we actually in this case.",
                    "label": 0
                },
                {
                    "sent": "Skipped those faults where we did not have examples from both of the classes and this makes this various artificially higher.",
                    "label": 0
                },
                {
                    "sent": "You can get a bit lower variance by doing something more sensible and such doing repeated holdout.",
                    "label": 0
                },
                {
                    "sent": "But still it will have lots of our iron stand.",
                    "label": 0
                },
                {
                    "sent": "Apolda proud says or the laborer.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out.",
                    "label": 0
                },
                {
                    "sent": "So some conclusions.",
                    "label": 0
                },
                {
                    "sent": "The pooled estimators were negatively biased on low dimensional data, the average tenfold and fivefold have higher brians pelipper out.",
                    "label": 1
                },
                {
                    "sent": "Cross validation was almost unbiased and had quite competitive variance.",
                    "label": 0
                },
                {
                    "sent": "So some recommendations for this setting where you have very little data available, paper out cross validation seems to be most robust.",
                    "label": 1
                },
                {
                    "sent": "So maybe it should be used if it can be afforded.",
                    "label": 0
                },
                {
                    "sent": "And pulling in our case seemed to be reliable on high dimensional data, but it showed some quite worrying behavior in the low dimensional case.",
                    "label": 0
                },
                {
                    "sent": "And for the learning algorithms and the cross Validation Method series test is open source, software packets are garlasco.",
                    "label": 0
                },
                {
                    "sent": "Available at this.",
                    "label": 0
                },
                {
                    "sent": "Address and we are developing it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "How much this comparison would change if you don't graduate?",
                    "label": 0
                },
                {
                    "sent": "REM where their offer for estimate makers?",
                    "label": 0
                },
                {
                    "sent": "Well, good question becaused.",
                    "label": 0
                },
                {
                    "sent": "They of course the averaging and polling is you doesn't really arise with accuracy becausw, they're basically equivale and it's only for this multivariate type performance measure value, like AUC or F score.",
                    "label": 0
                },
                {
                    "sent": "Some days it's at the information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So I think it would be maybe quite different than I think they are out there quite a lot.",
                    "label": 0
                },
                {
                    "sent": "Of articles about estimation for accuracy.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think it.",
                    "label": 0
                },
                {
                    "sent": "Think their focus paper also compared accuracy and so that there is can be bias severe bias also for accuracy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Find on doing them with the exact date dial soap.",
                    "label": 0
                },
                {
                    "sent": "That's right now right now.",
                    "label": 0
                },
                {
                    "sent": "What changes can be expected?",
                    "label": 0
                },
                {
                    "sent": "So yes, this is about AUC and also sometimes it can be expected for.",
                    "label": 0
                },
                {
                    "sent": "Lots of data sets we have done since then.",
                    "label": 0
                },
                {
                    "sent": "Some small.",
                    "label": 0
                },
                {
                    "sent": "They can run Suebi increased for example to 300 training examples and we started to see that variance of the average method was not really so much of a problem anyway, but the bias did not disappear.",
                    "label": 0
                },
                {
                    "sent": "This slowdown in some other guys.",
                    "label": 0
                },
                {
                    "sent": "Kermit comment here.",
                    "label": 0
                },
                {
                    "sent": "I think that the major issue is whether the loss function is edited across datasets or not.",
                    "label": 0
                },
                {
                    "sent": "So if you if you have a classifier and then you have two test sets and then you assess the error on one and on the other and the error on the Union of these is the sum of.",
                    "label": 0
                },
                {
                    "sent": "You know, be able see you don't have that in the more general case.",
                    "label": 0
                },
                {
                    "sent": "For example, if you would be talking about regression.",
                    "label": 0
                },
                {
                    "sent": "Squared error would care for that, but correlation would not have that.",
                    "label": 0
                },
                {
                    "sent": "Yes exactly.",
                    "label": 0
                },
                {
                    "sent": "I think that has to be studied and I was wondering whether you are approaching general enough to actually allow full.",
                    "label": 0
                },
                {
                    "sent": "So please let's regression case and correlation rather than just Pacific Asia.",
                    "label": 0
                },
                {
                    "sent": "I would not dare to guess at this point what I would say if I would start to look at these measures, but similar issues arise with then certainly.",
                    "label": 0
                },
                {
                    "sent": "Strikes me that the example you gave out the classifier 01 verses 2732 is.",
                    "label": 0
                },
                {
                    "sent": "I think in some settings almost more important than your variance on your estimates.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you're pooling versus combining all them into a single seeker, you're making different claims about whether or not the scores things coming out of your classifier are calibrated, whether they have an absolute measurement or whether they're just relative to that one training set.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, I would say yes, it's important to try to keep your head good.",
                    "label": 0
                },
                {
                    "sent": "Theoretical properties of your score, but it's also very important to say what is it that the end user wants to accomplish that they really care if these are calibrated, and if they don't then you know then maybe just averaging across different cross validations is fine.",
                    "label": 0
                },
                {
                    "sent": "Or if the calibration matters you know the difference is that you're going to get a less rosy picture of your performance.",
                    "label": 0
                },
                {
                    "sent": "You try merge everything together and we actually don't care about calibrate my thought so.",
                    "label": 0
                },
                {
                    "sent": "Relative to calibration matters a lot, maybe the AUC is not the measure.",
                    "label": 0
                },
                {
                    "sent": "You should have chosen also possibly.",
                    "label": 0
                },
                {
                    "sent": "Try to optimize.",
                    "label": 0
                },
                {
                    "sent": "Not exactly big 'cause they.",
                    "label": 0
                },
                {
                    "sent": "Oculus is actually, let's say, optimizing least squares approximation of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Which I think quite a lot of machine learning algorithms to under Anchorless is optimizing.",
                    "label": 0
                },
                {
                    "sent": "Least squares approximation of AUC in this case.",
                    "label": 0
                },
                {
                    "sent": "So they are different.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they would be then closer to this are less what you would maybe expect to see with them but.",
                    "label": 0
                },
                {
                    "sent": "In some places exactly, especially for very small and imbalanced datasets, you can get sometimes quite a bit better performance.",
                    "label": 0
                },
                {
                    "sent": "With this I use a maximizing method selector anchorless or around Caspian working boost.",
                    "label": 0
                },
                {
                    "sent": "I think maybe others.",
                    "label": 0
                }
            ]
        }
    }
}