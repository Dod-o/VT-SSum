{
    "id": "jmmbzhifsk7jou4welnz4lvxjnk7auze",
    "title": "Efficient Bandit Algorithms for Online Multiclass Prediction",
    "info": {
        "author": [
            "Shai Shalev-Shwartz, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_shwartz_eba/",
    "segmentation": [
        [
            "OK, so everyone will talk about efficient bandit algorithms for online multiclass prediction.",
            "This is joint work with Sham, Kakade, and ambushed.",
            "Um?"
        ],
        [
            "So let me start with a motivating example.",
            "Consider the problem of putting advertisements on web pages.",
            "So the setting is that the user submits some query, then the system, which will be the learner in our case should place and an ad and then the user can respond to the ad.",
            "Either it clicks on the ad then we happy, or the user can ignore the ad and our goal is to maximize the number of clicks.",
            "So if we take this set up this problem and try to model it, it's not quite a common online learning setting, 'cause if the user ignores the ad then we don't know what would have been a correct add to place instead of what we did.",
            "You do not get full feedback for our predictions.",
            "On the other hand, it's not the common multi armed bandit setting becauses in multi armed bandit setting we don't have an input while here we have set of features that we can use for predicting the correct ad.",
            "So the goal for this talk is to try to do something in this online setting."
        ],
        [
            "I will start with giving more formal details about about the setting of online bandit multiclass categorization.",
            "Then, as a background, I will give the multiclass perceptron, which works for the.",
            "For the full information case and I will present an adaptation of the perception following the previous talk.",
            "It will also have a nice name, the bandit run.",
            "Present some analysis of the bandit run and then I will show you that it does something reasonable in practice and I will conclude with some.",
            "Extensions open questions, especially for the separable case."
        ],
        [
            "OK, so let's start more formally in online valid multiclass categorization.",
            "We have a sequence of rounds, and each round we first receive an input, which is just a vector of features.",
            "This will be the queries the user submit and we process it and get some features.",
            "Then we should place an ad.",
            "Placing an ad will be modeled like predicting some white hat of the, which is just a number between one of K it can be.",
            "Type of ad.",
            "We would like to.",
            "We would like to present to the user after we did this, the system or the user provides us with feedback.",
            "The feedback is just 01 feedback, it's one if the user ignores our ad, we didn't get money and it's zero if the user clicked on on the ad.",
            "The important thing which is different from the usual multiclass categorization setting is that the true correct ad is not revealed to us.",
            "We just received a 01 feedback while our predictions are multiclass."
        ],
        [
            "Similar to the Perceptron algorithm, we consider linear hypothesis.",
            "So hypothesis will be a function that Maps an input which will be the query, just the feature vector in Rd to one of the two one of the K possible ads that we have and we will use a linear hypothesis, which means that there exists some metrics W. We multiply our input vector by then because the dimension of W is K by D. We obtain a K dimensional vector as a result and then our actual prediction, which should be a number between one 2K, will just be the index of the largest element of this product.",
            "So graphically, if you look at X then WW will divide the space into cones and we will give prediction according to their position of the vector X. Oh"
        ],
        [
            "So this is standard and this is what the multiclass perceptron used for extending predictions, and so let me give a brief description of the multiclass perceptron.",
            "Each day we receive the input vector XP, and then we predict according to the metrics.",
            "WT again linear predictions as before.",
            "Then we receive the correct answer in the full information case, which we denote by writing and then we update our metrics.",
            "So the average metrics is very simple.",
            "We just add two WT and matrix UT for update and the update matrix is just matrix which is almost always zero except in two rows in the row corresponding to the correct prediction Whitey.",
            "We just place the feature Vector XT and in the role corresponding to the incorrect prediction that we that we used.",
            "Why happy we?",
            "And put the vector minus, XD and in particular, if our prediction is correct, meaning that these are actually the same row, then our prediction matrix is the zero matrix, meaning that we do not make an update at all."
        ],
        [
            "OK, so this is nice and the perception is efficient for this problem.",
            "But the problem is that the bandit setting we do not know the identity of these metrics Utah because we are blind to the value of the true correct label YT.",
            "So the main idea of the solution following the multiple multi bandit.",
            "Techniques is just to use randomization for somehow guessing sometime what this matrix update matrix duty should should look like.",
            "So let me."
        ],
        [
            "Try to explain idea of this construction.",
            "It uses the idea of exploration and exploitation, so the idea of exploration is.",
            "Instead of predicting what our models say, we should predict, which is why healthy.",
            "We get some other label which we denote by tilled YT.",
            "Now, if after our guessing we receive the correct feedback from the user, so we just guessed some why tilletti and the user tells us that this is the correct label.",
            "It clicked on this ad.",
            "Then after doing that we know that our model would have done prediction mistake an.",
            "Moreover we know what is a correct prediction because we just guessed it correctly.",
            "So in this case we can update W using the matrix.",
            "See, now we know both YT and why happy, so we know the matrix, the matrix UT and we can use the update of the perceptron.",
            "So this is very nice just by using a simple trick, we guessed correctly.",
            "What is the update of the person?"
        ],
        [
            "But the other side is that if our current model is correct, that is the model gives us the correct answer and we did this.",
            "This exploration described before we get some other tilled YT, then the user will just not click on it and we suffer loss and we do not have any knowledge about UTI.",
            "We cannot update and then we just lose lose situation.",
            "You lose lose situation because we both.",
            "A paid unit loss and we cannot improve.",
            "So in any case.",
            "So if we do this, we exploit the quality of the current model and and just predict why healthy.",
            "So the basic questions similar to other bandit setting is how to control the tradeoff between exploration and exploitation.",
            "Exploration again is around on which we should guess something else.",
            "Instead of saying what our model predicts and exploitation is using the power of our of our current model and predict just what it says."
        ],
        [
            "And the band which one is just a way to control this exploration and exploitation tradeoff.",
            "So let me briefly describe this algorithm.",
            "Like the perceptron.",
            "We start with receiving an input vector and defining why Hattie to be the prediction of this model.",
            "The current model of the perceptron using the matrix WT.",
            "Now instead of saying instead of saying the prediction why Haiti we define some probability over the entire set of possible answers, the cancers and this product.",
            "Probability will say that the prediction of the perception why Hattie with probability 1 minus gamma and will just guess randomly some other some other possible target with probability 1 / K for each possible other target, and then we randomly sample our prediction based on this probability vector and predict it receives the bandit setting bandit feedback that is just.",
            "Whether we gave a correct answer or not, and then the heart of the algorithm is an update which we denote by you."
        ],
        [
            "Field and this update is.",
            "This matrix looks very similar to their perceptions update in row YT we put XD multiplied by something and in the role of of the prediction of our model we put minus XD this is exactly like the perceptron, the changes the scaling here which is which will be just one if.",
            "Y T = 2 killed Whitey because in this case we know that this is a correct row and we need to divide it by the probability of YT.",
            "And note that this update can be implemented in the full in their limited feedback case because we were just using the predicate whether Y T = 2 till YT and we access YT directly just through this feedback."
        ],
        [
            "The interesting thing about this update is that if we take the expectation of the update matrix, where expectation is with respect to our internal randomization in the algorithm, then simple calculation shows that we just sum over the possible labels and then the probability cancel out and we obtain exactly the update matrix of the perceptron.",
            "So in expectation we're just doing what we.",
            "Could have done.",
            "We had the full information."
        ],
        [
            "OK, so this was the bandit and algorithm.",
            "Now a little bit about its analysis, like in the perceptron we use the hinge loss as a surrogate convex loss function for the 01 loss.",
            "So the hinge loss is just to penalize the margin between the.",
            "The value that the true outcome yet and the second runner up, and then it's an easy calculation to show that the hinge loss is an upper bound for 01."
        ],
        [
            "Now switch, which is what we would like to do.",
            "And just to note that in the separable case the hinges will be zero, which means that we separate all the examples with a sufficient margin."
        ],
        [
            "OK, so here are two mistake bounds.",
            "One is for the first one and the other one is for the bandit run.",
            "It looks a little bit scary but I only want you to pay attention to the left hand side in the perceptron we bound the number of mistakes in the bandit run.",
            "We bound the expected number of mistakes where again expectation is with respect to our internal randomization and to interpret this by."
        ],
        [
            "And let us look at at some special cases.",
            "So I divide 2 three possible situations corresponding to the noise level of the problem.",
            "So Big L here is cumulative loss of the competitor, which is the best you can do in hindsight.",
            "So it just determines the amount of the separability of the problem.",
            "So in the separable case, meaning that L is.",
            "Zero, we have no noise.",
            "Then the perceptron has a mistake bound which is constant.",
            "It is D where these are from Venus Norm of the competitor and in contrast the bandit run still has a mistake bound that grows with TTS and number of online rounds.",
            "So we certainly have a gap between the performance of the perceptron and their perception in the performance of the bandit run.",
            "I will get back to this point later.",
            "Now the other three genes are the low noise case in which the loss of the competitor is not too large.",
            "It is of the order of square root of T, and in this case the processor and the bandit run has similar performance after constant and lost in the noisy case where the losses even larger than the loss of the perceptron is the loss of the competitor plus the terms it grows like.",
            "Out of T while for the bandit one we obtain a lower rate of convergence.",
            "OK."
        ],
        [
            "Let's see how this penetron thing works.",
            "In practice, we took.",
            "And as Rogers data set, which has almost .7 million documents, we representative using bag of words.",
            "So we have a large dimension here and we just used for labels from this data set.",
            "Constructed it to be multiclass, simple multiclass data set, and we also used synthetic datasets.",
            "One is separable and one is non separable by just adding 5% level noise and the data the construction of the data set.",
            "The idea was to use a simple simulation of how to generate text documents.",
            "So the idea is to compare the bandit run to the perceptron, showing how much we lose from the fact that we do not."
        ],
        [
            "Have the full feedback, so in this plot you see the number of examples, which is exactly the number of online rounds.",
            "This is the X axis and the Y axis is just the error rate which is average number of mistakes.",
            "The perceptron or the bandit run makes on the sequence.",
            "So the interesting thing here to see this is on Reuters data set that the perceptron performs better, but in terms of slope.",
            "The slope of the perception and the bandit run at the end seems similar, which means that if we had some more data that bandwidth run would have reached the same accuracy as the perceptron, which is nice.",
            "But on the other hand, Reuters data set is already already large, so we really need a lot of examples in order to find the correct solution.",
            "Um?",
            "Is the next experiment is with separable data set the noise, no noise case and there we saw in the analysis that there is a gap between the person and the bandit run.",
            "The perceptron has a constant mistake bound, meaning that the error should drop like 1 / T where T is the number of online rounds, meaning that in a log log plot like we have here is a slope."
        ],
        [
            "Would be minus one and this is what we see in contrast for the bandit run, the mistake bound grows like square square root of T, meaning that the average number of mistakes should be decreasing, like 1 / sqrt T. So we expect minus half slope and we did see similar slope.",
            "So it seems that our analysis catches the true phenomenon of what's going on in pract."
        ],
        [
            "And the next experiment shows what's going on when we have 5% level of noise.",
            "So in this case the perceptron converge quite fast to solution with 10% label noise, twice the sorry, 10% error rate twice the amount of Labor noise, and the bandit run takes some more time to converge, but.",
            "But that gets to 13.",
            "13% accuracy which is not so."
        ],
        [
            "Bad.",
            "Finally, these graphs show the dependence of the of the performance of the bandit run as a function of gamma, where gamma is a parameter that controls the tradeoff between exploration and exploitation.",
            "So as you can see, the performance depends on gamma and there is some magic number that gives the best performance.",
            "So tuning this parameter is really important.",
            "OK, so this is."
        ],
        [
            "All about the experiment.",
            "So before I conclude, let me just say few words about the separable case, the noise case.",
            "So as we saw, there is a big gap between the performance of the perceptron and the performance of the bandit run and natural question is whether the this is the best we can do, or can we be closer to their performance of the perceptron?",
            "Also, in the limited feedback case and the answer the answer is we probably can do better.",
            "But not necessarily with efficient algorithms.",
            "So here is a simple algorithm and adaptation of the huffing algorithm for the bandit case.",
            "The idea here is to discretize the set of hypothesis.",
            "So this circle represents all the matrices with bounded Frobenius norm and we just take a discretization of this space.",
            "Now the algorithm works as follows.",
            "We take the majority vote of.",
            "Of given each example, we take a majority vote over the entire set of."
        ],
        [
            "Hypothesis, so an example, divide this set into into regions.",
            "Each region here corresponding to all the hypothesis that say some prediction.",
            "Now after we get the feedback, remember that we have only the bandit feedback, so we know whether we predicted correctly or incorrectly.",
            "So if our prediction is based on majority vote over the set of hypothesis.",
            "Meaning that we will choose either this prediction or this prediction, because these are the larger size of hypothesis, say some prediction.",
            "So suppose we take the blue one and then when we get feedback, we know that all the hypothesis that were incorrect can be removed from the hypothesis set."
        ],
        [
            "So we end up after each after each mistake we end up with a smaller hypothesis space.",
            "Now just analyzing this process, we obtain mistake bound which is now constant.",
            "It doesn't depend on T where T was the number of online rounds, in contrast to the bandwidth one in which we had a dependent on square root of T. On the other hand, this mistake bound depends on the dimension and also depends quadratically.",
            "On the number of classes.",
            "This can be improved by by just using the Johnson Lindenstrauss lemma, just doing random projections, and then we can eliminate the dependence on the dimension.",
            "But the price we have to pay is that we have logarithmic dependence on the number of online rounds, so it's not clear whether this is tight, but this is certainly better than the bandit run.",
            "The bandit run mistake bound on the other hand, this algorithm is inefficient, its computational complexity is exponential with the dimension so."
        ],
        [
            "Actually not.",
            "A good idea to use it in practice?",
            "OK, so just to wrap up.",
            "Let me mention some extensions to the bandit run, so a natural question is, can we generalize the ideas to the more challenging problem of label ranking, in which instead of predicting just one ad, we give ranking over the possible ads and say predicting the five top ranked ads and then the question is how to interpret the user feedback.",
            "There are some initial ideas but.",
            "Many open questions and another possible extension is other type of updates.",
            "Instead of using the perceptron, we can use multiplicative updates.",
            "We know like algorithms or the margin based updates like the passive aggressive and then one should find an extensions for the bandit feedback.",
            "Another question is do we have to use randomization for for the bandit setting or can we?",
            "Somehow derandomized our algorithm and give more deterministic predictions, and of course, what are the achievable rates.",
            "We believe that Bandit run is not tight and the question is what is achievable using efficient algorithm, in particular for the separable case, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so everyone will talk about efficient bandit algorithms for online multiclass prediction.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Sham, Kakade, and ambushed.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start with a motivating example.",
                    "label": 1
                },
                {
                    "sent": "Consider the problem of putting advertisements on web pages.",
                    "label": 1
                },
                {
                    "sent": "So the setting is that the user submits some query, then the system, which will be the learner in our case should place and an ad and then the user can respond to the ad.",
                    "label": 1
                },
                {
                    "sent": "Either it clicks on the ad then we happy, or the user can ignore the ad and our goal is to maximize the number of clicks.",
                    "label": 0
                },
                {
                    "sent": "So if we take this set up this problem and try to model it, it's not quite a common online learning setting, 'cause if the user ignores the ad then we don't know what would have been a correct add to place instead of what we did.",
                    "label": 1
                },
                {
                    "sent": "You do not get full feedback for our predictions.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it's not the common multi armed bandit setting becauses in multi armed bandit setting we don't have an input while here we have set of features that we can use for predicting the correct ad.",
                    "label": 1
                },
                {
                    "sent": "So the goal for this talk is to try to do something in this online setting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will start with giving more formal details about about the setting of online bandit multiclass categorization.",
                    "label": 1
                },
                {
                    "sent": "Then, as a background, I will give the multiclass perceptron, which works for the.",
                    "label": 0
                },
                {
                    "sent": "For the full information case and I will present an adaptation of the perception following the previous talk.",
                    "label": 0
                },
                {
                    "sent": "It will also have a nice name, the bandit run.",
                    "label": 0
                },
                {
                    "sent": "Present some analysis of the bandit run and then I will show you that it does something reasonable in practice and I will conclude with some.",
                    "label": 1
                },
                {
                    "sent": "Extensions open questions, especially for the separable case.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start more formally in online valid multiclass categorization.",
                    "label": 0
                },
                {
                    "sent": "We have a sequence of rounds, and each round we first receive an input, which is just a vector of features.",
                    "label": 0
                },
                {
                    "sent": "This will be the queries the user submit and we process it and get some features.",
                    "label": 0
                },
                {
                    "sent": "Then we should place an ad.",
                    "label": 0
                },
                {
                    "sent": "Placing an ad will be modeled like predicting some white hat of the, which is just a number between one of K it can be.",
                    "label": 0
                },
                {
                    "sent": "Type of ad.",
                    "label": 0
                },
                {
                    "sent": "We would like to.",
                    "label": 0
                },
                {
                    "sent": "We would like to present to the user after we did this, the system or the user provides us with feedback.",
                    "label": 0
                },
                {
                    "sent": "The feedback is just 01 feedback, it's one if the user ignores our ad, we didn't get money and it's zero if the user clicked on on the ad.",
                    "label": 0
                },
                {
                    "sent": "The important thing which is different from the usual multiclass categorization setting is that the true correct ad is not revealed to us.",
                    "label": 1
                },
                {
                    "sent": "We just received a 01 feedback while our predictions are multiclass.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similar to the Perceptron algorithm, we consider linear hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So hypothesis will be a function that Maps an input which will be the query, just the feature vector in Rd to one of the two one of the K possible ads that we have and we will use a linear hypothesis, which means that there exists some metrics W. We multiply our input vector by then because the dimension of W is K by D. We obtain a K dimensional vector as a result and then our actual prediction, which should be a number between one 2K, will just be the index of the largest element of this product.",
                    "label": 0
                },
                {
                    "sent": "So graphically, if you look at X then WW will divide the space into cones and we will give prediction according to their position of the vector X. Oh",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is standard and this is what the multiclass perceptron used for extending predictions, and so let me give a brief description of the multiclass perceptron.",
                    "label": 1
                },
                {
                    "sent": "Each day we receive the input vector XP, and then we predict according to the metrics.",
                    "label": 0
                },
                {
                    "sent": "WT again linear predictions as before.",
                    "label": 0
                },
                {
                    "sent": "Then we receive the correct answer in the full information case, which we denote by writing and then we update our metrics.",
                    "label": 0
                },
                {
                    "sent": "So the average metrics is very simple.",
                    "label": 0
                },
                {
                    "sent": "We just add two WT and matrix UT for update and the update matrix is just matrix which is almost always zero except in two rows in the row corresponding to the correct prediction Whitey.",
                    "label": 0
                },
                {
                    "sent": "We just place the feature Vector XT and in the role corresponding to the incorrect prediction that we that we used.",
                    "label": 0
                },
                {
                    "sent": "Why happy we?",
                    "label": 0
                },
                {
                    "sent": "And put the vector minus, XD and in particular, if our prediction is correct, meaning that these are actually the same row, then our prediction matrix is the zero matrix, meaning that we do not make an update at all.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is nice and the perception is efficient for this problem.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that the bandit setting we do not know the identity of these metrics Utah because we are blind to the value of the true correct label YT.",
                    "label": 1
                },
                {
                    "sent": "So the main idea of the solution following the multiple multi bandit.",
                    "label": 0
                },
                {
                    "sent": "Techniques is just to use randomization for somehow guessing sometime what this matrix update matrix duty should should look like.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to explain idea of this construction.",
                    "label": 0
                },
                {
                    "sent": "It uses the idea of exploration and exploitation, so the idea of exploration is.",
                    "label": 0
                },
                {
                    "sent": "Instead of predicting what our models say, we should predict, which is why healthy.",
                    "label": 0
                },
                {
                    "sent": "We get some other label which we denote by tilled YT.",
                    "label": 0
                },
                {
                    "sent": "Now, if after our guessing we receive the correct feedback from the user, so we just guessed some why tilletti and the user tells us that this is the correct label.",
                    "label": 0
                },
                {
                    "sent": "It clicked on this ad.",
                    "label": 0
                },
                {
                    "sent": "Then after doing that we know that our model would have done prediction mistake an.",
                    "label": 0
                },
                {
                    "sent": "Moreover we know what is a correct prediction because we just guessed it correctly.",
                    "label": 0
                },
                {
                    "sent": "So in this case we can update W using the matrix.",
                    "label": 1
                },
                {
                    "sent": "See, now we know both YT and why happy, so we know the matrix, the matrix UT and we can use the update of the perceptron.",
                    "label": 0
                },
                {
                    "sent": "So this is very nice just by using a simple trick, we guessed correctly.",
                    "label": 0
                },
                {
                    "sent": "What is the update of the person?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the other side is that if our current model is correct, that is the model gives us the correct answer and we did this.",
                    "label": 1
                },
                {
                    "sent": "This exploration described before we get some other tilled YT, then the user will just not click on it and we suffer loss and we do not have any knowledge about UTI.",
                    "label": 0
                },
                {
                    "sent": "We cannot update and then we just lose lose situation.",
                    "label": 0
                },
                {
                    "sent": "You lose lose situation because we both.",
                    "label": 0
                },
                {
                    "sent": "A paid unit loss and we cannot improve.",
                    "label": 0
                },
                {
                    "sent": "So in any case.",
                    "label": 1
                },
                {
                    "sent": "So if we do this, we exploit the quality of the current model and and just predict why healthy.",
                    "label": 0
                },
                {
                    "sent": "So the basic questions similar to other bandit setting is how to control the tradeoff between exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "Exploration again is around on which we should guess something else.",
                    "label": 0
                },
                {
                    "sent": "Instead of saying what our model predicts and exploitation is using the power of our of our current model and predict just what it says.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the band which one is just a way to control this exploration and exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So let me briefly describe this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Like the perceptron.",
                    "label": 0
                },
                {
                    "sent": "We start with receiving an input vector and defining why Hattie to be the prediction of this model.",
                    "label": 0
                },
                {
                    "sent": "The current model of the perceptron using the matrix WT.",
                    "label": 0
                },
                {
                    "sent": "Now instead of saying instead of saying the prediction why Haiti we define some probability over the entire set of possible answers, the cancers and this product.",
                    "label": 0
                },
                {
                    "sent": "Probability will say that the prediction of the perception why Hattie with probability 1 minus gamma and will just guess randomly some other some other possible target with probability 1 / K for each possible other target, and then we randomly sample our prediction based on this probability vector and predict it receives the bandit setting bandit feedback that is just.",
                    "label": 0
                },
                {
                    "sent": "Whether we gave a correct answer or not, and then the heart of the algorithm is an update which we denote by you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Field and this update is.",
                    "label": 0
                },
                {
                    "sent": "This matrix looks very similar to their perceptions update in row YT we put XD multiplied by something and in the role of of the prediction of our model we put minus XD this is exactly like the perceptron, the changes the scaling here which is which will be just one if.",
                    "label": 0
                },
                {
                    "sent": "Y T = 2 killed Whitey because in this case we know that this is a correct row and we need to divide it by the probability of YT.",
                    "label": 0
                },
                {
                    "sent": "And note that this update can be implemented in the full in their limited feedback case because we were just using the predicate whether Y T = 2 till YT and we access YT directly just through this feedback.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The interesting thing about this update is that if we take the expectation of the update matrix, where expectation is with respect to our internal randomization in the algorithm, then simple calculation shows that we just sum over the possible labels and then the probability cancel out and we obtain exactly the update matrix of the perceptron.",
                    "label": 0
                },
                {
                    "sent": "So in expectation we're just doing what we.",
                    "label": 0
                },
                {
                    "sent": "Could have done.",
                    "label": 0
                },
                {
                    "sent": "We had the full information.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was the bandit and algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now a little bit about its analysis, like in the perceptron we use the hinge loss as a surrogate convex loss function for the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So the hinge loss is just to penalize the margin between the.",
                    "label": 0
                },
                {
                    "sent": "The value that the true outcome yet and the second runner up, and then it's an easy calculation to show that the hinge loss is an upper bound for 01.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now switch, which is what we would like to do.",
                    "label": 0
                },
                {
                    "sent": "And just to note that in the separable case the hinges will be zero, which means that we separate all the examples with a sufficient margin.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are two mistake bounds.",
                    "label": 0
                },
                {
                    "sent": "One is for the first one and the other one is for the bandit run.",
                    "label": 0
                },
                {
                    "sent": "It looks a little bit scary but I only want you to pay attention to the left hand side in the perceptron we bound the number of mistakes in the bandit run.",
                    "label": 0
                },
                {
                    "sent": "We bound the expected number of mistakes where again expectation is with respect to our internal randomization and to interpret this by.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let us look at at some special cases.",
                    "label": 0
                },
                {
                    "sent": "So I divide 2 three possible situations corresponding to the noise level of the problem.",
                    "label": 0
                },
                {
                    "sent": "So Big L here is cumulative loss of the competitor, which is the best you can do in hindsight.",
                    "label": 0
                },
                {
                    "sent": "So it just determines the amount of the separability of the problem.",
                    "label": 0
                },
                {
                    "sent": "So in the separable case, meaning that L is.",
                    "label": 0
                },
                {
                    "sent": "Zero, we have no noise.",
                    "label": 1
                },
                {
                    "sent": "Then the perceptron has a mistake bound which is constant.",
                    "label": 0
                },
                {
                    "sent": "It is D where these are from Venus Norm of the competitor and in contrast the bandit run still has a mistake bound that grows with TTS and number of online rounds.",
                    "label": 0
                },
                {
                    "sent": "So we certainly have a gap between the performance of the perceptron and their perception in the performance of the bandit run.",
                    "label": 0
                },
                {
                    "sent": "I will get back to this point later.",
                    "label": 1
                },
                {
                    "sent": "Now the other three genes are the low noise case in which the loss of the competitor is not too large.",
                    "label": 0
                },
                {
                    "sent": "It is of the order of square root of T, and in this case the processor and the bandit run has similar performance after constant and lost in the noisy case where the losses even larger than the loss of the perceptron is the loss of the competitor plus the terms it grows like.",
                    "label": 0
                },
                {
                    "sent": "Out of T while for the bandit one we obtain a lower rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see how this penetron thing works.",
                    "label": 0
                },
                {
                    "sent": "In practice, we took.",
                    "label": 0
                },
                {
                    "sent": "And as Rogers data set, which has almost .7 million documents, we representative using bag of words.",
                    "label": 0
                },
                {
                    "sent": "So we have a large dimension here and we just used for labels from this data set.",
                    "label": 0
                },
                {
                    "sent": "Constructed it to be multiclass, simple multiclass data set, and we also used synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "One is separable and one is non separable by just adding 5% level noise and the data the construction of the data set.",
                    "label": 0
                },
                {
                    "sent": "The idea was to use a simple simulation of how to generate text documents.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to compare the bandit run to the perceptron, showing how much we lose from the fact that we do not.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have the full feedback, so in this plot you see the number of examples, which is exactly the number of online rounds.",
                    "label": 1
                },
                {
                    "sent": "This is the X axis and the Y axis is just the error rate which is average number of mistakes.",
                    "label": 1
                },
                {
                    "sent": "The perceptron or the bandit run makes on the sequence.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing here to see this is on Reuters data set that the perceptron performs better, but in terms of slope.",
                    "label": 0
                },
                {
                    "sent": "The slope of the perception and the bandit run at the end seems similar, which means that if we had some more data that bandwidth run would have reached the same accuracy as the perceptron, which is nice.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, Reuters data set is already already large, so we really need a lot of examples in order to find the correct solution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is the next experiment is with separable data set the noise, no noise case and there we saw in the analysis that there is a gap between the person and the bandit run.",
                    "label": 0
                },
                {
                    "sent": "The perceptron has a constant mistake bound, meaning that the error should drop like 1 / T where T is the number of online rounds, meaning that in a log log plot like we have here is a slope.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be minus one and this is what we see in contrast for the bandit run, the mistake bound grows like square square root of T, meaning that the average number of mistakes should be decreasing, like 1 / sqrt T. So we expect minus half slope and we did see similar slope.",
                    "label": 0
                },
                {
                    "sent": "So it seems that our analysis catches the true phenomenon of what's going on in pract.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the next experiment shows what's going on when we have 5% level of noise.",
                    "label": 0
                },
                {
                    "sent": "So in this case the perceptron converge quite fast to solution with 10% label noise, twice the sorry, 10% error rate twice the amount of Labor noise, and the bandit run takes some more time to converge, but.",
                    "label": 1
                },
                {
                    "sent": "But that gets to 13.",
                    "label": 0
                },
                {
                    "sent": "13% accuracy which is not so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bad.",
                    "label": 0
                },
                {
                    "sent": "Finally, these graphs show the dependence of the of the performance of the bandit run as a function of gamma, where gamma is a parameter that controls the tradeoff between exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the performance depends on gamma and there is some magic number that gives the best performance.",
                    "label": 0
                },
                {
                    "sent": "So tuning this parameter is really important.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All about the experiment.",
                    "label": 0
                },
                {
                    "sent": "So before I conclude, let me just say few words about the separable case, the noise case.",
                    "label": 1
                },
                {
                    "sent": "So as we saw, there is a big gap between the performance of the perceptron and the performance of the bandit run and natural question is whether the this is the best we can do, or can we be closer to their performance of the perceptron?",
                    "label": 0
                },
                {
                    "sent": "Also, in the limited feedback case and the answer the answer is we probably can do better.",
                    "label": 0
                },
                {
                    "sent": "But not necessarily with efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple algorithm and adaptation of the huffing algorithm for the bandit case.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to discretize the set of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So this circle represents all the matrices with bounded Frobenius norm and we just take a discretization of this space.",
                    "label": 0
                },
                {
                    "sent": "Now the algorithm works as follows.",
                    "label": 1
                },
                {
                    "sent": "We take the majority vote of.",
                    "label": 0
                },
                {
                    "sent": "Of given each example, we take a majority vote over the entire set of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hypothesis, so an example, divide this set into into regions.",
                    "label": 0
                },
                {
                    "sent": "Each region here corresponding to all the hypothesis that say some prediction.",
                    "label": 0
                },
                {
                    "sent": "Now after we get the feedback, remember that we have only the bandit feedback, so we know whether we predicted correctly or incorrectly.",
                    "label": 0
                },
                {
                    "sent": "So if our prediction is based on majority vote over the set of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Meaning that we will choose either this prediction or this prediction, because these are the larger size of hypothesis, say some prediction.",
                    "label": 0
                },
                {
                    "sent": "So suppose we take the blue one and then when we get feedback, we know that all the hypothesis that were incorrect can be removed from the hypothesis set.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we end up after each after each mistake we end up with a smaller hypothesis space.",
                    "label": 1
                },
                {
                    "sent": "Now just analyzing this process, we obtain mistake bound which is now constant.",
                    "label": 1
                },
                {
                    "sent": "It doesn't depend on T where T was the number of online rounds, in contrast to the bandwidth one in which we had a dependent on square root of T. On the other hand, this mistake bound depends on the dimension and also depends quadratically.",
                    "label": 0
                },
                {
                    "sent": "On the number of classes.",
                    "label": 0
                },
                {
                    "sent": "This can be improved by by just using the Johnson Lindenstrauss lemma, just doing random projections, and then we can eliminate the dependence on the dimension.",
                    "label": 1
                },
                {
                    "sent": "But the price we have to pay is that we have logarithmic dependence on the number of online rounds, so it's not clear whether this is tight, but this is certainly better than the bandit run.",
                    "label": 0
                },
                {
                    "sent": "The bandit run mistake bound on the other hand, this algorithm is inefficient, its computational complexity is exponential with the dimension so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually not.",
                    "label": 0
                },
                {
                    "sent": "A good idea to use it in practice?",
                    "label": 0
                },
                {
                    "sent": "OK, so just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "Let me mention some extensions to the bandit run, so a natural question is, can we generalize the ideas to the more challenging problem of label ranking, in which instead of predicting just one ad, we give ranking over the possible ads and say predicting the five top ranked ads and then the question is how to interpret the user feedback.",
                    "label": 0
                },
                {
                    "sent": "There are some initial ideas but.",
                    "label": 0
                },
                {
                    "sent": "Many open questions and another possible extension is other type of updates.",
                    "label": 0
                },
                {
                    "sent": "Instead of using the perceptron, we can use multiplicative updates.",
                    "label": 0
                },
                {
                    "sent": "We know like algorithms or the margin based updates like the passive aggressive and then one should find an extensions for the bandit feedback.",
                    "label": 0
                },
                {
                    "sent": "Another question is do we have to use randomization for for the bandit setting or can we?",
                    "label": 0
                },
                {
                    "sent": "Somehow derandomized our algorithm and give more deterministic predictions, and of course, what are the achievable rates.",
                    "label": 1
                },
                {
                    "sent": "We believe that Bandit run is not tight and the question is what is achievable using efficient algorithm, in particular for the separable case, thank you.",
                    "label": 1
                }
            ]
        }
    }
}