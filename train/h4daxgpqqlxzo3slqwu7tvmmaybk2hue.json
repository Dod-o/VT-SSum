{
    "id": "h4daxgpqqlxzo3slqwu7tvmmaybk2hue",
    "title": "Analyzing Non-Convex Optimization for Sparse Coding",
    "info": {
        "author": [
            "Tengyu Ma, Computer Science Department, Princeton University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_ma_sparse_coding/",
    "segmentation": [
        [
            "This is joint work with my advisor, Sanjeev, around in Princeton, and a longer from Microsoft Research and commercial from MIT.",
            "I'm talking more so."
        ],
        [
            "So it's about non convex optimization for sparse coding.",
            "So what is sparse coding?",
            "So it turns out in many cases our data has a sparse representation of yeah, a proper chosen basis, so we have a data vector Y and this this is a set of columns, each of them is a basis vector and we call this dictionary and you want to represent this vector Y as a sparse linear combination of this column's eyes.",
            "And usually it's interesting.",
            "Interesting cases when this dictionary is overcomplete, which means that you have more columns that dimension so that this columns are not linearly independent.",
            "And by sparsity I mean that the representation has K nonzeros, so it looks very similar to compress sensing.",
            "But the key difference here is that in comparison seeing the dictionary is known and designed and hand graphics and.",
            "But here we want to learn the unknown dictionary which is probably designed by the nature.",
            "And so, but certainly."
        ],
        [
            "Learn just a dictionary on the sparse vector from just one vector Y.",
            "So you have to be given a set of factors right under you represent them into a using a single dictionary, unknown dictionary and with sparse representations."
        ],
        [
            "And just some notation I'm going to call this three matrix capital Y&X.",
            "So."
        ],
        [
            "So it turns out that the original sparse coding dates back to a newer two neuro scientists notions in the field, and they were studying.",
            "They were trying to formalize the the the tree or try to give us mathematical formalization of the primal visual system called Cortex System V1, which is, in summary our brain, which is basically the first layer of the visual system.",
            "And actually, the idea is exactly the same as I described, so there are two layers, neurons and the first layer is just activated by the raw data, the light and the second layer is basically computes the special position and their synopsis that connects these neurons and which encodes the weights of the dictionary.",
            "So basically they just say this exactly the neurons they propose, that the neurons solve this sparse coding problem."
        ],
        [
            "And then it is an easy to imagine that if the visual system can can solve it and and it can and eyes are very good at recognizing objects.",
            "So you should use it for computer science, right?",
            "You should use the computer to simulate it and and it turns out that is indeed very useful for a lot of applications in machine learning, like denoising, edge detection, super resolution, deep learning.",
            "So you just do this exactly the same thing and you can use the sparse representation for.",
            "Some other things.",
            "For example, you can use them for another layer of unsupervised learning.",
            "All you can use the reconstruction so you take a times the sparse vector as the reconstruction for image, which you can hope to hope that it removes noise that you don't like all you can.",
            "There's a notion of a sparse auto Kindle, which is a director's generalization of this bus coding, and you can stack them together and get even higher level representations, and that's is used for unsupervised learning.",
            "Of deep learning.",
            "So basically different is one of the long term goal.",
            "In this line of research and we hope that understanding sparse coding could lead to something for it."
        ],
        [
            "Right so so then the question basically here is that whether we can have efficient improve algorithms for the sparse coding problem and in the next part of my talk I'm going to talk about the convex you risztics used in practice and and our result finalizing it."
        ],
        [
            "Right so.",
            "So this actually this to neuro scientist and when they proposed the sparse coding they also proposed an algorithm which is which works very well in practice.",
            "So just do the most simple thing that you can.",
            "The simplest thing you can imagine problem.",
            "So you just define the energy function to be the sum of the reconstruction error plus a term that is from for regularization for the sparsity under the exact form for that organization doesn't matter.",
            "For this talk you can think of this as 001.",
            "And so this is certainly convex, nonconvex because you have two arguments with A and X and it's not convex.",
            "But anyway, this New Scientist just tried to solve it using this following heuristic.",
            "You just do this following."
        ],
        [
            "Update the alternating update algorithms.",
            "Basically there are two parts where they alternate between the decoding step in learning step in the decoding step you fix A and you update X and in the learning step you fix any update and there are many variants about how to do this decoding step and update step.",
            "So basically what they proposed is used the main minimizer which is also nonconvex.",
            "And if user one regularization is convex but for the updated Forex and you use gradient descent.",
            "All update of a.",
            "And.",
            "Under so, but actually we are what we are going to analyze.",
            "This selected variant of this we need to replace this crazy minimizer by something simpler.",
            "But just to motivate her to be more why you want to analyze such a kind of crazy heuristic.",
            "So let me talk about about."
        ],
        [
            "The previous works so, so there are probably 3 algorithms, and there's a LP.",
            "I wouldn't buy this film at all, and the sparsity is square root and and it is only deals with a complete case when the number of columns less than the number of dimension and the sample complexes in square probabilities improved to null and the converter algorithm.",
            "So there's a 2 commentary algorithms, bio writer, and aggregate all so they can deal with over complete case.",
            "On the sample, complexity is at least M ^2, maybe slightly larger, and there's also this amazing work by bark at all and which uses the sum of squares medication, which is the modern version of lesser, and they can deal with the Indian leader sparsity, but the sample complexity is pretty high is time to at least one over epsilon, and it's a.",
            "It's a big explosion there, probably 10 should be I'm their estimate I guess.",
            "So, but I guess my point here is that this.",
            "It turns out that this approval approach is highlighted in competitive compared to the existing nonconvex approach even proposed by the new incentives.",
            "And then this risk question whether we can analyze nonconvex optimization.",
            "Are directly."
        ],
        [
            "So it turns out just let me note that it turns out that this on nonconvex approach is also was also used in the in the paper by Laura and I talked on the aggregate all for local refinement of the dictionary to learn from the Commodore algorithm, but they are.",
            "You need to start with.",
            "The dictionary is 1 / K close to optimal.",
            "I'm going to define what does mean by one work clothes, but it the point is that here in this paper we enlarge the convergence radius from 1 / K two one over log in.",
            "And and also as a set productive, we get better runtime and simple complexity.",
            "And so just before."
        ],
        [
            "Talking about the theorem more formally, let me just describe the model formally so.",
            "So basically we assume that our data is really generated from a ground truth dictionary times the ground truth, sparse representation and we cannot allow some noise.",
            "But we didn't really write it in the paper, but it should be reasonably noise tolerant.",
            "So and we assume something about the ground truth dictionary, and so it's incoherent in the sense that the two I need to pair two pairs of columns to turn high from small inner product on order of over certain on the spectral norm of the matrix shouldn't be too large.",
            "And the important thing here is that we only allow him to be little for N, so I'm can be bigger than him, but not too much bigger.",
            "And the representation, the sparsity is less than square root, and the and the will assume that the the columns ID generated from some distribution of sparse vectors.",
            "And also we need to assume that for each vector each column, so the the correlation between the entries are not too.",
            "I mean there's the correlation abounded in some sense, so so, but we don't want to assume independence, because if it is independence, maybe you can use ICA to solve it.",
            "But presumably even use sparse coding.",
            "It should be better runtime fault.",
            "Then I see."
        ],
        [
            "Anyway, so so under the assumptions in the previous license, we prove that if you are given a dictionary that is one over log enclosed ground truth.",
            "By this I mean that really is parallelism.",
            "You can is really just for each column the clean distance.",
            "Distance Euclidean distance of the difference is bounded by var log in.",
            "But of course this is invariant to the permutation of the column.",
            "So you can permute the column so that this is true.",
            "I mean, the condition is that if you can permute the column of that this is true, then it's fine.",
            "So and I know this initialization, you can show that modification of the options in the field algorithm actually converge geometry to to the ground truth up to some some systematic bias which is longer term.",
            "I can promise you, and if you further change the algorithm a little bit and you can remove this log this systematic best.",
            "But the point here is that with this simple modification is still quite of neurally possible.",
            "Actually we believe that this morning really possible, and so that's yeah so.",
            "And also we have initialization that shows that new tradition which can gives you one over log in close to a star.",
            "So now we."
        ],
        [
            "Return to this table and I guess the OK, there's a typo here, but I guess the point here is we get better sample complexity even you don't care about the non convex optimization or anything, but just at the theoretical problem we get better sample complexity, although in terms of sparsity this is still in square root in sparsity.",
            "And I guess the key point that I want to convey here is that covered here is that the the nonconvex approach seems to be powerful and actually as an interesting side note, so our new insertion procedure was also actually heavily inspired by the alternating update algorithms.",
            "So basically what your story is, we're trying to analyze the update.",
            "I will know from randomization, but we failed but with.",
            "But it turns out that we modify the very little bit and we can have this initialization, but you need to use some commentary tricks.",
            "Fight to make it provable.",
            "So."
        ],
        [
            "Oh, so now let me take some time to.",
            "Talk about the proof technique."
        ],
        [
            "So I think the so this is an automatic updates, so you are alternating between the update for ANX.",
            "It seems a little bit crazy and you have to worry about the local minimas and many things.",
            "But on the conceptually what we are doing is that we just look at the update on a a new push.",
            "Everything for the update of A and we just say OK if they converge late Saturday is fine and you can write a update for IIS.",
            "This simple form which is.",
            "X + 1 is equals S -- Y times the direction that you want to move."
        ],
        [
            "And actually for this General 1st order up the rule so so is supposed to have such a good rule and you have some desired pointed star and this tree doesn't have to be greeting actually.",
            "So for proof framework so and maybe it's not even a wedding to funny objective function, it doesn't matter.",
            "So as long as the point is that as long as.",
            "The direction the movement of direction that you make is not really correlated with the desired direction that you want to the desired direction is ASA Star star Mencius, as long as they have some nontrivial correlation in this sense, then you should converge to.",
            "Star.",
            "On geometrically and and so here.",
            "This inner product is the two matrices.",
            "The inner product just give you the two matrices and vectors and you take the inner product of the vectors and then it is forbidden.",
            "Norm is just actually just need arise.",
            "The Matrix and the problem is just increasing distance of two vectors.",
            "So I just write it in this way.",
            "So that is we don't need to change the rotation.",
            "So now you see that we need to allow a little bit the systematic back here because some.",
            "It turns out that you cannot get exactly cannot make this 04 application, but this will translate to some systematic bias at the end, so, but if you modify the algorithm, you can make this epsilon to be goes to zero, and so the picture is here.",
            "As long as you make less than 90 degree angle with the as the desired direction, you can converge they start.",
            "So."
        ],
        [
            "So so now I know this proof.",
            "This proof America it's kind of our analysis is kind of pretty.",
            "At a high level is pretty simple, so so just you replace this by a simple projection pursuit kind of algorithm and which tends to be approximately correct when you have incoherent and then you replace, you write this as a is updated by a mosquito G and then the only thing you need to do is to check G is correlated with a S -- A star.",
            "And so.",
            "Uh.",
            "So under the so OK so.",
            "Sorry.",
            "So."
        ],
        [
            "So, and it turns out to this tree is really on depends on.",
            "Only in a very non linear way, right?",
            "G is the gradient of partial grading of a of this non convex function under this axes really obtained from this thresholding function.",
            "So but so there is a lot of work to really get a good form for G, But you can do it and using many tricks and it turns out that you can get G of this form.",
            "It's really some metrics, A minus some matrix times a star plus epsilon.",
            "These two metrics are very close to identity, it's actually diagnosed.",
            "Most of our identity and then it is very clear that G should be correlated with them a -- A star and right?",
            "So then you just use the framework that is the previous slides and and it is done right so?",
            "So so, but I'm not going to detail about how to really get the this G. You need to do a little trick, but let me just show you one slides about why you hope something like this can happen so intuitively, so why you believe that G is the gradient of the nonconvex function should be correlated with MSR.",
            "So so you can do this kind of thought experiment in some sense, so we consider an unknown convex function.",
            "So basically by plugging X star into this into the discount nonconvex function, and now you remove one of the argument and you get East Star of a right.",
            "This is a convex function because so we are using basically so in some sense we are using the fact that.",
            "This E of axes, kind of biconvex, or at least it has a good reasonable form.",
            "It's not too crazy and now you get Dizzy Star which is unknown, but it's complex.",
            "And if you are close to the ground truth then X is closed Dec star and so the gradient of A at some point X which is wrong is close to the true gradient of istar at 8:00.",
            "And then because Easter is convex and you know that the gradient of the star should be corrected with the direction the desired direction.",
            "My sister.",
            "So basically the picture here is that you have this purple direction, which you don't have access to, but it is correlated with the degree induction, which is the direct the desired one and you are using this right direction to simulate the purple one in some sense.",
            "And so ultimately, updates is kind of like simulating the gradient descent on Easter, although you don't have access to the star, at least locally.",
            "I think this is the idea, but but we, but actually the proof that they really exactly use this.",
            "It's just check exactly directly East what G with the form of G is.",
            "But I think this is the right intuition.",
            "And.",
            "OK, so."
        ],
        [
            "Just some discussions, open questions so, so the first one is whether we can use this proof framework to do some other alternative updates.",
            "Algorithms for other hidden variable models, and I think the key definite technical difficulty here is that what if the decoding algorithm that has a simple closed form?",
            "So basically what we are doing is like we so we we had this problem but we kind of avoided by using a heuristic which we can kind of control, but for many other problems there is not clear whether you can have a close form decoding algorithm.",
            "So and if you don't have that, it's very hard to to calculate the form of G. And certainly this there's a limitation of this proof technique.",
            "It only limited to local convergence, although I mean we can make it less local as as before, but still limited to local convergence.",
            "So interesting question is whether we can analyzing the global convergence from random research mission of the non convex optimization so it turns out the randomization actually works reasonably well, not perfect, but it's pretty good so and I guess the last question is whether we can get.",
            "More practical algorithms for on initialization beyond K = 2 ^2.",
            "Text."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with my advisor, Sanjeev, around in Princeton, and a longer from Microsoft Research and commercial from MIT.",
                    "label": 0
                },
                {
                    "sent": "I'm talking more so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's about non convex optimization for sparse coding.",
                    "label": 1
                },
                {
                    "sent": "So what is sparse coding?",
                    "label": 0
                },
                {
                    "sent": "So it turns out in many cases our data has a sparse representation of yeah, a proper chosen basis, so we have a data vector Y and this this is a set of columns, each of them is a basis vector and we call this dictionary and you want to represent this vector Y as a sparse linear combination of this column's eyes.",
                    "label": 0
                },
                {
                    "sent": "And usually it's interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting cases when this dictionary is overcomplete, which means that you have more columns that dimension so that this columns are not linearly independent.",
                    "label": 0
                },
                {
                    "sent": "And by sparsity I mean that the representation has K nonzeros, so it looks very similar to compress sensing.",
                    "label": 1
                },
                {
                    "sent": "But the key difference here is that in comparison seeing the dictionary is known and designed and hand graphics and.",
                    "label": 0
                },
                {
                    "sent": "But here we want to learn the unknown dictionary which is probably designed by the nature.",
                    "label": 0
                },
                {
                    "sent": "And so, but certainly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn just a dictionary on the sparse vector from just one vector Y.",
                    "label": 0
                },
                {
                    "sent": "So you have to be given a set of factors right under you represent them into a using a single dictionary, unknown dictionary and with sparse representations.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just some notation I'm going to call this three matrix capital Y&X.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that the original sparse coding dates back to a newer two neuro scientists notions in the field, and they were studying.",
                    "label": 1
                },
                {
                    "sent": "They were trying to formalize the the the tree or try to give us mathematical formalization of the primal visual system called Cortex System V1, which is, in summary our brain, which is basically the first layer of the visual system.",
                    "label": 0
                },
                {
                    "sent": "And actually, the idea is exactly the same as I described, so there are two layers, neurons and the first layer is just activated by the raw data, the light and the second layer is basically computes the special position and their synopsis that connects these neurons and which encodes the weights of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So basically they just say this exactly the neurons they propose, that the neurons solve this sparse coding problem.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then it is an easy to imagine that if the visual system can can solve it and and it can and eyes are very good at recognizing objects.",
                    "label": 0
                },
                {
                    "sent": "So you should use it for computer science, right?",
                    "label": 0
                },
                {
                    "sent": "You should use the computer to simulate it and and it turns out that is indeed very useful for a lot of applications in machine learning, like denoising, edge detection, super resolution, deep learning.",
                    "label": 1
                },
                {
                    "sent": "So you just do this exactly the same thing and you can use the sparse representation for.",
                    "label": 0
                },
                {
                    "sent": "Some other things.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use them for another layer of unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "All you can use the reconstruction so you take a times the sparse vector as the reconstruction for image, which you can hope to hope that it removes noise that you don't like all you can.",
                    "label": 0
                },
                {
                    "sent": "There's a notion of a sparse auto Kindle, which is a director's generalization of this bus coding, and you can stack them together and get even higher level representations, and that's is used for unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Of deep learning.",
                    "label": 0
                },
                {
                    "sent": "So basically different is one of the long term goal.",
                    "label": 0
                },
                {
                    "sent": "In this line of research and we hope that understanding sparse coding could lead to something for it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so so then the question basically here is that whether we can have efficient improve algorithms for the sparse coding problem and in the next part of my talk I'm going to talk about the convex you risztics used in practice and and our result finalizing it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So this actually this to neuro scientist and when they proposed the sparse coding they also proposed an algorithm which is which works very well in practice.",
                    "label": 0
                },
                {
                    "sent": "So just do the most simple thing that you can.",
                    "label": 0
                },
                {
                    "sent": "The simplest thing you can imagine problem.",
                    "label": 0
                },
                {
                    "sent": "So you just define the energy function to be the sum of the reconstruction error plus a term that is from for regularization for the sparsity under the exact form for that organization doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "For this talk you can think of this as 001.",
                    "label": 0
                },
                {
                    "sent": "And so this is certainly convex, nonconvex because you have two arguments with A and X and it's not convex.",
                    "label": 0
                },
                {
                    "sent": "But anyway, this New Scientist just tried to solve it using this following heuristic.",
                    "label": 0
                },
                {
                    "sent": "You just do this following.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Update the alternating update algorithms.",
                    "label": 1
                },
                {
                    "sent": "Basically there are two parts where they alternate between the decoding step in learning step in the decoding step you fix A and you update X and in the learning step you fix any update and there are many variants about how to do this decoding step and update step.",
                    "label": 0
                },
                {
                    "sent": "So basically what they proposed is used the main minimizer which is also nonconvex.",
                    "label": 0
                },
                {
                    "sent": "And if user one regularization is convex but for the updated Forex and you use gradient descent.",
                    "label": 0
                },
                {
                    "sent": "All update of a.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Under so, but actually we are what we are going to analyze.",
                    "label": 0
                },
                {
                    "sent": "This selected variant of this we need to replace this crazy minimizer by something simpler.",
                    "label": 0
                },
                {
                    "sent": "But just to motivate her to be more why you want to analyze such a kind of crazy heuristic.",
                    "label": 0
                },
                {
                    "sent": "So let me talk about about.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The previous works so, so there are probably 3 algorithms, and there's a LP.",
                    "label": 1
                },
                {
                    "sent": "I wouldn't buy this film at all, and the sparsity is square root and and it is only deals with a complete case when the number of columns less than the number of dimension and the sample complexes in square probabilities improved to null and the converter algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there's a 2 commentary algorithms, bio writer, and aggregate all so they can deal with over complete case.",
                    "label": 0
                },
                {
                    "sent": "On the sample, complexity is at least M ^2, maybe slightly larger, and there's also this amazing work by bark at all and which uses the sum of squares medication, which is the modern version of lesser, and they can deal with the Indian leader sparsity, but the sample complexity is pretty high is time to at least one over epsilon, and it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a big explosion there, probably 10 should be I'm their estimate I guess.",
                    "label": 0
                },
                {
                    "sent": "So, but I guess my point here is that this.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this approval approach is highlighted in competitive compared to the existing nonconvex approach even proposed by the new incentives.",
                    "label": 0
                },
                {
                    "sent": "And then this risk question whether we can analyze nonconvex optimization.",
                    "label": 1
                },
                {
                    "sent": "Are directly.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out just let me note that it turns out that this on nonconvex approach is also was also used in the in the paper by Laura and I talked on the aggregate all for local refinement of the dictionary to learn from the Commodore algorithm, but they are.",
                    "label": 0
                },
                {
                    "sent": "You need to start with.",
                    "label": 0
                },
                {
                    "sent": "The dictionary is 1 / K close to optimal.",
                    "label": 0
                },
                {
                    "sent": "I'm going to define what does mean by one work clothes, but it the point is that here in this paper we enlarge the convergence radius from 1 / K two one over log in.",
                    "label": 0
                },
                {
                    "sent": "And and also as a set productive, we get better runtime and simple complexity.",
                    "label": 0
                },
                {
                    "sent": "And so just before.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking about the theorem more formally, let me just describe the model formally so.",
                    "label": 0
                },
                {
                    "sent": "So basically we assume that our data is really generated from a ground truth dictionary times the ground truth, sparse representation and we cannot allow some noise.",
                    "label": 0
                },
                {
                    "sent": "But we didn't really write it in the paper, but it should be reasonably noise tolerant.",
                    "label": 0
                },
                {
                    "sent": "So and we assume something about the ground truth dictionary, and so it's incoherent in the sense that the two I need to pair two pairs of columns to turn high from small inner product on order of over certain on the spectral norm of the matrix shouldn't be too large.",
                    "label": 0
                },
                {
                    "sent": "And the important thing here is that we only allow him to be little for N, so I'm can be bigger than him, but not too much bigger.",
                    "label": 0
                },
                {
                    "sent": "And the representation, the sparsity is less than square root, and the and the will assume that the the columns ID generated from some distribution of sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "And also we need to assume that for each vector each column, so the the correlation between the entries are not too.",
                    "label": 0
                },
                {
                    "sent": "I mean there's the correlation abounded in some sense, so so, but we don't want to assume independence, because if it is independence, maybe you can use ICA to solve it.",
                    "label": 0
                },
                {
                    "sent": "But presumably even use sparse coding.",
                    "label": 0
                },
                {
                    "sent": "It should be better runtime fault.",
                    "label": 0
                },
                {
                    "sent": "Then I see.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so so under the assumptions in the previous license, we prove that if you are given a dictionary that is one over log enclosed ground truth.",
                    "label": 0
                },
                {
                    "sent": "By this I mean that really is parallelism.",
                    "label": 0
                },
                {
                    "sent": "You can is really just for each column the clean distance.",
                    "label": 0
                },
                {
                    "sent": "Distance Euclidean distance of the difference is bounded by var log in.",
                    "label": 0
                },
                {
                    "sent": "But of course this is invariant to the permutation of the column.",
                    "label": 0
                },
                {
                    "sent": "So you can permute the column so that this is true.",
                    "label": 0
                },
                {
                    "sent": "I mean, the condition is that if you can permute the column of that this is true, then it's fine.",
                    "label": 0
                },
                {
                    "sent": "So and I know this initialization, you can show that modification of the options in the field algorithm actually converge geometry to to the ground truth up to some some systematic bias which is longer term.",
                    "label": 0
                },
                {
                    "sent": "I can promise you, and if you further change the algorithm a little bit and you can remove this log this systematic best.",
                    "label": 0
                },
                {
                    "sent": "But the point here is that with this simple modification is still quite of neurally possible.",
                    "label": 0
                },
                {
                    "sent": "Actually we believe that this morning really possible, and so that's yeah so.",
                    "label": 0
                },
                {
                    "sent": "And also we have initialization that shows that new tradition which can gives you one over log in close to a star.",
                    "label": 0
                },
                {
                    "sent": "So now we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Return to this table and I guess the OK, there's a typo here, but I guess the point here is we get better sample complexity even you don't care about the non convex optimization or anything, but just at the theoretical problem we get better sample complexity, although in terms of sparsity this is still in square root in sparsity.",
                    "label": 0
                },
                {
                    "sent": "And I guess the key point that I want to convey here is that covered here is that the the nonconvex approach seems to be powerful and actually as an interesting side note, so our new insertion procedure was also actually heavily inspired by the alternating update algorithms.",
                    "label": 1
                },
                {
                    "sent": "So basically what your story is, we're trying to analyze the update.",
                    "label": 0
                },
                {
                    "sent": "I will know from randomization, but we failed but with.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that we modify the very little bit and we can have this initialization, but you need to use some commentary tricks.",
                    "label": 0
                },
                {
                    "sent": "Fight to make it provable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, so now let me take some time to.",
                    "label": 0
                },
                {
                    "sent": "Talk about the proof technique.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think the so this is an automatic updates, so you are alternating between the update for ANX.",
                    "label": 0
                },
                {
                    "sent": "It seems a little bit crazy and you have to worry about the local minimas and many things.",
                    "label": 0
                },
                {
                    "sent": "But on the conceptually what we are doing is that we just look at the update on a a new push.",
                    "label": 0
                },
                {
                    "sent": "Everything for the update of A and we just say OK if they converge late Saturday is fine and you can write a update for IIS.",
                    "label": 0
                },
                {
                    "sent": "This simple form which is.",
                    "label": 0
                },
                {
                    "sent": "X + 1 is equals S -- Y times the direction that you want to move.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually for this General 1st order up the rule so so is supposed to have such a good rule and you have some desired pointed star and this tree doesn't have to be greeting actually.",
                    "label": 1
                },
                {
                    "sent": "So for proof framework so and maybe it's not even a wedding to funny objective function, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So as long as the point is that as long as.",
                    "label": 0
                },
                {
                    "sent": "The direction the movement of direction that you make is not really correlated with the desired direction that you want to the desired direction is ASA Star star Mencius, as long as they have some nontrivial correlation in this sense, then you should converge to.",
                    "label": 0
                },
                {
                    "sent": "Star.",
                    "label": 0
                },
                {
                    "sent": "On geometrically and and so here.",
                    "label": 0
                },
                {
                    "sent": "This inner product is the two matrices.",
                    "label": 0
                },
                {
                    "sent": "The inner product just give you the two matrices and vectors and you take the inner product of the vectors and then it is forbidden.",
                    "label": 0
                },
                {
                    "sent": "Norm is just actually just need arise.",
                    "label": 0
                },
                {
                    "sent": "The Matrix and the problem is just increasing distance of two vectors.",
                    "label": 0
                },
                {
                    "sent": "So I just write it in this way.",
                    "label": 0
                },
                {
                    "sent": "So that is we don't need to change the rotation.",
                    "label": 0
                },
                {
                    "sent": "So now you see that we need to allow a little bit the systematic back here because some.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you cannot get exactly cannot make this 04 application, but this will translate to some systematic bias at the end, so, but if you modify the algorithm, you can make this epsilon to be goes to zero, and so the picture is here.",
                    "label": 0
                },
                {
                    "sent": "As long as you make less than 90 degree angle with the as the desired direction, you can converge they start.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so now I know this proof.",
                    "label": 0
                },
                {
                    "sent": "This proof America it's kind of our analysis is kind of pretty.",
                    "label": 0
                },
                {
                    "sent": "At a high level is pretty simple, so so just you replace this by a simple projection pursuit kind of algorithm and which tends to be approximately correct when you have incoherent and then you replace, you write this as a is updated by a mosquito G and then the only thing you need to do is to check G is correlated with a S -- A star.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So under the so OK so.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and it turns out to this tree is really on depends on.",
                    "label": 0
                },
                {
                    "sent": "Only in a very non linear way, right?",
                    "label": 0
                },
                {
                    "sent": "G is the gradient of partial grading of a of this non convex function under this axes really obtained from this thresholding function.",
                    "label": 0
                },
                {
                    "sent": "So but so there is a lot of work to really get a good form for G, But you can do it and using many tricks and it turns out that you can get G of this form.",
                    "label": 0
                },
                {
                    "sent": "It's really some metrics, A minus some matrix times a star plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "These two metrics are very close to identity, it's actually diagnosed.",
                    "label": 0
                },
                {
                    "sent": "Most of our identity and then it is very clear that G should be correlated with them a -- A star and right?",
                    "label": 0
                },
                {
                    "sent": "So then you just use the framework that is the previous slides and and it is done right so?",
                    "label": 0
                },
                {
                    "sent": "So so, but I'm not going to detail about how to really get the this G. You need to do a little trick, but let me just show you one slides about why you hope something like this can happen so intuitively, so why you believe that G is the gradient of the nonconvex function should be correlated with MSR.",
                    "label": 0
                },
                {
                    "sent": "So so you can do this kind of thought experiment in some sense, so we consider an unknown convex function.",
                    "label": 0
                },
                {
                    "sent": "So basically by plugging X star into this into the discount nonconvex function, and now you remove one of the argument and you get East Star of a right.",
                    "label": 0
                },
                {
                    "sent": "This is a convex function because so we are using basically so in some sense we are using the fact that.",
                    "label": 0
                },
                {
                    "sent": "This E of axes, kind of biconvex, or at least it has a good reasonable form.",
                    "label": 0
                },
                {
                    "sent": "It's not too crazy and now you get Dizzy Star which is unknown, but it's complex.",
                    "label": 0
                },
                {
                    "sent": "And if you are close to the ground truth then X is closed Dec star and so the gradient of A at some point X which is wrong is close to the true gradient of istar at 8:00.",
                    "label": 0
                },
                {
                    "sent": "And then because Easter is convex and you know that the gradient of the star should be corrected with the direction the desired direction.",
                    "label": 0
                },
                {
                    "sent": "My sister.",
                    "label": 0
                },
                {
                    "sent": "So basically the picture here is that you have this purple direction, which you don't have access to, but it is correlated with the degree induction, which is the direct the desired one and you are using this right direction to simulate the purple one in some sense.",
                    "label": 0
                },
                {
                    "sent": "And so ultimately, updates is kind of like simulating the gradient descent on Easter, although you don't have access to the star, at least locally.",
                    "label": 0
                },
                {
                    "sent": "I think this is the idea, but but we, but actually the proof that they really exactly use this.",
                    "label": 0
                },
                {
                    "sent": "It's just check exactly directly East what G with the form of G is.",
                    "label": 0
                },
                {
                    "sent": "But I think this is the right intuition.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some discussions, open questions so, so the first one is whether we can use this proof framework to do some other alternative updates.",
                    "label": 0
                },
                {
                    "sent": "Algorithms for other hidden variable models, and I think the key definite technical difficulty here is that what if the decoding algorithm that has a simple closed form?",
                    "label": 0
                },
                {
                    "sent": "So basically what we are doing is like we so we we had this problem but we kind of avoided by using a heuristic which we can kind of control, but for many other problems there is not clear whether you can have a close form decoding algorithm.",
                    "label": 0
                },
                {
                    "sent": "So and if you don't have that, it's very hard to to calculate the form of G. And certainly this there's a limitation of this proof technique.",
                    "label": 0
                },
                {
                    "sent": "It only limited to local convergence, although I mean we can make it less local as as before, but still limited to local convergence.",
                    "label": 0
                },
                {
                    "sent": "So interesting question is whether we can analyzing the global convergence from random research mission of the non convex optimization so it turns out the randomization actually works reasonably well, not perfect, but it's pretty good so and I guess the last question is whether we can get.",
                    "label": 0
                },
                {
                    "sent": "More practical algorithms for on initialization beyond K = 2 ^2.",
                    "label": 0
                },
                {
                    "sent": "Text.",
                    "label": 0
                }
            ]
        }
    }
}