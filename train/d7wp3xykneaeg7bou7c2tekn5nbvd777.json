{
    "id": "d7wp3xykneaeg7bou7c2tekn5nbvd777",
    "title": "The Unreasonable Effectivness Of Deep Learning",
    "info": {
        "author": [
            "Yann LeCun, Computer Science Department, New York University (NYU)"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Information Theory",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_lecun_deep_learning/",
    "segmentation": [
        [
            "So let me say a few words about Facebook first, because this is sort of primitive response to a lot of questions I'm getting, so I've been at Facebook since December and I'm creating a essentially what is an AI research lab, which is sort of the first research organization on Facebook.",
            "We're currently about 30 people, about 30 scientists and engineers, probably about 1/3 of the people at Facebook, Facebook, AI research now are are kind of more on the engineering side of things, sort of building, machine learning, infrastructure, and.",
            "Software and things of that type.",
            "Handling data collecting data, things like that type and then about 2 third 4 for now, research scientists spending the spectrum between theory, principles, algorithms and applications to various fields, mostly computer vision and natural language processing.",
            "And we are setting up ourselves as a real research organization, somewhat similar to what you would have expected of Bell Labs in the old days.",
            "Microsoft Research and some to some extent, except that there is a sort of tighter link to the rest of the company in the sense that it's much easier in a company like Facebook to get things out the door out of the labs into into products.",
            "But we're really research organization, so we're publishing papers.",
            "We're participating in conferences like this, and workshops like this and.",
            "And have projects that are very long term and theoretical in nature."
        ],
        [
            "So in Facebook for now we are other than the fact that we are hiring.",
            "So currently we have about 30 people and probably over the next few years will grow to about 100 people and will most likely open a branch in Europe.",
            "Not entirely clear where yet, but somewhere in Europe.",
            "And we'll see with the other goes OK so so much for boring stuff.",
            "I'm going to talk about deep learning and surprisingly, and I usually there's sort of a standard talk.",
            "I've been giving, which you know the last year or so, which is sort of a rehash of talks.",
            "I was giving 20 years ago, basically explained explaining to people with back problem.",
            "Commercial Nets are an really this kind of more interesting stuff that we've been doing in the last 20 years, and I thought for this audience I would.",
            "I would go very quickly on sort of, you know, the stuff that everybody is using an industry which is.",
            "Really technology that was around 20 years ago and then talk more about data representation.",
            "Unsupervised learning which is topics that got people excited.",
            "About deep learning, but really in practice or not yet used, but that's where the interesting research is taking place really.",
            "So OK.",
            "So let's start talking quickly about about competition, and so I'm going to really quickly.",
            "So the history of."
        ],
        [
            "So.",
            "Pattern recognition is you build your feature extractor, you stick your classifier, you train your classifier and you're done.",
            "And that's going back to the 50s and the perceptron in more recent years.",
            "Sort of the mid 2000s.",
            "If you want or earlier for speech recognition, you had this kind of architecture where you have a fixed front end feature extractor at the other front end you have."
        ],
        [
            "Unsupervised learning to extract mid level features.",
            "There's a lot of kind of standard pipeline of object recognition in computer vision that are based on this on this idea, and for this using you use techniques like vector quantization of sparse coding.",
            "So techniques are very familiar to a lot of people in this, in this audience, and then you could have some sort of aggregation of features and you know supervised running on top of it, and perhaps a post processing.",
            "So that's sort of the standard pipeline for both object recognition and speech recognition for you until about two years ago, and that's been essentially displaced."
        ],
        [
            "By by deep learning.",
            "So deep learning is the idea of stacking a whole bunch of parameterized function an have them all be trained in this case in the supervised manner.",
            "But ultimately what we want is."
        ],
        [
            "Try and not just the part that those feature extraction and classification, but also trained the post processor, which generally is some sort of graphical model, some sort of objects that can sort of produce an output, not by just computing and output, but by kind of minimizing some function, or by maximizing the likelihood or something of that type.",
            "So there's a bit of reasoning in it.",
            "Then you've also optimization of some kind, and we'd like to sort of integrate the training of this with the training of everything else.",
            "And conceptually we've known how to do this.",
            "About 20 years.",
            "In fact, some of the check reading systems are built at AT&T about 20 years ago were actually integrating training of feature extraction, classification, and the post processing in the form of what we now call the conditional random field, but really was was done at the time, so we know how to do.",
            "Conceptually, there's a lot of practical issues on how to actually sort of implement that, and I used a different color here than the Redwood to symbolize the fact that eventually would like to integrate.",
            "Not just to use supervised learning, which is what people have used mostly in practical applications of deep learning so far, but also integrate unsupervised learning.",
            "The reason being that there is a lot of tasks for which we have tons and tons of data, but relatively speaking we have small amount of labeled data.",
            "So that's the case for video, for example is the case.",
            "Also for things like semantic segmentation, where the vision system is meant to label every pixel in an image, so the datasets like this where we have tons of.",
            "Unlabeled into another huge amount of labeled data.",
            "And there's a sense that there's a lot of kind of regularity's in the world that can be captured with unsupervised learning.",
            "So the second half of my talk will be about unsupervised running.",
            "And also."
        ],
        [
            "We would like to look for some sort of learning rule that sort of can be used seamlessly for unsupervised or supervised learning.",
            "We already have learning algorithms that have that property, but they don't work.",
            "So this machine algorithms for example or similar things where the fact that his supervisor unsupervised entirely depends on how you use it and doesn't depend on the algorithm itself.",
            "Now that's a very nice property to have, except both machines don't really work.",
            "They'll scale, they don't really work OK, so.",
            "The cute that's it.",
            "Also, the request sampling ahead sampling.",
            "Actually, I'm allergic to sampling.",
            "So, so the idea of having hierarchical architecture is of course is due to the fact that the world is compositional.",
            "I heard someone said a joke, I don't know where it comes from, which is the world is compositional or there is a God?",
            "So my confession is on the former.",
            "And so you have low level features that are local and it's useful to extract local local features in an image because there are high correlations between groups of neighboring neighboring pixels.",
            "And so that's the idea of compositionality, where you have kind of blocks of things that can happen and not everything in the world can happen, so that's what makes the world intelligible, and as you grow up the layers, you build features that are kind of compositions of lower level features.",
            "And you sort of have to increase the number of possible feature types as you go up the layers, But then possibly decrease the spatial resolution or resolution in other dimensions and sort of eliminates irrelevant variability in the signal.",
            "So images you know you start from things like edges.",
            "These are learned features in a commercial net.",
            "This was visualization produced by my colleague Rob Fergus an at the level you have oriented edges and then a couple of layers up, you end up having after training.",
            "Having motive detectors and then couple it goes up.",
            "You have like parts of objects that are being detected and then all the way up you have objects, entire objects.",
            "This kind of hierarchy."
        ],
        [
            "Of course exists for all kinds of signals, not just pixels, but also text, speech and things of that type, but."
        ],
        [
            "We're deep learning, is trying to do is learn the problem we need to solve with deep learning is learning representations of the world and this is all about high dimensional data representation.",
            "So how do we?",
            "How do we learn representations of the world by just observing it?",
            "That's the question we need to answer the question.",
            "Or scientists have been."
        ],
        [
            "In themselves also for a long time.",
            "How does the brain does that do that?",
            "So the visual cortex is hierarchical.",
            "We've known that for quite quite some time, the notion of areas in the visual cortex are not exactly equivalent to the notion of layers in the neural net, but sort of.",
            "And it's the fact that our neural Nets are feedforward is a gross simplification of really the architecture that.",
            "Biology uses which has all kinds of feedback and lateral connections and things of that type.",
            "But there is this sort of sense that for very fast perception, the visual cortex uses an essentially feedforward process because it just goes too fast for any kind of feedback to really have an impact.",
            "There's something that we're not modeling in our in our feedforward recognition system, which is the visual cortex is really has two different pathways.",
            "One that is used to recognize objects."
        ],
        [
            "Independently position an other one that's used to localize objects independently of where they are, so the second one.",
            "Helps you when you are navigating and avoiding obstacles, grabbing things, look at localising things whereas the first one is just for identifying.",
            "Those of course talk to each other.",
            "So I'm going to sort of paraphrase some of the things that near Lawrence talked about this morning."
        ],
        [
            "Which is the fact that.",
            "A good way to kind of."
        ],
        [
            "Build a stage that ends up extracting good features is to have have you do a sequence of two operations, and that's really a very old idea.",
            "This is not certainly not something that was recently.",
            "Arrived at from the deep learning side.",
            "It's a very old idea for parameterising functions.",
            "You have an input vector.",
            "You have a first layer that essentially is a nonlinear mapping to a higher dimensional space.",
            "So we expanded dimension in a non linear way.",
            "It's very important that this be nonlinear and the role of this of this stage is to this layer is traditionally essentially break apart regions of the space that are semantically dissimilar that need to be classified as different things.",
            "And so you break things apart and then you have another layer that reduces the dimension that essentially re aggregates things that are semantically similar.",
            "That happened to have been broken apart by the first stage.",
            "OK, so break things apart nonlinearly and then glue them back together and so you have this sort of expansion reduction which is similar to the sort of basic module near Lawrence was talking about this morning where you know he would have, you know radial basis functions here, or Gaussian bumps or whatever, but you can put whatever you want."
        ],
        [
            "OK, so we're going to build our architectures by essentially stacking stages of this sort of basic sequence of operation.",
            "First, there is a optional normalization.",
            "In fact, in practice, we don't have that very often.",
            "Then we'll have a linear dimensionality expansion in a form of a filter bank for images or for sort of array signals in general, but think of it as just a matrix.",
            "Then we'll have a pointwise nonlinearity, and there's all kinds of arguments for why it's just fine.",
            "It's nonlinearity is point wise at this point.",
            "Jean Boone and Stephen Miller have arguments about this.",
            "And then there is feature pulling, which is places role of aggregating features over space over kind of variabilities of various kinds, but sort of reducing the dimension so very often in things like convolutional Nets, but also in other types of networks.",
            "This feature pulling is fixed.",
            "There's no learned parameters in this, all alone parameters are in the filter banks.",
            "So you take those stage and you repeat it multiple times.",
            "You stick a classifier on top, but in fact the classifier is nothing more than another stage of those filter banks in our linearity actually.",
            "So don't think of this as kind of a different as different in nature from this is just another one of those.",
            "The proving operation is very often nowadays a Max operation, although people have been doing other things and the nonlinearity very often.",
            "If there is 1, here is a Max of X and zero, so it's halfway rectification.",
            "There's a technique by Ian Goodfellow and his collaborators at in Toronto, where they actually do they do away with the point wasn't linearity here and the pulling directly doesn't Max over multiple feature types and over space.",
            "And that seems to actually work quite well.",
            "They call that Max out."
        ],
        [
            "So that's what a.",
            "Deep neural net sort of modern day practical deep neural net looks like it's a stack of linear operation interspersed with Max.",
            "Max is essentially, and you can think of Max.",
            "That's going to switch that switches between zero and X if it's a value or between the different values, and taking the biggest one if it's a Max pooling."
        ],
        [
            "So.",
            "OK, you train this with backdrop, that's just application of Chinua centrally to compute sub."
        ],
        [
            "Audience the functions you have to."
        ],
        [
            "Guys, when you do this end up being very highly nonconvex, they have tons and tons and tons of several points.",
            "In fact, the commentarial number of several points and local minima there is a picture that emerged that a lot of us have known for a long time intuitively but not really sort of being able to put a finger on it, which is that.",
            "You never have local minima problems with deep learning systems.",
            "Whenever you try them, you always get solutions that have the same kind of performance.",
            "Those systems we know have a community really large number of local minima, but whatever local minimum you find.",
            "Is always kind of the same type of performance.",
            "They're always always different, but they always give you the same kind of performance.",
            "So you know there's different types of non convexity.",
            "There is nasty non convexity and then complexity you get with deep learning systems, but you could leave the oversized is not nasty in the sense that it doesn't matter which minimum you find.",
            "But that creates lots of lots of several points, so whenever you have two local minima, there's several points in between, or perhaps multiple because it's in high dimension and that creates kind of a landscape that's extremely complicated that none of us really begins to understand, so I hope these people in the audience.",
            "Here we kind of.",
            "Get interested in this question.",
            "This is too complicated for me.",
            "So right?",
            "So let's think about deep network with Max with values or Max.",
            "What does he do?",
            "So let's say it has a single output.",
            "You could write it as.",
            "You could take a single path in that network.",
            "And write the influence of this path on the output.",
            "OK and this path.",
            "So essentially you take an input, you multiply it by the first wait and then by the second wait and then by the 3rd weights and you accumulate that in the output.",
            "And if a path is active, which means if all the units in there are in their linear region in the value, then this this this path contributes linearly to the output.",
            "And so you do the some of her old path of all the contributions and you get the input output relations.",
            "But what's going to happen is that some of the paths are going to be turned off because the corresponding unit is in the flat spot or the Max switch, so didn't select it.",
            "OK, so essentially it is kind of funny, sort of switched path integral, right?",
            "Figure out all the path from all the inputs to the output, and that's your input output function.",
            "But you have those switching very binary variables that indicate whether a particular path is on or off, and of course.",
            "This depends on the input as well as on the parameters.",
            "OK, so that's what it looks like now.",
            "If you stick A cause."
        ],
        [
            "Function on this, particularly stick a hinge cost function.",
            "For example to do learning.",
            "So now you have a loss function that you want to minimize for learning.",
            "Again, one more of the same.",
            "It's a Max of zero and something else.",
            "It's a hinge.",
            "Or one in this case, and the desired output enters multiplicatively in this in this function, but in the end you can sort of wrap this into some sort of coefficient at the end you get a form that's like this, so the loss function is a polynomial in W whose degree is the number of layers.",
            "And then in front of this you have coefficients that depend on the input that depend on the weights and depends on the desired output.",
            "That sort of determines which of those terms which of those paths are turned on alright.",
            "But essentially it's kind of a switch polynomial if you want.",
            "In W. Since the operations are maxes you can you can show that the function is continuous.",
            "It's not differentiable, but it's continuous.",
            "It's got things, but it's continuous, so it's a continuous piecewise polynomial with switch and partially random coefficients random because the input is comes from some distribution."
        ],
        [
            "So it turns out a lot is known about functions of this form.",
            "If you assume that those coefficients are Gaussians, Gaussian distributed in a Gaussian way.",
            "If you assume that the WS owners here, there's a lot of results from essentially from the spin class people or random matrix theory people.",
            "On, you know the sort of properties and distribution of critical points of functions of this type.",
            "When the when the coefficients are Gaussian.",
            "In fact this is work."
        ],
        [
            "By my colleague Robin, who set the quantity is the director of the Institute.",
            "So what those results?",
            "So those results don't directly apply because our coefficients have this kind of funny property that they depend on the data and they are switched and the other binary.",
            "But there is some randomness to them which you could assume is Gaussian.",
            "So what's interesting about those results is that they say that the distribution of critical points which are subtle points, minima, Maxima of those functions.",
            "Putting on this fear is such that there is a if you do the distribution of minima or the.",
            "You know, over energy levels, right?",
            "So on the X axis, you have sort of energy levels, value, objective function and you count how many minima are there at any particular value or benefit values of the cost function.",
            "You get this kind of histogram where there is sort of what's called the bulk, where pretty much all of the minima are there within a very narrow region of energy, and then you have a tiny exponentially small number of ones that are slightly lower energy and then they another exponentially small numbers that have higher energy.",
            "So these are the nasty local minima.",
            "These other local minima you actually want and these other ones that you might want, except they're very rare, and there's no way you're going to find them.",
            "So if you just do stochastic gradient descent without any particular trick, you're going to find one of those big guys here.",
            "When one of those guys are numerous and these guys is very, very unlikely, you're going to get trapped in those because there are sort of exponentially less probable.",
            "There's just a lot less of them.",
            "Anan these guys.",
            "You essentially have no chance of finding them, but if you did find them you probably would overfit, so it doesn't matter."
        ],
        [
            "Um?",
            "OK, so much.",
            "That's hand WAVY theory.",
            "And I have to admit that I."
        ],
        [
            "I don't know something that's behind this, but OK. Like, I mean, the techniques to find those distributions.",
            "It's really not my field, but it's cool.",
            "It's cool results that I hope we could transfer, you know, modify to sort of apply to this particular situation.",
            "So again, if anyone is interested.",
            "So that's again a special case of the pipeline I talked about earlier where you have filter banks so you take an image, you apply a bunch of filters to it.",
            "Those filters will be learned.",
            "The coefficients of the filter will, and so you do this convolutionally you get this result you pass or through a nonlinearity.",
            "In this particular case is the sigmoid, but we use values and then there is the pooling, which in the case of commercial net or simple methods is purely spatial and the research sampling so that reduces the dependency of the representation on the precise position of.",
            "Distinctive features on the input and you repeat the process so this guy again is result of applying a bunch of filters to these things and adding up the results, passing the result.",
            "Who nonlinearity and each of these guys use different sets of filters we call each of those feature Maps and then there is pulling again and then you know multiple layers of that.",
            "This."
        ],
        [
            "An idea that you know comes from from neuroscience, essentially from classic work in the 60s."
        ],
        [
            "And then there were models little bit along those lines in the 70s, the new computer and I for kushima in the early 80s, but he didn't quite have the right learning algorithm for that.",
            "So using backdrop to train those architectures I've been working on this since the late 80s, basically."
        ],
        [
            "And that works really well."
        ],
        [
            "So this is kind of an old commercial net that was used to recognize characters.",
            "In fact, instead of this I'm just going to show you."
        ],
        [
            "The video of the."
        ],
        [
            "Is me young.",
            "This is video from the system we built was in, was built in 1990.",
            "One 1992 the video I think is from 1993.",
            "This is actually my phone number at Bell Labs.",
            "And these are the PC with a camera, TV camera and hit a hitaki the.",
            "Images is acquired and then the system recognizes it and this is a 46 PC, so I'm sure some of you are not actually born when the 46 was around, so the 46 PC is not powerful enough to run a commercial net.",
            "This kind of speed.",
            "So we uses DSP board so at the time actually built chips and boards and so we use the NTSB 32 board that was capable of the amazing speed of 20 mega flops.",
            "It was like it was ridiculous, it was like.",
            "Incredible in floating point.",
            "This is Donnie Henderson.",
            "What are the engineers at Bell Labs?",
            "And this is rich Howard who is the director of the whole lab at Glendale.",
            "And so this was really cool demo.",
            "We showed this to a executive at AT&T and he said, oh that's very, very interesting.",
            "So now can you actually add those numbers?",
            "That tells you how clueless the business people were at ATT.",
            "Fortunately, this is not the case at Facebook, OK?"
        ],
        [
            "So OK, so we can have sort of brute force approaches to object recognition.",
            "Which or to kind of you can use those things to not recognize SQL objects with multiple objects and that consists."
        ],
        [
            "Basically, applying the convolutions onto a larger image and replicating the outputs and so it's a very powerful technique because it allows you to train a small accomplishment on the small window and then replicate it over a large image and use it to do object detection localization you know, etc.",
            "So originally we developed this for reading cursive handwriting or handwriting handwritten words."
        ],
        [
            "So here are some examples of that.",
            "So here you have one of those covenants.",
            "It's leaking in multiple characters at the time.",
            "An each output is basically a different instance of the continent that looks at us.",
            "You know, a different window on the input, but it's really not multiple conditions.",
            "This one commercial net and you know the structure is such that it gives you an answer for every window 3232 window on the input shifted by every four pixels.",
            "So in the end there is.",
            "There is a response for every window, and then you're gonna have to use a little.",
            "Markov model event machine to pull out the correct answer and it's doing a pretty good job at that.",
            "So this is from."
        ],
        [
            "1996 or so 1997.",
            "So there's the cool thing about this is that there is no need for explicit explicit segmentation, and it's for character like this.",
            "It's easy to segment, but for this is a fun one.",
            "But you know when you're going to when you deal with natural images, it's very hard to just find the object, so you want something that's able to implicitly kind of separate separate objects from the background.",
            "So that's."
        ],
        [
            "People to use."
        ],
        [
            "Format for object recognition and I've been working on this for a long time, but sort of having."
        ],
        [
            "A hard time convincing the computer vision community that this was really worth looking at until recently, so we've known for it."
        ],
        [
            "In time that we could do object recognition for."
        ],
        [
            "It should be simple natural images like you know things like Rd signs and numbers and simple objects in complex backgrounds and fiset action and you know various other things like this.",
            "That's my grandparents waiting."
        ],
        [
            "Then and then in the."
        ],
        [
            "In 2000, Computer Vision Community started to realize that you needed datasets to try to test your methods, so they came up with datasets.",
            "So this is the Catholic one data set."
        ],
        [
            "And it had the advantage of existing, but the disadvantage of being very small.",
            "30 training samples for Category one category.",
            "So it's just a very small amount of training data.",
            "And when you train a large enough neural net commercial net that it can see the object at reasonable size, this commercial that ends up having you know millions of parameters and you have 3000 training samples.",
            "Performance really is crap.",
            "I mean it's not horrible, but it's bad.",
            "In fact it's really bad.",
            "So the state of the art.",
            "In 2006, was about 65% correct recognition on this data set using a standard pipeline with SIFT the low level."
        ],
        [
            "Sparse coding or it's more like K means at the mid level and then SVM at the top with some sort of pyramid pooling.",
            "And if you use a straight convolutional net or the same type we use for CR in the 80s and 90s you get something like 30% correct?",
            "So it's twice the error rate.",
            "Half the recognition rate actually.",
            "But now we discovered something.",
            "If you instead of using an average pooling with a sigmoid function, you use a absolute value rectification for the nonlinearity.",
            "Or you use Max pooling.",
            "Then all of a sudden the accuracy is 60%, it doubles the accuracy.",
            "So say there's something about rectification that's really magical here.",
            "That's what it captures, something.",
            "Maybe about images may be specific about images.",
            "If you combine this with contrast organization and you do a little bit of unsupervised learning, you get to 65%, which was the state of the art in 2006.",
            "But unfortunately we got this results in 2008, by which time the you know other pipeline was at 7075%.",
            "But it was OK.",
            "It was also doing my lab so.",
            "So that was kind of disappointing."
        ],
        [
            "But then of course two things happened.",
            "One we started having really large datasets like the image net data set which has 1.5 million training samples and the second thing that happened is we got our hands on."
        ],
        [
            "Super fast GPU's and so well that causes us to get interested in building very large company format strain on GPU's for weeks at a time and see what performance we got and the first people to really kind of have a good implementation on GPU of those large networks where was.",
            "The Toronto people Jeff and turn into the students that Alex Crisci and Eds discover, and so they're very clever implementation of those things on GPU's which have been sort of reproduced multiple times in various contexts since then, and in 2012 they want the image net competition.",
            "So again, this 1.5 million training samples 1000 categories.",
            "Of you know, general categories of objects with, you know, breeds of dogs and you know stuff like that.",
            "And."
        ],
        [
            "Not really good performance.",
            "I'm sure a lot of you are aware of this.",
            "They got 15% error top five, which means the correct answer is no."
        ],
        [
            "In the top five among the 1015% of the time when computing approaches, the ones that were getting 7585% on Capital One basically do better than about 25%, so that was sort of a watershed for the computer vision community because, you know, it was the first time that the problem that's going to front of center, front, and center you know in the interest of the Community.",
            "With solved with a big margin by such a method, you know lot of time they were going to competitive for some tasks, which I'll talk about later, like image segmentation.",
            "There were actually quite quite good, but it was easy to ignore it.",
            "And for physician also it was good for a long time."
        ],
        [
            "So within a few months, Google deployed Sir."
        ],
        [
            "This is based on this for image tagging.",
            "Baidu did the same very quickly.",
            "Facebook has been doing it for a few months so there is a commercial net based image tagging it.",
            "Facebook, Google and Baidu and now other companies as well actually.",
            "So we have our own version of this commercial Nets.",
            "We got this radio new few months after that or people we didn't participate in the original competition.",
            "This is called the over feat system.",
            "If you type over feet and were you in Google you'll find it and there's a piece of code you can just download and run.",
            "You show it an image from a camera or from a file and it will.",
            "Produce either the classification on the output or the feature vector at any level you want, and you can use this as input to whatever system you want to do.",
            "This works a little bit better than this project is.",
            "Network is a successor to the student of mine who."
        ],
        [
            "On this last year, Pearson may join Google few months ago and he participated in the latest image net competition.",
            "The 2014 and his team won essentially all three of the competitions.",
            "Classification detection and localization.",
            "Although in the case of localization only if they use extra extra data.",
            "So kind of the the the call is Google or NET which is kind of playing world on words and it's the performance of this is pretty amazing."
        ],
        [
            "So these are the filters that allow."
        ],
        [
            "For the system me."
        ],
        [
            "Skip over some boring details and go to how can we do localization of objects or localization is."
        ],
        [
            "We can use this idea of replicating a network over overfield, so essentially we take the image and we scale it down to multiple sizes and then we run the convolutional net with the sliding window overall scale.",
            "So here we only have one and here we have a number of different frames.",
            "Those are the yellow squares if you want, and for each of those squares we get an answer whether you know what's the object and what's the score inside of that square.",
            "But what we do, in addition to this, so we get sort of heat Maps for each category, and then we can do some sort of voting mechanism.",
            "Not that simple suppression bubble out to figure out what the dominant object in the image, but if you want to localize.",
            "What you need to do is also predict where the bounding boxes.",
            "So in fact here what I plotted is the showing is the bounding boxes.",
            "The predicted bounding boxes that are produced by the system.",
            "So this system is looking at a particular window in the image and is trained to classify what it sees in the window.",
            "The domain object in the window, but it's also trying to produce a prediction for numbers that indicate the position of the bounding box for the whole object that is seeing so it could be looking at just the head of that bear with the square by this size.",
            "But if you see the head and the neck, it knows where the bear is, so it predicts where the bounding box, so there should be what you see here are the all the predictions of all the windows, regardless of where they are in the.",
            "Original image and so there it's easy to do a kind of a voting mechanism to figure out that."
        ],
        [
            "The dominant object here is a bear.",
            "That's how you do localization."
        ],
        [
            "So that works really well for animals of various kinds.",
            "Bird."
        ],
        [
            "You know, so you can use this for detection as well, so detection is exactly the same system except now you allow the system to produce multiple answers per image.",
            "Basically the higher scoring objects that don't overlap too much.",
            "So here you have two versus the ground truth, and that's the answer of the system.",
            "Here is another one with Burrito works really well with burritos."
        ],
        [
            "Here's where it makes a mistake.",
            "So the correct is a person can monitor and it says mini skirt.",
            "And doesn't detect a person apparently hypnotized better miniskirt.",
            "Sexist comments were going to say."
        ],
        [
            "Uh.",
            "So the system works very well, although at the time of the of the competition we actually got this is metrical mean average precision.",
            "We were at 19% and there's two teams from University of Amsterdam and NEC also using, Network and slightly better than us and then within two weeks after we had a little more time for tuning we got 24% and then three months later Roscoe Sheet from Berkeley.",
            "Also using your commercial net and a few other tweaks with sort of attention got 34% and now the Google Annette technique is at 40 something.",
            "So it's progressing really quick."
        ],
        [
            "So that's."
        ],
        [
            "And the Google Annette again is a direct descendant of that system 'cause it's the same guy basically."
        ],
        [
            "You are so many.",
            "So that you know it's detection with people and sunglasses and babies and.",
            "Dogs and French horns and.",
            "Whatever this is."
        ],
        [
            "Other mistakes too."
        ],
        [
            "He thinks that's a Corkscrew that's actually a snake and it doesn't like.",
            "Green strawberries, but I don't like green strawberries either, so."
        ],
        [
            "OK, so one thing that people have done with this is that they've used this as generic feature extractors.",
            "They've essentially taken the overfeed system or other similar systems, chop off the last layer and then re trying the last layer to do another task.",
            "So use the this sort of a feature extractor.",
            "So with this you can win competition."
        ],
        [
            "Distinguish cats from dogs.",
            "Which means if you are cat lover, dog lover, you can eliminate one or the other from your Facebook feed.",
            "When it suggested."
        ],
        [
            "This is Facebook I said oh if we take out the cats like half of the traffic goes so."
        ],
        [
            "Um, there is this team at KTH."
        ],
        [
            "That tried a lot of different computer vision tasks datasets by using essentially the overheat features dropping off the last layer, retraining, and they got pretty much state of the art performance on just about everything they tried or slightly below slightly above, except for one.",
            "I was told yesterday to really insist on this, the Oxford team said the it's actually not working very well for the Oxford data set, 'cause the protocol the user was not appropriate.",
            "So this is kind of a.",
            "An application to classify to recognize buildings from the Oxford campus.",
            "So it's kind of specific object recognition if you want.",
            "So for that it works OK, but not great.",
            "You can use this to do similarity match."
        ],
        [
            "So there's some."
        ],
        [
            "We used to call Siamese networks.",
            "Now we can group this in the.",
            "Name of embedding.",
            "Basic idea is you get 2 neural Nets, 2 private function.",
            "It doesn't matter where they are that are controlled by the same parameters.",
            "The same weights you feed 2 images.",
            "If the two images are semantically identical.",
            "So for example 2 pictures of the same person.",
            "You train the system to produce vectors that are nearby on the output.",
            "You don't specify the vector, you just said the two actors that are nearby, and then if you show 2 images of different persons semantically different, then you push those two vectors apart until they are at a certain distance.",
            "We call this technique so there's several loss functions you can use for this.",
            "There's one that we like this kind of margin based loss function that we."
        ],
        [
            "Doctor Lim that means dimensionality reduction by learning an invariant mapping one of those cute acronyms.",
            "Sorry about that, but."
        ],
        [
            "It's been used for various applications and there's one thing we like at Facebook which is embedding.",
            "Pieces of.",
            "Media, whatever they are.",
            "Text, images, videos, people into vector space.",
            "Such that distance in that vector space order products correspond to similarity of some sort.",
            "OK, so for example, you have a vector for a user and a vector for a piece of.",
            "Like a piece of news or an image is a dot product between the two vectors.",
            "After mapping is large then that means this person is likely to be interested by that piece of content.",
            "If you have a piece of text on an image and they matched, that means probably that corresponds to the image.",
            "Or it could be a good query for the image or the image could be a good answer to the text.",
            "So there are things like this or you know the text similar if it's to text.",
            "So there's a lot of things you can do with embedding and one of the things we're trying to do is basically embed the world.",
            "And one of the things we have been using this embedding techniques for is face to face recognition.",
            "So this is a work I'm not involved in, although it's taking place at Facebook in my lab.",
            "Since the flagman and Ming Yang mostly at Facebook AI research, and they've used a commercial net to do do face recognition, there's a bit of alignment of the face before hand.",
            "That's actually very sophisticated, and then there is a training of this thing as a classifier, and then there is another phase of sort of refining the representation by using embedding.",
            "Also, using criteria to produce embeddings that are small dimension and mostly binary so that you can store them on very compact.",
            "Very compact."
        ],
        [
            "And so you can search through.",
            "You know, hundreds of millions of faces very, very quickly, and on standard datasets, probably datasets like the liberal face in the world.",
            "This system basically performs at human level essentially.",
            "So."
        ],
        [
            "This is really exciting.",
            "Another very recent, so this year."
        ],
        [
            "Face it are the first people to hear about this.",
            "Or at least to see the slides we participated.",
            "So I had a visiting student from from Slovenia called you Richemont R and he.",
            "It sort of fell into a habit of winning competitions.",
            "He was visiting my lab and say, you know, what competition can win, say well, why don't you try this stereo stuff?",
            "So it's a key data set which is a large data set of so ground truth stereo images with LIDAR data and he used a conversation that the training content to learn the similarity function between patches.",
            "That's what you need to do is terrible matching.",
            "Need to find a Patch in the left image that corresponds to a particular Patch in the right image, and when you find it you have a disparity and from that you can assume it depth.",
            "And so we trained a pretty large commercial net, about 600,000 parameters to basically tell it the similarity between two patches using the ground truth from the data set, and he managed to actually beat the record.",
            "So he's got 2.6% error.",
            "There's some error measure of what percentage of points are.",
            "Have disparity that's more more than three pixels away from the real one.",
            "And the runner ups is a team from T in Toronto.",
            "It's been around for awhile and so that's very impressive.",
            "These are examples of."
        ],
        [
            "It's in the state of, so this is you get stereo with relatively small baseline for those images, black and white.",
            "This is from a car driving around and you get those really beautiful.",
            "That's images, so he's going to talk about this at the workshop at ECC this Saturday basically was invited today to come go to the CV Saturday because you just published results yesterday on the leaderboard, so."
        ],
        [
            "And things like this you can do like what it was."
        ],
        [
            "Automation stuff which I may or may not show, but before I I switch topic, let me show you a demo of the system.",
            "So.",
            "Let's see I might have to.",
            "OK, so this is a I have a webcam here on this grabbing images and I can point this camera at various things.",
            "So let me point it at.",
            "This computer and it says notebook.",
            "OK, that's about right.",
            "Space bar.",
            "Computer keyboard.",
            "Yeah, that's about right too.",
            "How about this?",
            "Monitor, yeah, that's a monitor.",
            "Computer keyboard.",
            "That's a mouse, yeah, what about this?",
            "Remote control mouse joystick yeah, one of those.",
            "How about this?",
            "IPod, OK, that's actually a Samsung Galaxy S4.",
            "It turns out I have two phones and they're both Samsung Galaxy S4.",
            "This one is a set of phone which is sort of more correct.",
            "OK, but it's black so it's a cell phone.",
            "When it's white.",
            "It's an iPod.",
            "Um?",
            "So I stole a little piece of history here.",
            "Prezzo Lake Warren plate.",
            "OK, so it knows it's, you know, it knows it's kind of a history of some kind.",
            "You know we can eat it.",
            "Console me OK because you know Eggnog played, you know this food?",
            "What about this?",
            "What about all?",
            "OK, that's what about it?",
            "There's no question coffee mug.",
            "Yeah, that's about right.",
            "How about this?",
            "Electric switch, not really.",
            "Maybe it doesn't know about.",
            "Actually I don't know if he knows about telephones.",
            "Remote control always got keys on it right about this.",
            "Two, it was loop sunglass, yeah?",
            "There is no underclass, no.",
            "OK, that's a handheld computer, of course, because it's a, you know another sense on widget.",
            "I don't actually have a.",
            "A. OK, let's try this.",
            "Central.",
            "Running shoe that's better.",
            "Interframe it properly loafer, yeah, it's good enough.",
            "You know this is true so you get the idea.",
            "Works OK.",
            "I don't know why you would say to this actually.",
            "Library.",
            "OK, well it needs to focus.",
            "Monitor, yeah, that's close enough.",
            "Cash machine no.",
            "So.",
            "Spot alright, let's do so right.",
            "You get the idea.",
            "Let me show you another similar demo.",
            "But this one I can.",
            "How to interpret this probabilities well so so this is trained with cross entropy loss on it's like the last way you can think of it as multinomial logistic regression.",
            "So you get you get probability estimates out.",
            "Think of them as scores, really.",
            "So calibrated scores?",
            "Really, that's what they are.",
            "OK, so this one.",
            "This."
        ],
        [
            "I can train so I can point the camera at something or nothing like this.",
            "And then click learn.",
            "Oops, need to wait for it to focus.",
            "And it's kind of learned this.",
            "So if I put it someplace else, it doesn't know.",
            "But if I put it back in at this need to focus, here we go right now I can put it at the left.",
            "Right side of the room or left side of the room depending on your point of view.",
            "So you are category B.",
            "And you guys the other side are category C. And the number you see here on the right side is the number of samples that I used to train.",
            "So left side is C. Thread size B.",
            "The screen is here A is low light, so that's why it's hard time.",
            "I'm going to use myself, but I look very different from the rest.",
            "Of those pictures.",
            "And you know, I could use whatever random.",
            "Random thing here.",
            "OK, so we have this.",
            "Project or whatever it is.",
            "Jan we have.",
            "There's still screen down there.",
            "We have the right side of the room.",
            "Now we have this side of the room.",
            "And.",
            "And you know, I could train this with a number of categories with just a few examples.",
            "And what that shows you is that.",
            "So first I should tell you how this works, right?",
            "So you take the Internet, you drop off the last layer and you retrain the last layer using essentially what amounts to nearest neighbor's window classifier.",
            "Essentially, by just storing templates.",
            "So every time I hit the button that stores the template and it compares it.",
            "Um?",
            "Right, so let me go back to.",
            "The talking and loops or the direction and.",
            "So you got a few more things, so there is a really impressive application of this two image segmentation which I'm not going to show because I want to tell you about unsupervised running.",
            "And since this is a 3 hour talk, I have two hours left or so so.",
            "OK.",
            "Right, OK?"
        ],
        [
            "Answer me."
        ],
        [
            "OK, So what is unsupervised learning?",
            "That's the kind of question we might ask."
        ],
        [
            "And."
        ],
        [
            "To me, the way I like to see unsupervised learning is sort of capturing dependencies between variables and essentially one possible way of capturing dependencies between between variables observed variables is to find, so it's very likely that the data that comes to you is going to be living in a manifold or some surface really mention surface in the ambient space.",
            "And if you find if you can train a function that tells you whenever you show it a vector, whether it's on any folder off the manifold, or even better if it tells you which direction to move.",
            "Towards to get closer to the manifold.",
            "Then you've basically solved unsupervised learning because we've captured the dependencies between the variables OK and the one of the best ways to do this is through an energy function that tells you how far you are from the manifold.",
            "OK, a contrast function."
        ],
        [
            "Some kind.",
            "So imagine your observations are those blue little Sears here.",
            "These are two.",
            "You know, this is a kind of a two different learning algorithms or two different punctuation and learning algorithm running that sort of changes and energy surface so that the blue beads have lower energy than everything else.",
            "So this guy here starts flat and then the sum of those areas get pushed up so that the blue beads get have within a groove.",
            "This guy.",
            "The energy here is designed in such a way that it already has a groove and you just need to move it around so that it's at the right place.",
            "So that's what I call energy, baserunning, and and you know, probably."
        ],
        [
            "Density estimation is kind of a sort of special kids with this.",
            "If you want, you can sort of formulate it in those terms, but but this technique includes things that are not probabilistic in nature, and there's a little more general, so they kind of seven different methods to strategies to shape the energy function in the right way in such a way that data points have lower energy than things outside the manifold of high density essentially.",
            "So the first one is you build a machine in such a way that the volume of low energy stuff is constant.",
            "So basically you have, say, a normalized distribution.",
            "OK, so your energy function will be the negative log probability of some normalized distribution.",
            "There's a finite fixed volume of stuff that can have low energy because your distribution is normalized.",
            "But there are other non probabilistic models that have this property like PCA or K means or gas mixture models, square C and stuff like that.",
            "The second strategy is to push down the energy on data points and push up everywhere else.",
            "So that's basically what maximum likelihood does when your function has an explicit normalizer.",
            "When your probability distribution if you density estimation as an explicit normalization term.",
            "An that doesn't quite work all the time because it's possible that the normalization turned the partition function would be intractable, or its gradient will be intractable, and so there are lots of situations where you can't really do this, and there's a lot of papers in the machine learning community, including by people in this room that have to do with how do you kind of make approximations of this so that it still works for you automation sampling, whatever.",
            "3rd method push down on the energy of data points and push up on chosen locations sometimes close to the data point, but so you know a little bit outside.",
            "So the things like contrastive divergent racial matching noise contrastive estimation, you know every time I showed this slide, people take pictures of it and you know I should probably write this paper at some point.",
            "So there's others.",
            "Other methods of this type.",
            "Divergences you take a data point and then you move a little bit away from it, preferentially in directions where the energy goes down, and then you push up on that point and whether it does is that it creates a local groove in the energy surface.",
            "It doesn't determine the shape of the energy surface far away from data, but doesn't start problem.",
            "Now before you minimize the gradient to maximize the curvature around data point, that's what schools score matching.",
            "That's actually pretty much impractical for complex models.",
            "This method you train dynamical system, so the dynamics causes many folders.",
            "Are you around Montreal but?",
            "You know and beyond.",
            "But I'm not going to talk too much about this.",
            "This is my favorite one.",
            "So basically you use a regularizer and this is your favorite 12 by the way, for most of you at least, even if you don't know which is the user regularizer, that limits the volume of space that has low energy, and that includes things like dictionary learning for sparse coding, so additional info, space coding.",
            "Essentially you can think of the sparsity term as when you do dictionary learning as a way of limiting the volume of space that can be properly constructed OK. And if you use the reconstruction error as the energy function, then that means you're kind of creating a groove that with minimum volume around the data points that did have any fold through the training.",
            "The other one I'm going to talk about.",
            "OK, so peace."
        ],
        [
            "OK, so if you have a bunch of points drawn from this spiral, few PCA with one dimension you get this is terrible approximation, but at least you have some.",
            "You know the energy function is 0 on the black and sort of increase increases with intensity.",
            "Here K means you know K means after you train puts little.",
            "Quadratic wells all around this little surface seems that it's great, except doesn't scale up in high dimension."
        ],
        [
            "And."
        ],
        [
            "So let's go."
        ],
        [
            "This coding, scripting and sparse modeling in general, when you learn a dictionary learning is you have an energy function of this type where you have the square reconstruction error.",
            "Why isn't observation?",
            "WD is a dictionary matrix and Z is a sparse vector which when multiplied by the dictionary matrix is meant to reconstruct the inputs at least approximately with square penalty function.",
            "And then you have this sparsity term.",
            "So I like to represent this as kind of a funny type of factor graph where those are variables.",
            "This one is observed.",
            "This one is latent has to be inferred, and then you have deterministic functions that are those symbols and those are factors which are kind of turns in the energy function that enter additively in the energy function or multiplicatively and the likelihood function if you take exponentials.",
            "So the problem with the sparse coding is that even once you've learned the dictionary, inference is relatively expensive.",
            "There's lots of efficient algorithms we heard about when this morning in the first talk, CGI HD sister Fister whatever.",
            "According to the centers, all kinds of efficient algorithms for this, but it's still slow if you want to do sparse coding with four thousand dimension on every window in an image, it's going to take you a few seconds.",
            "I mean, there are tricks that Ultraslan came up with to do this approximately really fast fast, but.",
            "You know it's going to be expensive now."
        ],
        [
            "Coding essentially makes the assumption that the data is sort of a union of plane, so the energy function really has kind of linear grooves of low, relatively low dimension, and then you can add them up and you get this sort of approximation of the day."
        ],
        [
            "So."
        ],
        [
            "So one trick that we came up with back in 2008 or seven was to learn quick approximations of sparse coding inference.",
            "So instead of running an ESTA or, you know CGI HD or whatever, we're going to train.",
            "Fastenal net essentially to produce a good approximation of the ultimate solution of this coding inference problem.",
            "So that."
        ],
        [
            "The what's called a spot autoencoder, so you have the top is just pass coding the same same as I drew originally, but there's a thing at the bottom here, which is a nonlinear parameterized function which predicts tries to predict what the optimal code is for a particular.",
            "Why.",
            "OK, so you plug Y, you find the Z that minimizes the energy function for space coding, and now what you have is a pair of input and sparse code that represents the input and you train a feedforward network to basically predict this.",
            "Alright, you can actually do both.",
            "Seriously, and learn them cooperatively by having an energy function that includes two terms, three terms, the sparsity, the reconstruction and the prediction error.",
            "So a very simple prioritization of this box is something like a shrinkage of a linear map of why.",
            "But that is kind of very simple."
        ],
        [
            "You can also do this with group sparsity, so use those you train this kind of."
        ],
        [
            "Also to encoders, and we turn this on on natural image patches.",
            "This is alone."
        ],
        [
            "Algorithm running in the end.",
            "You get business functions that end up being those oriented edge detectors.",
            "That's what we get."
        ],
        [
            "So any decent algorithm will give you this.",
            "But here is."
        ],
        [
            "Cool tricks so the cool trick here we call."
        ],
        [
            "That list which stands for learning iterative shrinkage, thresholding algorithm and the basic idea of this one is to realize this was done by my former postdoc Calla Gregor, who is now in London that declined and the basic architecture of the basic idea of this is to say look at the look at the East algorithm.",
            "So this algorithm says you take a step of gradient of the square reconstruction error and then you shrink all the coordinates by some factor.",
            "OK, just bring them to 0.",
            "So I'm going to 0.",
            "And then iterate this and you can prove this converges to the solution of the sparse coding inference problem.",
            "We heard about this this morning.",
            "In the first talk actually, so you can view this as some sort of block diagram here to take the input multiplied by some matrix, then shrink, then multiplied by some big matrix which you add to the previous result you had, shrink again and iterate this, which is kind of this iteration inside, so kind of reply me tries this thing here by writing, watching it this way is ET, plus one equals shrink of.",
            "WE transpose y * Z of T where WE is.",
            "One over LWD transpose, where L is some.",
            "Which is constant and S is 1 -- 1 / L WW transpose.",
            "OK, so the crucial idea of Vista is to say forget about defining WNS.",
            "This way we're going to learn WE&S in such a way that with this fixed number of iterations of this we get the best possible approximation of Z.",
            "And so this was an HTML paper in 2010.",
            "This idea is being recycled by being used a lot by Guillermo Sapiro.",
            "With his collaborators, one of them brothers.",
            "Which one addicts OK and public man, who is now.",
            "Unfortunately it was documented."
        ],
        [
            "And so essentially you can think of this as a recurrent neural net where the shrinkage is, you know, like your value essentially, so you can unfold this in time and then you get a feedforward neural net with shared parameters among layers which you can train with what's called backdrop through time.",
            "And this works amazingly well, so you get."
        ],
        [
            "This is the reconstruction error you get after a given number of iteration, which is on the X axis here of Vista or Vista in that case, and this list algorithm.",
            "So after just one iteration, so one cycle through the shrinkage and the S matrix.",
            "This is for low over completeness and high over completeness, and this is for Lister.",
            "So you get much better approximation, much fast."
        ],
        [
            "Sure, this is the same thing for the coordinated descent algorithm.",
            "Uh, this is normal for."
        ],
        [
            "This is for dissent, this one.",
            "This is the standard with them.",
            "This one is for kind of a partial S matrix that is low rank.",
            "You can make."
        ],
        [
            "Conditional versions of this, where the dictionary is not just a matrix, but it's basically a filterbank for images.",
            "And you get beautiful filters."
        ],
        [
            "They look pretty nice, so this is Patch level training and this is image level training with convolutional filters an the what happens there is that the filters you learn are much less redundant than what you get here because here you train on patches and then you apply those filters conventionally and you get redundant codes because you get here you can observe that you get horizontal edge at various locations in the image, which is kind of useless because you're going to apply this convolutional in the 1st place so.",
            "So it's not a good idea.",
            "So if you train at the image level, all those filters know that you know they have a copy of themselves in another place, so they they spend their energy.",
            "You kind of.",
            "Getting more diverse and so you get crosses and centers around and and sort of gradings an you know corners and stuff like that which is really great.",
            "Here's a nice corner as well.",
            "Um?"
        ],
        [
            "And.",
            "And there's at least one application."
        ],
        [
            "Whether it works really well, unfortunately not many.",
            "OK, so the idea is you train those little autoencoders.",
            "Convolutional sparse autoencoders unsupervised.",
            "OK, when you're happy with it, you get rid of the feedback, you just keep the feedforward path which basically is filterbank and linearity, and pulling.",
            "If you do, good sparsity is appalling, and that's like the first stage of a commercial net, so you use that as your first major accomplishment for initialization of the filters, and then you stick a second layer."
        ],
        [
            "To obtain that unsupervised again when you're happy, you get rid of them."
        ],
        [
            "Back and now what you have is A to stay."
        ],
        [
            "Company format where the weights are pre trained using unsupervised learning and we've."
        ],
        [
            "Are these two?"
        ],
        [
            "Different things, but particularly pedestrian detection."
        ],
        [
            "And these are performance measures which I'm not going there.",
            "Basically precision recall curves, so good is kind."
        ],
        [
            "You know a curve that sort of is to the bottom left is good, and those are values systems in literature as of about two years ago.",
            "So this is the conversation that pre trained with this unsupervised method and then fine tune with back prop for pedestrian detection.",
            "At that time it was a record Holder.",
            "Since then people have gotten better results.",
            "But interesting thing is that you need to compare this curve with that curve which is the same commercial net which is not pre trained.",
            "Just train from random initial conditions with supervised backdrop and so it goes from sort of middle of the pack to kind of best system at the time.",
            "So that's kind of really impressive.",
            "Unfortunately that's pretty much the only application for which.",
            "SB training seems to help, so we haven't really tried this in a big way on things like image, net 'cause.",
            "It's kind of this.",
            "Practical issues with that, but there are very few applications for which unsupervised training like this helps.",
            "Yes, training."
        ],
        [
            "The same data set?",
            "Or is it on a larger datasets on the same data set?",
            "Actually, yeah, I mean you could train on."
        ],
        [
            "Yeah, but usually you know because it's unsupervised, you don't have the overfitting problem usually have."
        ],
        [
            "No, I think the answer question was done on the Berkeley data set.",
            "If I remember, at least for the first layer, you get the same features anyway.",
            "So this is what the.",
            "Pedestrian detection works, so again the network is applied conditionally to the image at multiple scales, and then there is some sort of maximum feed system, but just for two categories."
        ],
        [
            "And.",
            "OK."
        ],
        [
            "So there's variations of this."
        ],
        [
            "There the sparsity constraint is kind of a group sparsity type constraint, where you have."
        ],
        [
            "Take blocks of features.",
            "And you insist that the L2 norm of those bugs of features be.",
            "SM and Usum over those bugs.",
            "But otherwise it's the same thing.",
            "Yes, OK."
        ],
        [
            "Right?",
            "Yeah, that's just what I need, so there's a long history of those group sparsity stuff going back to.",
            "Essentially, he Varian Hoyer about 15 years ago or 13 years ago called subspace.",
            "I see this was for kind of square analysis filters.",
            "And Toronto also had a few ideas around those lines, and there's sort of a evolution.",
            "There's something called reconstruction ICA coming out quickly, which is sort of a linear encoder and decoder, which doesn't make any sense to me.",
            "But then this sort of nonlinear versions."
        ],
        [
            "Of sparsity encoders that actually predate.",
            "Reconstruction ICA.",
            "So here is an example where each square here is a business function or is actually an analysis filter, so it's in the encoder.",
            "And we have 256 of those filters and we arrange them on a, you know.",
            "Dreamed up artificial toroidal topology.",
            "So this is this is a Taurus where the left side touches the right side on the top touches the bottom and the groups.",
            "Here are groups of 36 filters where the L2 norm of those groups they overlap by three, so you stab them every three things, so he creates an artificial topology in this kind of territorial.",
            "Topology and what happens is that the system wants to turn on the smallest number of groups as you can, because that's the sparsity constraint.",
            "This crusty penalty on the groups, and so the best way for you to do this is to regroup similar filters to learn filters in such a way that similar filters are within a single group, because similar filters will tend to turn themselves on for, you know the same input for a given input.",
            "Similar filters were all turned on, so if you can regroup all the filters that turn on for a particular input within a group, then only that group will be on.",
            "And that will be very sparse, and so you spontaneously the system can build those really nice topographique graphic Maps."
        ],
        [
            "And if you do this without shared weights on you."
        ],
        [
            "Kind of, you know, kind of put those filters around an image.",
            "You get those you get.",
            "This is for 4X over completeness.",
            "Here again you do this group sparsity.",
            "You get those nice looking sort of continuously varying Maps with sort of singular points here that are called pin holes.",
            "An when North scientist spoke the visual cortex of various for animals with electrodes, they figure out that the organization said activity of neurons organized spatially in pictures that are very much like this.",
            "In fact, it just like that.",
            "So this is from."
        ],
        [
            "No Science Journal where the color indicates the dominant orientations activity and this is what the algorithm produces."
        ],
        [
            "Here's another example where the algorithm produces.",
            "OK, let's see I want to say."
        ],
        [
            "Is another way to do sparse coding and I've not talked about this very much.",
            "There's a paper that we had at nips a few years ago with auto Slam and Carol Gregor and the idea of this is kind of a non convex.",
            "Funny way of doing sparse coding which allow you to structure this pass coding makes connection with no science by using the notion of lateral inhibition.",
            "So here is the energy function here.",
            "For this sparse coding model with square reconstruction error, we know that that's the same thing and then the sparsity term is something like this where you have an S matrix which hopefully is symmetric and then you have the vectors here, which is the code vector.",
            "The Spotswood vector you take the this is kind of funny notation to say that we are taking the absolute value of all the components.",
            "So basically rectified components of the vector an.",
            "You compute this this bilinear form and so you can think of it in those terms.",
            "If you have a nonzero term in the S matrix.",
            "It means that say SIJ is non zero.",
            "That means you pay a price for making the inz drain on zero at the same time right?",
            "So it creates a mutual information between zero and DJ if one of them goes to 0 then this term in the energy function goes to 0.",
            "So it allows you to do is build particularly matrix matrices that have a particular structure.",
            "That kind of organizes the lateral inhibition.",
            "So for example."
        ],
        [
            "So you can organize it as a tree, so he would have drawn is.",
            "I've organized all the filters around the tree where if you have a link in that tree then you don't have.",
            "In non zero terminate you have a zero in the estimator in the corresponding matrix and every other term is non zero.",
            "OK, so two things here in each other.",
            "But if there are children in each other, they don't invite each other.",
            "In fact there is an S term that depends on the tree distance but OK. And what you get is this.",
            "Again those kind of organization of this tree according to kind of sort of continuous variation you get low frequency at the center high frequency at the periphery and this sort of some sort of continuously.",
            "During orientation, because of this tree tree structure, you can also have."
        ],
        [
            "You know other topologies of various kinds.",
            "So OK, so this is sort of attempts.",
            "Feeble attempts at solving the unsupervised learning problem.",
            "The bad news is that those things you know are cute.",
            "You can have nice pictures that you know get scientists interested, but in terms of practical applications, there is really not much so far.",
            "It really doesn't work as well as just supervised running with tons of data.",
            "And so there is a sense that for things like video and natural language and perhaps other kind of large scale applications and supervised learning will help us and we're still looking for the perfect unsupervised learning rule, particularly if it's one that.",
            "It works, you know, is basically a single learning world for supervised and unsupervised.",
            "So something like both machines, except that both machines don't work.",
            "As I said previously, or don't scale at least.",
            "And."
        ],
        [
            "Another Ave we're exploring which I'm not going into the details of is the wild kind of separation or the factorization of of inputs into independent factors.",
            "So when we run.",
            "Yeah, we trying to spot encoder with group sparsity where we get our sort of invariant response units.",
            "That sort of respond with a little bit of invariants to change the position of the features, but then we lose information about position.",
            "So one question is how do you keep that information as kind of a complementary bit of information that you can separate and kind of factor out right?",
            "So if you could do this well separation which we know is going on in the.",
            "Visual cortex.",
            "He's anatomically we think we can sort of.",
            "There's a lot of problems would be able to solve, so we've been sort of playing with between quarters that do this sort of continuous wear separation.",
            "We know some success, but not a huge amount of success."
        ],
        [
            "And I'm completely out of time, so I'm going to stop here and thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me say a few words about Facebook first, because this is sort of primitive response to a lot of questions I'm getting, so I've been at Facebook since December and I'm creating a essentially what is an AI research lab, which is sort of the first research organization on Facebook.",
                    "label": 0
                },
                {
                    "sent": "We're currently about 30 people, about 30 scientists and engineers, probably about 1/3 of the people at Facebook, Facebook, AI research now are are kind of more on the engineering side of things, sort of building, machine learning, infrastructure, and.",
                    "label": 1
                },
                {
                    "sent": "Software and things of that type.",
                    "label": 0
                },
                {
                    "sent": "Handling data collecting data, things like that type and then about 2 third 4 for now, research scientists spending the spectrum between theory, principles, algorithms and applications to various fields, mostly computer vision and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "And we are setting up ourselves as a real research organization, somewhat similar to what you would have expected of Bell Labs in the old days.",
                    "label": 0
                },
                {
                    "sent": "Microsoft Research and some to some extent, except that there is a sort of tighter link to the rest of the company in the sense that it's much easier in a company like Facebook to get things out the door out of the labs into into products.",
                    "label": 0
                },
                {
                    "sent": "But we're really research organization, so we're publishing papers.",
                    "label": 0
                },
                {
                    "sent": "We're participating in conferences like this, and workshops like this and.",
                    "label": 0
                },
                {
                    "sent": "And have projects that are very long term and theoretical in nature.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in Facebook for now we are other than the fact that we are hiring.",
                    "label": 0
                },
                {
                    "sent": "So currently we have about 30 people and probably over the next few years will grow to about 100 people and will most likely open a branch in Europe.",
                    "label": 0
                },
                {
                    "sent": "Not entirely clear where yet, but somewhere in Europe.",
                    "label": 0
                },
                {
                    "sent": "And we'll see with the other goes OK so so much for boring stuff.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about deep learning and surprisingly, and I usually there's sort of a standard talk.",
                    "label": 0
                },
                {
                    "sent": "I've been giving, which you know the last year or so, which is sort of a rehash of talks.",
                    "label": 0
                },
                {
                    "sent": "I was giving 20 years ago, basically explained explaining to people with back problem.",
                    "label": 0
                },
                {
                    "sent": "Commercial Nets are an really this kind of more interesting stuff that we've been doing in the last 20 years, and I thought for this audience I would.",
                    "label": 0
                },
                {
                    "sent": "I would go very quickly on sort of, you know, the stuff that everybody is using an industry which is.",
                    "label": 0
                },
                {
                    "sent": "Really technology that was around 20 years ago and then talk more about data representation.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning which is topics that got people excited.",
                    "label": 0
                },
                {
                    "sent": "About deep learning, but really in practice or not yet used, but that's where the interesting research is taking place really.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So let's start talking quickly about about competition, and so I'm going to really quickly.",
                    "label": 0
                },
                {
                    "sent": "So the history of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Pattern recognition is you build your feature extractor, you stick your classifier, you train your classifier and you're done.",
                    "label": 0
                },
                {
                    "sent": "And that's going back to the 50s and the perceptron in more recent years.",
                    "label": 0
                },
                {
                    "sent": "Sort of the mid 2000s.",
                    "label": 0
                },
                {
                    "sent": "If you want or earlier for speech recognition, you had this kind of architecture where you have a fixed front end feature extractor at the other front end you have.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unsupervised learning to extract mid level features.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of kind of standard pipeline of object recognition in computer vision that are based on this on this idea, and for this using you use techniques like vector quantization of sparse coding.",
                    "label": 0
                },
                {
                    "sent": "So techniques are very familiar to a lot of people in this, in this audience, and then you could have some sort of aggregation of features and you know supervised running on top of it, and perhaps a post processing.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the standard pipeline for both object recognition and speech recognition for you until about two years ago, and that's been essentially displaced.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By by deep learning.",
                    "label": 0
                },
                {
                    "sent": "So deep learning is the idea of stacking a whole bunch of parameterized function an have them all be trained in this case in the supervised manner.",
                    "label": 0
                },
                {
                    "sent": "But ultimately what we want is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try and not just the part that those feature extraction and classification, but also trained the post processor, which generally is some sort of graphical model, some sort of objects that can sort of produce an output, not by just computing and output, but by kind of minimizing some function, or by maximizing the likelihood or something of that type.",
                    "label": 0
                },
                {
                    "sent": "So there's a bit of reasoning in it.",
                    "label": 0
                },
                {
                    "sent": "Then you've also optimization of some kind, and we'd like to sort of integrate the training of this with the training of everything else.",
                    "label": 0
                },
                {
                    "sent": "And conceptually we've known how to do this.",
                    "label": 0
                },
                {
                    "sent": "About 20 years.",
                    "label": 0
                },
                {
                    "sent": "In fact, some of the check reading systems are built at AT&T about 20 years ago were actually integrating training of feature extraction, classification, and the post processing in the form of what we now call the conditional random field, but really was was done at the time, so we know how to do.",
                    "label": 0
                },
                {
                    "sent": "Conceptually, there's a lot of practical issues on how to actually sort of implement that, and I used a different color here than the Redwood to symbolize the fact that eventually would like to integrate.",
                    "label": 0
                },
                {
                    "sent": "Not just to use supervised learning, which is what people have used mostly in practical applications of deep learning so far, but also integrate unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "The reason being that there is a lot of tasks for which we have tons and tons of data, but relatively speaking we have small amount of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So that's the case for video, for example is the case.",
                    "label": 0
                },
                {
                    "sent": "Also for things like semantic segmentation, where the vision system is meant to label every pixel in an image, so the datasets like this where we have tons of.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled into another huge amount of labeled data.",
                    "label": 0
                },
                {
                    "sent": "And there's a sense that there's a lot of kind of regularity's in the world that can be captured with unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So the second half of my talk will be about unsupervised running.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We would like to look for some sort of learning rule that sort of can be used seamlessly for unsupervised or supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We already have learning algorithms that have that property, but they don't work.",
                    "label": 0
                },
                {
                    "sent": "So this machine algorithms for example or similar things where the fact that his supervisor unsupervised entirely depends on how you use it and doesn't depend on the algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "Now that's a very nice property to have, except both machines don't really work.",
                    "label": 0
                },
                {
                    "sent": "They'll scale, they don't really work OK, so.",
                    "label": 0
                },
                {
                    "sent": "The cute that's it.",
                    "label": 0
                },
                {
                    "sent": "Also, the request sampling ahead sampling.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm allergic to sampling.",
                    "label": 0
                },
                {
                    "sent": "So, so the idea of having hierarchical architecture is of course is due to the fact that the world is compositional.",
                    "label": 0
                },
                {
                    "sent": "I heard someone said a joke, I don't know where it comes from, which is the world is compositional or there is a God?",
                    "label": 0
                },
                {
                    "sent": "So my confession is on the former.",
                    "label": 0
                },
                {
                    "sent": "And so you have low level features that are local and it's useful to extract local local features in an image because there are high correlations between groups of neighboring neighboring pixels.",
                    "label": 0
                },
                {
                    "sent": "And so that's the idea of compositionality, where you have kind of blocks of things that can happen and not everything in the world can happen, so that's what makes the world intelligible, and as you grow up the layers, you build features that are kind of compositions of lower level features.",
                    "label": 0
                },
                {
                    "sent": "And you sort of have to increase the number of possible feature types as you go up the layers, But then possibly decrease the spatial resolution or resolution in other dimensions and sort of eliminates irrelevant variability in the signal.",
                    "label": 0
                },
                {
                    "sent": "So images you know you start from things like edges.",
                    "label": 0
                },
                {
                    "sent": "These are learned features in a commercial net.",
                    "label": 0
                },
                {
                    "sent": "This was visualization produced by my colleague Rob Fergus an at the level you have oriented edges and then a couple of layers up, you end up having after training.",
                    "label": 0
                },
                {
                    "sent": "Having motive detectors and then couple it goes up.",
                    "label": 0
                },
                {
                    "sent": "You have like parts of objects that are being detected and then all the way up you have objects, entire objects.",
                    "label": 0
                },
                {
                    "sent": "This kind of hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course exists for all kinds of signals, not just pixels, but also text, speech and things of that type, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're deep learning, is trying to do is learn the problem we need to solve with deep learning is learning representations of the world and this is all about high dimensional data representation.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we learn representations of the world by just observing it?",
                    "label": 0
                },
                {
                    "sent": "That's the question we need to answer the question.",
                    "label": 0
                },
                {
                    "sent": "Or scientists have been.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In themselves also for a long time.",
                    "label": 0
                },
                {
                    "sent": "How does the brain does that do that?",
                    "label": 0
                },
                {
                    "sent": "So the visual cortex is hierarchical.",
                    "label": 0
                },
                {
                    "sent": "We've known that for quite quite some time, the notion of areas in the visual cortex are not exactly equivalent to the notion of layers in the neural net, but sort of.",
                    "label": 0
                },
                {
                    "sent": "And it's the fact that our neural Nets are feedforward is a gross simplification of really the architecture that.",
                    "label": 0
                },
                {
                    "sent": "Biology uses which has all kinds of feedback and lateral connections and things of that type.",
                    "label": 0
                },
                {
                    "sent": "But there is this sort of sense that for very fast perception, the visual cortex uses an essentially feedforward process because it just goes too fast for any kind of feedback to really have an impact.",
                    "label": 0
                },
                {
                    "sent": "There's something that we're not modeling in our in our feedforward recognition system, which is the visual cortex is really has two different pathways.",
                    "label": 0
                },
                {
                    "sent": "One that is used to recognize objects.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independently position an other one that's used to localize objects independently of where they are, so the second one.",
                    "label": 0
                },
                {
                    "sent": "Helps you when you are navigating and avoiding obstacles, grabbing things, look at localising things whereas the first one is just for identifying.",
                    "label": 0
                },
                {
                    "sent": "Those of course talk to each other.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to sort of paraphrase some of the things that near Lawrence talked about this morning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is the fact that.",
                    "label": 0
                },
                {
                    "sent": "A good way to kind of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Build a stage that ends up extracting good features is to have have you do a sequence of two operations, and that's really a very old idea.",
                    "label": 0
                },
                {
                    "sent": "This is not certainly not something that was recently.",
                    "label": 0
                },
                {
                    "sent": "Arrived at from the deep learning side.",
                    "label": 0
                },
                {
                    "sent": "It's a very old idea for parameterising functions.",
                    "label": 0
                },
                {
                    "sent": "You have an input vector.",
                    "label": 0
                },
                {
                    "sent": "You have a first layer that essentially is a nonlinear mapping to a higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we expanded dimension in a non linear way.",
                    "label": 0
                },
                {
                    "sent": "It's very important that this be nonlinear and the role of this of this stage is to this layer is traditionally essentially break apart regions of the space that are semantically dissimilar that need to be classified as different things.",
                    "label": 0
                },
                {
                    "sent": "And so you break things apart and then you have another layer that reduces the dimension that essentially re aggregates things that are semantically similar.",
                    "label": 0
                },
                {
                    "sent": "That happened to have been broken apart by the first stage.",
                    "label": 0
                },
                {
                    "sent": "OK, so break things apart nonlinearly and then glue them back together and so you have this sort of expansion reduction which is similar to the sort of basic module near Lawrence was talking about this morning where you know he would have, you know radial basis functions here, or Gaussian bumps or whatever, but you can put whatever you want.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to build our architectures by essentially stacking stages of this sort of basic sequence of operation.",
                    "label": 0
                },
                {
                    "sent": "First, there is a optional normalization.",
                    "label": 0
                },
                {
                    "sent": "In fact, in practice, we don't have that very often.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have a linear dimensionality expansion in a form of a filter bank for images or for sort of array signals in general, but think of it as just a matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have a pointwise nonlinearity, and there's all kinds of arguments for why it's just fine.",
                    "label": 0
                },
                {
                    "sent": "It's nonlinearity is point wise at this point.",
                    "label": 0
                },
                {
                    "sent": "Jean Boone and Stephen Miller have arguments about this.",
                    "label": 0
                },
                {
                    "sent": "And then there is feature pulling, which is places role of aggregating features over space over kind of variabilities of various kinds, but sort of reducing the dimension so very often in things like convolutional Nets, but also in other types of networks.",
                    "label": 0
                },
                {
                    "sent": "This feature pulling is fixed.",
                    "label": 0
                },
                {
                    "sent": "There's no learned parameters in this, all alone parameters are in the filter banks.",
                    "label": 0
                },
                {
                    "sent": "So you take those stage and you repeat it multiple times.",
                    "label": 0
                },
                {
                    "sent": "You stick a classifier on top, but in fact the classifier is nothing more than another stage of those filter banks in our linearity actually.",
                    "label": 0
                },
                {
                    "sent": "So don't think of this as kind of a different as different in nature from this is just another one of those.",
                    "label": 0
                },
                {
                    "sent": "The proving operation is very often nowadays a Max operation, although people have been doing other things and the nonlinearity very often.",
                    "label": 0
                },
                {
                    "sent": "If there is 1, here is a Max of X and zero, so it's halfway rectification.",
                    "label": 0
                },
                {
                    "sent": "There's a technique by Ian Goodfellow and his collaborators at in Toronto, where they actually do they do away with the point wasn't linearity here and the pulling directly doesn't Max over multiple feature types and over space.",
                    "label": 0
                },
                {
                    "sent": "And that seems to actually work quite well.",
                    "label": 0
                },
                {
                    "sent": "They call that Max out.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what a.",
                    "label": 0
                },
                {
                    "sent": "Deep neural net sort of modern day practical deep neural net looks like it's a stack of linear operation interspersed with Max.",
                    "label": 0
                },
                {
                    "sent": "Max is essentially, and you can think of Max.",
                    "label": 0
                },
                {
                    "sent": "That's going to switch that switches between zero and X if it's a value or between the different values, and taking the biggest one if it's a Max pooling.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, you train this with backdrop, that's just application of Chinua centrally to compute sub.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Audience the functions you have to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, when you do this end up being very highly nonconvex, they have tons and tons and tons of several points.",
                    "label": 0
                },
                {
                    "sent": "In fact, the commentarial number of several points and local minima there is a picture that emerged that a lot of us have known for a long time intuitively but not really sort of being able to put a finger on it, which is that.",
                    "label": 0
                },
                {
                    "sent": "You never have local minima problems with deep learning systems.",
                    "label": 0
                },
                {
                    "sent": "Whenever you try them, you always get solutions that have the same kind of performance.",
                    "label": 0
                },
                {
                    "sent": "Those systems we know have a community really large number of local minima, but whatever local minimum you find.",
                    "label": 0
                },
                {
                    "sent": "Is always kind of the same type of performance.",
                    "label": 0
                },
                {
                    "sent": "They're always always different, but they always give you the same kind of performance.",
                    "label": 0
                },
                {
                    "sent": "So you know there's different types of non convexity.",
                    "label": 0
                },
                {
                    "sent": "There is nasty non convexity and then complexity you get with deep learning systems, but you could leave the oversized is not nasty in the sense that it doesn't matter which minimum you find.",
                    "label": 0
                },
                {
                    "sent": "But that creates lots of lots of several points, so whenever you have two local minima, there's several points in between, or perhaps multiple because it's in high dimension and that creates kind of a landscape that's extremely complicated that none of us really begins to understand, so I hope these people in the audience.",
                    "label": 0
                },
                {
                    "sent": "Here we kind of.",
                    "label": 0
                },
                {
                    "sent": "Get interested in this question.",
                    "label": 0
                },
                {
                    "sent": "This is too complicated for me.",
                    "label": 0
                },
                {
                    "sent": "So right?",
                    "label": 0
                },
                {
                    "sent": "So let's think about deep network with Max with values or Max.",
                    "label": 0
                },
                {
                    "sent": "What does he do?",
                    "label": 0
                },
                {
                    "sent": "So let's say it has a single output.",
                    "label": 0
                },
                {
                    "sent": "You could write it as.",
                    "label": 0
                },
                {
                    "sent": "You could take a single path in that network.",
                    "label": 0
                },
                {
                    "sent": "And write the influence of this path on the output.",
                    "label": 0
                },
                {
                    "sent": "OK and this path.",
                    "label": 0
                },
                {
                    "sent": "So essentially you take an input, you multiply it by the first wait and then by the second wait and then by the 3rd weights and you accumulate that in the output.",
                    "label": 0
                },
                {
                    "sent": "And if a path is active, which means if all the units in there are in their linear region in the value, then this this this path contributes linearly to the output.",
                    "label": 0
                },
                {
                    "sent": "And so you do the some of her old path of all the contributions and you get the input output relations.",
                    "label": 0
                },
                {
                    "sent": "But what's going to happen is that some of the paths are going to be turned off because the corresponding unit is in the flat spot or the Max switch, so didn't select it.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially it is kind of funny, sort of switched path integral, right?",
                    "label": 0
                },
                {
                    "sent": "Figure out all the path from all the inputs to the output, and that's your input output function.",
                    "label": 0
                },
                {
                    "sent": "But you have those switching very binary variables that indicate whether a particular path is on or off, and of course.",
                    "label": 0
                },
                {
                    "sent": "This depends on the input as well as on the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what it looks like now.",
                    "label": 0
                },
                {
                    "sent": "If you stick A cause.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function on this, particularly stick a hinge cost function.",
                    "label": 0
                },
                {
                    "sent": "For example to do learning.",
                    "label": 0
                },
                {
                    "sent": "So now you have a loss function that you want to minimize for learning.",
                    "label": 0
                },
                {
                    "sent": "Again, one more of the same.",
                    "label": 0
                },
                {
                    "sent": "It's a Max of zero and something else.",
                    "label": 0
                },
                {
                    "sent": "It's a hinge.",
                    "label": 0
                },
                {
                    "sent": "Or one in this case, and the desired output enters multiplicatively in this in this function, but in the end you can sort of wrap this into some sort of coefficient at the end you get a form that's like this, so the loss function is a polynomial in W whose degree is the number of layers.",
                    "label": 0
                },
                {
                    "sent": "And then in front of this you have coefficients that depend on the input that depend on the weights and depends on the desired output.",
                    "label": 0
                },
                {
                    "sent": "That sort of determines which of those terms which of those paths are turned on alright.",
                    "label": 0
                },
                {
                    "sent": "But essentially it's kind of a switch polynomial if you want.",
                    "label": 0
                },
                {
                    "sent": "In W. Since the operations are maxes you can you can show that the function is continuous.",
                    "label": 0
                },
                {
                    "sent": "It's not differentiable, but it's continuous.",
                    "label": 0
                },
                {
                    "sent": "It's got things, but it's continuous, so it's a continuous piecewise polynomial with switch and partially random coefficients random because the input is comes from some distribution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out a lot is known about functions of this form.",
                    "label": 0
                },
                {
                    "sent": "If you assume that those coefficients are Gaussians, Gaussian distributed in a Gaussian way.",
                    "label": 0
                },
                {
                    "sent": "If you assume that the WS owners here, there's a lot of results from essentially from the spin class people or random matrix theory people.",
                    "label": 0
                },
                {
                    "sent": "On, you know the sort of properties and distribution of critical points of functions of this type.",
                    "label": 0
                },
                {
                    "sent": "When the when the coefficients are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In fact this is work.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By my colleague Robin, who set the quantity is the director of the Institute.",
                    "label": 0
                },
                {
                    "sent": "So what those results?",
                    "label": 0
                },
                {
                    "sent": "So those results don't directly apply because our coefficients have this kind of funny property that they depend on the data and they are switched and the other binary.",
                    "label": 0
                },
                {
                    "sent": "But there is some randomness to them which you could assume is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about those results is that they say that the distribution of critical points which are subtle points, minima, Maxima of those functions.",
                    "label": 0
                },
                {
                    "sent": "Putting on this fear is such that there is a if you do the distribution of minima or the.",
                    "label": 0
                },
                {
                    "sent": "You know, over energy levels, right?",
                    "label": 0
                },
                {
                    "sent": "So on the X axis, you have sort of energy levels, value, objective function and you count how many minima are there at any particular value or benefit values of the cost function.",
                    "label": 0
                },
                {
                    "sent": "You get this kind of histogram where there is sort of what's called the bulk, where pretty much all of the minima are there within a very narrow region of energy, and then you have a tiny exponentially small number of ones that are slightly lower energy and then they another exponentially small numbers that have higher energy.",
                    "label": 0
                },
                {
                    "sent": "So these are the nasty local minima.",
                    "label": 0
                },
                {
                    "sent": "These other local minima you actually want and these other ones that you might want, except they're very rare, and there's no way you're going to find them.",
                    "label": 0
                },
                {
                    "sent": "So if you just do stochastic gradient descent without any particular trick, you're going to find one of those big guys here.",
                    "label": 0
                },
                {
                    "sent": "When one of those guys are numerous and these guys is very, very unlikely, you're going to get trapped in those because there are sort of exponentially less probable.",
                    "label": 0
                },
                {
                    "sent": "There's just a lot less of them.",
                    "label": 0
                },
                {
                    "sent": "Anan these guys.",
                    "label": 0
                },
                {
                    "sent": "You essentially have no chance of finding them, but if you did find them you probably would overfit, so it doesn't matter.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so much.",
                    "label": 0
                },
                {
                    "sent": "That's hand WAVY theory.",
                    "label": 0
                },
                {
                    "sent": "And I have to admit that I.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know something that's behind this, but OK. Like, I mean, the techniques to find those distributions.",
                    "label": 0
                },
                {
                    "sent": "It's really not my field, but it's cool.",
                    "label": 0
                },
                {
                    "sent": "It's cool results that I hope we could transfer, you know, modify to sort of apply to this particular situation.",
                    "label": 0
                },
                {
                    "sent": "So again, if anyone is interested.",
                    "label": 0
                },
                {
                    "sent": "So that's again a special case of the pipeline I talked about earlier where you have filter banks so you take an image, you apply a bunch of filters to it.",
                    "label": 0
                },
                {
                    "sent": "Those filters will be learned.",
                    "label": 0
                },
                {
                    "sent": "The coefficients of the filter will, and so you do this convolutionally you get this result you pass or through a nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "In this particular case is the sigmoid, but we use values and then there is the pooling, which in the case of commercial net or simple methods is purely spatial and the research sampling so that reduces the dependency of the representation on the precise position of.",
                    "label": 0
                },
                {
                    "sent": "Distinctive features on the input and you repeat the process so this guy again is result of applying a bunch of filters to these things and adding up the results, passing the result.",
                    "label": 0
                },
                {
                    "sent": "Who nonlinearity and each of these guys use different sets of filters we call each of those feature Maps and then there is pulling again and then you know multiple layers of that.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An idea that you know comes from from neuroscience, essentially from classic work in the 60s.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there were models little bit along those lines in the 70s, the new computer and I for kushima in the early 80s, but he didn't quite have the right learning algorithm for that.",
                    "label": 0
                },
                {
                    "sent": "So using backdrop to train those architectures I've been working on this since the late 80s, basically.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that works really well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is kind of an old commercial net that was used to recognize characters.",
                    "label": 0
                },
                {
                    "sent": "In fact, instead of this I'm just going to show you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The video of the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is me young.",
                    "label": 0
                },
                {
                    "sent": "This is video from the system we built was in, was built in 1990.",
                    "label": 0
                },
                {
                    "sent": "One 1992 the video I think is from 1993.",
                    "label": 0
                },
                {
                    "sent": "This is actually my phone number at Bell Labs.",
                    "label": 0
                },
                {
                    "sent": "And these are the PC with a camera, TV camera and hit a hitaki the.",
                    "label": 0
                },
                {
                    "sent": "Images is acquired and then the system recognizes it and this is a 46 PC, so I'm sure some of you are not actually born when the 46 was around, so the 46 PC is not powerful enough to run a commercial net.",
                    "label": 0
                },
                {
                    "sent": "This kind of speed.",
                    "label": 0
                },
                {
                    "sent": "So we uses DSP board so at the time actually built chips and boards and so we use the NTSB 32 board that was capable of the amazing speed of 20 mega flops.",
                    "label": 0
                },
                {
                    "sent": "It was like it was ridiculous, it was like.",
                    "label": 0
                },
                {
                    "sent": "Incredible in floating point.",
                    "label": 0
                },
                {
                    "sent": "This is Donnie Henderson.",
                    "label": 0
                },
                {
                    "sent": "What are the engineers at Bell Labs?",
                    "label": 0
                },
                {
                    "sent": "And this is rich Howard who is the director of the whole lab at Glendale.",
                    "label": 0
                },
                {
                    "sent": "And so this was really cool demo.",
                    "label": 0
                },
                {
                    "sent": "We showed this to a executive at AT&T and he said, oh that's very, very interesting.",
                    "label": 0
                },
                {
                    "sent": "So now can you actually add those numbers?",
                    "label": 0
                },
                {
                    "sent": "That tells you how clueless the business people were at ATT.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, this is not the case at Facebook, OK?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so we can have sort of brute force approaches to object recognition.",
                    "label": 0
                },
                {
                    "sent": "Which or to kind of you can use those things to not recognize SQL objects with multiple objects and that consists.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, applying the convolutions onto a larger image and replicating the outputs and so it's a very powerful technique because it allows you to train a small accomplishment on the small window and then replicate it over a large image and use it to do object detection localization you know, etc.",
                    "label": 0
                },
                {
                    "sent": "So originally we developed this for reading cursive handwriting or handwriting handwritten words.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples of that.",
                    "label": 0
                },
                {
                    "sent": "So here you have one of those covenants.",
                    "label": 0
                },
                {
                    "sent": "It's leaking in multiple characters at the time.",
                    "label": 0
                },
                {
                    "sent": "An each output is basically a different instance of the continent that looks at us.",
                    "label": 0
                },
                {
                    "sent": "You know, a different window on the input, but it's really not multiple conditions.",
                    "label": 0
                },
                {
                    "sent": "This one commercial net and you know the structure is such that it gives you an answer for every window 3232 window on the input shifted by every four pixels.",
                    "label": 0
                },
                {
                    "sent": "So in the end there is.",
                    "label": 0
                },
                {
                    "sent": "There is a response for every window, and then you're gonna have to use a little.",
                    "label": 0
                },
                {
                    "sent": "Markov model event machine to pull out the correct answer and it's doing a pretty good job at that.",
                    "label": 0
                },
                {
                    "sent": "So this is from.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1996 or so 1997.",
                    "label": 0
                },
                {
                    "sent": "So there's the cool thing about this is that there is no need for explicit explicit segmentation, and it's for character like this.",
                    "label": 0
                },
                {
                    "sent": "It's easy to segment, but for this is a fun one.",
                    "label": 0
                },
                {
                    "sent": "But you know when you're going to when you deal with natural images, it's very hard to just find the object, so you want something that's able to implicitly kind of separate separate objects from the background.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People to use.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Format for object recognition and I've been working on this for a long time, but sort of having.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A hard time convincing the computer vision community that this was really worth looking at until recently, so we've known for it.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In time that we could do object recognition for.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It should be simple natural images like you know things like Rd signs and numbers and simple objects in complex backgrounds and fiset action and you know various other things like this.",
                    "label": 0
                },
                {
                    "sent": "That's my grandparents waiting.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then and then in the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In 2000, Computer Vision Community started to realize that you needed datasets to try to test your methods, so they came up with datasets.",
                    "label": 0
                },
                {
                    "sent": "So this is the Catholic one data set.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it had the advantage of existing, but the disadvantage of being very small.",
                    "label": 0
                },
                {
                    "sent": "30 training samples for Category one category.",
                    "label": 0
                },
                {
                    "sent": "So it's just a very small amount of training data.",
                    "label": 0
                },
                {
                    "sent": "And when you train a large enough neural net commercial net that it can see the object at reasonable size, this commercial that ends up having you know millions of parameters and you have 3000 training samples.",
                    "label": 0
                },
                {
                    "sent": "Performance really is crap.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not horrible, but it's bad.",
                    "label": 0
                },
                {
                    "sent": "In fact it's really bad.",
                    "label": 0
                },
                {
                    "sent": "So the state of the art.",
                    "label": 0
                },
                {
                    "sent": "In 2006, was about 65% correct recognition on this data set using a standard pipeline with SIFT the low level.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparse coding or it's more like K means at the mid level and then SVM at the top with some sort of pyramid pooling.",
                    "label": 0
                },
                {
                    "sent": "And if you use a straight convolutional net or the same type we use for CR in the 80s and 90s you get something like 30% correct?",
                    "label": 0
                },
                {
                    "sent": "So it's twice the error rate.",
                    "label": 0
                },
                {
                    "sent": "Half the recognition rate actually.",
                    "label": 0
                },
                {
                    "sent": "But now we discovered something.",
                    "label": 0
                },
                {
                    "sent": "If you instead of using an average pooling with a sigmoid function, you use a absolute value rectification for the nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "Or you use Max pooling.",
                    "label": 0
                },
                {
                    "sent": "Then all of a sudden the accuracy is 60%, it doubles the accuracy.",
                    "label": 0
                },
                {
                    "sent": "So say there's something about rectification that's really magical here.",
                    "label": 0
                },
                {
                    "sent": "That's what it captures, something.",
                    "label": 0
                },
                {
                    "sent": "Maybe about images may be specific about images.",
                    "label": 0
                },
                {
                    "sent": "If you combine this with contrast organization and you do a little bit of unsupervised learning, you get to 65%, which was the state of the art in 2006.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately we got this results in 2008, by which time the you know other pipeline was at 7075%.",
                    "label": 0
                },
                {
                    "sent": "But it was OK.",
                    "label": 0
                },
                {
                    "sent": "It was also doing my lab so.",
                    "label": 0
                },
                {
                    "sent": "So that was kind of disappointing.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then of course two things happened.",
                    "label": 0
                },
                {
                    "sent": "One we started having really large datasets like the image net data set which has 1.5 million training samples and the second thing that happened is we got our hands on.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Super fast GPU's and so well that causes us to get interested in building very large company format strain on GPU's for weeks at a time and see what performance we got and the first people to really kind of have a good implementation on GPU of those large networks where was.",
                    "label": 0
                },
                {
                    "sent": "The Toronto people Jeff and turn into the students that Alex Crisci and Eds discover, and so they're very clever implementation of those things on GPU's which have been sort of reproduced multiple times in various contexts since then, and in 2012 they want the image net competition.",
                    "label": 0
                },
                {
                    "sent": "So again, this 1.5 million training samples 1000 categories.",
                    "label": 0
                },
                {
                    "sent": "Of you know, general categories of objects with, you know, breeds of dogs and you know stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not really good performance.",
                    "label": 0
                },
                {
                    "sent": "I'm sure a lot of you are aware of this.",
                    "label": 0
                },
                {
                    "sent": "They got 15% error top five, which means the correct answer is no.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the top five among the 1015% of the time when computing approaches, the ones that were getting 7585% on Capital One basically do better than about 25%, so that was sort of a watershed for the computer vision community because, you know, it was the first time that the problem that's going to front of center, front, and center you know in the interest of the Community.",
                    "label": 0
                },
                {
                    "sent": "With solved with a big margin by such a method, you know lot of time they were going to competitive for some tasks, which I'll talk about later, like image segmentation.",
                    "label": 0
                },
                {
                    "sent": "There were actually quite quite good, but it was easy to ignore it.",
                    "label": 0
                },
                {
                    "sent": "And for physician also it was good for a long time.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So within a few months, Google deployed Sir.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is based on this for image tagging.",
                    "label": 0
                },
                {
                    "sent": "Baidu did the same very quickly.",
                    "label": 0
                },
                {
                    "sent": "Facebook has been doing it for a few months so there is a commercial net based image tagging it.",
                    "label": 0
                },
                {
                    "sent": "Facebook, Google and Baidu and now other companies as well actually.",
                    "label": 0
                },
                {
                    "sent": "So we have our own version of this commercial Nets.",
                    "label": 0
                },
                {
                    "sent": "We got this radio new few months after that or people we didn't participate in the original competition.",
                    "label": 0
                },
                {
                    "sent": "This is called the over feat system.",
                    "label": 0
                },
                {
                    "sent": "If you type over feet and were you in Google you'll find it and there's a piece of code you can just download and run.",
                    "label": 0
                },
                {
                    "sent": "You show it an image from a camera or from a file and it will.",
                    "label": 0
                },
                {
                    "sent": "Produce either the classification on the output or the feature vector at any level you want, and you can use this as input to whatever system you want to do.",
                    "label": 0
                },
                {
                    "sent": "This works a little bit better than this project is.",
                    "label": 0
                },
                {
                    "sent": "Network is a successor to the student of mine who.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this last year, Pearson may join Google few months ago and he participated in the latest image net competition.",
                    "label": 0
                },
                {
                    "sent": "The 2014 and his team won essentially all three of the competitions.",
                    "label": 0
                },
                {
                    "sent": "Classification detection and localization.",
                    "label": 0
                },
                {
                    "sent": "Although in the case of localization only if they use extra extra data.",
                    "label": 0
                },
                {
                    "sent": "So kind of the the the call is Google or NET which is kind of playing world on words and it's the performance of this is pretty amazing.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the filters that allow.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the system me.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip over some boring details and go to how can we do localization of objects or localization is.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can use this idea of replicating a network over overfield, so essentially we take the image and we scale it down to multiple sizes and then we run the convolutional net with the sliding window overall scale.",
                    "label": 0
                },
                {
                    "sent": "So here we only have one and here we have a number of different frames.",
                    "label": 0
                },
                {
                    "sent": "Those are the yellow squares if you want, and for each of those squares we get an answer whether you know what's the object and what's the score inside of that square.",
                    "label": 0
                },
                {
                    "sent": "But what we do, in addition to this, so we get sort of heat Maps for each category, and then we can do some sort of voting mechanism.",
                    "label": 0
                },
                {
                    "sent": "Not that simple suppression bubble out to figure out what the dominant object in the image, but if you want to localize.",
                    "label": 0
                },
                {
                    "sent": "What you need to do is also predict where the bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "So in fact here what I plotted is the showing is the bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "The predicted bounding boxes that are produced by the system.",
                    "label": 0
                },
                {
                    "sent": "So this system is looking at a particular window in the image and is trained to classify what it sees in the window.",
                    "label": 0
                },
                {
                    "sent": "The domain object in the window, but it's also trying to produce a prediction for numbers that indicate the position of the bounding box for the whole object that is seeing so it could be looking at just the head of that bear with the square by this size.",
                    "label": 0
                },
                {
                    "sent": "But if you see the head and the neck, it knows where the bear is, so it predicts where the bounding box, so there should be what you see here are the all the predictions of all the windows, regardless of where they are in the.",
                    "label": 0
                },
                {
                    "sent": "Original image and so there it's easy to do a kind of a voting mechanism to figure out that.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dominant object here is a bear.",
                    "label": 0
                },
                {
                    "sent": "That's how you do localization.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that works really well for animals of various kinds.",
                    "label": 0
                },
                {
                    "sent": "Bird.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, so you can use this for detection as well, so detection is exactly the same system except now you allow the system to produce multiple answers per image.",
                    "label": 0
                },
                {
                    "sent": "Basically the higher scoring objects that don't overlap too much.",
                    "label": 0
                },
                {
                    "sent": "So here you have two versus the ground truth, and that's the answer of the system.",
                    "label": 0
                },
                {
                    "sent": "Here is another one with Burrito works really well with burritos.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's where it makes a mistake.",
                    "label": 0
                },
                {
                    "sent": "So the correct is a person can monitor and it says mini skirt.",
                    "label": 0
                },
                {
                    "sent": "And doesn't detect a person apparently hypnotized better miniskirt.",
                    "label": 0
                },
                {
                    "sent": "Sexist comments were going to say.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So the system works very well, although at the time of the of the competition we actually got this is metrical mean average precision.",
                    "label": 0
                },
                {
                    "sent": "We were at 19% and there's two teams from University of Amsterdam and NEC also using, Network and slightly better than us and then within two weeks after we had a little more time for tuning we got 24% and then three months later Roscoe Sheet from Berkeley.",
                    "label": 0
                },
                {
                    "sent": "Also using your commercial net and a few other tweaks with sort of attention got 34% and now the Google Annette technique is at 40 something.",
                    "label": 0
                },
                {
                    "sent": "So it's progressing really quick.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the Google Annette again is a direct descendant of that system 'cause it's the same guy basically.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You are so many.",
                    "label": 0
                },
                {
                    "sent": "So that you know it's detection with people and sunglasses and babies and.",
                    "label": 0
                },
                {
                    "sent": "Dogs and French horns and.",
                    "label": 0
                },
                {
                    "sent": "Whatever this is.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other mistakes too.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He thinks that's a Corkscrew that's actually a snake and it doesn't like.",
                    "label": 0
                },
                {
                    "sent": "Green strawberries, but I don't like green strawberries either, so.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so one thing that people have done with this is that they've used this as generic feature extractors.",
                    "label": 0
                },
                {
                    "sent": "They've essentially taken the overfeed system or other similar systems, chop off the last layer and then re trying the last layer to do another task.",
                    "label": 0
                },
                {
                    "sent": "So use the this sort of a feature extractor.",
                    "label": 0
                },
                {
                    "sent": "So with this you can win competition.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distinguish cats from dogs.",
                    "label": 0
                },
                {
                    "sent": "Which means if you are cat lover, dog lover, you can eliminate one or the other from your Facebook feed.",
                    "label": 0
                },
                {
                    "sent": "When it suggested.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is Facebook I said oh if we take out the cats like half of the traffic goes so.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, there is this team at KTH.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That tried a lot of different computer vision tasks datasets by using essentially the overheat features dropping off the last layer, retraining, and they got pretty much state of the art performance on just about everything they tried or slightly below slightly above, except for one.",
                    "label": 0
                },
                {
                    "sent": "I was told yesterday to really insist on this, the Oxford team said the it's actually not working very well for the Oxford data set, 'cause the protocol the user was not appropriate.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a.",
                    "label": 0
                },
                {
                    "sent": "An application to classify to recognize buildings from the Oxford campus.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of specific object recognition if you want.",
                    "label": 0
                },
                {
                    "sent": "So for that it works OK, but not great.",
                    "label": 0
                },
                {
                    "sent": "You can use this to do similarity match.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's some.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We used to call Siamese networks.",
                    "label": 0
                },
                {
                    "sent": "Now we can group this in the.",
                    "label": 0
                },
                {
                    "sent": "Name of embedding.",
                    "label": 0
                },
                {
                    "sent": "Basic idea is you get 2 neural Nets, 2 private function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter where they are that are controlled by the same parameters.",
                    "label": 0
                },
                {
                    "sent": "The same weights you feed 2 images.",
                    "label": 0
                },
                {
                    "sent": "If the two images are semantically identical.",
                    "label": 0
                },
                {
                    "sent": "So for example 2 pictures of the same person.",
                    "label": 0
                },
                {
                    "sent": "You train the system to produce vectors that are nearby on the output.",
                    "label": 0
                },
                {
                    "sent": "You don't specify the vector, you just said the two actors that are nearby, and then if you show 2 images of different persons semantically different, then you push those two vectors apart until they are at a certain distance.",
                    "label": 0
                },
                {
                    "sent": "We call this technique so there's several loss functions you can use for this.",
                    "label": 0
                },
                {
                    "sent": "There's one that we like this kind of margin based loss function that we.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctor Lim that means dimensionality reduction by learning an invariant mapping one of those cute acronyms.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that, but.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's been used for various applications and there's one thing we like at Facebook which is embedding.",
                    "label": 0
                },
                {
                    "sent": "Pieces of.",
                    "label": 0
                },
                {
                    "sent": "Media, whatever they are.",
                    "label": 0
                },
                {
                    "sent": "Text, images, videos, people into vector space.",
                    "label": 0
                },
                {
                    "sent": "Such that distance in that vector space order products correspond to similarity of some sort.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, you have a vector for a user and a vector for a piece of.",
                    "label": 0
                },
                {
                    "sent": "Like a piece of news or an image is a dot product between the two vectors.",
                    "label": 0
                },
                {
                    "sent": "After mapping is large then that means this person is likely to be interested by that piece of content.",
                    "label": 0
                },
                {
                    "sent": "If you have a piece of text on an image and they matched, that means probably that corresponds to the image.",
                    "label": 0
                },
                {
                    "sent": "Or it could be a good query for the image or the image could be a good answer to the text.",
                    "label": 0
                },
                {
                    "sent": "So there are things like this or you know the text similar if it's to text.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of things you can do with embedding and one of the things we're trying to do is basically embed the world.",
                    "label": 0
                },
                {
                    "sent": "And one of the things we have been using this embedding techniques for is face to face recognition.",
                    "label": 0
                },
                {
                    "sent": "So this is a work I'm not involved in, although it's taking place at Facebook in my lab.",
                    "label": 0
                },
                {
                    "sent": "Since the flagman and Ming Yang mostly at Facebook AI research, and they've used a commercial net to do do face recognition, there's a bit of alignment of the face before hand.",
                    "label": 0
                },
                {
                    "sent": "That's actually very sophisticated, and then there is a training of this thing as a classifier, and then there is another phase of sort of refining the representation by using embedding.",
                    "label": 0
                },
                {
                    "sent": "Also, using criteria to produce embeddings that are small dimension and mostly binary so that you can store them on very compact.",
                    "label": 0
                },
                {
                    "sent": "Very compact.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you can search through.",
                    "label": 0
                },
                {
                    "sent": "You know, hundreds of millions of faces very, very quickly, and on standard datasets, probably datasets like the liberal face in the world.",
                    "label": 0
                },
                {
                    "sent": "This system basically performs at human level essentially.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is really exciting.",
                    "label": 0
                },
                {
                    "sent": "Another very recent, so this year.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Face it are the first people to hear about this.",
                    "label": 0
                },
                {
                    "sent": "Or at least to see the slides we participated.",
                    "label": 0
                },
                {
                    "sent": "So I had a visiting student from from Slovenia called you Richemont R and he.",
                    "label": 0
                },
                {
                    "sent": "It sort of fell into a habit of winning competitions.",
                    "label": 0
                },
                {
                    "sent": "He was visiting my lab and say, you know, what competition can win, say well, why don't you try this stereo stuff?",
                    "label": 0
                },
                {
                    "sent": "So it's a key data set which is a large data set of so ground truth stereo images with LIDAR data and he used a conversation that the training content to learn the similarity function between patches.",
                    "label": 0
                },
                {
                    "sent": "That's what you need to do is terrible matching.",
                    "label": 0
                },
                {
                    "sent": "Need to find a Patch in the left image that corresponds to a particular Patch in the right image, and when you find it you have a disparity and from that you can assume it depth.",
                    "label": 0
                },
                {
                    "sent": "And so we trained a pretty large commercial net, about 600,000 parameters to basically tell it the similarity between two patches using the ground truth from the data set, and he managed to actually beat the record.",
                    "label": 0
                },
                {
                    "sent": "So he's got 2.6% error.",
                    "label": 0
                },
                {
                    "sent": "There's some error measure of what percentage of points are.",
                    "label": 0
                },
                {
                    "sent": "Have disparity that's more more than three pixels away from the real one.",
                    "label": 0
                },
                {
                    "sent": "And the runner ups is a team from T in Toronto.",
                    "label": 0
                },
                {
                    "sent": "It's been around for awhile and so that's very impressive.",
                    "label": 0
                },
                {
                    "sent": "These are examples of.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's in the state of, so this is you get stereo with relatively small baseline for those images, black and white.",
                    "label": 0
                },
                {
                    "sent": "This is from a car driving around and you get those really beautiful.",
                    "label": 0
                },
                {
                    "sent": "That's images, so he's going to talk about this at the workshop at ECC this Saturday basically was invited today to come go to the CV Saturday because you just published results yesterday on the leaderboard, so.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And things like this you can do like what it was.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Automation stuff which I may or may not show, but before I I switch topic, let me show you a demo of the system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's see I might have to.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a I have a webcam here on this grabbing images and I can point this camera at various things.",
                    "label": 0
                },
                {
                    "sent": "So let me point it at.",
                    "label": 0
                },
                {
                    "sent": "This computer and it says notebook.",
                    "label": 0
                },
                {
                    "sent": "OK, that's about right.",
                    "label": 0
                },
                {
                    "sent": "Space bar.",
                    "label": 0
                },
                {
                    "sent": "Computer keyboard.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's about right too.",
                    "label": 0
                },
                {
                    "sent": "How about this?",
                    "label": 0
                },
                {
                    "sent": "Monitor, yeah, that's a monitor.",
                    "label": 0
                },
                {
                    "sent": "Computer keyboard.",
                    "label": 0
                },
                {
                    "sent": "That's a mouse, yeah, what about this?",
                    "label": 0
                },
                {
                    "sent": "Remote control mouse joystick yeah, one of those.",
                    "label": 0
                },
                {
                    "sent": "How about this?",
                    "label": 0
                },
                {
                    "sent": "IPod, OK, that's actually a Samsung Galaxy S4.",
                    "label": 0
                },
                {
                    "sent": "It turns out I have two phones and they're both Samsung Galaxy S4.",
                    "label": 0
                },
                {
                    "sent": "This one is a set of phone which is sort of more correct.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's black so it's a cell phone.",
                    "label": 0
                },
                {
                    "sent": "When it's white.",
                    "label": 0
                },
                {
                    "sent": "It's an iPod.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I stole a little piece of history here.",
                    "label": 0
                },
                {
                    "sent": "Prezzo Lake Warren plate.",
                    "label": 0
                },
                {
                    "sent": "OK, so it knows it's, you know, it knows it's kind of a history of some kind.",
                    "label": 0
                },
                {
                    "sent": "You know we can eat it.",
                    "label": 0
                },
                {
                    "sent": "Console me OK because you know Eggnog played, you know this food?",
                    "label": 0
                },
                {
                    "sent": "What about this?",
                    "label": 0
                },
                {
                    "sent": "What about all?",
                    "label": 0
                },
                {
                    "sent": "OK, that's what about it?",
                    "label": 0
                },
                {
                    "sent": "There's no question coffee mug.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's about right.",
                    "label": 0
                },
                {
                    "sent": "How about this?",
                    "label": 0
                },
                {
                    "sent": "Electric switch, not really.",
                    "label": 0
                },
                {
                    "sent": "Maybe it doesn't know about.",
                    "label": 0
                },
                {
                    "sent": "Actually I don't know if he knows about telephones.",
                    "label": 0
                },
                {
                    "sent": "Remote control always got keys on it right about this.",
                    "label": 0
                },
                {
                    "sent": "Two, it was loop sunglass, yeah?",
                    "label": 0
                },
                {
                    "sent": "There is no underclass, no.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a handheld computer, of course, because it's a, you know another sense on widget.",
                    "label": 0
                },
                {
                    "sent": "I don't actually have a.",
                    "label": 0
                },
                {
                    "sent": "A. OK, let's try this.",
                    "label": 0
                },
                {
                    "sent": "Central.",
                    "label": 0
                },
                {
                    "sent": "Running shoe that's better.",
                    "label": 0
                },
                {
                    "sent": "Interframe it properly loafer, yeah, it's good enough.",
                    "label": 0
                },
                {
                    "sent": "You know this is true so you get the idea.",
                    "label": 0
                },
                {
                    "sent": "Works OK.",
                    "label": 0
                },
                {
                    "sent": "I don't know why you would say to this actually.",
                    "label": 0
                },
                {
                    "sent": "Library.",
                    "label": 0
                },
                {
                    "sent": "OK, well it needs to focus.",
                    "label": 0
                },
                {
                    "sent": "Monitor, yeah, that's close enough.",
                    "label": 0
                },
                {
                    "sent": "Cash machine no.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Spot alright, let's do so right.",
                    "label": 0
                },
                {
                    "sent": "You get the idea.",
                    "label": 0
                },
                {
                    "sent": "Let me show you another similar demo.",
                    "label": 0
                },
                {
                    "sent": "But this one I can.",
                    "label": 0
                },
                {
                    "sent": "How to interpret this probabilities well so so this is trained with cross entropy loss on it's like the last way you can think of it as multinomial logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So you get you get probability estimates out.",
                    "label": 0
                },
                {
                    "sent": "Think of them as scores, really.",
                    "label": 0
                },
                {
                    "sent": "So calibrated scores?",
                    "label": 0
                },
                {
                    "sent": "Really, that's what they are.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can train so I can point the camera at something or nothing like this.",
                    "label": 0
                },
                {
                    "sent": "And then click learn.",
                    "label": 0
                },
                {
                    "sent": "Oops, need to wait for it to focus.",
                    "label": 0
                },
                {
                    "sent": "And it's kind of learned this.",
                    "label": 0
                },
                {
                    "sent": "So if I put it someplace else, it doesn't know.",
                    "label": 0
                },
                {
                    "sent": "But if I put it back in at this need to focus, here we go right now I can put it at the left.",
                    "label": 0
                },
                {
                    "sent": "Right side of the room or left side of the room depending on your point of view.",
                    "label": 0
                },
                {
                    "sent": "So you are category B.",
                    "label": 0
                },
                {
                    "sent": "And you guys the other side are category C. And the number you see here on the right side is the number of samples that I used to train.",
                    "label": 0
                },
                {
                    "sent": "So left side is C. Thread size B.",
                    "label": 0
                },
                {
                    "sent": "The screen is here A is low light, so that's why it's hard time.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use myself, but I look very different from the rest.",
                    "label": 0
                },
                {
                    "sent": "Of those pictures.",
                    "label": 0
                },
                {
                    "sent": "And you know, I could use whatever random.",
                    "label": 0
                },
                {
                    "sent": "Random thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this.",
                    "label": 0
                },
                {
                    "sent": "Project or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "Jan we have.",
                    "label": 0
                },
                {
                    "sent": "There's still screen down there.",
                    "label": 0
                },
                {
                    "sent": "We have the right side of the room.",
                    "label": 0
                },
                {
                    "sent": "Now we have this side of the room.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And you know, I could train this with a number of categories with just a few examples.",
                    "label": 0
                },
                {
                    "sent": "And what that shows you is that.",
                    "label": 0
                },
                {
                    "sent": "So first I should tell you how this works, right?",
                    "label": 0
                },
                {
                    "sent": "So you take the Internet, you drop off the last layer and you retrain the last layer using essentially what amounts to nearest neighbor's window classifier.",
                    "label": 0
                },
                {
                    "sent": "Essentially, by just storing templates.",
                    "label": 0
                },
                {
                    "sent": "So every time I hit the button that stores the template and it compares it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so let me go back to.",
                    "label": 0
                },
                {
                    "sent": "The talking and loops or the direction and.",
                    "label": 0
                },
                {
                    "sent": "So you got a few more things, so there is a really impressive application of this two image segmentation which I'm not going to show because I want to tell you about unsupervised running.",
                    "label": 0
                },
                {
                    "sent": "And since this is a 3 hour talk, I have two hours left or so so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer me.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "That's the kind of question we might ask.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To me, the way I like to see unsupervised learning is sort of capturing dependencies between variables and essentially one possible way of capturing dependencies between between variables observed variables is to find, so it's very likely that the data that comes to you is going to be living in a manifold or some surface really mention surface in the ambient space.",
                    "label": 0
                },
                {
                    "sent": "And if you find if you can train a function that tells you whenever you show it a vector, whether it's on any folder off the manifold, or even better if it tells you which direction to move.",
                    "label": 0
                },
                {
                    "sent": "Towards to get closer to the manifold.",
                    "label": 0
                },
                {
                    "sent": "Then you've basically solved unsupervised learning because we've captured the dependencies between the variables OK and the one of the best ways to do this is through an energy function that tells you how far you are from the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, a contrast function.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some kind.",
                    "label": 0
                },
                {
                    "sent": "So imagine your observations are those blue little Sears here.",
                    "label": 0
                },
                {
                    "sent": "These are two.",
                    "label": 0
                },
                {
                    "sent": "You know, this is a kind of a two different learning algorithms or two different punctuation and learning algorithm running that sort of changes and energy surface so that the blue beads have lower energy than everything else.",
                    "label": 0
                },
                {
                    "sent": "So this guy here starts flat and then the sum of those areas get pushed up so that the blue beads get have within a groove.",
                    "label": 0
                },
                {
                    "sent": "This guy.",
                    "label": 0
                },
                {
                    "sent": "The energy here is designed in such a way that it already has a groove and you just need to move it around so that it's at the right place.",
                    "label": 0
                },
                {
                    "sent": "So that's what I call energy, baserunning, and and you know, probably.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Density estimation is kind of a sort of special kids with this.",
                    "label": 0
                },
                {
                    "sent": "If you want, you can sort of formulate it in those terms, but but this technique includes things that are not probabilistic in nature, and there's a little more general, so they kind of seven different methods to strategies to shape the energy function in the right way in such a way that data points have lower energy than things outside the manifold of high density essentially.",
                    "label": 0
                },
                {
                    "sent": "So the first one is you build a machine in such a way that the volume of low energy stuff is constant.",
                    "label": 0
                },
                {
                    "sent": "So basically you have, say, a normalized distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so your energy function will be the negative log probability of some normalized distribution.",
                    "label": 1
                },
                {
                    "sent": "There's a finite fixed volume of stuff that can have low energy because your distribution is normalized.",
                    "label": 0
                },
                {
                    "sent": "But there are other non probabilistic models that have this property like PCA or K means or gas mixture models, square C and stuff like that.",
                    "label": 1
                },
                {
                    "sent": "The second strategy is to push down the energy on data points and push up everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what maximum likelihood does when your function has an explicit normalizer.",
                    "label": 0
                },
                {
                    "sent": "When your probability distribution if you density estimation as an explicit normalization term.",
                    "label": 0
                },
                {
                    "sent": "An that doesn't quite work all the time because it's possible that the normalization turned the partition function would be intractable, or its gradient will be intractable, and so there are lots of situations where you can't really do this, and there's a lot of papers in the machine learning community, including by people in this room that have to do with how do you kind of make approximations of this so that it still works for you automation sampling, whatever.",
                    "label": 0
                },
                {
                    "sent": "3rd method push down on the energy of data points and push up on chosen locations sometimes close to the data point, but so you know a little bit outside.",
                    "label": 1
                },
                {
                    "sent": "So the things like contrastive divergent racial matching noise contrastive estimation, you know every time I showed this slide, people take pictures of it and you know I should probably write this paper at some point.",
                    "label": 0
                },
                {
                    "sent": "So there's others.",
                    "label": 0
                },
                {
                    "sent": "Other methods of this type.",
                    "label": 0
                },
                {
                    "sent": "Divergences you take a data point and then you move a little bit away from it, preferentially in directions where the energy goes down, and then you push up on that point and whether it does is that it creates a local groove in the energy surface.",
                    "label": 0
                },
                {
                    "sent": "It doesn't determine the shape of the energy surface far away from data, but doesn't start problem.",
                    "label": 0
                },
                {
                    "sent": "Now before you minimize the gradient to maximize the curvature around data point, that's what schools score matching.",
                    "label": 0
                },
                {
                    "sent": "That's actually pretty much impractical for complex models.",
                    "label": 0
                },
                {
                    "sent": "This method you train dynamical system, so the dynamics causes many folders.",
                    "label": 0
                },
                {
                    "sent": "Are you around Montreal but?",
                    "label": 0
                },
                {
                    "sent": "You know and beyond.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to talk too much about this.",
                    "label": 0
                },
                {
                    "sent": "This is my favorite one.",
                    "label": 0
                },
                {
                    "sent": "So basically you use a regularizer and this is your favorite 12 by the way, for most of you at least, even if you don't know which is the user regularizer, that limits the volume of space that has low energy, and that includes things like dictionary learning for sparse coding, so additional info, space coding.",
                    "label": 0
                },
                {
                    "sent": "Essentially you can think of the sparsity term as when you do dictionary learning as a way of limiting the volume of space that can be properly constructed OK. And if you use the reconstruction error as the energy function, then that means you're kind of creating a groove that with minimum volume around the data points that did have any fold through the training.",
                    "label": 0
                },
                {
                    "sent": "The other one I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "OK, so peace.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if you have a bunch of points drawn from this spiral, few PCA with one dimension you get this is terrible approximation, but at least you have some.",
                    "label": 0
                },
                {
                    "sent": "You know the energy function is 0 on the black and sort of increase increases with intensity.",
                    "label": 1
                },
                {
                    "sent": "Here K means you know K means after you train puts little.",
                    "label": 0
                },
                {
                    "sent": "Quadratic wells all around this little surface seems that it's great, except doesn't scale up in high dimension.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This coding, scripting and sparse modeling in general, when you learn a dictionary learning is you have an energy function of this type where you have the square reconstruction error.",
                    "label": 1
                },
                {
                    "sent": "Why isn't observation?",
                    "label": 0
                },
                {
                    "sent": "WD is a dictionary matrix and Z is a sparse vector which when multiplied by the dictionary matrix is meant to reconstruct the inputs at least approximately with square penalty function.",
                    "label": 0
                },
                {
                    "sent": "And then you have this sparsity term.",
                    "label": 0
                },
                {
                    "sent": "So I like to represent this as kind of a funny type of factor graph where those are variables.",
                    "label": 0
                },
                {
                    "sent": "This one is observed.",
                    "label": 0
                },
                {
                    "sent": "This one is latent has to be inferred, and then you have deterministic functions that are those symbols and those are factors which are kind of turns in the energy function that enter additively in the energy function or multiplicatively and the likelihood function if you take exponentials.",
                    "label": 0
                },
                {
                    "sent": "So the problem with the sparse coding is that even once you've learned the dictionary, inference is relatively expensive.",
                    "label": 0
                },
                {
                    "sent": "There's lots of efficient algorithms we heard about when this morning in the first talk, CGI HD sister Fister whatever.",
                    "label": 0
                },
                {
                    "sent": "According to the centers, all kinds of efficient algorithms for this, but it's still slow if you want to do sparse coding with four thousand dimension on every window in an image, it's going to take you a few seconds.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are tricks that Ultraslan came up with to do this approximately really fast fast, but.",
                    "label": 0
                },
                {
                    "sent": "You know it's going to be expensive now.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coding essentially makes the assumption that the data is sort of a union of plane, so the energy function really has kind of linear grooves of low, relatively low dimension, and then you can add them up and you get this sort of approximation of the day.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one trick that we came up with back in 2008 or seven was to learn quick approximations of sparse coding inference.",
                    "label": 0
                },
                {
                    "sent": "So instead of running an ESTA or, you know CGI HD or whatever, we're going to train.",
                    "label": 0
                },
                {
                    "sent": "Fastenal net essentially to produce a good approximation of the ultimate solution of this coding inference problem.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The what's called a spot autoencoder, so you have the top is just pass coding the same same as I drew originally, but there's a thing at the bottom here, which is a nonlinear parameterized function which predicts tries to predict what the optimal code is for a particular.",
                    "label": 0
                },
                {
                    "sent": "Why.",
                    "label": 0
                },
                {
                    "sent": "OK, so you plug Y, you find the Z that minimizes the energy function for space coding, and now what you have is a pair of input and sparse code that represents the input and you train a feedforward network to basically predict this.",
                    "label": 0
                },
                {
                    "sent": "Alright, you can actually do both.",
                    "label": 0
                },
                {
                    "sent": "Seriously, and learn them cooperatively by having an energy function that includes two terms, three terms, the sparsity, the reconstruction and the prediction error.",
                    "label": 0
                },
                {
                    "sent": "So a very simple prioritization of this box is something like a shrinkage of a linear map of why.",
                    "label": 0
                },
                {
                    "sent": "But that is kind of very simple.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do this with group sparsity, so use those you train this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also to encoders, and we turn this on on natural image patches.",
                    "label": 0
                },
                {
                    "sent": "This is alone.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm running in the end.",
                    "label": 0
                },
                {
                    "sent": "You get business functions that end up being those oriented edge detectors.",
                    "label": 0
                },
                {
                    "sent": "That's what we get.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any decent algorithm will give you this.",
                    "label": 0
                },
                {
                    "sent": "But here is.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool tricks so the cool trick here we call.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That list which stands for learning iterative shrinkage, thresholding algorithm and the basic idea of this one is to realize this was done by my former postdoc Calla Gregor, who is now in London that declined and the basic architecture of the basic idea of this is to say look at the look at the East algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm says you take a step of gradient of the square reconstruction error and then you shrink all the coordinates by some factor.",
                    "label": 0
                },
                {
                    "sent": "OK, just bring them to 0.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to 0.",
                    "label": 0
                },
                {
                    "sent": "And then iterate this and you can prove this converges to the solution of the sparse coding inference problem.",
                    "label": 0
                },
                {
                    "sent": "We heard about this this morning.",
                    "label": 0
                },
                {
                    "sent": "In the first talk actually, so you can view this as some sort of block diagram here to take the input multiplied by some matrix, then shrink, then multiplied by some big matrix which you add to the previous result you had, shrink again and iterate this, which is kind of this iteration inside, so kind of reply me tries this thing here by writing, watching it this way is ET, plus one equals shrink of.",
                    "label": 0
                },
                {
                    "sent": "WE transpose y * Z of T where WE is.",
                    "label": 0
                },
                {
                    "sent": "One over LWD transpose, where L is some.",
                    "label": 0
                },
                {
                    "sent": "Which is constant and S is 1 -- 1 / L WW transpose.",
                    "label": 0
                },
                {
                    "sent": "OK, so the crucial idea of Vista is to say forget about defining WNS.",
                    "label": 0
                },
                {
                    "sent": "This way we're going to learn WE&S in such a way that with this fixed number of iterations of this we get the best possible approximation of Z.",
                    "label": 0
                },
                {
                    "sent": "And so this was an HTML paper in 2010.",
                    "label": 0
                },
                {
                    "sent": "This idea is being recycled by being used a lot by Guillermo Sapiro.",
                    "label": 0
                },
                {
                    "sent": "With his collaborators, one of them brothers.",
                    "label": 0
                },
                {
                    "sent": "Which one addicts OK and public man, who is now.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it was documented.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so essentially you can think of this as a recurrent neural net where the shrinkage is, you know, like your value essentially, so you can unfold this in time and then you get a feedforward neural net with shared parameters among layers which you can train with what's called backdrop through time.",
                    "label": 0
                },
                {
                    "sent": "And this works amazingly well, so you get.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the reconstruction error you get after a given number of iteration, which is on the X axis here of Vista or Vista in that case, and this list algorithm.",
                    "label": 0
                },
                {
                    "sent": "So after just one iteration, so one cycle through the shrinkage and the S matrix.",
                    "label": 0
                },
                {
                    "sent": "This is for low over completeness and high over completeness, and this is for Lister.",
                    "label": 0
                },
                {
                    "sent": "So you get much better approximation, much fast.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, this is the same thing for the coordinated descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "Uh, this is normal for.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is for dissent, this one.",
                    "label": 0
                },
                {
                    "sent": "This is the standard with them.",
                    "label": 0
                },
                {
                    "sent": "This one is for kind of a partial S matrix that is low rank.",
                    "label": 0
                },
                {
                    "sent": "You can make.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conditional versions of this, where the dictionary is not just a matrix, but it's basically a filterbank for images.",
                    "label": 0
                },
                {
                    "sent": "And you get beautiful filters.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They look pretty nice, so this is Patch level training and this is image level training with convolutional filters an the what happens there is that the filters you learn are much less redundant than what you get here because here you train on patches and then you apply those filters conventionally and you get redundant codes because you get here you can observe that you get horizontal edge at various locations in the image, which is kind of useless because you're going to apply this convolutional in the 1st place so.",
                    "label": 0
                },
                {
                    "sent": "So it's not a good idea.",
                    "label": 0
                },
                {
                    "sent": "So if you train at the image level, all those filters know that you know they have a copy of themselves in another place, so they they spend their energy.",
                    "label": 0
                },
                {
                    "sent": "You kind of.",
                    "label": 0
                },
                {
                    "sent": "Getting more diverse and so you get crosses and centers around and and sort of gradings an you know corners and stuff like that which is really great.",
                    "label": 0
                },
                {
                    "sent": "Here's a nice corner as well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And there's at least one application.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whether it works really well, unfortunately not many.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is you train those little autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Convolutional sparse autoencoders unsupervised.",
                    "label": 0
                },
                {
                    "sent": "OK, when you're happy with it, you get rid of the feedback, you just keep the feedforward path which basically is filterbank and linearity, and pulling.",
                    "label": 0
                },
                {
                    "sent": "If you do, good sparsity is appalling, and that's like the first stage of a commercial net, so you use that as your first major accomplishment for initialization of the filters, and then you stick a second layer.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To obtain that unsupervised again when you're happy, you get rid of them.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back and now what you have is A to stay.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Company format where the weights are pre trained using unsupervised learning and we've.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are these two?",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different things, but particularly pedestrian detection.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are performance measures which I'm not going there.",
                    "label": 0
                },
                {
                    "sent": "Basically precision recall curves, so good is kind.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know a curve that sort of is to the bottom left is good, and those are values systems in literature as of about two years ago.",
                    "label": 0
                },
                {
                    "sent": "So this is the conversation that pre trained with this unsupervised method and then fine tune with back prop for pedestrian detection.",
                    "label": 1
                },
                {
                    "sent": "At that time it was a record Holder.",
                    "label": 0
                },
                {
                    "sent": "Since then people have gotten better results.",
                    "label": 0
                },
                {
                    "sent": "But interesting thing is that you need to compare this curve with that curve which is the same commercial net which is not pre trained.",
                    "label": 0
                },
                {
                    "sent": "Just train from random initial conditions with supervised backdrop and so it goes from sort of middle of the pack to kind of best system at the time.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of really impressive.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately that's pretty much the only application for which.",
                    "label": 0
                },
                {
                    "sent": "SB training seems to help, so we haven't really tried this in a big way on things like image, net 'cause.",
                    "label": 0
                },
                {
                    "sent": "It's kind of this.",
                    "label": 0
                },
                {
                    "sent": "Practical issues with that, but there are very few applications for which unsupervised training like this helps.",
                    "label": 0
                },
                {
                    "sent": "Yes, training.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same data set?",
                    "label": 0
                },
                {
                    "sent": "Or is it on a larger datasets on the same data set?",
                    "label": 0
                },
                {
                    "sent": "Actually, yeah, I mean you could train on.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, but usually you know because it's unsupervised, you don't have the overfitting problem usually have.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, I think the answer question was done on the Berkeley data set.",
                    "label": 0
                },
                {
                    "sent": "If I remember, at least for the first layer, you get the same features anyway.",
                    "label": 0
                },
                {
                    "sent": "So this is what the.",
                    "label": 0
                },
                {
                    "sent": "Pedestrian detection works, so again the network is applied conditionally to the image at multiple scales, and then there is some sort of maximum feed system, but just for two categories.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's variations of this.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There the sparsity constraint is kind of a group sparsity type constraint, where you have.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take blocks of features.",
                    "label": 0
                },
                {
                    "sent": "And you insist that the L2 norm of those bugs of features be.",
                    "label": 0
                },
                {
                    "sent": "SM and Usum over those bugs.",
                    "label": 0
                },
                {
                    "sent": "But otherwise it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's just what I need, so there's a long history of those group sparsity stuff going back to.",
                    "label": 0
                },
                {
                    "sent": "Essentially, he Varian Hoyer about 15 years ago or 13 years ago called subspace.",
                    "label": 0
                },
                {
                    "sent": "I see this was for kind of square analysis filters.",
                    "label": 0
                },
                {
                    "sent": "And Toronto also had a few ideas around those lines, and there's sort of a evolution.",
                    "label": 0
                },
                {
                    "sent": "There's something called reconstruction ICA coming out quickly, which is sort of a linear encoder and decoder, which doesn't make any sense to me.",
                    "label": 0
                },
                {
                    "sent": "But then this sort of nonlinear versions.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of sparsity encoders that actually predate.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction ICA.",
                    "label": 0
                },
                {
                    "sent": "So here is an example where each square here is a business function or is actually an analysis filter, so it's in the encoder.",
                    "label": 0
                },
                {
                    "sent": "And we have 256 of those filters and we arrange them on a, you know.",
                    "label": 0
                },
                {
                    "sent": "Dreamed up artificial toroidal topology.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a Taurus where the left side touches the right side on the top touches the bottom and the groups.",
                    "label": 0
                },
                {
                    "sent": "Here are groups of 36 filters where the L2 norm of those groups they overlap by three, so you stab them every three things, so he creates an artificial topology in this kind of territorial.",
                    "label": 1
                },
                {
                    "sent": "Topology and what happens is that the system wants to turn on the smallest number of groups as you can, because that's the sparsity constraint.",
                    "label": 1
                },
                {
                    "sent": "This crusty penalty on the groups, and so the best way for you to do this is to regroup similar filters to learn filters in such a way that similar filters are within a single group, because similar filters will tend to turn themselves on for, you know the same input for a given input.",
                    "label": 1
                },
                {
                    "sent": "Similar filters were all turned on, so if you can regroup all the filters that turn on for a particular input within a group, then only that group will be on.",
                    "label": 0
                },
                {
                    "sent": "And that will be very sparse, and so you spontaneously the system can build those really nice topographique graphic Maps.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you do this without shared weights on you.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of, you know, kind of put those filters around an image.",
                    "label": 0
                },
                {
                    "sent": "You get those you get.",
                    "label": 0
                },
                {
                    "sent": "This is for 4X over completeness.",
                    "label": 0
                },
                {
                    "sent": "Here again you do this group sparsity.",
                    "label": 0
                },
                {
                    "sent": "You get those nice looking sort of continuously varying Maps with sort of singular points here that are called pin holes.",
                    "label": 0
                },
                {
                    "sent": "An when North scientist spoke the visual cortex of various for animals with electrodes, they figure out that the organization said activity of neurons organized spatially in pictures that are very much like this.",
                    "label": 0
                },
                {
                    "sent": "In fact, it just like that.",
                    "label": 0
                },
                {
                    "sent": "So this is from.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No Science Journal where the color indicates the dominant orientations activity and this is what the algorithm produces.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example where the algorithm produces.",
                    "label": 0
                },
                {
                    "sent": "OK, let's see I want to say.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is another way to do sparse coding and I've not talked about this very much.",
                    "label": 0
                },
                {
                    "sent": "There's a paper that we had at nips a few years ago with auto Slam and Carol Gregor and the idea of this is kind of a non convex.",
                    "label": 0
                },
                {
                    "sent": "Funny way of doing sparse coding which allow you to structure this pass coding makes connection with no science by using the notion of lateral inhibition.",
                    "label": 0
                },
                {
                    "sent": "So here is the energy function here.",
                    "label": 0
                },
                {
                    "sent": "For this sparse coding model with square reconstruction error, we know that that's the same thing and then the sparsity term is something like this where you have an S matrix which hopefully is symmetric and then you have the vectors here, which is the code vector.",
                    "label": 0
                },
                {
                    "sent": "The Spotswood vector you take the this is kind of funny notation to say that we are taking the absolute value of all the components.",
                    "label": 0
                },
                {
                    "sent": "So basically rectified components of the vector an.",
                    "label": 0
                },
                {
                    "sent": "You compute this this bilinear form and so you can think of it in those terms.",
                    "label": 0
                },
                {
                    "sent": "If you have a nonzero term in the S matrix.",
                    "label": 0
                },
                {
                    "sent": "It means that say SIJ is non zero.",
                    "label": 0
                },
                {
                    "sent": "That means you pay a price for making the inz drain on zero at the same time right?",
                    "label": 0
                },
                {
                    "sent": "So it creates a mutual information between zero and DJ if one of them goes to 0 then this term in the energy function goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So it allows you to do is build particularly matrix matrices that have a particular structure.",
                    "label": 0
                },
                {
                    "sent": "That kind of organizes the lateral inhibition.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can organize it as a tree, so he would have drawn is.",
                    "label": 0
                },
                {
                    "sent": "I've organized all the filters around the tree where if you have a link in that tree then you don't have.",
                    "label": 0
                },
                {
                    "sent": "In non zero terminate you have a zero in the estimator in the corresponding matrix and every other term is non zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so two things here in each other.",
                    "label": 0
                },
                {
                    "sent": "But if there are children in each other, they don't invite each other.",
                    "label": 0
                },
                {
                    "sent": "In fact there is an S term that depends on the tree distance but OK. And what you get is this.",
                    "label": 0
                },
                {
                    "sent": "Again those kind of organization of this tree according to kind of sort of continuous variation you get low frequency at the center high frequency at the periphery and this sort of some sort of continuously.",
                    "label": 0
                },
                {
                    "sent": "During orientation, because of this tree tree structure, you can also have.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know other topologies of various kinds.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is sort of attempts.",
                    "label": 0
                },
                {
                    "sent": "Feeble attempts at solving the unsupervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "The bad news is that those things you know are cute.",
                    "label": 0
                },
                {
                    "sent": "You can have nice pictures that you know get scientists interested, but in terms of practical applications, there is really not much so far.",
                    "label": 0
                },
                {
                    "sent": "It really doesn't work as well as just supervised running with tons of data.",
                    "label": 0
                },
                {
                    "sent": "And so there is a sense that for things like video and natural language and perhaps other kind of large scale applications and supervised learning will help us and we're still looking for the perfect unsupervised learning rule, particularly if it's one that.",
                    "label": 0
                },
                {
                    "sent": "It works, you know, is basically a single learning world for supervised and unsupervised.",
                    "label": 0
                },
                {
                    "sent": "So something like both machines, except that both machines don't work.",
                    "label": 0
                },
                {
                    "sent": "As I said previously, or don't scale at least.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another Ave we're exploring which I'm not going into the details of is the wild kind of separation or the factorization of of inputs into independent factors.",
                    "label": 0
                },
                {
                    "sent": "So when we run.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we trying to spot encoder with group sparsity where we get our sort of invariant response units.",
                    "label": 0
                },
                {
                    "sent": "That sort of respond with a little bit of invariants to change the position of the features, but then we lose information about position.",
                    "label": 0
                },
                {
                    "sent": "So one question is how do you keep that information as kind of a complementary bit of information that you can separate and kind of factor out right?",
                    "label": 0
                },
                {
                    "sent": "So if you could do this well separation which we know is going on in the.",
                    "label": 0
                },
                {
                    "sent": "Visual cortex.",
                    "label": 0
                },
                {
                    "sent": "He's anatomically we think we can sort of.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of problems would be able to solve, so we've been sort of playing with between quarters that do this sort of continuous wear separation.",
                    "label": 0
                },
                {
                    "sent": "We know some success, but not a huge amount of success.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm completely out of time, so I'm going to stop here and thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}