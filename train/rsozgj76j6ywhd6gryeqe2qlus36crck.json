{
    "id": "rsozgj76j6ywhd6gryeqe2qlus36crck",
    "title": "Determinantal Point Processes",
    "info": {
        "author": [
            "Ben Taskar, Department of Computer and Information Science, University of Pennsylvania"
        ],
        "published": "Jan. 23, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_taskar_point/",
    "segmentation": [
        [
            "Alright, thanks Arthur and Song and Alex for inviting me, so I'm going to talk about these processes that are fairly exotic to machine learning, at least up until a couple of years from now.",
            "They come from physics and commit Erickson random matrix theory and will be working in my group in is trying to make them useful and.",
            "Appealing to this community in machine learning community, so this is joint work with Alex Kulesza who's here and Jennifer Gillenwater will give a talk about this.",
            "Some of this work in the main conference."
        ],
        [
            "I'm going to motivate things with two very simple example can applications and then move on to some more complex stuff.",
            "So imagine you're doing image search and you search for the word Jaguar.",
            "You might get these hits that are basically the most frequent, the most salient hits for Jaguar, but you know."
        ],
        [
            "To satisfy, the largest number of users you might actually want to return a diverse set of results that cover kind of the space of possible things you could have meant.",
            "So that's one notion of diversity that you might want to model in these kinds of search problems.",
            "Another."
        ],
        [
            "Kind of similar problem in some way to this is some summarization.",
            "So suppose you have a bunch of articles on a topic and you want to build a short."
        ],
        [
            "Summary of what happened.",
            "Alright, so here's an example."
        ],
        [
            "If you remember the Republican primaries, some of us want to forget those, but you know you're trying to summarize what happened, right?",
            "So you might look at sentences?",
            "Or are titles that are very common and frequent and salient, and that you would get."
        ],
        [
            "But if you're trying to sort of cover the space of what actually happened during that time, there was also a few other candidates.",
            "Yeah, so and sort of you would get much broader coverage of what happened, right?",
            "So in general these two problems are kind of subset selection problems where you're looking for a subset of items that is both good and salient, but also diverse and covers a large proportion of the space you're interested in."
        ],
        [
            "So what point processes?",
            "Let's deconstruct the name so point process is a big name for something fairly simple is just a distribution over subsets of a set.",
            "OK, you can make the set continuous, but in what I'm talking about is discrete, so we're talking about discrete point processes.",
            "I have some ground set calligraphic Y and have a distribution of all possible two to the N subsets."
        ],
        [
            "So the simplest discrete point process is just a uniform independent kind of flip a coin for every particular item.",
            "So in general, going to end items and there's two of them possible subsets, and we're going to know those subsets by why."
        ],
        [
            "So as I said, the independent process is just this for each item I I flip a coin and keep it or not, right?",
            "So it's a simple point point process."
        ],
        [
            "You know the uniform point process is the one that basically picks any set arbitrarily, so we're interested in much more interesting processes than this one, but also somehow we want to retain the efficiency because this is a very large space."
        ],
        [
            "So if we take just a plain and then you know, discretize it and just flip a coin for every pixel on the plane, we might get a sample.",
            "It looks like this, so this is kind of approximation to apportion process on a plane."
        ],
        [
            "Determinantal point process.",
            "Using a Gaussian kernel, and I'll explain a little bit later.",
            "Looks like this.",
            "So what you see here is sort of much less clumpy.",
            "Distribution fewer holes, smaller holes than this one, right?",
            "So what it favors is distributions of points that are fairly diverse and cover the space OK."
        ],
        [
            "So that's the detrimental part, and so, how do you measure this diversity?",
            "I keep talking about and in DPS this notion of diversity is very Jim metric, so think of it this way, so coming back to the example of images, we might have some feature description of images, right?",
            "So I might be the color histogram.",
            "It might be sifts and just and other things you might think of, so that's sort of a vector in some high dimensional space that describes our item.",
            "And."
        ],
        [
            "There might be another vector describes a different car, and so the notion of diversity that we are interested in."
        ],
        [
            "It is going to be this area and more generally volume of the space spanned by these things, so these things are very similar, so the area of this of this parallelogram is fairly small."
        ],
        [
            "Compared to for example, the features of this image of a Tiger and the sort of Jaguar and Jaguar the car so.",
            "That's the kind of notion of diversity."
        ],
        [
            "That were after an so."
        ],
        [
            "If you sort of take all of your items and put him in the set of data matrix, each row corresponds to an item and then."
        ],
        [
            "Think of the kernel matrix."
        ],
        [
            "Which is just the dot product of all of these guys.",
            "Each entry is going to be sort of the similarity between two items."
        ],
        [
            "Then the probability that mental point process the probability of selecting a subset of these rows and columns at the same time is proportional to the determinant of the submatrix of this kernel.",
            "OK, and this is defined in physics."
        ],
        [
            "As a model of fermions which are repulsive partic."
        ],
        [
            "So what does that look like?",
            "So suppose we have four items and we're selecting a subset of those four items.",
            "Here's my little kernel or four items."
        ],
        [
            "I'm going to select two and four, so what's the probability of two and four?",
            "Well, I'm going to find."
        ],
        [
            "Room columns that correspond to this two items and then compute."
        ],
        [
            "The principle minor determinant of that what that corresponds to."
        ],
        [
            "Is the squared volume spanned by those items in the subspace that they are in?",
            "OK, so that's that."
        ],
        [
            "Notion of diversity.",
            "So it seems like a natural thing if you like sort of geometric notions, but that's sort of the not the nicest part of DPS.",
            "The nicest part is the fact that you can do this."
        ],
        [
            "Right, the probability of a an item is easily normalized so we can compute probabilities normalization in North Cube time.",
            "And then we'll show techniques for doing this faster."
        ],
        [
            "Another beautiful property is the fact that you can compute all the marginals that you would want in also kind of N cubed or faster.",
            "So probability that particular subset was included in what was selected is given by determinant of another matrix, which is easy."
        ],
        [
            "To compute it, as you know, an inverse helpless I and then multiplying by L, right?",
            "So you just do an inverse or eigenvalue decomposition and you got this kernel matrix K which gives you all of the marginals, let's."
        ],
        [
            "Fact this a little bit so suppose."
        ],
        [
            "I'm interested in the probability of a particular item being selected.",
            "That's just the principle minor with just that one entry in diagonal, and it's just Ki if I'm look."
        ],
        [
            "The probability of two items get selected.",
            "I picked out a matrix of two by two and then if I expand it out, I have the kiks JJ it's just the determinant formula and if you sort of plug in back the probability definitions you see this sort of probability of two items being included is their marginal probabilities minus this coupling term.",
            "That makes them less likely.",
            "So the more so it's K squared, right?",
            "So the more similar they are to each other in the sort of dot product space, the more likely they are not to be selected right?",
            "And so this is always squared.",
            "This is always in."
        ],
        [
            "Urging diversity.",
            "OK, so you can see that you can think about volumes.",
            "Picking things more definitely, but you can also think about actually what happens with the entries of the matrix.",
            "So a similar thing happens with higher order determinant."
        ],
        [
            "Alright, so I told you about normalization and marginally marginalization.",
            "We're also interested in sampling from this model, and so sampling I'm going to describe the algorithm.",
            "But the key step in this is the eigenvalue decomposition in all of these methods were doing inverses of matrices, and we're doing basically things that are just taking the kernel matrix and doing some kind of eigen decomposition on it, right?",
            "And so that's the key step that's fairly expensive, and we're going to work on this.",
            "When we combine this with graphical models in large scale DPS."
        ],
        [
            "And this is just a picture of that."
        ],
        [
            "So let's come back instead of back a little bit.",
            "This notion of how do you kind of come up with this matrix L?",
            "What do these numbers mean, right?",
            "So we mentioned that you know.",
            "Just think of each item as a vector in space, right?",
            "But we're going to sort of decompose that."
        ],
        [
            "Into two pieces, sort of piece that refers to sort of the quality of the item, which is basically the length of that vector, and then so this is a."
        ],
        [
            "Equalities score which is non negative and then."
        ],
        [
            "A normalized kind of direction vector that corresponds to sort of you know which which direction this item is pointing.",
            "So this dot product in the middle of fee and fee is sort of the cosine of those two vectors, and then this basically scales them out right?",
            "So this is sort of the unary term and this together with the other one give you the pairwise term.",
            "So."
        ],
        [
            "Just to illustrate this, imagine basically have two vectors, Q measured the length and feed the direction and so as I change them."
        ],
        [
            "For example, increased Q of one of the vectors the area increases, so this this two items will be more likely to be picked if."
        ],
        [
            "I sort of change the directions of the fees.",
            "I make them more similar than I decrease the area and I make this less likely, right?",
            "So that's the notion of kind of quality and similarity in this decomposition allows us to do a bunch of stuff.",
            "You will see that this shooting position into these two parts is critical in making these things efficient and learnable."
        ],
        [
            "OK, so for learning, so there's a probabilistic model and we can basically given a bunch of data, do conditional likelihood or maximum likelihood estimation, and So what we know how to do well at this point is how to learn this quality model and so a natural thing to do because it's a non negative non negative quantity is to do a log linear kind of model right?",
            "So we have some features that basically measure kind of the salience of a particular item, right?",
            "How frequent it is, how important it is and so on.",
            "And then we learn how to weigh them from data where data is.",
            "You have subsets of a set that were selected by an expert and you're going to learn them back.",
            "And turns out this optimizing Theta using maximum likelihood is is the objective function is concave, so estimation is easy.",
            "Computing gradients basically involves computing marginals, so everything is nice and kind of N cubed in the worst case there's an open question of how to learn the diversity features, the fees.",
            "So this is learning Q, but how to learn fees is still difficult and we have some sort of conjectures, but in general we sort of hand tune it and sort of this is similar to kind of the kernel matrix.",
            "In kernel methods you have.",
            "Pick that or in cross validated in some way, and then you learn the other bits right?",
            "And so learning that kernel matrix for diversity would be an interesting thing that we don't want to do well right now."
        ],
        [
            "So new summarization is a task that NLP community.",
            "There's a bunch of benchmarks on this, so there's competitions on this every year, and so we tested this model on these competitions from 2003 in 2004, and the way it's setup is basically if 10 news articles per group and then you're trying to come up after out of these ten articles summary, which has about 660 characters, so very small set of set of sentences and the way it's done is typically big.",
            "Sentences that are important in diverse and the way you value this.",
            "If you know how like MTS evaluated by just comparing what the summary project you generated with what humans did, and you compare based on unigrams, bigrams etc.",
            "Right so it's kind of a rough metric of how well your summary corresponds to what humans do."
        ],
        [
            "And so these competitions, been, uh."
        ],
        [
            "For awhile now, so a simple baseline if you know, but maximum marginal relevance is a standard method in search to get diversity.",
            "I don't have time to talk about it, but that's sort of the baseline.",
            "One of the baselines, and so Rouge.",
            "You can do this is F measure.",
            "You can ignore the other ones if you don't care, but the specifics this."
        ],
        [
            "Sort of the best system at the time of the competition, which is a few years back."
        ],
        [
            "More recently, Jeff Bilmes and and his student.",
            "This is sort of the best performing system so far as of 2012."
        ],
        [
            "And until sort of we tried ourselves and so our our methods using GPS kind of allow us to learn very rich models that express quality and sort of dominate these models, right?",
            "So there's two methods of actually predicting I didn't get to talk about too much about how you predict, so I can sample from your DP.",
            "I can do map, kind of like what Jenny talked about earlier in the conference."
        ],
        [
            "And then another way you can do this which actually works much a little better is minimum Bayes risk decoding.",
            "So if you know what that is is basically there's a risk function which is Rouge and you have approximate posterior, so you can minimize expected risk because you have probabilistic model and so we do this approximate minimization of expected risk, which works much better.",
            "At least for this problem, just works actually sentences.",
            "Yeah, I didn't say yeah, so it's basically it's called extractive summarization and you just extract sentences and there's you know there's elaborations on this where you can select subsets of sentences and kind of shrink them.",
            "But this is the simplest one.",
            "The cleanest version of this.",
            "So."
        ],
        [
            "OK, so it works pretty well, at least for on these kinds of problems.",
            "But then there was not very large.",
            "It was something like 253 hundred sentences to pick from.",
            "What if you have a very very large number of images you want to select a subset from?",
            "So."
        ],
        [
            "So what we're going to do is do with sort of the inverse kernel trick instead of sort of kernel Ising things.",
            "In going into this number metric space, we're going to actually come back to the parametric space, so you know the way we build this matrix L is we have quality, which is sort of the diagonal matrix, and then we have these similarity features.",
            "So assuming that these guys are kind of low dimensional, the similarity features, then we can do."
        ],
        [
            "This rearrangement to get essentially kind of like a."
        ],
        [
            "Variance matrix C. Which will be of dementia."
        ],
        [
            "In D by D right so D was the dimension of this thing, right?",
            "So if the number of features defining the similarity is smaller than N, this makes a lot of sense, right?",
            "The nice thing about these models this transformation is that LNC have the same eigenvalues.",
            "The non zero eigenvalues and the eigenvectors of these two things are related in a very simple way.",
            "So basically if you can do things with C You can reconstruct things from L efficiently."
        ],
        [
            "So, but in some applications this is not enough, right?",
            "So you might have something that's going to be very low res D. Small, but we want to deal with sort of high resolution sort of similarity features, where D is fairly large.",
            "So what can we do about that?",
            "And so probably."
        ],
        [
            "A bunch of you have guests were going to project this down into even lower dimensional space, and then we're going to try to see what we can guarantee by this.",
            "So we have this big matrix, an items dimensions right each.",
            "Each row.",
            "Here is an image or features of an image or a sentence."
        ],
        [
            "And so we're going to do a projection to get something lower dimensional."
        ],
        [
            "And the question is, what should I watch the projection be?",
            "Well, turns out the simplest."
        ],
        [
            "Thing works pretty well, which is."
        ],
        [
            "Just the random projection, right?",
            "Just you know."
        ],
        [
            "Matrix with Gaussian random entries."
        ],
        [
            "There's a very well known result about random projections for preserving distances, right?",
            "So if I have a bunch of distances and."
        ],
        [
            "A bunch of points in space and I randomly projection to log N dimensions, where N is the number of these items.",
            "Then we know."
        ],
        [
            "For these things, the distances between them are preserved nicely, so this is Johnson Lindenstrauss.",
            "More recently, people looked at."
        ],
        [
            "Um?"
        ],
        [
            "Preservation of."
        ],
        [
            "Volume, so now if I project things using your own projections, what happens not to just distances but also volumes of things like like areas and higher dimensional things.",
            "So it turns out that you can do for all volumes, but you can do it 2 volumes up, two K vectors and so on.",
            "And you can show bounds on the volumes being preserved as well.",
            "And volumes are the thing that defines GPS, so having this as a tool allows us to basically give guarantee."
        ],
        [
            "Is on the approximation ratio."
        ],
        [
            "Video of the distribution.",
            "So we have P is the distribution of DB distribution and P~ is a distribution.",
            "Using this project.",
            "It randomly projected kernel, right?",
            "And we could look at the variational distance between these two distributions, right?",
            "So this one defined fully fully dimensional fee.",
            "This is the projected fee and we look at variational distance and so we can basically guarantee that we only need of kind of log North projections in order to get reasonable accuracy right?",
            "So this allows us to kind of project things.",
            "Without being penalized too much, and so it's a really nice consequences of this kind of volume preserving projections."
        ],
        [
            "So that allows us to basically go from."
        ],
        [
            "Big to small D and so this is sort of the picture of at this point of what we can do.",
            "So if we have small and small D, then you can use the GP in this sort of L form.",
            "Or C form doesn't matter if you have large end but kind of small dimension D, you can do deal dual DP over here when you have large D but small N just do standard P and then here you do the random projection and you get something reasonable.",
            "Alright.",
            "So."
        ],
        [
            "Now I want to go further.",
            "So what if I did?",
            "My end is exponential, So what does that mean?",
            "So this is sort of coming to the confluence of graphical models, indeterminant and kernels.",
            "Imagine that my ground set of things that I'm selecting from is now not just something that I can list, but it's actually a set of possible structure.",
            "So the set of all parses of a sentence, the set of all.",
            "For example, you know, sort of segmentations of a sequence, or instead of all paths in a graph.",
            "OK, so this exponentially many of these things and I still want to select a subset of these structures."
        ],
        [
            "So one example with the multiple pose estimation, I'll talk about this.",
            "So imagine you have."
        ],
        [
            "And you know an image and you're trying to basically find where all the people are right, and so these are kind of structured, structured objects that I'm trying to select from."
        ],
        [
            "So this is expanding exponentially.",
            "Many of these items I'm picking from we can construct N. We can construct C. Really unless we make some assumptions right?",
            "So what they assume?"
        ],
        [
            "And so we're going to factorize the model using essentially graphical model kind of tricks.",
            "And then we're going to go to the tools I described.",
            "These dual DPS and these are kind of one more trick that I'll mention briefly.",
            "Alright, so."
        ],
        [
            "Does that mean so?",
            "Here's where the kind of MRF get embedded inside of DPS.",
            "We're going to say that each item now is going to be basically a collection of little pieces of a structure, right?",
            "So an item I is a full trajectory in some.",
            "Hmm, let's say right, and so this is kind of, you know, some path, right?",
            "So imagine this is a sequence model and then an item is a trajectory in that state space."
        ],
        [
            "We're going to assume that the quality part factorizes over those parts.",
            "OK, so imagine here just an MRF or CRF in you know over a sequence, OK?"
        ],
        [
            "And."
        ],
        [
            "Diversity features factorize just like the same way, but additively.",
            "OK, so then, if you want to compute the similarity between two trajectories or two paths in the graph, I compute all of their similarity features and then do a dot product."
        ],
        [
            "So let me just illustrate this.",
            "So this is the multiples estimation problem I was I was mentioning before, so you have my imaginative images and I want to predict where all the people are and not just where they are, but also where the arms and heads and so on.",
            "So I usually use a picture of Arthur, but there was that was too hard to do here.",
            "So this is from TV shows.",
            "This is friends and so on.",
            "And So what are these pieces?"
        ],
        [
            "Been here.",
            "And that the quality part comes from model for detecting a human body in an image so."
        ],
        [
            "It's basically model that has kind of detectors for the head."
        ],
        [
            "Sectors for torso."
        ],
        [
            "Detectors for arms and then these are put together in a graphical model, so it's called pictorial structure in computer vision or deformable parts model.",
            "But it's basically a tree structured kind of MRF, so that's the quality model, right?",
            "That's the model for picking picking one body, right?"
        ],
        [
            "The diversity model basically looks like this.",
            "We look at sort of two possible poses and then."
        ],
        [
            "Compute sort of what part of what part of the image do they overlap and then?"
        ],
        [
            "Then the diversity is just kind of hamming."
        ],
        [
            "Similarity of those things.",
            "So this would be low diversity and this would be high def."
        ],
        [
            "OK, so basically the feature just come back to this."
        ],
        [
            "Second, the features are which."
        ],
        [
            "Squares, do you occupy and then adapt?"
        ],
        [
            "With features, computes the Hamming similarity, which is the diversity measure.",
            "OK, so that's the kind of decomposition we're talking about, right?"
        ],
        [
            "Clearly everything decomposes here into some over parts, right?",
            "And that's where the graphical model tricks come in."
        ],
        [
            "So coming back to this dual representation, we have L that's N by N&N is exponential in the length of a sequence, let's say, but see just depends on the number of features, right?",
            "So in application here that was kind of the number of squares in my image.",
            "So if I have a grid of 10 by 10, D would be 100 right?",
            "Which tracks which squared as my arm and head etc fall into OK?",
            "So see if we can get to see will be very happy because we can sort of do things using using C instead of L."
        ],
        [
            "So doing this basically instead of figuring out what does each entry of C is, it involves this sum over exponentially.",
            "Many things, like over all possible trajectories.",
            "And so it's some under this Q squared I of kind of this.",
            "Essentially covariance of 2nd moment of two similarity features, right?",
            "So this is basically kind of computing."
        ],
        [
            "Variance under a distribution on structures given by Q ^2 but equality squared right?",
            "So this is something we know how to do well, right?",
            "So I'm given a graphical model given by QA squared doesn't change anything and then I'm asking to compute kind of covariances of features.",
            "OK, so if you compute covariances of features using this graphical model that I'm done and I can, I can basically do my sampling marginals, everything using C as opposed to L. And so for this we use."
        ],
        [
            "Is a second order message passing, so you can compute these marginals in a sort of naive way, but turns out there's a really nice message passing algorithm that's linear time.",
            "It's basically kind of like some product, but instead of passing around these sort of normal messages, you pass around kind of special simmering of messages.",
            "It's really not that different, but don't have time to sort of go into details.",
            "So if I have my quality models, a sequence or a tree, this is essentially linear time in the number of components.",
            "OK, so we've gone from exponential in the number of components to linear.",
            "It's still quadratic in the dimension of diversity features, right?",
            "Because I mean computing covariances of everything, but I know how to handle that.",
            "Let's see if I need to using random projections."
        ],
        [
            "Alright, so coming back to this it I just want to illustrate this on a very particular kind of what inference look like, and so the inference of the sampling inference.",
            "Basically you can think of it as kind of sampling, one structure conditioning on having it already and then generating the next one condition on that and so on, right?",
            "So so the first sample basically pics."
        ],
        [
            "So this is kind of the distribution represented illustrated with kind of smeared out marginals where yellow are the heads, green are the arms and torsos are red.",
            "Right, and so the first sample.",
            "This is real."
        ],
        [
            "Simulation is here OK and then conditioned on the sample you can now show the marginals again and so you see what what's happening now is that this marginal sort of pops up and everything around here gets completely.",
            "Push down, right?",
            "Basically in first diversity by pushing down the probability of having anything nearby the next sample."
        ],
        [
            "Picks out something here.",
            "Conditioned on these two things being here.",
            "The module distribution sort of pops up.",
            "The third thing which before had a very small marginal distribution and makes this more likely, and then the third."
        ],
        [
            "Gets gets that right.",
            "So here we kind of lucky that it actually got three, but the number of structures is itself a random variable we can control.",
            "We can condition on that variable, but in general in DPS it's a random variable and so it tries to do inference over it, not just the set of structures, but also what is the number of those structures to pick."
        ],
        [
            "For more quantitatively, we compare it as VPS with an independent sampling approach.",
            "You just kind of sample several structures independently and then a non Max suppression which is sort of people doing computer vision.",
            "You sample one structure, you will block it off and you sample another one.",
            "You know it's going to Terministic version of that and so as you please do much better, well significantly better in F1 as you increase sort of.",
            "What do you consider a hit?",
            "What is accurate right?",
            "So you can increase the radius of where did you pick the arm?",
            "And.",
            "Hand and head etc.",
            "And so the way we do."
        ],
        [
            "The DPS do it.",
            "I mean you can break it out into precision and recall, and so DPS have precision.",
            "Recall, it's sort of roughly matched, and these guys, the non Max independent are basically not balancing them right?",
            "Right?"
        ],
        [
            "Talk about one more application that's kind of related, but in very different domain from vision and here basically we want to summarize a large set of articles in a compact way and so here."
        ],
        [
            "To illustrate that with this following story, so suppose you're away for a couple of months and now you want to catch up on the news.",
            "This is saying this summer or in the spring, you know, Insta."
        ],
        [
            "Graham get a lot of users Facebook like that bought them."
        ],
        [
            "And then you know quickly after that bunch of users left.",
            "I guess there's still a bunch users still in Instagram, but anyway, this is a kind of a thread of a story, right?",
            "So news basically follows these stories overtime, and I'd like to do is take very large collection of articles and pick out these threads of stories, right?",
            "That's basically show us what's happening."
        ],
        [
            "So you know these two articles."
        ],
        [
            "Embedded in some very large network of possible articles I can pick from right and so in this little graph that I'm showing is imagine I have some sort of notion of equality of a node right which corresponds to kind of how you know how how frequent this topic is, how sailing this topic is, and edges correspond to how likely is this next article to follow on the same story, right?",
            "So you can imagine how you would construct features for that, and I'm not going to get into too many details about how we do that, but we want to do is pick.",
            "A set of stories in this huge graph that are diverse and high quality right?",
            "So a set of paths through this graph that don't overlap too much in terms of what they're talking about, but cover through the important parts of what's happening.",
            "OK, so again, it's a structured EP here where it's a DP over paths in a graph.",
            "And so."
        ],
        [
            "So we compared to the topic models.",
            "We heard about topic models and topic models are not exactly designed for picking threads, so we had to kind of take the output of a dynamic topic model and then sort of.",
            "You know, big threads using that, but this is the kind of topic models you get, and so if you kind of zoom in on why?"
        ],
        [
            "One of them you get sort of this is actually from 06 or 07.",
            "There's some there's a general kind of health topic.",
            "Related articles were looking instead is some."
        ],
        [
            "Thing more thread like and so deep is kind of designed in this application to do that and so.",
            "For example, if you zoom in on one of these."
        ],
        [
            "Stories you see sort of Pope getting sick and passing away in the next Pope getting chosen, etc etc.",
            "Right so you know the stories here are very coherent in a sense that because we sort of encode our data as a graph where links correspond to being coherent and then we pick out things that are."
        ],
        [
            "That I pick, so it's a very very large set D. We just pick basically is the similarity.",
            "Features correspond to the words, it's just a bag of words TF IDF.",
            "So it's a very large high dimensional D. So it's a big challenge.",
            "They're going to get this to work.",
            "So even just doing this naively and using Dual GP, it's a lot of memory to just store this.",
            "So we use random projections to reduce this down something reasonable and make this really really fast."
        ],
        [
            "The way we evaluated this is to wait, so there's one.",
            "I mean, these things are all you know, like three was talking about is difficult to evaluate topic models.",
            "We did it in two ways.",
            "One, there's human news summaries that these agencies output, and so we compared kind of similar metrics to Rouge compared.",
            "How are summaries kind of how much they capture from human summaries?",
            "And then the second part, which is hard to measure is quality.",
            "How sort of how sort of coherent is a thread, right?",
            "And nobody sort of creates these threads by default, so we use Amazon Turk.",
            "And gave people threads from two different systems and ask them, you know what's better and what's worse."
        ],
        [
            "So we compared several things again Rouge.",
            "This metric of how well you compare things.",
            "So these two metrics compare against human summaries."
        ],
        [
            "We compared K means K means designed for kind of dynamic dynamic clustering.",
            "There's some work to be done there, but it's basically K means so high."
        ],
        [
            "Higher, higher is better dynamic topic model and then finally."
        ],
        [
            "This is BP, where we condition on having KK threads and so we beat both of these models in terms of in terms of Rouge as well as coherence and it can talk more about how we set up the experiments.",
            "It wasn't trivial, but we you know sort of made sure that it was sort of well defined tasks for the Turkers.",
            "I'll talk, I will skip that unless there's questions about that."
        ],
        [
            "But the most important part is this part.",
            "The runtime writes the very very large large problem K means is fairly efficient on this, But then any topic models take a lot of time, so this is using publicly available code.",
            "This takes 252 seconds, so with everything right?",
            "So it's fairly fast and sort of.",
            "Practical model How we doing?"
        ],
        [
            "One more minute good.",
            "This is basically the summary slide, so it's great.",
            "So what I hope I try to convince you is that DPS are these very nice elegant models which have a lot of really efficient properties for normalization, marginals conditioning and sampling that have tons tons more applications that we've sort of scratched the surface with.",
            "There's lots of interesting open questions still and.",
            "You know, we've been trying to make them useful for learning from large scale data."
        ],
        [
            "One more piece, if you're interested in knowing more, there is a very long survey about these things is going to be a foundation trends paper coming out soon, and there's code available from Alex is web page on this as well.",
            "I can't thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, thanks Arthur and Song and Alex for inviting me, so I'm going to talk about these processes that are fairly exotic to machine learning, at least up until a couple of years from now.",
                    "label": 0
                },
                {
                    "sent": "They come from physics and commit Erickson random matrix theory and will be working in my group in is trying to make them useful and.",
                    "label": 0
                },
                {
                    "sent": "Appealing to this community in machine learning community, so this is joint work with Alex Kulesza who's here and Jennifer Gillenwater will give a talk about this.",
                    "label": 1
                },
                {
                    "sent": "Some of this work in the main conference.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to motivate things with two very simple example can applications and then move on to some more complex stuff.",
                    "label": 0
                },
                {
                    "sent": "So imagine you're doing image search and you search for the word Jaguar.",
                    "label": 1
                },
                {
                    "sent": "You might get these hits that are basically the most frequent, the most salient hits for Jaguar, but you know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To satisfy, the largest number of users you might actually want to return a diverse set of results that cover kind of the space of possible things you could have meant.",
                    "label": 0
                },
                {
                    "sent": "So that's one notion of diversity that you might want to model in these kinds of search problems.",
                    "label": 0
                },
                {
                    "sent": "Another.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of similar problem in some way to this is some summarization.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have a bunch of articles on a topic and you want to build a short.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summary of what happened.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's an example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you remember the Republican primaries, some of us want to forget those, but you know you're trying to summarize what happened, right?",
                    "label": 0
                },
                {
                    "sent": "So you might look at sentences?",
                    "label": 0
                },
                {
                    "sent": "Or are titles that are very common and frequent and salient, and that you would get.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you're trying to sort of cover the space of what actually happened during that time, there was also a few other candidates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so and sort of you would get much broader coverage of what happened, right?",
                    "label": 0
                },
                {
                    "sent": "So in general these two problems are kind of subset selection problems where you're looking for a subset of items that is both good and salient, but also diverse and covers a large proportion of the space you're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what point processes?",
                    "label": 0
                },
                {
                    "sent": "Let's deconstruct the name so point process is a big name for something fairly simple is just a distribution over subsets of a set.",
                    "label": 0
                },
                {
                    "sent": "OK, you can make the set continuous, but in what I'm talking about is discrete, so we're talking about discrete point processes.",
                    "label": 1
                },
                {
                    "sent": "I have some ground set calligraphic Y and have a distribution of all possible two to the N subsets.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the simplest discrete point process is just a uniform independent kind of flip a coin for every particular item.",
                    "label": 0
                },
                {
                    "sent": "So in general, going to end items and there's two of them possible subsets, and we're going to know those subsets by why.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, the independent process is just this for each item I I flip a coin and keep it or not, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a simple point point process.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know the uniform point process is the one that basically picks any set arbitrarily, so we're interested in much more interesting processes than this one, but also somehow we want to retain the efficiency because this is a very large space.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take just a plain and then you know, discretize it and just flip a coin for every pixel on the plane, we might get a sample.",
                    "label": 0
                },
                {
                    "sent": "It looks like this, so this is kind of approximation to apportion process on a plane.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Determinantal point process.",
                    "label": 0
                },
                {
                    "sent": "Using a Gaussian kernel, and I'll explain a little bit later.",
                    "label": 0
                },
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is sort of much less clumpy.",
                    "label": 0
                },
                {
                    "sent": "Distribution fewer holes, smaller holes than this one, right?",
                    "label": 0
                },
                {
                    "sent": "So what it favors is distributions of points that are fairly diverse and cover the space OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the detrimental part, and so, how do you measure this diversity?",
                    "label": 0
                },
                {
                    "sent": "I keep talking about and in DPS this notion of diversity is very Jim metric, so think of it this way, so coming back to the example of images, we might have some feature description of images, right?",
                    "label": 0
                },
                {
                    "sent": "So I might be the color histogram.",
                    "label": 0
                },
                {
                    "sent": "It might be sifts and just and other things you might think of, so that's sort of a vector in some high dimensional space that describes our item.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There might be another vector describes a different car, and so the notion of diversity that we are interested in.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is going to be this area and more generally volume of the space spanned by these things, so these things are very similar, so the area of this of this parallelogram is fairly small.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared to for example, the features of this image of a Tiger and the sort of Jaguar and Jaguar the car so.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of notion of diversity.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That were after an so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you sort of take all of your items and put him in the set of data matrix, each row corresponds to an item and then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of the kernel matrix.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is just the dot product of all of these guys.",
                    "label": 0
                },
                {
                    "sent": "Each entry is going to be sort of the similarity between two items.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the probability that mental point process the probability of selecting a subset of these rows and columns at the same time is proportional to the determinant of the submatrix of this kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is defined in physics.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a model of fermions which are repulsive partic.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does that look like?",
                    "label": 0
                },
                {
                    "sent": "So suppose we have four items and we're selecting a subset of those four items.",
                    "label": 0
                },
                {
                    "sent": "Here's my little kernel or four items.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to select two and four, so what's the probability of two and four?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to find.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Room columns that correspond to this two items and then compute.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The principle minor determinant of that what that corresponds to.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the squared volume spanned by those items in the subspace that they are in?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notion of diversity.",
                    "label": 0
                },
                {
                    "sent": "So it seems like a natural thing if you like sort of geometric notions, but that's sort of the not the nicest part of DPS.",
                    "label": 0
                },
                {
                    "sent": "The nicest part is the fact that you can do this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, the probability of a an item is easily normalized so we can compute probabilities normalization in North Cube time.",
                    "label": 0
                },
                {
                    "sent": "And then we'll show techniques for doing this faster.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another beautiful property is the fact that you can compute all the marginals that you would want in also kind of N cubed or faster.",
                    "label": 0
                },
                {
                    "sent": "So probability that particular subset was included in what was selected is given by determinant of another matrix, which is easy.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compute it, as you know, an inverse helpless I and then multiplying by L, right?",
                    "label": 0
                },
                {
                    "sent": "So you just do an inverse or eigenvalue decomposition and you got this kernel matrix K which gives you all of the marginals, let's.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact this a little bit so suppose.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm interested in the probability of a particular item being selected.",
                    "label": 0
                },
                {
                    "sent": "That's just the principle minor with just that one entry in diagonal, and it's just Ki if I'm look.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The probability of two items get selected.",
                    "label": 0
                },
                {
                    "sent": "I picked out a matrix of two by two and then if I expand it out, I have the kiks JJ it's just the determinant formula and if you sort of plug in back the probability definitions you see this sort of probability of two items being included is their marginal probabilities minus this coupling term.",
                    "label": 0
                },
                {
                    "sent": "That makes them less likely.",
                    "label": 0
                },
                {
                    "sent": "So the more so it's K squared, right?",
                    "label": 0
                },
                {
                    "sent": "So the more similar they are to each other in the sort of dot product space, the more likely they are not to be selected right?",
                    "label": 0
                },
                {
                    "sent": "And so this is always squared.",
                    "label": 0
                },
                {
                    "sent": "This is always in.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Urging diversity.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see that you can think about volumes.",
                    "label": 0
                },
                {
                    "sent": "Picking things more definitely, but you can also think about actually what happens with the entries of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So a similar thing happens with higher order determinant.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I told you about normalization and marginally marginalization.",
                    "label": 0
                },
                {
                    "sent": "We're also interested in sampling from this model, and so sampling I'm going to describe the algorithm.",
                    "label": 0
                },
                {
                    "sent": "But the key step in this is the eigenvalue decomposition in all of these methods were doing inverses of matrices, and we're doing basically things that are just taking the kernel matrix and doing some kind of eigen decomposition on it, right?",
                    "label": 0
                },
                {
                    "sent": "And so that's the key step that's fairly expensive, and we're going to work on this.",
                    "label": 0
                },
                {
                    "sent": "When we combine this with graphical models in large scale DPS.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a picture of that.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's come back instead of back a little bit.",
                    "label": 0
                },
                {
                    "sent": "This notion of how do you kind of come up with this matrix L?",
                    "label": 0
                },
                {
                    "sent": "What do these numbers mean, right?",
                    "label": 0
                },
                {
                    "sent": "So we mentioned that you know.",
                    "label": 0
                },
                {
                    "sent": "Just think of each item as a vector in space, right?",
                    "label": 0
                },
                {
                    "sent": "But we're going to sort of decompose that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into two pieces, sort of piece that refers to sort of the quality of the item, which is basically the length of that vector, and then so this is a.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equalities score which is non negative and then.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A normalized kind of direction vector that corresponds to sort of you know which which direction this item is pointing.",
                    "label": 0
                },
                {
                    "sent": "So this dot product in the middle of fee and fee is sort of the cosine of those two vectors, and then this basically scales them out right?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the unary term and this together with the other one give you the pairwise term.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to illustrate this, imagine basically have two vectors, Q measured the length and feed the direction and so as I change them.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, increased Q of one of the vectors the area increases, so this this two items will be more likely to be picked if.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I sort of change the directions of the fees.",
                    "label": 0
                },
                {
                    "sent": "I make them more similar than I decrease the area and I make this less likely, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the notion of kind of quality and similarity in this decomposition allows us to do a bunch of stuff.",
                    "label": 0
                },
                {
                    "sent": "You will see that this shooting position into these two parts is critical in making these things efficient and learnable.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for learning, so there's a probabilistic model and we can basically given a bunch of data, do conditional likelihood or maximum likelihood estimation, and So what we know how to do well at this point is how to learn this quality model and so a natural thing to do because it's a non negative non negative quantity is to do a log linear kind of model right?",
                    "label": 0
                },
                {
                    "sent": "So we have some features that basically measure kind of the salience of a particular item, right?",
                    "label": 0
                },
                {
                    "sent": "How frequent it is, how important it is and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we learn how to weigh them from data where data is.",
                    "label": 0
                },
                {
                    "sent": "You have subsets of a set that were selected by an expert and you're going to learn them back.",
                    "label": 0
                },
                {
                    "sent": "And turns out this optimizing Theta using maximum likelihood is is the objective function is concave, so estimation is easy.",
                    "label": 0
                },
                {
                    "sent": "Computing gradients basically involves computing marginals, so everything is nice and kind of N cubed in the worst case there's an open question of how to learn the diversity features, the fees.",
                    "label": 1
                },
                {
                    "sent": "So this is learning Q, but how to learn fees is still difficult and we have some sort of conjectures, but in general we sort of hand tune it and sort of this is similar to kind of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "In kernel methods you have.",
                    "label": 0
                },
                {
                    "sent": "Pick that or in cross validated in some way, and then you learn the other bits right?",
                    "label": 0
                },
                {
                    "sent": "And so learning that kernel matrix for diversity would be an interesting thing that we don't want to do well right now.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So new summarization is a task that NLP community.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of benchmarks on this, so there's competitions on this every year, and so we tested this model on these competitions from 2003 in 2004, and the way it's setup is basically if 10 news articles per group and then you're trying to come up after out of these ten articles summary, which has about 660 characters, so very small set of set of sentences and the way it's done is typically big.",
                    "label": 1
                },
                {
                    "sent": "Sentences that are important in diverse and the way you value this.",
                    "label": 0
                },
                {
                    "sent": "If you know how like MTS evaluated by just comparing what the summary project you generated with what humans did, and you compare based on unigrams, bigrams etc.",
                    "label": 0
                },
                {
                    "sent": "Right so it's kind of a rough metric of how well your summary corresponds to what humans do.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these competitions, been, uh.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For awhile now, so a simple baseline if you know, but maximum marginal relevance is a standard method in search to get diversity.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to talk about it, but that's sort of the baseline.",
                    "label": 0
                },
                {
                    "sent": "One of the baselines, and so Rouge.",
                    "label": 0
                },
                {
                    "sent": "You can do this is F measure.",
                    "label": 0
                },
                {
                    "sent": "You can ignore the other ones if you don't care, but the specifics this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of the best system at the time of the competition, which is a few years back.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More recently, Jeff Bilmes and and his student.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the best performing system so far as of 2012.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And until sort of we tried ourselves and so our our methods using GPS kind of allow us to learn very rich models that express quality and sort of dominate these models, right?",
                    "label": 0
                },
                {
                    "sent": "So there's two methods of actually predicting I didn't get to talk about too much about how you predict, so I can sample from your DP.",
                    "label": 0
                },
                {
                    "sent": "I can do map, kind of like what Jenny talked about earlier in the conference.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then another way you can do this which actually works much a little better is minimum Bayes risk decoding.",
                    "label": 0
                },
                {
                    "sent": "So if you know what that is is basically there's a risk function which is Rouge and you have approximate posterior, so you can minimize expected risk because you have probabilistic model and so we do this approximate minimization of expected risk, which works much better.",
                    "label": 0
                },
                {
                    "sent": "At least for this problem, just works actually sentences.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I didn't say yeah, so it's basically it's called extractive summarization and you just extract sentences and there's you know there's elaborations on this where you can select subsets of sentences and kind of shrink them.",
                    "label": 0
                },
                {
                    "sent": "But this is the simplest one.",
                    "label": 0
                },
                {
                    "sent": "The cleanest version of this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it works pretty well, at least for on these kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "But then there was not very large.",
                    "label": 0
                },
                {
                    "sent": "It was something like 253 hundred sentences to pick from.",
                    "label": 0
                },
                {
                    "sent": "What if you have a very very large number of images you want to select a subset from?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to do is do with sort of the inverse kernel trick instead of sort of kernel Ising things.",
                    "label": 0
                },
                {
                    "sent": "In going into this number metric space, we're going to actually come back to the parametric space, so you know the way we build this matrix L is we have quality, which is sort of the diagonal matrix, and then we have these similarity features.",
                    "label": 0
                },
                {
                    "sent": "So assuming that these guys are kind of low dimensional, the similarity features, then we can do.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This rearrangement to get essentially kind of like a.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance matrix C. Which will be of dementia.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In D by D right so D was the dimension of this thing, right?",
                    "label": 0
                },
                {
                    "sent": "So if the number of features defining the similarity is smaller than N, this makes a lot of sense, right?",
                    "label": 0
                },
                {
                    "sent": "The nice thing about these models this transformation is that LNC have the same eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The non zero eigenvalues and the eigenvectors of these two things are related in a very simple way.",
                    "label": 0
                },
                {
                    "sent": "So basically if you can do things with C You can reconstruct things from L efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but in some applications this is not enough, right?",
                    "label": 0
                },
                {
                    "sent": "So you might have something that's going to be very low res D. Small, but we want to deal with sort of high resolution sort of similarity features, where D is fairly large.",
                    "label": 0
                },
                {
                    "sent": "So what can we do about that?",
                    "label": 0
                },
                {
                    "sent": "And so probably.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bunch of you have guests were going to project this down into even lower dimensional space, and then we're going to try to see what we can guarantee by this.",
                    "label": 0
                },
                {
                    "sent": "So we have this big matrix, an items dimensions right each.",
                    "label": 0
                },
                {
                    "sent": "Each row.",
                    "label": 0
                },
                {
                    "sent": "Here is an image or features of an image or a sentence.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're going to do a projection to get something lower dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the question is, what should I watch the projection be?",
                    "label": 0
                },
                {
                    "sent": "Well, turns out the simplest.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing works pretty well, which is.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the random projection, right?",
                    "label": 0
                },
                {
                    "sent": "Just you know.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix with Gaussian random entries.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a very well known result about random projections for preserving distances, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have a bunch of distances and.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bunch of points in space and I randomly projection to log N dimensions, where N is the number of these items.",
                    "label": 0
                },
                {
                    "sent": "Then we know.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For these things, the distances between them are preserved nicely, so this is Johnson Lindenstrauss.",
                    "label": 0
                },
                {
                    "sent": "More recently, people looked at.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Preservation of.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Volume, so now if I project things using your own projections, what happens not to just distances but also volumes of things like like areas and higher dimensional things.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that you can do for all volumes, but you can do it 2 volumes up, two K vectors and so on.",
                    "label": 1
                },
                {
                    "sent": "And you can show bounds on the volumes being preserved as well.",
                    "label": 0
                },
                {
                    "sent": "And volumes are the thing that defines GPS, so having this as a tool allows us to basically give guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is on the approximation ratio.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Video of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have P is the distribution of DB distribution and P~ is a distribution.",
                    "label": 1
                },
                {
                    "sent": "Using this project.",
                    "label": 0
                },
                {
                    "sent": "It randomly projected kernel, right?",
                    "label": 0
                },
                {
                    "sent": "And we could look at the variational distance between these two distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So this one defined fully fully dimensional fee.",
                    "label": 0
                },
                {
                    "sent": "This is the projected fee and we look at variational distance and so we can basically guarantee that we only need of kind of log North projections in order to get reasonable accuracy right?",
                    "label": 0
                },
                {
                    "sent": "So this allows us to kind of project things.",
                    "label": 0
                },
                {
                    "sent": "Without being penalized too much, and so it's a really nice consequences of this kind of volume preserving projections.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that allows us to basically go from.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big to small D and so this is sort of the picture of at this point of what we can do.",
                    "label": 0
                },
                {
                    "sent": "So if we have small and small D, then you can use the GP in this sort of L form.",
                    "label": 0
                },
                {
                    "sent": "Or C form doesn't matter if you have large end but kind of small dimension D, you can do deal dual DP over here when you have large D but small N just do standard P and then here you do the random projection and you get something reasonable.",
                    "label": 1
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I want to go further.",
                    "label": 0
                },
                {
                    "sent": "So what if I did?",
                    "label": 0
                },
                {
                    "sent": "My end is exponential, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of coming to the confluence of graphical models, indeterminant and kernels.",
                    "label": 0
                },
                {
                    "sent": "Imagine that my ground set of things that I'm selecting from is now not just something that I can list, but it's actually a set of possible structure.",
                    "label": 0
                },
                {
                    "sent": "So the set of all parses of a sentence, the set of all.",
                    "label": 0
                },
                {
                    "sent": "For example, you know, sort of segmentations of a sequence, or instead of all paths in a graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so this exponentially many of these things and I still want to select a subset of these structures.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one example with the multiple pose estimation, I'll talk about this.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know an image and you're trying to basically find where all the people are right, and so these are kind of structured, structured objects that I'm trying to select from.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is expanding exponentially.",
                    "label": 0
                },
                {
                    "sent": "Many of these items I'm picking from we can construct N. We can construct C. Really unless we make some assumptions right?",
                    "label": 0
                },
                {
                    "sent": "So what they assume?",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're going to factorize the model using essentially graphical model kind of tricks.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to go to the tools I described.",
                    "label": 0
                },
                {
                    "sent": "These dual DPS and these are kind of one more trick that I'll mention briefly.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does that mean so?",
                    "label": 0
                },
                {
                    "sent": "Here's where the kind of MRF get embedded inside of DPS.",
                    "label": 0
                },
                {
                    "sent": "We're going to say that each item now is going to be basically a collection of little pieces of a structure, right?",
                    "label": 1
                },
                {
                    "sent": "So an item I is a full trajectory in some.",
                    "label": 0
                },
                {
                    "sent": "Hmm, let's say right, and so this is kind of, you know, some path, right?",
                    "label": 0
                },
                {
                    "sent": "So imagine this is a sequence model and then an item is a trajectory in that state space.",
                    "label": 1
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to assume that the quality part factorizes over those parts.",
                    "label": 0
                },
                {
                    "sent": "OK, so imagine here just an MRF or CRF in you know over a sequence, OK?",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diversity features factorize just like the same way, but additively.",
                    "label": 0
                },
                {
                    "sent": "OK, so then, if you want to compute the similarity between two trajectories or two paths in the graph, I compute all of their similarity features and then do a dot product.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just illustrate this.",
                    "label": 0
                },
                {
                    "sent": "So this is the multiples estimation problem I was I was mentioning before, so you have my imaginative images and I want to predict where all the people are and not just where they are, but also where the arms and heads and so on.",
                    "label": 0
                },
                {
                    "sent": "So I usually use a picture of Arthur, but there was that was too hard to do here.",
                    "label": 0
                },
                {
                    "sent": "So this is from TV shows.",
                    "label": 1
                },
                {
                    "sent": "This is friends and so on.",
                    "label": 0
                },
                {
                    "sent": "And So what are these pieces?",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Been here.",
                    "label": 0
                },
                {
                    "sent": "And that the quality part comes from model for detecting a human body in an image so.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's basically model that has kind of detectors for the head.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sectors for torso.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detectors for arms and then these are put together in a graphical model, so it's called pictorial structure in computer vision or deformable parts model.",
                    "label": 0
                },
                {
                    "sent": "But it's basically a tree structured kind of MRF, so that's the quality model, right?",
                    "label": 0
                },
                {
                    "sent": "That's the model for picking picking one body, right?",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The diversity model basically looks like this.",
                    "label": 0
                },
                {
                    "sent": "We look at sort of two possible poses and then.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute sort of what part of what part of the image do they overlap and then?",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the diversity is just kind of hamming.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarity of those things.",
                    "label": 0
                },
                {
                    "sent": "So this would be low diversity and this would be high def.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically the feature just come back to this.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, the features are which.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Squares, do you occupy and then adapt?",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With features, computes the Hamming similarity, which is the diversity measure.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the kind of decomposition we're talking about, right?",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clearly everything decomposes here into some over parts, right?",
                    "label": 0
                },
                {
                    "sent": "And that's where the graphical model tricks come in.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So coming back to this dual representation, we have L that's N by N&N is exponential in the length of a sequence, let's say, but see just depends on the number of features, right?",
                    "label": 0
                },
                {
                    "sent": "So in application here that was kind of the number of squares in my image.",
                    "label": 0
                },
                {
                    "sent": "So if I have a grid of 10 by 10, D would be 100 right?",
                    "label": 0
                },
                {
                    "sent": "Which tracks which squared as my arm and head etc fall into OK?",
                    "label": 0
                },
                {
                    "sent": "So see if we can get to see will be very happy because we can sort of do things using using C instead of L.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So doing this basically instead of figuring out what does each entry of C is, it involves this sum over exponentially.",
                    "label": 1
                },
                {
                    "sent": "Many things, like over all possible trajectories.",
                    "label": 0
                },
                {
                    "sent": "And so it's some under this Q squared I of kind of this.",
                    "label": 0
                },
                {
                    "sent": "Essentially covariance of 2nd moment of two similarity features, right?",
                    "label": 1
                },
                {
                    "sent": "So this is basically kind of computing.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance under a distribution on structures given by Q ^2 but equality squared right?",
                    "label": 0
                },
                {
                    "sent": "So this is something we know how to do well, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm given a graphical model given by QA squared doesn't change anything and then I'm asking to compute kind of covariances of features.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you compute covariances of features using this graphical model that I'm done and I can, I can basically do my sampling marginals, everything using C as opposed to L. And so for this we use.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a second order message passing, so you can compute these marginals in a sort of naive way, but turns out there's a really nice message passing algorithm that's linear time.",
                    "label": 1
                },
                {
                    "sent": "It's basically kind of like some product, but instead of passing around these sort of normal messages, you pass around kind of special simmering of messages.",
                    "label": 0
                },
                {
                    "sent": "It's really not that different, but don't have time to sort of go into details.",
                    "label": 0
                },
                {
                    "sent": "So if I have my quality models, a sequence or a tree, this is essentially linear time in the number of components.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've gone from exponential in the number of components to linear.",
                    "label": 0
                },
                {
                    "sent": "It's still quadratic in the dimension of diversity features, right?",
                    "label": 1
                },
                {
                    "sent": "Because I mean computing covariances of everything, but I know how to handle that.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I need to using random projections.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so coming back to this it I just want to illustrate this on a very particular kind of what inference look like, and so the inference of the sampling inference.",
                    "label": 0
                },
                {
                    "sent": "Basically you can think of it as kind of sampling, one structure conditioning on having it already and then generating the next one condition on that and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So so the first sample basically pics.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is kind of the distribution represented illustrated with kind of smeared out marginals where yellow are the heads, green are the arms and torsos are red.",
                    "label": 0
                },
                {
                    "sent": "Right, and so the first sample.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simulation is here OK and then conditioned on the sample you can now show the marginals again and so you see what what's happening now is that this marginal sort of pops up and everything around here gets completely.",
                    "label": 0
                },
                {
                    "sent": "Push down, right?",
                    "label": 0
                },
                {
                    "sent": "Basically in first diversity by pushing down the probability of having anything nearby the next sample.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picks out something here.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on these two things being here.",
                    "label": 0
                },
                {
                    "sent": "The module distribution sort of pops up.",
                    "label": 0
                },
                {
                    "sent": "The third thing which before had a very small marginal distribution and makes this more likely, and then the third.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gets gets that right.",
                    "label": 0
                },
                {
                    "sent": "So here we kind of lucky that it actually got three, but the number of structures is itself a random variable we can control.",
                    "label": 0
                },
                {
                    "sent": "We can condition on that variable, but in general in DPS it's a random variable and so it tries to do inference over it, not just the set of structures, but also what is the number of those structures to pick.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For more quantitatively, we compare it as VPS with an independent sampling approach.",
                    "label": 0
                },
                {
                    "sent": "You just kind of sample several structures independently and then a non Max suppression which is sort of people doing computer vision.",
                    "label": 0
                },
                {
                    "sent": "You sample one structure, you will block it off and you sample another one.",
                    "label": 0
                },
                {
                    "sent": "You know it's going to Terministic version of that and so as you please do much better, well significantly better in F1 as you increase sort of.",
                    "label": 0
                },
                {
                    "sent": "What do you consider a hit?",
                    "label": 0
                },
                {
                    "sent": "What is accurate right?",
                    "label": 0
                },
                {
                    "sent": "So you can increase the radius of where did you pick the arm?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Hand and head etc.",
                    "label": 0
                },
                {
                    "sent": "And so the way we do.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The DPS do it.",
                    "label": 0
                },
                {
                    "sent": "I mean you can break it out into precision and recall, and so DPS have precision.",
                    "label": 0
                },
                {
                    "sent": "Recall, it's sort of roughly matched, and these guys, the non Max independent are basically not balancing them right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about one more application that's kind of related, but in very different domain from vision and here basically we want to summarize a large set of articles in a compact way and so here.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To illustrate that with this following story, so suppose you're away for a couple of months and now you want to catch up on the news.",
                    "label": 0
                },
                {
                    "sent": "This is saying this summer or in the spring, you know, Insta.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graham get a lot of users Facebook like that bought them.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you know quickly after that bunch of users left.",
                    "label": 0
                },
                {
                    "sent": "I guess there's still a bunch users still in Instagram, but anyway, this is a kind of a thread of a story, right?",
                    "label": 0
                },
                {
                    "sent": "So news basically follows these stories overtime, and I'd like to do is take very large collection of articles and pick out these threads of stories, right?",
                    "label": 0
                },
                {
                    "sent": "That's basically show us what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know these two articles.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Embedded in some very large network of possible articles I can pick from right and so in this little graph that I'm showing is imagine I have some sort of notion of equality of a node right which corresponds to kind of how you know how how frequent this topic is, how sailing this topic is, and edges correspond to how likely is this next article to follow on the same story, right?",
                    "label": 0
                },
                {
                    "sent": "So you can imagine how you would construct features for that, and I'm not going to get into too many details about how we do that, but we want to do is pick.",
                    "label": 0
                },
                {
                    "sent": "A set of stories in this huge graph that are diverse and high quality right?",
                    "label": 0
                },
                {
                    "sent": "So a set of paths through this graph that don't overlap too much in terms of what they're talking about, but cover through the important parts of what's happening.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, it's a structured EP here where it's a DP over paths in a graph.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compared to the topic models.",
                    "label": 0
                },
                {
                    "sent": "We heard about topic models and topic models are not exactly designed for picking threads, so we had to kind of take the output of a dynamic topic model and then sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, big threads using that, but this is the kind of topic models you get, and so if you kind of zoom in on why?",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of them you get sort of this is actually from 06 or 07.",
                    "label": 0
                },
                {
                    "sent": "There's some there's a general kind of health topic.",
                    "label": 0
                },
                {
                    "sent": "Related articles were looking instead is some.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing more thread like and so deep is kind of designed in this application to do that and so.",
                    "label": 0
                },
                {
                    "sent": "For example, if you zoom in on one of these.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stories you see sort of Pope getting sick and passing away in the next Pope getting chosen, etc etc.",
                    "label": 0
                },
                {
                    "sent": "Right so you know the stories here are very coherent in a sense that because we sort of encode our data as a graph where links correspond to being coherent and then we pick out things that are.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That I pick, so it's a very very large set D. We just pick basically is the similarity.",
                    "label": 0
                },
                {
                    "sent": "Features correspond to the words, it's just a bag of words TF IDF.",
                    "label": 0
                },
                {
                    "sent": "So it's a very large high dimensional D. So it's a big challenge.",
                    "label": 0
                },
                {
                    "sent": "They're going to get this to work.",
                    "label": 0
                },
                {
                    "sent": "So even just doing this naively and using Dual GP, it's a lot of memory to just store this.",
                    "label": 1
                },
                {
                    "sent": "So we use random projections to reduce this down something reasonable and make this really really fast.",
                    "label": 1
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way we evaluated this is to wait, so there's one.",
                    "label": 0
                },
                {
                    "sent": "I mean, these things are all you know, like three was talking about is difficult to evaluate topic models.",
                    "label": 0
                },
                {
                    "sent": "We did it in two ways.",
                    "label": 0
                },
                {
                    "sent": "One, there's human news summaries that these agencies output, and so we compared kind of similar metrics to Rouge compared.",
                    "label": 1
                },
                {
                    "sent": "How are summaries kind of how much they capture from human summaries?",
                    "label": 0
                },
                {
                    "sent": "And then the second part, which is hard to measure is quality.",
                    "label": 0
                },
                {
                    "sent": "How sort of how sort of coherent is a thread, right?",
                    "label": 0
                },
                {
                    "sent": "And nobody sort of creates these threads by default, so we use Amazon Turk.",
                    "label": 0
                },
                {
                    "sent": "And gave people threads from two different systems and ask them, you know what's better and what's worse.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compared several things again Rouge.",
                    "label": 0
                },
                {
                    "sent": "This metric of how well you compare things.",
                    "label": 0
                },
                {
                    "sent": "So these two metrics compare against human summaries.",
                    "label": 1
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compared K means K means designed for kind of dynamic dynamic clustering.",
                    "label": 0
                },
                {
                    "sent": "There's some work to be done there, but it's basically K means so high.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Higher, higher is better dynamic topic model and then finally.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is BP, where we condition on having KK threads and so we beat both of these models in terms of in terms of Rouge as well as coherence and it can talk more about how we set up the experiments.",
                    "label": 0
                },
                {
                    "sent": "It wasn't trivial, but we you know sort of made sure that it was sort of well defined tasks for the Turkers.",
                    "label": 0
                },
                {
                    "sent": "I'll talk, I will skip that unless there's questions about that.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the most important part is this part.",
                    "label": 0
                },
                {
                    "sent": "The runtime writes the very very large large problem K means is fairly efficient on this, But then any topic models take a lot of time, so this is using publicly available code.",
                    "label": 0
                },
                {
                    "sent": "This takes 252 seconds, so with everything right?",
                    "label": 0
                },
                {
                    "sent": "So it's fairly fast and sort of.",
                    "label": 0
                },
                {
                    "sent": "Practical model How we doing?",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One more minute good.",
                    "label": 0
                },
                {
                    "sent": "This is basically the summary slide, so it's great.",
                    "label": 0
                },
                {
                    "sent": "So what I hope I try to convince you is that DPS are these very nice elegant models which have a lot of really efficient properties for normalization, marginals conditioning and sampling that have tons tons more applications that we've sort of scratched the surface with.",
                    "label": 0
                },
                {
                    "sent": "There's lots of interesting open questions still and.",
                    "label": 0
                },
                {
                    "sent": "You know, we've been trying to make them useful for learning from large scale data.",
                    "label": 1
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One more piece, if you're interested in knowing more, there is a very long survey about these things is going to be a foundation trends paper coming out soon, and there's code available from Alex is web page on this as well.",
                    "label": 0
                },
                {
                    "sent": "I can't thank you.",
                    "label": 0
                }
            ]
        }
    }
}