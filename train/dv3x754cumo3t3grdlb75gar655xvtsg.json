{
    "id": "dv3x754cumo3t3grdlb75gar655xvtsg",
    "title": "Aggregating local descriptors into a compact image representation",
    "info": {
        "author": [
            "Herv\u00e9 J\u00e9gou, INRIA Rennes"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Image & Video Retrieval"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_jegou_ald/",
    "segmentation": [
        [
            "So I'm officially go from here and I will be presenting aggregating local descriptors into a compact image presentation, so this is joint work with materials tools on Kali Schmidt from India, Grenoble, but repairs from technical."
        ],
        [
            "Program that we want to address is the image search on a large case, so we want to be able to retrieve these through images of the same scene or object taken on the different viewpoints or background.",
            "So for instance, the first query on the bottom is a zoom out of the database image.",
            "We also want to be able to under Copyright attacks such as cropping on editing.",
            "The second example on the bottom is image database that has been printing scrap on the scan.",
            "All this must be done with the short response time on for Image image database to billions of images on one machine.",
            "So much."
        ],
        [
            "So related work depends a bag of features.",
            "The framework so called video, Google Approach by yes if it comes on man on the improvement includes the use of large vocabularies, so improved descriptor presentation.",
            "Also geometry has been integrating within the index and some nice techniques from text retrieval as being related to image search such as the expansion.",
            "However, this approach is remain tractable for few moon images only.",
            "So efficiency may be improved by using mean ash or it's Azure metrical extension, or by directly compressing the bag of features representation.",
            "However, still hundreds of bytes are required to obtain visible quality.",
            "Unreasonable is not exactly as good as bag of features with large vocabularies, there's an alternative which consists in using these descriptors combined with Spectra lashing techniques.",
            "However, the descriptor itself gives only a very limited variance to scaling, rotation or crops.",
            "And it is not sufficient for our application."
        ],
        [
            "So our aim is to optimize the tradeoff between search quality on search bit of course, but also memory with Edge, which is a critical parameters if you want to be able to store the index in memory for bigger size databases and our approach realizing the joint optimization of three stages.",
            "So first we assume that some self descriptors have already been instructed from the images, so this is quite standard or this gives us an vectors that represent the image and now we're going to optimize jointly.",
            "As for us to look at the script aggregation, which from the set of vectors or use a single vector presenting the image on this vector is subsequently reduced using dimensionality reduction.",
            "So in this case this PCA so not that dimensionality reduction is context is not exactly standard on, then this reduce vector is uncoded produce code which is the final image representation.",
            "So let's start with the first stage.",
            "So we want to be able to present an image by a single fixed size vector.",
            "So this is the aggregation stages that I consider all the most popular ID consists in using the burger features, a framework which in the case of large scale image search produce some sparse."
        ],
        [
            "Those which are highly dimensional and we will see you now is paramount that this Idol dimensionality is not good because it is difficult to reduce his vectors without introducing some loss on this narrative, which is less known, which is official representation proposed by piranha which produce a non sparse vectors, at least are less sparse and still provide excellent results with limited vector dimensionality.",
            "So search, it can be seen as a simplification of this representation.",
            "For this."
        ],
        [
            "Aggregation stage it is called Vlad, which stands for Victor of locally aggregated descriptors.",
            "The learning stage is quite simple.",
            "It's exactly the same As for the bag of features approach that we just actually learn the cabins container.",
            "This is quite simple.",
            "Now if you want to construct the representation, we first have to assign each seed descriptors to its nearest on trade.",
            "So centuries of color visual world.",
            "But instead of counting the number of occurrences of each.",
            "Visual World we're going to compute the difference between each descriptors on its own trade, so this produce a set of N vector of differences which are gather person traits for each centroid.",
            "We have several vector of differences.",
            "In some cases we have known of them and we just have to sum up these vectors to produce a set of character.",
            "Each vector are the same dimensionality as receive descriptors on our final representation is just the concatenation of this vector components, which is.",
            "Normalized, so you might think that the final dimensionality is much higher than Firebug than for Burger features, because we have this K times dimensionality of the local descriptors, but in practice it is not the case because the typical value for the number of centroid is 64 to be compared to up to millions of visual words.",
            "For Burger features approaches.",
            "So finally we have a vector which is only a few thousand components."
        ],
        [
            "So for those who are familiar with the picture, I'll see if representation, not that we can use the same kind of representation.",
            "That is, for each century we can depict the 4 * 4 grid of the energy of the gradient for each possible orientation.",
            "So we have a rotation on the main difference.",
            "In this case is that due to the difference between descriptor on the century we might have some negative values on.",
            "These are represented in red, in this case against in blue for positive values.",
            "You can notice that for these two images which are quite similar, we obtain comperable representation and actually it is because the vectors are quite close."
        ],
        [
            "Now, if you compare the descriptors with a bag of feature circle presentation, so we have used in the early days data set for this with mean average precision as a performance measure.",
            "But we can first see that for the best line on, this is well known, the performance increases as a dimension grows up two points.",
            "So in this case optimum is obtained for 200,000 components.",
            "But you can also see that if we perform some dimensionality reduction then the performance drops on actually.",
            "It's not a good thing to have very long vector or use a few dimensions, so in this case if you reduce the I vectors very long vector, two only 64 components of performance is only 41 on the best is obtained for 20,000 components reduced to 64 for 44.5, which is not that bad for 64 dimensional vector but not good enough in our case.",
            "So now if you look at the representation that we propose.",
            "With only very small number of centuries, that is for low dimensional gives, final vector will obtain some very good performance.",
            "257.5 which out performs the best bag of features or presentation and also our representation is more likely to be reduced using principal component analysis.",
            "So if you want only 128 components for the radius vectors, we can get 51 point of map.",
            "So what you can see again is that it is difficult to reduce.",
            "Victoria's number of components, so there's a tradeoff, depending on the number of components that we want and output, you should fix K more or less I."
        ],
        [
            "So now that you have obtained this reduced vectors, we have to encourage them for this purpose we use a recent research that user search and indexing problem as a distance approximation problem.",
            "So the idea is to approximate the distance between a query vector X and the database Victor Y by the distance between X on the quantized version of the database vector, so that every sector is contains.",
            "This is a code, but there is no need to contest the query vector, so we estimate this distance here by this distance.",
            "So this is a vector to Kurt Distance, in contrast to spectral hashing methods that provide some explicit memory on bedding when we compare.",
            "Both 2 codes on the query is uncoded.",
            "We introduce another approximation, which is not indeed.",
            "So the choice is the container is critical.",
            "We need many century to get a fair estimation of the distance.",
            "That is why we cannot use regular Cummins on approximate cabins, because we typically want about two power 64 centroid to produce 64 bits codes.",
            "In"
        ],
        [
            "Instead, we are using product container.",
            "So the idea is quite simple.",
            "We have to split vector into M subvectors on each subvector is contained separately using container with a limited number of centroids.",
            "So in our case the container is still the chemist container but with only a few numbers on trade.",
            "In this case, on this example I take 256 century for each except container.",
            "So finally when you have a vector to be included.",
            "You separately contest component.",
            "We have about 8 bits index person teiser on the other code is a 64 bit quantization index.",
            "Now the interesting thing with this."
        ],
        [
            "It contains are is that you can compute the square distance approximation directly in the compressed domain.",
            "That is, we can decompose the square distance between two vectors as a summation of the square distance between each vectors on each quantized.",
            "So vector of the databases on what is interesting what you want when you want to compute the distance between a query vector on many codes of the databases.",
            "Is that the email elementary terms square distant terms in this summation, or shared by all database vectors?",
            "So this we first have to in the first stage compute the distance between each vector on all possible some trades?",
            "Or this is the question, is this not bottleneck on then we want to compute the distance between the query on database code.",
            "We just need M additions on end table lookups.",
            "In the previous example it means that you can compute the distance between vector on a 64 bit code using.",
            "Only 8 addition.",
            "I would like to mention that this scheme can be combined with an elevated file to avoid exhaustive search on this provide comparable result with better efficiency."
        ],
        [
            "Now you know that the red vectors have suffered two approximation.",
            "So first approximation is due to PCA.",
            "We have some projection with introducer mean square error on you have another approximation due to the quantization by the product contains are so we can measure this explicitly.",
            "So now if we fix given number of century K on number of bytes per images we want to use.",
            "So we fix the constraint on the memory.",
            "Then we can automatically select the number of components to be kept.",
            "Basic dimensionality reduction on this case, for example, have chosen 16 some trade or make constraint is 16 bytes.",
            "So as the dimensionality grows for the number of selected components, so projection error will be reduced.",
            "This is not surprising clearly, but at the same time we ask more jobs to the container so the quantization error increases.",
            "So finally there is a tradeoff, is optimized on errors or optimal values obtained for 64 components to be kept by the.",
            "A PCA.",
            "Now."
        ],
        [
            "Let's go to the results on some standard that asset, so we're first first use University of Kent Winky Object Recognition benchmark.",
            "On this data set, the score is usually measured as a number of relevant images, which are correctly wrong in the first four position.",
            "So this car is maximum four.",
            "We have also used in Holidays data set with performance measured by average precision.",
            "The two first line for the baseline of the bag of features representation on the score on counting key or about three of them.",
            "In average precision go from from 45 or medium size vocabulary 255.",
            "For the longer larger vocabulary.",
            "So now if you look at the most compatible work two hours, but you can see that we can reduce some representation to only 20 bytes.",
            "But in that case the performance is not very good.",
            "Now if you want to have better results with this approach, you must use several hundreds of bite on never like this.",
            "You cannot achieve the same performance as Bagger features.",
            "Now with the proposed approach, using only 16 to 40 bytes, we actually outperformed 40 bytes.",
            "The bag of features representation with.",
            "Keep on counting key on your very competitive result.",
            "49.5 on Holidays with 40 bytes for the world image representation.",
            "So now we have also conducted some large scale."
        ],
        [
            "Experiments on earth to 10,000,000 images.",
            "So in this case we have inserted some destructor, some images from Flickr with ground truth that asset early days only measure the performance as a function of the database size.",
            "The performance is measured by his Oracle at 100, which means that we're going to measure the quality of the short list provided by the system.",
            "'cause this kind of system is interesting, particular when you want to provide a short list to a very strong system that is able to perform geometrical verification, for instance, but which is costly.",
            "So the two first line on the top here first, the bag of features representation with logical, blurry in red, on the Vlad descriptors.",
            "The plane that descriptors reduce for 64 some treats for 256.",
            "It would be better for flood.",
            "So you can see that using dimensionality reduction to the D prime equal to 96 components, we obtain a small decrease of performance.",
            "Now, if you subsequently uncovered using 16 bytes in this case have chosen an external case, we use only 16 bytes per image.",
            "You obtain this performance, which is, I think, quite acceptable on just to mention concurrent indexing scheme.",
            "So spectral hashing, if you spit relation to index directly with the descriptors, you obtain the this line for the same number of byte as this performance using our method.",
            "Now if you look at the timings, so first.",
            "Exhaustive search using the PCR.",
            "Vlad descriptors takes about 5 seconds and actually it is quite competitive because it is only for one single car on tenure images.",
            "So exhaustive search is not so bad.",
            "Now if you use our approach based on product azatian, it takes less than 300 minutes ago to query the system.",
            "And if you use non existing extension we provide comparable result you need for four additional bytes.",
            "In that case it is 20 bytes of memory only 40 minutes ago and to return the result associated with this query as a comparison, Spectra lashing would take using binary distance computation about 300 millisecond, comparable to our exhaustive version."
        ],
        [
            "To conclude my talk, we have proposed method that proposes that obtain competitive search accuracy with only a few dozen bytes per index images and we have actually tested our method on up to 220 million video frames, and if you extrapolate our measurements for one billion images, the index would take about 20 gigabytes of memory on the query would be less than one second on.",
            "Actually, this number is now quite under verse teammate.",
            "It would be much less than one soon.",
            "We have put the code online so we have representation which is included in the meta care package.",
            "Also the indexing algorithm and you have also proposed also proposed precomputed distracted descriptors to reproduce the result of our paper.",
            "I would like to mention the work by your floppy owner on Adderall, improve facial representation.",
            "Combining our approach is when I have presented today with this missile which is presented in the poster session.",
            "We have actually improve our results and finally we have a demo that is running on this small laptop undies which is able to return, make some query in 10,000,000 images and about from 50 to 80 milliseconds.",
            "So thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm officially go from here and I will be presenting aggregating local descriptors into a compact image presentation, so this is joint work with materials tools on Kali Schmidt from India, Grenoble, but repairs from technical.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Program that we want to address is the image search on a large case, so we want to be able to retrieve these through images of the same scene or object taken on the different viewpoints or background.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the first query on the bottom is a zoom out of the database image.",
                    "label": 0
                },
                {
                    "sent": "We also want to be able to under Copyright attacks such as cropping on editing.",
                    "label": 0
                },
                {
                    "sent": "The second example on the bottom is image database that has been printing scrap on the scan.",
                    "label": 0
                },
                {
                    "sent": "All this must be done with the short response time on for Image image database to billions of images on one machine.",
                    "label": 1
                },
                {
                    "sent": "So much.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So related work depends a bag of features.",
                    "label": 1
                },
                {
                    "sent": "The framework so called video, Google Approach by yes if it comes on man on the improvement includes the use of large vocabularies, so improved descriptor presentation.",
                    "label": 0
                },
                {
                    "sent": "Also geometry has been integrating within the index and some nice techniques from text retrieval as being related to image search such as the expansion.",
                    "label": 1
                },
                {
                    "sent": "However, this approach is remain tractable for few moon images only.",
                    "label": 1
                },
                {
                    "sent": "So efficiency may be improved by using mean ash or it's Azure metrical extension, or by directly compressing the bag of features representation.",
                    "label": 0
                },
                {
                    "sent": "However, still hundreds of bytes are required to obtain visible quality.",
                    "label": 1
                },
                {
                    "sent": "Unreasonable is not exactly as good as bag of features with large vocabularies, there's an alternative which consists in using these descriptors combined with Spectra lashing techniques.",
                    "label": 0
                },
                {
                    "sent": "However, the descriptor itself gives only a very limited variance to scaling, rotation or crops.",
                    "label": 0
                },
                {
                    "sent": "And it is not sufficient for our application.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our aim is to optimize the tradeoff between search quality on search bit of course, but also memory with Edge, which is a critical parameters if you want to be able to store the index in memory for bigger size databases and our approach realizing the joint optimization of three stages.",
                    "label": 1
                },
                {
                    "sent": "So first we assume that some self descriptors have already been instructed from the images, so this is quite standard or this gives us an vectors that represent the image and now we're going to optimize jointly.",
                    "label": 0
                },
                {
                    "sent": "As for us to look at the script aggregation, which from the set of vectors or use a single vector presenting the image on this vector is subsequently reduced using dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So in this case this PCA so not that dimensionality reduction is context is not exactly standard on, then this reduce vector is uncoded produce code which is the final image representation.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the first stage.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to present an image by a single fixed size vector.",
                    "label": 0
                },
                {
                    "sent": "So this is the aggregation stages that I consider all the most popular ID consists in using the burger features, a framework which in the case of large scale image search produce some sparse.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those which are highly dimensional and we will see you now is paramount that this Idol dimensionality is not good because it is difficult to reduce his vectors without introducing some loss on this narrative, which is less known, which is official representation proposed by piranha which produce a non sparse vectors, at least are less sparse and still provide excellent results with limited vector dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So search, it can be seen as a simplification of this representation.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Aggregation stage it is called Vlad, which stands for Victor of locally aggregated descriptors.",
                    "label": 1
                },
                {
                    "sent": "The learning stage is quite simple.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same As for the bag of features approach that we just actually learn the cabins container.",
                    "label": 0
                },
                {
                    "sent": "This is quite simple.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to construct the representation, we first have to assign each seed descriptors to its nearest on trade.",
                    "label": 0
                },
                {
                    "sent": "So centuries of color visual world.",
                    "label": 0
                },
                {
                    "sent": "But instead of counting the number of occurrences of each.",
                    "label": 0
                },
                {
                    "sent": "Visual World we're going to compute the difference between each descriptors on its own trade, so this produce a set of N vector of differences which are gather person traits for each centroid.",
                    "label": 1
                },
                {
                    "sent": "We have several vector of differences.",
                    "label": 0
                },
                {
                    "sent": "In some cases we have known of them and we just have to sum up these vectors to produce a set of character.",
                    "label": 0
                },
                {
                    "sent": "Each vector are the same dimensionality as receive descriptors on our final representation is just the concatenation of this vector components, which is.",
                    "label": 0
                },
                {
                    "sent": "Normalized, so you might think that the final dimensionality is much higher than Firebug than for Burger features, because we have this K times dimensionality of the local descriptors, but in practice it is not the case because the typical value for the number of centroid is 64 to be compared to up to millions of visual words.",
                    "label": 0
                },
                {
                    "sent": "For Burger features approaches.",
                    "label": 0
                },
                {
                    "sent": "So finally we have a vector which is only a few thousand components.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for those who are familiar with the picture, I'll see if representation, not that we can use the same kind of representation.",
                    "label": 0
                },
                {
                    "sent": "That is, for each century we can depict the 4 * 4 grid of the energy of the gradient for each possible orientation.",
                    "label": 0
                },
                {
                    "sent": "So we have a rotation on the main difference.",
                    "label": 0
                },
                {
                    "sent": "In this case is that due to the difference between descriptor on the century we might have some negative values on.",
                    "label": 0
                },
                {
                    "sent": "These are represented in red, in this case against in blue for positive values.",
                    "label": 0
                },
                {
                    "sent": "You can notice that for these two images which are quite similar, we obtain comperable representation and actually it is because the vectors are quite close.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if you compare the descriptors with a bag of feature circle presentation, so we have used in the early days data set for this with mean average precision as a performance measure.",
                    "label": 0
                },
                {
                    "sent": "But we can first see that for the best line on, this is well known, the performance increases as a dimension grows up two points.",
                    "label": 0
                },
                {
                    "sent": "So in this case optimum is obtained for 200,000 components.",
                    "label": 0
                },
                {
                    "sent": "But you can also see that if we perform some dimensionality reduction then the performance drops on actually.",
                    "label": 0
                },
                {
                    "sent": "It's not a good thing to have very long vector or use a few dimensions, so in this case if you reduce the I vectors very long vector, two only 64 components of performance is only 41 on the best is obtained for 20,000 components reduced to 64 for 44.5, which is not that bad for 64 dimensional vector but not good enough in our case.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at the representation that we propose.",
                    "label": 0
                },
                {
                    "sent": "With only very small number of centuries, that is for low dimensional gives, final vector will obtain some very good performance.",
                    "label": 0
                },
                {
                    "sent": "257.5 which out performs the best bag of features or presentation and also our representation is more likely to be reduced using principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "So if you want only 128 components for the radius vectors, we can get 51 point of map.",
                    "label": 0
                },
                {
                    "sent": "So what you can see again is that it is difficult to reduce.",
                    "label": 0
                },
                {
                    "sent": "Victoria's number of components, so there's a tradeoff, depending on the number of components that we want and output, you should fix K more or less I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now that you have obtained this reduced vectors, we have to encourage them for this purpose we use a recent research that user search and indexing problem as a distance approximation problem.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to approximate the distance between a query vector X and the database Victor Y by the distance between X on the quantized version of the database vector, so that every sector is contains.",
                    "label": 1
                },
                {
                    "sent": "This is a code, but there is no need to contest the query vector, so we estimate this distance here by this distance.",
                    "label": 0
                },
                {
                    "sent": "So this is a vector to Kurt Distance, in contrast to spectral hashing methods that provide some explicit memory on bedding when we compare.",
                    "label": 0
                },
                {
                    "sent": "Both 2 codes on the query is uncoded.",
                    "label": 0
                },
                {
                    "sent": "We introduce another approximation, which is not indeed.",
                    "label": 0
                },
                {
                    "sent": "So the choice is the container is critical.",
                    "label": 0
                },
                {
                    "sent": "We need many century to get a fair estimation of the distance.",
                    "label": 0
                },
                {
                    "sent": "That is why we cannot use regular Cummins on approximate cabins, because we typically want about two power 64 centroid to produce 64 bits codes.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead, we are using product container.",
                    "label": 0
                },
                {
                    "sent": "So the idea is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We have to split vector into M subvectors on each subvector is contained separately using container with a limited number of centroids.",
                    "label": 1
                },
                {
                    "sent": "So in our case the container is still the chemist container but with only a few numbers on trade.",
                    "label": 0
                },
                {
                    "sent": "In this case, on this example I take 256 century for each except container.",
                    "label": 0
                },
                {
                    "sent": "So finally when you have a vector to be included.",
                    "label": 0
                },
                {
                    "sent": "You separately contest component.",
                    "label": 0
                },
                {
                    "sent": "We have about 8 bits index person teiser on the other code is a 64 bit quantization index.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing with this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It contains are is that you can compute the square distance approximation directly in the compressed domain.",
                    "label": 1
                },
                {
                    "sent": "That is, we can decompose the square distance between two vectors as a summation of the square distance between each vectors on each quantized.",
                    "label": 0
                },
                {
                    "sent": "So vector of the databases on what is interesting what you want when you want to compute the distance between a query vector on many codes of the databases.",
                    "label": 0
                },
                {
                    "sent": "Is that the email elementary terms square distant terms in this summation, or shared by all database vectors?",
                    "label": 0
                },
                {
                    "sent": "So this we first have to in the first stage compute the distance between each vector on all possible some trades?",
                    "label": 1
                },
                {
                    "sent": "Or this is the question, is this not bottleneck on then we want to compute the distance between the query on database code.",
                    "label": 0
                },
                {
                    "sent": "We just need M additions on end table lookups.",
                    "label": 0
                },
                {
                    "sent": "In the previous example it means that you can compute the distance between vector on a 64 bit code using.",
                    "label": 0
                },
                {
                    "sent": "Only 8 addition.",
                    "label": 1
                },
                {
                    "sent": "I would like to mention that this scheme can be combined with an elevated file to avoid exhaustive search on this provide comparable result with better efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you know that the red vectors have suffered two approximation.",
                    "label": 0
                },
                {
                    "sent": "So first approximation is due to PCA.",
                    "label": 0
                },
                {
                    "sent": "We have some projection with introducer mean square error on you have another approximation due to the quantization by the product contains are so we can measure this explicitly.",
                    "label": 0
                },
                {
                    "sent": "So now if we fix given number of century K on number of bytes per images we want to use.",
                    "label": 0
                },
                {
                    "sent": "So we fix the constraint on the memory.",
                    "label": 0
                },
                {
                    "sent": "Then we can automatically select the number of components to be kept.",
                    "label": 0
                },
                {
                    "sent": "Basic dimensionality reduction on this case, for example, have chosen 16 some trade or make constraint is 16 bytes.",
                    "label": 0
                },
                {
                    "sent": "So as the dimensionality grows for the number of selected components, so projection error will be reduced.",
                    "label": 0
                },
                {
                    "sent": "This is not surprising clearly, but at the same time we ask more jobs to the container so the quantization error increases.",
                    "label": 0
                },
                {
                    "sent": "So finally there is a tradeoff, is optimized on errors or optimal values obtained for 64 components to be kept by the.",
                    "label": 0
                },
                {
                    "sent": "A PCA.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's go to the results on some standard that asset, so we're first first use University of Kent Winky Object Recognition benchmark.",
                    "label": 1
                },
                {
                    "sent": "On this data set, the score is usually measured as a number of relevant images, which are correctly wrong in the first four position.",
                    "label": 0
                },
                {
                    "sent": "So this car is maximum four.",
                    "label": 0
                },
                {
                    "sent": "We have also used in Holidays data set with performance measured by average precision.",
                    "label": 0
                },
                {
                    "sent": "The two first line for the baseline of the bag of features representation on the score on counting key or about three of them.",
                    "label": 0
                },
                {
                    "sent": "In average precision go from from 45 or medium size vocabulary 255.",
                    "label": 0
                },
                {
                    "sent": "For the longer larger vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at the most compatible work two hours, but you can see that we can reduce some representation to only 20 bytes.",
                    "label": 0
                },
                {
                    "sent": "But in that case the performance is not very good.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to have better results with this approach, you must use several hundreds of bite on never like this.",
                    "label": 0
                },
                {
                    "sent": "You cannot achieve the same performance as Bagger features.",
                    "label": 0
                },
                {
                    "sent": "Now with the proposed approach, using only 16 to 40 bytes, we actually outperformed 40 bytes.",
                    "label": 0
                },
                {
                    "sent": "The bag of features representation with.",
                    "label": 0
                },
                {
                    "sent": "Keep on counting key on your very competitive result.",
                    "label": 0
                },
                {
                    "sent": "49.5 on Holidays with 40 bytes for the world image representation.",
                    "label": 0
                },
                {
                    "sent": "So now we have also conducted some large scale.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments on earth to 10,000,000 images.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have inserted some destructor, some images from Flickr with ground truth that asset early days only measure the performance as a function of the database size.",
                    "label": 1
                },
                {
                    "sent": "The performance is measured by his Oracle at 100, which means that we're going to measure the quality of the short list provided by the system.",
                    "label": 0
                },
                {
                    "sent": "'cause this kind of system is interesting, particular when you want to provide a short list to a very strong system that is able to perform geometrical verification, for instance, but which is costly.",
                    "label": 0
                },
                {
                    "sent": "So the two first line on the top here first, the bag of features representation with logical, blurry in red, on the Vlad descriptors.",
                    "label": 0
                },
                {
                    "sent": "The plane that descriptors reduce for 64 some treats for 256.",
                    "label": 0
                },
                {
                    "sent": "It would be better for flood.",
                    "label": 0
                },
                {
                    "sent": "So you can see that using dimensionality reduction to the D prime equal to 96 components, we obtain a small decrease of performance.",
                    "label": 0
                },
                {
                    "sent": "Now, if you subsequently uncovered using 16 bytes in this case have chosen an external case, we use only 16 bytes per image.",
                    "label": 1
                },
                {
                    "sent": "You obtain this performance, which is, I think, quite acceptable on just to mention concurrent indexing scheme.",
                    "label": 0
                },
                {
                    "sent": "So spectral hashing, if you spit relation to index directly with the descriptors, you obtain the this line for the same number of byte as this performance using our method.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the timings, so first.",
                    "label": 0
                },
                {
                    "sent": "Exhaustive search using the PCR.",
                    "label": 0
                },
                {
                    "sent": "Vlad descriptors takes about 5 seconds and actually it is quite competitive because it is only for one single car on tenure images.",
                    "label": 0
                },
                {
                    "sent": "So exhaustive search is not so bad.",
                    "label": 0
                },
                {
                    "sent": "Now if you use our approach based on product azatian, it takes less than 300 minutes ago to query the system.",
                    "label": 0
                },
                {
                    "sent": "And if you use non existing extension we provide comparable result you need for four additional bytes.",
                    "label": 0
                },
                {
                    "sent": "In that case it is 20 bytes of memory only 40 minutes ago and to return the result associated with this query as a comparison, Spectra lashing would take using binary distance computation about 300 millisecond, comparable to our exhaustive version.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude my talk, we have proposed method that proposes that obtain competitive search accuracy with only a few dozen bytes per index images and we have actually tested our method on up to 220 million video frames, and if you extrapolate our measurements for one billion images, the index would take about 20 gigabytes of memory on the query would be less than one second on.",
                    "label": 1
                },
                {
                    "sent": "Actually, this number is now quite under verse teammate.",
                    "label": 0
                },
                {
                    "sent": "It would be much less than one soon.",
                    "label": 0
                },
                {
                    "sent": "We have put the code online so we have representation which is included in the meta care package.",
                    "label": 0
                },
                {
                    "sent": "Also the indexing algorithm and you have also proposed also proposed precomputed distracted descriptors to reproduce the result of our paper.",
                    "label": 0
                },
                {
                    "sent": "I would like to mention the work by your floppy owner on Adderall, improve facial representation.",
                    "label": 0
                },
                {
                    "sent": "Combining our approach is when I have presented today with this missile which is presented in the poster session.",
                    "label": 0
                },
                {
                    "sent": "We have actually improve our results and finally we have a demo that is running on this small laptop undies which is able to return, make some query in 10,000,000 images and about from 50 to 80 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}