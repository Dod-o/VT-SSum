{
    "id": "gpgz34ttbskmmhj5eu6wdncfhcsihbq6",
    "title": "Higher Order Contractive auto-encoder",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Salah Rifai, University of Montreal"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_rifai_contractive/",
    "segmentation": [
        [
            "So hello everyone and this work, I'm going to present how to train.",
            "Simply Train another quarter an efficient way."
        ],
        [
            "Now.",
            "So I'm going to start by introducing what is an autoencoder, basic definition, and introduce the contractive order encoder which is published work in acnl and.",
            "The C E + H to higher order contractive autoencoder, which is the contribution in this paper.",
            "After that I'm going to motivate the contractive penalty, why we're using it, and I'm going to show you some results."
        ],
        [
            "So what is an auto encoder?",
            "An auto encoder is simply.",
            "22 simple parts that are the encoder and decoder so the encoder is parameterized by a matrix W that is just enough rain projection into a feature space and you can choose the number of basis you want, which you can see also has hidden units.",
            "And the quarter is just exactly the same as the encoder.",
            "Your reproject back your feature.",
            "Feature space in the inputs and then put space.",
            "And of course you apply a nonlinearity elementwise on every feature vector.",
            "Here we are using the sigmoid."
        ],
        [
            "Function.",
            "So how do you train another encoder?",
            "You train a note encoder.",
            "You can trade with many criterion.",
            "Here we consider two criterion, one that is in squared error between the quarters output and the input or the cross entropy for binary units.",
            "And that's their reconstruction."
        ],
        [
            "Will be using.",
            "So what's the 1st order contractive auto encoder where it's exactly as the regular auto encoder and with an added penalty on the Jacobian on the norm of the furnace?",
            "Normal stage again.",
            "So we're taking every element of the Jacobian.",
            "Jacobian is of the hidden layer, so the feature space with respect to the input for calculating derivative.",
            "And then we're summing the square elements of this derivative.",
            "And this gives us the problem.",
            "All of the Jacobian and we can we penalize it more or less with hyperparameter, Lambda, and in the case of a sigmoidal function, the expression of the Jacobian.",
            "The problem is not part of the Jacobian, takes the following form."
        ],
        [
            "So the contribution in this paper, which is a higher order contractive autoencoder is.",
            "Is the same thing as the contractive auto encoder with an additional penalty that takes account for the higher order terms.",
            "Of course it's not possible to do this in practice because it's not compositionally efficient, so we have to come up with a clever approximation of those higher orders.",
            "In particular the Haitian.",
            "In this case, the norm of the Hessian and the way we do that is that we we take the difference between the Jacobian evaluated at a clean example and the Jacobian evaluated at the same example plus.",
            "Corrupted vector that is sampled from a Goshen with a particular variance.",
            "And there is a proof that.",
            "When you do the Taylor expansion of this expression, you get exactly the problem.",
            "This norm of the Haitian.",
            "And so, the last criterion we used to train, the higher order contractive autoencoder is just the regular contractive order in order to 1st order plus.",
            "This additional stochastic approximation.",
            "And of course, because in our experiments will never be able to use sigmoid.",
            "Sorry, Sigma.",
            "Tending to zero will never be able to reach this limit where you can also approximate higher order terms that appears in Taylor expansion."
        ],
        [
            "Expansion.",
            "So why do?",
            "Why are we doing this?",
            "The initial intuition it was to get invariants in the hidden layer.",
            "So you want to learn a new representation that is robust to small changes in your input space.",
            "So assume you have like noisy input an A lot of inputs are similar, but with an added noise, and you want them to have exactly the same representation.",
            "So you want to be robust, so this noise in the input space so.",
            "Regularising the Jacobian norm and higher order Norm is a something that you can do to achieve this.",
            "The other thing is that we want this criterion to be local.",
            "Why?",
            "Because if you can track the whole space you get like just a scaling of your input space.",
            "So you just want it localy around your training example.",
            "You don't want to do this."
        ],
        [
            "Jennifer space so to to understand better how is working this penalty.",
            "We did a few experiments to measure how contract.",
            "If the features we're learning are around our sample points and we measure this and a local way and in a global way, so see what's happening really nearby.",
            "The simple point and we're moving further away.",
            "And we also compared the learned features to other algae."
        ],
        [
            "Isms so in.",
            "To measure the local local contraction, what we did is to take to calculate the singular values of the Jacobian around at the training points and we calculated for a batch of points and then we averaged the.",
            "The this is the singular values to get an estimate of the variance and this."
        ],
        [
            "This is how contracted is our projection in the light and space.",
            "So as you can see in this graph on the X axis, you have the number of singular values and on the Y axis you have the.",
            "The amplitude of the singular values and we can see that the contractive autoencoder and the higher order contractive autoencoder have a sharp drop off in the number of singular values.",
            "So approximately after 50 we have almost all the directions in the input space contracted as opposed to other models such as just regular, non regularised autoencoder that just do little or no contraction at all.",
            "And PBM and EAI.",
            "Doing better than this is also a good sign that other algorithm, other algorithms are also trying to do some contracting of the input space."
        ],
        [
            "So the other experiment we did to estimate the global contraction of the input space was to define a contraction ratio.",
            "So what's the contraction ratio you take to training 2 two points in your input space?",
            "You calculate the distance the collision distance between those two points.",
            "Then you take those same two points.",
            "Your projection you project them in your feature space and you re calculate the distance.",
            "An in the future space.",
            "Then you take the ratio between the distance in the feature space and in the input space and you get the contraction ratio.",
            "So what we did is that taken a sample points from the training set and sampling around the simple point on a sphere.",
            "Random points around it with a certain distance.",
            "There were all at even distance from the simple point, and then we're increasing the radius of the sphere from where we're sampling and we were calculating this contraction ratio."
        ],
        [
            "As we're moving.",
            "We are moving away from the sample point, So what we can see is that for the contractive auto encoder we're having like a really strong contraction locali around when the radius is 0, like it's, it's really contracting a lot and then we're moving away from the sample.",
            "Points were saying that they're not contracting anymore and they're joining other algorithms that are not contracted, so.",
            "And what we can see also is that for the contracted, the higher order contractive autoencoder, where we're even contracting at longer distance, that's also something we went tended to do when we introduced this is to have a.",
            "So two tool to improve the contraction further away."
        ],
        [
            "So we can make two observations from this is that we have a highly localized contraction.",
            "As I just said, and we see that we have only a few direction that are not contracted.",
            "And yeah."
        ],
        [
            "Daniel, why is that?",
            "Or try to so?",
            "What's happening, we have two terms in our criterion.",
            "One is the reconstruction and one is a contraction penalty.",
            "Or, in the case of higher order, we have multiple contraction penalties.",
            "So what we're doing is that the contraction penalty is asking the encoder to be contractive in all possible directions, but the reconstruction penalty penalty is trying to faithfully reconstruct the input, and there is a sort of.",
            "Like they work together too.",
            "So what's happening is that.",
            "If you want to reconstruct your input in a faithful way, you have to allow your hidden representation to very to have different representation.",
            "If you have different points.",
            "Because if you have two points that Maps to the same hidden representation, then you won't be able to reconstruct it, so.",
            "It's allowing the hidden the encoder is allowing the hidden representation to move when to be sensitive to a few direction, and this direction that are necessary for discriminating different example and then hence they correspond to a low dimensional manifold where the neighboring samples kind."
        ],
        [
            "Grades.",
            "Another interesting aspect of this work is that what we can do with this is approximate the manifold using the encoders mapping so.",
            "Usually we have when we have like real problems, we have no analytical parametrization of the manifold, so we don't really know how the data very it varies in the input space and the contract if other encoder learns the duration of variation in the data.",
            "As I said earlier, because it needs to reconstruct the data and.",
            "What we were looking to the directions of the directions associated to the singular value with high amplitude, where we're saying that we have, we're characterizing low dimensional manifold or the local tangent to the manifold and the input space, and we can characterize how the inputs that the sample points virus in the input space without using any prior knowledge."
        ],
        [
            "So for example.",
            "Yeah, what we're saying that when we're moving parallel to the manifold we have like.",
            "We have a sensitive representation and the subspace spanned by the orthogonal directions to the manifold is a direction where the data there's no data density, so that is unlikely.",
            "And what we can see is at the local directions are spanned by the 1st principle component of the Jacobian."
        ],
        [
            "So we can defined we can define local charts of the manifolds.",
            "By using those vectors so at every sample point we can calculate the Jacobian associated with a sample point and doing a singular value decomposition with give us the first tangent that can be seen as a local chart.",
            "So we're saying that only a few direction are.",
            "Are plausible only a few direction represents some possible direction and that and if we take the union of all this direction, we can build what we call an Atlas so.",
            "So what we can see here, for example, imagine this is like a linear approximation pointwise in your manifold, and we can see that if there are two to go from this example to that example that most likely direction is this one, and so on so.",
            "That's what we're doing."
        ],
        [
            "So.",
            "We can also interpret the hidden representation as a coordinate system, so the high order contractive autoencoder gives you a highly saturated representation and also has sparse representation and you can see the hidden units that are saturated as having zero Jacobian because we are in the flat region of the sigmoidal function.",
            "That's zero Jacobian and only than unsaturated.",
            "Then air units are responsible for the high values in the spectrum of the Jacobian.",
            "So this direction defined the local chart.",
            "So what you can remember from this is that non saturated units.",
            "Defined the local coordinates and saturated units in code."
        ],
        [
            "Global coordinates so formally we can see this as.",
            "We define the the Atlas as.",
            "Being the union of the local charts, so a local chart you take your Jacobian.",
            "You do singular value that composition and then.",
            "Find a basis that is all the principle component of your Jacobian that have an associated singular values superior to a certain epsilon that you can fix as a hyper major.",
            "Then you take all the all the union of all this and you."
        ],
        [
            "That's for athletes.",
            "So that's the tangent.",
            "We're learning the first images on cifar, so the first row here.",
            "Is the image of the dog and what we did is we calculated PCA on a set of.",
            "Samples that were chosen to be the nearest neighbor to the initial image, and we can see that the direction the principle components of the PCA are just noise, and there they don't seem to make any sense.",
            "And if we look at the tensions extracted by the CAE plus H, we can see that there are many meaningful transformations.",
            "So for example here we haven't log in and we see that we have translation of.",
            "They had the legs and so on.",
            "It's highly localized and not as noisy as a PCA, and we also did the same on the IMS data set and we can see that they also respond to local transformation of the input space, and I want you to know that this is really an interesting result because we're not using any information about the input space, so we have no idea that there are images or not and we use this also on text text data set.",
            "Reuters Corpus and we're also getting meaningful tensions."
        ],
        [
            "So see the CD plus H which is a higher order contracted motor encoder.",
            "Users can take advantage of overcomplete representation so you can choose a lot of basis vectors and would still have a very good convergence and it will specialize the features in different parts of the."
        ],
        [
            "Put space.",
            "And to test this quantitively we we stacked on top of their representation.",
            "So in this in this table where showing the pretraining method we used and the first row tells you whether or not we fine tune the whole network or we just fine tuned the top layer linear classifier so the second role is fine tuning everything.",
            "So we see that the C, E + H is less dependent on the.",
            "The fine tuning is that of the whole.",
            "Of the whole network, let's see.",
            "The Delta is really small for the CA plus H as opposed to all the."
        ],
        [
            "Models and we try to stack the models and we see also that we have.",
            "Deep networks with this initialization for the for the MLP gives you a very good result."
        ],
        [
            "Using the state of the art and here we are equal in the state of the art in the convolutional metric, initialized with filters pre trained on."
        ],
        [
            "Patches and the future work.",
            "We're planning to do is extending our definition of the local chart to higher order approximation to curvilinear.",
            "Very later coordinates and we want also to try sampling new data points.",
            "We follow this direction, see if we can get some interesting points and we also used those this local chart into supervised learning algorithm.",
            "But when we took advantage of this definition and we published some work and NIPS 2011 interested."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello everyone and this work, I'm going to present how to train.",
                    "label": 0
                },
                {
                    "sent": "Simply Train another quarter an efficient way.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start by introducing what is an autoencoder, basic definition, and introduce the contractive order encoder which is published work in acnl and.",
                    "label": 0
                },
                {
                    "sent": "The C E + H to higher order contractive autoencoder, which is the contribution in this paper.",
                    "label": 1
                },
                {
                    "sent": "After that I'm going to motivate the contractive penalty, why we're using it, and I'm going to show you some results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is an auto encoder?",
                    "label": 0
                },
                {
                    "sent": "An auto encoder is simply.",
                    "label": 0
                },
                {
                    "sent": "22 simple parts that are the encoder and decoder so the encoder is parameterized by a matrix W that is just enough rain projection into a feature space and you can choose the number of basis you want, which you can see also has hidden units.",
                    "label": 0
                },
                {
                    "sent": "And the quarter is just exactly the same as the encoder.",
                    "label": 0
                },
                {
                    "sent": "Your reproject back your feature.",
                    "label": 0
                },
                {
                    "sent": "Feature space in the inputs and then put space.",
                    "label": 0
                },
                {
                    "sent": "And of course you apply a nonlinearity elementwise on every feature vector.",
                    "label": 0
                },
                {
                    "sent": "Here we are using the sigmoid.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "So how do you train another encoder?",
                    "label": 0
                },
                {
                    "sent": "You train a note encoder.",
                    "label": 0
                },
                {
                    "sent": "You can trade with many criterion.",
                    "label": 0
                },
                {
                    "sent": "Here we consider two criterion, one that is in squared error between the quarters output and the input or the cross entropy for binary units.",
                    "label": 0
                },
                {
                    "sent": "And that's their reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be using.",
                    "label": 0
                },
                {
                    "sent": "So what's the 1st order contractive auto encoder where it's exactly as the regular auto encoder and with an added penalty on the Jacobian on the norm of the furnace?",
                    "label": 1
                },
                {
                    "sent": "Normal stage again.",
                    "label": 1
                },
                {
                    "sent": "So we're taking every element of the Jacobian.",
                    "label": 1
                },
                {
                    "sent": "Jacobian is of the hidden layer, so the feature space with respect to the input for calculating derivative.",
                    "label": 0
                },
                {
                    "sent": "And then we're summing the square elements of this derivative.",
                    "label": 0
                },
                {
                    "sent": "And this gives us the problem.",
                    "label": 0
                },
                {
                    "sent": "All of the Jacobian and we can we penalize it more or less with hyperparameter, Lambda, and in the case of a sigmoidal function, the expression of the Jacobian.",
                    "label": 0
                },
                {
                    "sent": "The problem is not part of the Jacobian, takes the following form.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the contribution in this paper, which is a higher order contractive autoencoder is.",
                    "label": 1
                },
                {
                    "sent": "Is the same thing as the contractive auto encoder with an additional penalty that takes account for the higher order terms.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not possible to do this in practice because it's not compositionally efficient, so we have to come up with a clever approximation of those higher orders.",
                    "label": 0
                },
                {
                    "sent": "In particular the Haitian.",
                    "label": 0
                },
                {
                    "sent": "In this case, the norm of the Hessian and the way we do that is that we we take the difference between the Jacobian evaluated at a clean example and the Jacobian evaluated at the same example plus.",
                    "label": 0
                },
                {
                    "sent": "Corrupted vector that is sampled from a Goshen with a particular variance.",
                    "label": 0
                },
                {
                    "sent": "And there is a proof that.",
                    "label": 1
                },
                {
                    "sent": "When you do the Taylor expansion of this expression, you get exactly the problem.",
                    "label": 0
                },
                {
                    "sent": "This norm of the Haitian.",
                    "label": 0
                },
                {
                    "sent": "And so, the last criterion we used to train, the higher order contractive autoencoder is just the regular contractive order in order to 1st order plus.",
                    "label": 0
                },
                {
                    "sent": "This additional stochastic approximation.",
                    "label": 0
                },
                {
                    "sent": "And of course, because in our experiments will never be able to use sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Sigma.",
                    "label": 0
                },
                {
                    "sent": "Tending to zero will never be able to reach this limit where you can also approximate higher order terms that appears in Taylor expansion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expansion.",
                    "label": 0
                },
                {
                    "sent": "So why do?",
                    "label": 0
                },
                {
                    "sent": "Why are we doing this?",
                    "label": 0
                },
                {
                    "sent": "The initial intuition it was to get invariants in the hidden layer.",
                    "label": 1
                },
                {
                    "sent": "So you want to learn a new representation that is robust to small changes in your input space.",
                    "label": 1
                },
                {
                    "sent": "So assume you have like noisy input an A lot of inputs are similar, but with an added noise, and you want them to have exactly the same representation.",
                    "label": 0
                },
                {
                    "sent": "So you want to be robust, so this noise in the input space so.",
                    "label": 0
                },
                {
                    "sent": "Regularising the Jacobian norm and higher order Norm is a something that you can do to achieve this.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that we want this criterion to be local.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if you can track the whole space you get like just a scaling of your input space.",
                    "label": 0
                },
                {
                    "sent": "So you just want it localy around your training example.",
                    "label": 0
                },
                {
                    "sent": "You don't want to do this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jennifer space so to to understand better how is working this penalty.",
                    "label": 0
                },
                {
                    "sent": "We did a few experiments to measure how contract.",
                    "label": 1
                },
                {
                    "sent": "If the features we're learning are around our sample points and we measure this and a local way and in a global way, so see what's happening really nearby.",
                    "label": 1
                },
                {
                    "sent": "The simple point and we're moving further away.",
                    "label": 0
                },
                {
                    "sent": "And we also compared the learned features to other algae.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isms so in.",
                    "label": 0
                },
                {
                    "sent": "To measure the local local contraction, what we did is to take to calculate the singular values of the Jacobian around at the training points and we calculated for a batch of points and then we averaged the.",
                    "label": 1
                },
                {
                    "sent": "The this is the singular values to get an estimate of the variance and this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is how contracted is our projection in the light and space.",
                    "label": 0
                },
                {
                    "sent": "So as you can see in this graph on the X axis, you have the number of singular values and on the Y axis you have the.",
                    "label": 0
                },
                {
                    "sent": "The amplitude of the singular values and we can see that the contractive autoencoder and the higher order contractive autoencoder have a sharp drop off in the number of singular values.",
                    "label": 1
                },
                {
                    "sent": "So approximately after 50 we have almost all the directions in the input space contracted as opposed to other models such as just regular, non regularised autoencoder that just do little or no contraction at all.",
                    "label": 0
                },
                {
                    "sent": "And PBM and EAI.",
                    "label": 0
                },
                {
                    "sent": "Doing better than this is also a good sign that other algorithm, other algorithms are also trying to do some contracting of the input space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other experiment we did to estimate the global contraction of the input space was to define a contraction ratio.",
                    "label": 0
                },
                {
                    "sent": "So what's the contraction ratio you take to training 2 two points in your input space?",
                    "label": 0
                },
                {
                    "sent": "You calculate the distance the collision distance between those two points.",
                    "label": 0
                },
                {
                    "sent": "Then you take those same two points.",
                    "label": 0
                },
                {
                    "sent": "Your projection you project them in your feature space and you re calculate the distance.",
                    "label": 0
                },
                {
                    "sent": "An in the future space.",
                    "label": 0
                },
                {
                    "sent": "Then you take the ratio between the distance in the feature space and in the input space and you get the contraction ratio.",
                    "label": 1
                },
                {
                    "sent": "So what we did is that taken a sample points from the training set and sampling around the simple point on a sphere.",
                    "label": 0
                },
                {
                    "sent": "Random points around it with a certain distance.",
                    "label": 0
                },
                {
                    "sent": "There were all at even distance from the simple point, and then we're increasing the radius of the sphere from where we're sampling and we were calculating this contraction ratio.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we're moving.",
                    "label": 0
                },
                {
                    "sent": "We are moving away from the sample point, So what we can see is that for the contractive auto encoder we're having like a really strong contraction locali around when the radius is 0, like it's, it's really contracting a lot and then we're moving away from the sample.",
                    "label": 0
                },
                {
                    "sent": "Points were saying that they're not contracting anymore and they're joining other algorithms that are not contracted, so.",
                    "label": 0
                },
                {
                    "sent": "And what we can see also is that for the contracted, the higher order contractive autoencoder, where we're even contracting at longer distance, that's also something we went tended to do when we introduced this is to have a.",
                    "label": 1
                },
                {
                    "sent": "So two tool to improve the contraction further away.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can make two observations from this is that we have a highly localized contraction.",
                    "label": 1
                },
                {
                    "sent": "As I just said, and we see that we have only a few direction that are not contracted.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Daniel, why is that?",
                    "label": 0
                },
                {
                    "sent": "Or try to so?",
                    "label": 0
                },
                {
                    "sent": "What's happening, we have two terms in our criterion.",
                    "label": 0
                },
                {
                    "sent": "One is the reconstruction and one is a contraction penalty.",
                    "label": 1
                },
                {
                    "sent": "Or, in the case of higher order, we have multiple contraction penalties.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is that the contraction penalty is asking the encoder to be contractive in all possible directions, but the reconstruction penalty penalty is trying to faithfully reconstruct the input, and there is a sort of.",
                    "label": 1
                },
                {
                    "sent": "Like they work together too.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is that.",
                    "label": 0
                },
                {
                    "sent": "If you want to reconstruct your input in a faithful way, you have to allow your hidden representation to very to have different representation.",
                    "label": 0
                },
                {
                    "sent": "If you have different points.",
                    "label": 0
                },
                {
                    "sent": "Because if you have two points that Maps to the same hidden representation, then you won't be able to reconstruct it, so.",
                    "label": 1
                },
                {
                    "sent": "It's allowing the hidden the encoder is allowing the hidden representation to move when to be sensitive to a few direction, and this direction that are necessary for discriminating different example and then hence they correspond to a low dimensional manifold where the neighboring samples kind.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grades.",
                    "label": 0
                },
                {
                    "sent": "Another interesting aspect of this work is that what we can do with this is approximate the manifold using the encoders mapping so.",
                    "label": 1
                },
                {
                    "sent": "Usually we have when we have like real problems, we have no analytical parametrization of the manifold, so we don't really know how the data very it varies in the input space and the contract if other encoder learns the duration of variation in the data.",
                    "label": 1
                },
                {
                    "sent": "As I said earlier, because it needs to reconstruct the data and.",
                    "label": 0
                },
                {
                    "sent": "What we were looking to the directions of the directions associated to the singular value with high amplitude, where we're saying that we have, we're characterizing low dimensional manifold or the local tangent to the manifold and the input space, and we can characterize how the inputs that the sample points virus in the input space without using any prior knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what we're saying that when we're moving parallel to the manifold we have like.",
                    "label": 1
                },
                {
                    "sent": "We have a sensitive representation and the subspace spanned by the orthogonal directions to the manifold is a direction where the data there's no data density, so that is unlikely.",
                    "label": 0
                },
                {
                    "sent": "And what we can see is at the local directions are spanned by the 1st principle component of the Jacobian.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can defined we can define local charts of the manifolds.",
                    "label": 1
                },
                {
                    "sent": "By using those vectors so at every sample point we can calculate the Jacobian associated with a sample point and doing a singular value decomposition with give us the first tangent that can be seen as a local chart.",
                    "label": 0
                },
                {
                    "sent": "So we're saying that only a few direction are.",
                    "label": 1
                },
                {
                    "sent": "Are plausible only a few direction represents some possible direction and that and if we take the union of all this direction, we can build what we call an Atlas so.",
                    "label": 0
                },
                {
                    "sent": "So what we can see here, for example, imagine this is like a linear approximation pointwise in your manifold, and we can see that if there are two to go from this example to that example that most likely direction is this one, and so on so.",
                    "label": 0
                },
                {
                    "sent": "That's what we're doing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can also interpret the hidden representation as a coordinate system, so the high order contractive autoencoder gives you a highly saturated representation and also has sparse representation and you can see the hidden units that are saturated as having zero Jacobian because we are in the flat region of the sigmoidal function.",
                    "label": 1
                },
                {
                    "sent": "That's zero Jacobian and only than unsaturated.",
                    "label": 0
                },
                {
                    "sent": "Then air units are responsible for the high values in the spectrum of the Jacobian.",
                    "label": 1
                },
                {
                    "sent": "So this direction defined the local chart.",
                    "label": 1
                },
                {
                    "sent": "So what you can remember from this is that non saturated units.",
                    "label": 0
                },
                {
                    "sent": "Defined the local coordinates and saturated units in code.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Global coordinates so formally we can see this as.",
                    "label": 0
                },
                {
                    "sent": "We define the the Atlas as.",
                    "label": 1
                },
                {
                    "sent": "Being the union of the local charts, so a local chart you take your Jacobian.",
                    "label": 1
                },
                {
                    "sent": "You do singular value that composition and then.",
                    "label": 0
                },
                {
                    "sent": "Find a basis that is all the principle component of your Jacobian that have an associated singular values superior to a certain epsilon that you can fix as a hyper major.",
                    "label": 0
                },
                {
                    "sent": "Then you take all the all the union of all this and you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's for athletes.",
                    "label": 0
                },
                {
                    "sent": "So that's the tangent.",
                    "label": 0
                },
                {
                    "sent": "We're learning the first images on cifar, so the first row here.",
                    "label": 0
                },
                {
                    "sent": "Is the image of the dog and what we did is we calculated PCA on a set of.",
                    "label": 0
                },
                {
                    "sent": "Samples that were chosen to be the nearest neighbor to the initial image, and we can see that the direction the principle components of the PCA are just noise, and there they don't seem to make any sense.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the tensions extracted by the CAE plus H, we can see that there are many meaningful transformations.",
                    "label": 0
                },
                {
                    "sent": "So for example here we haven't log in and we see that we have translation of.",
                    "label": 0
                },
                {
                    "sent": "They had the legs and so on.",
                    "label": 0
                },
                {
                    "sent": "It's highly localized and not as noisy as a PCA, and we also did the same on the IMS data set and we can see that they also respond to local transformation of the input space, and I want you to know that this is really an interesting result because we're not using any information about the input space, so we have no idea that there are images or not and we use this also on text text data set.",
                    "label": 0
                },
                {
                    "sent": "Reuters Corpus and we're also getting meaningful tensions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So see the CD plus H which is a higher order contracted motor encoder.",
                    "label": 0
                },
                {
                    "sent": "Users can take advantage of overcomplete representation so you can choose a lot of basis vectors and would still have a very good convergence and it will specialize the features in different parts of the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put space.",
                    "label": 0
                },
                {
                    "sent": "And to test this quantitively we we stacked on top of their representation.",
                    "label": 0
                },
                {
                    "sent": "So in this in this table where showing the pretraining method we used and the first row tells you whether or not we fine tune the whole network or we just fine tuned the top layer linear classifier so the second role is fine tuning everything.",
                    "label": 0
                },
                {
                    "sent": "So we see that the C, E + H is less dependent on the.",
                    "label": 0
                },
                {
                    "sent": "The fine tuning is that of the whole.",
                    "label": 0
                },
                {
                    "sent": "Of the whole network, let's see.",
                    "label": 0
                },
                {
                    "sent": "The Delta is really small for the CA plus H as opposed to all the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models and we try to stack the models and we see also that we have.",
                    "label": 0
                },
                {
                    "sent": "Deep networks with this initialization for the for the MLP gives you a very good result.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the state of the art and here we are equal in the state of the art in the convolutional metric, initialized with filters pre trained on.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patches and the future work.",
                    "label": 1
                },
                {
                    "sent": "We're planning to do is extending our definition of the local chart to higher order approximation to curvilinear.",
                    "label": 1
                },
                {
                    "sent": "Very later coordinates and we want also to try sampling new data points.",
                    "label": 0
                },
                {
                    "sent": "We follow this direction, see if we can get some interesting points and we also used those this local chart into supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "But when we took advantage of this definition and we published some work and NIPS 2011 interested.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}