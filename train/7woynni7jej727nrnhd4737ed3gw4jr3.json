{
    "id": "7woynni7jej727nrnhd4737ed3gw4jr3",
    "title": "Foundations of Machine Learning",
    "info": {
        "author": [
            "Marcus Hutter, Australian National University"
        ],
        "published": "March 11, 2008",
        "recorded": "March 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss08au_hutter_fund/",
    "segmentation": [
        [
            "OK morning.",
            "We lost a few guys.",
            "I think they're coming.",
            "Um?",
            "So so far you hurt.",
            "A bunch of methods solving a bunch of problems which are sort of.",
            "You know, in the area for enforcement learning.",
            "And you start to wonder, maybe?",
            "Or hopefully I mean how all these things fit together.",
            "I mean, this is just, you know this.",
            "Thousands and more and more methods apply to problems where you.",
            "Have to learn from data or something?",
            "Or is there some underlying principle or underlying principles or a few?",
            "Rules or.",
            "Paradigms, yeah, we sort of hold everything together.",
            "And in the next three hours.",
            "I will present the philosophical, statistical, and computational foundations of machine learning.",
            "Unfortunately, I mean three hours is not too much, so I cannot really flesh out all the connections to all the other talks you've heard.",
            "But I mean you will see here and there where the connections are.",
            "Especially to Java Stockon reinforcement learning.",
            "Meet you heard last week."
        ],
        [
            "OK, so you have one slide which is sort of a one slide summary of my 3 hours today and at the end of the talk I have the same slide and you know then you can put it sort of in context.",
            "So the set up I will consider.",
            "There's some.",
            "Like maybe we can.",
            "Can you see that OK?",
            "Or is it OK?",
            "So I will consider.",
            "Sequential data.",
            "So that means essentially non IID data.",
            "So you have sequence on state data.",
            "For instance, I mean typical weather data or stock market data or other time series X1X2X3 updates N and you want to predict XN plus one OK.",
            "So most of the talk later I come to the Agent framework and reinforcement learning.",
            "And of course this general set up.",
            "Includes ideas, a special case.",
            "So and actually even predicting, I think I mentioned it already.",
            "You could ask.",
            "I mean, why should we care about predictions?",
            "The ultimate goal is to do something with these predictions and to maximize your profit.",
            "If your capitalist or more neutral cap maximizes some motility function or minimize loss if you.",
            "I'll take the negative picture, but that's just a - OK.",
            "So in say, nearly all approaches.",
            "What you do is you start with a class of models or hypothesis where your data could come from.",
            "And then you try to find.",
            "Ideally, a true hypothesis or the true hypothesis, but it's hard to tell what the truth the ground truth is.",
            "So let's talk about good hypothesis.",
            "Whatever good means in detail and one classical estimators and maximum likelihood estimator, so you have.",
            "Are we still not have a pointer?",
            "You take this likelihood function.",
            "And.",
            "This is sort of defining what the hypothesis mean.",
            "I mean what is hi means this is hypothesis and hypothesis tells you a probability distribution over.",
            "Over your sample space.",
            "So in the maximum likelihood estimate, it just takes the hypothesis which has maximum likelihood becausw.",
            "The argument is that.",
            "Um?",
            "Is likely is very low.",
            "Then it's very exceptional that you saw this data, but we saw this data so it should not be too exceptional.",
            "And if the likelihood is high.",
            "You can expect this data and the data happens so that sort of seems to confirm the hypothesis.",
            "I don't know whether they're better arguments.",
            "I'm in any case I will not consider the maximum likelihood.",
            "And further becausw it has some problems if the class is too large.",
            "It leads to overfitting.",
            "So in the Bayesian approach, what you do is you consider the posterior, which is a much cleaner interpretation.",
            "So what you want is you want the probability or degree of belief in a hypothesis after you have seen the data.",
            "And Luckily you can compute it from the likelihood times aprior.",
            "So what we Additionally need now is a prior.",
            "The question is, where does this prior come from?",
            "And there are several principles, but there's.",
            "Actually, only one principle which is general enough to cover really, I mean.",
            "All of machine learning or most.",
            "And there is a comes razor, say combined with epicor's principle.",
            "So what you should do is you should have a high prior for simple models and I will quantify that.",
            "So you quantify simplicity or complexity with Kolmogorov complexity.",
            "Or solo models induction scheme.",
            "And then you have everything you need.",
            "The theorems typically.",
            "I mean very general generic tell you that all this works if your true hypothesis value data is sampled from is in the model class.",
            "So it's very rough statement, but but they're all sort of like that.",
            "OK, so so this is covers the prediction case and I develop.",
            "Actually, some of it, but I mean I want to show you here a universal way of doing prediction in any kind of domain.",
            "And the last step is to.",
            "To generalize that to the reinforcement learning setup where you decisions influence the environment.",
            "So the general Agent set up.",
            "OK, so there was a yeah.",
            "Yeah.",
            "Yeah, for instance, if M is the class of all polynomials all degrees.",
            "Yeah, then you have a few data points.",
            "And then the best fitting polynomial, so the maximum likelihood polynomial.",
            "I mean, if you take, say Gaussian noise and then.",
            "Maximum likelihood is just minimizing the squared error and then you get a polynomial perfectly fits through the data.",
            "Which is.",
            "Is what?",
            "No, no it contains everything, so it contains sort of all degree one polynomials or degree two polynomials, or degrees of three polyester union.",
            "For instance.",
            "OK.",
            "So I think I gave this definition already."
        ],
        [
            "In my first lecture, but I can repeat it, so that's one way to define machine learning, so machine learning is concerned with developing with developing algorithms that learn from experience with models of the environment, from the acquainted knowledge, and I mean so far so good, but but distinguishes.",
            "Machine learning from some other fields is that you really want to use this model for something, and something typically means for prediction.",
            "OK."
        ],
        [
            "So.",
            "OK overview comes first, then some philosophical issues.",
            "Then about Basean sequence prediction.",
            "Then I come to this Solomon of universal inductive inference scheme.",
            "And then that's a slight digression becausw this is.",
            "Some application or Alex would say then or in an analogy.",
            "I would say this is then non theoretical this work.",
            "Remember, Alex always told you that work is unapplied.",
            "If it was theoretical.",
            "So this is.",
            "This is an theoretical this work then, but it really works well in practice if you care.",
            "And then about the universal AI stuff."
        ],
        [
            "OK.",
            "So.",
            "I."
        ],
        [
            "OK, you will see that."
        ],
        [
            "So some philosophical questions around induction mean induction means inferring models from data roughly OK, so.",
            "You can ask.",
            "I mean this inductive or inductive inference or adaptive prediction.",
            "That you use this model stand for prediction.",
            "I often will not distinguish between these two things.",
            "So one question is first, I mean does inductive inference work at all?",
            "OK, it seems so yeah, but then you can ask why?",
            "And how and?",
            "In the basic framework.",
            "You have to choose a model class.",
            "You have to choose a prior.",
            "So how do we do that in a generic way?",
            "How then we can do optimal decisions if nothing is known about environment and finally, for instance, what is intelligence?",
            "If you want to work on AI, it's good to know what intelligence means.",
            "OK, let me."
        ],
        [
            "Give an analogy to a different field.",
            "I mean I could.",
            "But many fields it, let's take complexity theory becausw you probably know that.",
            "So what is the goal of complexity theory?",
            "The goal is to find plus algorithms.",
            "For solving problems.",
            "Order show lower balance on their computation time, so if you read this statement or look at the literature, everything is rigorously defined.",
            "I mean, what is an algorithm Turing machine problem, class computation time?",
            "So nearly everything is rigorous.",
            "OK, but most disciplines starts in an informal way and then with time they get more more formalized and get absolutely rigorous.",
            "And then you have maybe even axiomatization, and then it's pure math in a certain sense, look at probability theory may now have measure theory.",
            "Start with Clover axioms and then you can do all kinds of fancy math.",
            "OK, for example set theory started in an informal way, then you got the Russell Paradox.",
            "The set of all sets which do not contain each other itself, and at some point they formalized it axiomatized, and now it's done.",
            "I mean, at least from a conceptual point of view, logical reasoning, proof theory, probability theory, infinitesimal calculus, physics to conserve energy and temperature, and quantum field theory.",
            "So all apart from the last half an hour rigorous mathematical formulation with quantum field theory also developed in the 1930s.",
            "And the most important theory in physics, and most successful, still not rigorously mathematically defined, but it doesn't really matter in this case because physicists are able to compute numbers out of it.",
            "OK, so so.",
            "Now let's look at machine learning.",
            "OK, so machine learning OK. Repeat myself, tries to build and understand systems that learn from past data, make good predictions, are able to generalize, maybe act intelligently.",
            "So many of these terms are vaguely defined or there are many alternative definitions.",
            "I mean, what does generalize mean?",
            "What does good prediction means?",
            "Rather, understanding me and so on so it would be nice and probably very useful to have a formal general definition.",
            "Or characterization of machine learning axiomatization.",
            "So.",
            "Next"
        ],
        [
            "I will present a classical example by Laplace from 1700 something.",
            "So he asked what is the probability that the sun will rise tomorrow?",
            "So what you have so far as you have seen the sun rising every day?",
            "I mean, if it's cloudy, sort of.",
            "I mean, you can still see there is light, so it's not about clouds or.",
            "And so on.",
            "So we have seen the sun rising, or he's seen it D days.",
            "And he asked.",
            "I mean, what is the chance that it raises tomorrow?",
            "I mean, it's good to know.",
            "So I mean, if it's not raises sort of you associated with doomsday.",
            "OK, so you can think of several answers, so you could say OK, please undefined because there's never been experiment that tested existence of the tomorrow.",
            "Right?",
            "OK. Another answer would be P = 1 because the sun rose in all past experiments.",
            "So you make another frequency estimate.",
            "So the number of counts of ones divided by the total count is 1.",
            "So the probability is 1.",
            "Which.",
            "Is not good because you can never be sure in real world or anything, so no probability should ever be one.",
            "Maybe if it's a logical tautology then it may be one, but even then you can think of Hermes or logic really, I mean safe.",
            "Or you take statistical approach and you look at the proportion of stars which they explode per day.",
            "Then the probability is 1 minus epsilon.",
            "Then the sun will rise tomorrow.",
            "No, not so bad.",
            "Or Laplace and I will derive this rule, said the probability should be D + 1 / D + 2 who does not know this rule.",
            "We have seen this here first time.",
            "OK.",
            "This should be the first rule taught, taught in statistics, or at least in machine learning.",
            "OK. Or your physicist.",
            "And then I mean you look at the type at age and size and temperature of the sun and then you make some conclusions.",
            "Although you've never seen say another star exploding or something.",
            "Which you can do also.",
            "So there are many answers and.",
            "Yeah, which is the right one?",
            "Yes, sorry.",
            "Yeah, but often you have in the past experience.",
            "I'm on both sides.",
            "I mean, you flip a coin, yeah, I mean, maybe not this coin, but other coins you've seen a lot of heads and tails and you know their heads and tails.",
            "Yeah, and then you make up your sample space.",
            "OK, their heads and tails and maybe ID.",
            "And then you make some prediction.",
            "But you've never experienced you know you've never been on another planet or whatever in another life or whatever.",
            "Seen a son not raising so it always rose so.",
            "Maybe it's a logical tautology that it arises every day.",
            "Added, sorry, that's the first one.",
            "Yeah, there are people who think that some things cannot be predicted here.",
            "Yeah, I mean I don't know.",
            "I'm particularly keen on this first thing, but with the sun, I mean it's not an IID experiment.",
            "You cannot.",
            "It's it's hard to argue, sort of that, OK?",
            "I mean the sunrise every day is independent of each other, and so these are independent experiments.",
            "And then you predict on them.",
            "So auto Laplace did it for deriving this rule.",
            "As you will see.",
            "So I mean you can come up with some arguments by in this case and you can't do a prediction.",
            "Or the generic argument?",
            "I mean, you could generalize that induction is not possible at all.",
            "OK so but apart from the third answer and the first answer and the 2nd.",
            "All these P seem to be close to one, but not exactly 1.",
            "So independent of the justification, we say that the sun will rise tomorrow with high probability.",
            "So we all agree on that somehow.",
            "So it would be good that we could also agree more quantitatively on the answer and even have some formal system.",
            "Which gives you the answer."
        ],
        [
            "Here's another examples, look at this.",
            "OK, some of you may think OK, that looks pretty random.",
            "So you count the frequencies of say ones and tools and so on, and you see in the long run the relative frequency is 1 / 10.",
            "So the probability of the next digit being whatever say one or two.",
            "Should be 1 / 10 of.",
            "Could be 1 / 10.",
            "But I guess most of you see.",
            "That this looks like the digits after pie, and once you recognize this structure and you don't know it by heart, or you switch on some math program, you would probably predict.",
            "I guess that is a file for the next digit.",
            "So, so the reason why you answer 5 is that you you see some structure in the sequence and you think OK, that can't be accidentally this probably you know came from a computer printing pie or something."
        ],
        [
            "Here a few other examples, so that's a classical IQ test 1234.",
            "What comes next?",
            "Yeah, it's very tough.",
            "I.",
            "You would probably say 5 and you may even come up with a reason for five becausw.",
            "I mean the sequence is just X is equal to I.",
            "For the first few first four items and then you plug in X5 and this gives you five.",
            "OK, it's good, but.",
            "I can say no look, I mean that's a 4th order polynomial, and if you put in the numbers 1234 it gives 1234 and then X5 is 29.",
            "OK, I've also justification.",
            "But I mean they're serious.",
            "Researchers think that's equally good, yeah?",
            "But then I tell them you're failing AQ test and then they say the cutest are.",
            "Bias towards humans or I mean all these kinds of things here.",
            "The answer why we prefer 5 is that a linear relation sort of is simply.",
            "It involves less arbitrary parameters than 1/4 order polynomial, because if you start with 4th order polynomials, I mean you can't predict anything for this.",
            "Linear line is just nice and.",
            "I mean this simplicity.",
            "I mean there is a human bias in some problems, but.",
            "There's also a general notion of complexity, and it's important to get reasonable answers.",
            "OK, here's a that's my favorite.",
            "So what comes next?",
            "60 three OK what else do we have?",
            "Why 63?",
            "So 63 / 3.",
            "OK, next prime number 61.",
            "OK, yeah.",
            "That sounds reasonable.",
            "But why not 60?",
            "So if you know what simple groups are and they have orders, I mean the number of elements and the number of elements is 235 over blah blah 59 and then come 60.",
            "Yeah.",
            "So 60 is the first simple group which is not of prime cardinality.",
            "So I mean, you can be fooled, but I mean now you know that and now you get going to an IQ test.",
            "What would you answer?",
            "61 or 60?",
            "Still 61, I mean in this case because it's a much more familiar concept and you would say, OK, I mean.",
            "You know the guy who came up with the sequence probably thought about prime numbers and not about simple groups.",
            "Um?",
            "You have time, I mean so there we have some bias towards culture maybe.",
            "Or maybe not.",
            "Maybe prime numbers are much more fundamental than simple groups, but maybe it's just cause of our culture and I will tell you briefly how to put this cultural bias in this universal setup.",
            "OK, so there's a nice website.",
            "We can just type in the beginning of some sequences and it has a rule large rule based and some semi smart algorithms for finding the pattern behind your sequence.",
            "And I mean often if you do research.",
            "You do.",
            "It's a force up for N equals 1234.",
            "You get some numbers and you want to find the general rule behind it, which can be quite complex.",
            "So if you had the rule to prove it is much easier to find the rules so you can just type in the numbers it finds you the rules, and then afterwards to prove what you want to prove is often much easier.",
            "I mean, it's sort of smart because I mean if you put twice the primes in, I mean it really recognized it.",
            "So if you have some transformations."
        ],
        [
            "OK, um so.",
            "Now, after these examples, the question is, is there a unique principle which allows us to formally arrive?",
            "It prediction which say coincides.",
            "Most of the time, with our intuitive guess.",
            "Or even better, which gives us in some sense, most likely the best or correct answer, whatever that means in exactly.",
            "And Luckily, the answer is yes, and this is Occam's Razor, and Orkans razor is a very simple principle.",
            "It just tells you if you have many explanations for your data which are equally good to take the most simple one.",
            "So an amazing thing is that it works.",
            "And you can prove that.",
            "And I mean, if you look at the previous examples made sense in all cases.",
            "Maybe the sunrise is a bit tricky.",
            "So actually.",
            "Or comes razor can serve as a foundation of machine learning?",
            "Some would disagree.",
            "I think you could go even further.",
            "It's a fundamental principle in science.",
            "I would even go further.",
            "I would say that's the definition of science or comes razor.",
            "So what we try to do, we try to understand our world.",
            "Understanding our world means collecting data.",
            "And finding regularity's in them regularity's means finding models which are simple, simpler than the data themselves, and the simpler the model, as long as it's equally good, yeah.",
            "Um?",
            "Yeah, I'm almost finished, so the simpler the model, the.",
            "The more typically you attracted to it, so you apply or comes razor, maybe even unconsciously.",
            "The system is already established, so you've already decided what you want.",
            "We're actually going to be, yeah.",
            "Yeah.",
            "The conceptual advances are actually establishing what is the correct set of measurements to take whatever correct theoretical entities and soul.",
            "So there's a yeah, yeah.",
            "But what you what you could do is you know, just just take a video camera and place it wherever you want to.",
            "And I mean take.",
            "I mean in physics and chemistry, I mean small molecules or something.",
            "I mean then you need special measuring devices, but biology, medicine or whatever?",
            "Say let's be in the optical space.",
            "Just take the camera and move around and this is your sequence of data without any preprocessing.",
            "And then you ask, you know build simple models from that.",
            "Which are you typically doing?",
            "You're not in a formal way, but yeah, I mean you think deeply about the data and try to extract and so on.",
            "But you can ask.",
            "I mean is there a formal way or what is what is behind what you're doing?",
            "Compass Nation of new theoretical entities which is actually in some sense against all things, right?",
            "So because oftentimes something done in general don't create new explanatory entities, no, no, no, no, no.",
            "You can do that, and if it makes things simpler, so for instance, I mean make a very simple example, you have a long sequence and the pattern 2357 eleven.",
            "So the primes occur often.",
            "Then you make a special entity of primes.",
            "You build a subroutine which prints primes.",
            "Because this you use often, then to reconstruct your data.",
            "So you develop the concept of primes because it allows to compress your data.",
            "Essentially, we talk about something which is.",
            "Effectively.",
            "There are infinitely many relational answers you can come up with the final data set, so yeah, but there are only finitely many, which are simpler than the data themselves.",
            "Yeah, I haven't.",
            "I haven't told you how to find the simplest theory, but I mean once you formalize it, you just.",
            "I mean, the simple sticky enumerate all theories from simpler to more complex and look how well they are.",
            "And then you pick the best one.",
            "Of course, I mean this is, you know, it takes a lot of time and neither you do it, nor can a computer do it, but.",
            "Or can the universe do it?",
            "Yeah, but this is the gold standard which you try to achieve with some more clever methods.",
            "You know, by know by partial search, clever search or whatever.",
            "OK so but before we come to that there is one severe problem and this is that Occam's razor not have formal or mathematical objective principles so far.",
            "So what is simple for one may be complex for another, so we have to define a quantitative notion of complexity first.",
            "Before I do that, give me another exam."
        ],
        [
            "People are the famous crew emerald paradox.",
            "We've heard about that or not heard about it.",
            "OK, not yeah OK so.",
            "Now that's one of the more famous paradoxes in philosophy, which is still not completely solved.",
            "So we have two hypothesis.",
            "All emeralds are green and.",
            "Most of you know probably that the emeralds are green.",
            "But you have a second hypothesis.",
            "All emeralds found till 2010 are green and thereafter they are blue.",
            "OK, so which hypothesis is more plausible?",
            "I mean, both are totally consistent with all observations.",
            "So.",
            "From this perspective, they are equally good, but nevertheless I guess most of you would.",
            "Bet on hypothesis H1.",
            "But the question is what is the justification of that?",
            "And you can invoke Occam's Razor here too.",
            "I mean, the first hypothesis looks simpler.",
            "I mean, just count the number of letters in the sentence.",
            "Or use your intuition in this case.",
            "I mean, you could argue all the time switch in 2010, but I mean there are other processes who switch after sometime and you can come up with them and then you would argue for the switch.",
            "So.",
            "Yeah.",
            "Experience of never having seen such a thing happened.",
            "No, I have seen Emeralds.",
            "Know what you mean have yeah, I mean you want to predict the future.",
            "So what do you mean?",
            "Things never happen.",
            "You also never experienced the hypothesis that all emeralds are green in 2011.",
            "You also didn't experience that.",
            "I mean what you're trying to do now is you try to generalize.",
            "Oh, I have here class of hypothesis.",
            "Experience every day.",
            "Yeah, that's a continuity argument, which is not that bad, but there are processes which jump around and.",
            "Say for instance.",
            "The sun, I mean, rose every day, but you wouldn't predict that it's there in 20 billion years.",
            "Although it has ever been there in the past five billion years, becausw.",
            "I mean we know by physical process is that it will explode someday.",
            "So you would predict a discontinuity, although you have never experienced a discontinuity in this case.",
            "Yeah, this is another thing where you are maybe now no.",
            "I mean this happens, but this happened.",
            "The eclipse happened in the past, so you have some experience that this happens to you.",
            "So you need an example.",
            "It has not happened in the past, but will happen in the future according to our.",
            "Knowledge.",
            "OK, I mean the continuity argument is pretty good here in many cases, but it's not universal."
        ],
        [
            "OK. OK, here's another nice paradox.",
            "The black Raven paradox or confirmation paradox so.",
            "As you know, we go out and we observe Ravens and they seem all to be black.",
            "So what you typically then infer, or biologists do is OK.",
            "Probably all Ravens are black, so you have this hypothesis that.",
            "Rayveness implies blackness.",
            "So what you do is you go out, you find instances of Ravens and they have the property of being black.",
            "And if you observe enough of them then you say OK.",
            "I'm strongly believing in this hypothesis that all Ravens are black.",
            "So what you do is.",
            "Now you can abstract from it, so if you see are instances this property B and enough of them and no counterexamples, then you say this confirms the implication are implies B.",
            "Now that is what you do in practice.",
            "So now look.",
            "As another instance of this rule, I mean because R&B adjust any predicates.",
            "So let's look at not B implies not RI mean we just, you know, replace are by not be also formula.",
            "So that means that not be instances with property, not R. Should then confirm the rule not B implies not R because we can plug in for R&B anything we like.",
            "OK, so next.",
            "We know that our implies B is logically equivalent to not be implies not R, so that means.",
            "And our implies B instances also confirmed by not be instances with property not R. So according to the second rule not be, instances would probably not R confirm this rule.",
            "But this role is logically equivalent to the first rule.",
            "So these instances confirm also number one.",
            "OK, so far so good.",
            "So now consider that black bag for example.",
            "So all Ravens are black.",
            "This is our hypothesis.",
            "And our is Raven Bee is black, so observing a black Raven confirms our hypothesis.",
            "Most of you would agree.",
            "So, but now I involve Rule 3, which is derived from one and two.",
            "That means that also observing White Sox will confirm that all Ravens are black because White Sox are non Raven.",
            "Which are not black.",
            "And so they confirmed that all Ravens are black.",
            "Which is very convenient if you live in a city you just, you know, go to a drawer and look at all these white socks and then you can confirm you know some biological rules.",
            "Yeah.",
            "Sounds a little bit absurd.",
            "So I leave it as a homework.",
            "So what is going wrong here?",
            "I mean, we have a week here confined to kill us so we can discuss."
        ],
        [
            "Said later.",
            "OK, let me.",
            "Formalize before, and the philosophical part is set up.",
            "I'm considering so first it's already mentioned you can sort of roughly say the deduction problems can all be phrased.",
            "The sequence prediction tasks.",
            "For instance, classification is also a special case of sequence prediction and classification.",
            "Let's say you have a feature vector in a class label.",
            "So this is observation and then another feature vector class label feature class label.",
            "Now you have a new instance feature vector and you want to predict the class label.",
            "So your feature class feature class feature class feature?",
            "So it's just a sequence prediction problem.",
            "So as I mentioned, I will focus on maximizing profit or minimizing loss.",
            "Just by the way which is which we did not in the summer school, so we don't make any profit.",
            "So actually the a new and Nick to heavily sponsored the summer school to keep the fees low for the students.",
            "Um?",
            "So maybe maximize long-term property, or maybe some of you want to come to do a PhD with us.",
            "OK, so and I'm not primarily interested in finding a model.",
            "Point is a true model.",
            "Whatever this is or causal model.",
            "But in doing predictions or maximizing my profit.",
            "Another thing you may ask during the lecture that had not never talk about noise and useful data, and separating them in this framework.",
            "You don't need to do that.",
            "So from this highly abstract point of view, and this is a purely sort of.",
            "Practical problem."
        ],
        [
            "OK, so I presented that last week already.",
            "Well, it's good to repeat that.",
            "So now the colors are true.",
            "So last time it was just a comparison.",
            "So here this lecture will concentrate on the blue parts.",
            "And not on the black part.",
            "So there are many difficulties where how you can classify different problems or approaches to AI and machine learning.",
            "And I mean first you can be there on the machine learning side or on the knowledge based good old fashioned AI site.",
            "So we not talk about good old fashioned AI.",
            "Dan Statistical logic based approaches.",
            "I mean, this is sort of similar to the first one, so we also have a logic summer school at the end you every year, which is normally in December.",
            "So we will hear more about this side here.",
            "Then you could classify whether you just do induction or prediction or making decisions, or considering the full reinforcement learning setup with actions.",
            "Classification regression.",
            "Most of machine learning is IID.",
            "As you have seen already talks except Java STK.",
            "Mostly accepted Java stock.",
            "I will concentrate on the non IID case.",
            "So and the reinforcement learning set up later.",
            "Online versus Patch and so on.",
            "Let's see.",
            "OK, you can read that."
        ],
        [
            "OK so OK so.",
            "So the setup will be the following, so we have a time which runs from 1234 and so on.",
            "So I discretize time if it's not already discrete then we have a predictor P which makes some prediction in some prediction space Y at time T. OK, and this prediction is based on the past observations X one to T -- 1 and thereafter we observe the next instance.",
            "So for instance we picked a better tomorrow and then we observe the weather tomorrow and then we suffer a loss.",
            "And then we sum the loss over all time instances from one to say some.",
            "Cut off time T that is our total loss.",
            "For instance, in the weather forecasting.",
            "Make it binary.",
            "You can either predict sun or rain and the actions or decisions or predictions you do are taking umbrellas or or sunglasses.",
            "And here say my personal loss matrix and everybody has his own personal loss metrics, so there's little you can do about that.",
            "There's no real, and I think there cannot be a real general theory about Las matrices.",
            "Because that is sort of part of the specification of your goal.",
            "OK, so one point I want to mention is that this prediction space can be a different space than the observation space off.",
            "Very often it's the same.",
            "OK. Any questions?",
            "To this part so far.",
            "OK, now I will go and.",
            "Develop the base."
        ],
        [
            "That way of doing predictions in the sequential case.",
            "So first.",
            "Um?",
            "I will little bit of talk about probabilities.",
            "You've heard a lot about it, but there are several kinds of probabilities and several or several interpretations.",
            "Subjective objective, frequentist.",
            "Then I will show you base in Laplace rule, present another nice paradox.",
            "But based distribute based mixture distributions entropy and then some convergence bounds and loss bounds, and then my generalization to continuous probability classes.",
            "So most of the time I will talk about discrete finite or countable classes because that is first keeps them out simple.",
            "Second, I will argue, at least from our philosophical point of view, that's all you need.",
            "And 3rd is once you have mastered the discrete stuff.",
            "I mean then the continuous is just math.",
            "Yeah, I mean it has to take limits and so on which can get quite fancy.",
            "But it's very nice too if you're able to separate these two cases.",
            "OK, so.",
            "So the aim of probability theory is to discuss."
        ],
        [
            "Uncertainty, but they're very sources of uncertainty, so there is the classical frequentist interpretation that probabilities are just relative frequencies.",
            "So you toss a coin and you count more or less counting.",
            "There's the Objectivist POV who argues that probabilities are some aspects.",
            "Real aspects of the world.",
            "For instance, quantum mechanics that an Atom decays in the next hour has a certain probability, which you can compute OK and you don't have to sort of count these instances.",
            "And the subjectivist point of view which is popular in AI is that probability is just degrees of belief about our world.",
            "So for instance, I mean our world extraterrestrials either exist or not.",
            "There is no.",
            "Objective or frequentist probability?",
            "I mean they are there or not, but you can have a personal belief, so I believe that with 90%.",
            "Let's call it plausibility there.",
            "Extra restaurant, based on some evidence or theoretical considerations.",
            "And some other guy may think it's only 10%, but they exist or not."
        ],
        [
            "OK, so the frequency interpretation.",
            "So what you do is you.",
            "Start and that is sort of unavoidable with my ID.",
            "Samples and because I'm not talking about idea will also not talk about frequent, just stuff.",
            "And take a coin or die.",
            "You count the events which occur after N. Throw off your dice or your coin, and you take the ratio K, / N and the limit and this would be fine as the probability of this event OK looks pretty simple and straightforward.",
            "For instance here with a coin with fair coin you get 1/2.",
            "So it's very easy to grasp, but.",
            "There is a little bit of a.",
            "Illusion.",
            "Because first if you look for carefully what you do is.",
            "So what do we say here?",
            "So we say that this ratio converges to something.",
            "But that is not really true.",
            "I mean, the coin could come up head, head, head, head, head, head, head, head, head, even a fair coin.",
            "I mean, yeah, I mean this is extremely unlikely.",
            "OK, it's fine for me.",
            "So you say with high probability this ratio converges to a number which you called.",
            "Probability.",
            "So OK, can you please tell me what high probability means?",
            "Yeah, so you have a circular definition.",
            "And there's no.",
            "Please, I don't know any easy way out of it.",
            "What you could do?",
            "You could say OK in the limit the probability is 1, so you have reduced the problem of defining, but probabilities mean.",
            "Two, defining what probability 1 means and corner.",
            "I think it was corn or develop the principle.",
            "Everything which holds with probability one holds for sure.",
            "In our real world.",
            "Yeah, and then we're done.",
            "I disagree with the principle.",
            "Because I've never really, I mean, I mean, forget about continuous.",
            "You know probability zero event.",
            "But I mean these cases.",
            "Well, that's a way out, yeah.",
            "Yeah, it's limited to IID, and there's no real way out.",
            "And then you have this reference."
        ],
        [
            "Math problem.",
            "OK, the objective interpretation so.",
            "Typically I mean that's adopted by physicists because they have all these beautiful probabilistic theories and the predicted probabilities, and they work so well that it must be true that the real world has.",
            "True randomness in it.",
            "OK, so you start with the sample space and then you have events which are subsets of the sample space.",
            "You contact also normally IID experiments.",
            "And then you can assign probabilities to events but.",
            "You, I mean you don't have to do this IID experiments.",
            "I mean, if you have this beautiful theory which have tested.",
            "And it sort of works.",
            "In sufficiently many instances where the idea or not.",
            "And then you use this theory and then you can come up with probabilities even for events which only happened once.",
            "For instance, what is the probability that our sun explodes in this in this time interval you don't need for this, you know many other stars exploding to have some frequency count, you just use the theory which was developed based on very very different experiments.",
            "OK, and here the axioms of probability.",
            "They're all except for the.",
            "Are countable additivity which is missing.",
            "Countable additivity is just.",
            "Now everybody down because we don't need it really explicitly.",
            "OK, so and."
        ],
        [
            "Subjective interpretation is that probabilities measure degree of belief in something rather than some physical random process.",
            "And.",
            "As I said, that's most relevant interpretation or most often used, interpretation in AI.",
            "Some even.",
            "Sort of argue that's the only probabilities there are, and there are not objective probabilities.",
            "And the question is now, while with objective probabilities and when the frequency picture, I mean the axioms are pretty obvious that they are as they are.",
            "I mean you just throw the Venn diagrams or do some experiments and I had experiments and you see that's the right thing to do.",
            "But with beliefs specially human beliefs, I mean how should they operate?",
            "I mean what's the rules of of transforming beliefs?",
            "So you could start with, you know, some reasonable plausible assumptions.",
            "How this belief functions our belief in a given B.",
            "1st is it's reasonable to represent beliefs by real numbers or you need some total order.",
            "So I believe in this.",
            "More than this, yeah, so here's some space which has a total order.",
            "Say take the real numbers and then you wanted the rules qualitatively correspond to common sense.",
            "And the root should of course be mathematically consistent.",
            "And the amazing thing is, you can come up with very, very weak.",
            "Quite plausable properties which the belief functions satisfy, and from them you can derive and this is what Cox did in 1947 that Billy function is isomorphic to a probability function that satisfies the standard axioms of probability.",
            "So it's not identically to a probability, but is isomorphic, so the difference doesn't matter.",
            "So let's.",
            "A good argument that or why beliefs also behave in the same way as objective probabilities and.",
            "This is one of the reasons why you can mostly forget about the distinction."
        ],
        [
            "OK.",
            "So now if everything together we have subjective probabilities, so OK, so you can forget about it, but it often helps you to keep in mind you know this distinction.",
            "Look for my mathematical point of view.",
            "You can forget about it.",
            "OK, so based famous rule written in a way in which is mostly applied.",
            "So let D be some possible data.",
            "So it means the probability is larger than zero.",
            "Hi, is accountable complete class of mutually exclusive hypothesis?",
            "I mean normally you don't write it down, but it is implicitly what you assume.",
            "So that means that events are disjoint, disjunct, and that the Union gives the whole sample space an gives the whole space of.",
            "The whole model space.",
            "So what is given?",
            "So given even a priore plausibility of hypothesis, so which are normally regarded subjective probabilities, so before you do experiment, you think about oh, it's much more likely that this coin is fair and you come up with some you know some.",
            "Some arguments I say OK, maybe the probability is here.",
            "It's 90%, and then you spread out another 10% to the other possibilities.",
            "Then you have the likelihood function, which is more or less defining what this hypothesis mean and normally you grab them as objective probabilities or sampling probabilities.",
            "So your sample.",
            "Now your data from this model.",
            "And what you want is is the posterior distribution.",
            "Which is again a subjective belief.",
            "Probabilities or after I've seen that you believe in this in that hypothesis to this degree.",
            "And OK, here's based rule, which I've seen now 100 thousands of times.",
            "I wrote out this demon Dominator.",
            "Becausw the only quantities we have is the prior and likelihood and the evidence.",
            "Often Brittany, 9 minutes.",
            "We don't have.",
            "We have to compute and we can't compute it from the prior likelihood.",
            "And the proof is here.",
            "So first from the complete Ness and exclusiveness you get that the probability of the hypothesis sum to one.",
            "Then you use the property of conditional probabilities or PFD.",
            "Given 8 * P of HP of H given D times PFD.",
            "So, because this is just P of the NH, so you swap it around two times up lying.",
            "The definition of conditional probability and now this sum is 1 as indicated here and you get PFD.",
            "So that means the denominator is just P of D and with peavine dominantes trivial by this rule holds.",
            "I recommend that you do the proof yourself and really take the axioms and only the axioms and do the proof step by step.",
            "I mean not cheat and the nice thing is that you will see that you need all the axioms.",
            "There's no super no one, which is superfluous.",
            "But only the axioms.",
            "So there are enough.",
            "I mean, many percent base rule is something totally trivial here, but it isn't.",
            "Especially for countably infinite classes.",
            "And then you need the countable additivity."
        ],
        [
            "OK, so now come let's come to Laplace's rule.",
            "You start again with your biased coin.",
            "Or any experiment which can have two outcomes and with which are independent.",
            "Alright, you assume to be independent.",
            "So the probability of one or head will be tighter.",
            "Data can be any number in the interval zero and one, and this is called ability to process.",
            "OK, so your hypothesis is that my data is sampled from.",
            "A biased coin with biased data.",
            "So we have observed a finite sequence X one to XN.",
            "And let's assume this sequence is N11 and N00.",
            "And your sample space.",
            "You can see that the infinite sequences because you go on and go on is the class of infinite sequences, so that is Omega.",
            "So now the events.",
            "Remember, events are subsets of Omega.",
            "Cause we have observed only the 1st and instances.",
            "But our sample space is the set of all infinite sequences.",
            "The events are all infinite sequences.",
            "Which starts with a certain finite.",
            "String, so we take all infinite sequences which have X1 to XN equal to the observed thing.",
            "So this is your basic event.",
            "Um?",
            "Your data likelihood is so if observed, probability of observing A1 instead of better.",
            "So you take it to the power in one and the same folder N 0.",
            "So there's your likelihood.",
            "I write it as Peter of X.",
            "And So what based it already in 1763?",
            "So the rule is nearly 250 years old.",
            "Um?",
            "He assumed a uniform prior over the theater.",
            "So our priority set all biases are equally likely.",
            "And here instead of having the sum because it's a continuous variable, you you have to replace it by integrals.",
            "Yeah, I will not mention that this is just math.",
            "OK, and then you compute the evidence, which is the denominator in base rule.",
            "That is this integral here and you can look it up, which gives you know.",
            "Here reflection of factorials.",
            "Which is again hard to interpret as I mentioned, but very important quantity."
        ],
        [
            "So what we really want is the posterior.",
            "So after I've seen the data, what should we believe?",
            "And you use now?",
            "Bayes rule, you plug everything in and you get this function.",
            "So here's a plot.",
            "So here is Theta.",
            "And here is P of Theta given X.",
            "So I assumed that the number of zeros in numbers ones in the actual case.",
            "Here the plotted case is equal, so.",
            "And for an equal 10, for instance, after sleeping five hits and five tails, you posterior distribution looks like that.",
            "So you have a strong belief that theater is between 0.3 and 0.7, so probably 90% and the more data you have, the more confident you are that it's around 0.5, yes?",
            "Versus just stretching, I guess that's stretching.",
            "Yeah, and the N = 3 quasi mean.",
            "They have 1.5 observations head and 1.5 observations tail.",
            "Yeah, it makes no sense here.",
            "Mathematically, yes, but.",
            "Yeah, forgot about that.",
            "OK, I mean this concentration.",
            "You're all aware of, and so OK, that is based famous rule, nearly 250 years old.",
            "So in what Laplace did, he said OK, it's nice to have this data, so we have this whole distribution, but the complex object.",
            "What do we do with it?",
            "We usually use it for making predictions.",
            "So what we really want is what is the probability that, for instance, the next observation over the next 10 observations or whatever is 1 given our past observations and there are two ways to compute that.",
            "One is you can say the probability of X N + 1 given X one up to XN is OK, so we derive the probability of Theta given X12 N. And then we ask what is the probability of X N + 1 given Theta and then we integrate out?",
            "Tetteh, so that's the posterior.",
            "And then for this data, the prediction should be this.",
            "And I mean this is just data.",
            "But status uncertain, so we average over Theta.",
            "Yeah, which is just I mean in this case the expectation of P of Theta given X1.",
            "Yes yes, sorry.",
            "OK so but there's a much simpler way to calculate that the conditional probability is defined as the probability.",
            "I mean of a given B is for build A&B, so it's the probability that the beginning was X one to North and it is continued by X N + 1.",
            "Divided by the probability of X 1 + 1.",
            "At X12 N. So here you see nicely the evidence.",
            "This abstract object on the last slide, you can just compute it for.",
            "For N + 1 observations and divided by N observations and then you will see that all these factorials.",
            "All these factorials here.",
            "Cancel out and what remains is just N 1 + 1.",
            "And 1 + 1 / 2 where N one is the number of.",
            "Once you have observed.",
            "So that's a very reasonable rule, becausw.",
            "First in the long run, this converges this probability one yeah to the.",
            "To the probability, so you have 1 + 1 and one and two.",
            "It doesn't matter, but the good thing is that in the short run it gives reasonable answers.",
            "So if you just take the ratio and you have observed only hits, you would predict the probability of one which makes no sense.",
            "Whereas here if I have observed say nothing, it gives probability 1/2.",
            "So it's defined and if observed with someone's it gives a large number but not exactly 1.",
            "So Butler plus did.",
            "He believed that the sun had risen for 5000 years.",
            "I think he took this from the Bible.",
            "So which is about 1.8 million days and it ever always rose.",
            "So we had 1.8 million divided by.",
            "So one point 8,000,000 + 1 / 1.8 million +2.",
            "Look at the probability that the sun will rise tomorrow, so the compliment that the sun will not rise of doomsday is 1 / 2 million roughly.",
            "And.",
            "At least this answer is not obviously nonsense.",
            "Like other answers.",
            "Probably one or undefined, yes.",
            "Crazy question.",
            "Automatically question.",
            "What is the probability of the sun disappearing out of existence before tomorrow?",
            "Then I can ask another question, what's the probability of the sun being stolen by aliens before tomorrow and all of these questions?",
            "We had this same number of observations over a sequence of days.",
            "So all of these questions you have to give that same probability of the answer, but then so be phrased.",
            "That question is that a whole sequence of different questions that all involve reasons why the sunrise tomorrow you add them all up, you'll get a probability that.",
            "Now what you get is here.",
            "What you get is here.",
            "I mean, these numbers change.",
            "Yeah, and I don't claim that 1 / 2 million is the perfect right answer so.",
            "But for instance, if you take Jeffreys prior then you have here 1/2 and here one and you get a different answer if you take I mean you get more or less 1/2 of this answer.",
            "Yes, 1/2 of this answer if you increase your samples page and say OK, the sunken is might not rise because of 10 other reasons.",
            "Then you probably increase this number by a factor of 10.",
            "Yes, you can screw it up and this is definitely not.",
            "You know, you cannot say this is the right answer within a factor of two or something and I will come to that later.",
            "Even more severe problems because what he did is assumed.",
            "An IID model, he said.",
            "You know the rise of the sun every day is independent of the previous day.",
            "So like flipping a coin, which maybe it's his times was not so bad because they didn't know anything about stars.",
            "But even then it was pretty absorbed.",
            "I just want to say that this gives an answer which.",
            "Is not obviously nonsense.",
            "If you if you increase your sample space or whatever and you get something which is a factor of 100 larger effective 100 smaller and so on.",
            "There all answers which seem to make sense, I agree.",
            "Yeah.",
            "Essentially, the idea is equivalent to an argument of structural consistency overtime, so that if you're if you're going to entertain the notion of alien abduction of the sun, the pictures you've already got in your X1 doing already reflect the probability of the earnings would have adopted the sun in the past.",
            "OK. Sunrise.",
            "I mean, you could argue becausw.",
            "We have never observed an alternative we put should should put all alternatives in one box, but you could argue differently.",
            "And I mean, if you take the knife uniform prior.",
            "Then I mean you have a difficult process with uniform prior and it.",
            "It depends on it and.",
            "Yeah, but I will come to that.",
            "Not as explicit of giving you, you know, a precise answer to this question, But yeah, problem.",
            "Approach to this would be to say something like if the probability of the sun.",
            "Arising was substantial enough, less than one.",
            "Only .9 Xtremely unlikely that I would have written this long.",
            "The answer is somewhere between one and however far down such that it would be extremely unlikely would have survived for today, but you wouldn't then commit any particular number in there.",
            "We just say another.",
            "Yeah, yeah, I mean, this is sort of the robust page, not imprecise probability approach, which I'm not really a fan of cause.",
            "I mean, why do you?",
            "Why do you think it could be?",
            "You know larger than?",
            "0.99 but you're absolutely sure it's not 0.989999.",
            "So what I don't like is that this that is, intervals have sharp boundaries because the place where you put the boundaries are pretty arbitrary.",
            "So what you should do you check I should soften this boundary somehow.",
            "But then what you do is you do a second order probability.",
            "Over your original probabilities so.",
            "It depends what question you ask.",
            "Yeah, if you just ask what is the probability that sunrise is tomorrow, I think you should give some number.",
            "Yeah, but if you ask about the distribution of probabilities, you know in some larger space or something.",
            "Yeah.",
            "I mean then I agree that you could give something which is sort of an analog of an interval of toy would prefer.",
            "I'm a distribution over it.",
            "Um?",
            "Yeah, but I mean what you do is you give some yeah confidence interval.",
            "I mean, it's.",
            "Yeah, it's a good answer, but I personally don't don't like it too much.",
            "OK, any other questions?",
            "OK, let's have a break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK morning.",
                    "label": 0
                },
                {
                    "sent": "We lost a few guys.",
                    "label": 0
                },
                {
                    "sent": "I think they're coming.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So so far you hurt.",
                    "label": 0
                },
                {
                    "sent": "A bunch of methods solving a bunch of problems which are sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, in the area for enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And you start to wonder, maybe?",
                    "label": 0
                },
                {
                    "sent": "Or hopefully I mean how all these things fit together.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is just, you know this.",
                    "label": 0
                },
                {
                    "sent": "Thousands and more and more methods apply to problems where you.",
                    "label": 0
                },
                {
                    "sent": "Have to learn from data or something?",
                    "label": 0
                },
                {
                    "sent": "Or is there some underlying principle or underlying principles or a few?",
                    "label": 0
                },
                {
                    "sent": "Rules or.",
                    "label": 0
                },
                {
                    "sent": "Paradigms, yeah, we sort of hold everything together.",
                    "label": 0
                },
                {
                    "sent": "And in the next three hours.",
                    "label": 0
                },
                {
                    "sent": "I will present the philosophical, statistical, and computational foundations of machine learning.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, I mean three hours is not too much, so I cannot really flesh out all the connections to all the other talks you've heard.",
                    "label": 0
                },
                {
                    "sent": "But I mean you will see here and there where the connections are.",
                    "label": 0
                },
                {
                    "sent": "Especially to Java Stockon reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Meet you heard last week.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you have one slide which is sort of a one slide summary of my 3 hours today and at the end of the talk I have the same slide and you know then you can put it sort of in context.",
                    "label": 0
                },
                {
                    "sent": "So the set up I will consider.",
                    "label": 0
                },
                {
                    "sent": "There's some.",
                    "label": 0
                },
                {
                    "sent": "Like maybe we can.",
                    "label": 0
                },
                {
                    "sent": "Can you see that OK?",
                    "label": 0
                },
                {
                    "sent": "Or is it OK?",
                    "label": 0
                },
                {
                    "sent": "So I will consider.",
                    "label": 0
                },
                {
                    "sent": "Sequential data.",
                    "label": 0
                },
                {
                    "sent": "So that means essentially non IID data.",
                    "label": 0
                },
                {
                    "sent": "So you have sequence on state data.",
                    "label": 0
                },
                {
                    "sent": "For instance, I mean typical weather data or stock market data or other time series X1X2X3 updates N and you want to predict XN plus one OK.",
                    "label": 0
                },
                {
                    "sent": "So most of the talk later I come to the Agent framework and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And of course this general set up.",
                    "label": 0
                },
                {
                    "sent": "Includes ideas, a special case.",
                    "label": 0
                },
                {
                    "sent": "So and actually even predicting, I think I mentioned it already.",
                    "label": 0
                },
                {
                    "sent": "You could ask.",
                    "label": 0
                },
                {
                    "sent": "I mean, why should we care about predictions?",
                    "label": 0
                },
                {
                    "sent": "The ultimate goal is to do something with these predictions and to maximize your profit.",
                    "label": 1
                },
                {
                    "sent": "If your capitalist or more neutral cap maximizes some motility function or minimize loss if you.",
                    "label": 0
                },
                {
                    "sent": "I'll take the negative picture, but that's just a - OK.",
                    "label": 0
                },
                {
                    "sent": "So in say, nearly all approaches.",
                    "label": 0
                },
                {
                    "sent": "What you do is you start with a class of models or hypothesis where your data could come from.",
                    "label": 0
                },
                {
                    "sent": "And then you try to find.",
                    "label": 0
                },
                {
                    "sent": "Ideally, a true hypothesis or the true hypothesis, but it's hard to tell what the truth the ground truth is.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about good hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Whatever good means in detail and one classical estimators and maximum likelihood estimator, so you have.",
                    "label": 0
                },
                {
                    "sent": "Are we still not have a pointer?",
                    "label": 0
                },
                {
                    "sent": "You take this likelihood function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is sort of defining what the hypothesis mean.",
                    "label": 0
                },
                {
                    "sent": "I mean what is hi means this is hypothesis and hypothesis tells you a probability distribution over.",
                    "label": 0
                },
                {
                    "sent": "Over your sample space.",
                    "label": 0
                },
                {
                    "sent": "So in the maximum likelihood estimate, it just takes the hypothesis which has maximum likelihood becausw.",
                    "label": 0
                },
                {
                    "sent": "The argument is that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is likely is very low.",
                    "label": 0
                },
                {
                    "sent": "Then it's very exceptional that you saw this data, but we saw this data so it should not be too exceptional.",
                    "label": 0
                },
                {
                    "sent": "And if the likelihood is high.",
                    "label": 0
                },
                {
                    "sent": "You can expect this data and the data happens so that sort of seems to confirm the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether they're better arguments.",
                    "label": 0
                },
                {
                    "sent": "I'm in any case I will not consider the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And further becausw it has some problems if the class is too large.",
                    "label": 0
                },
                {
                    "sent": "It leads to overfitting.",
                    "label": 0
                },
                {
                    "sent": "So in the Bayesian approach, what you do is you consider the posterior, which is a much cleaner interpretation.",
                    "label": 0
                },
                {
                    "sent": "So what you want is you want the probability or degree of belief in a hypothesis after you have seen the data.",
                    "label": 0
                },
                {
                    "sent": "And Luckily you can compute it from the likelihood times aprior.",
                    "label": 0
                },
                {
                    "sent": "So what we Additionally need now is a prior.",
                    "label": 0
                },
                {
                    "sent": "The question is, where does this prior come from?",
                    "label": 0
                },
                {
                    "sent": "And there are several principles, but there's.",
                    "label": 0
                },
                {
                    "sent": "Actually, only one principle which is general enough to cover really, I mean.",
                    "label": 0
                },
                {
                    "sent": "All of machine learning or most.",
                    "label": 0
                },
                {
                    "sent": "And there is a comes razor, say combined with epicor's principle.",
                    "label": 1
                },
                {
                    "sent": "So what you should do is you should have a high prior for simple models and I will quantify that.",
                    "label": 0
                },
                {
                    "sent": "So you quantify simplicity or complexity with Kolmogorov complexity.",
                    "label": 0
                },
                {
                    "sent": "Or solo models induction scheme.",
                    "label": 0
                },
                {
                    "sent": "And then you have everything you need.",
                    "label": 1
                },
                {
                    "sent": "The theorems typically.",
                    "label": 0
                },
                {
                    "sent": "I mean very general generic tell you that all this works if your true hypothesis value data is sampled from is in the model class.",
                    "label": 0
                },
                {
                    "sent": "So it's very rough statement, but but they're all sort of like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is covers the prediction case and I develop.",
                    "label": 0
                },
                {
                    "sent": "Actually, some of it, but I mean I want to show you here a universal way of doing prediction in any kind of domain.",
                    "label": 0
                },
                {
                    "sent": "And the last step is to.",
                    "label": 0
                },
                {
                    "sent": "To generalize that to the reinforcement learning setup where you decisions influence the environment.",
                    "label": 0
                },
                {
                    "sent": "So the general Agent set up.",
                    "label": 0
                },
                {
                    "sent": "OK, so there was a yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for instance, if M is the class of all polynomials all degrees.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then you have a few data points.",
                    "label": 0
                },
                {
                    "sent": "And then the best fitting polynomial, so the maximum likelihood polynomial.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you take, say Gaussian noise and then.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood is just minimizing the squared error and then you get a polynomial perfectly fits through the data.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Is what?",
                    "label": 0
                },
                {
                    "sent": "No, no it contains everything, so it contains sort of all degree one polynomials or degree two polynomials, or degrees of three polyester union.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I think I gave this definition already.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In my first lecture, but I can repeat it, so that's one way to define machine learning, so machine learning is concerned with developing with developing algorithms that learn from experience with models of the environment, from the acquainted knowledge, and I mean so far so good, but but distinguishes.",
                    "label": 1
                },
                {
                    "sent": "Machine learning from some other fields is that you really want to use this model for something, and something typically means for prediction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK overview comes first, then some philosophical issues.",
                    "label": 1
                },
                {
                    "sent": "Then about Basean sequence prediction.",
                    "label": 1
                },
                {
                    "sent": "Then I come to this Solomon of universal inductive inference scheme.",
                    "label": 1
                },
                {
                    "sent": "And then that's a slight digression becausw this is.",
                    "label": 0
                },
                {
                    "sent": "Some application or Alex would say then or in an analogy.",
                    "label": 0
                },
                {
                    "sent": "I would say this is then non theoretical this work.",
                    "label": 0
                },
                {
                    "sent": "Remember, Alex always told you that work is unapplied.",
                    "label": 0
                },
                {
                    "sent": "If it was theoretical.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 1
                },
                {
                    "sent": "This is an theoretical this work then, but it really works well in practice if you care.",
                    "label": 0
                },
                {
                    "sent": "And then about the universal AI stuff.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you will see that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some philosophical questions around induction mean induction means inferring models from data roughly OK, so.",
                    "label": 0
                },
                {
                    "sent": "You can ask.",
                    "label": 0
                },
                {
                    "sent": "I mean this inductive or inductive inference or adaptive prediction.",
                    "label": 0
                },
                {
                    "sent": "That you use this model stand for prediction.",
                    "label": 0
                },
                {
                    "sent": "I often will not distinguish between these two things.",
                    "label": 0
                },
                {
                    "sent": "So one question is first, I mean does inductive inference work at all?",
                    "label": 1
                },
                {
                    "sent": "OK, it seems so yeah, but then you can ask why?",
                    "label": 0
                },
                {
                    "sent": "And how and?",
                    "label": 0
                },
                {
                    "sent": "In the basic framework.",
                    "label": 1
                },
                {
                    "sent": "You have to choose a model class.",
                    "label": 0
                },
                {
                    "sent": "You have to choose a prior.",
                    "label": 1
                },
                {
                    "sent": "So how do we do that in a generic way?",
                    "label": 0
                },
                {
                    "sent": "How then we can do optimal decisions if nothing is known about environment and finally, for instance, what is intelligence?",
                    "label": 0
                },
                {
                    "sent": "If you want to work on AI, it's good to know what intelligence means.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give an analogy to a different field.",
                    "label": 0
                },
                {
                    "sent": "I mean I could.",
                    "label": 0
                },
                {
                    "sent": "But many fields it, let's take complexity theory becausw you probably know that.",
                    "label": 0
                },
                {
                    "sent": "So what is the goal of complexity theory?",
                    "label": 0
                },
                {
                    "sent": "The goal is to find plus algorithms.",
                    "label": 1
                },
                {
                    "sent": "For solving problems.",
                    "label": 1
                },
                {
                    "sent": "Order show lower balance on their computation time, so if you read this statement or look at the literature, everything is rigorously defined.",
                    "label": 0
                },
                {
                    "sent": "I mean, what is an algorithm Turing machine problem, class computation time?",
                    "label": 0
                },
                {
                    "sent": "So nearly everything is rigorous.",
                    "label": 0
                },
                {
                    "sent": "OK, but most disciplines starts in an informal way and then with time they get more more formalized and get absolutely rigorous.",
                    "label": 1
                },
                {
                    "sent": "And then you have maybe even axiomatization, and then it's pure math in a certain sense, look at probability theory may now have measure theory.",
                    "label": 0
                },
                {
                    "sent": "Start with Clover axioms and then you can do all kinds of fancy math.",
                    "label": 0
                },
                {
                    "sent": "OK, for example set theory started in an informal way, then you got the Russell Paradox.",
                    "label": 0
                },
                {
                    "sent": "The set of all sets which do not contain each other itself, and at some point they formalized it axiomatized, and now it's done.",
                    "label": 0
                },
                {
                    "sent": "I mean, at least from a conceptual point of view, logical reasoning, proof theory, probability theory, infinitesimal calculus, physics to conserve energy and temperature, and quantum field theory.",
                    "label": 1
                },
                {
                    "sent": "So all apart from the last half an hour rigorous mathematical formulation with quantum field theory also developed in the 1930s.",
                    "label": 0
                },
                {
                    "sent": "And the most important theory in physics, and most successful, still not rigorously mathematically defined, but it doesn't really matter in this case because physicists are able to compute numbers out of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 1
                },
                {
                    "sent": "Now let's look at machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so machine learning OK. Repeat myself, tries to build and understand systems that learn from past data, make good predictions, are able to generalize, maybe act intelligently.",
                    "label": 1
                },
                {
                    "sent": "So many of these terms are vaguely defined or there are many alternative definitions.",
                    "label": 0
                },
                {
                    "sent": "I mean, what does generalize mean?",
                    "label": 0
                },
                {
                    "sent": "What does good prediction means?",
                    "label": 0
                },
                {
                    "sent": "Rather, understanding me and so on so it would be nice and probably very useful to have a formal general definition.",
                    "label": 0
                },
                {
                    "sent": "Or characterization of machine learning axiomatization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will present a classical example by Laplace from 1700 something.",
                    "label": 0
                },
                {
                    "sent": "So he asked what is the probability that the sun will rise tomorrow?",
                    "label": 1
                },
                {
                    "sent": "So what you have so far as you have seen the sun rising every day?",
                    "label": 0
                },
                {
                    "sent": "I mean, if it's cloudy, sort of.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can still see there is light, so it's not about clouds or.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So we have seen the sun rising, or he's seen it D days.",
                    "label": 0
                },
                {
                    "sent": "And he asked.",
                    "label": 0
                },
                {
                    "sent": "I mean, what is the chance that it raises tomorrow?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's good to know.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if it's not raises sort of you associated with doomsday.",
                    "label": 1
                },
                {
                    "sent": "OK, so you can think of several answers, so you could say OK, please undefined because there's never been experiment that tested existence of the tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK. Another answer would be P = 1 because the sun rose in all past experiments.",
                    "label": 1
                },
                {
                    "sent": "So you make another frequency estimate.",
                    "label": 0
                },
                {
                    "sent": "So the number of counts of ones divided by the total count is 1.",
                    "label": 0
                },
                {
                    "sent": "So the probability is 1.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Is not good because you can never be sure in real world or anything, so no probability should ever be one.",
                    "label": 1
                },
                {
                    "sent": "Maybe if it's a logical tautology then it may be one, but even then you can think of Hermes or logic really, I mean safe.",
                    "label": 0
                },
                {
                    "sent": "Or you take statistical approach and you look at the proportion of stars which they explode per day.",
                    "label": 0
                },
                {
                    "sent": "Then the probability is 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "Then the sun will rise tomorrow.",
                    "label": 0
                },
                {
                    "sent": "No, not so bad.",
                    "label": 0
                },
                {
                    "sent": "Or Laplace and I will derive this rule, said the probability should be D + 1 / D + 2 who does not know this rule.",
                    "label": 0
                },
                {
                    "sent": "We have seen this here first time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "This should be the first rule taught, taught in statistics, or at least in machine learning.",
                    "label": 1
                },
                {
                    "sent": "OK. Or your physicist.",
                    "label": 0
                },
                {
                    "sent": "And then I mean you look at the type at age and size and temperature of the sun and then you make some conclusions.",
                    "label": 0
                },
                {
                    "sent": "Although you've never seen say another star exploding or something.",
                    "label": 0
                },
                {
                    "sent": "Which you can do also.",
                    "label": 0
                },
                {
                    "sent": "So there are many answers and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is the right one?",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but often you have in the past experience.",
                    "label": 0
                },
                {
                    "sent": "I'm on both sides.",
                    "label": 0
                },
                {
                    "sent": "I mean, you flip a coin, yeah, I mean, maybe not this coin, but other coins you've seen a lot of heads and tails and you know their heads and tails.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then you make up your sample space.",
                    "label": 0
                },
                {
                    "sent": "OK, their heads and tails and maybe ID.",
                    "label": 0
                },
                {
                    "sent": "And then you make some prediction.",
                    "label": 0
                },
                {
                    "sent": "But you've never experienced you know you've never been on another planet or whatever in another life or whatever.",
                    "label": 0
                },
                {
                    "sent": "Seen a son not raising so it always rose so.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a logical tautology that it arises every day.",
                    "label": 0
                },
                {
                    "sent": "Added, sorry, that's the first one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there are people who think that some things cannot be predicted here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm particularly keen on this first thing, but with the sun, I mean it's not an IID experiment.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "It's it's hard to argue, sort of that, OK?",
                    "label": 0
                },
                {
                    "sent": "I mean the sunrise every day is independent of each other, and so these are independent experiments.",
                    "label": 0
                },
                {
                    "sent": "And then you predict on them.",
                    "label": 0
                },
                {
                    "sent": "So auto Laplace did it for deriving this rule.",
                    "label": 0
                },
                {
                    "sent": "As you will see.",
                    "label": 0
                },
                {
                    "sent": "So I mean you can come up with some arguments by in this case and you can't do a prediction.",
                    "label": 0
                },
                {
                    "sent": "Or the generic argument?",
                    "label": 0
                },
                {
                    "sent": "I mean, you could generalize that induction is not possible at all.",
                    "label": 0
                },
                {
                    "sent": "OK so but apart from the third answer and the first answer and the 2nd.",
                    "label": 0
                },
                {
                    "sent": "All these P seem to be close to one, but not exactly 1.",
                    "label": 0
                },
                {
                    "sent": "So independent of the justification, we say that the sun will rise tomorrow with high probability.",
                    "label": 1
                },
                {
                    "sent": "So we all agree on that somehow.",
                    "label": 0
                },
                {
                    "sent": "So it would be good that we could also agree more quantitatively on the answer and even have some formal system.",
                    "label": 0
                },
                {
                    "sent": "Which gives you the answer.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another examples, look at this.",
                    "label": 0
                },
                {
                    "sent": "OK, some of you may think OK, that looks pretty random.",
                    "label": 0
                },
                {
                    "sent": "So you count the frequencies of say ones and tools and so on, and you see in the long run the relative frequency is 1 / 10.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the next digit being whatever say one or two.",
                    "label": 1
                },
                {
                    "sent": "Should be 1 / 10 of.",
                    "label": 0
                },
                {
                    "sent": "Could be 1 / 10.",
                    "label": 0
                },
                {
                    "sent": "But I guess most of you see.",
                    "label": 1
                },
                {
                    "sent": "That this looks like the digits after pie, and once you recognize this structure and you don't know it by heart, or you switch on some math program, you would probably predict.",
                    "label": 0
                },
                {
                    "sent": "I guess that is a file for the next digit.",
                    "label": 1
                },
                {
                    "sent": "So, so the reason why you answer 5 is that you you see some structure in the sequence and you think OK, that can't be accidentally this probably you know came from a computer printing pie or something.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here a few other examples, so that's a classical IQ test 1234.",
                    "label": 0
                },
                {
                    "sent": "What comes next?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's very tough.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "You would probably say 5 and you may even come up with a reason for five becausw.",
                    "label": 0
                },
                {
                    "sent": "I mean the sequence is just X is equal to I.",
                    "label": 0
                },
                {
                    "sent": "For the first few first four items and then you plug in X5 and this gives you five.",
                    "label": 0
                },
                {
                    "sent": "OK, it's good, but.",
                    "label": 0
                },
                {
                    "sent": "I can say no look, I mean that's a 4th order polynomial, and if you put in the numbers 1234 it gives 1234 and then X5 is 29.",
                    "label": 0
                },
                {
                    "sent": "OK, I've also justification.",
                    "label": 0
                },
                {
                    "sent": "But I mean they're serious.",
                    "label": 0
                },
                {
                    "sent": "Researchers think that's equally good, yeah?",
                    "label": 0
                },
                {
                    "sent": "But then I tell them you're failing AQ test and then they say the cutest are.",
                    "label": 0
                },
                {
                    "sent": "Bias towards humans or I mean all these kinds of things here.",
                    "label": 0
                },
                {
                    "sent": "The answer why we prefer 5 is that a linear relation sort of is simply.",
                    "label": 1
                },
                {
                    "sent": "It involves less arbitrary parameters than 1/4 order polynomial, because if you start with 4th order polynomials, I mean you can't predict anything for this.",
                    "label": 1
                },
                {
                    "sent": "Linear line is just nice and.",
                    "label": 0
                },
                {
                    "sent": "I mean this simplicity.",
                    "label": 0
                },
                {
                    "sent": "I mean there is a human bias in some problems, but.",
                    "label": 0
                },
                {
                    "sent": "There's also a general notion of complexity, and it's important to get reasonable answers.",
                    "label": 0
                },
                {
                    "sent": "OK, here's a that's my favorite.",
                    "label": 0
                },
                {
                    "sent": "So what comes next?",
                    "label": 0
                },
                {
                    "sent": "60 three OK what else do we have?",
                    "label": 0
                },
                {
                    "sent": "Why 63?",
                    "label": 1
                },
                {
                    "sent": "So 63 / 3.",
                    "label": 0
                },
                {
                    "sent": "OK, next prime number 61.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "That sounds reasonable.",
                    "label": 0
                },
                {
                    "sent": "But why not 60?",
                    "label": 0
                },
                {
                    "sent": "So if you know what simple groups are and they have orders, I mean the number of elements and the number of elements is 235 over blah blah 59 and then come 60.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So 60 is the first simple group which is not of prime cardinality.",
                    "label": 1
                },
                {
                    "sent": "So I mean, you can be fooled, but I mean now you know that and now you get going to an IQ test.",
                    "label": 0
                },
                {
                    "sent": "What would you answer?",
                    "label": 0
                },
                {
                    "sent": "61 or 60?",
                    "label": 0
                },
                {
                    "sent": "Still 61, I mean in this case because it's a much more familiar concept and you would say, OK, I mean.",
                    "label": 0
                },
                {
                    "sent": "You know the guy who came up with the sequence probably thought about prime numbers and not about simple groups.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You have time, I mean so there we have some bias towards culture maybe.",
                    "label": 0
                },
                {
                    "sent": "Or maybe not.",
                    "label": 0
                },
                {
                    "sent": "Maybe prime numbers are much more fundamental than simple groups, but maybe it's just cause of our culture and I will tell you briefly how to put this cultural bias in this universal setup.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a nice website.",
                    "label": 0
                },
                {
                    "sent": "We can just type in the beginning of some sequences and it has a rule large rule based and some semi smart algorithms for finding the pattern behind your sequence.",
                    "label": 0
                },
                {
                    "sent": "And I mean often if you do research.",
                    "label": 0
                },
                {
                    "sent": "You do.",
                    "label": 0
                },
                {
                    "sent": "It's a force up for N equals 1234.",
                    "label": 0
                },
                {
                    "sent": "You get some numbers and you want to find the general rule behind it, which can be quite complex.",
                    "label": 0
                },
                {
                    "sent": "So if you had the rule to prove it is much easier to find the rules so you can just type in the numbers it finds you the rules, and then afterwards to prove what you want to prove is often much easier.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's sort of smart because I mean if you put twice the primes in, I mean it really recognized it.",
                    "label": 0
                },
                {
                    "sent": "So if you have some transformations.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, um so.",
                    "label": 0
                },
                {
                    "sent": "Now, after these examples, the question is, is there a unique principle which allows us to formally arrive?",
                    "label": 1
                },
                {
                    "sent": "It prediction which say coincides.",
                    "label": 1
                },
                {
                    "sent": "Most of the time, with our intuitive guess.",
                    "label": 0
                },
                {
                    "sent": "Or even better, which gives us in some sense, most likely the best or correct answer, whatever that means in exactly.",
                    "label": 1
                },
                {
                    "sent": "And Luckily, the answer is yes, and this is Occam's Razor, and Orkans razor is a very simple principle.",
                    "label": 0
                },
                {
                    "sent": "It just tells you if you have many explanations for your data which are equally good to take the most simple one.",
                    "label": 0
                },
                {
                    "sent": "So an amazing thing is that it works.",
                    "label": 0
                },
                {
                    "sent": "And you can prove that.",
                    "label": 0
                },
                {
                    "sent": "And I mean, if you look at the previous examples made sense in all cases.",
                    "label": 0
                },
                {
                    "sent": "Maybe the sunrise is a bit tricky.",
                    "label": 1
                },
                {
                    "sent": "So actually.",
                    "label": 1
                },
                {
                    "sent": "Or comes razor can serve as a foundation of machine learning?",
                    "label": 0
                },
                {
                    "sent": "Some would disagree.",
                    "label": 0
                },
                {
                    "sent": "I think you could go even further.",
                    "label": 0
                },
                {
                    "sent": "It's a fundamental principle in science.",
                    "label": 0
                },
                {
                    "sent": "I would even go further.",
                    "label": 0
                },
                {
                    "sent": "I would say that's the definition of science or comes razor.",
                    "label": 0
                },
                {
                    "sent": "So what we try to do, we try to understand our world.",
                    "label": 0
                },
                {
                    "sent": "Understanding our world means collecting data.",
                    "label": 0
                },
                {
                    "sent": "And finding regularity's in them regularity's means finding models which are simple, simpler than the data themselves, and the simpler the model, as long as it's equally good, yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm almost finished, so the simpler the model, the.",
                    "label": 0
                },
                {
                    "sent": "The more typically you attracted to it, so you apply or comes razor, maybe even unconsciously.",
                    "label": 0
                },
                {
                    "sent": "The system is already established, so you've already decided what you want.",
                    "label": 0
                },
                {
                    "sent": "We're actually going to be, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The conceptual advances are actually establishing what is the correct set of measurements to take whatever correct theoretical entities and soul.",
                    "label": 0
                },
                {
                    "sent": "So there's a yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "But what you what you could do is you know, just just take a video camera and place it wherever you want to.",
                    "label": 0
                },
                {
                    "sent": "And I mean take.",
                    "label": 0
                },
                {
                    "sent": "I mean in physics and chemistry, I mean small molecules or something.",
                    "label": 0
                },
                {
                    "sent": "I mean then you need special measuring devices, but biology, medicine or whatever?",
                    "label": 0
                },
                {
                    "sent": "Say let's be in the optical space.",
                    "label": 0
                },
                {
                    "sent": "Just take the camera and move around and this is your sequence of data without any preprocessing.",
                    "label": 0
                },
                {
                    "sent": "And then you ask, you know build simple models from that.",
                    "label": 0
                },
                {
                    "sent": "Which are you typically doing?",
                    "label": 0
                },
                {
                    "sent": "You're not in a formal way, but yeah, I mean you think deeply about the data and try to extract and so on.",
                    "label": 0
                },
                {
                    "sent": "But you can ask.",
                    "label": 0
                },
                {
                    "sent": "I mean is there a formal way or what is what is behind what you're doing?",
                    "label": 0
                },
                {
                    "sent": "Compass Nation of new theoretical entities which is actually in some sense against all things, right?",
                    "label": 0
                },
                {
                    "sent": "So because oftentimes something done in general don't create new explanatory entities, no, no, no, no, no.",
                    "label": 0
                },
                {
                    "sent": "You can do that, and if it makes things simpler, so for instance, I mean make a very simple example, you have a long sequence and the pattern 2357 eleven.",
                    "label": 0
                },
                {
                    "sent": "So the primes occur often.",
                    "label": 0
                },
                {
                    "sent": "Then you make a special entity of primes.",
                    "label": 0
                },
                {
                    "sent": "You build a subroutine which prints primes.",
                    "label": 0
                },
                {
                    "sent": "Because this you use often, then to reconstruct your data.",
                    "label": 0
                },
                {
                    "sent": "So you develop the concept of primes because it allows to compress your data.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we talk about something which is.",
                    "label": 0
                },
                {
                    "sent": "Effectively.",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many relational answers you can come up with the final data set, so yeah, but there are only finitely many, which are simpler than the data themselves.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't told you how to find the simplest theory, but I mean once you formalize it, you just.",
                    "label": 0
                },
                {
                    "sent": "I mean, the simple sticky enumerate all theories from simpler to more complex and look how well they are.",
                    "label": 0
                },
                {
                    "sent": "And then you pick the best one.",
                    "label": 0
                },
                {
                    "sent": "Of course, I mean this is, you know, it takes a lot of time and neither you do it, nor can a computer do it, but.",
                    "label": 0
                },
                {
                    "sent": "Or can the universe do it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but this is the gold standard which you try to achieve with some more clever methods.",
                    "label": 1
                },
                {
                    "sent": "You know, by know by partial search, clever search or whatever.",
                    "label": 0
                },
                {
                    "sent": "OK so but before we come to that there is one severe problem and this is that Occam's razor not have formal or mathematical objective principles so far.",
                    "label": 0
                },
                {
                    "sent": "So what is simple for one may be complex for another, so we have to define a quantitative notion of complexity first.",
                    "label": 0
                },
                {
                    "sent": "Before I do that, give me another exam.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People are the famous crew emerald paradox.",
                    "label": 0
                },
                {
                    "sent": "We've heard about that or not heard about it.",
                    "label": 0
                },
                {
                    "sent": "OK, not yeah OK so.",
                    "label": 0
                },
                {
                    "sent": "Now that's one of the more famous paradoxes in philosophy, which is still not completely solved.",
                    "label": 0
                },
                {
                    "sent": "So we have two hypothesis.",
                    "label": 0
                },
                {
                    "sent": "All emeralds are green and.",
                    "label": 1
                },
                {
                    "sent": "Most of you know probably that the emeralds are green.",
                    "label": 0
                },
                {
                    "sent": "But you have a second hypothesis.",
                    "label": 0
                },
                {
                    "sent": "All emeralds found till 2010 are green and thereafter they are blue.",
                    "label": 1
                },
                {
                    "sent": "OK, so which hypothesis is more plausible?",
                    "label": 0
                },
                {
                    "sent": "I mean, both are totally consistent with all observations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "From this perspective, they are equally good, but nevertheless I guess most of you would.",
                    "label": 0
                },
                {
                    "sent": "Bet on hypothesis H1.",
                    "label": 0
                },
                {
                    "sent": "But the question is what is the justification of that?",
                    "label": 0
                },
                {
                    "sent": "And you can invoke Occam's Razor here too.",
                    "label": 0
                },
                {
                    "sent": "I mean, the first hypothesis looks simpler.",
                    "label": 0
                },
                {
                    "sent": "I mean, just count the number of letters in the sentence.",
                    "label": 0
                },
                {
                    "sent": "Or use your intuition in this case.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could argue all the time switch in 2010, but I mean there are other processes who switch after sometime and you can come up with them and then you would argue for the switch.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Experience of never having seen such a thing happened.",
                    "label": 0
                },
                {
                    "sent": "No, I have seen Emeralds.",
                    "label": 0
                },
                {
                    "sent": "Know what you mean have yeah, I mean you want to predict the future.",
                    "label": 0
                },
                {
                    "sent": "So what do you mean?",
                    "label": 0
                },
                {
                    "sent": "Things never happen.",
                    "label": 0
                },
                {
                    "sent": "You also never experienced the hypothesis that all emeralds are green in 2011.",
                    "label": 0
                },
                {
                    "sent": "You also didn't experience that.",
                    "label": 0
                },
                {
                    "sent": "I mean what you're trying to do now is you try to generalize.",
                    "label": 0
                },
                {
                    "sent": "Oh, I have here class of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Experience every day.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a continuity argument, which is not that bad, but there are processes which jump around and.",
                    "label": 0
                },
                {
                    "sent": "Say for instance.",
                    "label": 0
                },
                {
                    "sent": "The sun, I mean, rose every day, but you wouldn't predict that it's there in 20 billion years.",
                    "label": 0
                },
                {
                    "sent": "Although it has ever been there in the past five billion years, becausw.",
                    "label": 0
                },
                {
                    "sent": "I mean we know by physical process is that it will explode someday.",
                    "label": 0
                },
                {
                    "sent": "So you would predict a discontinuity, although you have never experienced a discontinuity in this case.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is another thing where you are maybe now no.",
                    "label": 0
                },
                {
                    "sent": "I mean this happens, but this happened.",
                    "label": 0
                },
                {
                    "sent": "The eclipse happened in the past, so you have some experience that this happens to you.",
                    "label": 0
                },
                {
                    "sent": "So you need an example.",
                    "label": 0
                },
                {
                    "sent": "It has not happened in the past, but will happen in the future according to our.",
                    "label": 0
                },
                {
                    "sent": "Knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean the continuity argument is pretty good here in many cases, but it's not universal.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, here's another nice paradox.",
                    "label": 0
                },
                {
                    "sent": "The black Raven paradox or confirmation paradox so.",
                    "label": 0
                },
                {
                    "sent": "As you know, we go out and we observe Ravens and they seem all to be black.",
                    "label": 0
                },
                {
                    "sent": "So what you typically then infer, or biologists do is OK.",
                    "label": 0
                },
                {
                    "sent": "Probably all Ravens are black, so you have this hypothesis that.",
                    "label": 0
                },
                {
                    "sent": "Rayveness implies blackness.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you go out, you find instances of Ravens and they have the property of being black.",
                    "label": 0
                },
                {
                    "sent": "And if you observe enough of them then you say OK.",
                    "label": 0
                },
                {
                    "sent": "I'm strongly believing in this hypothesis that all Ravens are black.",
                    "label": 0
                },
                {
                    "sent": "So what you do is.",
                    "label": 0
                },
                {
                    "sent": "Now you can abstract from it, so if you see are instances this property B and enough of them and no counterexamples, then you say this confirms the implication are implies B.",
                    "label": 0
                },
                {
                    "sent": "Now that is what you do in practice.",
                    "label": 0
                },
                {
                    "sent": "So now look.",
                    "label": 0
                },
                {
                    "sent": "As another instance of this rule, I mean because R&B adjust any predicates.",
                    "label": 0
                },
                {
                    "sent": "So let's look at not B implies not RI mean we just, you know, replace are by not be also formula.",
                    "label": 0
                },
                {
                    "sent": "So that means that not be instances with property, not R. Should then confirm the rule not B implies not R because we can plug in for R&B anything we like.",
                    "label": 0
                },
                {
                    "sent": "OK, so next.",
                    "label": 0
                },
                {
                    "sent": "We know that our implies B is logically equivalent to not be implies not R, so that means.",
                    "label": 0
                },
                {
                    "sent": "And our implies B instances also confirmed by not be instances with property not R. So according to the second rule not be, instances would probably not R confirm this rule.",
                    "label": 0
                },
                {
                    "sent": "But this role is logically equivalent to the first rule.",
                    "label": 0
                },
                {
                    "sent": "So these instances confirm also number one.",
                    "label": 0
                },
                {
                    "sent": "OK, so far so good.",
                    "label": 0
                },
                {
                    "sent": "So now consider that black bag for example.",
                    "label": 0
                },
                {
                    "sent": "So all Ravens are black.",
                    "label": 1
                },
                {
                    "sent": "This is our hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And our is Raven Bee is black, so observing a black Raven confirms our hypothesis.",
                    "label": 1
                },
                {
                    "sent": "Most of you would agree.",
                    "label": 0
                },
                {
                    "sent": "So, but now I involve Rule 3, which is derived from one and two.",
                    "label": 0
                },
                {
                    "sent": "That means that also observing White Sox will confirm that all Ravens are black because White Sox are non Raven.",
                    "label": 0
                },
                {
                    "sent": "Which are not black.",
                    "label": 0
                },
                {
                    "sent": "And so they confirmed that all Ravens are black.",
                    "label": 1
                },
                {
                    "sent": "Which is very convenient if you live in a city you just, you know, go to a drawer and look at all these white socks and then you can confirm you know some biological rules.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sounds a little bit absurd.",
                    "label": 0
                },
                {
                    "sent": "So I leave it as a homework.",
                    "label": 0
                },
                {
                    "sent": "So what is going wrong here?",
                    "label": 0
                },
                {
                    "sent": "I mean, we have a week here confined to kill us so we can discuss.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Said later.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                },
                {
                    "sent": "Formalize before, and the philosophical part is set up.",
                    "label": 0
                },
                {
                    "sent": "I'm considering so first it's already mentioned you can sort of roughly say the deduction problems can all be phrased.",
                    "label": 0
                },
                {
                    "sent": "The sequence prediction tasks.",
                    "label": 0
                },
                {
                    "sent": "For instance, classification is also a special case of sequence prediction and classification.",
                    "label": 1
                },
                {
                    "sent": "Let's say you have a feature vector in a class label.",
                    "label": 0
                },
                {
                    "sent": "So this is observation and then another feature vector class label feature class label.",
                    "label": 0
                },
                {
                    "sent": "Now you have a new instance feature vector and you want to predict the class label.",
                    "label": 0
                },
                {
                    "sent": "So your feature class feature class feature class feature?",
                    "label": 0
                },
                {
                    "sent": "So it's just a sequence prediction problem.",
                    "label": 1
                },
                {
                    "sent": "So as I mentioned, I will focus on maximizing profit or minimizing loss.",
                    "label": 0
                },
                {
                    "sent": "Just by the way which is which we did not in the summer school, so we don't make any profit.",
                    "label": 0
                },
                {
                    "sent": "So actually the a new and Nick to heavily sponsored the summer school to keep the fees low for the students.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "So maybe maximize long-term property, or maybe some of you want to come to do a PhD with us.",
                    "label": 1
                },
                {
                    "sent": "OK, so and I'm not primarily interested in finding a model.",
                    "label": 0
                },
                {
                    "sent": "Point is a true model.",
                    "label": 0
                },
                {
                    "sent": "Whatever this is or causal model.",
                    "label": 0
                },
                {
                    "sent": "But in doing predictions or maximizing my profit.",
                    "label": 0
                },
                {
                    "sent": "Another thing you may ask during the lecture that had not never talk about noise and useful data, and separating them in this framework.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do that.",
                    "label": 0
                },
                {
                    "sent": "So from this highly abstract point of view, and this is a purely sort of.",
                    "label": 0
                },
                {
                    "sent": "Practical problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I presented that last week already.",
                    "label": 0
                },
                {
                    "sent": "Well, it's good to repeat that.",
                    "label": 0
                },
                {
                    "sent": "So now the colors are true.",
                    "label": 0
                },
                {
                    "sent": "So last time it was just a comparison.",
                    "label": 0
                },
                {
                    "sent": "So here this lecture will concentrate on the blue parts.",
                    "label": 0
                },
                {
                    "sent": "And not on the black part.",
                    "label": 0
                },
                {
                    "sent": "So there are many difficulties where how you can classify different problems or approaches to AI and machine learning.",
                    "label": 0
                },
                {
                    "sent": "And I mean first you can be there on the machine learning side or on the knowledge based good old fashioned AI site.",
                    "label": 0
                },
                {
                    "sent": "So we not talk about good old fashioned AI.",
                    "label": 0
                },
                {
                    "sent": "Dan Statistical logic based approaches.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is sort of similar to the first one, so we also have a logic summer school at the end you every year, which is normally in December.",
                    "label": 0
                },
                {
                    "sent": "So we will hear more about this side here.",
                    "label": 0
                },
                {
                    "sent": "Then you could classify whether you just do induction or prediction or making decisions, or considering the full reinforcement learning setup with actions.",
                    "label": 0
                },
                {
                    "sent": "Classification regression.",
                    "label": 0
                },
                {
                    "sent": "Most of machine learning is IID.",
                    "label": 0
                },
                {
                    "sent": "As you have seen already talks except Java STK.",
                    "label": 0
                },
                {
                    "sent": "Mostly accepted Java stock.",
                    "label": 0
                },
                {
                    "sent": "I will concentrate on the non IID case.",
                    "label": 0
                },
                {
                    "sent": "So and the reinforcement learning set up later.",
                    "label": 0
                },
                {
                    "sent": "Online versus Patch and so on.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, you can read that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so OK so.",
                    "label": 0
                },
                {
                    "sent": "So the setup will be the following, so we have a time which runs from 1234 and so on.",
                    "label": 0
                },
                {
                    "sent": "So I discretize time if it's not already discrete then we have a predictor P which makes some prediction in some prediction space Y at time T. OK, and this prediction is based on the past observations X one to T -- 1 and thereafter we observe the next instance.",
                    "label": 1
                },
                {
                    "sent": "So for instance we picked a better tomorrow and then we observe the weather tomorrow and then we suffer a loss.",
                    "label": 0
                },
                {
                    "sent": "And then we sum the loss over all time instances from one to say some.",
                    "label": 0
                },
                {
                    "sent": "Cut off time T that is our total loss.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the weather forecasting.",
                    "label": 0
                },
                {
                    "sent": "Make it binary.",
                    "label": 0
                },
                {
                    "sent": "You can either predict sun or rain and the actions or decisions or predictions you do are taking umbrellas or or sunglasses.",
                    "label": 0
                },
                {
                    "sent": "And here say my personal loss matrix and everybody has his own personal loss metrics, so there's little you can do about that.",
                    "label": 0
                },
                {
                    "sent": "There's no real, and I think there cannot be a real general theory about Las matrices.",
                    "label": 0
                },
                {
                    "sent": "Because that is sort of part of the specification of your goal.",
                    "label": 0
                },
                {
                    "sent": "OK, so one point I want to mention is that this prediction space can be a different space than the observation space off.",
                    "label": 0
                },
                {
                    "sent": "Very often it's the same.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "To this part so far.",
                    "label": 0
                },
                {
                    "sent": "OK, now I will go and.",
                    "label": 0
                },
                {
                    "sent": "Develop the base.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That way of doing predictions in the sequential case.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I will little bit of talk about probabilities.",
                    "label": 0
                },
                {
                    "sent": "You've heard a lot about it, but there are several kinds of probabilities and several or several interpretations.",
                    "label": 0
                },
                {
                    "sent": "Subjective objective, frequentist.",
                    "label": 0
                },
                {
                    "sent": "Then I will show you base in Laplace rule, present another nice paradox.",
                    "label": 0
                },
                {
                    "sent": "But based distribute based mixture distributions entropy and then some convergence bounds and loss bounds, and then my generalization to continuous probability classes.",
                    "label": 1
                },
                {
                    "sent": "So most of the time I will talk about discrete finite or countable classes because that is first keeps them out simple.",
                    "label": 0
                },
                {
                    "sent": "Second, I will argue, at least from our philosophical point of view, that's all you need.",
                    "label": 0
                },
                {
                    "sent": "And 3rd is once you have mastered the discrete stuff.",
                    "label": 0
                },
                {
                    "sent": "I mean then the continuous is just math.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it has to take limits and so on which can get quite fancy.",
                    "label": 0
                },
                {
                    "sent": "But it's very nice too if you're able to separate these two cases.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the aim of probability theory is to discuss.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uncertainty, but they're very sources of uncertainty, so there is the classical frequentist interpretation that probabilities are just relative frequencies.",
                    "label": 0
                },
                {
                    "sent": "So you toss a coin and you count more or less counting.",
                    "label": 0
                },
                {
                    "sent": "There's the Objectivist POV who argues that probabilities are some aspects.",
                    "label": 0
                },
                {
                    "sent": "Real aspects of the world.",
                    "label": 1
                },
                {
                    "sent": "For instance, quantum mechanics that an Atom decays in the next hour has a certain probability, which you can compute OK and you don't have to sort of count these instances.",
                    "label": 0
                },
                {
                    "sent": "And the subjectivist point of view which is popular in AI is that probability is just degrees of belief about our world.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I mean our world extraterrestrials either exist or not.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                },
                {
                    "sent": "Objective or frequentist probability?",
                    "label": 0
                },
                {
                    "sent": "I mean they are there or not, but you can have a personal belief, so I believe that with 90%.",
                    "label": 0
                },
                {
                    "sent": "Let's call it plausibility there.",
                    "label": 0
                },
                {
                    "sent": "Extra restaurant, based on some evidence or theoretical considerations.",
                    "label": 0
                },
                {
                    "sent": "And some other guy may think it's only 10%, but they exist or not.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the frequency interpretation.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you.",
                    "label": 0
                },
                {
                    "sent": "Start and that is sort of unavoidable with my ID.",
                    "label": 0
                },
                {
                    "sent": "Samples and because I'm not talking about idea will also not talk about frequent, just stuff.",
                    "label": 0
                },
                {
                    "sent": "And take a coin or die.",
                    "label": 0
                },
                {
                    "sent": "You count the events which occur after N. Throw off your dice or your coin, and you take the ratio K, / N and the limit and this would be fine as the probability of this event OK looks pretty simple and straightforward.",
                    "label": 1
                },
                {
                    "sent": "For instance here with a coin with fair coin you get 1/2.",
                    "label": 1
                },
                {
                    "sent": "So it's very easy to grasp, but.",
                    "label": 0
                },
                {
                    "sent": "There is a little bit of a.",
                    "label": 0
                },
                {
                    "sent": "Illusion.",
                    "label": 0
                },
                {
                    "sent": "Because first if you look for carefully what you do is.",
                    "label": 0
                },
                {
                    "sent": "So what do we say here?",
                    "label": 0
                },
                {
                    "sent": "So we say that this ratio converges to something.",
                    "label": 0
                },
                {
                    "sent": "But that is not really true.",
                    "label": 0
                },
                {
                    "sent": "I mean, the coin could come up head, head, head, head, head, head, head, head, head, even a fair coin.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, I mean this is extremely unlikely.",
                    "label": 0
                },
                {
                    "sent": "OK, it's fine for me.",
                    "label": 0
                },
                {
                    "sent": "So you say with high probability this ratio converges to a number which you called.",
                    "label": 0
                },
                {
                    "sent": "Probability.",
                    "label": 0
                },
                {
                    "sent": "So OK, can you please tell me what high probability means?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you have a circular definition.",
                    "label": 0
                },
                {
                    "sent": "And there's no.",
                    "label": 0
                },
                {
                    "sent": "Please, I don't know any easy way out of it.",
                    "label": 0
                },
                {
                    "sent": "What you could do?",
                    "label": 0
                },
                {
                    "sent": "You could say OK in the limit the probability is 1, so you have reduced the problem of defining, but probabilities mean.",
                    "label": 0
                },
                {
                    "sent": "Two, defining what probability 1 means and corner.",
                    "label": 0
                },
                {
                    "sent": "I think it was corn or develop the principle.",
                    "label": 0
                },
                {
                    "sent": "Everything which holds with probability one holds for sure.",
                    "label": 0
                },
                {
                    "sent": "In our real world.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then we're done.",
                    "label": 0
                },
                {
                    "sent": "I disagree with the principle.",
                    "label": 0
                },
                {
                    "sent": "Because I've never really, I mean, I mean, forget about continuous.",
                    "label": 0
                },
                {
                    "sent": "You know probability zero event.",
                    "label": 0
                },
                {
                    "sent": "But I mean these cases.",
                    "label": 1
                },
                {
                    "sent": "Well, that's a way out, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's limited to IID, and there's no real way out.",
                    "label": 0
                },
                {
                    "sent": "And then you have this reference.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Math problem.",
                    "label": 0
                },
                {
                    "sent": "OK, the objective interpretation so.",
                    "label": 1
                },
                {
                    "sent": "Typically I mean that's adopted by physicists because they have all these beautiful probabilistic theories and the predicted probabilities, and they work so well that it must be true that the real world has.",
                    "label": 0
                },
                {
                    "sent": "True randomness in it.",
                    "label": 1
                },
                {
                    "sent": "OK, so you start with the sample space and then you have events which are subsets of the sample space.",
                    "label": 1
                },
                {
                    "sent": "You contact also normally IID experiments.",
                    "label": 0
                },
                {
                    "sent": "And then you can assign probabilities to events but.",
                    "label": 0
                },
                {
                    "sent": "You, I mean you don't have to do this IID experiments.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have this beautiful theory which have tested.",
                    "label": 0
                },
                {
                    "sent": "And it sort of works.",
                    "label": 0
                },
                {
                    "sent": "In sufficiently many instances where the idea or not.",
                    "label": 0
                },
                {
                    "sent": "And then you use this theory and then you can come up with probabilities even for events which only happened once.",
                    "label": 0
                },
                {
                    "sent": "For instance, what is the probability that our sun explodes in this in this time interval you don't need for this, you know many other stars exploding to have some frequency count, you just use the theory which was developed based on very very different experiments.",
                    "label": 1
                },
                {
                    "sent": "OK, and here the axioms of probability.",
                    "label": 0
                },
                {
                    "sent": "They're all except for the.",
                    "label": 0
                },
                {
                    "sent": "Are countable additivity which is missing.",
                    "label": 0
                },
                {
                    "sent": "Countable additivity is just.",
                    "label": 0
                },
                {
                    "sent": "Now everybody down because we don't need it really explicitly.",
                    "label": 0
                },
                {
                    "sent": "OK, so and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subjective interpretation is that probabilities measure degree of belief in something rather than some physical random process.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "As I said, that's most relevant interpretation or most often used, interpretation in AI.",
                    "label": 0
                },
                {
                    "sent": "Some even.",
                    "label": 0
                },
                {
                    "sent": "Sort of argue that's the only probabilities there are, and there are not objective probabilities.",
                    "label": 0
                },
                {
                    "sent": "And the question is now, while with objective probabilities and when the frequency picture, I mean the axioms are pretty obvious that they are as they are.",
                    "label": 0
                },
                {
                    "sent": "I mean you just throw the Venn diagrams or do some experiments and I had experiments and you see that's the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "But with beliefs specially human beliefs, I mean how should they operate?",
                    "label": 0
                },
                {
                    "sent": "I mean what's the rules of of transforming beliefs?",
                    "label": 0
                },
                {
                    "sent": "So you could start with, you know, some reasonable plausible assumptions.",
                    "label": 0
                },
                {
                    "sent": "How this belief functions our belief in a given B.",
                    "label": 0
                },
                {
                    "sent": "1st is it's reasonable to represent beliefs by real numbers or you need some total order.",
                    "label": 0
                },
                {
                    "sent": "So I believe in this.",
                    "label": 0
                },
                {
                    "sent": "More than this, yeah, so here's some space which has a total order.",
                    "label": 1
                },
                {
                    "sent": "Say take the real numbers and then you wanted the rules qualitatively correspond to common sense.",
                    "label": 0
                },
                {
                    "sent": "And the root should of course be mathematically consistent.",
                    "label": 1
                },
                {
                    "sent": "And the amazing thing is, you can come up with very, very weak.",
                    "label": 0
                },
                {
                    "sent": "Quite plausable properties which the belief functions satisfy, and from them you can derive and this is what Cox did in 1947 that Billy function is isomorphic to a probability function that satisfies the standard axioms of probability.",
                    "label": 0
                },
                {
                    "sent": "So it's not identically to a probability, but is isomorphic, so the difference doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "A good argument that or why beliefs also behave in the same way as objective probabilities and.",
                    "label": 0
                },
                {
                    "sent": "This is one of the reasons why you can mostly forget about the distinction.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now if everything together we have subjective probabilities, so OK, so you can forget about it, but it often helps you to keep in mind you know this distinction.",
                    "label": 0
                },
                {
                    "sent": "Look for my mathematical point of view.",
                    "label": 0
                },
                {
                    "sent": "You can forget about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so based famous rule written in a way in which is mostly applied.",
                    "label": 0
                },
                {
                    "sent": "So let D be some possible data.",
                    "label": 1
                },
                {
                    "sent": "So it means the probability is larger than zero.",
                    "label": 1
                },
                {
                    "sent": "Hi, is accountable complete class of mutually exclusive hypothesis?",
                    "label": 0
                },
                {
                    "sent": "I mean normally you don't write it down, but it is implicitly what you assume.",
                    "label": 0
                },
                {
                    "sent": "So that means that events are disjoint, disjunct, and that the Union gives the whole sample space an gives the whole space of.",
                    "label": 0
                },
                {
                    "sent": "The whole model space.",
                    "label": 0
                },
                {
                    "sent": "So what is given?",
                    "label": 0
                },
                {
                    "sent": "So given even a priore plausibility of hypothesis, so which are normally regarded subjective probabilities, so before you do experiment, you think about oh, it's much more likely that this coin is fair and you come up with some you know some.",
                    "label": 0
                },
                {
                    "sent": "Some arguments I say OK, maybe the probability is here.",
                    "label": 0
                },
                {
                    "sent": "It's 90%, and then you spread out another 10% to the other possibilities.",
                    "label": 0
                },
                {
                    "sent": "Then you have the likelihood function, which is more or less defining what this hypothesis mean and normally you grab them as objective probabilities or sampling probabilities.",
                    "label": 0
                },
                {
                    "sent": "So your sample.",
                    "label": 0
                },
                {
                    "sent": "Now your data from this model.",
                    "label": 0
                },
                {
                    "sent": "And what you want is is the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is again a subjective belief.",
                    "label": 0
                },
                {
                    "sent": "Probabilities or after I've seen that you believe in this in that hypothesis to this degree.",
                    "label": 0
                },
                {
                    "sent": "And OK, here's based rule, which I've seen now 100 thousands of times.",
                    "label": 0
                },
                {
                    "sent": "I wrote out this demon Dominator.",
                    "label": 0
                },
                {
                    "sent": "Becausw the only quantities we have is the prior and likelihood and the evidence.",
                    "label": 0
                },
                {
                    "sent": "Often Brittany, 9 minutes.",
                    "label": 0
                },
                {
                    "sent": "We don't have.",
                    "label": 0
                },
                {
                    "sent": "We have to compute and we can't compute it from the prior likelihood.",
                    "label": 0
                },
                {
                    "sent": "And the proof is here.",
                    "label": 0
                },
                {
                    "sent": "So first from the complete Ness and exclusiveness you get that the probability of the hypothesis sum to one.",
                    "label": 0
                },
                {
                    "sent": "Then you use the property of conditional probabilities or PFD.",
                    "label": 0
                },
                {
                    "sent": "Given 8 * P of HP of H given D times PFD.",
                    "label": 0
                },
                {
                    "sent": "So, because this is just P of the NH, so you swap it around two times up lying.",
                    "label": 1
                },
                {
                    "sent": "The definition of conditional probability and now this sum is 1 as indicated here and you get PFD.",
                    "label": 0
                },
                {
                    "sent": "So that means the denominator is just P of D and with peavine dominantes trivial by this rule holds.",
                    "label": 0
                },
                {
                    "sent": "I recommend that you do the proof yourself and really take the axioms and only the axioms and do the proof step by step.",
                    "label": 0
                },
                {
                    "sent": "I mean not cheat and the nice thing is that you will see that you need all the axioms.",
                    "label": 0
                },
                {
                    "sent": "There's no super no one, which is superfluous.",
                    "label": 0
                },
                {
                    "sent": "But only the axioms.",
                    "label": 0
                },
                {
                    "sent": "So there are enough.",
                    "label": 0
                },
                {
                    "sent": "I mean, many percent base rule is something totally trivial here, but it isn't.",
                    "label": 0
                },
                {
                    "sent": "Especially for countably infinite classes.",
                    "label": 0
                },
                {
                    "sent": "And then you need the countable additivity.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now come let's come to Laplace's rule.",
                    "label": 1
                },
                {
                    "sent": "You start again with your biased coin.",
                    "label": 0
                },
                {
                    "sent": "Or any experiment which can have two outcomes and with which are independent.",
                    "label": 0
                },
                {
                    "sent": "Alright, you assume to be independent.",
                    "label": 0
                },
                {
                    "sent": "So the probability of one or head will be tighter.",
                    "label": 0
                },
                {
                    "sent": "Data can be any number in the interval zero and one, and this is called ability to process.",
                    "label": 0
                },
                {
                    "sent": "OK, so your hypothesis is that my data is sampled from.",
                    "label": 0
                },
                {
                    "sent": "A biased coin with biased data.",
                    "label": 1
                },
                {
                    "sent": "So we have observed a finite sequence X one to XN.",
                    "label": 0
                },
                {
                    "sent": "And let's assume this sequence is N11 and N00.",
                    "label": 0
                },
                {
                    "sent": "And your sample space.",
                    "label": 0
                },
                {
                    "sent": "You can see that the infinite sequences because you go on and go on is the class of infinite sequences, so that is Omega.",
                    "label": 0
                },
                {
                    "sent": "So now the events.",
                    "label": 0
                },
                {
                    "sent": "Remember, events are subsets of Omega.",
                    "label": 1
                },
                {
                    "sent": "Cause we have observed only the 1st and instances.",
                    "label": 0
                },
                {
                    "sent": "But our sample space is the set of all infinite sequences.",
                    "label": 0
                },
                {
                    "sent": "The events are all infinite sequences.",
                    "label": 1
                },
                {
                    "sent": "Which starts with a certain finite.",
                    "label": 0
                },
                {
                    "sent": "String, so we take all infinite sequences which have X1 to XN equal to the observed thing.",
                    "label": 0
                },
                {
                    "sent": "So this is your basic event.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Your data likelihood is so if observed, probability of observing A1 instead of better.",
                    "label": 0
                },
                {
                    "sent": "So you take it to the power in one and the same folder N 0.",
                    "label": 0
                },
                {
                    "sent": "So there's your likelihood.",
                    "label": 0
                },
                {
                    "sent": "I write it as Peter of X.",
                    "label": 1
                },
                {
                    "sent": "And So what based it already in 1763?",
                    "label": 0
                },
                {
                    "sent": "So the rule is nearly 250 years old.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "He assumed a uniform prior over the theater.",
                    "label": 0
                },
                {
                    "sent": "So our priority set all biases are equally likely.",
                    "label": 0
                },
                {
                    "sent": "And here instead of having the sum because it's a continuous variable, you you have to replace it by integrals.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I will not mention that this is just math.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you compute the evidence, which is the denominator in base rule.",
                    "label": 0
                },
                {
                    "sent": "That is this integral here and you can look it up, which gives you know.",
                    "label": 0
                },
                {
                    "sent": "Here reflection of factorials.",
                    "label": 0
                },
                {
                    "sent": "Which is again hard to interpret as I mentioned, but very important quantity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we really want is the posterior.",
                    "label": 0
                },
                {
                    "sent": "So after I've seen the data, what should we believe?",
                    "label": 0
                },
                {
                    "sent": "And you use now?",
                    "label": 0
                },
                {
                    "sent": "Bayes rule, you plug everything in and you get this function.",
                    "label": 0
                },
                {
                    "sent": "So here's a plot.",
                    "label": 0
                },
                {
                    "sent": "So here is Theta.",
                    "label": 0
                },
                {
                    "sent": "And here is P of Theta given X.",
                    "label": 0
                },
                {
                    "sent": "So I assumed that the number of zeros in numbers ones in the actual case.",
                    "label": 0
                },
                {
                    "sent": "Here the plotted case is equal, so.",
                    "label": 0
                },
                {
                    "sent": "And for an equal 10, for instance, after sleeping five hits and five tails, you posterior distribution looks like that.",
                    "label": 0
                },
                {
                    "sent": "So you have a strong belief that theater is between 0.3 and 0.7, so probably 90% and the more data you have, the more confident you are that it's around 0.5, yes?",
                    "label": 0
                },
                {
                    "sent": "Versus just stretching, I guess that's stretching.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the N = 3 quasi mean.",
                    "label": 0
                },
                {
                    "sent": "They have 1.5 observations head and 1.5 observations tail.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it makes no sense here.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, yes, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, forgot about that.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean this concentration.",
                    "label": 0
                },
                {
                    "sent": "You're all aware of, and so OK, that is based famous rule, nearly 250 years old.",
                    "label": 0
                },
                {
                    "sent": "So in what Laplace did, he said OK, it's nice to have this data, so we have this whole distribution, but the complex object.",
                    "label": 0
                },
                {
                    "sent": "What do we do with it?",
                    "label": 0
                },
                {
                    "sent": "We usually use it for making predictions.",
                    "label": 0
                },
                {
                    "sent": "So what we really want is what is the probability that, for instance, the next observation over the next 10 observations or whatever is 1 given our past observations and there are two ways to compute that.",
                    "label": 0
                },
                {
                    "sent": "One is you can say the probability of X N + 1 given X one up to XN is OK, so we derive the probability of Theta given X12 N. And then we ask what is the probability of X N + 1 given Theta and then we integrate out?",
                    "label": 0
                },
                {
                    "sent": "Tetteh, so that's the posterior.",
                    "label": 0
                },
                {
                    "sent": "And then for this data, the prediction should be this.",
                    "label": 0
                },
                {
                    "sent": "And I mean this is just data.",
                    "label": 0
                },
                {
                    "sent": "But status uncertain, so we average over Theta.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is just I mean in this case the expectation of P of Theta given X1.",
                    "label": 0
                },
                {
                    "sent": "Yes yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK so but there's a much simpler way to calculate that the conditional probability is defined as the probability.",
                    "label": 0
                },
                {
                    "sent": "I mean of a given B is for build A&B, so it's the probability that the beginning was X one to North and it is continued by X N + 1.",
                    "label": 0
                },
                {
                    "sent": "Divided by the probability of X 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "At X12 N. So here you see nicely the evidence.",
                    "label": 0
                },
                {
                    "sent": "This abstract object on the last slide, you can just compute it for.",
                    "label": 0
                },
                {
                    "sent": "For N + 1 observations and divided by N observations and then you will see that all these factorials.",
                    "label": 0
                },
                {
                    "sent": "All these factorials here.",
                    "label": 0
                },
                {
                    "sent": "Cancel out and what remains is just N 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "And 1 + 1 / 2 where N one is the number of.",
                    "label": 0
                },
                {
                    "sent": "Once you have observed.",
                    "label": 0
                },
                {
                    "sent": "So that's a very reasonable rule, becausw.",
                    "label": 0
                },
                {
                    "sent": "First in the long run, this converges this probability one yeah to the.",
                    "label": 0
                },
                {
                    "sent": "To the probability, so you have 1 + 1 and one and two.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter, but the good thing is that in the short run it gives reasonable answers.",
                    "label": 0
                },
                {
                    "sent": "So if you just take the ratio and you have observed only hits, you would predict the probability of one which makes no sense.",
                    "label": 0
                },
                {
                    "sent": "Whereas here if I have observed say nothing, it gives probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "So it's defined and if observed with someone's it gives a large number but not exactly 1.",
                    "label": 0
                },
                {
                    "sent": "So Butler plus did.",
                    "label": 0
                },
                {
                    "sent": "He believed that the sun had risen for 5000 years.",
                    "label": 1
                },
                {
                    "sent": "I think he took this from the Bible.",
                    "label": 0
                },
                {
                    "sent": "So which is about 1.8 million days and it ever always rose.",
                    "label": 0
                },
                {
                    "sent": "So we had 1.8 million divided by.",
                    "label": 0
                },
                {
                    "sent": "So one point 8,000,000 + 1 / 1.8 million +2.",
                    "label": 0
                },
                {
                    "sent": "Look at the probability that the sun will rise tomorrow, so the compliment that the sun will not rise of doomsday is 1 / 2 million roughly.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "At least this answer is not obviously nonsense.",
                    "label": 0
                },
                {
                    "sent": "Like other answers.",
                    "label": 0
                },
                {
                    "sent": "Probably one or undefined, yes.",
                    "label": 0
                },
                {
                    "sent": "Crazy question.",
                    "label": 0
                },
                {
                    "sent": "Automatically question.",
                    "label": 1
                },
                {
                    "sent": "What is the probability of the sun disappearing out of existence before tomorrow?",
                    "label": 0
                },
                {
                    "sent": "Then I can ask another question, what's the probability of the sun being stolen by aliens before tomorrow and all of these questions?",
                    "label": 0
                },
                {
                    "sent": "We had this same number of observations over a sequence of days.",
                    "label": 0
                },
                {
                    "sent": "So all of these questions you have to give that same probability of the answer, but then so be phrased.",
                    "label": 0
                },
                {
                    "sent": "That question is that a whole sequence of different questions that all involve reasons why the sunrise tomorrow you add them all up, you'll get a probability that.",
                    "label": 0
                },
                {
                    "sent": "Now what you get is here.",
                    "label": 0
                },
                {
                    "sent": "What you get is here.",
                    "label": 0
                },
                {
                    "sent": "I mean, these numbers change.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I don't claim that 1 / 2 million is the perfect right answer so.",
                    "label": 0
                },
                {
                    "sent": "But for instance, if you take Jeffreys prior then you have here 1/2 and here one and you get a different answer if you take I mean you get more or less 1/2 of this answer.",
                    "label": 0
                },
                {
                    "sent": "Yes, 1/2 of this answer if you increase your samples page and say OK, the sunken is might not rise because of 10 other reasons.",
                    "label": 0
                },
                {
                    "sent": "Then you probably increase this number by a factor of 10.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can screw it up and this is definitely not.",
                    "label": 0
                },
                {
                    "sent": "You know, you cannot say this is the right answer within a factor of two or something and I will come to that later.",
                    "label": 0
                },
                {
                    "sent": "Even more severe problems because what he did is assumed.",
                    "label": 0
                },
                {
                    "sent": "An IID model, he said.",
                    "label": 0
                },
                {
                    "sent": "You know the rise of the sun every day is independent of the previous day.",
                    "label": 0
                },
                {
                    "sent": "So like flipping a coin, which maybe it's his times was not so bad because they didn't know anything about stars.",
                    "label": 0
                },
                {
                    "sent": "But even then it was pretty absorbed.",
                    "label": 0
                },
                {
                    "sent": "I just want to say that this gives an answer which.",
                    "label": 0
                },
                {
                    "sent": "Is not obviously nonsense.",
                    "label": 0
                },
                {
                    "sent": "If you if you increase your sample space or whatever and you get something which is a factor of 100 larger effective 100 smaller and so on.",
                    "label": 0
                },
                {
                    "sent": "There all answers which seem to make sense, I agree.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the idea is equivalent to an argument of structural consistency overtime, so that if you're if you're going to entertain the notion of alien abduction of the sun, the pictures you've already got in your X1 doing already reflect the probability of the earnings would have adopted the sun in the past.",
                    "label": 0
                },
                {
                    "sent": "OK. Sunrise.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could argue becausw.",
                    "label": 0
                },
                {
                    "sent": "We have never observed an alternative we put should should put all alternatives in one box, but you could argue differently.",
                    "label": 0
                },
                {
                    "sent": "And I mean, if you take the knife uniform prior.",
                    "label": 0
                },
                {
                    "sent": "Then I mean you have a difficult process with uniform prior and it.",
                    "label": 0
                },
                {
                    "sent": "It depends on it and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I will come to that.",
                    "label": 0
                },
                {
                    "sent": "Not as explicit of giving you, you know, a precise answer to this question, But yeah, problem.",
                    "label": 0
                },
                {
                    "sent": "Approach to this would be to say something like if the probability of the sun.",
                    "label": 0
                },
                {
                    "sent": "Arising was substantial enough, less than one.",
                    "label": 0
                },
                {
                    "sent": "Only .9 Xtremely unlikely that I would have written this long.",
                    "label": 0
                },
                {
                    "sent": "The answer is somewhere between one and however far down such that it would be extremely unlikely would have survived for today, but you wouldn't then commit any particular number in there.",
                    "label": 0
                },
                {
                    "sent": "We just say another.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I mean, this is sort of the robust page, not imprecise probability approach, which I'm not really a fan of cause.",
                    "label": 0
                },
                {
                    "sent": "I mean, why do you?",
                    "label": 0
                },
                {
                    "sent": "Why do you think it could be?",
                    "label": 0
                },
                {
                    "sent": "You know larger than?",
                    "label": 0
                },
                {
                    "sent": "0.99 but you're absolutely sure it's not 0.989999.",
                    "label": 0
                },
                {
                    "sent": "So what I don't like is that this that is, intervals have sharp boundaries because the place where you put the boundaries are pretty arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So what you should do you check I should soften this boundary somehow.",
                    "label": 0
                },
                {
                    "sent": "But then what you do is you do a second order probability.",
                    "label": 0
                },
                {
                    "sent": "Over your original probabilities so.",
                    "label": 0
                },
                {
                    "sent": "It depends what question you ask.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you just ask what is the probability that sunrise is tomorrow, I think you should give some number.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if you ask about the distribution of probabilities, you know in some larger space or something.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean then I agree that you could give something which is sort of an analog of an interval of toy would prefer.",
                    "label": 0
                },
                {
                    "sent": "I'm a distribution over it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I mean what you do is you give some yeah confidence interval.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good answer, but I personally don't don't like it too much.",
                    "label": 0
                },
                {
                    "sent": "OK, any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let's have a break.",
                    "label": 0
                }
            ]
        }
    }
}