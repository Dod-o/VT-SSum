{
    "id": "fi7zv55ludsxiy5wo7svpewcplajzvyg",
    "title": "Variance Approximation in Large-Scale Gaussian Markov Random Fields",
    "info": {
        "author": [
            "Dmitry Malioutov, Stochastic Systems Group, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_malioutov_itva/",
    "segmentation": [
        [
            "So thank you very much for inviting me here, and thanks for organizing this very interesting workshop.",
            "So my name is Dimitri and the I'll talk about work that I was doing about a year ago back at MIT.",
            "On the for inspiration.",
            "In large scale Godsmack, random fields in the region it was presented in signal processing community and that hasn't propagated the machine learning yet.",
            "So hopefully it will be interesting to machine learning as well.",
            "And this is joint work with Jason Johnson who is not right now at the Los Alamos Laboratory and Alan will ski at MIT."
        ],
        [
            "So let me give you first some overview what are Gaussian graphical models?",
            "So Gaussian graphical models are multivariate jointly Gaussian models, which also have some sparse structure according to a graph, and they happier in a number of applications, including sensor networks, even collaborative filtering, computer low level computer vision, and remote sensing.",
            "So for example, weather prediction can be posed very large scale, Gaussian graphical model estimation problem.",
            "In addition to recently, some researchers have applied methods which were developed in sort of in the graphical models community to solve.",
            "Problems arising and optimization and so.",
            "For example, the news methods, interior point methods, sequential quadratic programming, so it can be fruitful to view those quadratic problems as instances of graphical models.",
            "Gaussian graphical models."
        ],
        [
            "So in this talk we will talk about inference in Gaussian graphical models, and there are two parts of inference inferring the means and the variance or kind of governance is and the means are somewhat of a of an easier problem, so I will specifically focus on computing the variances.",
            "Um?",
            "So in Gaussian graphical models, it's seems trivial to compute them in.",
            "The variance is because it just reduces the matrix inversion, so the interest comes from really large scale applications.",
            "So for example, in computer vision where your image might be 1000 by 1000, so we have a million nodes in your graphical model, and so the coverage matrix is a million by million.",
            "So if you just.",
            "Open Matlab and type INFJ J matrix which is parsed and will probably crash.",
            "And so.",
            "So in this talk L describe a very simple approach to compute approximate variances, but the surprising thing is that it has nice guarantees on accuracy.",
            "This is a little bit of a.",
            "Rough statement, but the for some models we can compute accurate variances in order of N computations.",
            "Um?",
            "So first we will.",
            "Let me move onto a hotline open so this much of this talk is presented in this paper last year in transactions on signal processing."
        ],
        [
            "So the outline of the talk is as follows.",
            "1st, I'll give more detailed background on Gaussian graphical models.",
            "Set up some notation and the.",
            "Describe the motivation for computing variances and Lastly, so some existing approaches to compute variances.",
            "And then I'll motivate the approach that we proposed the low rank up there in summation, and the 1st I'll describe how to use iterative solvers to compute variances.",
            "Then I'll describe how to play this construction to models with short correlation length and then move onto models with.",
            "Long range correlation, but which have sort of governance is using wavelet construction.",
            "And the then I will describe some very preliminary work.",
            "On the distribute common filtering, which is also in the same framework and may try to make connections to open problems and.",
            "Suggest further directions for research."
        ],
        [
            "So first I'm a brief background on Gaussian graphical models.",
            "Slate is probably."
        ],
        [
            "Very useful for machine learning audience but brief description of water graphical models so graphical models are multivariate statistical models where the joint density splits into product of factors which depends only on small subsets of the variables.",
            "There are many kinds of graphical model representations.",
            "In this talk, I'll talk about pairwise Markov random field factorization, where.",
            "So the joint density splits into local unary terms and the paralyzed terms according to the edges of the graph.",
            "Also, I forgot to mention that the graphical model, so what's the?",
            "What is it?",
            "It's relation to the graph, so the variables, the nodes in the graph correspond to random variables and the edges correspond to sort of interactions between the variables.",
            "And I'll make it more precise for Gaussian random variables."
        ],
        [
            "So Gauss Markov random fields are jointly Gaussian models which are also a graphical model, so they have sparse structure according to a graph and so.",
            "In the typical representation of Gaussian model, which will learn through the 1st course in statistics, is in terms of the mean and the covariance infusing.",
            "Can simple change of notation we can switch to so called information parameters where define J to beef inverse of P and so this so called information matrix and H is the potential vector which is just a.",
            "B inverse recovering steps.",
            "They mean inverse of governance mean and so using this notation then.",
            "Gaussian graphical models looks just like the exponent of a quadratic form, with J being the.",
            "Matrix and H. The unary term.",
            "So the wonderful thing about Gaussian graphical models is that the structure of the graph is immediately apparent in this information matrix J.",
            "If there is an edge missing in the graph, then it appears as an missing entry in the information matrix.",
            "So let me mention.",
            "A quantity called the partial correlation coefficient.",
            "It shows for measures the influence of pairs of nodes, so it's defined as the conditional correlation coefficient between two variables defined on all the other variables being set.",
            "So it's like additional interaction between the variables given.",
            "That you will fully take into account all the other variables.",
            "So it measures conditional dependence and for nodes which do not have an edge between them, there are conditionally independent given the rest of the variables in the graph.",
            "And so that's why this entry is 0."
        ],
        [
            "So now how does this information from construction of Gaussian graphical models relate to the factorization for graphical models?",
            "And it's it's very simple, so recall when I defined the pairwise summer factorization.",
            "So we have a big joint function.",
            "Probably function defined in the vector X, which splits into a local paradise and single term functions, and So what we can do for Gaussian models is basically take the big matrix J sparse matrix and the split it into local 2 by two term center single note terms.",
            "And so that's.",
            "In this slide, in this line and of course you have to zero pad them so that they have proper dimensions.",
            "And to know the local potentials are just the exponents of those small quadratic terms.",
            "Corresponding to the edges and and the nodes and so to give you a concrete example of a Gaussian graphical model, consider the thin membrane model which is often used in simple computer vision or image processing applications.",
            "So here.",
            "There are two terms, so access are the hidden image variables and wise are noisy measurements and in addition you would like to impose a prior that the image is more or less smooth.",
            "Neighbors are likely to have similar values, so we have penalties and deviations of neighbors, and so that's quadratic form and you can after very simple algebra converted into the information form and read out the information matrix and the potential vector."
        ],
        [
            "So inference in gas mark random fields.",
            "So there are two problems in infants computing or their many.",
            "But to the two.",
            "The marginal computing the marginal probabilities and the computing device to make sense for Gaussian graphical models, they both reduced to.",
            "Competing through compliments.",
            "So it's basically the formula for inverse of part of the matrix and you're marginalizing everything out.",
            "And the map estimate this update using the same.",
            "The same formula.",
            "So.",
            "In principle this is trivial.",
            "This is just matrix inversion, but as I said before, for very large scale problems, computing the inverse is is not really feasible solution.",
            "So in this talk, how?",
            "Will do not worry about the means as much and I'll worry about the variances, so just the diagonal of the inverse of the of a positive definite matrix, so that's the numerical problem that I'm approaching."
        ],
        [
            "And to let me also mention briefly very nice analysis tool for Gaussian graphical models, which the so called Vox amps.",
            "And there was another talk earlier in ICM Elf which described it also.",
            "So this is joint work with Jason Johnson.",
            "Also in a few years ago.",
            "I'm.",
            "And just provide some very visual interpretation of what Gaussian inference responds to.",
            "It has been used in the past to analyze several through approximate inference algorithms.",
            "So if you take the second inference response to sort matrix inversion to compute the covering, so you need to invert the information matrix.",
            "And you can write the information matrix J.",
            "So let's say it's normalized to have unit diagonal.",
            "Then you can split it into the diagonal part, the identity and off diagonal part, which will denote by R which actually contains the partial correlation coefficients.",
            "Then using the power series for the matrix inverse, can expressed as the following sum.",
            "And which converges if the spectral radius so far is less than one.",
            "And so it may seem.",
            "Learn expression for the means where you also have to multiply by the potential vector and now so are is a sparse matrix.",
            "And if you take powers of our, then essentially what you're doing is you're computing sums over walks in the graph, so just.",
            "A walk is just a sequence of steps connected by edges.",
            "So for example 123431 would be a walk and the weight of such a work would be just the product of the partial correlation coefficients in the work and so.",
            "It's easy to show that the variances are nothing but the sums overall walks that so the variance at node I is nothing but the sum over of weights or walks that start node.",
            "I wander around in the graph for little bit and return to die, and similarly for the means.",
            "It's also works on which starts anywhere at the graph and wanders around the graph and ends at node I where the input is weighted by the potential vector had the starting node.",
            "And so we have a very simple interpretation for inferencing Gaussian models as computing works, and it has been used to analyze convergence and accuracy of Gaussian loopy belief propagation and also some other algorithms for embedded trees.",
            "And I think there is a lot of potential and it's sheds a lot of light on the structure of what.",
            "Go, some models are.",
            "OK, so this is."
        ],
        [
            "To the less light of the background, so I'll move on to the body of the talk."
        ],
        [
            "So first slide this.",
            "Why do we even need to compute the variance is why it means are not enough?",
            "So the first reason is that the variances give you reliability information.",
            "How confident you are in your means.",
            "So for unbiased estimators, when it's equal to MC, so the mean squared error.",
            "Another reason is that when you're learning the models and part of learning, the model is actually doing inference.",
            "So for example in.",
            "It is proportional fitting cure.",
            "You start with some set of parameters.",
            "You compute the moments that are implied by those parameters and you compare it to the sample moments from your data into learning.",
            "Moment matching for Gaussian models.",
            "Computing means and variances and matching them to the samples.",
            "Another motivation is that the variances can be used to detect and correct model mismatch.",
            "So for example, if you have a smoothness prior on an image and suppose you have a structural break in the image, you have some sort of terrain and there's amount, and then you can see that the.",
            "The residuals in the image are much greater than the variances that you are predicting, and so that can be used to change the model A little bit to introduce maybe vicar interaction right around the edge.",
            "Um?",
            "And that's also related to iteratively weighted least squares.",
            "And when you are trying to solve.",
            "Edge preserving cost function, such as total variation by using Gaussian approximation step.",
            "So when are useful in for inactive sensing scenarios where you're trying to make a few new measurements to maximally reduce uncertainty and.",
            "So there is nice work by Mathias Seger on variation by Bayesian inference in sparse linear models were part of the computation is computing the marginal variance of the model, and I think it's one of the.",
            "Challenging parts, how to compute them efficiently?"
        ],
        [
            "So let me mention some existing methods, how to compute variances, so a simple one would be just too.",
            "Given your sparse graphical model 2 front Gibbs sampling, get the sample, get a few samples and then just take an average and you get the sample sample variance at each node and the trouble is that deep sampling can be very slow for large scale models and another reason is that you need quite a lot of samples to get good variance of pretty samples here, just the standard deviation by sqrt 10.",
            "I'm looking belief propagation gives variances for three structured models.",
            "It's actually it's exactly.",
            "It gives exact variances, but for loopy graphs on occasion can give accurate variance and sometimes can be very bad variances and there are.",
            "It's hard to tell if you worry how good.",
            "They will be so landsource methods which Mattice have used extensively in his work.",
            "They roughly they work by.",
            "Taking Kylo Ren Proximation to the various matrix where pure focusing on the principle subspaces and so they work very well when the spectrum exhibits fast decay.",
            "But for models which have more flat spectrum and this approximation may not be very accurate so.",
            "There are additional algorithms which also provide some sort of indication of variances, but there is something called embedded trees which.",
            "Only works for models which are near trees, which are trees plus some additional edges, and their complexity depends on the number of cottages that you have to remove to make the model tree.",
            "There's also something called LaGrange relaxation methods, which split models with loops into several tree structured models and then show them independently, but then enforce the condition that.",
            "The solutions should be equal.",
            "And they give upper bounds on the variances, but sometimes can be rather loose.",
            "So.",
            "And let me motivate.",
            "Another approach how to compute the."
        ],
        [
            "This is using just iterative solvers.",
            "So for the purpose of computing the means in a sparse graphical model, you just need to solve linear system.",
            "JMU is equal to H where you're trying to compute the mean given the potential electric information matrix and the J is very very sparse, so you can use a variety of iterative solvers such as congregants or precondition gradients to compute the mean sufficiently even for large scale models and for some models.",
            "The whole operation can be done in order of N steps, so just matrix multiplication by a sparse matrix.",
            "Where the number of nonzero components for each element is roughly constant is a of an operation, and then if you don't do too many of those multiplications.",
            "If it's roughly constant, test the model growth then.",
            "For example, we have limited correlation length and you can compute the whole mean in order of N steps.",
            "So now this can be used also to compute the variances in the following way so.",
            "If on the right hand side, instead of your potential vector H, you just plug in.",
            "Um?",
            "So it depends on the application, but it can be very large and so that's why using good preconditioners sort of is often necessary in applications to achieve both convergence.",
            "Um?",
            "Yes, so instead of the potential vector on the right hand side, we if you plug in the Earth standard basis vector, which is just all zeros except one in the application.",
            "Then by solving the system we get the right column of the coverage matrix.",
            "So just by solving the system we can get one variance, but the trouble is that in order to get all the variances then we need to do it and times is on the order of millions.",
            "So it's not really very suitable approach.",
            "So basically what we're doing is selling for inverse of JJP is equal to identity.",
            "If you want to do it all together.",
            "I'm.",
            "And so here is a."
        ],
        [
            "Motivation how for models with short correlation you can actually do.",
            "Many of those solves in parallel.",
            "At the same time, So what they described on the previous slide is in the first row.",
            "So you take a standard basis vector.",
            "This spike application.",
            "I do solve the linear system G. Jane versus H and you get the covariance of the note.",
            "I missed the rest of the nodes and for models with short correlation length then.",
            "You get very fast decay right around high and most of the solution is 0, so it looks like you're wasting computation, so you're just doing computation, which is more or less local, but the cost of a blank.",
            "The linear solve the whole graphical model, and so the idea.",
            "The very simple version of this idea is to basically do many of them in parallel, as long as they don't interfere too much, so do.",
            "Instead of taking the high standard basis vector, take a number of them which are separated by roughly the correlation length and solve them in parallel.",
            "And so you get the variances right near the peaks and you get some small interaction from the nearby nearby terms, and so this way you get.",
            "Instead of 1 variance you get 6 variance in this picture.",
            "So additional ingredient is that in order to have destructive interference between the errors, you can add random science.",
            "So instead of just doing spike train of plus ones, you also multiply them by minus ones occasionally, and so on average.",
            "You'll get your cancellation, and that's a very nice property which I will mention later also.",
            "And So what does this really do and what can be said?",
            "More generally, apart from models from the short correlation so?"
        ],
        [
            "So what we're trying to do is solve the system.",
            "JP is equal to Y, but.",
            "It's the computation intensive.",
            "Since we were replacing identity by a low rank matrix where B is a whole matrix, it has much more.",
            "Pro stem columns.",
            "And there is no meaningful sense in which we can approximate the identity by a low rank matrix.",
            "Obviously identity as all singular values equal to 1.",
            "So this equation is meaningless.",
            "But what we're trying to do is to make the identity close to this BB transpose with respect to a graph.",
            "I'll explain what this means, so suppose so B is a matrix which we have freedom of choosing, and we want to choose it such that the resulting variances are accurate.",
            "So suppose that the which would be such that the rows are unit norm.",
            "Then if you just solve the system, Jeep is equal to BB transpose.",
            "Then what you get is.",
            "The true variance BIA which you're after plus some interference terms.",
            "And the interference terms, our relations with no dye and OJ and the inner product between being BI and BJ.",
            "So in particular the matrix below a certain normal.",
            "Then all those inner products would be 0 and it would get the correct answer.",
            "But since we are restricting be to be adult matrix then.",
            "Some of those guys have to be non zero and the question is how to design this be such that.",
            "Those terms are.",
            "I'm sorry, cancel out in a useful way.",
            "So one simple example is again for models with short correlation length, where if you take 2 nodes which are far apart.",
            "So for example in spatial model then the correlation between them will be very small, close to zero and so there we don't have to care about what happens to the inner product.",
            "So there are these terms automatically 0, so we only have to design B such that this inner product is close to 0 for nearby terms.",
            "And so there is a."
        ],
        [
            "A simple construction.",
            "How to achieve this?",
            "There's no real design, it's just.",
            "Immediately have it.",
            "So let me describe it and it will become clear so.",
            "We split all the nodes in the graph according to colors such that each color is separated by a certain length.",
            "For example, nodes of red color have.",
            "Four steps between them.",
            "And the.",
            "So at each node with the same color, you put the spikes.",
            "So we have several right hand sides.",
            "Which we are trying to solve.",
            "For each of them looks like a sort of field with spikes and together they covered the whole matrix, but individually they are well separated beyond the correlation length.",
            "Um?",
            "And then actually let me draw a picture for one day, which might be very simple.",
            "So let's say you have a chain model.",
            "So you just partition.",
            "So this would be 1 right inside and this would be.",
            "The other right hand side and the distance between any two spikes is.",
            "Is at least two, and so it's a similar similar construction for the square grid where you also should do it in checkerboard pattern too.",
            "Increase the distance.",
            "Now this can also be used for irregular graphs, but there you have to do some sort of a graph coloring and you don't have to solve it exactly, but approximately how long will do just fine?"
        ],
        [
            "And so the whole algorithm consists of constructing, constructing such a right hand side.",
            "Solving it, you get some sort of response and feeding out the points corresponding to the variances, and you get the big subset of the variances already, and then you solve it for another right hand side where those spikes are shifted, and this way you reconstruct all variances.",
            "So the algorithm is very very simple in terms of complexity.",
            "It depends on the number of right hand side that you're solving for, so it depends on how compactly can you place the spikes without adding too much interference."
        ],
        [
            "But the interesting thing is that it has very nice guarantees.",
            "So first of all, since we're adding random science, then on average tend biased.",
            "So the expectation of the our estimate of the variance is actually is the variance itself.",
            "So if you.",
            "Take one set of science, solve it, and then take another set of science and average, then eventually you'll get the correct variances.",
            "We can also, if you have some sort of decay on the correlations, and we can bound there in the variances, and as the separation between sort of the spikes increases, then converges to correct variances."
        ],
        [
            "And let me give some examples.",
            "So here we have a 1 dimensional chain model with short correlation length and the length is 256 and we.",
            "Sure, put the.",
            "Makes 16 apart and the so we get almost the correct variances, unfortunately, so the limitation is that for models with long range correlation there is.",
            "If you put this by closely, then the variance estimates are very noisy.",
            "So that's actually an additional feature of the approaches that the.",
            "If you expect the variances to be more or less most field and this gives you an indication of how inaccurate or variances are so there you can see that there are variations.",
            "Are very small for us here.",
            "They are much larger so it gives you confidence in your approximate computation.",
            "So this correlation be quiet, BK is exactly related to these rulings of either chicken soup for the simple model, yes, yes.",
            "Well, so you could do it in two ways.",
            "Either you can set the correlation length to be a pretty large and then just do oneself and then immediately get in one sort of solution.",
            "You get very occurrences.",
            "Another approach would be to space them closer apart and then you can get fairly noisy estimate and then you can repeat it for another realization of the random science and you get another.",
            "Set the variances and you can average together and you get do something coverage.",
            "But for models with."
        ],
        [
            "Still long range correlations, something?",
            "More can be done and basically the idea is that if you apply wavelets to process with long range correlations that they tend to do correlate them, so it's further off motivating idea.",
            "Although the details are slightly different than the standard, the wavelet.",
            "The prosecutor.",
            "And so the starting point is very similar.",
            "So if you want to solve the system J times, P is equal to identity and instead of identity we can replace it by the.",
            "The Matrix the wave.",
            "Transform matrix discretely transform matrix.",
            "So this is what we have done in the short range collation case.",
            "You just take the identity matrix and Ellis together, served by adding several columns together and we do the same for wavelets.",
            "So we had several columns together such that.",
            "Interference links between nearby at least terms is roughly equal to the correlation length of the process in the available domain.",
            "So it's a very very simple construction, and the.",
            "Now to get the variances you again solve it for this alias table matrix and the post multiplied by its transpose to get the diagonal terms."
        ],
        [
            "I'm.",
            "So here's some motivation for.",
            "And the difference between the two approaches.",
            "So if you have long range correlations, and if you space daily storms fairly close, then you get significant fear interference.",
            "But if you first.",
            "If instead of using the standard basis used the wavelet basis, then essentially what you're doing is you're convolving the correlation function is disabled, and so we get the much.",
            "More localized response and the correlation becomes much shorter and the interference comes from the interaction of this blue curve with the list replicas of the wavelets.",
            "And here it's you can pack a lot more of those spikes in the same area.",
            "And the this approach is also unbiased.",
            "And also we can control the variance.",
            "And there's some analysis in the paper."
        ],
        [
            "And so let me give some examples.",
            "So here is the chain model with links to 156 where we use just 2028 columns instead of 256.",
            "First we use the.",
            "Simple short correlation approach reviews.",
            "Just a list spikes and get somewhat useful.",
            "Very assessments, but they are very noisy and.",
            "But when we replace this construction, by the way, based construction, then you get very accurate, very system.",
            "It's using just 28 linear solves instead of 256.",
            "And so here is the 2 dimensional example.",
            "So this is a, I think a simulation of a template model and the.",
            "I forgot to put the number of right hand sides, but I think it's an order of a couple 100.",
            "Instead of 64,000.",
            "So you get very accurate variance estimates."
        ],
        [
            "So here is a fairly large scale example.",
            "I'm not sure if you can see.",
            "It's quite dark, but this is the picture of the Pacific Ocean and here is Australia.",
            "This is New Zealand and this is part of California somewhere on the top.",
            "I'm.",
            "So the problem is, given satellite measurements, sparse irregular noisy measurements to compute the variance variance of the estimates of the surface height of the Pacific Ocean.",
            "The problem is a very large scale problem.",
            "The number of nodes is a million 1020, four 1024.",
            "The model has.",
            "I think on this scale it has a correlation length of about 60, so it's short but not that short for 60 by 60 is already several.",
            "Thousands would have to use that many linear solves.",
            "So if you use just a couple of scales with the wavelet transform, then we can reduce the number of linear.",
            "So I think to 450, which is still large number.",
            "But the you can compute the variance in the whole field in the 150 linear solves."
        ],
        [
            "So there's another ingredient in how they actually do the linear solves so.",
            "For that purpose we have to use efficient preconditioners.",
            "There are very many approaches to how to do.",
            "Big large sparse systems.",
            "Including multigrid and even belief propagation can be used to give exact means, but for this work we focused on.",
            "A simple class of tree structure preconditioners so they'll describe two particular variations.",
            "So one is called embedded trees where.",
            "Essentially, split the matrix into a tree structured component and the remaining remaining terms.",
            "So J the information matrix is placed into the edges which belong to a tree and all the remaining pages.",
            "Now you can use.",
            "A simple iterative algorithm.",
            "So you split.",
            "It's just a presentation with conditioner.",
            "So the mean at time N plus one is equal to three structured universe times.",
            "The rest of the terms, the cottages multiplying the previous mean plus the potential vector, and so the interesting thing is that the.",
            "JT universe, it can be computed by three structured belief propagation in order event, so it's it's a.",
            "Very fast computation.",
            "And so this this kind of preconditioners are very useful for sparse graphical models due to the existence of efficient solvers for tree structure models.",
            "So another idea.",
            "So the first preconditioner it is guaranteed to converge for so called voxel models for a subclass of model.",
            "But on occasion it might it may diverge and so another idea is to use.",
            "Block or distant, where your blocks are tree structured, so simple example would be just to use some sort of big chains.",
            "Chains which extent across the whole graph.",
            "So here you can do inference or computing versus apply the inverse operator in order of N steps.",
            "So you fix variables which are not in the block you apply this fast restructure computation, and then you do.",
            "; Block or dissent?",
            "A nice idea for the converge fast is to make those blocks overlapping, and this has a nice motivation using box I'm Alesis you can look at what works.",
            "Does this preconditioner capture and so if you have blocks which are non overlapping then you're always missing the short walk which goes from unblocked another, but if you make the blocks significantly overlapping then you capture capture much longer box and the shortest folks work that is missing after a few iterations is has already very nontrivial length."
        ],
        [
            "So this is fairly preliminary work, but let me also mention so.",
            "In some problems, the structure is somewhat more complex, so instead of justice sparse information matrix, you may have a sparse plus low rank structure.",
            "Don't mention a couple of applications where this is the case.",
            "In the so there you can also use some sort of preconditioners by various splitting of the information matrix into tree structure terms or blocks and.",
            "Hun.",
            "Another possibility is actually to use belief propagation, but instead of using the paradise version, you can use the factor graph version of qualification.",
            "So for Gaussian models.",
            "In terms of representation, there is no difference between the pairwise model and the factor graph structured version 'cause all the models are inherently paralyzed.",
            "There are determined by the edges and by the nodes, but the factor graph representations actually useful for Bill Ification, because then you're operating with larger blocks and the.",
            "The number of loops can be reduced by going through larger blocks.",
            "And this approach is also related to a paper by Montanari Parker answer from relation for multiuser detection, where he does some very interesting tricks and converting the.",
            "So degenerate information introduced into complex domain.",
            "If I have some time, I can describe this in a bit more detail."
        ],
        [
            "So let me give an example of a problem where there might be sparse plus low rank structure and this is the problem of.",
            "Dunst estimation.",
            "From gravity measurements.",
            "So the problem is as follows.",
            "So you have a slice of the Earth vertical slide and you.",
            "You have measurements, gravity measurements to the current vertical gravity at the surface.",
            "Let's say you assume a thin plate Model 4.",
            "For the density at each node, each node corresponds to the mass right around that location, and the gravity measurements.",
            "They couple all.",
            "The whole the whole grid, so essentially.",
            "So the gravity measurements are linear in the mass and so you have a problem which has sparse prior structure but the measurement structure couple all of the nodes.",
            "You have a sparse plus low rank and a matrix is according to the measurements and so here we applied.",
            "I think the preconditioning method to compute the variance is into.",
            "I'm sorry I didn't put any of the sizes, but I think the field is.",
            "Is quite small.",
            "It's 100 by 100.",
            "Perhaps in the I think it uses something like.",
            "A couple 100 downsides, so it's still applicable for.",
            "It's basically the example to demonstrate that there are some other structures which can also be handled in this framework."
        ],
        [
            "And this is a very primitive work, so it's.",
            "Approximately distributed filtering and linear dynamical systems.",
            "They'll just have one slide on it, but the only way to put them how this can be useful.",
            "So basically, the idea that you have some sort of dynamical system which is on a chain and you have some measurements and you want to.",
            "Sure.",
            "Get the estimates as time goes on, and so the structure of the information matrix is low diagonal, so the interesting part is that the each node itself corresponds to a large spatially distributed field which.",
            "So each of the top notes itself is A is a big mark my calendar field.",
            "So if you just do plain common filtering then you have to pass the operations from the full covariance matrix for the field and that may not even be feasible, and so the idea is to represent.",
            "Those medicine information form instead to keep an approximate representation after each update.",
            "The so and update the prediction step from time T + 1 to T is just a social component separation.",
            "And if you have initially a sparse plus low rank representation for this inverse for the marginal estimate that empty, then.",
            "Should the separation preserves this parcel rank structure at the next step, so this is just a suggestion for further research basically, but.",
            "There are questions of stability of all the errors with accumulate and so forth, but.",
            "So you decide to get it in, just not to be confined to play."
        ],
        [
            "Microphone built.",
            "And so this is coming to the end of my talk.",
            "Oh, how my doing time should tell Mr.",
            "Finished.",
            "So as a summary, we have developed a very simple approach which has for computing marginal variances in large scale cosmic random field problems.",
            "Um?",
            "By using random sign flips, this method gives unbiased variance estimates.",
            "And we can also put bounds on the errors if we assume some nice correlation decay.",
            "Of the model.",
            "And we have proposed construction for a couple of.",
            "Scenarios for a short correlation like scenario and for models with us most long range dependence and also if I suggested an extension to models with sparse clustering structure.",
            "So in terms of relevance to."
        ],
        [
            "Machine learning at large, so this is basically a method for computing the diagonal of the inverse of a matrix, where it's easy to.",
            "Do linear solving the matrix so if you can do linear so efficiently, then you can compute the variance as well so.",
            "In particular, it could be relevant for operations in Gaussian processes, so Gaussian processes are basically another presentation of.",
            "Gentle Goshen large scale Gentry.",
            "Gaussian models were instead of specifying the inverse governance matrix, you're explicitly specifying the sort of correlation structure.",
            "Um?",
            "So some other directions for further work or using so-called diffusion wavelets for regular graphs.",
            "And the.",
            "Using expressions in other bases, so for example, curvelets edges, since the favourites and the.",
            "Perhaps similarities can be used to approximate the log determinant of the information matrix, but this I haven't done too much work on.",
            "OK, I think this is the talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you very much for inviting me here, and thanks for organizing this very interesting workshop.",
                    "label": 0
                },
                {
                    "sent": "So my name is Dimitri and the I'll talk about work that I was doing about a year ago back at MIT.",
                    "label": 0
                },
                {
                    "sent": "On the for inspiration.",
                    "label": 0
                },
                {
                    "sent": "In large scale Godsmack, random fields in the region it was presented in signal processing community and that hasn't propagated the machine learning yet.",
                    "label": 0
                },
                {
                    "sent": "So hopefully it will be interesting to machine learning as well.",
                    "label": 0
                },
                {
                    "sent": "And this is joint work with Jason Johnson who is not right now at the Los Alamos Laboratory and Alan will ski at MIT.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give you first some overview what are Gaussian graphical models?",
                    "label": 0
                },
                {
                    "sent": "So Gaussian graphical models are multivariate jointly Gaussian models, which also have some sparse structure according to a graph, and they happier in a number of applications, including sensor networks, even collaborative filtering, computer low level computer vision, and remote sensing.",
                    "label": 1
                },
                {
                    "sent": "So for example, weather prediction can be posed very large scale, Gaussian graphical model estimation problem.",
                    "label": 0
                },
                {
                    "sent": "In addition to recently, some researchers have applied methods which were developed in sort of in the graphical models community to solve.",
                    "label": 0
                },
                {
                    "sent": "Problems arising and optimization and so.",
                    "label": 0
                },
                {
                    "sent": "For example, the news methods, interior point methods, sequential quadratic programming, so it can be fruitful to view those quadratic problems as instances of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Gaussian graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk we will talk about inference in Gaussian graphical models, and there are two parts of inference inferring the means and the variance or kind of governance is and the means are somewhat of a of an easier problem, so I will specifically focus on computing the variances.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "So in Gaussian graphical models, it's seems trivial to compute them in.",
                    "label": 0
                },
                {
                    "sent": "The variance is because it just reduces the matrix inversion, so the interest comes from really large scale applications.",
                    "label": 0
                },
                {
                    "sent": "So for example, in computer vision where your image might be 1000 by 1000, so we have a million nodes in your graphical model, and so the coverage matrix is a million by million.",
                    "label": 0
                },
                {
                    "sent": "So if you just.",
                    "label": 0
                },
                {
                    "sent": "Open Matlab and type INFJ J matrix which is parsed and will probably crash.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So in this talk L describe a very simple approach to compute approximate variances, but the surprising thing is that it has nice guarantees on accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit of a.",
                    "label": 1
                },
                {
                    "sent": "Rough statement, but the for some models we can compute accurate variances in order of N computations.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So first we will.",
                    "label": 0
                },
                {
                    "sent": "Let me move onto a hotline open so this much of this talk is presented in this paper last year in transactions on signal processing.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of the talk is as follows.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll give more detailed background on Gaussian graphical models.",
                    "label": 1
                },
                {
                    "sent": "Set up some notation and the.",
                    "label": 0
                },
                {
                    "sent": "Describe the motivation for computing variances and Lastly, so some existing approaches to compute variances.",
                    "label": 1
                },
                {
                    "sent": "And then I'll motivate the approach that we proposed the low rank up there in summation, and the 1st I'll describe how to use iterative solvers to compute variances.",
                    "label": 1
                },
                {
                    "sent": "Then I'll describe how to play this construction to models with short correlation length and then move onto models with.",
                    "label": 0
                },
                {
                    "sent": "Long range correlation, but which have sort of governance is using wavelet construction.",
                    "label": 0
                },
                {
                    "sent": "And the then I will describe some very preliminary work.",
                    "label": 0
                },
                {
                    "sent": "On the distribute common filtering, which is also in the same framework and may try to make connections to open problems and.",
                    "label": 0
                },
                {
                    "sent": "Suggest further directions for research.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'm a brief background on Gaussian graphical models.",
                    "label": 0
                },
                {
                    "sent": "Slate is probably.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very useful for machine learning audience but brief description of water graphical models so graphical models are multivariate statistical models where the joint density splits into product of factors which depends only on small subsets of the variables.",
                    "label": 1
                },
                {
                    "sent": "There are many kinds of graphical model representations.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'll talk about pairwise Markov random field factorization, where.",
                    "label": 0
                },
                {
                    "sent": "So the joint density splits into local unary terms and the paralyzed terms according to the edges of the graph.",
                    "label": 0
                },
                {
                    "sent": "Also, I forgot to mention that the graphical model, so what's the?",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 1
                },
                {
                    "sent": "It's relation to the graph, so the variables, the nodes in the graph correspond to random variables and the edges correspond to sort of interactions between the variables.",
                    "label": 1
                },
                {
                    "sent": "And I'll make it more precise for Gaussian random variables.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Gauss Markov random fields are jointly Gaussian models which are also a graphical model, so they have sparse structure according to a graph and so.",
                    "label": 1
                },
                {
                    "sent": "In the typical representation of Gaussian model, which will learn through the 1st course in statistics, is in terms of the mean and the covariance infusing.",
                    "label": 0
                },
                {
                    "sent": "Can simple change of notation we can switch to so called information parameters where define J to beef inverse of P and so this so called information matrix and H is the potential vector which is just a.",
                    "label": 0
                },
                {
                    "sent": "B inverse recovering steps.",
                    "label": 0
                },
                {
                    "sent": "They mean inverse of governance mean and so using this notation then.",
                    "label": 1
                },
                {
                    "sent": "Gaussian graphical models looks just like the exponent of a quadratic form, with J being the.",
                    "label": 0
                },
                {
                    "sent": "Matrix and H. The unary term.",
                    "label": 0
                },
                {
                    "sent": "So the wonderful thing about Gaussian graphical models is that the structure of the graph is immediately apparent in this information matrix J.",
                    "label": 0
                },
                {
                    "sent": "If there is an edge missing in the graph, then it appears as an missing entry in the information matrix.",
                    "label": 0
                },
                {
                    "sent": "So let me mention.",
                    "label": 1
                },
                {
                    "sent": "A quantity called the partial correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "It shows for measures the influence of pairs of nodes, so it's defined as the conditional correlation coefficient between two variables defined on all the other variables being set.",
                    "label": 0
                },
                {
                    "sent": "So it's like additional interaction between the variables given.",
                    "label": 0
                },
                {
                    "sent": "That you will fully take into account all the other variables.",
                    "label": 0
                },
                {
                    "sent": "So it measures conditional dependence and for nodes which do not have an edge between them, there are conditionally independent given the rest of the variables in the graph.",
                    "label": 0
                },
                {
                    "sent": "And so that's why this entry is 0.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now how does this information from construction of Gaussian graphical models relate to the factorization for graphical models?",
                    "label": 0
                },
                {
                    "sent": "And it's it's very simple, so recall when I defined the pairwise summer factorization.",
                    "label": 0
                },
                {
                    "sent": "So we have a big joint function.",
                    "label": 0
                },
                {
                    "sent": "Probably function defined in the vector X, which splits into a local paradise and single term functions, and So what we can do for Gaussian models is basically take the big matrix J sparse matrix and the split it into local 2 by two term center single note terms.",
                    "label": 0
                },
                {
                    "sent": "And so that's.",
                    "label": 0
                },
                {
                    "sent": "In this slide, in this line and of course you have to zero pad them so that they have proper dimensions.",
                    "label": 0
                },
                {
                    "sent": "And to know the local potentials are just the exponents of those small quadratic terms.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to the edges and and the nodes and so to give you a concrete example of a Gaussian graphical model, consider the thin membrane model which is often used in simple computer vision or image processing applications.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "There are two terms, so access are the hidden image variables and wise are noisy measurements and in addition you would like to impose a prior that the image is more or less smooth.",
                    "label": 0
                },
                {
                    "sent": "Neighbors are likely to have similar values, so we have penalties and deviations of neighbors, and so that's quadratic form and you can after very simple algebra converted into the information form and read out the information matrix and the potential vector.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So inference in gas mark random fields.",
                    "label": 1
                },
                {
                    "sent": "So there are two problems in infants computing or their many.",
                    "label": 0
                },
                {
                    "sent": "But to the two.",
                    "label": 1
                },
                {
                    "sent": "The marginal computing the marginal probabilities and the computing device to make sense for Gaussian graphical models, they both reduced to.",
                    "label": 0
                },
                {
                    "sent": "Competing through compliments.",
                    "label": 0
                },
                {
                    "sent": "So it's basically the formula for inverse of part of the matrix and you're marginalizing everything out.",
                    "label": 1
                },
                {
                    "sent": "And the map estimate this update using the same.",
                    "label": 0
                },
                {
                    "sent": "The same formula.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In principle this is trivial.",
                    "label": 0
                },
                {
                    "sent": "This is just matrix inversion, but as I said before, for very large scale problems, computing the inverse is is not really feasible solution.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, how?",
                    "label": 0
                },
                {
                    "sent": "Will do not worry about the means as much and I'll worry about the variances, so just the diagonal of the inverse of the of a positive definite matrix, so that's the numerical problem that I'm approaching.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to let me also mention briefly very nice analysis tool for Gaussian graphical models, which the so called Vox amps.",
                    "label": 0
                },
                {
                    "sent": "And there was another talk earlier in ICM Elf which described it also.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Jason Johnson.",
                    "label": 0
                },
                {
                    "sent": "Also in a few years ago.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And just provide some very visual interpretation of what Gaussian inference responds to.",
                    "label": 0
                },
                {
                    "sent": "It has been used in the past to analyze several through approximate inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "So if you take the second inference response to sort matrix inversion to compute the covering, so you need to invert the information matrix.",
                    "label": 0
                },
                {
                    "sent": "And you can write the information matrix J.",
                    "label": 0
                },
                {
                    "sent": "So let's say it's normalized to have unit diagonal.",
                    "label": 0
                },
                {
                    "sent": "Then you can split it into the diagonal part, the identity and off diagonal part, which will denote by R which actually contains the partial correlation coefficients.",
                    "label": 0
                },
                {
                    "sent": "Then using the power series for the matrix inverse, can expressed as the following sum.",
                    "label": 0
                },
                {
                    "sent": "And which converges if the spectral radius so far is less than one.",
                    "label": 0
                },
                {
                    "sent": "And so it may seem.",
                    "label": 0
                },
                {
                    "sent": "Learn expression for the means where you also have to multiply by the potential vector and now so are is a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "And if you take powers of our, then essentially what you're doing is you're computing sums over walks in the graph, so just.",
                    "label": 0
                },
                {
                    "sent": "A walk is just a sequence of steps connected by edges.",
                    "label": 0
                },
                {
                    "sent": "So for example 123431 would be a walk and the weight of such a work would be just the product of the partial correlation coefficients in the work and so.",
                    "label": 0
                },
                {
                    "sent": "It's easy to show that the variances are nothing but the sums overall walks that so the variance at node I is nothing but the sum over of weights or walks that start node.",
                    "label": 0
                },
                {
                    "sent": "I wander around in the graph for little bit and return to die, and similarly for the means.",
                    "label": 0
                },
                {
                    "sent": "It's also works on which starts anywhere at the graph and wanders around the graph and ends at node I where the input is weighted by the potential vector had the starting node.",
                    "label": 0
                },
                {
                    "sent": "And so we have a very simple interpretation for inferencing Gaussian models as computing works, and it has been used to analyze convergence and accuracy of Gaussian loopy belief propagation and also some other algorithms for embedded trees.",
                    "label": 0
                },
                {
                    "sent": "And I think there is a lot of potential and it's sheds a lot of light on the structure of what.",
                    "label": 0
                },
                {
                    "sent": "Go, some models are.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the less light of the background, so I'll move on to the body of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first slide this.",
                    "label": 0
                },
                {
                    "sent": "Why do we even need to compute the variance is why it means are not enough?",
                    "label": 0
                },
                {
                    "sent": "So the first reason is that the variances give you reliability information.",
                    "label": 0
                },
                {
                    "sent": "How confident you are in your means.",
                    "label": 0
                },
                {
                    "sent": "So for unbiased estimators, when it's equal to MC, so the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "Another reason is that when you're learning the models and part of learning, the model is actually doing inference.",
                    "label": 0
                },
                {
                    "sent": "So for example in.",
                    "label": 0
                },
                {
                    "sent": "It is proportional fitting cure.",
                    "label": 1
                },
                {
                    "sent": "You start with some set of parameters.",
                    "label": 0
                },
                {
                    "sent": "You compute the moments that are implied by those parameters and you compare it to the sample moments from your data into learning.",
                    "label": 0
                },
                {
                    "sent": "Moment matching for Gaussian models.",
                    "label": 1
                },
                {
                    "sent": "Computing means and variances and matching them to the samples.",
                    "label": 0
                },
                {
                    "sent": "Another motivation is that the variances can be used to detect and correct model mismatch.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you have a smoothness prior on an image and suppose you have a structural break in the image, you have some sort of terrain and there's amount, and then you can see that the.",
                    "label": 0
                },
                {
                    "sent": "The residuals in the image are much greater than the variances that you are predicting, and so that can be used to change the model A little bit to introduce maybe vicar interaction right around the edge.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And that's also related to iteratively weighted least squares.",
                    "label": 0
                },
                {
                    "sent": "And when you are trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Edge preserving cost function, such as total variation by using Gaussian approximation step.",
                    "label": 1
                },
                {
                    "sent": "So when are useful in for inactive sensing scenarios where you're trying to make a few new measurements to maximally reduce uncertainty and.",
                    "label": 1
                },
                {
                    "sent": "So there is nice work by Mathias Seger on variation by Bayesian inference in sparse linear models were part of the computation is computing the marginal variance of the model, and I think it's one of the.",
                    "label": 0
                },
                {
                    "sent": "Challenging parts, how to compute them efficiently?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me mention some existing methods, how to compute variances, so a simple one would be just too.",
                    "label": 1
                },
                {
                    "sent": "Given your sparse graphical model 2 front Gibbs sampling, get the sample, get a few samples and then just take an average and you get the sample sample variance at each node and the trouble is that deep sampling can be very slow for large scale models and another reason is that you need quite a lot of samples to get good variance of pretty samples here, just the standard deviation by sqrt 10.",
                    "label": 0
                },
                {
                    "sent": "I'm looking belief propagation gives variances for three structured models.",
                    "label": 1
                },
                {
                    "sent": "It's actually it's exactly.",
                    "label": 0
                },
                {
                    "sent": "It gives exact variances, but for loopy graphs on occasion can give accurate variance and sometimes can be very bad variances and there are.",
                    "label": 0
                },
                {
                    "sent": "It's hard to tell if you worry how good.",
                    "label": 0
                },
                {
                    "sent": "They will be so landsource methods which Mattice have used extensively in his work.",
                    "label": 0
                },
                {
                    "sent": "They roughly they work by.",
                    "label": 1
                },
                {
                    "sent": "Taking Kylo Ren Proximation to the various matrix where pure focusing on the principle subspaces and so they work very well when the spectrum exhibits fast decay.",
                    "label": 0
                },
                {
                    "sent": "But for models which have more flat spectrum and this approximation may not be very accurate so.",
                    "label": 1
                },
                {
                    "sent": "There are additional algorithms which also provide some sort of indication of variances, but there is something called embedded trees which.",
                    "label": 0
                },
                {
                    "sent": "Only works for models which are near trees, which are trees plus some additional edges, and their complexity depends on the number of cottages that you have to remove to make the model tree.",
                    "label": 1
                },
                {
                    "sent": "There's also something called LaGrange relaxation methods, which split models with loops into several tree structured models and then show them independently, but then enforce the condition that.",
                    "label": 0
                },
                {
                    "sent": "The solutions should be equal.",
                    "label": 0
                },
                {
                    "sent": "And they give upper bounds on the variances, but sometimes can be rather loose.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And let me motivate.",
                    "label": 0
                },
                {
                    "sent": "Another approach how to compute the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is using just iterative solvers.",
                    "label": 1
                },
                {
                    "sent": "So for the purpose of computing the means in a sparse graphical model, you just need to solve linear system.",
                    "label": 0
                },
                {
                    "sent": "JMU is equal to H where you're trying to compute the mean given the potential electric information matrix and the J is very very sparse, so you can use a variety of iterative solvers such as congregants or precondition gradients to compute the mean sufficiently even for large scale models and for some models.",
                    "label": 1
                },
                {
                    "sent": "The whole operation can be done in order of N steps, so just matrix multiplication by a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Where the number of nonzero components for each element is roughly constant is a of an operation, and then if you don't do too many of those multiplications.",
                    "label": 0
                },
                {
                    "sent": "If it's roughly constant, test the model growth then.",
                    "label": 0
                },
                {
                    "sent": "For example, we have limited correlation length and you can compute the whole mean in order of N steps.",
                    "label": 0
                },
                {
                    "sent": "So now this can be used also to compute the variances in the following way so.",
                    "label": 0
                },
                {
                    "sent": "If on the right hand side, instead of your potential vector H, you just plug in.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the application, but it can be very large and so that's why using good preconditioners sort of is often necessary in applications to achieve both convergence.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Yes, so instead of the potential vector on the right hand side, we if you plug in the Earth standard basis vector, which is just all zeros except one in the application.",
                    "label": 0
                },
                {
                    "sent": "Then by solving the system we get the right column of the coverage matrix.",
                    "label": 1
                },
                {
                    "sent": "So just by solving the system we can get one variance, but the trouble is that in order to get all the variances then we need to do it and times is on the order of millions.",
                    "label": 0
                },
                {
                    "sent": "So it's not really very suitable approach.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're doing is selling for inverse of JJP is equal to identity.",
                    "label": 0
                },
                {
                    "sent": "If you want to do it all together.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And so here is a.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivation how for models with short correlation you can actually do.",
                    "label": 0
                },
                {
                    "sent": "Many of those solves in parallel.",
                    "label": 0
                },
                {
                    "sent": "At the same time, So what they described on the previous slide is in the first row.",
                    "label": 0
                },
                {
                    "sent": "So you take a standard basis vector.",
                    "label": 0
                },
                {
                    "sent": "This spike application.",
                    "label": 0
                },
                {
                    "sent": "I do solve the linear system G. Jane versus H and you get the covariance of the note.",
                    "label": 0
                },
                {
                    "sent": "I missed the rest of the nodes and for models with short correlation length then.",
                    "label": 0
                },
                {
                    "sent": "You get very fast decay right around high and most of the solution is 0, so it looks like you're wasting computation, so you're just doing computation, which is more or less local, but the cost of a blank.",
                    "label": 0
                },
                {
                    "sent": "The linear solve the whole graphical model, and so the idea.",
                    "label": 0
                },
                {
                    "sent": "The very simple version of this idea is to basically do many of them in parallel, as long as they don't interfere too much, so do.",
                    "label": 0
                },
                {
                    "sent": "Instead of taking the high standard basis vector, take a number of them which are separated by roughly the correlation length and solve them in parallel.",
                    "label": 0
                },
                {
                    "sent": "And so you get the variances right near the peaks and you get some small interaction from the nearby nearby terms, and so this way you get.",
                    "label": 0
                },
                {
                    "sent": "Instead of 1 variance you get 6 variance in this picture.",
                    "label": 0
                },
                {
                    "sent": "So additional ingredient is that in order to have destructive interference between the errors, you can add random science.",
                    "label": 0
                },
                {
                    "sent": "So instead of just doing spike train of plus ones, you also multiply them by minus ones occasionally, and so on average.",
                    "label": 0
                },
                {
                    "sent": "You'll get your cancellation, and that's a very nice property which I will mention later also.",
                    "label": 0
                },
                {
                    "sent": "And So what does this really do and what can be said?",
                    "label": 0
                },
                {
                    "sent": "More generally, apart from models from the short correlation so?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're trying to do is solve the system.",
                    "label": 0
                },
                {
                    "sent": "JP is equal to Y, but.",
                    "label": 0
                },
                {
                    "sent": "It's the computation intensive.",
                    "label": 0
                },
                {
                    "sent": "Since we were replacing identity by a low rank matrix where B is a whole matrix, it has much more.",
                    "label": 0
                },
                {
                    "sent": "Pro stem columns.",
                    "label": 0
                },
                {
                    "sent": "And there is no meaningful sense in which we can approximate the identity by a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Obviously identity as all singular values equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this equation is meaningless.",
                    "label": 0
                },
                {
                    "sent": "But what we're trying to do is to make the identity close to this BB transpose with respect to a graph.",
                    "label": 0
                },
                {
                    "sent": "I'll explain what this means, so suppose so B is a matrix which we have freedom of choosing, and we want to choose it such that the resulting variances are accurate.",
                    "label": 0
                },
                {
                    "sent": "So suppose that the which would be such that the rows are unit norm.",
                    "label": 0
                },
                {
                    "sent": "Then if you just solve the system, Jeep is equal to BB transpose.",
                    "label": 0
                },
                {
                    "sent": "Then what you get is.",
                    "label": 0
                },
                {
                    "sent": "The true variance BIA which you're after plus some interference terms.",
                    "label": 0
                },
                {
                    "sent": "And the interference terms, our relations with no dye and OJ and the inner product between being BI and BJ.",
                    "label": 0
                },
                {
                    "sent": "So in particular the matrix below a certain normal.",
                    "label": 0
                },
                {
                    "sent": "Then all those inner products would be 0 and it would get the correct answer.",
                    "label": 0
                },
                {
                    "sent": "But since we are restricting be to be adult matrix then.",
                    "label": 0
                },
                {
                    "sent": "Some of those guys have to be non zero and the question is how to design this be such that.",
                    "label": 0
                },
                {
                    "sent": "Those terms are.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, cancel out in a useful way.",
                    "label": 0
                },
                {
                    "sent": "So one simple example is again for models with short correlation length, where if you take 2 nodes which are far apart.",
                    "label": 0
                },
                {
                    "sent": "So for example in spatial model then the correlation between them will be very small, close to zero and so there we don't have to care about what happens to the inner product.",
                    "label": 0
                },
                {
                    "sent": "So there are these terms automatically 0, so we only have to design B such that this inner product is close to 0 for nearby terms.",
                    "label": 1
                },
                {
                    "sent": "And so there is a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A simple construction.",
                    "label": 0
                },
                {
                    "sent": "How to achieve this?",
                    "label": 0
                },
                {
                    "sent": "There's no real design, it's just.",
                    "label": 0
                },
                {
                    "sent": "Immediately have it.",
                    "label": 0
                },
                {
                    "sent": "So let me describe it and it will become clear so.",
                    "label": 0
                },
                {
                    "sent": "We split all the nodes in the graph according to colors such that each color is separated by a certain length.",
                    "label": 1
                },
                {
                    "sent": "For example, nodes of red color have.",
                    "label": 0
                },
                {
                    "sent": "Four steps between them.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "So at each node with the same color, you put the spikes.",
                    "label": 1
                },
                {
                    "sent": "So we have several right hand sides.",
                    "label": 0
                },
                {
                    "sent": "Which we are trying to solve.",
                    "label": 0
                },
                {
                    "sent": "For each of them looks like a sort of field with spikes and together they covered the whole matrix, but individually they are well separated beyond the correlation length.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then actually let me draw a picture for one day, which might be very simple.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a chain model.",
                    "label": 1
                },
                {
                    "sent": "So you just partition.",
                    "label": 0
                },
                {
                    "sent": "So this would be 1 right inside and this would be.",
                    "label": 0
                },
                {
                    "sent": "The other right hand side and the distance between any two spikes is.",
                    "label": 0
                },
                {
                    "sent": "Is at least two, and so it's a similar similar construction for the square grid where you also should do it in checkerboard pattern too.",
                    "label": 0
                },
                {
                    "sent": "Increase the distance.",
                    "label": 0
                },
                {
                    "sent": "Now this can also be used for irregular graphs, but there you have to do some sort of a graph coloring and you don't have to solve it exactly, but approximately how long will do just fine?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the whole algorithm consists of constructing, constructing such a right hand side.",
                    "label": 0
                },
                {
                    "sent": "Solving it, you get some sort of response and feeding out the points corresponding to the variances, and you get the big subset of the variances already, and then you solve it for another right hand side where those spikes are shifted, and this way you reconstruct all variances.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is very very simple in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "It depends on the number of right hand side that you're solving for, so it depends on how compactly can you place the spikes without adding too much interference.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the interesting thing is that it has very nice guarantees.",
                    "label": 0
                },
                {
                    "sent": "So first of all, since we're adding random science, then on average tend biased.",
                    "label": 0
                },
                {
                    "sent": "So the expectation of the our estimate of the variance is actually is the variance itself.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                },
                {
                    "sent": "Take one set of science, solve it, and then take another set of science and average, then eventually you'll get the correct variances.",
                    "label": 0
                },
                {
                    "sent": "We can also, if you have some sort of decay on the correlations, and we can bound there in the variances, and as the separation between sort of the spikes increases, then converges to correct variances.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me give some examples.",
                    "label": 0
                },
                {
                    "sent": "So here we have a 1 dimensional chain model with short correlation length and the length is 256 and we.",
                    "label": 1
                },
                {
                    "sent": "Sure, put the.",
                    "label": 0
                },
                {
                    "sent": "Makes 16 apart and the so we get almost the correct variances, unfortunately, so the limitation is that for models with long range correlation there is.",
                    "label": 0
                },
                {
                    "sent": "If you put this by closely, then the variance estimates are very noisy.",
                    "label": 1
                },
                {
                    "sent": "So that's actually an additional feature of the approaches that the.",
                    "label": 0
                },
                {
                    "sent": "If you expect the variances to be more or less most field and this gives you an indication of how inaccurate or variances are so there you can see that there are variations.",
                    "label": 0
                },
                {
                    "sent": "Are very small for us here.",
                    "label": 0
                },
                {
                    "sent": "They are much larger so it gives you confidence in your approximate computation.",
                    "label": 0
                },
                {
                    "sent": "So this correlation be quiet, BK is exactly related to these rulings of either chicken soup for the simple model, yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, so you could do it in two ways.",
                    "label": 0
                },
                {
                    "sent": "Either you can set the correlation length to be a pretty large and then just do oneself and then immediately get in one sort of solution.",
                    "label": 0
                },
                {
                    "sent": "You get very occurrences.",
                    "label": 0
                },
                {
                    "sent": "Another approach would be to space them closer apart and then you can get fairly noisy estimate and then you can repeat it for another realization of the random science and you get another.",
                    "label": 0
                },
                {
                    "sent": "Set the variances and you can average together and you get do something coverage.",
                    "label": 0
                },
                {
                    "sent": "But for models with.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still long range correlations, something?",
                    "label": 0
                },
                {
                    "sent": "More can be done and basically the idea is that if you apply wavelets to process with long range correlations that they tend to do correlate them, so it's further off motivating idea.",
                    "label": 0
                },
                {
                    "sent": "Although the details are slightly different than the standard, the wavelet.",
                    "label": 0
                },
                {
                    "sent": "The prosecutor.",
                    "label": 0
                },
                {
                    "sent": "And so the starting point is very similar.",
                    "label": 0
                },
                {
                    "sent": "So if you want to solve the system J times, P is equal to identity and instead of identity we can replace it by the.",
                    "label": 0
                },
                {
                    "sent": "The Matrix the wave.",
                    "label": 0
                },
                {
                    "sent": "Transform matrix discretely transform matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have done in the short range collation case.",
                    "label": 0
                },
                {
                    "sent": "You just take the identity matrix and Ellis together, served by adding several columns together and we do the same for wavelets.",
                    "label": 0
                },
                {
                    "sent": "So we had several columns together such that.",
                    "label": 0
                },
                {
                    "sent": "Interference links between nearby at least terms is roughly equal to the correlation length of the process in the available domain.",
                    "label": 0
                },
                {
                    "sent": "So it's a very very simple construction, and the.",
                    "label": 0
                },
                {
                    "sent": "Now to get the variances you again solve it for this alias table matrix and the post multiplied by its transpose to get the diagonal terms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So here's some motivation for.",
                    "label": 0
                },
                {
                    "sent": "And the difference between the two approaches.",
                    "label": 0
                },
                {
                    "sent": "So if you have long range correlations, and if you space daily storms fairly close, then you get significant fear interference.",
                    "label": 0
                },
                {
                    "sent": "But if you first.",
                    "label": 0
                },
                {
                    "sent": "If instead of using the standard basis used the wavelet basis, then essentially what you're doing is you're convolving the correlation function is disabled, and so we get the much.",
                    "label": 0
                },
                {
                    "sent": "More localized response and the correlation becomes much shorter and the interference comes from the interaction of this blue curve with the list replicas of the wavelets.",
                    "label": 0
                },
                {
                    "sent": "And here it's you can pack a lot more of those spikes in the same area.",
                    "label": 0
                },
                {
                    "sent": "And the this approach is also unbiased.",
                    "label": 0
                },
                {
                    "sent": "And also we can control the variance.",
                    "label": 0
                },
                {
                    "sent": "And there's some analysis in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let me give some examples.",
                    "label": 0
                },
                {
                    "sent": "So here is the chain model with links to 156 where we use just 2028 columns instead of 256.",
                    "label": 0
                },
                {
                    "sent": "First we use the.",
                    "label": 0
                },
                {
                    "sent": "Simple short correlation approach reviews.",
                    "label": 0
                },
                {
                    "sent": "Just a list spikes and get somewhat useful.",
                    "label": 0
                },
                {
                    "sent": "Very assessments, but they are very noisy and.",
                    "label": 0
                },
                {
                    "sent": "But when we replace this construction, by the way, based construction, then you get very accurate, very system.",
                    "label": 0
                },
                {
                    "sent": "It's using just 28 linear solves instead of 256.",
                    "label": 0
                },
                {
                    "sent": "And so here is the 2 dimensional example.",
                    "label": 0
                },
                {
                    "sent": "So this is a, I think a simulation of a template model and the.",
                    "label": 0
                },
                {
                    "sent": "I forgot to put the number of right hand sides, but I think it's an order of a couple 100.",
                    "label": 0
                },
                {
                    "sent": "Instead of 64,000.",
                    "label": 0
                },
                {
                    "sent": "So you get very accurate variance estimates.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a fairly large scale example.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you can see.",
                    "label": 0
                },
                {
                    "sent": "It's quite dark, but this is the picture of the Pacific Ocean and here is Australia.",
                    "label": 0
                },
                {
                    "sent": "This is New Zealand and this is part of California somewhere on the top.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So the problem is, given satellite measurements, sparse irregular noisy measurements to compute the variance variance of the estimates of the surface height of the Pacific Ocean.",
                    "label": 0
                },
                {
                    "sent": "The problem is a very large scale problem.",
                    "label": 0
                },
                {
                    "sent": "The number of nodes is a million 1020, four 1024.",
                    "label": 0
                },
                {
                    "sent": "The model has.",
                    "label": 0
                },
                {
                    "sent": "I think on this scale it has a correlation length of about 60, so it's short but not that short for 60 by 60 is already several.",
                    "label": 0
                },
                {
                    "sent": "Thousands would have to use that many linear solves.",
                    "label": 0
                },
                {
                    "sent": "So if you use just a couple of scales with the wavelet transform, then we can reduce the number of linear.",
                    "label": 0
                },
                {
                    "sent": "So I think to 450, which is still large number.",
                    "label": 0
                },
                {
                    "sent": "But the you can compute the variance in the whole field in the 150 linear solves.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's another ingredient in how they actually do the linear solves so.",
                    "label": 0
                },
                {
                    "sent": "For that purpose we have to use efficient preconditioners.",
                    "label": 1
                },
                {
                    "sent": "There are very many approaches to how to do.",
                    "label": 0
                },
                {
                    "sent": "Big large sparse systems.",
                    "label": 0
                },
                {
                    "sent": "Including multigrid and even belief propagation can be used to give exact means, but for this work we focused on.",
                    "label": 0
                },
                {
                    "sent": "A simple class of tree structure preconditioners so they'll describe two particular variations.",
                    "label": 1
                },
                {
                    "sent": "So one is called embedded trees where.",
                    "label": 1
                },
                {
                    "sent": "Essentially, split the matrix into a tree structured component and the remaining remaining terms.",
                    "label": 0
                },
                {
                    "sent": "So J the information matrix is placed into the edges which belong to a tree and all the remaining pages.",
                    "label": 0
                },
                {
                    "sent": "Now you can use.",
                    "label": 0
                },
                {
                    "sent": "A simple iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you split.",
                    "label": 0
                },
                {
                    "sent": "It's just a presentation with conditioner.",
                    "label": 1
                },
                {
                    "sent": "So the mean at time N plus one is equal to three structured universe times.",
                    "label": 0
                },
                {
                    "sent": "The rest of the terms, the cottages multiplying the previous mean plus the potential vector, and so the interesting thing is that the.",
                    "label": 0
                },
                {
                    "sent": "JT universe, it can be computed by three structured belief propagation in order event, so it's it's a.",
                    "label": 0
                },
                {
                    "sent": "Very fast computation.",
                    "label": 0
                },
                {
                    "sent": "And so this this kind of preconditioners are very useful for sparse graphical models due to the existence of efficient solvers for tree structure models.",
                    "label": 0
                },
                {
                    "sent": "So another idea.",
                    "label": 0
                },
                {
                    "sent": "So the first preconditioner it is guaranteed to converge for so called voxel models for a subclass of model.",
                    "label": 1
                },
                {
                    "sent": "But on occasion it might it may diverge and so another idea is to use.",
                    "label": 0
                },
                {
                    "sent": "Block or distant, where your blocks are tree structured, so simple example would be just to use some sort of big chains.",
                    "label": 0
                },
                {
                    "sent": "Chains which extent across the whole graph.",
                    "label": 0
                },
                {
                    "sent": "So here you can do inference or computing versus apply the inverse operator in order of N steps.",
                    "label": 0
                },
                {
                    "sent": "So you fix variables which are not in the block you apply this fast restructure computation, and then you do.",
                    "label": 0
                },
                {
                    "sent": "; Block or dissent?",
                    "label": 0
                },
                {
                    "sent": "A nice idea for the converge fast is to make those blocks overlapping, and this has a nice motivation using box I'm Alesis you can look at what works.",
                    "label": 0
                },
                {
                    "sent": "Does this preconditioner capture and so if you have blocks which are non overlapping then you're always missing the short walk which goes from unblocked another, but if you make the blocks significantly overlapping then you capture capture much longer box and the shortest folks work that is missing after a few iterations is has already very nontrivial length.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is fairly preliminary work, but let me also mention so.",
                    "label": 0
                },
                {
                    "sent": "In some problems, the structure is somewhat more complex, so instead of justice sparse information matrix, you may have a sparse plus low rank structure.",
                    "label": 1
                },
                {
                    "sent": "Don't mention a couple of applications where this is the case.",
                    "label": 0
                },
                {
                    "sent": "In the so there you can also use some sort of preconditioners by various splitting of the information matrix into tree structure terms or blocks and.",
                    "label": 0
                },
                {
                    "sent": "Hun.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is actually to use belief propagation, but instead of using the paradise version, you can use the factor graph version of qualification.",
                    "label": 0
                },
                {
                    "sent": "So for Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "In terms of representation, there is no difference between the pairwise model and the factor graph structured version 'cause all the models are inherently paralyzed.",
                    "label": 1
                },
                {
                    "sent": "There are determined by the edges and by the nodes, but the factor graph representations actually useful for Bill Ification, because then you're operating with larger blocks and the.",
                    "label": 0
                },
                {
                    "sent": "The number of loops can be reduced by going through larger blocks.",
                    "label": 0
                },
                {
                    "sent": "And this approach is also related to a paper by Montanari Parker answer from relation for multiuser detection, where he does some very interesting tricks and converting the.",
                    "label": 1
                },
                {
                    "sent": "So degenerate information introduced into complex domain.",
                    "label": 0
                },
                {
                    "sent": "If I have some time, I can describe this in a bit more detail.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give an example of a problem where there might be sparse plus low rank structure and this is the problem of.",
                    "label": 0
                },
                {
                    "sent": "Dunst estimation.",
                    "label": 0
                },
                {
                    "sent": "From gravity measurements.",
                    "label": 0
                },
                {
                    "sent": "So the problem is as follows.",
                    "label": 0
                },
                {
                    "sent": "So you have a slice of the Earth vertical slide and you.",
                    "label": 0
                },
                {
                    "sent": "You have measurements, gravity measurements to the current vertical gravity at the surface.",
                    "label": 0
                },
                {
                    "sent": "Let's say you assume a thin plate Model 4.",
                    "label": 0
                },
                {
                    "sent": "For the density at each node, each node corresponds to the mass right around that location, and the gravity measurements.",
                    "label": 0
                },
                {
                    "sent": "They couple all.",
                    "label": 0
                },
                {
                    "sent": "The whole the whole grid, so essentially.",
                    "label": 0
                },
                {
                    "sent": "So the gravity measurements are linear in the mass and so you have a problem which has sparse prior structure but the measurement structure couple all of the nodes.",
                    "label": 0
                },
                {
                    "sent": "You have a sparse plus low rank and a matrix is according to the measurements and so here we applied.",
                    "label": 0
                },
                {
                    "sent": "I think the preconditioning method to compute the variance is into.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I didn't put any of the sizes, but I think the field is.",
                    "label": 0
                },
                {
                    "sent": "Is quite small.",
                    "label": 0
                },
                {
                    "sent": "It's 100 by 100.",
                    "label": 0
                },
                {
                    "sent": "Perhaps in the I think it uses something like.",
                    "label": 0
                },
                {
                    "sent": "A couple 100 downsides, so it's still applicable for.",
                    "label": 0
                },
                {
                    "sent": "It's basically the example to demonstrate that there are some other structures which can also be handled in this framework.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a very primitive work, so it's.",
                    "label": 0
                },
                {
                    "sent": "Approximately distributed filtering and linear dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "They'll just have one slide on it, but the only way to put them how this can be useful.",
                    "label": 0
                },
                {
                    "sent": "So basically, the idea that you have some sort of dynamical system which is on a chain and you have some measurements and you want to.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Get the estimates as time goes on, and so the structure of the information matrix is low diagonal, so the interesting part is that the each node itself corresponds to a large spatially distributed field which.",
                    "label": 0
                },
                {
                    "sent": "So each of the top notes itself is A is a big mark my calendar field.",
                    "label": 0
                },
                {
                    "sent": "So if you just do plain common filtering then you have to pass the operations from the full covariance matrix for the field and that may not even be feasible, and so the idea is to represent.",
                    "label": 0
                },
                {
                    "sent": "Those medicine information form instead to keep an approximate representation after each update.",
                    "label": 0
                },
                {
                    "sent": "The so and update the prediction step from time T + 1 to T is just a social component separation.",
                    "label": 0
                },
                {
                    "sent": "And if you have initially a sparse plus low rank representation for this inverse for the marginal estimate that empty, then.",
                    "label": 0
                },
                {
                    "sent": "Should the separation preserves this parcel rank structure at the next step, so this is just a suggestion for further research basically, but.",
                    "label": 0
                },
                {
                    "sent": "There are questions of stability of all the errors with accumulate and so forth, but.",
                    "label": 0
                },
                {
                    "sent": "So you decide to get it in, just not to be confined to play.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Microphone built.",
                    "label": 0
                },
                {
                    "sent": "And so this is coming to the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Oh, how my doing time should tell Mr.",
                    "label": 0
                },
                {
                    "sent": "Finished.",
                    "label": 0
                },
                {
                    "sent": "So as a summary, we have developed a very simple approach which has for computing marginal variances in large scale cosmic random field problems.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "By using random sign flips, this method gives unbiased variance estimates.",
                    "label": 1
                },
                {
                    "sent": "And we can also put bounds on the errors if we assume some nice correlation decay.",
                    "label": 0
                },
                {
                    "sent": "Of the model.",
                    "label": 0
                },
                {
                    "sent": "And we have proposed construction for a couple of.",
                    "label": 1
                },
                {
                    "sent": "Scenarios for a short correlation like scenario and for models with us most long range dependence and also if I suggested an extension to models with sparse clustering structure.",
                    "label": 0
                },
                {
                    "sent": "So in terms of relevance to.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning at large, so this is basically a method for computing the diagonal of the inverse of a matrix, where it's easy to.",
                    "label": 0
                },
                {
                    "sent": "Do linear solving the matrix so if you can do linear so efficiently, then you can compute the variance as well so.",
                    "label": 0
                },
                {
                    "sent": "In particular, it could be relevant for operations in Gaussian processes, so Gaussian processes are basically another presentation of.",
                    "label": 0
                },
                {
                    "sent": "Gentle Goshen large scale Gentry.",
                    "label": 0
                },
                {
                    "sent": "Gaussian models were instead of specifying the inverse governance matrix, you're explicitly specifying the sort of correlation structure.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So some other directions for further work or using so-called diffusion wavelets for regular graphs.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 1
                },
                {
                    "sent": "Using expressions in other bases, so for example, curvelets edges, since the favourites and the.",
                    "label": 0
                },
                {
                    "sent": "Perhaps similarities can be used to approximate the log determinant of the information matrix, but this I haven't done too much work on.",
                    "label": 0
                },
                {
                    "sent": "OK, I think this is the talk.",
                    "label": 0
                }
            ]
        }
    }
}