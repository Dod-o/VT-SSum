{
    "id": "yv53ko6kna3o6bnsrb3ai76lwo5wrqhq",
    "title": "An Asymptotic Analysis of Generative, Discriminative, and Pseudolikelihood Estimators",
    "info": {
        "author": [
            "Percy Liang, Computer Science Department, Stanford University"
        ],
        "published": "July 24, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_liang_aagdpe/",
    "segmentation": [
        [
            "So I'm going to be talking about it as some type analysis of some estimators that some of us might be familiar with already."
        ],
        [
            "So I want to do this in the context of structure prediction.",
            "So here we're trying to predict an output.",
            "Why given an input X, you can think of predicting part of speech tags from sentences.",
            "If you're doing natural language and generally why is going to be a structure object using a collection of variables which have some dependencies and for purposes of this talk, we're concentrating on probabilistic models."
        ],
        [
            "X&Y.",
            "So in the literature, you've encountered countless numbers of approaches to solving problems such as this.",
            "There's discriminative approaches from CRF's to the discretion generative approaches, and there's some people have argued which ones you know better than the other."
        ],
        [
            "So sometimes when your why is very complicated, you might not be able to do either generative discriminative exactly and some you might turn to alternatives such as super like."
        ],
        [
            "Hood or a number of other approaches.",
            "So basically there's so many things out there.",
            "How do we choose among these and how do we have a principled way of determining which ones?"
        ],
        [
            "For our task, so our work is we're just going to focus on the 1st three in this paper, and the way we're going to go about it is put them in a unified framework based on composite likelihood, which was developed by Lindsay and 1988, and it turns out to be very similar to multi conditional learning as well.",
            "And by putting these in the Unified framework we can compute the statistical properties and then compare this."
        ],
        [
            "So we have all of us probably have some existing intuitions about these approaches.",
            "So for discriminative there they have lower bias in the asymptotic limit.",
            "But generative models, my regularize better and."
        ],
        [
            "Lower variance pseudo likelihood.",
            "It was proven that has slower statistical."
        ],
        [
            "Convergence and so on.",
            "So are we in this paper we prove a general result, which is we're going to derive the excess risk of composite likelihood estimate."
        ],
        [
            "There's in general, and then this is going to allow us to get insight into a particular cases.",
            "For example, one conclusion is if the model is well specified.",
            "That is, if the true distribution of the data is in the model family, then there's some tatic S excess risk of the generative estimator is going to be better than this primitive, which is better than pseudolite."
        ],
        [
            "On the other hand, if the model is misspecified, then the discriminative estimator is going to outperform the other two, so."
        ],
        [
            "Now I'm going to present a little bit of notation to get us acquainted with the composite likelihood estimators so.",
            "The notation I'm going to use is that's going to be the estimator, and this is so in the probabilistic framework.",
            "Aware Joyce is going to be maximizing the log likelihood of the data, so this is a joint log likelihood of X&Y and this ehad denotes a summation over our training examples so very."
        ],
        [
            "Entered and this will become useful, but I'm going to depict each of these estimators with one of these diagrams, so this box the corresponds to the set of XY pairs and the orange is going to represent.",
            "There's also the set of all XY pairs in this case.",
            "So the idea that the intuition here is that the generative estimator is trying to put mass on XY, because that's what we observed.",
            "And since probably sum to one, it's going to take away mass from the rest of the orange air."
        ],
        [
            "So for the discriminative classifier were estimating why?"
        ],
        [
            "10X an we can write that as the probability of the joint minus problem."
        ],
        [
            "Facts, what does that look like in the diagram?",
            "So we're still trying to put mass on XY, but we're taking mass away from this orange strip, which corresponds to only the pairs for which the X matches the input."
        ],
        [
            "And more generally, you can think of arbitrary regions.",
            "Putting mass on XY and taking away mass from some."
        ],
        [
            "Neighborhood around XY, which is what we want to.",
            "Contrast."
        ],
        [
            "So now this handles a case of the generative, discriminative or pseudo likelihood.",
            "We need something else, so just refresher of what pseudo likelihood is.",
            "We have our X.",
            "Suppose we have a graphical model an.",
            "Why is graph axes are the observations so pseudo likelihood is going to maximize?",
            "Some of the log probabilities of each node while J given all the other nodes."
        ],
        [
            "And if we write it slightly differently, we can see that this form is the kind of the general form we had before where we're putting mass on XY and we're taking mass away from.",
            "The other values which agree on X on the conditioning neighborhood and the."
        ],
        [
            "Difference now is that we have an extra sum over here so."
        ],
        [
            "In general, we can think about a composite likelihood.",
            "As being defined as estimator, where we maximize a sum of a weighted combination of likelihood terms and I'm going to call each of these likelihood terms a component.",
            "So in this example there are two different components, one defined by a region R1 and another defined by region or two."
        ],
        [
            "So OK, so now this slide is just going to review exponential families briefly in the paper.",
            "We carry out all the math using exponential families because it's simplifies a lot of things that makes.",
            "The results cleaner and more intuitive, so exponential families is.",
            "Collection of distributions from tries as follows.",
            "We have a set of feature vector inner product with a set of vector of parameters and then normalize user log partition function and we notice that this work conditioning on the neighborhood.",
            "So it's just a minor tweak on the standard."
        ],
        [
            "Definition of exponential family.",
            "And now the standard properties will give us the meaning of this log probability and also the variance also.",
            "So if you take the first derivative, you get the mean.",
            "And if you take the second derivative log partition function, you get the variance, and so why are we taking derivatives?",
            "So when we do the analysis, we're going to see that their derivatives play a role in the Taylor expansions to come, but I just wanted to set it up."
        ],
        [
            "There.",
            "OK, so this is probably the most important slide of the talk, and because I don't have time to go into the proofs and analysis what I'm going to do is sketch kinda intuition and the core idea of what's going on that makes us allows us to make the comparisons so we have estimator as we represent using one of the."
        ],
        [
            "Diagrams.",
            "And so integration that we're going to either develop or have already is that as we more model more about that as we grow this region.",
            "That were contrasting were modeling more about the data.",
            "Then the data.",
            "If you do that, then the data will have more of an opportunity to tell us about the."
        ],
        [
            "Amateurs, so that's the intuition.",
            "So in the case of exponential families, or we have this picture, where on the X axis are the parameter Canonical parameters of the exponential family and on the Y axis we have the features which are the main parameters and there is a one to one relationship between the features and the parameters."
        ],
        [
            "What happens when we get data in for working with exponential families is we're going to have.",
            "Observe some empirical features and there's going to be some noise, So what?"
        ],
        [
            "But we estimation corresponds to is trying to map these features back onto a parameters."
        ],
        [
            "And as the noise gets also in this case amplified by.",
            "The slope, so the slope is going to determine how much the noise in the features translates to noise in the parameters.",
            "And a slope for is the derivative of this function, which is derivative mean, which gives you the variance.",
            "So that's why I was taking derivatives and the last slide, because the slope here, which is what we call the sensitivity, is going to give us an indication of how accurately we can estimate the parameters."
        ],
        [
            "So if the sensitivity or the slope goes up, then this noise on the parameters goes down and therefore we can make more accurate predictions."
        ],
        [
            "OK, So what happens when we do this for?",
            "At this generative discriminative so we can show that for.",
            "The generative model the sensitivity is basically the Fisher information, which is the variance over the features an for discriminative.",
            "It's the expected variance conditioned on the input.",
            "So now."
        ],
        [
            "Now we want to see which one of these is bigger.",
            "The key identity which follows from just.",
            "Probability is that the variance can be decomposed in the expert in terms of expected variance plus variance of expectation where this term, since it's a variance is always positive there."
        ],
        [
            "Or the sensitivity of the generative model is greater than the sensitivity of the."
        ],
        [
            "Primitive model and by our intuition of before the risk of the generative model is going to be smaller than the risk of a discriminative.",
            "So that's a very hand WAVY sketch of how we arrive at these results, but.",
            "Just to give you."
        ],
        [
            "Sick idea, so now we're going to kind of jump up and tell about kind of the general results.",
            "So we're going to do this as I'm talking to analysis.",
            "The first thing we might be interested is how actually are we going to estimate the parameters so we get 10 data points.",
            "We're going to get a proper estimate, and then as we get more, we can kind of the noisy parameter estimate will converge to the limiting parameter estimate.",
            "And Sigma is going to denote the asymptotic variance of those parameter estimates.",
            "An N is the number of data points, so we could have showed that the parameter error is basically that some variance of the parameters over the square root of an A."
        ],
        [
            "Of data points.",
            "But now we don't really care about the parameters.",
            "We care about the using the parameters for prediction.",
            "So in prediction we're going to measure the risk, which is the X is the expected log loss.",
            "For now.",
            "And Furthermore, we're going to look at the excess risk, which is the risk of the prom dress tement minus the risk of the limiting parameter estimate an when I write risk, why?",
            "I mean it's access risk.",
            "So in general this error in the parameters translates to the same kind of rate.",
            "On the risk were up to some."
        ],
        [
            "Different concepts, but if some set if some condition is satisfied, which I'll explain in a minute, we actually get a much better rate.",
            "So if we can kind of get this rate which is now, the error goes down with N instead of square root of N, then we're."
        ],
        [
            "Good shape.",
            "So now the two issues are when do we get scroger event versus order N convergence?",
            "And if two estimators give us the same rate, well which one has a lower asymptotic variance?",
            "So that's going to tell us which estimators."
        ],
        [
            "Better.",
            "OK, so now in the next slides we're going to have go through first the well specified case where the model true distribution is in the model family and working with only one component, then we're going to look at multiple components.",
            "So in the wet spot, well specified case, and then we're going to look at model misspecification.",
            "So that's an agenda."
        ],
        [
            "So for the well specified case, it turns out that the risk is always going to be single over order N, so we get the nice red and this happens as long as our estimate is consistent.",
            "So therefore for in the well specified case, we're just need to look at the asymptotics variants and see which."
        ],
        [
            "Tons better, so remember, let's just pop up a estimator generic S."
        ],
        [
            "Amateur here.",
            "It turns out that the asymptotic variance is exactly the inverse of the sensitivity.",
            "Remember, the sensitivity was the slope of that graph I had on the earlier slide, so it turns out that the larger the sensitivity, the smaller the asymptotic variance, and the better our premise."
        ],
        [
            "Estimates and the proof which I'm not going to go through, is simply using Taylor expansion and using moment generating properties of exponential family to get the."
        ],
        [
            "Motives.",
            "OK, so now we have a general form for that same tatic variance for in the well specified case, and now we'd like to compare two estimators.",
            "So let's take Theta, have one, and they had two as two estimators each estimator remember is just going to be represented by one of these regions, so one of Theta one is going to be represented by this orange region and Theta two is going to be represented by the green region."
        ],
        [
            "So.",
            "So the theorem we proved in the paper is if the model is well specified, and suppose that the region R1 is always a super set of the region R2.",
            "So estimator one is modeling more of the data, then we can conclude that the risk of the first estimator is smaller than the excess risk of thus."
        ],
        [
            "I can estimate it.",
            "And just a sketch of a proof.",
            "So remember definition of that.",
            "Some target variance is inverse of the sensitivity and we showed before that.",
            "So the argument that the variance decomposition lemma tells us that when you have a larger region that gives you."
        ],
        [
            "Higher sensitivity, therefore the first estimator has.",
            "A smaller asymptotic variance then the second estimator."
        ],
        [
            "And turning, we know that the risk for all these estimators is order of.",
            "Had some tatica variance over N, therefore this translates to a better access risk for the first estimator."
        ],
        [
            "So the general intuition that's kind of confirmed in math here is that more modeling more of the data reduces error when the model is miss well specified."
        ],
        [
            "OK, so now we turn to a case where we have multiple components, so the previous slide dealt with one component which is covers the case of the generative and discriminative classifiers.",
            "But now we want to look at sort of likelihood so it turns out that.",
            "Well, some talk very."
        ],
        [
            "So the estimator is a sensitivity to negative one as before, but in addition we incur this."
        ],
        [
            "Um?",
            "Access asset tag variants and this is kind of a correction or a penalty due to using multiple components.",
            "So we can generalize Arthur a little bit, and."
        ],
        [
            "Say that if the male models well specified and we have two estimators, suppose that Theta one has one component and Theta two has multiple components."
        ],
        [
            "Then an suppose Furthermore, that all the component, the single component of Theta one is a super set of all the components and."
        ],
        [
            "Data 2 then we get the same result that data one is better than theater.",
            "So this shows that it seems that having multiple components."
        ],
        [
            "It's going to hurt you, but you have to be a little bit careful because the same theorem does not hold in general.",
            "If Theta one the first estimator also has many components.",
            "So when you have many components over here, many components are here even if you have kind of a nested relationship of the components, it's not going to follow that one is necessarily better than other.",
            "OK, so now we've previous result will allow us to conclude that in the well specified case the pseudo likelihood which has more components an has smaller regions is going to be worse than the discriminative, which is going to be worse than generative."
        ],
        [
            "OK, so now what about the Misspecified case?",
            "So remember the well specified case we got this nice Sigma over N rate, but it turns out in the Misspecified case we only get Sigma over square root of an right."
        ],
        [
            "The risk, however, there is a single exception for the discriminative estimator.",
            "We do still get this Sigma over N rate, which is much better."
        ],
        [
            "And similar square event.",
            "So using this we can conclude that in the Misspecified case, the discriminative estimator is going to be better than the pseudo likelihood and the."
        ],
        [
            "Image of estimators and here property that we exploit is that the training criteria is the same as the testing criteria for the discriminative estimator alone and this so we have the criteria and the model an estimator, all playing nicely and that's why we get a better rate."
        ],
        [
            "OK, so now we've finished up in the going through the general theory side.",
            "We can actually look at a few experiments to see how the theory plays out.",
            "So here's the setup.",
            "Take this a little graphical model and.",
            "We're going to learn one of these from N training examples, and we're going to redo this to 10,000 times and look at the."
        ],
        [
            "Excess risk.",
            "So in the well specified case, we're going to generate our data from.",
            "The same model family with some setting of the parameters.",
            "And here's where you get, so we're comparing, generate discriminative and sewer likelihood and the variance of the risk is going down to zero as expected as the number of data points increases, but you know they all three kind of look the same."
        ],
        [
            "So let's blow it up a bit, so we multiply the variance by square root of N, so that's amplifying the error.",
            "It's still a little bit hard to say, so let's."
        ],
        [
            "Apply bionica so now this is kind of interesting because when you multiply by N. What you get, what these asymptotes are?",
            "The asymptotic variances?",
            "Some of the risk, so there's two things in."
        ],
        [
            "Note here one is that all three estimators do in fact.",
            "Have one over and convergence because we are when we multiply N we get a constant, so that's one of our convergence, and Furthermore we see that the generative has asymptotically variance smaller than a discriminative, which is smaller than two likelihood as."
        ],
        [
            "Theory predicts.",
            "OK, so now what about an SS misspecified case?",
            "So now we're going to generate data from this alternative graphical model which has this these extra edges.",
            "So now as we.",
            "Increase the number of data points.",
            "Every access risk is."
        ],
        [
            "We're going to 0.",
            "When we multiply by end, it seems like the genitive."
        ],
        [
            "Model is suffering a little bit, and if we multiply by the variance of the risk, we see that the discriminative one is only one that's constant, so the both pseudo likelihood.",
            "In general one go up, which means that they have.",
            "But a convergence rate of 1 /, sqrt N, whereas discriminative has the one over."
        ],
        [
            "And convergence."
        ],
        [
            "OK, so now this is a looking at asymptotically theory.",
            "Let's look at what happens on a finite sample on the real world task.",
            "So we looked at part of speech tagging, in which the X is a sentence and the Y is a sequence of part of speech."
        ],
        [
            "Tags.",
            "So in the data we use is the Wall Street Journal news articles, which is standard."
        ],
        [
            "An open data set.",
            "So first of all we look at the well specified case and what we did to get this synthetic data is.",
            "Used some, we used a generative estimate of the parameters.",
            "Based on this data and then we generated data using that parameter.",
            "So the idea was to get kind of realistic setting of the parameters.",
            "And we see here that the generative.",
            "So test error is simply the number of tags that it got wrong, and the generative is in the well specified case is better than the discriminative, only enough pseudo likelihood is slightly better than discriminative, which is not exact."
        ],
        [
            "What that theory predicts, but in the misspecified case, it turns out that the generative model.",
            "Has the highest test error rate where the discriminative has the lowest which is in concordance with the theory."
        ],
        [
            "OK, so summarize we've the main contribution of this work is to take on these three estimators.",
            "Generative discriminative likelihood and try to put them in the unifying framework, which captures all also other types of estimators that one might want to use and."
        ],
        [
            "And by doing this an we use asymptotic statistics which gives us a powerful tool for comparing these various estimates."
        ],
        [
            "And the general conclusions that we can draw from this is that in the well specified case, modeling more of the data reduces error.",
            "We saw this both empirically and we proved theorem about it and.",
            "Especially in the Misspecified case, it's desirable for the training and training criteria to be.",
            "Match the testing criteria and this explains the better performance of the discriminative classifier in the limit.",
            "So given this framework, we can then think about ways of.",
            "Combining making hybrid approaches based on generative and discriminative which has.",
            "And done to some extent in the literature and hope."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to be talking about it as some type analysis of some estimators that some of us might be familiar with already.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to do this in the context of structure prediction.",
                    "label": 0
                },
                {
                    "sent": "So here we're trying to predict an output.",
                    "label": 0
                },
                {
                    "sent": "Why given an input X, you can think of predicting part of speech tags from sentences.",
                    "label": 0
                },
                {
                    "sent": "If you're doing natural language and generally why is going to be a structure object using a collection of variables which have some dependencies and for purposes of this talk, we're concentrating on probabilistic models.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X&Y.",
                    "label": 0
                },
                {
                    "sent": "So in the literature, you've encountered countless numbers of approaches to solving problems such as this.",
                    "label": 0
                },
                {
                    "sent": "There's discriminative approaches from CRF's to the discretion generative approaches, and there's some people have argued which ones you know better than the other.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sometimes when your why is very complicated, you might not be able to do either generative discriminative exactly and some you might turn to alternatives such as super like.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hood or a number of other approaches.",
                    "label": 0
                },
                {
                    "sent": "So basically there's so many things out there.",
                    "label": 0
                },
                {
                    "sent": "How do we choose among these and how do we have a principled way of determining which ones?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For our task, so our work is we're just going to focus on the 1st three in this paper, and the way we're going to go about it is put them in a unified framework based on composite likelihood, which was developed by Lindsay and 1988, and it turns out to be very similar to multi conditional learning as well.",
                    "label": 0
                },
                {
                    "sent": "And by putting these in the Unified framework we can compute the statistical properties and then compare this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have all of us probably have some existing intuitions about these approaches.",
                    "label": 1
                },
                {
                    "sent": "So for discriminative there they have lower bias in the asymptotic limit.",
                    "label": 1
                },
                {
                    "sent": "But generative models, my regularize better and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lower variance pseudo likelihood.",
                    "label": 0
                },
                {
                    "sent": "It was proven that has slower statistical.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convergence and so on.",
                    "label": 0
                },
                {
                    "sent": "So are we in this paper we prove a general result, which is we're going to derive the excess risk of composite likelihood estimate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's in general, and then this is going to allow us to get insight into a particular cases.",
                    "label": 0
                },
                {
                    "sent": "For example, one conclusion is if the model is well specified.",
                    "label": 1
                },
                {
                    "sent": "That is, if the true distribution of the data is in the model family, then there's some tatic S excess risk of the generative estimator is going to be better than this primitive, which is better than pseudolite.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, if the model is misspecified, then the discriminative estimator is going to outperform the other two, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to present a little bit of notation to get us acquainted with the composite likelihood estimators so.",
                    "label": 0
                },
                {
                    "sent": "The notation I'm going to use is that's going to be the estimator, and this is so in the probabilistic framework.",
                    "label": 0
                },
                {
                    "sent": "Aware Joyce is going to be maximizing the log likelihood of the data, so this is a joint log likelihood of X&Y and this ehad denotes a summation over our training examples so very.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entered and this will become useful, but I'm going to depict each of these estimators with one of these diagrams, so this box the corresponds to the set of XY pairs and the orange is going to represent.",
                    "label": 0
                },
                {
                    "sent": "There's also the set of all XY pairs in this case.",
                    "label": 0
                },
                {
                    "sent": "So the idea that the intuition here is that the generative estimator is trying to put mass on XY, because that's what we observed.",
                    "label": 0
                },
                {
                    "sent": "And since probably sum to one, it's going to take away mass from the rest of the orange air.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the discriminative classifier were estimating why?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10X an we can write that as the probability of the joint minus problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facts, what does that look like in the diagram?",
                    "label": 0
                },
                {
                    "sent": "So we're still trying to put mass on XY, but we're taking mass away from this orange strip, which corresponds to only the pairs for which the X matches the input.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more generally, you can think of arbitrary regions.",
                    "label": 0
                },
                {
                    "sent": "Putting mass on XY and taking away mass from some.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neighborhood around XY, which is what we want to.",
                    "label": 0
                },
                {
                    "sent": "Contrast.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now this handles a case of the generative, discriminative or pseudo likelihood.",
                    "label": 0
                },
                {
                    "sent": "We need something else, so just refresher of what pseudo likelihood is.",
                    "label": 0
                },
                {
                    "sent": "We have our X.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a graphical model an.",
                    "label": 0
                },
                {
                    "sent": "Why is graph axes are the observations so pseudo likelihood is going to maximize?",
                    "label": 0
                },
                {
                    "sent": "Some of the log probabilities of each node while J given all the other nodes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we write it slightly differently, we can see that this form is the kind of the general form we had before where we're putting mass on XY and we're taking mass away from.",
                    "label": 0
                },
                {
                    "sent": "The other values which agree on X on the conditioning neighborhood and the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference now is that we have an extra sum over here so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, we can think about a composite likelihood.",
                    "label": 0
                },
                {
                    "sent": "As being defined as estimator, where we maximize a sum of a weighted combination of likelihood terms and I'm going to call each of these likelihood terms a component.",
                    "label": 0
                },
                {
                    "sent": "So in this example there are two different components, one defined by a region R1 and another defined by region or two.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so now this slide is just going to review exponential families briefly in the paper.",
                    "label": 0
                },
                {
                    "sent": "We carry out all the math using exponential families because it's simplifies a lot of things that makes.",
                    "label": 0
                },
                {
                    "sent": "The results cleaner and more intuitive, so exponential families is.",
                    "label": 1
                },
                {
                    "sent": "Collection of distributions from tries as follows.",
                    "label": 0
                },
                {
                    "sent": "We have a set of feature vector inner product with a set of vector of parameters and then normalize user log partition function and we notice that this work conditioning on the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So it's just a minor tweak on the standard.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition of exponential family.",
                    "label": 0
                },
                {
                    "sent": "And now the standard properties will give us the meaning of this log probability and also the variance also.",
                    "label": 0
                },
                {
                    "sent": "So if you take the first derivative, you get the mean.",
                    "label": 0
                },
                {
                    "sent": "And if you take the second derivative log partition function, you get the variance, and so why are we taking derivatives?",
                    "label": 0
                },
                {
                    "sent": "So when we do the analysis, we're going to see that their derivatives play a role in the Taylor expansions to come, but I just wanted to set it up.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is probably the most important slide of the talk, and because I don't have time to go into the proofs and analysis what I'm going to do is sketch kinda intuition and the core idea of what's going on that makes us allows us to make the comparisons so we have estimator as we represent using one of the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Diagrams.",
                    "label": 0
                },
                {
                    "sent": "And so integration that we're going to either develop or have already is that as we more model more about that as we grow this region.",
                    "label": 1
                },
                {
                    "sent": "That were contrasting were modeling more about the data.",
                    "label": 0
                },
                {
                    "sent": "Then the data.",
                    "label": 0
                },
                {
                    "sent": "If you do that, then the data will have more of an opportunity to tell us about the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amateurs, so that's the intuition.",
                    "label": 0
                },
                {
                    "sent": "So in the case of exponential families, or we have this picture, where on the X axis are the parameter Canonical parameters of the exponential family and on the Y axis we have the features which are the main parameters and there is a one to one relationship between the features and the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens when we get data in for working with exponential families is we're going to have.",
                    "label": 0
                },
                {
                    "sent": "Observe some empirical features and there's going to be some noise, So what?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we estimation corresponds to is trying to map these features back onto a parameters.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as the noise gets also in this case amplified by.",
                    "label": 0
                },
                {
                    "sent": "The slope, so the slope is going to determine how much the noise in the features translates to noise in the parameters.",
                    "label": 0
                },
                {
                    "sent": "And a slope for is the derivative of this function, which is derivative mean, which gives you the variance.",
                    "label": 0
                },
                {
                    "sent": "So that's why I was taking derivatives and the last slide, because the slope here, which is what we call the sensitivity, is going to give us an indication of how accurately we can estimate the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if the sensitivity or the slope goes up, then this noise on the parameters goes down and therefore we can make more accurate predictions.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what happens when we do this for?",
                    "label": 0
                },
                {
                    "sent": "At this generative discriminative so we can show that for.",
                    "label": 0
                },
                {
                    "sent": "The generative model the sensitivity is basically the Fisher information, which is the variance over the features an for discriminative.",
                    "label": 0
                },
                {
                    "sent": "It's the expected variance conditioned on the input.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we want to see which one of these is bigger.",
                    "label": 0
                },
                {
                    "sent": "The key identity which follows from just.",
                    "label": 0
                },
                {
                    "sent": "Probability is that the variance can be decomposed in the expert in terms of expected variance plus variance of expectation where this term, since it's a variance is always positive there.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the sensitivity of the generative model is greater than the sensitivity of the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Primitive model and by our intuition of before the risk of the generative model is going to be smaller than the risk of a discriminative.",
                    "label": 0
                },
                {
                    "sent": "So that's a very hand WAVY sketch of how we arrive at these results, but.",
                    "label": 0
                },
                {
                    "sent": "Just to give you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sick idea, so now we're going to kind of jump up and tell about kind of the general results.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do this as I'm talking to analysis.",
                    "label": 0
                },
                {
                    "sent": "The first thing we might be interested is how actually are we going to estimate the parameters so we get 10 data points.",
                    "label": 1
                },
                {
                    "sent": "We're going to get a proper estimate, and then as we get more, we can kind of the noisy parameter estimate will converge to the limiting parameter estimate.",
                    "label": 0
                },
                {
                    "sent": "And Sigma is going to denote the asymptotic variance of those parameter estimates.",
                    "label": 1
                },
                {
                    "sent": "An N is the number of data points, so we could have showed that the parameter error is basically that some variance of the parameters over the square root of an A.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of data points.",
                    "label": 0
                },
                {
                    "sent": "But now we don't really care about the parameters.",
                    "label": 0
                },
                {
                    "sent": "We care about the using the parameters for prediction.",
                    "label": 0
                },
                {
                    "sent": "So in prediction we're going to measure the risk, which is the X is the expected log loss.",
                    "label": 0
                },
                {
                    "sent": "For now.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, we're going to look at the excess risk, which is the risk of the prom dress tement minus the risk of the limiting parameter estimate an when I write risk, why?",
                    "label": 1
                },
                {
                    "sent": "I mean it's access risk.",
                    "label": 0
                },
                {
                    "sent": "So in general this error in the parameters translates to the same kind of rate.",
                    "label": 1
                },
                {
                    "sent": "On the risk were up to some.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different concepts, but if some set if some condition is satisfied, which I'll explain in a minute, we actually get a much better rate.",
                    "label": 0
                },
                {
                    "sent": "So if we can kind of get this rate which is now, the error goes down with N instead of square root of N, then we're.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good shape.",
                    "label": 0
                },
                {
                    "sent": "So now the two issues are when do we get scroger event versus order N convergence?",
                    "label": 0
                },
                {
                    "sent": "And if two estimators give us the same rate, well which one has a lower asymptotic variance?",
                    "label": 0
                },
                {
                    "sent": "So that's going to tell us which estimators.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "OK, so now in the next slides we're going to have go through first the well specified case where the model true distribution is in the model family and working with only one component, then we're going to look at multiple components.",
                    "label": 0
                },
                {
                    "sent": "So in the wet spot, well specified case, and then we're going to look at model misspecification.",
                    "label": 0
                },
                {
                    "sent": "So that's an agenda.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the well specified case, it turns out that the risk is always going to be single over order N, so we get the nice red and this happens as long as our estimate is consistent.",
                    "label": 0
                },
                {
                    "sent": "So therefore for in the well specified case, we're just need to look at the asymptotics variants and see which.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tons better, so remember, let's just pop up a estimator generic S.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Amateur here.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the asymptotic variance is exactly the inverse of the sensitivity.",
                    "label": 1
                },
                {
                    "sent": "Remember, the sensitivity was the slope of that graph I had on the earlier slide, so it turns out that the larger the sensitivity, the smaller the asymptotic variance, and the better our premise.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimates and the proof which I'm not going to go through, is simply using Taylor expansion and using moment generating properties of exponential family to get the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motives.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have a general form for that same tatic variance for in the well specified case, and now we'd like to compare two estimators.",
                    "label": 0
                },
                {
                    "sent": "So let's take Theta, have one, and they had two as two estimators each estimator remember is just going to be represented by one of these regions, so one of Theta one is going to be represented by this orange region and Theta two is going to be represented by the green region.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the theorem we proved in the paper is if the model is well specified, and suppose that the region R1 is always a super set of the region R2.",
                    "label": 0
                },
                {
                    "sent": "So estimator one is modeling more of the data, then we can conclude that the risk of the first estimator is smaller than the excess risk of thus.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can estimate it.",
                    "label": 0
                },
                {
                    "sent": "And just a sketch of a proof.",
                    "label": 0
                },
                {
                    "sent": "So remember definition of that.",
                    "label": 0
                },
                {
                    "sent": "Some target variance is inverse of the sensitivity and we showed before that.",
                    "label": 0
                },
                {
                    "sent": "So the argument that the variance decomposition lemma tells us that when you have a larger region that gives you.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Higher sensitivity, therefore the first estimator has.",
                    "label": 0
                },
                {
                    "sent": "A smaller asymptotic variance then the second estimator.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And turning, we know that the risk for all these estimators is order of.",
                    "label": 0
                },
                {
                    "sent": "Had some tatica variance over N, therefore this translates to a better access risk for the first estimator.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the general intuition that's kind of confirmed in math here is that more modeling more of the data reduces error when the model is miss well specified.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we turn to a case where we have multiple components, so the previous slide dealt with one component which is covers the case of the generative and discriminative classifiers.",
                    "label": 1
                },
                {
                    "sent": "But now we want to look at sort of likelihood so it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Well, some talk very.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the estimator is a sensitivity to negative one as before, but in addition we incur this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Access asset tag variants and this is kind of a correction or a penalty due to using multiple components.",
                    "label": 1
                },
                {
                    "sent": "So we can generalize Arthur a little bit, and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say that if the male models well specified and we have two estimators, suppose that Theta one has one component and Theta two has multiple components.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then an suppose Furthermore, that all the component, the single component of Theta one is a super set of all the components and.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data 2 then we get the same result that data one is better than theater.",
                    "label": 0
                },
                {
                    "sent": "So this shows that it seems that having multiple components.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's going to hurt you, but you have to be a little bit careful because the same theorem does not hold in general.",
                    "label": 0
                },
                {
                    "sent": "If Theta one the first estimator also has many components.",
                    "label": 0
                },
                {
                    "sent": "So when you have many components over here, many components are here even if you have kind of a nested relationship of the components, it's not going to follow that one is necessarily better than other.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we've previous result will allow us to conclude that in the well specified case the pseudo likelihood which has more components an has smaller regions is going to be worse than the discriminative, which is going to be worse than generative.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now what about the Misspecified case?",
                    "label": 0
                },
                {
                    "sent": "So remember the well specified case we got this nice Sigma over N rate, but it turns out in the Misspecified case we only get Sigma over square root of an right.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The risk, however, there is a single exception for the discriminative estimator.",
                    "label": 0
                },
                {
                    "sent": "We do still get this Sigma over N rate, which is much better.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And similar square event.",
                    "label": 0
                },
                {
                    "sent": "So using this we can conclude that in the Misspecified case, the discriminative estimator is going to be better than the pseudo likelihood and the.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image of estimators and here property that we exploit is that the training criteria is the same as the testing criteria for the discriminative estimator alone and this so we have the criteria and the model an estimator, all playing nicely and that's why we get a better rate.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we've finished up in the going through the general theory side.",
                    "label": 0
                },
                {
                    "sent": "We can actually look at a few experiments to see how the theory plays out.",
                    "label": 0
                },
                {
                    "sent": "So here's the setup.",
                    "label": 0
                },
                {
                    "sent": "Take this a little graphical model and.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn one of these from N training examples, and we're going to redo this to 10,000 times and look at the.",
                    "label": 1
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Excess risk.",
                    "label": 0
                },
                {
                    "sent": "So in the well specified case, we're going to generate our data from.",
                    "label": 0
                },
                {
                    "sent": "The same model family with some setting of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And here's where you get, so we're comparing, generate discriminative and sewer likelihood and the variance of the risk is going down to zero as expected as the number of data points increases, but you know they all three kind of look the same.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's blow it up a bit, so we multiply the variance by square root of N, so that's amplifying the error.",
                    "label": 0
                },
                {
                    "sent": "It's still a little bit hard to say, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply bionica so now this is kind of interesting because when you multiply by N. What you get, what these asymptotes are?",
                    "label": 0
                },
                {
                    "sent": "The asymptotic variances?",
                    "label": 0
                },
                {
                    "sent": "Some of the risk, so there's two things in.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note here one is that all three estimators do in fact.",
                    "label": 0
                },
                {
                    "sent": "Have one over and convergence because we are when we multiply N we get a constant, so that's one of our convergence, and Furthermore we see that the generative has asymptotically variance smaller than a discriminative, which is smaller than two likelihood as.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theory predicts.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what about an SS misspecified case?",
                    "label": 0
                },
                {
                    "sent": "So now we're going to generate data from this alternative graphical model which has this these extra edges.",
                    "label": 0
                },
                {
                    "sent": "So now as we.",
                    "label": 0
                },
                {
                    "sent": "Increase the number of data points.",
                    "label": 0
                },
                {
                    "sent": "Every access risk is.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to 0.",
                    "label": 0
                },
                {
                    "sent": "When we multiply by end, it seems like the genitive.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model is suffering a little bit, and if we multiply by the variance of the risk, we see that the discriminative one is only one that's constant, so the both pseudo likelihood.",
                    "label": 0
                },
                {
                    "sent": "In general one go up, which means that they have.",
                    "label": 0
                },
                {
                    "sent": "But a convergence rate of 1 /, sqrt N, whereas discriminative has the one over.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And convergence.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now this is a looking at asymptotically theory.",
                    "label": 0
                },
                {
                    "sent": "Let's look at what happens on a finite sample on the real world task.",
                    "label": 0
                },
                {
                    "sent": "So we looked at part of speech tagging, in which the X is a sentence and the Y is a sequence of part of speech.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tags.",
                    "label": 0
                },
                {
                    "sent": "So in the data we use is the Wall Street Journal news articles, which is standard.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An open data set.",
                    "label": 0
                },
                {
                    "sent": "So first of all we look at the well specified case and what we did to get this synthetic data is.",
                    "label": 0
                },
                {
                    "sent": "Used some, we used a generative estimate of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Based on this data and then we generated data using that parameter.",
                    "label": 0
                },
                {
                    "sent": "So the idea was to get kind of realistic setting of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And we see here that the generative.",
                    "label": 0
                },
                {
                    "sent": "So test error is simply the number of tags that it got wrong, and the generative is in the well specified case is better than the discriminative, only enough pseudo likelihood is slightly better than discriminative, which is not exact.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What that theory predicts, but in the misspecified case, it turns out that the generative model.",
                    "label": 0
                },
                {
                    "sent": "Has the highest test error rate where the discriminative has the lowest which is in concordance with the theory.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so summarize we've the main contribution of this work is to take on these three estimators.",
                    "label": 0
                },
                {
                    "sent": "Generative discriminative likelihood and try to put them in the unifying framework, which captures all also other types of estimators that one might want to use and.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by doing this an we use asymptotic statistics which gives us a powerful tool for comparing these various estimates.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the general conclusions that we can draw from this is that in the well specified case, modeling more of the data reduces error.",
                    "label": 1
                },
                {
                    "sent": "We saw this both empirically and we proved theorem about it and.",
                    "label": 0
                },
                {
                    "sent": "Especially in the Misspecified case, it's desirable for the training and training criteria to be.",
                    "label": 0
                },
                {
                    "sent": "Match the testing criteria and this explains the better performance of the discriminative classifier in the limit.",
                    "label": 0
                },
                {
                    "sent": "So given this framework, we can then think about ways of.",
                    "label": 0
                },
                {
                    "sent": "Combining making hybrid approaches based on generative and discriminative which has.",
                    "label": 0
                },
                {
                    "sent": "And done to some extent in the literature and hope.",
                    "label": 0
                }
            ]
        }
    }
}