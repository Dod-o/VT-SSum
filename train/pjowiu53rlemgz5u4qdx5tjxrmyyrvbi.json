{
    "id": "pjowiu53rlemgz5u4qdx5tjxrmyyrvbi",
    "title": "Independent Factor Topic Models",
    "info": {
        "author": [
            "Duangmanee (Pew) Putthividhya, Department of Electrical and Computer Engineering, UC San Diego"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_putthividhya_iftm/",
    "segmentation": [
        [
            "OK, thank you very much.",
            "And three Nevada.",
            "Yeah.",
            "OK, thanks for the introduction.",
            "So today I would like to present my work on independent factor topic models."
        ],
        [
            "This work is done in collaboration with a guy ATS and three in Agrigen from UCSF.",
            "Cat.",
            "So the type of topic models that we discussed today in this talk are topic models that can capture correlations between topics.",
            "One of the motivating applications is the use of topic models in exploratory analysis, where the idea is that you given a large collection of documents and you interested in building a browsing interface.",
            "Something like a topic graph for the archive, like the one shown here in this picture.",
            "That gives you information not just on what are the set of dominant topics that are contained in this archive.",
            "But also, what are the relationships between these topics so that the user of the archive can navigate through the documents in the collection in a topic guided fashion using the graph?",
            "So here's the out."
        ],
        [
            "Side of the presentation, 1st, I'll talk about some of the previous work that has been model that has been proposed on how to model topic correlations and then I'll talk about the model that we're proposing, which we call independent factor topic models.",
            "Then I'll talk a little bit about how to do inference for our model and show some experimental results on model comparison and how we can visualize the patterns of correlations.",
            "Learn from the model."
        ],
        [
            "First, let me mention the reason why Leighton reallocation cannot capture correlations between topics in Audi.",
            "A document is model as a mixture of topics or proportion of topics and is denoted by Theta.",
            "So data is assumed to have additional distribution, but because under originally the components data integer model is nearly independent, so because it is precisely because of this convenient choice of prior for Theta that limits the ability for LDA to learn correlations between topics."
        ],
        [
            "There's been some previous work done on how to model correlations.",
            "One of the model, the first one that I would like to talk about is pachinko allocation models proposed by Liam McCallum in 2006.",
            "So the idea is that two captured to be able to capture a topic correlation before you sample a topic for each word in a document.",
            "What they propose is that you first have to sample this additional hidden variable called super topic, which is a discrete value hidden variable.",
            "It takes a value from one to Al.",
            "And essentially what it does is it selects from one of the our distributions over topics to choose to generate the topic for the current word.",
            "So first you condition on the value of the Super topic.",
            "Then you sample a topic and by introducing this extra layer of super super topic it gives the model the ability to learn correlations between topics and in a sense that under different super topics you expect to see a different set of topics to that are more likely to Co occur than others.",
            "So a."
        ],
        [
            "Other model that's been proposed to learn correlations is correlated topic models proposed by Blind Lafferty in 2006, so the idea is that they replace the use of a duration in Audi A with logistic normal distribution in CTM.",
            "So now data is a draw from my direction from logistic normal distribution and is generated in two steps.",
            "So first you sample X which is a real value random variable from a Gaussian distribution with mean mu, an covariance Sigma.",
            "And then you obtain data, which is the topic proportion for the document by mapping X to Theta using a softmax transformation.",
            "So data care is each the XK divided by summation over JE to exchange.",
            "So it is essentially in the covariance parameter Sigma of this of CTM that encodes correlation between topics in that.",
            "If you have topic Ion Topic J Co occurring very often in the training set, then.",
            "This relationship should be reflected in the entry Sigma IJ of the covariance parameter."
        ],
        [
            "So I will model builds on CTM, so we'll first talk about some of the issues that CTM has, and then we'll discuss the kind of solutions that we propose to address them.",
            "So the first problem is in the use of the full rank covariance parameter.",
            "So as you know, the number of parameters of a full rank covariance matrix grows quadratically with K, so CTM does not scale very well.",
            "As K increases.",
            "The second problem is that inference in CTM is very quite slow.",
            "Especially for large K. So the reason for this is that CTM uses a logistic normal distribution which is not a conjugate prior to the multinomial distribution.",
            "So some of the variational parameters do not have closed form updates.",
            "In fact, conjugate gradient descent is used to update one of the parameters and that causes quite a significant slowdown in inference for CTM.",
            "So what can we do to address these problems?",
            "So the first thing that we ask is.",
            "Can we get by with the use of a low rank covariance and that's how we first look?",
            "Start looking at factor analysis as a way to constrain the covariance structure so factor analysis is a well known linear latent variable model for dimensionality reduction.",
            "So the idea is that we want to model the linear correlation in the data, which sort of data is usually in high dimension.",
            "We want to model the linear correlation in the data using hidden factors in low dimension, so X.",
            "Here, denotes the data and it's in Acadia national space, and K is usually high, so the factor is S, which is so sorry.",
            "So as here is in, in, in, in, and out dimensional space.",
            "And it assumes to have a Gaussian distribution, zero mean identity covariance.",
            "A here is a factor loading matrix of the mixing matrix.",
            "Male is a mean parameter and then N is a noise.",
            "Gaussian noise, zero mean and then this is the precision matrix for noise.",
            "It assumes to be assumes that the noise is uncorrelated.",
            "So Lambda is diagonal.",
            "So by by using factor analysis to constrain the covariance structure, we essentially parameterise the covariance to be in this factorized form.",
            "So Sigma is now 8 * A transpose plus Lambda inverse, an essentially reduced the number of parameters from the order of K squared to the audio."
        ],
        [
            "Nearby owl so here's the model that he is a generative model for independent factor topic models.",
            "So to sample to generate a document using our model, we first sample F from POS.",
            "So is the source source contribution, or the factors as I mentioned in the previous slide, P OS is the Gaussian distribution.",
            "Zero mean identical variants, but, but as I will show in the next slide.",
            "We also explore some other non Gaussian source distribution.",
            "So once we have S then we sample the correlated topic vector X from a conditional distribution PX given S which is a Gaussian with mean as plus mu and precision Lambda and then the topic proportion for each document data is obtained again by using a softmax that Maps from X today to an once we have data.",
            "The rest of the generative process is the same as LDA and CTM.",
            "So really, the main difference between our model, Audi and CTM is in how we generate data, how data is modeled so inaudi again Theta is a distribution in CTM data is a logistic normal distribution.",
            "You for sample X from a Gaussian then get data from X by a softmax.",
            "So in our model we first get X by by using a linear latent variable model and then get data from X by mapping using a softmax.",
            "Again, so we feel that this way of modeling correlation is more general because we modeling the source of correlation directly when P OS is Gaussian in.",
            "Like in like what I showed in the previous slide and the number of the dimensionality of the factors L is smaller than K, then our model is a special case of CTM with a factorized covariance parameter.",
            "But when Al is the same as K, our model is equivalent to CTM because now we can model any.",
            "Form of covariance parameter an when PMS is non Gaussian like what and I'll talk about that in the next slide.",
            "Then we can model nonlinear correlation in the topics.",
            "So the framework is more general and we can capture many different types of correlations in topics.",
            "I."
        ],
        [
            "So there's some problems with the Gaussian sources.",
            "The first problem is that the mixing matrix of the factor loading matrix a that we learn when Pierre versus Gaussian is often not very interpretable.",
            "The second problem is that there's not identify ability associated with rotations, so this is the same problem as with factor analysis, that the solution that you get for a is non unique.",
            "But if you want to be able to give interpretation to what a is.",
            "If you want to look at the column away and see what kind of pattern of topic correlation there represents, you want you kind of want to have a unique solution.",
            "So the solution that we propose is.",
            "Now we're just going to use the sparse source prior in the form of a Laplacian distribution by using such as source prior.",
            "Now a is now it becomes more interpretable an it can now be uniquely determined up to a scale factor."
        ],
        [
            "So for inference, we just use the standard mean field variational inference by approximating the true posterior, which is intractable using variational posterior in the factorized form.",
            "Now the goal is to find a variational posterior that minimizes the care divergent between the approximate and the true posterior.",
            "But there's some problem because of the use of a softmax transformation and the use of the non Gaussian source prior.",
            "So we need to introduce additional variational bounds.",
            "So we the idea is to make use of the concavity of the lock function and the square convexity of the lock of the Laplacian source prior and express this functions in their convex dual representations.",
            "So the computation is simplify this way and again this is a pretty standard technique and the details are given in the paper.",
            "So by using the variational bound that we propose, and together with the choice in modeling the noise precision as a diagonal matrix, we end up having that the posterior over X is now in a factorized form.",
            "So all the variational parameter updates that are in close form, we are able to be able to derive 1 dimensional Newton update for those parameters.",
            "So our inference is actually really fast and efficient."
        ],
        [
            "OK, so here we show a subset of 60 topics learn from 4000 NSF abstracts.",
            "So each topic we show 10 most likely words and then the sources or the factors are shown in circles.",
            "So here we show four sources as one through 2 S 4.",
            "So for each source their arrows pointing to the topics, right?",
            "So these are the topics that are most likely to Co occur under each source, and we have a weight for on each arrow corresponding to the entry of the.",
            "Entry of the mixing matrix A.",
            "So, so what's being shown here are essentially patterns of topic correlations.",
            "OK, so remember again, the data that we processing here are NSF abstracts data, so these are abstracts submitted to NSF from all disciplines of science.",
            "Some abstract from material science, summer from physics, summer from biology.",
            "So let me just point out a specific example.",
            "If you look at S3 and the topics that are being pointed to by S3.",
            "So topic 37 Topic 7 topic 34 So topic 37.",
            "If you look at the words that are very likely under this topic so you can see that it corresponds to the study of electromagnetism and Topic 7 is the study of particle physics and topic 34 is about wave modeling, is about the study with modeling.",
            "So these three topics are found to Co occur.",
            "Together, often in the data set, but you know there are arrows connecting between the three of them, but it will be very, very unlikely for a topic 37.",
            "The study of electromagnetism to Co occur in the same document as topic 24, which is the study of food chain and ecosystems.",
            "So I mean this groupings of topics kind of makes sense and it kind of is what we expect from this data set."
        ],
        [
            "So here I show.",
            "So so here we showing how our model comp."
        ],
        [
            "Pairs with CTM, an Audi A on held out likelihood so it's a plot of held out likelihood as a function of the number of topics.",
            "As you can see, that also OK, so the blue curve is our model, the red curve here is CTM, and the pink curve is LDA.",
            "So when we use a small number of topics so all the three algorithms kind of perform.",
            "Comparably, they're pretty much the same.",
            "They're not.",
            "There's not a big difference between them, but as we increase the number of topics you can see that the performance of RDA kind of drops dramatically.",
            "And while the models that can capture the topic correlations, which is CTM in our model, are able to support more topics, we perform better as a little bit better than LDA as we increase the number of topics.",
            "So we think that the reason that LDA perform so poorly is because Audi models the topics as being independent, so the words are not.",
            "Shared between the topics so as we increase the number, the number of topics, then each topic becomes more more more more more specific so it doesn't generalize as well on the test data.",
            "Think and for CTM an hour model it seems that CTM or fits the data because our model performs better and we only use 1500 documents in this data set so.",
            "So we think that CTM overfits the data.",
            "Here is again the same plot, but we use 4000 documents instead of 1500, so you can see that the models that learn topic correlations perform very competitively.",
            "They perform pretty much the same, so the blue again is our model, red is CTM, and then the black is out is FTM with Laplacian sources so there.",
            "Very compareable, but Audi A performs pretty poorly.",
            "So again it has that drop.",
            "So with competitive performance."
        ],
        [
            "We find that our model takes about 1/5 of the computation of the of CTM.",
            "So meaning that if it takes one day to train our model then it will take."
        ],
        [
            "Five days history in CTM.",
            "We do."
        ],
        [
            "Have a fast computer.",
            "To summarize.",
            "The model that we propose, which is FTM, learns spice correlations between topics.",
            "All models performs competitively with CTM, but we are able to achieve about three to five times speed gain compared to CTM.",
            "The limitation of course is that we need to specify the number of sources Al right now we just doing it arbitrarily to set it 2K or 4K over 5, but we want to be able to maybe automatically determine the number of sources.",
            "By putting a prior putting us prior over the mixing matrix a an we also interested in exploring other source distribution.",
            "So that concludes the talk.",
            "Time for.",
            "Try again.",
            "Geolocation compares to this.",
            "We have to keep them slightly different way of modeling topics on deferred darkening basis right, not really know because we haven't done a comparison.",
            "But I mean I think to be fair, I think we have to compare our model the with pachinko allocation, with the variational inference implementation, not like Gibbs sampling implementation.",
            "An algorithm is, you know, in particular because the advantage of having a simple Gibbs sampling algorithm right so right, but then unnecessary.",
            "OK, so we'll be doing we conducting experiments.",
            "Comparing yeah, OK, sure, yeah, yeah, I don't know.",
            "I don't know.",
            "I don't know off the top of my head how they would perform, but according to the paper I mean, they said Pachinko performs better than CTM and our model our model is comperable to CTM.",
            "Maybe they'll perform better.",
            "Yeah I don't know.",
            "I.",
            "You are showing your results.",
            "You were showing that LDA with like 200 topics was heading considerably worse like within alligator 10 topics.",
            "Yeah, this is a bit weird.",
            "In the standard LDA, if you do keep sampling where you integrate over all of your parameters, presumably that shouldn't be happening.",
            "Dentition why this was happening?",
            "200 hyperparameters that we doing empirical Bayes on and do some kind of fitting.",
            "So.",
            "Like so, it's pretty consistent.",
            "Like I we we actually try some other data set besides this data set that we show and also it shows the drop.",
            "It also shows drop.",
            "No, we use important sampling to compute the true likelihood.",
            "So using the proposed posterior distribution as a proposal distribution.",
            "Sorry.",
            "Important sampling.",
            "Yeah, I was just asking why they're doing harmonic mean estimator or estimator workflow.",
            "Thank you.",
            "Are you stealing your hyperparameters topics?",
            "Oh Alpha alright yeah, actually we fix it to 0.1.",
            "Language is a powerful quiet, actually increasing because then this morning becomes really big problems with title.",
            "I see her, I see you will try to hide.",
            "I found some problems with correlated topic model and with numerical instability in estimating the covariance matrix.",
            "Did you encounter that at all?",
            "And so you think that the simpler factored representation would be more robot?",
            "So like you mean Sigma is when the estimate of Sigma is not positive definite.",
            "No, we actually don't see that, but no, no.",
            "But I would guess that the factor representation.",
            "Yes, I think so.",
            "Right, right, right, yeah.",
            "So I was unclear did using the Laplace source model.",
            "It helps it helps with the visualization.",
            "Yeah, 'cause if you use Gaussian source and yeah, but not in terms of accuracy.",
            "I mean it's pretty much the same as you can see.",
            "Yeah yeah.",
            "And like in terms of the topic graph that you showed, are there topics that are being threshold that you're not showing links to?",
            "Yes, not try shouting, so I mean, I just look at each column of A and then pick out the three or four topics that are highly that.",
            "So I rank I saw it the column away and then pick out three or four topics.",
            "Yeah, so no threshold.",
            "Yeah, I guess they're showing.",
            "OK, well thank you very much.",
            "Yeah thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "And three Nevada.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "So today I would like to present my work on independent factor topic models.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work is done in collaboration with a guy ATS and three in Agrigen from UCSF.",
                    "label": 0
                },
                {
                    "sent": "Cat.",
                    "label": 0
                },
                {
                    "sent": "So the type of topic models that we discussed today in this talk are topic models that can capture correlations between topics.",
                    "label": 1
                },
                {
                    "sent": "One of the motivating applications is the use of topic models in exploratory analysis, where the idea is that you given a large collection of documents and you interested in building a browsing interface.",
                    "label": 0
                },
                {
                    "sent": "Something like a topic graph for the archive, like the one shown here in this picture.",
                    "label": 0
                },
                {
                    "sent": "That gives you information not just on what are the set of dominant topics that are contained in this archive.",
                    "label": 0
                },
                {
                    "sent": "But also, what are the relationships between these topics so that the user of the archive can navigate through the documents in the collection in a topic guided fashion using the graph?",
                    "label": 0
                },
                {
                    "sent": "So here's the out.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Side of the presentation, 1st, I'll talk about some of the previous work that has been model that has been proposed on how to model topic correlations and then I'll talk about the model that we're proposing, which we call independent factor topic models.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk a little bit about how to do inference for our model and show some experimental results on model comparison and how we can visualize the patterns of correlations.",
                    "label": 0
                },
                {
                    "sent": "Learn from the model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, let me mention the reason why Leighton reallocation cannot capture correlations between topics in Audi.",
                    "label": 0
                },
                {
                    "sent": "A document is model as a mixture of topics or proportion of topics and is denoted by Theta.",
                    "label": 1
                },
                {
                    "sent": "So data is assumed to have additional distribution, but because under originally the components data integer model is nearly independent, so because it is precisely because of this convenient choice of prior for Theta that limits the ability for LDA to learn correlations between topics.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's been some previous work done on how to model correlations.",
                    "label": 0
                },
                {
                    "sent": "One of the model, the first one that I would like to talk about is pachinko allocation models proposed by Liam McCallum in 2006.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that two captured to be able to capture a topic correlation before you sample a topic for each word in a document.",
                    "label": 0
                },
                {
                    "sent": "What they propose is that you first have to sample this additional hidden variable called super topic, which is a discrete value hidden variable.",
                    "label": 0
                },
                {
                    "sent": "It takes a value from one to Al.",
                    "label": 0
                },
                {
                    "sent": "And essentially what it does is it selects from one of the our distributions over topics to choose to generate the topic for the current word.",
                    "label": 0
                },
                {
                    "sent": "So first you condition on the value of the Super topic.",
                    "label": 0
                },
                {
                    "sent": "Then you sample a topic and by introducing this extra layer of super super topic it gives the model the ability to learn correlations between topics and in a sense that under different super topics you expect to see a different set of topics to that are more likely to Co occur than others.",
                    "label": 1
                },
                {
                    "sent": "So a.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other model that's been proposed to learn correlations is correlated topic models proposed by Blind Lafferty in 2006, so the idea is that they replace the use of a duration in Audi A with logistic normal distribution in CTM.",
                    "label": 1
                },
                {
                    "sent": "So now data is a draw from my direction from logistic normal distribution and is generated in two steps.",
                    "label": 0
                },
                {
                    "sent": "So first you sample X which is a real value random variable from a Gaussian distribution with mean mu, an covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "And then you obtain data, which is the topic proportion for the document by mapping X to Theta using a softmax transformation.",
                    "label": 0
                },
                {
                    "sent": "So data care is each the XK divided by summation over JE to exchange.",
                    "label": 0
                },
                {
                    "sent": "So it is essentially in the covariance parameter Sigma of this of CTM that encodes correlation between topics in that.",
                    "label": 1
                },
                {
                    "sent": "If you have topic Ion Topic J Co occurring very often in the training set, then.",
                    "label": 0
                },
                {
                    "sent": "This relationship should be reflected in the entry Sigma IJ of the covariance parameter.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will model builds on CTM, so we'll first talk about some of the issues that CTM has, and then we'll discuss the kind of solutions that we propose to address them.",
                    "label": 0
                },
                {
                    "sent": "So the first problem is in the use of the full rank covariance parameter.",
                    "label": 0
                },
                {
                    "sent": "So as you know, the number of parameters of a full rank covariance matrix grows quadratically with K, so CTM does not scale very well.",
                    "label": 1
                },
                {
                    "sent": "As K increases.",
                    "label": 0
                },
                {
                    "sent": "The second problem is that inference in CTM is very quite slow.",
                    "label": 0
                },
                {
                    "sent": "Especially for large K. So the reason for this is that CTM uses a logistic normal distribution which is not a conjugate prior to the multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So some of the variational parameters do not have closed form updates.",
                    "label": 0
                },
                {
                    "sent": "In fact, conjugate gradient descent is used to update one of the parameters and that causes quite a significant slowdown in inference for CTM.",
                    "label": 0
                },
                {
                    "sent": "So what can we do to address these problems?",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we ask is.",
                    "label": 0
                },
                {
                    "sent": "Can we get by with the use of a low rank covariance and that's how we first look?",
                    "label": 0
                },
                {
                    "sent": "Start looking at factor analysis as a way to constrain the covariance structure so factor analysis is a well known linear latent variable model for dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that we want to model the linear correlation in the data, which sort of data is usually in high dimension.",
                    "label": 0
                },
                {
                    "sent": "We want to model the linear correlation in the data using hidden factors in low dimension, so X.",
                    "label": 0
                },
                {
                    "sent": "Here, denotes the data and it's in Acadia national space, and K is usually high, so the factor is S, which is so sorry.",
                    "label": 0
                },
                {
                    "sent": "So as here is in, in, in, in, and out dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And it assumes to have a Gaussian distribution, zero mean identity covariance.",
                    "label": 0
                },
                {
                    "sent": "A here is a factor loading matrix of the mixing matrix.",
                    "label": 0
                },
                {
                    "sent": "Male is a mean parameter and then N is a noise.",
                    "label": 0
                },
                {
                    "sent": "Gaussian noise, zero mean and then this is the precision matrix for noise.",
                    "label": 0
                },
                {
                    "sent": "It assumes to be assumes that the noise is uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "So Lambda is diagonal.",
                    "label": 0
                },
                {
                    "sent": "So by by using factor analysis to constrain the covariance structure, we essentially parameterise the covariance to be in this factorized form.",
                    "label": 0
                },
                {
                    "sent": "So Sigma is now 8 * A transpose plus Lambda inverse, an essentially reduced the number of parameters from the order of K squared to the audio.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nearby owl so here's the model that he is a generative model for independent factor topic models.",
                    "label": 1
                },
                {
                    "sent": "So to sample to generate a document using our model, we first sample F from POS.",
                    "label": 0
                },
                {
                    "sent": "So is the source source contribution, or the factors as I mentioned in the previous slide, P OS is the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Zero mean identical variants, but, but as I will show in the next slide.",
                    "label": 0
                },
                {
                    "sent": "We also explore some other non Gaussian source distribution.",
                    "label": 0
                },
                {
                    "sent": "So once we have S then we sample the correlated topic vector X from a conditional distribution PX given S which is a Gaussian with mean as plus mu and precision Lambda and then the topic proportion for each document data is obtained again by using a softmax that Maps from X today to an once we have data.",
                    "label": 0
                },
                {
                    "sent": "The rest of the generative process is the same as LDA and CTM.",
                    "label": 0
                },
                {
                    "sent": "So really, the main difference between our model, Audi and CTM is in how we generate data, how data is modeled so inaudi again Theta is a distribution in CTM data is a logistic normal distribution.",
                    "label": 1
                },
                {
                    "sent": "You for sample X from a Gaussian then get data from X by a softmax.",
                    "label": 0
                },
                {
                    "sent": "So in our model we first get X by by using a linear latent variable model and then get data from X by mapping using a softmax.",
                    "label": 0
                },
                {
                    "sent": "Again, so we feel that this way of modeling correlation is more general because we modeling the source of correlation directly when P OS is Gaussian in.",
                    "label": 0
                },
                {
                    "sent": "Like in like what I showed in the previous slide and the number of the dimensionality of the factors L is smaller than K, then our model is a special case of CTM with a factorized covariance parameter.",
                    "label": 0
                },
                {
                    "sent": "But when Al is the same as K, our model is equivalent to CTM because now we can model any.",
                    "label": 0
                },
                {
                    "sent": "Form of covariance parameter an when PMS is non Gaussian like what and I'll talk about that in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Then we can model nonlinear correlation in the topics.",
                    "label": 0
                },
                {
                    "sent": "So the framework is more general and we can capture many different types of correlations in topics.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's some problems with the Gaussian sources.",
                    "label": 1
                },
                {
                    "sent": "The first problem is that the mixing matrix of the factor loading matrix a that we learn when Pierre versus Gaussian is often not very interpretable.",
                    "label": 0
                },
                {
                    "sent": "The second problem is that there's not identify ability associated with rotations, so this is the same problem as with factor analysis, that the solution that you get for a is non unique.",
                    "label": 0
                },
                {
                    "sent": "But if you want to be able to give interpretation to what a is.",
                    "label": 0
                },
                {
                    "sent": "If you want to look at the column away and see what kind of pattern of topic correlation there represents, you want you kind of want to have a unique solution.",
                    "label": 0
                },
                {
                    "sent": "So the solution that we propose is.",
                    "label": 0
                },
                {
                    "sent": "Now we're just going to use the sparse source prior in the form of a Laplacian distribution by using such as source prior.",
                    "label": 0
                },
                {
                    "sent": "Now a is now it becomes more interpretable an it can now be uniquely determined up to a scale factor.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for inference, we just use the standard mean field variational inference by approximating the true posterior, which is intractable using variational posterior in the factorized form.",
                    "label": 0
                },
                {
                    "sent": "Now the goal is to find a variational posterior that minimizes the care divergent between the approximate and the true posterior.",
                    "label": 0
                },
                {
                    "sent": "But there's some problem because of the use of a softmax transformation and the use of the non Gaussian source prior.",
                    "label": 0
                },
                {
                    "sent": "So we need to introduce additional variational bounds.",
                    "label": 0
                },
                {
                    "sent": "So we the idea is to make use of the concavity of the lock function and the square convexity of the lock of the Laplacian source prior and express this functions in their convex dual representations.",
                    "label": 0
                },
                {
                    "sent": "So the computation is simplify this way and again this is a pretty standard technique and the details are given in the paper.",
                    "label": 0
                },
                {
                    "sent": "So by using the variational bound that we propose, and together with the choice in modeling the noise precision as a diagonal matrix, we end up having that the posterior over X is now in a factorized form.",
                    "label": 0
                },
                {
                    "sent": "So all the variational parameter updates that are in close form, we are able to be able to derive 1 dimensional Newton update for those parameters.",
                    "label": 0
                },
                {
                    "sent": "So our inference is actually really fast and efficient.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here we show a subset of 60 topics learn from 4000 NSF abstracts.",
                    "label": 1
                },
                {
                    "sent": "So each topic we show 10 most likely words and then the sources or the factors are shown in circles.",
                    "label": 0
                },
                {
                    "sent": "So here we show four sources as one through 2 S 4.",
                    "label": 0
                },
                {
                    "sent": "So for each source their arrows pointing to the topics, right?",
                    "label": 0
                },
                {
                    "sent": "So these are the topics that are most likely to Co occur under each source, and we have a weight for on each arrow corresponding to the entry of the.",
                    "label": 0
                },
                {
                    "sent": "Entry of the mixing matrix A.",
                    "label": 0
                },
                {
                    "sent": "So, so what's being shown here are essentially patterns of topic correlations.",
                    "label": 0
                },
                {
                    "sent": "OK, so remember again, the data that we processing here are NSF abstracts data, so these are abstracts submitted to NSF from all disciplines of science.",
                    "label": 0
                },
                {
                    "sent": "Some abstract from material science, summer from physics, summer from biology.",
                    "label": 0
                },
                {
                    "sent": "So let me just point out a specific example.",
                    "label": 0
                },
                {
                    "sent": "If you look at S3 and the topics that are being pointed to by S3.",
                    "label": 0
                },
                {
                    "sent": "So topic 37 Topic 7 topic 34 So topic 37.",
                    "label": 0
                },
                {
                    "sent": "If you look at the words that are very likely under this topic so you can see that it corresponds to the study of electromagnetism and Topic 7 is the study of particle physics and topic 34 is about wave modeling, is about the study with modeling.",
                    "label": 0
                },
                {
                    "sent": "So these three topics are found to Co occur.",
                    "label": 0
                },
                {
                    "sent": "Together, often in the data set, but you know there are arrows connecting between the three of them, but it will be very, very unlikely for a topic 37.",
                    "label": 0
                },
                {
                    "sent": "The study of electromagnetism to Co occur in the same document as topic 24, which is the study of food chain and ecosystems.",
                    "label": 0
                },
                {
                    "sent": "So I mean this groupings of topics kind of makes sense and it kind of is what we expect from this data set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I show.",
                    "label": 0
                },
                {
                    "sent": "So so here we showing how our model comp.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pairs with CTM, an Audi A on held out likelihood so it's a plot of held out likelihood as a function of the number of topics.",
                    "label": 0
                },
                {
                    "sent": "As you can see, that also OK, so the blue curve is our model, the red curve here is CTM, and the pink curve is LDA.",
                    "label": 0
                },
                {
                    "sent": "So when we use a small number of topics so all the three algorithms kind of perform.",
                    "label": 0
                },
                {
                    "sent": "Comparably, they're pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "There's not a big difference between them, but as we increase the number of topics you can see that the performance of RDA kind of drops dramatically.",
                    "label": 0
                },
                {
                    "sent": "And while the models that can capture the topic correlations, which is CTM in our model, are able to support more topics, we perform better as a little bit better than LDA as we increase the number of topics.",
                    "label": 0
                },
                {
                    "sent": "So we think that the reason that LDA perform so poorly is because Audi models the topics as being independent, so the words are not.",
                    "label": 0
                },
                {
                    "sent": "Shared between the topics so as we increase the number, the number of topics, then each topic becomes more more more more more specific so it doesn't generalize as well on the test data.",
                    "label": 0
                },
                {
                    "sent": "Think and for CTM an hour model it seems that CTM or fits the data because our model performs better and we only use 1500 documents in this data set so.",
                    "label": 0
                },
                {
                    "sent": "So we think that CTM overfits the data.",
                    "label": 0
                },
                {
                    "sent": "Here is again the same plot, but we use 4000 documents instead of 1500, so you can see that the models that learn topic correlations perform very competitively.",
                    "label": 0
                },
                {
                    "sent": "They perform pretty much the same, so the blue again is our model, red is CTM, and then the black is out is FTM with Laplacian sources so there.",
                    "label": 0
                },
                {
                    "sent": "Very compareable, but Audi A performs pretty poorly.",
                    "label": 0
                },
                {
                    "sent": "So again it has that drop.",
                    "label": 0
                },
                {
                    "sent": "So with competitive performance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find that our model takes about 1/5 of the computation of the of CTM.",
                    "label": 0
                },
                {
                    "sent": "So meaning that if it takes one day to train our model then it will take.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Five days history in CTM.",
                    "label": 0
                },
                {
                    "sent": "We do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a fast computer.",
                    "label": 0
                },
                {
                    "sent": "To summarize.",
                    "label": 0
                },
                {
                    "sent": "The model that we propose, which is FTM, learns spice correlations between topics.",
                    "label": 0
                },
                {
                    "sent": "All models performs competitively with CTM, but we are able to achieve about three to five times speed gain compared to CTM.",
                    "label": 1
                },
                {
                    "sent": "The limitation of course is that we need to specify the number of sources Al right now we just doing it arbitrarily to set it 2K or 4K over 5, but we want to be able to maybe automatically determine the number of sources.",
                    "label": 0
                },
                {
                    "sent": "By putting a prior putting us prior over the mixing matrix a an we also interested in exploring other source distribution.",
                    "label": 0
                },
                {
                    "sent": "So that concludes the talk.",
                    "label": 0
                },
                {
                    "sent": "Time for.",
                    "label": 0
                },
                {
                    "sent": "Try again.",
                    "label": 0
                },
                {
                    "sent": "Geolocation compares to this.",
                    "label": 0
                },
                {
                    "sent": "We have to keep them slightly different way of modeling topics on deferred darkening basis right, not really know because we haven't done a comparison.",
                    "label": 0
                },
                {
                    "sent": "But I mean I think to be fair, I think we have to compare our model the with pachinko allocation, with the variational inference implementation, not like Gibbs sampling implementation.",
                    "label": 0
                },
                {
                    "sent": "An algorithm is, you know, in particular because the advantage of having a simple Gibbs sampling algorithm right so right, but then unnecessary.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll be doing we conducting experiments.",
                    "label": 0
                },
                {
                    "sent": "Comparing yeah, OK, sure, yeah, yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know off the top of my head how they would perform, but according to the paper I mean, they said Pachinko performs better than CTM and our model our model is comperable to CTM.",
                    "label": 0
                },
                {
                    "sent": "Maybe they'll perform better.",
                    "label": 0
                },
                {
                    "sent": "Yeah I don't know.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "You are showing your results.",
                    "label": 0
                },
                {
                    "sent": "You were showing that LDA with like 200 topics was heading considerably worse like within alligator 10 topics.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a bit weird.",
                    "label": 0
                },
                {
                    "sent": "In the standard LDA, if you do keep sampling where you integrate over all of your parameters, presumably that shouldn't be happening.",
                    "label": 0
                },
                {
                    "sent": "Dentition why this was happening?",
                    "label": 0
                },
                {
                    "sent": "200 hyperparameters that we doing empirical Bayes on and do some kind of fitting.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Like so, it's pretty consistent.",
                    "label": 0
                },
                {
                    "sent": "Like I we we actually try some other data set besides this data set that we show and also it shows the drop.",
                    "label": 0
                },
                {
                    "sent": "It also shows drop.",
                    "label": 0
                },
                {
                    "sent": "No, we use important sampling to compute the true likelihood.",
                    "label": 0
                },
                {
                    "sent": "So using the proposed posterior distribution as a proposal distribution.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Important sampling.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was just asking why they're doing harmonic mean estimator or estimator workflow.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Are you stealing your hyperparameters topics?",
                    "label": 0
                },
                {
                    "sent": "Oh Alpha alright yeah, actually we fix it to 0.1.",
                    "label": 0
                },
                {
                    "sent": "Language is a powerful quiet, actually increasing because then this morning becomes really big problems with title.",
                    "label": 0
                },
                {
                    "sent": "I see her, I see you will try to hide.",
                    "label": 0
                },
                {
                    "sent": "I found some problems with correlated topic model and with numerical instability in estimating the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Did you encounter that at all?",
                    "label": 0
                },
                {
                    "sent": "And so you think that the simpler factored representation would be more robot?",
                    "label": 0
                },
                {
                    "sent": "So like you mean Sigma is when the estimate of Sigma is not positive definite.",
                    "label": 0
                },
                {
                    "sent": "No, we actually don't see that, but no, no.",
                    "label": 0
                },
                {
                    "sent": "But I would guess that the factor representation.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think so.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I was unclear did using the Laplace source model.",
                    "label": 0
                },
                {
                    "sent": "It helps it helps with the visualization.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause if you use Gaussian source and yeah, but not in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "I mean it's pretty much the same as you can see.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And like in terms of the topic graph that you showed, are there topics that are being threshold that you're not showing links to?",
                    "label": 0
                },
                {
                    "sent": "Yes, not try shouting, so I mean, I just look at each column of A and then pick out the three or four topics that are highly that.",
                    "label": 0
                },
                {
                    "sent": "So I rank I saw it the column away and then pick out three or four topics.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so no threshold.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess they're showing.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah thank you.",
                    "label": 0
                }
            ]
        }
    }
}