{
    "id": "26yyoobqe7tbkthg6ok5pgzyrebiepjr",
    "title": "Boosting with the Logistic Loss is Consistent",
    "info": {
        "author": [
            "Matus Telgarsky, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_telgarsky_loss/",
    "segmentation": [
        [
            "Everybody, so for the next."
        ],
        [
            "Couple minutes, let's talk about Ada boost.",
            "In its original form, and we've just swapped out one thing we've taken exponential loss and plugged in the logistic loss, and so we'd like to say some statistical properties about this and so kind of the knee jerk thing you want to do is, you know, just take one of Nesterov's books off the shelf.",
            "Maybe something by Lugosi plug them together and get the answer.",
            "The problem is that in this case we're using the original algorithm, it's not regularised, and we have no constraints.",
            "We have no strong convexity, no minimizers, no compact level sets.",
            "The usual kinds of convex results.",
            "Don't tell you anything now.",
            "This was an effect resolved by Bartlett and trans couple years ago and then a very new nice proof by Robert Shapiro and his colleagues for the exponential loss.",
            "But there's a new interesting difficulty with logistic and similar Lipschitz losses that we handle in this paper, which is that there could be very bad margin errors and you have a very flat Hessian, so it's almost telling you the wrong thing.",
            "So now there you might have another knee jerk reaction in this case, which is, why should we even consider this problem?",
            "Why don't we just add a regularizer and go home?",
            "So I have a couple of answers to this.",
            "One is you know, people use this algorithm.",
            "I say something about it.",
            "The other thing is that you know, maybe we're regularising wrong.",
            "So there are some cases where without regularization very nice happens.",
            "And like the rates that Rob has for the exponential.",
            "Also very nice, and so you know, maybe there's something maybe a different way to regularize some things that don't need to be penalized.",
            "So there's no way to look at it.",
            "So, but I really like to do.",
            "Here is make some general techniques and you know, maybe highlight some of the structure of the problem.",
            "Not just give like a rate and the way we're going to solve this problem is pretty easy or just going to say, look if we regularize.",
            "We have an explicit algorithmic constraint on how the algorithm behaves if we don't regularize the data constrains the paper algebra of the algorithm, so."
        ],
        [
            "There will be 2 cases I consider so.",
            "But just look at the look at the convex optimization point to solve, but not over finite sample over the distribution.",
            "So just write down this duality formula so this is.",
            "So let's say logistic loss, and I'm going to write out some math just because I want to make sure the form of what I'm talking about is clear.",
            "So in the infinite sample case, this is an integral where new is the source distribution and this is an infinite dimensional problem, because that's how boosting works and the dual for the case logistic loss thing very nice, called the Fermi Dirac interview.",
            "So this is already kind of interesting, because logistic loss is flat, but for me dry entropies is nice, very, very sharply convex thing.",
            "So this gives us a hint that the dual is actually very, very well behaved now.",
            "What are the constraints look like?",
            "This is actually a Max entropy problem and the constraint site is very interesting.",
            "What it does is it decorrelate the constraint says that there's this reweighting of your source distribution, whereby everything is not good anymore.",
            "So it's kind of the opposite of weak learnability.",
            "And this is very useful because what this tells us is that when the optimal value is positive, we have a nontrivial dual optimum, and this dual optimum is a certificate for the difficulty of the problem.",
            "Is the dual often tells us that we can't go in any One Direction, because very quickly we encounter a lot of errors.",
            "And so that's exactly the way to proof goes.",
            "We say look we have no regularization, but the dual often can be used to show us a constraint.",
            "So in this case you can actually prove rigorously, but there is an L1 norm with high probability that exists for the outputs of the algorithm.",
            "And the constant normal are data dependent, but their distribution dependent on the sample.",
            "That's with high probability concentrates.",
            "For this case, I can't get a good rate.",
            "I get emphasized the sample EM to the minus C where CS, hypothesis, class and distribution dependent.",
            "We'd like that to be empty minus 1/2 or the minus 1/4 like that.",
            "I could not do that yet."
        ],
        [
            "There's another case, so I assumed that the dual optimum was positive.",
            "If it's zero, the this is effectively a separable case.",
            "It's been very nice, happens, so this is a fancy trying to pretend I'm smart.",
            "Way of writing the week learning condition and what we can do is we can replace Infinity with a fixed concept cap.",
            "The ways that the week learning assumption can use to construct this value.",
            "And this is not something new.",
            "This is something that's been used by a lot of people in the audience, it's.",
            "Like a smooth version of the week learning rate, but I didn't have to think it just popped out of the dual form and just like with expansion loss and then we learning rate, this thing will indicate the algorithm very quickly decrease its value.",
            "And also this thing sharply concentrates.",
            "Unsurprisingly, the concentration depends on one over epsilon because you have a little bit of math for the infant suit, but it's not that bad and in this case we get a very nice rate, so that's kind of the top."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everybody, so for the next.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple minutes, let's talk about Ada boost.",
                    "label": 0
                },
                {
                    "sent": "In its original form, and we've just swapped out one thing we've taken exponential loss and plugged in the logistic loss, and so we'd like to say some statistical properties about this and so kind of the knee jerk thing you want to do is, you know, just take one of Nesterov's books off the shelf.",
                    "label": 0
                },
                {
                    "sent": "Maybe something by Lugosi plug them together and get the answer.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in this case we're using the original algorithm, it's not regularised, and we have no constraints.",
                    "label": 0
                },
                {
                    "sent": "We have no strong convexity, no minimizers, no compact level sets.",
                    "label": 1
                },
                {
                    "sent": "The usual kinds of convex results.",
                    "label": 0
                },
                {
                    "sent": "Don't tell you anything now.",
                    "label": 0
                },
                {
                    "sent": "This was an effect resolved by Bartlett and trans couple years ago and then a very new nice proof by Robert Shapiro and his colleagues for the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "But there's a new interesting difficulty with logistic and similar Lipschitz losses that we handle in this paper, which is that there could be very bad margin errors and you have a very flat Hessian, so it's almost telling you the wrong thing.",
                    "label": 1
                },
                {
                    "sent": "So now there you might have another knee jerk reaction in this case, which is, why should we even consider this problem?",
                    "label": 0
                },
                {
                    "sent": "Why don't we just add a regularizer and go home?",
                    "label": 0
                },
                {
                    "sent": "So I have a couple of answers to this.",
                    "label": 0
                },
                {
                    "sent": "One is you know, people use this algorithm.",
                    "label": 0
                },
                {
                    "sent": "I say something about it.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that you know, maybe we're regularising wrong.",
                    "label": 0
                },
                {
                    "sent": "So there are some cases where without regularization very nice happens.",
                    "label": 0
                },
                {
                    "sent": "And like the rates that Rob has for the exponential.",
                    "label": 0
                },
                {
                    "sent": "Also very nice, and so you know, maybe there's something maybe a different way to regularize some things that don't need to be penalized.",
                    "label": 0
                },
                {
                    "sent": "So there's no way to look at it.",
                    "label": 0
                },
                {
                    "sent": "So, but I really like to do.",
                    "label": 0
                },
                {
                    "sent": "Here is make some general techniques and you know, maybe highlight some of the structure of the problem.",
                    "label": 0
                },
                {
                    "sent": "Not just give like a rate and the way we're going to solve this problem is pretty easy or just going to say, look if we regularize.",
                    "label": 0
                },
                {
                    "sent": "We have an explicit algorithmic constraint on how the algorithm behaves if we don't regularize the data constrains the paper algebra of the algorithm, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There will be 2 cases I consider so.",
                    "label": 0
                },
                {
                    "sent": "But just look at the look at the convex optimization point to solve, but not over finite sample over the distribution.",
                    "label": 0
                },
                {
                    "sent": "So just write down this duality formula so this is.",
                    "label": 0
                },
                {
                    "sent": "So let's say logistic loss, and I'm going to write out some math just because I want to make sure the form of what I'm talking about is clear.",
                    "label": 0
                },
                {
                    "sent": "So in the infinite sample case, this is an integral where new is the source distribution and this is an infinite dimensional problem, because that's how boosting works and the dual for the case logistic loss thing very nice, called the Fermi Dirac interview.",
                    "label": 0
                },
                {
                    "sent": "So this is already kind of interesting, because logistic loss is flat, but for me dry entropies is nice, very, very sharply convex thing.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a hint that the dual is actually very, very well behaved now.",
                    "label": 0
                },
                {
                    "sent": "What are the constraints look like?",
                    "label": 0
                },
                {
                    "sent": "This is actually a Max entropy problem and the constraint site is very interesting.",
                    "label": 0
                },
                {
                    "sent": "What it does is it decorrelate the constraint says that there's this reweighting of your source distribution, whereby everything is not good anymore.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of the opposite of weak learnability.",
                    "label": 0
                },
                {
                    "sent": "And this is very useful because what this tells us is that when the optimal value is positive, we have a nontrivial dual optimum, and this dual optimum is a certificate for the difficulty of the problem.",
                    "label": 0
                },
                {
                    "sent": "Is the dual often tells us that we can't go in any One Direction, because very quickly we encounter a lot of errors.",
                    "label": 0
                },
                {
                    "sent": "And so that's exactly the way to proof goes.",
                    "label": 0
                },
                {
                    "sent": "We say look we have no regularization, but the dual often can be used to show us a constraint.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can actually prove rigorously, but there is an L1 norm with high probability that exists for the outputs of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the constant normal are data dependent, but their distribution dependent on the sample.",
                    "label": 0
                },
                {
                    "sent": "That's with high probability concentrates.",
                    "label": 0
                },
                {
                    "sent": "For this case, I can't get a good rate.",
                    "label": 0
                },
                {
                    "sent": "I get emphasized the sample EM to the minus C where CS, hypothesis, class and distribution dependent.",
                    "label": 0
                },
                {
                    "sent": "We'd like that to be empty minus 1/2 or the minus 1/4 like that.",
                    "label": 0
                },
                {
                    "sent": "I could not do that yet.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another case, so I assumed that the dual optimum was positive.",
                    "label": 0
                },
                {
                    "sent": "If it's zero, the this is effectively a separable case.",
                    "label": 0
                },
                {
                    "sent": "It's been very nice, happens, so this is a fancy trying to pretend I'm smart.",
                    "label": 0
                },
                {
                    "sent": "Way of writing the week learning condition and what we can do is we can replace Infinity with a fixed concept cap.",
                    "label": 0
                },
                {
                    "sent": "The ways that the week learning assumption can use to construct this value.",
                    "label": 0
                },
                {
                    "sent": "And this is not something new.",
                    "label": 0
                },
                {
                    "sent": "This is something that's been used by a lot of people in the audience, it's.",
                    "label": 0
                },
                {
                    "sent": "Like a smooth version of the week learning rate, but I didn't have to think it just popped out of the dual form and just like with expansion loss and then we learning rate, this thing will indicate the algorithm very quickly decrease its value.",
                    "label": 0
                },
                {
                    "sent": "And also this thing sharply concentrates.",
                    "label": 0
                },
                {
                    "sent": "Unsurprisingly, the concentration depends on one over epsilon because you have a little bit of math for the infant suit, but it's not that bad and in this case we get a very nice rate, so that's kind of the top.",
                    "label": 0
                }
            ]
        }
    }
}