{
    "id": "g4b77gwt3jzngsudhnszf54mocgfac2o",
    "title": "Functional Analysis in Data Modelling",
    "info": {
        "author": [
            "Tony Dodd, Department of Molecular Biology and Biotechnology, University of Sheffield"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Data Modeling"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_dodd_fadm/",
    "segmentation": [
        [
            "So.",
            "I think we'll get started.",
            "Today is all about kernel methods, so I'm sure a lot of you are familiar with support vector machines, or at least heard of support vector machines.",
            "On the other, related kernel methods we have.",
            "Hopefully Colin Campbell will be turning up shortly, and he's going to do the bulk of today.",
            "On support vector machines and kernel methods, what I'm going to do to start off with is.",
            "We're going to jump from yesterday's fairly gentle introduction with not very much maths into today where.",
            "There's quite a bit of maths in my first talk, so don't worry if you get lost.",
            "We are going to do.",
            "It's the idea of this talk is sort of give you an idea of where functional analysis is important in data modeling, but most of all I can do is try and introduce you.",
            "So some of the basics of functional analysis.",
            "OK, I'll try to highlight why it's important along the way.",
            "Forgive me if I.",
            "Don't get the mathematics 100% correct because I'm and I'm an engineer and don't care about some of the nicer CS.",
            "On in functional analysis and again as we go through, you'll see some of those things.",
            "There are some very technical things which I'm certainly not going to the proof of, and partly 'cause I don't understand them.",
            "But this is not the time to do it here, and it's not important for our purposes, but I think it's it's a useful thing for you to be familiar with and aware of, because functional analysis is.",
            "The backbone of kernel methods, or it's one of the backbones of kernel methods?",
            "And also it's actually closely related to Gaussian processes, which you'll learn about's on Thursday.",
            "I doubt very much anyone on Thursday will do anything on the functional analysis and Hilbert spaces of Gaussian processes, but there is actually a whole body of literature of people who've developed Garrison process is from a purely functional analysis perspective.",
            "Exactly, people like Grace Wahba.",
            "He's done a lot of work on splines.",
            "Everything she does is very much from Hilbert spaces, and the conclusion of what I'm going to do today in this short lecture is reproducing kernel Hilbert spaces, and you'll certainly hear about those OK if not during this week, when you're reading about kernel methods."
        ],
        [
            "So again, ask questions as we go through.",
            "So we're going to start off by talking about metric spaces, and then we're going to go into linear spaces, and then three types of linear spaces.",
            "So we're going to put norms on them in a product, and then we'll finally get to Hilbert spaces.",
            "Then we'll get into where it's perhaps a bit more useful for you lots and will briefly go through what is meant by best approximation in Hilbert spaces.",
            "I will then very very briefly introduce reproducing kernel Hilbert spaces, and this really is very brief.",
            "I'm going to miss out quite a lot on that.",
            "And then my final slide sort of follows on from some of the issues that were raised yesterday and I'm going to talk briefly about approximation versus estimation and this sort of relates back to some of the issues we talked about yesterday in terms of choice of basis functions and things like that.",
            "I want to show you this is quite a nice picture."
        ],
        [
            "Which represents that.",
            "OK, so.",
            "What we look at.",
            "For the next three calls, an hour, so are spaces.",
            "OK, so now.",
            "What we've got is some.",
            "Big blob OK Sweden.",
            "Note my F, which is some space of functions.",
            "Can we come on to what they may want?",
            "That space may be on the next slide and within that space we denote.",
            "The individual functions my points.",
            "OK, so functions now become points.",
            "In some space.",
            "So whereas yesterday functions we are representing as a linear superposition of basis functions, can we had to worry about parameters on all this sort thing and we'll come back to that in a minute.",
            "But the key thing here is that we have some space of functions and we represent the individual functions within that space by points or elements.",
            "OK."
        ],
        [
            "Now, what can those spaces be?",
            "Could be anything.",
            "The simple.",
            "Well, I suppose the simplest would be the real line.",
            "OK, in which case they're not really functions, they are literally just numbers, but numbers are the simplest type of function you can get if you want to consider it that way.",
            "It could be Euclidean space.",
            "So after the D OK, in which case now the points are vectors are still not quite functions.",
            "But that's that's the space that you're most familiar with.",
            "Typically and intuitively you will understand Euclidean space very well, and as we go through one of the other types of spaces, I'm going to try and think about the terms I'm going to talk about in terms of Euclidean space, because you will be familiar with most of the terms I'm going to come across in terms of Euclidean space, and actually what we're doing is we're moving towards Hilbert spaces.",
            "And the reason we work with Hilbert spaces partly is.",
            "In practice, they are very, very useful anyway, but also intuitively they're very nice because they're very, very similar, so Euclidean space in terms of all of our understanding of Euclidean space, we can pretty much apply that to Hilbert spaces.",
            "And you can actually.",
            "Source of think of functions.",
            "In something Hilbert space as being infinitely long vectors and that may help you in terms of thinking about these things, but we get more interesting spaces, so there's sort of 4A space is the space of all sines and cosines.",
            "So any function that can be made up of supervision of sines and cosines that would be a specific space of functions, an another quite interesting one is the space of bands, limited functions.",
            "So anyone who knows anything about signal processing when we're doing band limited signal reconstruction, that's known as the pay Livina space, and that could be a space of functions which has.",
            "Certain frequency characteristics.",
            "R L2.",
            "Very common one is the space of square integrable integrable functions suddenly get slightly more technical now in terms of what these things are, but again, that's a very another very well known space of functions.",
            "These all happen to be linear spaces, which will come on.",
            "So in a minute, but in the very general case Aspace functions could be nonlinear.",
            "OK, so it could be a nonlinear space of functions and we're not going to worry about those.",
            "And unless you're doing a PhD in pure mathematics.",
            "I wouldn't worry about nonlinear spaces of functions.",
            "OK, this is really really heavy duty stuff.",
            "But the interesting point is what we're going to be looking at today are linear spaces."
        ],
        [
            "But although.",
            "This is a linear space of functions.",
            "The actual functions within this space.",
            "Can themselves be nonlinear?",
            "So think of things like sines, cosines.",
            "They are non linear functions.",
            "OK in terms of their nonlinear in with respect to the inputs.",
            "So.",
            "A space being linear means you can still have nonlinear functions.",
            "The points within it can themselves be nonlinear, so there is a difference in the definition of what linear and nonlinear means here.",
            "OK."
        ],
        [
            "And that should become clearer in a minute.",
            "OK, so the first type of space that we need to concern ourselves and what we're doing here is we're progressively going to be putting more and more structure onto our spaces, so we're going to start with metric spaces, and we're going to norm space, lend linear, then norms, then inner products in Hilbert spaces and what we're doing each stage is we're putting a little bit more structure onto that space, so essentially restricting it in some way to make it nicer.",
            "Um?",
            "So in the very general case, we can define a metric and all the metric is is if you've got some space of functions and you got two points within that space.",
            "So two functions F1 and F2.",
            "We define some distance.",
            "Between those two functions OK. And.",
            "Metric spaces don't have to be linear.",
            "OK, we've not said anything about linearity here, so this is a very, very general concept, and when we talk about metric spaces there also known as topological spaces, and we're looking at these are algebraic concepts, and this is the stuff that I really do find quite difficult.",
            "Some of the technical stuff on algebraic spaces is beyond me, so I'm not going to go into too much detail, but this concept of a distance between two functions is.",
            "Very useful, can anyone tell me?",
            "What should be the characteristics of our metric for it to be a distance?",
            "Yep.",
            "Anything else?",
            "Yes, one other.",
            "The most."
        ],
        [
            "Yeah.",
            "So.",
            "What we do is when we, in terms of defining a metric space, is literally a set of axioms which define what that space is.",
            "So we decide or someone has decided what constitutes a metric space, and these are all things that will be very very familiar to us.",
            "So as being said.",
            "We define some distance on and it Maps from F cross F, so that simply means it's defines on two different functions, so it's distance between two things and it Maps to the real line, so it's a real number.",
            "That real number happens to be greater than or equal to 0, so you don't get negative distances.",
            "Um?",
            "It is only ever equal to 0.",
            "If your two functions are identical.",
            "It is symmetric, so the distance between F1 and F2 is the same as the distance between F2 and F1.",
            "And finally we get triangular triangle inequality which all that saying.",
            "Is it that distance there is greater than?",
            "Is less than sorry?",
            "Some of those two, those two distances basically Pythagoras Theorem or when it's not Pythagoras theorem, but it's related to that sort of thing is what we're familiar with when we're looking at triangles and standard geometry.",
            "So.",
            "You know, although you may not come across metric spaces before in the abstract sense, these should all be very familiar ideas.",
            "OK, and it's pretty intuitive what's going on here?",
            "Say that again.",
            "They will have what if user go into looking at the history of maths and the history of the development of these things.",
            "Then typically what will happen is yeah, I know who.",
            "Who invented or who work with Euclidean space?",
            "But it's probably the sort of Egyptians when they're building pyramids or something, and they're doing working purely with Euclidean space, and then probably in the 17th, 18th, 19th century.",
            "Someone smart will come along and they needed to work with some more abstract mathematical concepts.",
            "And yeah, what they would have done is they will have looks at Euclidean space and how we define things when you clear space and will have generalized it for these cases.",
            "But I mean, Euclidean space is obviously an example.",
            "Of a metric space, but is far far more general than that, because this could be a nonlinear space.",
            "Um?",
            "Yeah.",
            "Any other questions?"
        ],
        [
            "OK, so why are metric spaces important?",
            "And we've really not defined a huge amount of structure on them at the moment.",
            "Well.",
            "Like the first sentence is pretty obvious.",
            "It allows us to define distance between functions.",
            "As a result of that, we can look at or how similar are two functions, and if we're looking at doing approximations or estimating functions, knowing how to measure the distance between all the similarity between two functions is obviously very, very important.",
            "Result price will lead us onto the concepts of what is a best approximation.",
            "So you want to house how close things are to each other.",
            "OK, so that's why they're important.",
            "This is one of the technical things.",
            "There's another concept called Completeness Within metric Spaces, which we're going to need later on for Hilbert spaces.",
            "I'm not going to get into the technical bit of that, but my simple way of thinking of it is that there are no holes in the space.",
            "OK, it's all to do with convergence of functions.",
            "Basically, says if you got a sequence of functions.",
            "They will converge.",
            "That sequence of functions will converge to a point within the metric space.",
            "OK, it's a relatively simple example that is on the real line if you take.",
            "The sequence of numbers which are equal to 1 / N. OK, then what does 1 / N converge to as N increases?",
            "0.",
            "OK, so if you include zero in your space.",
            "So if you include the real line from zero, zero and all positive numbers, then it will be a metric space if you exclude as it would be a complete space if you exclude 0.",
            "And it's no longer a complete space because that sequence convergent sequence converges to a number that doesn't belong to the space.",
            "OK, and this is where it gets a bit abstracts, so we don't need to worry too much with basic simple way of thinking about it is that there are no holes in the space.",
            "OK, so no missing functions effectively.",
            "This is where I mean.",
            "This things are pretty abstract here.",
            "Well, yeah, I'm not.",
            "That was an example in terms of something more practical.",
            "We will never need to worry about it.",
            "OK. Or you will never need to worry about that.",
            "I can't.",
            "Off that was what it's all to do with convergent sequences of functions.",
            "No, 'cause it's not it.",
            "That's getting too far for the purposes of today.",
            "Just think about it in terms of there are no holes in the space.",
            "If you want to know what it is, I can go and get a book later on for you and show you that the simple thing is just going to look at a book on functional analysis.",
            "Not something that you need to worry too much about, but it's something 'cause I'm going to use this term completeness in a minute and it'll probably annoy you even more that you don't know what it means in a minute when I really use it.",
            "OK, so, but you just need to be aware of that.",
            "Um?",
            "And what we're going to progress on now is we want to look at spaces that are very similar to Euclidean space because of us said before, we're very familiar with Euclidean space.",
            "You know, intuitively we.",
            "We know what's going on.",
            "OK, so let's try and construct."
        ],
        [
            "Spaces that are like Euclidean space.",
            "So the first thing.",
            "We're going to restrict ourselves to now are linear spaces OK?",
            "Instead, define a linear space.",
            "We need 2 algebraic operations, so we need addition and we need multiplication by scalars.",
            "And basically all addition means is that if we if we sum 2 functions in our space then that new function will also belong to the space.",
            "OK, and that's something you'd expect to be quite reasonable.",
            "OK, so if you sum 2 functions it with that, that new function will belong to this space, it doesn't.",
            "Fall outside the space of functions.",
            "And we also get this sort of rule that it doesn't really matter in which order you do the summations.",
            "Um, according to both of these two things here, and again, that's something you'd expect to be true.",
            "And the other thing is that we can multiply our functions by scalars.",
            "And again, that new function forms by that multiplication will again still belong to the space.",
            "And this becomes particularly important later on when we look at basis bases basis functions in our space and trying to construct functions.",
            "And again we get all of these rules, which again we would expect pretty intuitive.",
            "OK, in terms of multiplying by summations of scalars and scalars by summations of functions.",
            "So all of those are they fairly straightforward."
        ],
        [
            "OK.",
            "So the next time we go on to now is defining.",
            "A basis for our.",
            "Linear space and the first thing we need to say is if we've got a set of functions in our space, F1 to FM.",
            "Then they are linearly independent.",
            "If we take any arbitrary summation of them.",
            "And that is only ever equal to 0 if.",
            "All of the coefficients are equal to 0.",
            "Is not familiar from vectors.",
            "So that's exactly the same definition you will come across when we're looking at vectors.",
            "OK, so for a set of vectors to be linearly independent, exactly the same.",
            "So just think of these as infinitely long vectors.",
            "If you want to.",
            "Now space functions can be infinite dimensional and often will be infinite dimensional.",
            "But it'll also often be finite dimensional, and certainly when we're doing approximations with finite number of data points, our approximation space will typically be in the end finite dimensional.",
            "On a space is finite dimensional.",
            "Of dimension N if there are only N linearly independent functions or elements within it.",
            "So effectively, if our space was made up of only these F1 to FN and they were linearly independent, then that would be a finite dimensional space.",
            "Check.",
            "Possible values of Alpha.",
            "And don't don't think of.",
            "Don't think of realizations of the functions.",
            "Think of the functions as functions.",
            "So your your honor at safe you've got cause of X.",
            "If you plug in the value of X at the moment we're not.",
            "So that's one of the other things we're doing here is we're not worrying about an input, so when.",
            "Don't think of functions in sort of cause of X.",
            "Think of it more as cause of brackets, something.",
            "OK, so we're never evaluating.",
            "That function.",
            "This is why you should try and think of them as infinitely long vectors.",
            "Each function does that.",
            "Silver puzzles yes.",
            "I mean if yes if well know in in what this is saying is if you if you sum together.",
            "This set of N functions in your space.",
            "If those functions are linearly independent, that summation will only ever be equal to 0 if.",
            "All of the Alpha rise or equal to 0.",
            "If any of these Alpha rise are non zero and that will be non 0.",
            "Zero function yeah yes.",
            "Yeah, I think yeah, that's that's that's a good point actually.",
            "So yeah, that's not zero the number.",
            "So yeah, so for example, if this is like causes and signs defined over.",
            "Minus \u03c0 to \u03c0, then zero would be the zero function from minus \u03c0 to \u03c0. Yeah.",
            "OK, so it's finite dimensional if there are only N linearly independent elements within the space.",
            "We can then define what's known as a linear manifold within our space by simply.",
            "Adding together weighted sums of our functions.",
            "So we're sort of starting to define planes of functions within the space now.",
            "OK, so this is very similar, so if you were looking in.",
            "Euclidean space and user add together vectors within your Euclidean space you would form planes within that space.",
            "For basis OK, we can express every function in F. In this form here.",
            "So if our FS form a basis, then we can represent any function F in our space.",
            "By some linear superposition.",
            "Of a linearly independent set of functions in that space.",
            "OK, so we're starting now.",
            "It's starting to look sort of familiar in terms of basis function models that we looked at yesterday.",
            "OK, so whenever we're looking at sort of things like radial basis functions that we looked at yesterday.",
            "And when we look at kernel methods later on.",
            "They will all be representable in this form, where the F's.",
            "Have a particular form.",
            "Yes, yes, so the end must be yes or Ashes at that end should be finite dimensional.",
            "Yep.",
            "Technical definition.",
            "Eh, Yeah, I think as far as your concerns, don't worry about the difference too much.",
            "But there will be a subtle technical difference between the two."
        ],
        [
            "OK, so.",
            "If you can see this, so in effect your basis.",
            "So if we have three, So what we now start doing is rather than when we're thinking of a basis.",
            "Rather than thinking of our F1F2 and F3 is points.",
            "We now have to think of them, sort of as vectors with respect to zero in the same way in Euclidean space.",
            "That you can think of X is being points in Euclidean space.",
            "But when you think of them as being a basis, you have to think of them as vectors with respect to 0.",
            "OK, so then when you sum these together you can form some new function F. OK."
        ],
        [
            "Our next restriction is to think about norms spaces, so everything we're now taught for everything from now on, it's always a linear space.",
            "OK, so whenever I took norms, inner product Hilbert spaces, these are all linear spaces, but just adding a bit more structure.",
            "At each time.",
            "So the idea of normed spaces is that we define the notion of the size of an element in F. So we need to define what's meant by the size of a function or a point in our space.",
            "And again, I would hope so that all the leaves are fairly familiar.",
            "From standard Euclidean space, so our norm must be greater than equal to 0.",
            "It will only ever be equal to 0 if the function is equal to 0 and if the function is equal to 0 and norm will be 0.",
            "If you multiply your function by some constants.",
            "OK, that constant could be negative.",
            "Then then you not the norm of that constant times.",
            "The function will be equal to the absolute value of the constants times the normal function.",
            "And again.",
            "We get a triangle inequality.",
            "As well.",
            "Yep.",
            "Yeah, so I got him.",
            "Yeah, I'm perhaps here I should be using a different notation for O for a function 0.",
            "But I think so whenever, if, whenever, until well, it should be fairly clear whenever I'm talking about a number and whenever if it's an F = 0.",
            "Or some set of functions being set equals zero, then think of it as being a zero function.",
            "OK, but if it's if it's a norm or a metric.",
            "That's equal to 0.",
            "Then it will be the number.",
            "And then the interesting points is that our norm.",
            "We can always define a metric.",
            "Based on our norm and we can define.",
            "So if we've got some norm in our space, then we can take the norm of the difference between two functions and that will define a metric.",
            "So we can therefore make a normed space A metric space, so we can.",
            "Now we can keep this concept of the distance between two functions.",
            "We're not just looking at the norm of a single function, we can look at the norm of the difference between two functions and that gives us a metric.",
            "One of the interesting things, and again, I'm not going to get into this, is that.",
            "You can sometimes have different metrics, so you can.",
            "You can have what you think of as being the same space.",
            "But you can actually define different metrics and different norms for what?",
            "In theory you think of as being the same space of functions.",
            "OK, but if so, say you've got a set of sines and cosines.",
            "OK, those are the elements in your space.",
            "If you define some metric D1 and some metric D2.",
            "Those two metric spaces are actually different.",
            "Which is quite an old concept.",
            "OK, but I get.",
            "How can we think one on the train is a one on the real on the classic one on the real line would be that the metric is literally just the absolute difference.",
            "Between two points.",
            "But I'm sure there must be.",
            "Quite often they do.",
            "We almost over L2 is this.",
            "That's what it comes on to in a product that we're going to do in a minute.",
            "But yeah, this is.",
            "Getting a bit technical and.",
            "Beyond my knowledge on that one.",
            "Space.",
            "Square square.",
            "Yeah.",
            "But they will be different spaces although.",
            "Yeah, you probably think of them as being the same space, but if if you define a different metric they become different metric spaces."
        ],
        [
            "OK um.",
            "We've defined here sort of a number of algebraic and geometric concepts.",
            "So.",
            "The metric OK by defining the metric based on the norm.",
            "What we've done is we've combined.",
            "The algebraic concepts which is based around metrics.",
            "OK, so metric metric space is all about algebraic and topological concepts.",
            "Whereas when we start some linear enormous spaces, we're now talk about geometric properties.",
            "Predominantly so by defining the metric in the way that we've just done, we're starting to combine algebraic and geometric ideas.",
            "OK, so these can be considered sort of initially as two different schools of mathematics or two different areas of mathematics, geometry and algebra.",
            "And actually we're now starting to bring those together, and this is where things gets useful.",
            "OK, so any book on functional analysis will also have a load of stuff on this or the algebraic ideas or topological.",
            "Concepts as well.",
            "I just said the algebraic.",
            "To me, these are the technical bits.",
            "Things like completeness that I think are hard.",
            "So don't worry too much about.",
            "But often their concepts that are needed for us.",
            "OK, they do make the spaces nice and workable.",
            "We then things when we get into linear spaces with signs out much more about geometric things and to me this is the much more intuitive stuff because we've been brought up from a very young age to be very familiar with geometry and what's going on there, so we tend to underst."
        ],
        [
            "A little bit more.",
            "OK, our next restriction.",
            "So we started off with metric spaces.",
            "Which could be linear or nonlinear.",
            "We then went on and said, well, let's introduce linear spaces to put some real structure on our space.",
            "That's a pretty severe restriction to make them linear in terms of if you think of the space of all functions linear and nonlinear and linear is also with pretty small subsets, but Fortunately from us.",
            "It's a very very useful.",
            "Subset of functions.",
            "We then define the norm, so the next thing we can do is to define an inner product, and again this should all be familiar even if the notation isn't familiar.",
            "This should be familiar from vectors again, so investors are inner products is simply.",
            "If you got two vectors X&Y&X, transpose Y is our inner products and we can do a very similar thing.",
            "So we denote an inner products by these angle brackets.",
            "And the dot, the two dots are two functions, so it's taking inner products between say F2 and F1.",
            "This is how it knows that and again, it Maps from F cross F2.",
            "The rail line.",
            "OK, our inner products will have the properties again.",
            "It will be symmetric.",
            "Again, if we multiply functions by constant values.",
            "Then and.",
            "Inner product with another function.",
            "Then we can take the constants outside the inner products.",
            "Note here there's no concept of absolute value, so if these constants are negative, this will still be negative.",
            "OK, so inner products can obviously be positive or negative or positive or negative.",
            "However, if you take the inner products.",
            "Of a function with itself, then that will always be greater than, equal to 0, and again equality if and only if F = 0.",
            "Weather last where?",
            "Oh sat, three should be inside there at three.",
            "Should be inside that, yes?",
            "Sometimes, actually now you know you said that that's often.",
            "You will see in a product with something outside and that denotes the Hilbert space or the space for which that inner product is associated.",
            "So that really is a very unfortunate.",
            "Mistake there.",
            "We can then define a norm from our inner products so the norm on F is simply equal to the square roots of the inner product of F with itself.",
            "And again, that should be very, very familiar that one of the definitions of the norm of avexa is simply the square roots of, for example, X transpose X. Yeah.",
            "I can you be fine or without having an ocean open inner product?",
            "Yes.",
            "Yeah, I mean all of these things.",
            "So what we're doing is we're adding more and more structure.",
            "So yes, you can have soo metric spaces don't have to be linear spaces and then norm spaces don't have to be inner product spaces.",
            "OK, so you can have and don't ask for examples.",
            "So yeah, exactly right.",
            "There's a question over.",
            "No.",
            "Every guest.",
            "Space.",
            "No no.",
            "So what we're doing is just building a big array of tools, so it seems like when we come up with some sort of problem.",
            "Yeah, I'm where I'm getting, so I mean I'm.",
            "No, what what we're leading, so eventually we come to very shortly as Hilbert space is an really all we ever all you lot are ever likely to work with our Hilbert spaces and even those you probably won't be aware that you're working in Hilbert spaces.",
            "OK, so all the stuff that's going to be talked about on kernel methods and I don't know if Colin's going to.",
            "Particular you can refer to reproducing kernel Hilbert spaces.",
            "Yeah, but the reason I'm I'm doing this is if you do work in kernel methods.",
            "There is every possibility that you will read papers that will talk about reproducing kernel Hilbert spaces.",
            "And it's therefore useful that you're at least familiar with the concepts behind those.",
            "Um, so it's more familiarity, and certainly when I was learning about Hilbert spaces, and as you know, as an engineer, you never get taught this stuff.",
            "You only ever learn it when you're doing it.",
            "Reese doing research and I sort of self taught myself and a lot of this stuff.",
            "It's it comes about through familiarity.",
            "You know, I would argue that my understanding of Hilbert spaces came more from familiarity with the ideas than really a deep.",
            "Knowledge and understanding of what's going on.",
            "And that you know comes across because This is why I can't give loads of examples of different things, because, you know, as engineers, we don't really need to know this stuff and we don't need to know the real technical details.",
            "OK, 'cause we can actually do an awful lot, but it's nice for you to have.",
            "Some background knowledge to Hilbert spaces so that when you come across them when you're reading papers you know you are aware and you don't, you know as a researcher you will come across reproducing kernel Hilbert spaces and you'll think, oh, I really ought to know about these and then you'll end up spending another two months reading some book on functional analysis.",
            "And actually, you come to the end of that and realized, well, I don't really know.",
            "Need to know all of that in there.",
            "Um?",
            "Design.",
            "Explain why we're doing this.",
            "Say that again.",
            "It's just so if you got two vectors, if you got a got two vectors X&Y, then the inner product is X transpose Y.",
            "No, it's actually this OK.",
            "The thing that the best thing, the things that that is useful that inner product gives you is the notion of orthogonality.",
            "So two functions are orthogonal if their inner product is zero, and we're going to use that in a minute.",
            "So actually that's in terms of approximation.",
            "That's probably the most useful thing that inner product gives you that nothing else has given you up until now.",
            "What?",
            "Yeah.",
            "So you know, yeah, you can see if you've got two functions will be orthogonal if the inner product is 0, but if it's non zero it's giving you some idea of.",
            "The angle between those functions?",
            "Yeah, thanks for that.",
            "OK, so we've defined a norm based on our inner products and obviously from that norm you can go further back and define a metric.",
            "So in affects our inner product.",
            "Is also defined a metric if you want to."
        ],
        [
            "Right Hilbert spaces.",
            "And this is where we come back to completeness.",
            "And This is why I had to briefly talk about it earlier.",
            "And for me the best way of thinking about Hilbert spaces is these really do look like.",
            "Euclidean space.",
            "OK, so in terms of the development of Hilbert spaces, you can really pretty much in practical purposes we can pretty much think of Hilbert spaces.",
            "As like Euclidian space, except there now spaces over functions.",
            "And the only thing that we now add to the inner product spaces we make it complete.",
            "So very, very crudely, think of a Hilbert space is an inner product space with no holes in it.",
            "I know, yeah, that's not terribly good.",
            "Explanation but that should be good enough for you.",
            "Again, in practice we normally we would normally we rarely work with purely inner product spaces.",
            "You would normally work with a complete inner product space, so actually you don't really need to worry too much about this concept.",
            "But again is important because there are some spaces that aren't complete and that can cause problems 'cause you may do some function approximation and the function.",
            "That you approximate actually doesn't belong to the space.",
            "And mathematically, that could cause problems.",
            "In practice, it won't for you lots.",
            "OK so again in simple terms, Hilbert spaces have all the nice magical properties, mathematical properties that we need.",
            "So actually we don't really need to worry about all the complicated bits.",
            "There is something you may come across, something called benack spaces, BANACH and Bunac spaces are complete normed spaces.",
            "So when you get to norm spaces, you can go off in One Direction, have a complete norm space is known as a backspace, or you can carry on from norm spaces to go to inner product spaces and then complete inner product spaces are called Hilbert spaces.",
            "Norm space."
        ],
        [
            "They are very important within function approximation as well.",
            "OK, I need to speed up a bit.",
            "We're not going to get into the idea of best approximation.",
            "It's what we've now got.",
            "Some space of functions F and some subspace, and I've not really defined what subspace is, but just think of it as a space within this bigger space.",
            "It's a restriction.",
            "So we've got some subspace H and we have some function in our bigger space and we want to approximate it in our smaller space.",
            "OK, and this is what function approximation is all about.",
            "In terms of the sort of."
        ],
        [
            "Functional analysis perspective.",
            "So for any given F in our.",
            "Speaker Hilbert space F and.",
            "A closed subspace H which is within F and again don't worry what closed means.",
            "There always exists a unique best approximation F hat to F. And we use this word terminology out of H. A basic what this is saying is that there will always be.",
            "A best of a unique best approximation to our function within that subspace.",
            "OK, so it's just an an existence theory.",
            "And there's proofs.",
            "To do that?",
            "And what we can actually do is our space F. We can actually decompose into H. And.",
            "The orthogonal complement of H, So effects F here becomes equal to H. Plus this with summation, and this is the orthogonal complement of H. And we'll see why that's useful in a minute.",
            "And each when we come on to writing equations and what we're saying here, therefore, is the difference between F&F hat.",
            "So again, this is a different function that we think it's a little error function.",
            "That's error function will be orthogonal to our subspace.",
            "Questions.",
            "That's how we're going to do it.",
            "You could.",
            "This is why we've gone on to do so.",
            "Hilbert spaces 'cause we're going to do in terms of inner product, you can do best approximation in normal spaces.",
            "But yes, we're going to use this concept, and this comes back to.",
            "Watch AJ was asking about in terms of wire Hilbert space and stuff to important and hear the inner product is important because we're going to use that to form our best approximation based on orthogonality.",
            "We will."
        ],
        [
            "End up getting some equation that should become familiar with to you in the end.",
            "What we're going to do is, we assume H. So our subspace is finite dimensional and it has a basis K1K2K M and I very deliberately used the notation K because hopefully column we use kace later on.",
            "Perhaps not.",
            "We use kace typically just a note kernels within kernel space is OK.",
            "So what we're saying is that F hat can be represented as some linear superposition superposition.",
            "Of these basis or kernel functions, if we think about reproducing kernel Hilbert spaces.",
            "The condition that F -- F hats must be orthogonal's H gives us M conditions.",
            "And these same conditions are that.",
            "Since H itself is made up of these K eyes, if we take the inner products between each of our basis functions.",
            "With the difference between.",
            "Our true F minus.",
            "RF hats, all of those will equal 0.",
            "And we can simply.",
            "We can expand this out using the properties we did before on products.",
            "And we actually end up with this set of M conditions here.",
            "So it's M because we've got this Ki in each case.",
            "So for each Ki will be a separate equation, so we effectively we end up with a set of simultaneous equations that need to be solved.",
            "Is everyone understands?",
            "Flat.",
            "Yes yeah yeah.",
            "Yeah.",
            "Yeah.",
            "And we."
        ],
        [
            "Might become of very quickly.",
            "So at reproducing kernel Hilbert spaces and this is where we will end up with an equation which you can work with, reproducing kernel Hilbert spaces.",
            "Again, this is a further sort of restriction to Hilbert spaces.",
            "That's a particular class of Hilbert space, which is very, very important in machine learning.",
            "Can we source of as one of the theoretical foundations four or kernel methods?",
            "1 problems with kernel methods?",
            "As you find out during the week.",
            "Is that there are lots of different theoretical foundations, OK, of which Hilbert space is only one, and you probably won't hear too much more about this.",
            "OK, but it is one of the theoretical foundations and things like splines, kernel machines, Portland machines near some types of neural networks.",
            "National process is an even allow.",
            "The earlier work was done on Time series analysis, so people like Parsinen Kailath develops a load of reproducing kernel Hilbert space stuff for doing time series analysis.",
            "Again, I mentioned band names of functions.",
            "All of these.",
            "Can be insert proceeds in terms of reproducing kernel Hilbert spaces, which is a very very powerful idea.",
            "OK, and it can be applied to lots."
        ],
        [
            "Lots of different types of function approximation.",
            "So it's it's a Hilbert space with yet more structure associated with it.",
            "We're not going to worry about the technical details here, but we do need to worry about a couple of properties one of them.",
            "Is that we can represent observations.",
            "So now this is our observations, why I as the inner products?",
            "Of our kernel or basis functions with F. And this is particularly particularly useful property which we use on the next slide.",
            "And the other interesting thing is that we now need to worry about.",
            "These kernels are defined on some input space and if we take the inner product between two kernels or basis functions, kin KJ then actually that is equal to the kernel evaluated on XI and XJ.",
            "So the kernel becomes a bivariate function.",
            "I'm really I'm skipping over technical detail here.",
            "I'm afraid, um.",
            "And.",
            "These kernels are positive definite functions."
        ],
        [
            "We can use that to rewrite our M conditions, so here this was on the previous slide that was the inner."
        ],
        [
            "Products.",
            "Between K, INF and now, that becomes simply."
        ],
        [
            "Our observation.",
            "Use of that."
        ],
        [
            "So we now ends up with an inner products between the kernel functions can now represent this K of XI with X one X2X3 and so on.",
            "Where these are input data points.",
            "So we now have our M conditions.",
            "Become this set of equations.",
            "And what we can actually now do is write this in a vector matrix form, so we end up with.",
            "Why?",
            "Minus some matrix K which is formed by.",
            "These kernel evaluations times Alpha, so rearranging it, we get our coefficients or parameters are equal to the inverse of K. Times Y.",
            "And K will be invertible.",
            "Provides is.",
            "The RX eyes are distinct if they use their unique in the sense that there aren't any coincidence input points, I think because K in this case will always be square, so it will be invertible.",
            "What's?",
            "No, no, you just have.",
            "These are just observations, so going back to what we're yesterday are.",
            "Why eyes are just.",
            "Observed values don't think about states.",
            "You're just confusing things.",
            "It's just, it's just the output of your system.",
            "Oh yeah, it's the output of your function.",
            "It's you are evaluating the function as a number of points.",
            "So going back to yesterday.",
            "To do.",
            "To do neural networks you have to have a set of input output observations, didn't you?",
            "So these why eyes on this vector Y is just your vector of outputs observations.",
            "These axes are inputs.",
            "They're not states or anything that those are just inputs.",
            "Yes K, because these kernels are these kernels are symmetric and positive definite, so K will also be symmetric and positive definite.",
            "It will be well.",
            "It won't always be able conditions, but I can guarantee it virtually always will be able conditions hence.",
            "In practice, I've said it can be, and often it will be L conditions and also there will be noise on your observations, typically in to cope with both of these things.",
            "What we actually do is we minimize.",
            "Summation of our sum of squared errors.",
            "And again, I slightly abuse notation 'cause I've now used Zeds in here in that partition wise.",
            "And then we add on some regularization and now the regularization.",
            "Is on the norm of F. So what we're trying to do here is we're trying to restrict the size of our function.",
            "And effectively, what that does?",
            "Is it forces a preference for smooth functions?",
            "So very crudely, the normal function is related to how wiggly or how smooth that function is.",
            "The smoother function is.",
            "Typically the norm will be smaller.",
            "The more we clear function is, the bigger the norm will be, that's.",
            "So not strictly correct, but good enough for here.",
            "And if you solve this equation here, where Lambda is greater, greater, equal, 0, now our coefficients are equal to the inverse of our kernel matrix plus Lambda I.",
            "Times are observations.",
            "Yep.",
            "So the function is in this bigger space.",
            "Space.",
            "That's right, yes.",
            "Yeah it will be, yeah.",
            "In effect, in affect our big space is like a is of all possible functions.",
            "RH will be a finite dimensional subspace which is only made up of the K1.",
            "So the subspace is made up of this basis, so it's a restricted form.",
            "OK so it will have the same.",
            "Inner products.",
            "So it's like it's a finite dimensional subspace of the biggest space.",
            "Any other questions?",
            "I already have.",
            "I apologize, this has been.",
            "Very, very brief.",
            "You really need this in the whole day.",
            "You need to go through this properly.",
            "This one.",
            "Their independence.",
            "To be basis, they have to be independent, don't have to be orthogonal.",
            "And typically they won't be orthogonal.",
            "Yep.",
            "If K was a diagonal matrix that would only arise if your kernels were orthogonal.",
            "You can.",
            "I mean you can have and there are lots of orthogonal basis functions.",
            "People do use them.",
            "We don't in this case normally.",
            "Yes.",
            "Hi.",
            "No, I Volterra kernels are nothing to do with these sort of kernels.",
            "Kernels is a term that's used all over the place.",
            "They are also used in integral operator equations so.",
            "Going back in yeah and mathematicians will often use the term kernel to mean something.",
            "Very different.",
            "Is there any light schedule office crying, knowing the origin?",
            "I'm not sure where the term.",
            "I mean it is related, it sort of came about from work that was done on Integral equations.",
            "Because I wouldn't know it will probably confuse more than help actually.",
            "Yeah.",
            "I will."
        ],
        [
            "Hey, one quick slide and I've overrun.",
            "And this is to go back to some of the stuff we talked about yesterday, and this is sort of approximation versus estimation.",
            "And it's sort of related to what I've been talking about today, so here we've got some big space of functions, and we're going to assume, OK, it's a big assumption that our true function.",
            "Lies within this big space here and we have some what I've called hypothesis space here.",
            "Which is the space that we can actually use?",
            "So this could be the space of our neural networks.",
            "It could be space of support vector machines could be space of Fourier series, so this is where we can actually do the approximations.",
            "OK. Now.",
            "What we want to do is we want to form the best possible.",
            "Estimate of our true function within this space here, because this is the best we can do, we can only do something in this space here.",
            "The best possible estimates.",
            "Would be the closest points on this space to our function.",
            "OK, with that same I mean this is very very crude here.",
            "'cause to think about what is that closest point and when these are big spaces and things?",
            "OK, I'm trying to get across an idea here.",
            "In practice.",
            "We will only ever be able to estimate something.",
            "Over here.",
            "Because we will have noise on our data.",
            "And there are two things I want to get across here is that we're at what we've actually got are two different errors.",
            "This one here is known as our estimation error.",
            "And this is because even within our space of neural networks or whatever, we can't achieve the best possible estimate because we have noise on our data.",
            "OK, so the we can't even get the best possible.",
            "So we introduce an error here.",
            "But that's something you know we can practice do something about because we've looked at estimation, we know a bit about estimation.",
            "Again, a lot of what we talked yesterday is about getting best better estimates.",
            "This error here.",
            "Is really the approximation error and this arises?",
            "Because we've chosen.",
            "A particular space which doesn't correspond.",
            "To the space of the true functions.",
            "So trying to get this error here reduced is all about what is known as approximation theory.",
            "And that's effectively all about choosing.",
            "The best possible.",
            "Space of functions here.",
            "So the best possible neural networks or support vector machines or whatever, and that's the only way you can reduce that.",
            "But if you make some choice of what neural network you're going to do.",
            "You need to accept there will be always be an error here and there's nothing you can do around it.",
            "Once you've made that choice of functions.",
            "The origin of the approximation.",
            "It it can't.",
            "This is a game where we start getting very technical because it counts strictly approximate that the function.",
            "That the MLP is equal to.",
            "Will never be arbitrarily close to the true function in a sense than being functions, but in terms of the difference between them, you can get arbitrarily close.",
            "And that's a very, very, very subtle difference.",
            "So if you think about, think about impressions of Fourier series.",
            "If you've got a square wave.",
            "OK, we know with with Fourier series we can get arbitrarily close to that square wave.",
            "But that foot, but yeah, you get the little peaks if you ignore the little peaks, you get that those set of sines and cosines can never, ever be arbitrarily close to a square wave.",
            "They'll always be a sine wave is never like a square wave, is it so?",
            "It is very subtle and I wouldn't worry about it.",
            "Yep.",
            "Well, I guess I mean to take up on the big problem is you never really.",
            "You never know where this true function belongs.",
            "OK, so you never know this big space in practice.",
            "So that's that's a problem.",
            "A lot of this is, you never really need to worry about this, so in practice we can normally get good approximations.",
            "These are very technical ideas.",
            "I want wouldn't worry hugely about, but again, it's stuff that you may come across and it's worth being familiar with and aware of."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think we'll get started.",
                    "label": 0
                },
                {
                    "sent": "Today is all about kernel methods, so I'm sure a lot of you are familiar with support vector machines, or at least heard of support vector machines.",
                    "label": 0
                },
                {
                    "sent": "On the other, related kernel methods we have.",
                    "label": 0
                },
                {
                    "sent": "Hopefully Colin Campbell will be turning up shortly, and he's going to do the bulk of today.",
                    "label": 0
                },
                {
                    "sent": "On support vector machines and kernel methods, what I'm going to do to start off with is.",
                    "label": 0
                },
                {
                    "sent": "We're going to jump from yesterday's fairly gentle introduction with not very much maths into today where.",
                    "label": 0
                },
                {
                    "sent": "There's quite a bit of maths in my first talk, so don't worry if you get lost.",
                    "label": 0
                },
                {
                    "sent": "We are going to do.",
                    "label": 0
                },
                {
                    "sent": "It's the idea of this talk is sort of give you an idea of where functional analysis is important in data modeling, but most of all I can do is try and introduce you.",
                    "label": 0
                },
                {
                    "sent": "So some of the basics of functional analysis.",
                    "label": 1
                },
                {
                    "sent": "OK, I'll try to highlight why it's important along the way.",
                    "label": 0
                },
                {
                    "sent": "Forgive me if I.",
                    "label": 0
                },
                {
                    "sent": "Don't get the mathematics 100% correct because I'm and I'm an engineer and don't care about some of the nicer CS.",
                    "label": 0
                },
                {
                    "sent": "On in functional analysis and again as we go through, you'll see some of those things.",
                    "label": 1
                },
                {
                    "sent": "There are some very technical things which I'm certainly not going to the proof of, and partly 'cause I don't understand them.",
                    "label": 0
                },
                {
                    "sent": "But this is not the time to do it here, and it's not important for our purposes, but I think it's it's a useful thing for you to be familiar with and aware of, because functional analysis is.",
                    "label": 0
                },
                {
                    "sent": "The backbone of kernel methods, or it's one of the backbones of kernel methods?",
                    "label": 0
                },
                {
                    "sent": "And also it's actually closely related to Gaussian processes, which you'll learn about's on Thursday.",
                    "label": 0
                },
                {
                    "sent": "I doubt very much anyone on Thursday will do anything on the functional analysis and Hilbert spaces of Gaussian processes, but there is actually a whole body of literature of people who've developed Garrison process is from a purely functional analysis perspective.",
                    "label": 0
                },
                {
                    "sent": "Exactly, people like Grace Wahba.",
                    "label": 0
                },
                {
                    "sent": "He's done a lot of work on splines.",
                    "label": 0
                },
                {
                    "sent": "Everything she does is very much from Hilbert spaces, and the conclusion of what I'm going to do today in this short lecture is reproducing kernel Hilbert spaces, and you'll certainly hear about those OK if not during this week, when you're reading about kernel methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, ask questions as we go through.",
                    "label": 0
                },
                {
                    "sent": "So we're going to start off by talking about metric spaces, and then we're going to go into linear spaces, and then three types of linear spaces.",
                    "label": 0
                },
                {
                    "sent": "So we're going to put norms on them in a product, and then we'll finally get to Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "Then we'll get into where it's perhaps a bit more useful for you lots and will briefly go through what is meant by best approximation in Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "I will then very very briefly introduce reproducing kernel Hilbert spaces, and this really is very brief.",
                    "label": 1
                },
                {
                    "sent": "I'm going to miss out quite a lot on that.",
                    "label": 0
                },
                {
                    "sent": "And then my final slide sort of follows on from some of the issues that were raised yesterday and I'm going to talk briefly about approximation versus estimation and this sort of relates back to some of the issues we talked about yesterday in terms of choice of basis functions and things like that.",
                    "label": 0
                },
                {
                    "sent": "I want to show you this is quite a nice picture.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which represents that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we look at.",
                    "label": 0
                },
                {
                    "sent": "For the next three calls, an hour, so are spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "What we've got is some.",
                    "label": 0
                },
                {
                    "sent": "Big blob OK Sweden.",
                    "label": 0
                },
                {
                    "sent": "Note my F, which is some space of functions.",
                    "label": 0
                },
                {
                    "sent": "Can we come on to what they may want?",
                    "label": 0
                },
                {
                    "sent": "That space may be on the next slide and within that space we denote.",
                    "label": 0
                },
                {
                    "sent": "The individual functions my points.",
                    "label": 0
                },
                {
                    "sent": "OK, so functions now become points.",
                    "label": 0
                },
                {
                    "sent": "In some space.",
                    "label": 0
                },
                {
                    "sent": "So whereas yesterday functions we are representing as a linear superposition of basis functions, can we had to worry about parameters on all this sort thing and we'll come back to that in a minute.",
                    "label": 0
                },
                {
                    "sent": "But the key thing here is that we have some space of functions and we represent the individual functions within that space by points or elements.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what can those spaces be?",
                    "label": 0
                },
                {
                    "sent": "Could be anything.",
                    "label": 0
                },
                {
                    "sent": "The simple.",
                    "label": 0
                },
                {
                    "sent": "Well, I suppose the simplest would be the real line.",
                    "label": 0
                },
                {
                    "sent": "OK, in which case they're not really functions, they are literally just numbers, but numbers are the simplest type of function you can get if you want to consider it that way.",
                    "label": 0
                },
                {
                    "sent": "It could be Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "So after the D OK, in which case now the points are vectors are still not quite functions.",
                    "label": 0
                },
                {
                    "sent": "But that's that's the space that you're most familiar with.",
                    "label": 0
                },
                {
                    "sent": "Typically and intuitively you will understand Euclidean space very well, and as we go through one of the other types of spaces, I'm going to try and think about the terms I'm going to talk about in terms of Euclidean space, because you will be familiar with most of the terms I'm going to come across in terms of Euclidean space, and actually what we're doing is we're moving towards Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And the reason we work with Hilbert spaces partly is.",
                    "label": 0
                },
                {
                    "sent": "In practice, they are very, very useful anyway, but also intuitively they're very nice because they're very, very similar, so Euclidean space in terms of all of our understanding of Euclidean space, we can pretty much apply that to Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And you can actually.",
                    "label": 0
                },
                {
                    "sent": "Source of think of functions.",
                    "label": 0
                },
                {
                    "sent": "In something Hilbert space as being infinitely long vectors and that may help you in terms of thinking about these things, but we get more interesting spaces, so there's sort of 4A space is the space of all sines and cosines.",
                    "label": 0
                },
                {
                    "sent": "So any function that can be made up of supervision of sines and cosines that would be a specific space of functions, an another quite interesting one is the space of bands, limited functions.",
                    "label": 1
                },
                {
                    "sent": "So anyone who knows anything about signal processing when we're doing band limited signal reconstruction, that's known as the pay Livina space, and that could be a space of functions which has.",
                    "label": 0
                },
                {
                    "sent": "Certain frequency characteristics.",
                    "label": 0
                },
                {
                    "sent": "R L2.",
                    "label": 0
                },
                {
                    "sent": "Very common one is the space of square integrable integrable functions suddenly get slightly more technical now in terms of what these things are, but again, that's a very another very well known space of functions.",
                    "label": 0
                },
                {
                    "sent": "These all happen to be linear spaces, which will come on.",
                    "label": 0
                },
                {
                    "sent": "So in a minute, but in the very general case Aspace functions could be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "OK, so it could be a nonlinear space of functions and we're not going to worry about those.",
                    "label": 0
                },
                {
                    "sent": "And unless you're doing a PhD in pure mathematics.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't worry about nonlinear spaces of functions.",
                    "label": 0
                },
                {
                    "sent": "OK, this is really really heavy duty stuff.",
                    "label": 0
                },
                {
                    "sent": "But the interesting point is what we're going to be looking at today are linear spaces.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But although.",
                    "label": 0
                },
                {
                    "sent": "This is a linear space of functions.",
                    "label": 0
                },
                {
                    "sent": "The actual functions within this space.",
                    "label": 0
                },
                {
                    "sent": "Can themselves be nonlinear?",
                    "label": 0
                },
                {
                    "sent": "So think of things like sines, cosines.",
                    "label": 0
                },
                {
                    "sent": "They are non linear functions.",
                    "label": 0
                },
                {
                    "sent": "OK in terms of their nonlinear in with respect to the inputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A space being linear means you can still have nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "The points within it can themselves be nonlinear, so there is a difference in the definition of what linear and nonlinear means here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that should become clearer in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first type of space that we need to concern ourselves and what we're doing here is we're progressively going to be putting more and more structure onto our spaces, so we're going to start with metric spaces, and we're going to norm space, lend linear, then norms, then inner products in Hilbert spaces and what we're doing each stage is we're putting a little bit more structure onto that space, so essentially restricting it in some way to make it nicer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in the very general case, we can define a metric and all the metric is is if you've got some space of functions and you got two points within that space.",
                    "label": 0
                },
                {
                    "sent": "So two functions F1 and F2.",
                    "label": 0
                },
                {
                    "sent": "We define some distance.",
                    "label": 0
                },
                {
                    "sent": "Between those two functions OK. And.",
                    "label": 0
                },
                {
                    "sent": "Metric spaces don't have to be linear.",
                    "label": 0
                },
                {
                    "sent": "OK, we've not said anything about linearity here, so this is a very, very general concept, and when we talk about metric spaces there also known as topological spaces, and we're looking at these are algebraic concepts, and this is the stuff that I really do find quite difficult.",
                    "label": 0
                },
                {
                    "sent": "Some of the technical stuff on algebraic spaces is beyond me, so I'm not going to go into too much detail, but this concept of a distance between two functions is.",
                    "label": 0
                },
                {
                    "sent": "Very useful, can anyone tell me?",
                    "label": 0
                },
                {
                    "sent": "What should be the characteristics of our metric for it to be a distance?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "Yes, one other.",
                    "label": 0
                },
                {
                    "sent": "The most.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we do is when we, in terms of defining a metric space, is literally a set of axioms which define what that space is.",
                    "label": 0
                },
                {
                    "sent": "So we decide or someone has decided what constitutes a metric space, and these are all things that will be very very familiar to us.",
                    "label": 0
                },
                {
                    "sent": "So as being said.",
                    "label": 0
                },
                {
                    "sent": "We define some distance on and it Maps from F cross F, so that simply means it's defines on two different functions, so it's distance between two things and it Maps to the real line, so it's a real number.",
                    "label": 0
                },
                {
                    "sent": "That real number happens to be greater than or equal to 0, so you don't get negative distances.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It is only ever equal to 0.",
                    "label": 0
                },
                {
                    "sent": "If your two functions are identical.",
                    "label": 0
                },
                {
                    "sent": "It is symmetric, so the distance between F1 and F2 is the same as the distance between F2 and F1.",
                    "label": 0
                },
                {
                    "sent": "And finally we get triangular triangle inequality which all that saying.",
                    "label": 0
                },
                {
                    "sent": "Is it that distance there is greater than?",
                    "label": 0
                },
                {
                    "sent": "Is less than sorry?",
                    "label": 0
                },
                {
                    "sent": "Some of those two, those two distances basically Pythagoras Theorem or when it's not Pythagoras theorem, but it's related to that sort of thing is what we're familiar with when we're looking at triangles and standard geometry.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, although you may not come across metric spaces before in the abstract sense, these should all be very familiar ideas.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's pretty intuitive what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Say that again.",
                    "label": 0
                },
                {
                    "sent": "They will have what if user go into looking at the history of maths and the history of the development of these things.",
                    "label": 0
                },
                {
                    "sent": "Then typically what will happen is yeah, I know who.",
                    "label": 0
                },
                {
                    "sent": "Who invented or who work with Euclidean space?",
                    "label": 0
                },
                {
                    "sent": "But it's probably the sort of Egyptians when they're building pyramids or something, and they're doing working purely with Euclidean space, and then probably in the 17th, 18th, 19th century.",
                    "label": 0
                },
                {
                    "sent": "Someone smart will come along and they needed to work with some more abstract mathematical concepts.",
                    "label": 0
                },
                {
                    "sent": "And yeah, what they would have done is they will have looks at Euclidean space and how we define things when you clear space and will have generalized it for these cases.",
                    "label": 0
                },
                {
                    "sent": "But I mean, Euclidean space is obviously an example.",
                    "label": 0
                },
                {
                    "sent": "Of a metric space, but is far far more general than that, because this could be a nonlinear space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so why are metric spaces important?",
                    "label": 0
                },
                {
                    "sent": "And we've really not defined a huge amount of structure on them at the moment.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Like the first sentence is pretty obvious.",
                    "label": 0
                },
                {
                    "sent": "It allows us to define distance between functions.",
                    "label": 0
                },
                {
                    "sent": "As a result of that, we can look at or how similar are two functions, and if we're looking at doing approximations or estimating functions, knowing how to measure the distance between all the similarity between two functions is obviously very, very important.",
                    "label": 0
                },
                {
                    "sent": "Result price will lead us onto the concepts of what is a best approximation.",
                    "label": 0
                },
                {
                    "sent": "So you want to house how close things are to each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why they're important.",
                    "label": 0
                },
                {
                    "sent": "This is one of the technical things.",
                    "label": 0
                },
                {
                    "sent": "There's another concept called Completeness Within metric Spaces, which we're going to need later on for Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into the technical bit of that, but my simple way of thinking of it is that there are no holes in the space.",
                    "label": 0
                },
                {
                    "sent": "OK, it's all to do with convergence of functions.",
                    "label": 0
                },
                {
                    "sent": "Basically, says if you got a sequence of functions.",
                    "label": 0
                },
                {
                    "sent": "They will converge.",
                    "label": 0
                },
                {
                    "sent": "That sequence of functions will converge to a point within the metric space.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a relatively simple example that is on the real line if you take.",
                    "label": 0
                },
                {
                    "sent": "The sequence of numbers which are equal to 1 / N. OK, then what does 1 / N converge to as N increases?",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you include zero in your space.",
                    "label": 0
                },
                {
                    "sent": "So if you include the real line from zero, zero and all positive numbers, then it will be a metric space if you exclude as it would be a complete space if you exclude 0.",
                    "label": 0
                },
                {
                    "sent": "And it's no longer a complete space because that sequence convergent sequence converges to a number that doesn't belong to the space.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is where it gets a bit abstracts, so we don't need to worry too much with basic simple way of thinking about it is that there are no holes in the space.",
                    "label": 0
                },
                {
                    "sent": "OK, so no missing functions effectively.",
                    "label": 0
                },
                {
                    "sent": "This is where I mean.",
                    "label": 0
                },
                {
                    "sent": "This things are pretty abstract here.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, I'm not.",
                    "label": 0
                },
                {
                    "sent": "That was an example in terms of something more practical.",
                    "label": 0
                },
                {
                    "sent": "We will never need to worry about it.",
                    "label": 0
                },
                {
                    "sent": "OK. Or you will never need to worry about that.",
                    "label": 0
                },
                {
                    "sent": "I can't.",
                    "label": 0
                },
                {
                    "sent": "Off that was what it's all to do with convergent sequences of functions.",
                    "label": 0
                },
                {
                    "sent": "No, 'cause it's not it.",
                    "label": 0
                },
                {
                    "sent": "That's getting too far for the purposes of today.",
                    "label": 0
                },
                {
                    "sent": "Just think about it in terms of there are no holes in the space.",
                    "label": 0
                },
                {
                    "sent": "If you want to know what it is, I can go and get a book later on for you and show you that the simple thing is just going to look at a book on functional analysis.",
                    "label": 0
                },
                {
                    "sent": "Not something that you need to worry too much about, but it's something 'cause I'm going to use this term completeness in a minute and it'll probably annoy you even more that you don't know what it means in a minute when I really use it.",
                    "label": 0
                },
                {
                    "sent": "OK, so, but you just need to be aware of that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And what we're going to progress on now is we want to look at spaces that are very similar to Euclidean space because of us said before, we're very familiar with Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "You know, intuitively we.",
                    "label": 0
                },
                {
                    "sent": "We know what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try and construct.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spaces that are like Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So the first thing.",
                    "label": 0
                },
                {
                    "sent": "We're going to restrict ourselves to now are linear spaces OK?",
                    "label": 1
                },
                {
                    "sent": "Instead, define a linear space.",
                    "label": 0
                },
                {
                    "sent": "We need 2 algebraic operations, so we need addition and we need multiplication by scalars.",
                    "label": 1
                },
                {
                    "sent": "And basically all addition means is that if we if we sum 2 functions in our space then that new function will also belong to the space.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's something you'd expect to be quite reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you sum 2 functions it with that, that new function will belong to this space, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Fall outside the space of functions.",
                    "label": 0
                },
                {
                    "sent": "And we also get this sort of rule that it doesn't really matter in which order you do the summations.",
                    "label": 0
                },
                {
                    "sent": "Um, according to both of these two things here, and again, that's something you'd expect to be true.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that we can multiply our functions by scalars.",
                    "label": 0
                },
                {
                    "sent": "And again, that new function forms by that multiplication will again still belong to the space.",
                    "label": 0
                },
                {
                    "sent": "And this becomes particularly important later on when we look at basis bases basis functions in our space and trying to construct functions.",
                    "label": 0
                },
                {
                    "sent": "And again we get all of these rules, which again we would expect pretty intuitive.",
                    "label": 0
                },
                {
                    "sent": "OK, in terms of multiplying by summations of scalars and scalars by summations of functions.",
                    "label": 0
                },
                {
                    "sent": "So all of those are they fairly straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the next time we go on to now is defining.",
                    "label": 0
                },
                {
                    "sent": "A basis for our.",
                    "label": 0
                },
                {
                    "sent": "Linear space and the first thing we need to say is if we've got a set of functions in our space, F1 to FM.",
                    "label": 0
                },
                {
                    "sent": "Then they are linearly independent.",
                    "label": 0
                },
                {
                    "sent": "If we take any arbitrary summation of them.",
                    "label": 0
                },
                {
                    "sent": "And that is only ever equal to 0 if.",
                    "label": 0
                },
                {
                    "sent": "All of the coefficients are equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Is not familiar from vectors.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly the same definition you will come across when we're looking at vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so for a set of vectors to be linearly independent, exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So just think of these as infinitely long vectors.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Now space functions can be infinite dimensional and often will be infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "But it'll also often be finite dimensional, and certainly when we're doing approximations with finite number of data points, our approximation space will typically be in the end finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "On a space is finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "Of dimension N if there are only N linearly independent functions or elements within it.",
                    "label": 0
                },
                {
                    "sent": "So effectively, if our space was made up of only these F1 to FN and they were linearly independent, then that would be a finite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Check.",
                    "label": 0
                },
                {
                    "sent": "Possible values of Alpha.",
                    "label": 0
                },
                {
                    "sent": "And don't don't think of.",
                    "label": 0
                },
                {
                    "sent": "Don't think of realizations of the functions.",
                    "label": 0
                },
                {
                    "sent": "Think of the functions as functions.",
                    "label": 0
                },
                {
                    "sent": "So your your honor at safe you've got cause of X.",
                    "label": 0
                },
                {
                    "sent": "If you plug in the value of X at the moment we're not.",
                    "label": 0
                },
                {
                    "sent": "So that's one of the other things we're doing here is we're not worrying about an input, so when.",
                    "label": 0
                },
                {
                    "sent": "Don't think of functions in sort of cause of X.",
                    "label": 0
                },
                {
                    "sent": "Think of it more as cause of brackets, something.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're never evaluating.",
                    "label": 0
                },
                {
                    "sent": "That function.",
                    "label": 0
                },
                {
                    "sent": "This is why you should try and think of them as infinitely long vectors.",
                    "label": 0
                },
                {
                    "sent": "Each function does that.",
                    "label": 0
                },
                {
                    "sent": "Silver puzzles yes.",
                    "label": 0
                },
                {
                    "sent": "I mean if yes if well know in in what this is saying is if you if you sum together.",
                    "label": 0
                },
                {
                    "sent": "This set of N functions in your space.",
                    "label": 0
                },
                {
                    "sent": "If those functions are linearly independent, that summation will only ever be equal to 0 if.",
                    "label": 0
                },
                {
                    "sent": "All of the Alpha rise or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "If any of these Alpha rise are non zero and that will be non 0.",
                    "label": 0
                },
                {
                    "sent": "Zero function yeah yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think yeah, that's that's that's a good point actually.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's not zero the number.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so for example, if this is like causes and signs defined over.",
                    "label": 0
                },
                {
                    "sent": "Minus \u03c0 to \u03c0, then zero would be the zero function from minus \u03c0 to \u03c0. Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's finite dimensional if there are only N linearly independent elements within the space.",
                    "label": 0
                },
                {
                    "sent": "We can then define what's known as a linear manifold within our space by simply.",
                    "label": 0
                },
                {
                    "sent": "Adding together weighted sums of our functions.",
                    "label": 0
                },
                {
                    "sent": "So we're sort of starting to define planes of functions within the space now.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very similar, so if you were looking in.",
                    "label": 0
                },
                {
                    "sent": "Euclidean space and user add together vectors within your Euclidean space you would form planes within that space.",
                    "label": 0
                },
                {
                    "sent": "For basis OK, we can express every function in F. In this form here.",
                    "label": 0
                },
                {
                    "sent": "So if our FS form a basis, then we can represent any function F in our space.",
                    "label": 0
                },
                {
                    "sent": "By some linear superposition.",
                    "label": 0
                },
                {
                    "sent": "Of a linearly independent set of functions in that space.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're starting now.",
                    "label": 0
                },
                {
                    "sent": "It's starting to look sort of familiar in terms of basis function models that we looked at yesterday.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever we're looking at sort of things like radial basis functions that we looked at yesterday.",
                    "label": 0
                },
                {
                    "sent": "And when we look at kernel methods later on.",
                    "label": 0
                },
                {
                    "sent": "They will all be representable in this form, where the F's.",
                    "label": 0
                },
                {
                    "sent": "Have a particular form.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so the end must be yes or Ashes at that end should be finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Technical definition.",
                    "label": 0
                },
                {
                    "sent": "Eh, Yeah, I think as far as your concerns, don't worry about the difference too much.",
                    "label": 0
                },
                {
                    "sent": "But there will be a subtle technical difference between the two.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you can see this, so in effect your basis.",
                    "label": 0
                },
                {
                    "sent": "So if we have three, So what we now start doing is rather than when we're thinking of a basis.",
                    "label": 0
                },
                {
                    "sent": "Rather than thinking of our F1F2 and F3 is points.",
                    "label": 0
                },
                {
                    "sent": "We now have to think of them, sort of as vectors with respect to zero in the same way in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "That you can think of X is being points in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "But when you think of them as being a basis, you have to think of them as vectors with respect to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so then when you sum these together you can form some new function F. OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next restriction is to think about norms spaces, so everything we're now taught for everything from now on, it's always a linear space.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever I took norms, inner product Hilbert spaces, these are all linear spaces, but just adding a bit more structure.",
                    "label": 0
                },
                {
                    "sent": "At each time.",
                    "label": 0
                },
                {
                    "sent": "So the idea of normed spaces is that we define the notion of the size of an element in F. So we need to define what's meant by the size of a function or a point in our space.",
                    "label": 0
                },
                {
                    "sent": "And again, I would hope so that all the leaves are fairly familiar.",
                    "label": 0
                },
                {
                    "sent": "From standard Euclidean space, so our norm must be greater than equal to 0.",
                    "label": 0
                },
                {
                    "sent": "It will only ever be equal to 0 if the function is equal to 0 and if the function is equal to 0 and norm will be 0.",
                    "label": 0
                },
                {
                    "sent": "If you multiply your function by some constants.",
                    "label": 0
                },
                {
                    "sent": "OK, that constant could be negative.",
                    "label": 0
                },
                {
                    "sent": "Then then you not the norm of that constant times.",
                    "label": 0
                },
                {
                    "sent": "The function will be equal to the absolute value of the constants times the normal function.",
                    "label": 0
                },
                {
                    "sent": "And again.",
                    "label": 0
                },
                {
                    "sent": "We get a triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I got him.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm perhaps here I should be using a different notation for O for a function 0.",
                    "label": 0
                },
                {
                    "sent": "But I think so whenever, if, whenever, until well, it should be fairly clear whenever I'm talking about a number and whenever if it's an F = 0.",
                    "label": 0
                },
                {
                    "sent": "Or some set of functions being set equals zero, then think of it as being a zero function.",
                    "label": 0
                },
                {
                    "sent": "OK, but if it's if it's a norm or a metric.",
                    "label": 0
                },
                {
                    "sent": "That's equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Then it will be the number.",
                    "label": 0
                },
                {
                    "sent": "And then the interesting points is that our norm.",
                    "label": 0
                },
                {
                    "sent": "We can always define a metric.",
                    "label": 0
                },
                {
                    "sent": "Based on our norm and we can define.",
                    "label": 0
                },
                {
                    "sent": "So if we've got some norm in our space, then we can take the norm of the difference between two functions and that will define a metric.",
                    "label": 0
                },
                {
                    "sent": "So we can therefore make a normed space A metric space, so we can.",
                    "label": 0
                },
                {
                    "sent": "Now we can keep this concept of the distance between two functions.",
                    "label": 0
                },
                {
                    "sent": "We're not just looking at the norm of a single function, we can look at the norm of the difference between two functions and that gives us a metric.",
                    "label": 0
                },
                {
                    "sent": "One of the interesting things, and again, I'm not going to get into this, is that.",
                    "label": 0
                },
                {
                    "sent": "You can sometimes have different metrics, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can have what you think of as being the same space.",
                    "label": 0
                },
                {
                    "sent": "But you can actually define different metrics and different norms for what?",
                    "label": 0
                },
                {
                    "sent": "In theory you think of as being the same space of functions.",
                    "label": 0
                },
                {
                    "sent": "OK, but if so, say you've got a set of sines and cosines.",
                    "label": 0
                },
                {
                    "sent": "OK, those are the elements in your space.",
                    "label": 0
                },
                {
                    "sent": "If you define some metric D1 and some metric D2.",
                    "label": 0
                },
                {
                    "sent": "Those two metric spaces are actually different.",
                    "label": 0
                },
                {
                    "sent": "Which is quite an old concept.",
                    "label": 0
                },
                {
                    "sent": "OK, but I get.",
                    "label": 0
                },
                {
                    "sent": "How can we think one on the train is a one on the real on the classic one on the real line would be that the metric is literally just the absolute difference.",
                    "label": 0
                },
                {
                    "sent": "Between two points.",
                    "label": 0
                },
                {
                    "sent": "But I'm sure there must be.",
                    "label": 0
                },
                {
                    "sent": "Quite often they do.",
                    "label": 0
                },
                {
                    "sent": "We almost over L2 is this.",
                    "label": 0
                },
                {
                    "sent": "That's what it comes on to in a product that we're going to do in a minute.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Getting a bit technical and.",
                    "label": 0
                },
                {
                    "sent": "Beyond my knowledge on that one.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Square square.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But they will be different spaces although.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you probably think of them as being the same space, but if if you define a different metric they become different metric spaces.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "We've defined here sort of a number of algebraic and geometric concepts.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The metric OK by defining the metric based on the norm.",
                    "label": 1
                },
                {
                    "sent": "What we've done is we've combined.",
                    "label": 0
                },
                {
                    "sent": "The algebraic concepts which is based around metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, so metric metric space is all about algebraic and topological concepts.",
                    "label": 0
                },
                {
                    "sent": "Whereas when we start some linear enormous spaces, we're now talk about geometric properties.",
                    "label": 0
                },
                {
                    "sent": "Predominantly so by defining the metric in the way that we've just done, we're starting to combine algebraic and geometric ideas.",
                    "label": 0
                },
                {
                    "sent": "OK, so these can be considered sort of initially as two different schools of mathematics or two different areas of mathematics, geometry and algebra.",
                    "label": 0
                },
                {
                    "sent": "And actually we're now starting to bring those together, and this is where things gets useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so any book on functional analysis will also have a load of stuff on this or the algebraic ideas or topological.",
                    "label": 1
                },
                {
                    "sent": "Concepts as well.",
                    "label": 0
                },
                {
                    "sent": "I just said the algebraic.",
                    "label": 1
                },
                {
                    "sent": "To me, these are the technical bits.",
                    "label": 0
                },
                {
                    "sent": "Things like completeness that I think are hard.",
                    "label": 0
                },
                {
                    "sent": "So don't worry too much about.",
                    "label": 0
                },
                {
                    "sent": "But often their concepts that are needed for us.",
                    "label": 0
                },
                {
                    "sent": "OK, they do make the spaces nice and workable.",
                    "label": 0
                },
                {
                    "sent": "We then things when we get into linear spaces with signs out much more about geometric things and to me this is the much more intuitive stuff because we've been brought up from a very young age to be very familiar with geometry and what's going on there, so we tend to underst.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit more.",
                    "label": 0
                },
                {
                    "sent": "OK, our next restriction.",
                    "label": 0
                },
                {
                    "sent": "So we started off with metric spaces.",
                    "label": 0
                },
                {
                    "sent": "Which could be linear or nonlinear.",
                    "label": 0
                },
                {
                    "sent": "We then went on and said, well, let's introduce linear spaces to put some real structure on our space.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty severe restriction to make them linear in terms of if you think of the space of all functions linear and nonlinear and linear is also with pretty small subsets, but Fortunately from us.",
                    "label": 0
                },
                {
                    "sent": "It's a very very useful.",
                    "label": 0
                },
                {
                    "sent": "Subset of functions.",
                    "label": 0
                },
                {
                    "sent": "We then define the norm, so the next thing we can do is to define an inner product, and again this should all be familiar even if the notation isn't familiar.",
                    "label": 0
                },
                {
                    "sent": "This should be familiar from vectors again, so investors are inner products is simply.",
                    "label": 0
                },
                {
                    "sent": "If you got two vectors X&Y&X, transpose Y is our inner products and we can do a very similar thing.",
                    "label": 0
                },
                {
                    "sent": "So we denote an inner products by these angle brackets.",
                    "label": 0
                },
                {
                    "sent": "And the dot, the two dots are two functions, so it's taking inner products between say F2 and F1.",
                    "label": 0
                },
                {
                    "sent": "This is how it knows that and again, it Maps from F cross F2.",
                    "label": 0
                },
                {
                    "sent": "The rail line.",
                    "label": 0
                },
                {
                    "sent": "OK, our inner products will have the properties again.",
                    "label": 0
                },
                {
                    "sent": "It will be symmetric.",
                    "label": 0
                },
                {
                    "sent": "Again, if we multiply functions by constant values.",
                    "label": 0
                },
                {
                    "sent": "Then and.",
                    "label": 0
                },
                {
                    "sent": "Inner product with another function.",
                    "label": 0
                },
                {
                    "sent": "Then we can take the constants outside the inner products.",
                    "label": 0
                },
                {
                    "sent": "Note here there's no concept of absolute value, so if these constants are negative, this will still be negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so inner products can obviously be positive or negative or positive or negative.",
                    "label": 0
                },
                {
                    "sent": "However, if you take the inner products.",
                    "label": 0
                },
                {
                    "sent": "Of a function with itself, then that will always be greater than, equal to 0, and again equality if and only if F = 0.",
                    "label": 0
                },
                {
                    "sent": "Weather last where?",
                    "label": 0
                },
                {
                    "sent": "Oh sat, three should be inside there at three.",
                    "label": 0
                },
                {
                    "sent": "Should be inside that, yes?",
                    "label": 0
                },
                {
                    "sent": "Sometimes, actually now you know you said that that's often.",
                    "label": 0
                },
                {
                    "sent": "You will see in a product with something outside and that denotes the Hilbert space or the space for which that inner product is associated.",
                    "label": 0
                },
                {
                    "sent": "So that really is a very unfortunate.",
                    "label": 0
                },
                {
                    "sent": "Mistake there.",
                    "label": 0
                },
                {
                    "sent": "We can then define a norm from our inner products so the norm on F is simply equal to the square roots of the inner product of F with itself.",
                    "label": 0
                },
                {
                    "sent": "And again, that should be very, very familiar that one of the definitions of the norm of avexa is simply the square roots of, for example, X transpose X. Yeah.",
                    "label": 0
                },
                {
                    "sent": "I can you be fine or without having an ocean open inner product?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean all of these things.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're adding more and more structure.",
                    "label": 0
                },
                {
                    "sent": "So yes, you can have soo metric spaces don't have to be linear spaces and then norm spaces don't have to be inner product spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can have and don't ask for examples.",
                    "label": 0
                },
                {
                    "sent": "So yeah, exactly right.",
                    "label": 0
                },
                {
                    "sent": "There's a question over.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Every guest.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is just building a big array of tools, so it seems like when we come up with some sort of problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm where I'm getting, so I mean I'm.",
                    "label": 0
                },
                {
                    "sent": "No, what what we're leading, so eventually we come to very shortly as Hilbert space is an really all we ever all you lot are ever likely to work with our Hilbert spaces and even those you probably won't be aware that you're working in Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, so all the stuff that's going to be talked about on kernel methods and I don't know if Colin's going to.",
                    "label": 0
                },
                {
                    "sent": "Particular you can refer to reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but the reason I'm I'm doing this is if you do work in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "There is every possibility that you will read papers that will talk about reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And it's therefore useful that you're at least familiar with the concepts behind those.",
                    "label": 0
                },
                {
                    "sent": "Um, so it's more familiarity, and certainly when I was learning about Hilbert spaces, and as you know, as an engineer, you never get taught this stuff.",
                    "label": 0
                },
                {
                    "sent": "You only ever learn it when you're doing it.",
                    "label": 0
                },
                {
                    "sent": "Reese doing research and I sort of self taught myself and a lot of this stuff.",
                    "label": 0
                },
                {
                    "sent": "It's it comes about through familiarity.",
                    "label": 0
                },
                {
                    "sent": "You know, I would argue that my understanding of Hilbert spaces came more from familiarity with the ideas than really a deep.",
                    "label": 0
                },
                {
                    "sent": "Knowledge and understanding of what's going on.",
                    "label": 0
                },
                {
                    "sent": "And that you know comes across because This is why I can't give loads of examples of different things, because, you know, as engineers, we don't really need to know this stuff and we don't need to know the real technical details.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause we can actually do an awful lot, but it's nice for you to have.",
                    "label": 0
                },
                {
                    "sent": "Some background knowledge to Hilbert spaces so that when you come across them when you're reading papers you know you are aware and you don't, you know as a researcher you will come across reproducing kernel Hilbert spaces and you'll think, oh, I really ought to know about these and then you'll end up spending another two months reading some book on functional analysis.",
                    "label": 0
                },
                {
                    "sent": "And actually, you come to the end of that and realized, well, I don't really know.",
                    "label": 0
                },
                {
                    "sent": "Need to know all of that in there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Design.",
                    "label": 0
                },
                {
                    "sent": "Explain why we're doing this.",
                    "label": 0
                },
                {
                    "sent": "Say that again.",
                    "label": 0
                },
                {
                    "sent": "It's just so if you got two vectors, if you got a got two vectors X&Y, then the inner product is X transpose Y.",
                    "label": 0
                },
                {
                    "sent": "No, it's actually this OK.",
                    "label": 0
                },
                {
                    "sent": "The thing that the best thing, the things that that is useful that inner product gives you is the notion of orthogonality.",
                    "label": 0
                },
                {
                    "sent": "So two functions are orthogonal if their inner product is zero, and we're going to use that in a minute.",
                    "label": 0
                },
                {
                    "sent": "So actually that's in terms of approximation.",
                    "label": 0
                },
                {
                    "sent": "That's probably the most useful thing that inner product gives you that nothing else has given you up until now.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you know, yeah, you can see if you've got two functions will be orthogonal if the inner product is 0, but if it's non zero it's giving you some idea of.",
                    "label": 0
                },
                {
                    "sent": "The angle between those functions?",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've defined a norm based on our inner products and obviously from that norm you can go further back and define a metric.",
                    "label": 0
                },
                {
                    "sent": "So in affects our inner product.",
                    "label": 0
                },
                {
                    "sent": "Is also defined a metric if you want to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And this is where we come back to completeness.",
                    "label": 0
                },
                {
                    "sent": "And This is why I had to briefly talk about it earlier.",
                    "label": 0
                },
                {
                    "sent": "And for me the best way of thinking about Hilbert spaces is these really do look like.",
                    "label": 0
                },
                {
                    "sent": "Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "OK, so in terms of the development of Hilbert spaces, you can really pretty much in practical purposes we can pretty much think of Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "As like Euclidian space, except there now spaces over functions.",
                    "label": 0
                },
                {
                    "sent": "And the only thing that we now add to the inner product spaces we make it complete.",
                    "label": 0
                },
                {
                    "sent": "So very, very crudely, think of a Hilbert space is an inner product space with no holes in it.",
                    "label": 1
                },
                {
                    "sent": "I know, yeah, that's not terribly good.",
                    "label": 0
                },
                {
                    "sent": "Explanation but that should be good enough for you.",
                    "label": 0
                },
                {
                    "sent": "Again, in practice we normally we would normally we rarely work with purely inner product spaces.",
                    "label": 0
                },
                {
                    "sent": "You would normally work with a complete inner product space, so actually you don't really need to worry too much about this concept.",
                    "label": 0
                },
                {
                    "sent": "But again is important because there are some spaces that aren't complete and that can cause problems 'cause you may do some function approximation and the function.",
                    "label": 0
                },
                {
                    "sent": "That you approximate actually doesn't belong to the space.",
                    "label": 0
                },
                {
                    "sent": "And mathematically, that could cause problems.",
                    "label": 0
                },
                {
                    "sent": "In practice, it won't for you lots.",
                    "label": 0
                },
                {
                    "sent": "OK so again in simple terms, Hilbert spaces have all the nice magical properties, mathematical properties that we need.",
                    "label": 1
                },
                {
                    "sent": "So actually we don't really need to worry about all the complicated bits.",
                    "label": 0
                },
                {
                    "sent": "There is something you may come across, something called benack spaces, BANACH and Bunac spaces are complete normed spaces.",
                    "label": 0
                },
                {
                    "sent": "So when you get to norm spaces, you can go off in One Direction, have a complete norm space is known as a backspace, or you can carry on from norm spaces to go to inner product spaces and then complete inner product spaces are called Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "Norm space.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are very important within function approximation as well.",
                    "label": 0
                },
                {
                    "sent": "OK, I need to speed up a bit.",
                    "label": 0
                },
                {
                    "sent": "We're not going to get into the idea of best approximation.",
                    "label": 1
                },
                {
                    "sent": "It's what we've now got.",
                    "label": 0
                },
                {
                    "sent": "Some space of functions F and some subspace, and I've not really defined what subspace is, but just think of it as a space within this bigger space.",
                    "label": 0
                },
                {
                    "sent": "It's a restriction.",
                    "label": 0
                },
                {
                    "sent": "So we've got some subspace H and we have some function in our bigger space and we want to approximate it in our smaller space.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is what function approximation is all about.",
                    "label": 0
                },
                {
                    "sent": "In terms of the sort of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Functional analysis perspective.",
                    "label": 0
                },
                {
                    "sent": "So for any given F in our.",
                    "label": 1
                },
                {
                    "sent": "Speaker Hilbert space F and.",
                    "label": 0
                },
                {
                    "sent": "A closed subspace H which is within F and again don't worry what closed means.",
                    "label": 0
                },
                {
                    "sent": "There always exists a unique best approximation F hat to F. And we use this word terminology out of H. A basic what this is saying is that there will always be.",
                    "label": 1
                },
                {
                    "sent": "A best of a unique best approximation to our function within that subspace.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just an an existence theory.",
                    "label": 0
                },
                {
                    "sent": "And there's proofs.",
                    "label": 0
                },
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "And what we can actually do is our space F. We can actually decompose into H. And.",
                    "label": 0
                },
                {
                    "sent": "The orthogonal complement of H, So effects F here becomes equal to H. Plus this with summation, and this is the orthogonal complement of H. And we'll see why that's useful in a minute.",
                    "label": 0
                },
                {
                    "sent": "And each when we come on to writing equations and what we're saying here, therefore, is the difference between F&F hat.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a different function that we think it's a little error function.",
                    "label": 0
                },
                {
                    "sent": "That's error function will be orthogonal to our subspace.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "That's how we're going to do it.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "This is why we've gone on to do so.",
                    "label": 0
                },
                {
                    "sent": "Hilbert spaces 'cause we're going to do in terms of inner product, you can do best approximation in normal spaces.",
                    "label": 0
                },
                {
                    "sent": "But yes, we're going to use this concept, and this comes back to.",
                    "label": 0
                },
                {
                    "sent": "Watch AJ was asking about in terms of wire Hilbert space and stuff to important and hear the inner product is important because we're going to use that to form our best approximation based on orthogonality.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "End up getting some equation that should become familiar with to you in the end.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is, we assume H. So our subspace is finite dimensional and it has a basis K1K2K M and I very deliberately used the notation K because hopefully column we use kace later on.",
                    "label": 0
                },
                {
                    "sent": "Perhaps not.",
                    "label": 0
                },
                {
                    "sent": "We use kace typically just a note kernels within kernel space is OK.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is that F hat can be represented as some linear superposition superposition.",
                    "label": 0
                },
                {
                    "sent": "Of these basis or kernel functions, if we think about reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "The condition that F -- F hats must be orthogonal's H gives us M conditions.",
                    "label": 0
                },
                {
                    "sent": "And these same conditions are that.",
                    "label": 0
                },
                {
                    "sent": "Since H itself is made up of these K eyes, if we take the inner products between each of our basis functions.",
                    "label": 0
                },
                {
                    "sent": "With the difference between.",
                    "label": 0
                },
                {
                    "sent": "Our true F minus.",
                    "label": 0
                },
                {
                    "sent": "RF hats, all of those will equal 0.",
                    "label": 0
                },
                {
                    "sent": "And we can simply.",
                    "label": 0
                },
                {
                    "sent": "We can expand this out using the properties we did before on products.",
                    "label": 0
                },
                {
                    "sent": "And we actually end up with this set of M conditions here.",
                    "label": 0
                },
                {
                    "sent": "So it's M because we've got this Ki in each case.",
                    "label": 0
                },
                {
                    "sent": "So for each Ki will be a separate equation, so we effectively we end up with a set of simultaneous equations that need to be solved.",
                    "label": 0
                },
                {
                    "sent": "Is everyone understands?",
                    "label": 0
                },
                {
                    "sent": "Flat.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Might become of very quickly.",
                    "label": 0
                },
                {
                    "sent": "So at reproducing kernel Hilbert spaces and this is where we will end up with an equation which you can work with, reproducing kernel Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "Again, this is a further sort of restriction to Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "That's a particular class of Hilbert space, which is very, very important in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Can we source of as one of the theoretical foundations four or kernel methods?",
                    "label": 0
                },
                {
                    "sent": "1 problems with kernel methods?",
                    "label": 0
                },
                {
                    "sent": "As you find out during the week.",
                    "label": 0
                },
                {
                    "sent": "Is that there are lots of different theoretical foundations, OK, of which Hilbert space is only one, and you probably won't hear too much more about this.",
                    "label": 0
                },
                {
                    "sent": "OK, but it is one of the theoretical foundations and things like splines, kernel machines, Portland machines near some types of neural networks.",
                    "label": 0
                },
                {
                    "sent": "National process is an even allow.",
                    "label": 0
                },
                {
                    "sent": "The earlier work was done on Time series analysis, so people like Parsinen Kailath develops a load of reproducing kernel Hilbert space stuff for doing time series analysis.",
                    "label": 0
                },
                {
                    "sent": "Again, I mentioned band names of functions.",
                    "label": 0
                },
                {
                    "sent": "All of these.",
                    "label": 0
                },
                {
                    "sent": "Can be insert proceeds in terms of reproducing kernel Hilbert spaces, which is a very very powerful idea.",
                    "label": 0
                },
                {
                    "sent": "OK, and it can be applied to lots.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lots of different types of function approximation.",
                    "label": 0
                },
                {
                    "sent": "So it's it's a Hilbert space with yet more structure associated with it.",
                    "label": 1
                },
                {
                    "sent": "We're not going to worry about the technical details here, but we do need to worry about a couple of properties one of them.",
                    "label": 1
                },
                {
                    "sent": "Is that we can represent observations.",
                    "label": 0
                },
                {
                    "sent": "So now this is our observations, why I as the inner products?",
                    "label": 0
                },
                {
                    "sent": "Of our kernel or basis functions with F. And this is particularly particularly useful property which we use on the next slide.",
                    "label": 0
                },
                {
                    "sent": "And the other interesting thing is that we now need to worry about.",
                    "label": 0
                },
                {
                    "sent": "These kernels are defined on some input space and if we take the inner product between two kernels or basis functions, kin KJ then actually that is equal to the kernel evaluated on XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "So the kernel becomes a bivariate function.",
                    "label": 0
                },
                {
                    "sent": "I'm really I'm skipping over technical detail here.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid, um.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "These kernels are positive definite functions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can use that to rewrite our M conditions, so here this was on the previous slide that was the inner.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Products.",
                    "label": 0
                },
                {
                    "sent": "Between K, INF and now, that becomes simply.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our observation.",
                    "label": 0
                },
                {
                    "sent": "Use of that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we now ends up with an inner products between the kernel functions can now represent this K of XI with X one X2X3 and so on.",
                    "label": 0
                },
                {
                    "sent": "Where these are input data points.",
                    "label": 0
                },
                {
                    "sent": "So we now have our M conditions.",
                    "label": 0
                },
                {
                    "sent": "Become this set of equations.",
                    "label": 0
                },
                {
                    "sent": "And what we can actually now do is write this in a vector matrix form, so we end up with.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Minus some matrix K which is formed by.",
                    "label": 0
                },
                {
                    "sent": "These kernel evaluations times Alpha, so rearranging it, we get our coefficients or parameters are equal to the inverse of K. Times Y.",
                    "label": 0
                },
                {
                    "sent": "And K will be invertible.",
                    "label": 0
                },
                {
                    "sent": "Provides is.",
                    "label": 0
                },
                {
                    "sent": "The RX eyes are distinct if they use their unique in the sense that there aren't any coincidence input points, I think because K in this case will always be square, so it will be invertible.",
                    "label": 0
                },
                {
                    "sent": "What's?",
                    "label": 0
                },
                {
                    "sent": "No, no, you just have.",
                    "label": 0
                },
                {
                    "sent": "These are just observations, so going back to what we're yesterday are.",
                    "label": 0
                },
                {
                    "sent": "Why eyes are just.",
                    "label": 0
                },
                {
                    "sent": "Observed values don't think about states.",
                    "label": 0
                },
                {
                    "sent": "You're just confusing things.",
                    "label": 0
                },
                {
                    "sent": "It's just, it's just the output of your system.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, it's the output of your function.",
                    "label": 0
                },
                {
                    "sent": "It's you are evaluating the function as a number of points.",
                    "label": 0
                },
                {
                    "sent": "So going back to yesterday.",
                    "label": 0
                },
                {
                    "sent": "To do.",
                    "label": 0
                },
                {
                    "sent": "To do neural networks you have to have a set of input output observations, didn't you?",
                    "label": 0
                },
                {
                    "sent": "So these why eyes on this vector Y is just your vector of outputs observations.",
                    "label": 0
                },
                {
                    "sent": "These axes are inputs.",
                    "label": 0
                },
                {
                    "sent": "They're not states or anything that those are just inputs.",
                    "label": 0
                },
                {
                    "sent": "Yes K, because these kernels are these kernels are symmetric and positive definite, so K will also be symmetric and positive definite.",
                    "label": 0
                },
                {
                    "sent": "It will be well.",
                    "label": 0
                },
                {
                    "sent": "It won't always be able conditions, but I can guarantee it virtually always will be able conditions hence.",
                    "label": 0
                },
                {
                    "sent": "In practice, I've said it can be, and often it will be L conditions and also there will be noise on your observations, typically in to cope with both of these things.",
                    "label": 0
                },
                {
                    "sent": "What we actually do is we minimize.",
                    "label": 0
                },
                {
                    "sent": "Summation of our sum of squared errors.",
                    "label": 0
                },
                {
                    "sent": "And again, I slightly abuse notation 'cause I've now used Zeds in here in that partition wise.",
                    "label": 0
                },
                {
                    "sent": "And then we add on some regularization and now the regularization.",
                    "label": 0
                },
                {
                    "sent": "Is on the norm of F. So what we're trying to do here is we're trying to restrict the size of our function.",
                    "label": 0
                },
                {
                    "sent": "And effectively, what that does?",
                    "label": 0
                },
                {
                    "sent": "Is it forces a preference for smooth functions?",
                    "label": 0
                },
                {
                    "sent": "So very crudely, the normal function is related to how wiggly or how smooth that function is.",
                    "label": 0
                },
                {
                    "sent": "The smoother function is.",
                    "label": 0
                },
                {
                    "sent": "Typically the norm will be smaller.",
                    "label": 0
                },
                {
                    "sent": "The more we clear function is, the bigger the norm will be, that's.",
                    "label": 0
                },
                {
                    "sent": "So not strictly correct, but good enough for here.",
                    "label": 0
                },
                {
                    "sent": "And if you solve this equation here, where Lambda is greater, greater, equal, 0, now our coefficients are equal to the inverse of our kernel matrix plus Lambda I.",
                    "label": 0
                },
                {
                    "sent": "Times are observations.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So the function is in this bigger space.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "That's right, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah it will be, yeah.",
                    "label": 0
                },
                {
                    "sent": "In effect, in affect our big space is like a is of all possible functions.",
                    "label": 0
                },
                {
                    "sent": "RH will be a finite dimensional subspace which is only made up of the K1.",
                    "label": 0
                },
                {
                    "sent": "So the subspace is made up of this basis, so it's a restricted form.",
                    "label": 0
                },
                {
                    "sent": "OK so it will have the same.",
                    "label": 0
                },
                {
                    "sent": "Inner products.",
                    "label": 0
                },
                {
                    "sent": "So it's like it's a finite dimensional subspace of the biggest space.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "I already have.",
                    "label": 0
                },
                {
                    "sent": "I apologize, this has been.",
                    "label": 0
                },
                {
                    "sent": "Very, very brief.",
                    "label": 0
                },
                {
                    "sent": "You really need this in the whole day.",
                    "label": 0
                },
                {
                    "sent": "You need to go through this properly.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Their independence.",
                    "label": 0
                },
                {
                    "sent": "To be basis, they have to be independent, don't have to be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "And typically they won't be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "If K was a diagonal matrix that would only arise if your kernels were orthogonal.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "I mean you can have and there are lots of orthogonal basis functions.",
                    "label": 0
                },
                {
                    "sent": "People do use them.",
                    "label": 0
                },
                {
                    "sent": "We don't in this case normally.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "No, I Volterra kernels are nothing to do with these sort of kernels.",
                    "label": 0
                },
                {
                    "sent": "Kernels is a term that's used all over the place.",
                    "label": 0
                },
                {
                    "sent": "They are also used in integral operator equations so.",
                    "label": 0
                },
                {
                    "sent": "Going back in yeah and mathematicians will often use the term kernel to mean something.",
                    "label": 0
                },
                {
                    "sent": "Very different.",
                    "label": 0
                },
                {
                    "sent": "Is there any light schedule office crying, knowing the origin?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure where the term.",
                    "label": 0
                },
                {
                    "sent": "I mean it is related, it sort of came about from work that was done on Integral equations.",
                    "label": 0
                },
                {
                    "sent": "Because I wouldn't know it will probably confuse more than help actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, one quick slide and I've overrun.",
                    "label": 0
                },
                {
                    "sent": "And this is to go back to some of the stuff we talked about yesterday, and this is sort of approximation versus estimation.",
                    "label": 0
                },
                {
                    "sent": "And it's sort of related to what I've been talking about today, so here we've got some big space of functions, and we're going to assume, OK, it's a big assumption that our true function.",
                    "label": 0
                },
                {
                    "sent": "Lies within this big space here and we have some what I've called hypothesis space here.",
                    "label": 1
                },
                {
                    "sent": "Which is the space that we can actually use?",
                    "label": 0
                },
                {
                    "sent": "So this could be the space of our neural networks.",
                    "label": 0
                },
                {
                    "sent": "It could be space of support vector machines could be space of Fourier series, so this is where we can actually do the approximations.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to form the best possible.",
                    "label": 0
                },
                {
                    "sent": "Estimate of our true function within this space here, because this is the best we can do, we can only do something in this space here.",
                    "label": 0
                },
                {
                    "sent": "The best possible estimates.",
                    "label": 0
                },
                {
                    "sent": "Would be the closest points on this space to our function.",
                    "label": 0
                },
                {
                    "sent": "OK, with that same I mean this is very very crude here.",
                    "label": 0
                },
                {
                    "sent": "'cause to think about what is that closest point and when these are big spaces and things?",
                    "label": 0
                },
                {
                    "sent": "OK, I'm trying to get across an idea here.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "We will only ever be able to estimate something.",
                    "label": 0
                },
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "Because we will have noise on our data.",
                    "label": 0
                },
                {
                    "sent": "And there are two things I want to get across here is that we're at what we've actually got are two different errors.",
                    "label": 0
                },
                {
                    "sent": "This one here is known as our estimation error.",
                    "label": 0
                },
                {
                    "sent": "And this is because even within our space of neural networks or whatever, we can't achieve the best possible estimate because we have noise on our data.",
                    "label": 1
                },
                {
                    "sent": "OK, so the we can't even get the best possible.",
                    "label": 0
                },
                {
                    "sent": "So we introduce an error here.",
                    "label": 0
                },
                {
                    "sent": "But that's something you know we can practice do something about because we've looked at estimation, we know a bit about estimation.",
                    "label": 0
                },
                {
                    "sent": "Again, a lot of what we talked yesterday is about getting best better estimates.",
                    "label": 0
                },
                {
                    "sent": "This error here.",
                    "label": 0
                },
                {
                    "sent": "Is really the approximation error and this arises?",
                    "label": 0
                },
                {
                    "sent": "Because we've chosen.",
                    "label": 0
                },
                {
                    "sent": "A particular space which doesn't correspond.",
                    "label": 0
                },
                {
                    "sent": "To the space of the true functions.",
                    "label": 0
                },
                {
                    "sent": "So trying to get this error here reduced is all about what is known as approximation theory.",
                    "label": 0
                },
                {
                    "sent": "And that's effectively all about choosing.",
                    "label": 0
                },
                {
                    "sent": "The best possible.",
                    "label": 0
                },
                {
                    "sent": "Space of functions here.",
                    "label": 0
                },
                {
                    "sent": "So the best possible neural networks or support vector machines or whatever, and that's the only way you can reduce that.",
                    "label": 0
                },
                {
                    "sent": "But if you make some choice of what neural network you're going to do.",
                    "label": 0
                },
                {
                    "sent": "You need to accept there will be always be an error here and there's nothing you can do around it.",
                    "label": 0
                },
                {
                    "sent": "Once you've made that choice of functions.",
                    "label": 0
                },
                {
                    "sent": "The origin of the approximation.",
                    "label": 0
                },
                {
                    "sent": "It it can't.",
                    "label": 0
                },
                {
                    "sent": "This is a game where we start getting very technical because it counts strictly approximate that the function.",
                    "label": 0
                },
                {
                    "sent": "That the MLP is equal to.",
                    "label": 0
                },
                {
                    "sent": "Will never be arbitrarily close to the true function in a sense than being functions, but in terms of the difference between them, you can get arbitrarily close.",
                    "label": 0
                },
                {
                    "sent": "And that's a very, very, very subtle difference.",
                    "label": 0
                },
                {
                    "sent": "So if you think about, think about impressions of Fourier series.",
                    "label": 0
                },
                {
                    "sent": "If you've got a square wave.",
                    "label": 0
                },
                {
                    "sent": "OK, we know with with Fourier series we can get arbitrarily close to that square wave.",
                    "label": 0
                },
                {
                    "sent": "But that foot, but yeah, you get the little peaks if you ignore the little peaks, you get that those set of sines and cosines can never, ever be arbitrarily close to a square wave.",
                    "label": 0
                },
                {
                    "sent": "They'll always be a sine wave is never like a square wave, is it so?",
                    "label": 0
                },
                {
                    "sent": "It is very subtle and I wouldn't worry about it.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess I mean to take up on the big problem is you never really.",
                    "label": 1
                },
                {
                    "sent": "You never know where this true function belongs.",
                    "label": 0
                },
                {
                    "sent": "OK, so you never know this big space in practice.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a problem.",
                    "label": 0
                },
                {
                    "sent": "A lot of this is, you never really need to worry about this, so in practice we can normally get good approximations.",
                    "label": 0
                },
                {
                    "sent": "These are very technical ideas.",
                    "label": 0
                },
                {
                    "sent": "I want wouldn't worry hugely about, but again, it's stuff that you may come across and it's worth being familiar with and aware of.",
                    "label": 0
                }
            ]
        }
    }
}