{
    "id": "ac5ywqgaon7m5ockz7kvaa44uudba567",
    "title": "RDF2Vec: RDF Graph Embeddings for Data Mining",
    "info": {
        "author": [
            "Petar Ristoski, School of Business Informatics and Mathematics, University of Mannheim"
        ],
        "published": "Nov. 10, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2016_ristoski_rdf_graph/",
    "segmentation": [
        [
            "Coming from the University of Mannheim in this presentation, I will introduce the approach for RDF graph embeddings for data mining called RDF to work.",
            "This is joint work with Heiko Pal him from the same University."
        ],
        [
            "So here we are shown a pipeline for knowledge Discovery which was first introduced by filed and all at the Seminole Paper in 1996.",
            "It consists of five steps in total and which leads from raw data to actionable knowledge and insights which are of immediate value for the user in the end.",
            "So with the recent advancement of semantic Web, we can actually enhance."
        ],
        [
            "The the the pipeline with additional technologies.",
            "So we first start with linking the raw data to any kind of linked open data set, and once we have set the links then we can explore additional links in the linked open data cloud and in the next step we can apply approaches for consolidation and cleaning of the data and in the middle step we perform some graph data transformations so we can represent the data in a format that can be processed by any arbitrary data mining algorithm in the next step, we.",
            "Applied the most suitable data mining algorithm for the data and task we want to resolve, and in the end we present the results to the user, which again can be visualized and explained by using linked open data.",
            "So currently most of the."
        ],
        [
            "Rythms data mining algorithms work with propositional feature vectors which means that we have to define a finite feature space and each instance in the data set has to be represented as an dimensional vector where the values are Boolean numerical or nominal values.",
            "But"
        ],
        [
            "Linked open data comes in the form of graphs where resources are connected with relations and types, which are usually backed by schema or ontology.",
            "That means that we have to identify."
        ],
        [
            "My transformation function that is going to transform the graph."
        ],
        [
            "Into propositional feature vectors.",
            "Currently there are many approaches that are doing this.",
            "Most of them are supervised where the user actually defines the query.",
            "The Sparkle query or we have also unsupervised.",
            "The simple approaches which simply transform all the relations and values as features and we have more advanced approaches like graph kernels.",
            "But for example for the supervised approaches, the problem is that often users overseas some important and relevant features for the task and with the simple unsupervised approaches we miss a lot of information from the graph.",
            "And some of the advanced approaches actually do not scale on large RDF graphs.",
            "Or to others."
        ],
        [
            "Issues we define a set of requirements for propositional technique for RDF data, so the approach has to be able to preserve the information given in the original graph.",
            "It has to be unsupervised or it should be task and data set independent, and it should be compatible with traditional data mining algorithms and tools and the computation of such approach has to be efficient and the application in the machine learning algorithms also has to be efficient."
        ],
        [
            "These requirements we implemented the RDF to work approach."
        ],
        [
            "Which adapts neural language models for embedding entities in RDF graphs.",
            "Such neural language models.",
            "So take advantage of the position of words in the text documents and explicitly modeled assumptions that similar words appear in similar context and in the case of RDF graphs, we don't want to model words we want to model entities and relations, and therefore we have to convert the graph into sequences.",
            "For that we use.",
            "A graph books and the graph kernels.",
            "Once we have the sequences, then we train the same neural language models which embeds all the entities and relation and dimensional numerical vectors, and so when we project them into the embedding space, similar entities appear close to each other and in the end the feature vectors can be directly used in any machine learning task."
        ],
        [
            "A quick overview of the neural language model, so we use.",
            "We adapt the work work model, which is a very efficient two layer neural Nets which converts the raw text into into vectors.",
            "Oh it comes in 2 flavors.",
            "First is the continuous bag of words which.",
            "Use the predict target words from source contact words.",
            "So for example, for the given sentence, we take Tokyo to be the target word and then we take the surrounding words to be the context words.",
            "So this is the."
        ],
        [
            "Textural for the simple neural net.",
            "So on the input we have the context words and on the output we have the target word.",
            "That means that for all of the words on the input, we just average the vectors for the from the input weighted matrix.",
            "We project them into the hidden layer and then using softmax we just try to calculate the probability that the target word is going to be on the output.",
            "Then using gradient descent we simply back propagate the error back to the network and update the.",
            "Update the the weights of the input vectors.",
            "In the end, all of the."
        ],
        [
            "All of the words in the vocabulary are represented as an dimensional feature vectors."
        ],
        [
            "An IF we project them into the lower space or into lower dimensional feature space, then similar semantically similar words appear close to each other and also the relation between the words are preserved."
        ],
        [
            "For example Tokyo, we know the docu is the capital of Japan and this allows us to actually issue queries to the model.",
            "Like if we know that Doctor is the capital of Japan then."
        ],
        [
            "Berlin is the capital of which country and directly we can answer that.",
            "It's Germany, which in this query can be easily just."
        ],
        [
            "Transferred into arithmetic's like a vector of Japan minus the vector of Tokyo plus the vector Berlin should be somewhere close to the vector of Germany."
        ],
        [
            "I'll skip Brown, does the opposite of civil, so it tries to predict the context words from the target word.",
            "So in this case talk is going to be the only input and capital in Japan are going to be on the."
        ],
        [
            "Output of the neural net."
        ],
        [
            "And in the case of RDF graph.",
            "So we because we have the graph, we have to convert it into a sequence of entities and relations.",
            "For that we use to techniques.",
            "First this graph works and the second approach is using voice, file, lemon subtree, RDF graph kernel."
        ],
        [
            "So in the case of graph works for each of the entity in the graph we extract a subgraph with user specified depth and in this subgraph we simply extract all the all the walks."
        ],
        [
            "So for the given example, we will get sequence of works in this format.",
            "For example Trent Reznor is we dissociated band exotic birds which has member Chris Renna and then we use all these sequences."
        ],
        [
            "How to?",
            "To build the same neural language model and in the end again we get the feature vectors for each entity in the."
        ],
        [
            "So in the graph, and again if we present them into a low dimensional feature space, entities with similar semantic meaning are going to be grouped together and also the relation between the entities is preserved."
        ],
        [
            "In the second approach, we use the voice file now Lemon kernel, which is state of the art kernel for comparison of comparison of graphs.",
            "So it counts the number of sub trees that are shared between two or more graphs using the voice file laminal graph morphism test.",
            "So it runs into four steps.",
            "So in the first step we simply extract the subgraph of each entity in the."
        ],
        [
            "Next step or we?",
            "Hello concatenate to each.",
            "Note the labels of the neighbors and in the next step we see."
        ],
        [
            "Simply replace the the long tables with us or with shorter unique labels and in the final step we see."
        ],
        [
            "Please Re label the original graphs of the original graph with new labels and this runs in several iterations."
        ],
        [
            "Oh so for the given input graph, this is going to be the output graph of four after the first iteration.",
            "Then to convert the graph into sequences of into sequences, we simply start generating walks starting rooted in the original label of the node, and then we start doing walks in the reliable graph.",
            "So for example, for the given example, we will have walks like one which is the original node, and then we continue with six and then we continue with.",
            "Walks on the new graph, so an example would be one 611 and then again we simply feed this."
        ],
        [
            "All these sequences into the neural net model and we get the vectors for for the entities in the in the graph."
        ],
        [
            "Oh, next I will discuss some results of the evaluation."
        ],
        [
            "So we perform evaluation on two different datasets.",
            "First we have three domain specific domain specific RDF datasets which are quite small and we have two large cross domain datasets and five evaluation datasets.",
            "We used final feature vectors for the task of classification and regression for classification.",
            "We trained five different models and we calculate accuracy using 10 fold cross validation and for regression we calculate the root mean squared error.",
            "Also, using tenfold cross validation we have we compare our approach to several baselines.",
            "So first we have 6.",
            "Approaches for generating features from derived from incoming and outgoing correlations and values, and to.",
            "Also we have features derived from graph substructures like walk on kernel and vice file lemon kernel."
        ],
        [
            "Also, we have three domain specific RDF datasets which are commonly used in this area.",
            "I have BBGS and you talk.",
            "They are relatively small and for all of them we extract all the walks with depth then and for the Wi-Fi lemonick kernel we use four iterations and depth of two.",
            "We experiment with different sizes of the vectors 200 and 500 and we also build SIBO and ski gram models.",
            "So these are the results.",
            "We only show the best scores for the baselines and the best course of all the models using the different conversion algorithms.",
            "As we can see, the conversion of the graph using the graph kernels actually leads to the best results and which is also disappointing is that the baseline outperforms the works embedding.",
            "For the."
        ],
        [
            "Large cross domain RDF deal.",
            "Since we used the Pedia, an wiki data so the beer has around 5 million instances.",
            "We did a 17,000,000 instances and because with adding new new entities so the number of generated works like exponentially grows easily becomes unmanageable to extract all the possible sequences using the walks or the kernels.",
            "Therefore, we use random walks for each entity and we extract around 500 to walks per entity, which results in two.",
            "2.5 billion sequences generated for the pedia.",
            "An 8.5 billion, four wiki data.",
            "Again, we experiment with different vector sizes and see bohensky gram model.",
            "We have always had the feature vectors on five different datasets which can be used for regression and classification.",
            "They cover different domains.",
            "We have cities, albums, movies, universities and companies."
        ],
        [
            "So here we have the results for the classification.",
            "First we have the best baseline and then we."
        ],
        [
            "For the results of the different parameters for when using the DPD embedded feature vectors and wiki data, embedded vectors or the results showed that both wiki data and the pedia outperformed the baselines where DB pedia usually outperforms significantly weaker data.",
            "The same can be observed for the task of regression where again, DB pedia embedded vectors will lead to the best performances.",
            "So the summary."
        ],
        [
            "The result is that RDF workout performs all the baselines and also because the number of features, the dimension of the feature vectors is always 200 or 500.",
            "We have more efficient training of the machine learning models where, for example, when using the baseline.",
            "Sometimes we have from 1000 to 1,000,000 feature of dimensions.",
            "Also, the kernel sequences lead to better performance is because they capture more information from the graph.",
            "Or they are not.",
            "We cannot efficiently calculate them on large graphs and so.",
            "So we cannot scale.",
            "Also increasing the depth of the walks improve the quality of the embeddings.",
            "Of course, because again we capture more information from the graph and also it's interesting that the vector dimensionality doesn't really affect the performance is.",
            "So sometimes vectors of dimension 200 perform the same as the vectors of dimension 500, and also skipper models continuously outperformed the simple model, which has also been shown in our other.",
            "Empirical studies because we have enough data, so this keyboard model always captures more information.",
            "And also the pedia produces higher quality embeddings than Wikipedia."
        ],
        [
            "So in this in this paper we only evaluated approach on classification and regression.",
            "But also we have used it so for different tasks.",
            "For example, content based recommender systems, document modeling like calculating document similarity and entity relatedness.",
            "Also it can be used for alignment of different knowledge bases like the VPN Wiki data and also we can use it for relation prediction, an error detection in knowledge bases and linking text and semi structured knowledge like web tables too.",
            "Existing knowledge bases."
        ],
        [
            "So to conclude, we presented the RDF two VEC approach which adapts neural language models for embedding code for RDF entities.",
            "We managed to preserve the graph information and the feature vectors.",
            "The resulting feature vectors are compatible with various traditional machine learning algorithms.",
            "It will show that this feature vectors are more efficient than the baseline feature vectors and also the generation of the feature vectors is.",
            "Completely unsupervised, an independent of the given task and data set.",
            "And if you want to download the code or the models you can do it on the following click or just get the QR code.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming from the University of Mannheim in this presentation, I will introduce the approach for RDF graph embeddings for data mining called RDF to work.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Heiko Pal him from the same University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we are shown a pipeline for knowledge Discovery which was first introduced by filed and all at the Seminole Paper in 1996.",
                    "label": 0
                },
                {
                    "sent": "It consists of five steps in total and which leads from raw data to actionable knowledge and insights which are of immediate value for the user in the end.",
                    "label": 0
                },
                {
                    "sent": "So with the recent advancement of semantic Web, we can actually enhance.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the the pipeline with additional technologies.",
                    "label": 0
                },
                {
                    "sent": "So we first start with linking the raw data to any kind of linked open data set, and once we have set the links then we can explore additional links in the linked open data cloud and in the next step we can apply approaches for consolidation and cleaning of the data and in the middle step we perform some graph data transformations so we can represent the data in a format that can be processed by any arbitrary data mining algorithm in the next step, we.",
                    "label": 0
                },
                {
                    "sent": "Applied the most suitable data mining algorithm for the data and task we want to resolve, and in the end we present the results to the user, which again can be visualized and explained by using linked open data.",
                    "label": 0
                },
                {
                    "sent": "So currently most of the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rythms data mining algorithms work with propositional feature vectors which means that we have to define a finite feature space and each instance in the data set has to be represented as an dimensional vector where the values are Boolean numerical or nominal values.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linked open data comes in the form of graphs where resources are connected with relations and types, which are usually backed by schema or ontology.",
                    "label": 0
                },
                {
                    "sent": "That means that we have to identify.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My transformation function that is going to transform the graph.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into propositional feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Currently there are many approaches that are doing this.",
                    "label": 0
                },
                {
                    "sent": "Most of them are supervised where the user actually defines the query.",
                    "label": 0
                },
                {
                    "sent": "The Sparkle query or we have also unsupervised.",
                    "label": 0
                },
                {
                    "sent": "The simple approaches which simply transform all the relations and values as features and we have more advanced approaches like graph kernels.",
                    "label": 0
                },
                {
                    "sent": "But for example for the supervised approaches, the problem is that often users overseas some important and relevant features for the task and with the simple unsupervised approaches we miss a lot of information from the graph.",
                    "label": 0
                },
                {
                    "sent": "And some of the advanced approaches actually do not scale on large RDF graphs.",
                    "label": 0
                },
                {
                    "sent": "Or to others.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issues we define a set of requirements for propositional technique for RDF data, so the approach has to be able to preserve the information given in the original graph.",
                    "label": 0
                },
                {
                    "sent": "It has to be unsupervised or it should be task and data set independent, and it should be compatible with traditional data mining algorithms and tools and the computation of such approach has to be efficient and the application in the machine learning algorithms also has to be efficient.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These requirements we implemented the RDF to work approach.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which adapts neural language models for embedding entities in RDF graphs.",
                    "label": 1
                },
                {
                    "sent": "Such neural language models.",
                    "label": 0
                },
                {
                    "sent": "So take advantage of the position of words in the text documents and explicitly modeled assumptions that similar words appear in similar context and in the case of RDF graphs, we don't want to model words we want to model entities and relations, and therefore we have to convert the graph into sequences.",
                    "label": 0
                },
                {
                    "sent": "For that we use.",
                    "label": 0
                },
                {
                    "sent": "A graph books and the graph kernels.",
                    "label": 0
                },
                {
                    "sent": "Once we have the sequences, then we train the same neural language models which embeds all the entities and relation and dimensional numerical vectors, and so when we project them into the embedding space, similar entities appear close to each other and in the end the feature vectors can be directly used in any machine learning task.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A quick overview of the neural language model, so we use.",
                    "label": 1
                },
                {
                    "sent": "We adapt the work work model, which is a very efficient two layer neural Nets which converts the raw text into into vectors.",
                    "label": 1
                },
                {
                    "sent": "Oh it comes in 2 flavors.",
                    "label": 1
                },
                {
                    "sent": "First is the continuous bag of words which.",
                    "label": 0
                },
                {
                    "sent": "Use the predict target words from source contact words.",
                    "label": 1
                },
                {
                    "sent": "So for example, for the given sentence, we take Tokyo to be the target word and then we take the surrounding words to be the context words.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Textural for the simple neural net.",
                    "label": 0
                },
                {
                    "sent": "So on the input we have the context words and on the output we have the target word.",
                    "label": 0
                },
                {
                    "sent": "That means that for all of the words on the input, we just average the vectors for the from the input weighted matrix.",
                    "label": 0
                },
                {
                    "sent": "We project them into the hidden layer and then using softmax we just try to calculate the probability that the target word is going to be on the output.",
                    "label": 0
                },
                {
                    "sent": "Then using gradient descent we simply back propagate the error back to the network and update the.",
                    "label": 0
                },
                {
                    "sent": "Update the the weights of the input vectors.",
                    "label": 0
                },
                {
                    "sent": "In the end, all of the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of the words in the vocabulary are represented as an dimensional feature vectors.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An IF we project them into the lower space or into lower dimensional feature space, then similar semantically similar words appear close to each other and also the relation between the words are preserved.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example Tokyo, we know the docu is the capital of Japan and this allows us to actually issue queries to the model.",
                    "label": 0
                },
                {
                    "sent": "Like if we know that Doctor is the capital of Japan then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Berlin is the capital of which country and directly we can answer that.",
                    "label": 0
                },
                {
                    "sent": "It's Germany, which in this query can be easily just.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transferred into arithmetic's like a vector of Japan minus the vector of Tokyo plus the vector Berlin should be somewhere close to the vector of Germany.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll skip Brown, does the opposite of civil, so it tries to predict the context words from the target word.",
                    "label": 0
                },
                {
                    "sent": "So in this case talk is going to be the only input and capital in Japan are going to be on the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Output of the neural net.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the case of RDF graph.",
                    "label": 0
                },
                {
                    "sent": "So we because we have the graph, we have to convert it into a sequence of entities and relations.",
                    "label": 0
                },
                {
                    "sent": "For that we use to techniques.",
                    "label": 0
                },
                {
                    "sent": "First this graph works and the second approach is using voice, file, lemon subtree, RDF graph kernel.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the case of graph works for each of the entity in the graph we extract a subgraph with user specified depth and in this subgraph we simply extract all the all the walks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the given example, we will get sequence of works in this format.",
                    "label": 0
                },
                {
                    "sent": "For example Trent Reznor is we dissociated band exotic birds which has member Chris Renna and then we use all these sequences.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to?",
                    "label": 0
                },
                {
                    "sent": "To build the same neural language model and in the end again we get the feature vectors for each entity in the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the graph, and again if we present them into a low dimensional feature space, entities with similar semantic meaning are going to be grouped together and also the relation between the entities is preserved.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the second approach, we use the voice file now Lemon kernel, which is state of the art kernel for comparison of comparison of graphs.",
                    "label": 0
                },
                {
                    "sent": "So it counts the number of sub trees that are shared between two or more graphs using the voice file laminal graph morphism test.",
                    "label": 0
                },
                {
                    "sent": "So it runs into four steps.",
                    "label": 0
                },
                {
                    "sent": "So in the first step we simply extract the subgraph of each entity in the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next step or we?",
                    "label": 0
                },
                {
                    "sent": "Hello concatenate to each.",
                    "label": 0
                },
                {
                    "sent": "Note the labels of the neighbors and in the next step we see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simply replace the the long tables with us or with shorter unique labels and in the final step we see.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please Re label the original graphs of the original graph with new labels and this runs in several iterations.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh so for the given input graph, this is going to be the output graph of four after the first iteration.",
                    "label": 0
                },
                {
                    "sent": "Then to convert the graph into sequences of into sequences, we simply start generating walks starting rooted in the original label of the node, and then we start doing walks in the reliable graph.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the given example, we will have walks like one which is the original node, and then we continue with six and then we continue with.",
                    "label": 0
                },
                {
                    "sent": "Walks on the new graph, so an example would be one 611 and then again we simply feed this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these sequences into the neural net model and we get the vectors for for the entities in the in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, next I will discuss some results of the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we perform evaluation on two different datasets.",
                    "label": 0
                },
                {
                    "sent": "First we have three domain specific domain specific RDF datasets which are quite small and we have two large cross domain datasets and five evaluation datasets.",
                    "label": 0
                },
                {
                    "sent": "We used final feature vectors for the task of classification and regression for classification.",
                    "label": 0
                },
                {
                    "sent": "We trained five different models and we calculate accuracy using 10 fold cross validation and for regression we calculate the root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "Also, using tenfold cross validation we have we compare our approach to several baselines.",
                    "label": 0
                },
                {
                    "sent": "So first we have 6.",
                    "label": 0
                },
                {
                    "sent": "Approaches for generating features from derived from incoming and outgoing correlations and values, and to.",
                    "label": 0
                },
                {
                    "sent": "Also we have features derived from graph substructures like walk on kernel and vice file lemon kernel.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we have three domain specific RDF datasets which are commonly used in this area.",
                    "label": 0
                },
                {
                    "sent": "I have BBGS and you talk.",
                    "label": 0
                },
                {
                    "sent": "They are relatively small and for all of them we extract all the walks with depth then and for the Wi-Fi lemonick kernel we use four iterations and depth of two.",
                    "label": 0
                },
                {
                    "sent": "We experiment with different sizes of the vectors 200 and 500 and we also build SIBO and ski gram models.",
                    "label": 0
                },
                {
                    "sent": "So these are the results.",
                    "label": 0
                },
                {
                    "sent": "We only show the best scores for the baselines and the best course of all the models using the different conversion algorithms.",
                    "label": 0
                },
                {
                    "sent": "As we can see, the conversion of the graph using the graph kernels actually leads to the best results and which is also disappointing is that the baseline outperforms the works embedding.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large cross domain RDF deal.",
                    "label": 0
                },
                {
                    "sent": "Since we used the Pedia, an wiki data so the beer has around 5 million instances.",
                    "label": 0
                },
                {
                    "sent": "We did a 17,000,000 instances and because with adding new new entities so the number of generated works like exponentially grows easily becomes unmanageable to extract all the possible sequences using the walks or the kernels.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we use random walks for each entity and we extract around 500 to walks per entity, which results in two.",
                    "label": 0
                },
                {
                    "sent": "2.5 billion sequences generated for the pedia.",
                    "label": 0
                },
                {
                    "sent": "An 8.5 billion, four wiki data.",
                    "label": 0
                },
                {
                    "sent": "Again, we experiment with different vector sizes and see bohensky gram model.",
                    "label": 0
                },
                {
                    "sent": "We have always had the feature vectors on five different datasets which can be used for regression and classification.",
                    "label": 0
                },
                {
                    "sent": "They cover different domains.",
                    "label": 0
                },
                {
                    "sent": "We have cities, albums, movies, universities and companies.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have the results for the classification.",
                    "label": 0
                },
                {
                    "sent": "First we have the best baseline and then we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the results of the different parameters for when using the DPD embedded feature vectors and wiki data, embedded vectors or the results showed that both wiki data and the pedia outperformed the baselines where DB pedia usually outperforms significantly weaker data.",
                    "label": 0
                },
                {
                    "sent": "The same can be observed for the task of regression where again, DB pedia embedded vectors will lead to the best performances.",
                    "label": 0
                },
                {
                    "sent": "So the summary.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result is that RDF workout performs all the baselines and also because the number of features, the dimension of the feature vectors is always 200 or 500.",
                    "label": 0
                },
                {
                    "sent": "We have more efficient training of the machine learning models where, for example, when using the baseline.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we have from 1000 to 1,000,000 feature of dimensions.",
                    "label": 0
                },
                {
                    "sent": "Also, the kernel sequences lead to better performance is because they capture more information from the graph.",
                    "label": 0
                },
                {
                    "sent": "Or they are not.",
                    "label": 0
                },
                {
                    "sent": "We cannot efficiently calculate them on large graphs and so.",
                    "label": 0
                },
                {
                    "sent": "So we cannot scale.",
                    "label": 0
                },
                {
                    "sent": "Also increasing the depth of the walks improve the quality of the embeddings.",
                    "label": 0
                },
                {
                    "sent": "Of course, because again we capture more information from the graph and also it's interesting that the vector dimensionality doesn't really affect the performance is.",
                    "label": 0
                },
                {
                    "sent": "So sometimes vectors of dimension 200 perform the same as the vectors of dimension 500, and also skipper models continuously outperformed the simple model, which has also been shown in our other.",
                    "label": 0
                },
                {
                    "sent": "Empirical studies because we have enough data, so this keyboard model always captures more information.",
                    "label": 0
                },
                {
                    "sent": "And also the pedia produces higher quality embeddings than Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this in this paper we only evaluated approach on classification and regression.",
                    "label": 0
                },
                {
                    "sent": "But also we have used it so for different tasks.",
                    "label": 0
                },
                {
                    "sent": "For example, content based recommender systems, document modeling like calculating document similarity and entity relatedness.",
                    "label": 0
                },
                {
                    "sent": "Also it can be used for alignment of different knowledge bases like the VPN Wiki data and also we can use it for relation prediction, an error detection in knowledge bases and linking text and semi structured knowledge like web tables too.",
                    "label": 0
                },
                {
                    "sent": "Existing knowledge bases.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, we presented the RDF two VEC approach which adapts neural language models for embedding code for RDF entities.",
                    "label": 0
                },
                {
                    "sent": "We managed to preserve the graph information and the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "The resulting feature vectors are compatible with various traditional machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "It will show that this feature vectors are more efficient than the baseline feature vectors and also the generation of the feature vectors is.",
                    "label": 0
                },
                {
                    "sent": "Completely unsupervised, an independent of the given task and data set.",
                    "label": 0
                },
                {
                    "sent": "And if you want to download the code or the models you can do it on the following click or just get the QR code.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}