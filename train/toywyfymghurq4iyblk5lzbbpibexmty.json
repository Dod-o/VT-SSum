{
    "id": "toywyfymghurq4iyblk5lzbbpibexmty",
    "title": "Generalized Mixability via Entropic Duality",
    "info": {
        "author": [
            "Mark Reid, Research School of Information Sciences and Engineering, Australian National University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_reid_entropic_duality/",
    "segmentation": [
        [
            "So this is joint work with Rafan Bobana Shannon.",
            "We've been thinking about mix ability for awhile, and these are some observations and generalizations that came about by trying to understand mix ability."
        ],
        [
            "So thankfully, Khamal has already introduced the prediction with expert advice setting that we use, but just to pin down some notation, we have a loss function where if you take an action A and then some outcome occurs, you get some penalty.",
            "Ann, you have this game.",
            "You fix the loss and you fix some number of experts and you just repeatedly have the experts play predictions.",
            "The player makes prediction and outcome occurs.",
            "And what we're interested in is which games of this type admitted constant regret.",
            "That is, if you take the difference between the total cumulative loss for the player and subtract the best expert, when can you get this constant?",
            "That may depend on L and the number of experts.",
            "That bounds regret."
        ],
        [
            "And this has been known for a long time.",
            "Volf has this paper on.",
            "We introduced the notion of mix ability, which is this expression here that says, no matter what mixture of experts you choose and actions they play.",
            "You have to be able to guarantee that you can play an action that's better than this quantity here on the right, which I'm going to mix.",
            "For the expert actions and distributions, and if you have this property, you can do this sort of exponential weight updates.",
            "So mixtures an you can play the action that this property guarantees and you can guarantee a constant regret of log number of experts on Eater.",
            "And so this is over well known.",
            "And if you want this to hold for all experts, then this is a sort of fundamental property."
        ],
        [
            "The observation we make in this paper is this related towards Sebastian was talking about.",
            "I think is that this log sum X term in the definition of mix ability.",
            "You can think of as a kind of restricted convex jewel of Shannon entropy.",
            "So if you take this tool where you're restricting the jewel to the simplex, you can express this mix ability term in terms of a difference of this jewel of this entropy and interesting Lee, you can do this for any convex function on the simplex.",
            "And you get you get a bunch of nice properties.",
            "You get that the."
        ],
        [
            "Gradient of the jewel of these entropies takes you back onto the simplex always.",
            "It's just a result from convex analysis.",
            "You get this translation invariant result and you get that you can write the difference in potentials as a kind of minimization, and it turns out this is all you need to get the constant regret result.",
            "So it means that we can take the generalize the definition mix ability.",
            "We can generalize the aggregating algorithm, and we can still guarantee constant regret, and I think this gives us a bit of insight into what's going on here.",
            "Interestingly, the new definition of mix ability is naturally a tradeoff between a sort of fit to data and some notion of distance to prior.",
            "And the aggregating algorithm just plays the mixture that witnesses the mixability bound at each step.",
            "And actually you can show very easily that this is a.",
            "Actually I kind of dual updating step and the regret we get is just in terms of the Bregman divergences between the corners of the simplex and your initial starting point.",
            "So this."
        ],
        [
            "Yeah, so this you pick a general entropy and basically what you're doing is you're setting up a duality between the the set of distributions over experts and sort of some loss space where you have a vector of losses for each expert and."
        ],
        [
            "The results we have is that Fortunately there exists fine mixable losses.",
            "And an interesting ones at that.",
            "And we sort of characterize when this is the case.",
            "And it turns out that you need this five function to be Legendre for there to be interesting, fine mixable losses.",
            "We can also express the fine mix ability of a loss as a kind of comparison between the entropy FI and entropy that you can induce from the loss, which is the Bayes risk.",
            "And it turns out that if you have 5X ability of a loss, you also satisfy a Blackwell condition for the jewel of the entropy that you're using.",
            "We have a number of open questions too.",
            "So if you consider this set of five mixable losses as he verify, we sort of conjecture that these might all be the same in fact, but we don't know at the moment, so it's an interesting question.",
            "For a fixed set of experts on a fixed loss, the loss may be mixable for many choices of fire.",
            "Is there an optimal one?",
            "What's that itis regret bound we can get?",
            "So we don't know and you can, given an entropy, you can always construct a loss for that entropy and it would be nice if such a loss was always mixable with respect to that, it's natural entropy.",
            "So if you're interested in any of that, please come to the post, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Rafan Bobana Shannon.",
                    "label": 0
                },
                {
                    "sent": "We've been thinking about mix ability for awhile, and these are some observations and generalizations that came about by trying to understand mix ability.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thankfully, Khamal has already introduced the prediction with expert advice setting that we use, but just to pin down some notation, we have a loss function where if you take an action A and then some outcome occurs, you get some penalty.",
                    "label": 1
                },
                {
                    "sent": "Ann, you have this game.",
                    "label": 0
                },
                {
                    "sent": "You fix the loss and you fix some number of experts and you just repeatedly have the experts play predictions.",
                    "label": 0
                },
                {
                    "sent": "The player makes prediction and outcome occurs.",
                    "label": 1
                },
                {
                    "sent": "And what we're interested in is which games of this type admitted constant regret.",
                    "label": 0
                },
                {
                    "sent": "That is, if you take the difference between the total cumulative loss for the player and subtract the best expert, when can you get this constant?",
                    "label": 0
                },
                {
                    "sent": "That may depend on L and the number of experts.",
                    "label": 0
                },
                {
                    "sent": "That bounds regret.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has been known for a long time.",
                    "label": 0
                },
                {
                    "sent": "Volf has this paper on.",
                    "label": 0
                },
                {
                    "sent": "We introduced the notion of mix ability, which is this expression here that says, no matter what mixture of experts you choose and actions they play.",
                    "label": 0
                },
                {
                    "sent": "You have to be able to guarantee that you can play an action that's better than this quantity here on the right, which I'm going to mix.",
                    "label": 0
                },
                {
                    "sent": "For the expert actions and distributions, and if you have this property, you can do this sort of exponential weight updates.",
                    "label": 0
                },
                {
                    "sent": "So mixtures an you can play the action that this property guarantees and you can guarantee a constant regret of log number of experts on Eater.",
                    "label": 0
                },
                {
                    "sent": "And so this is over well known.",
                    "label": 0
                },
                {
                    "sent": "And if you want this to hold for all experts, then this is a sort of fundamental property.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The observation we make in this paper is this related towards Sebastian was talking about.",
                    "label": 0
                },
                {
                    "sent": "I think is that this log sum X term in the definition of mix ability.",
                    "label": 0
                },
                {
                    "sent": "You can think of as a kind of restricted convex jewel of Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "So if you take this tool where you're restricting the jewel to the simplex, you can express this mix ability term in terms of a difference of this jewel of this entropy and interesting Lee, you can do this for any convex function on the simplex.",
                    "label": 0
                },
                {
                    "sent": "And you get you get a bunch of nice properties.",
                    "label": 0
                },
                {
                    "sent": "You get that the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient of the jewel of these entropies takes you back onto the simplex always.",
                    "label": 0
                },
                {
                    "sent": "It's just a result from convex analysis.",
                    "label": 0
                },
                {
                    "sent": "You get this translation invariant result and you get that you can write the difference in potentials as a kind of minimization, and it turns out this is all you need to get the constant regret result.",
                    "label": 0
                },
                {
                    "sent": "So it means that we can take the generalize the definition mix ability.",
                    "label": 0
                },
                {
                    "sent": "We can generalize the aggregating algorithm, and we can still guarantee constant regret, and I think this gives us a bit of insight into what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, the new definition of mix ability is naturally a tradeoff between a sort of fit to data and some notion of distance to prior.",
                    "label": 0
                },
                {
                    "sent": "And the aggregating algorithm just plays the mixture that witnesses the mixability bound at each step.",
                    "label": 0
                },
                {
                    "sent": "And actually you can show very easily that this is a.",
                    "label": 0
                },
                {
                    "sent": "Actually I kind of dual updating step and the regret we get is just in terms of the Bregman divergences between the corners of the simplex and your initial starting point.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so this you pick a general entropy and basically what you're doing is you're setting up a duality between the the set of distributions over experts and sort of some loss space where you have a vector of losses for each expert and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results we have is that Fortunately there exists fine mixable losses.",
                    "label": 0
                },
                {
                    "sent": "And an interesting ones at that.",
                    "label": 0
                },
                {
                    "sent": "And we sort of characterize when this is the case.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you need this five function to be Legendre for there to be interesting, fine mixable losses.",
                    "label": 0
                },
                {
                    "sent": "We can also express the fine mix ability of a loss as a kind of comparison between the entropy FI and entropy that you can induce from the loss, which is the Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you have 5X ability of a loss, you also satisfy a Blackwell condition for the jewel of the entropy that you're using.",
                    "label": 0
                },
                {
                    "sent": "We have a number of open questions too.",
                    "label": 0
                },
                {
                    "sent": "So if you consider this set of five mixable losses as he verify, we sort of conjecture that these might all be the same in fact, but we don't know at the moment, so it's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "For a fixed set of experts on a fixed loss, the loss may be mixable for many choices of fire.",
                    "label": 0
                },
                {
                    "sent": "Is there an optimal one?",
                    "label": 0
                },
                {
                    "sent": "What's that itis regret bound we can get?",
                    "label": 0
                },
                {
                    "sent": "So we don't know and you can, given an entropy, you can always construct a loss for that entropy and it would be nice if such a loss was always mixable with respect to that, it's natural entropy.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in any of that, please come to the post, thank you.",
                    "label": 0
                }
            ]
        }
    }
}