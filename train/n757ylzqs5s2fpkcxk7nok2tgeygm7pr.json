{
    "id": "n757ylzqs5s2fpkcxk7nok2tgeygm7pr",
    "title": "Beyond Time: Dynamic Context-aware Entity Recommendation",
    "info": {
        "author": [
            "Tuan Tran, L3S Research Center, Leibniz University of Hannover"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_tran_entity_recommendation/",
    "segmentation": [
        [
            "Thank you all for coming and my name is Tuan and now I'm going to give a talk which is slightly different in different directions from the other previous talks and it's about building recommended reading list entities for the entity recommendation.",
            "And."
        ],
        [
            "So what are the motivation of the work?",
            "Is that entity, semantic, relatedness, or to quantify how strongly two entities are semantically related to each other?",
            "Is one of the crucial tasks for a variety of applications.",
            "So from entity linking entity receiver to entity recommendation, yet this by a lot of areas works in these areas.",
            "This thing remains challenging work questions and one of the difficulties is that.",
            "Systematic region is in itself is a complex concept and take for example the rapid and Angelina Jolie.",
            "This strength in there really nice can change and depends on the life of conditions and situations.",
            "So they changed over times depending on the nature of their relationships from Co.",
            "Starring in a movie to being exposed and then getting the voice and even during the high speak of their high pic of their redundancies.",
            "It also be driven by a lot of context so.",
            "Be it personal life or career or their social activities into some humanities, humanities works.",
            "And so the."
        ],
        [
            "Idea in this role is that is insufficient to come up with the universe or matrix car for all contexts.",
            "Instead, we rather take a look into a certain context and try to estimate how to entities are correlated related to each other and in what we called the context to a religionis.",
            "So for example, we can talk about how Brad Pitt and Angelina Jolie related to respect to career and with some certain time."
        ],
        [
            "And I'm essentially all kind of the related measures comes with some optimism for validation scenarios in mind.",
            "An in our work we try to optimize the entity recommended scenarios.",
            "Namely, we assume that the context here reflects some sort of the user intentions when they want to explore the user.",
            "The entity of interest.",
            "So anybody looking into might be interested in either his personal life or his career.",
            "And can look into different phase in his period of life and during each times a good semantic rated needs for entities will be can be used directly as the recommended score and can boost up the experience in general.",
            "So for example here if we talk about the career aspect of Brad Pitt during to the F15 then it makes much more sense to boost the movie florita the effort in which is just be releases a few months back rather than some other.",
            "Big movies of him.",
            "First Mississippi's or legends of the fall."
        ],
        [
            "So in summary, our approach can be summarized in one single sentence, so we want to learn the user navigation activities, which I'm going to expand along this talk.",
            "So for support here, if we observe that under certain context that user decides to move from a profiler Brad Pitt to the movie flurry into the 14th, we can assume that there is some established Association between two entities under that context.",
            "And as we collect the navigation behavior from.",
            "Users this statistics statistic of entities from that context will give us a hint about how strongly related the entities are with respect to rapid under that context."
        ],
        [
            "OK, so before we go into the technical details, I'm going to introduce some basic concepts.",
            "So first of all, we assume that there are available Internet compass or the corpus where we have the hell or high quality entity mentions to the well disseminated entities.",
            "So one obvious an popular examples can be Wikipedia where we have the links from the text to wiki pages or profile and entities or other.",
            "Actually there are many other.",
            "Examples nowadays due to the increase advance in the entity linking's, other examples can be the Freebase.",
            "Stream purpose with the free education released by the researchers, and.",
            "Each entity is also equipped with the entity profile or the text or description of the entities.",
            "Can it be a home page of the researchers or the Wikipedia page of a political science?",
            "For the context, in a work context consists of two dimensions, so the text or representations are we called the topic of the context and the time span or the interval in the daily granularity for the query we have an entity at hand and also the query context, which consists of the query terms at the time span for rapid or pics and the year 2016.",
            "And disease can also be represented as a free text with the help of some dictionary or the text to entity mapping and all of the components at the time or topic here are completely optional, so in case of any missing context components, we can simply push them as the placeholder to say that anything would be acceptable."
        ],
        [
            "And now the next concept won't be the entity graph.",
            "And how are we going to build context from that so entities in our case come from the terminal list of knowledge base.",
            "So at the Wikipedia or Freebase or some other linked data sources and the edges of entities can come from other links of their profile, or the facts in the knowledge base.",
            "Now we are going to build a context for entity graph and by that I mean contextual.",
            "I bought the note and the edges for the graph.",
            "For the nodes we.",
            "We simply take Contacts from the entity profile and from the edges we calculate how we compute what is called the pseudo context.",
            "Because there's no one to one mapping between the edge and the context is rather the context for any undirected NTP entity pairs.",
            "So here, what are we going to do?",
            "We take a look into the coppers and get on kind of the bout text snippets.",
            "In our case, will be sentences, and then we look into the presence of the entities in that.",
            "Doing some temporal pattern matchings to detect the temporal expressions and then build the corpus of these two components.",
            "So for example, here we look into sentences from the Wikipedia attracted temporal patterns and if we don't find anything we put with a placeholder which simply says that anything from the early to the latest time spent possible in our knowledge base and then we build as context an accumulated for any pairs of two entities."
        ],
        [
            "Alright, now with that on that setup we have the next methods, which is one of the contribution of our work and in this work we propose the idea of what is called entity embedding and the idea is that we try to map the entities from the entity graph to the continuous representation of vectors which can significantly simplify and also improve the computation of the entity.",
            "Related this and what we are doing is that we will learn simultaneously both the entities and the worse.",
            "From the corpus and from the entity graphs.",
            "So first we take a look into all kind of worse in the entity profile and then for each of these words we try to predict the distribute the context of the edges of the growth of that grows inside the energetic corpus.",
            "So for example, here we can look into the profile of the entity Jennifer Aniston's take one word and that and look into all Contacts that we have built for the entity graph.",
            "Seeing how are the other words an entities are agency in that worse?",
            "And then the pretty certain function would be this simple softmax function.",
            "For because we learned from the context and from the profile rather than from the entire corpus, the sparsity is will become more severe, and for that we need some adapted negative sampling for the cost function.",
            "So we negatively sample words from the profiles and also negatively sampled the entities from their corpus.",
            "So taking some indicative papers, entities and was based on their frequencies."
        ],
        [
            "Alright, now we own that set up.",
            "We go to.",
            "The problem with this model or how well we're going to recommend the entities.",
            "So in our work we model them in the generative settings where we try to model the context to a related list as the probability of user coming on one entities given one query and with a few of versions calculations we can't do, we can estimate them as a joint probability between the entities and the queries given the time.",
            "Great, I'm at the query terms and also with some simple biasion inference tricks.",
            "We can factorize this.",
            "We can factorize these probabilities into two factors.",
            "One is called temporalities, which is purely based on the time and the other is topical risk, which is also based on both query terms and times and the context.",
            "Now for the estimates, and we do the linear models where for the temporary list we assume that there is a linear combination between call.",
            "It is called static recentness and the dynamic release with some parameters for the control and for the topic or either this we will assume that there is a linear combination between the similarity models between any kind of single context and the query terms, and then we sum them up or or the context that we can extract from the two entities pairs."
        ],
        [
            "For the temporal region, this estimation, we apply two approaches for estimating these statically written lists.",
            "The first approach is called mini wit and which is quite popular, and one of the very simple and effective metrics for calculating the rigidness between two entities.",
            "So essentially the media we can calculate the Co occurrence between two entities and saying that based on their incoming links how closely they are ends the matrix.",
            "Derived from the mutual information angle, it isn't.",
            "We also calculate the static static release of 20s by based on their embedded vectors.",
            "So we calculate the cosine similarity between two vectors and get this as a static written this for the dynamic rigidness.",
            "We look into the activation functions of the entities, so we assume that there in any entities at any point in time will have some function to say how important they are.",
            "And this function can be derived from some source or sources.",
            "For example, the Wikipedia page views foreign entity or the edit history of the entities.",
            "In our case, we work with away, extract the time series of the Wikipedia page.",
            "Views is normalized them by.",
            "Dropping out the expectation and standard deviation factors and then calculate the pickle code coherence between the two entities."
        ],
        [
            "For the topic or released estimations, we actually do two models.",
            "The first model is called language models, where we assume that every topic or every text nip it in the context can be can be seen as the document and we see how we can generate the terms from the queries to that documents.",
            "So then we assume that there is an independent with all documents or context and between the query terms the next.",
            "Models for the topic of the estimates and East is also based on the event vectors of entities, and the idea is that now if we have all, this is actually the metric called word verse distance, and the idea can be illustrated from the pictures here, where we have two documents and for each documents we will see how we can move from any non stop words from one document to the closest one in the other documents and there's some distant roost will tell us about how closely two documents are in the.",
            "Embedding space, and it turns out that is quite effective and one of the state of the art in calculating the documents similarities.",
            "Also, we take into account the time factors here, saying that now for every document for every context and the query terms we also see how far they are in terms of the terms, and saying that there's a decay factor in the similarity.",
            "So the closer they are, the more contributing the similarity metric come into the role."
        ],
        [
            "OK, so now for datasets we experiment with.",
            "The Wikipedia dumped alot from March to an 15 as annotated corpus and becausw.",
            "Wikipedia consists in complete sets of the links.",
            "We also applied to these machine learning models.",
            "We called 3 top level to enrich any links in the text or content that we keep our pages so that we can get more interested about the links so far support if you talk about the profile of a bat pages, not many links actually.",
            "Not many message actually coming to other web pages because of the incomplete sets of the user editing process and so 3 W system here will help us in getting that complete set of annotations for the entity graph.",
            "We get the Wikipedia pages and calculate the overlapping between them and the freebase entities and get in total about three millions and 100,000 entities.",
            "And we actually answer about 100 and 108 million.",
            "Context from the edges and for building the temporal dynamic context, we calculate the time story up Wikipedia page views downloads from February 12th till major and 16.",
            "For extracting the context, the temporal patterns from the context we used for temporal parcel."
        ],
        [
            "Now to build the query at the graduate automatically, we realize on the datasets which is called Wiki, Black Clickstream data sets.",
            "It contains about 20 million different clicks, navigation click logs from one entity from Wikipedia page, the other pages within two months from February from January and February 2015, and we use fabric with the air fitting as one of our query time and for the query terms we extract or section Hector headings.",
            "From the entities.",
            "So the idea is that if user clicks from one entity of 1 wiki pages to the other wiki pages, and if that page if that entity only appears in uncertain situation principle, personal life or career that we assume that user has some interest in here interest in that aspect of that context of the two entities.",
            "With that holistic we built about 102 hundred 200,000 queries and to get the correct oven entities for graduates, we get all entities which have at least five clicks and to get to 5 entities, burgundy queries in total we have about 200,000 queries and too many extents.",
            "Our datasets is the 1st and largest of this guide.",
            "We also publicly public datasets for the research of these similar.",
            "9."
        ],
        [
            "Now for the experiments we.",
            "I'm compare our method against two.",
            "To set up the ASK the first, everything else is middle written for the septic recently and the secret ones from the paper in ICIS, Lucifer and 16th last year actually get the best paper award and for our queryset we want to see the impact of times and what we do is that we categorize the queries into four groups based on the ratio of how people views one entities from one month compared to the previous month.",
            "So the idea is that if there is a higher the resources, then there is a trending or there's some interest in this particular entity is for this particular month and we can see that.",
            "Here for the entity with the higher ratio, meaning that there's some training interests, there's a.",
            "There's a clear and significant improvement about our methods, and also the temporal and dynamic methods compared to the static one."
        ],
        [
            "In terms of recall, we also varying the different cut off threshold.",
            "So from five to five entities to the 50 entities and see how entities are clicked in terms of retrieved and recommended, and we can see that on our system we actually improve over 90% seven point 5 seven point, 9% to the statics and we together can get about 15 point.",
            "2% of the improvements compared with static methods.",
            "Come back with the temporal methods.",
            "Week also gets a significant improvement, especially for the trending entities and we get about 14.3% improvement."
        ],
        [
            "Now if we want to see the impact of topics, we will see the different categories of entities.",
            "So we give the query entities into four groups, one above the person, one about location organizing and the others.",
            "And the idea is that there with the person are organizing group.",
            "There are some other different aspect or topic or.",
            "We did it through each entities of interest, while for Lucas and the terms will pay more important and then here you can see that for the person Zanfer organization group, we actually get a significant improvement over the temporal, so adding the topic here actually plays a much better role in fighting the highly recommended, highly relevant entities.",
            "And what is interesting is that for the organization group actually going for temporal or in static, this would not make so much difference."
        ],
        [
            "OK, so to conclude, in this work, we propose a new model for the context to release an.",
            "It can improve the entity recommended scenarios in general cases, especially in the different entities or persons organizations we see the clear improvement of our topic aware system compared to the time where aware system from the SFC papers and also we can see that in different scenarios time.",
            "Contacts are very well complements to each other and for the future directions we want to see how we can generalize the different query sets, so not just coming from the section headings, but also come from the more general query like system line from search engines and we also want to cover a different time spans an granularities.",
            "We also want to see how this system our system performs in the presence of the context ambiguity so.",
            "People talk about personal lines.",
            "They can talk about some more granular context, or they want to talk about some other aspects.",
            "So we want to see how our system can do it in a more flexible and efficient way.",
            "And of course we also want to see how we can make a better way to interpret the topicals installed, not just coming from the section of the Wikipedia page."
        ],
        [
            "So.",
            "Thank you, thank you.",
            "Kind of."
        ],
        [
            "Rely on the fact that these entities are very well known and clicked on or whatever.",
            "But if we look at the entities, for example that are on DB pedia, Wikipedia, whatever, there is a very long tail is not maintain, is not frequent.",
            "So what happens if you have to cope with these type of entities so you get noise, because for example it is not being updated.",
            "The guy is no longer married with that person, but you assume that is married still by the person and therefore so just OK.",
            "So thank you.",
            "It's a very good question.",
            "So yes.",
            "We acknowledge that the fact that we rely on the Wikipedia clickstreams actually very biased squares the heavy child towards the popular entities.",
            "So and it's many of the entities in our graduates datasets actually come from the popular One becausw.",
            "It simply means that if we look into the clickstreams, there can be a lot of hidden factors driving how people will go from one entities to the others, and.",
            "As we rely on the statistique of this of such navigation, we.",
            "We will not be able to.",
            "So there will be some bias and errors errors in the long term entities and we wanted to improve this situations by building our applications and ask people for navigating into some control experiment for some other long-term entities.",
            "But on it is speaking the graduates of the datasets that we have actually is not as scale and quality that we have from the classrooms, so it's been an opening questions for our future world so.",
            "Does it mean that if by using an over simplification, the long tail is 20% of the entities, 20% of the queries?",
            "Chris Webber 20% error on top of water.",
            "You could have on the head so you have the error on the head plus the error on the 20%.",
            "Is it possible is so catastrophic?",
            "Well.",
            "I would say no because.",
            "So many of these currents there that in entity realness like Miller, Witten or even TR from the chaotic groups they actually get the same suffer from the launcher entities.",
            "Here we build the generic frameworks with some machinery that can cope with the general case given that we have a better, say clickstream, our growth oriented coppers.",
            "An such corpus.",
            "Actually we tried our best with the enrichment from the Wikipedia and we also do some experiments.",
            "With and without such enrichment, and because the idea is that with the enrichment for Wikipedia datasets, we can hopefully get more long-term tease into the into the place, and we see that there is some improvement, but not much for the event is in our in our experiments.",
            "And then we postulate that actually what can improve is that we will simply see user clicks more from the launcher entities.",
            "But yeah, so that that's the one issue that we we have to investigate, but we don't see any clearly improvement with.",
            "If we drop out the the management process, which means that the long term is actually cannot get that much problem affected by these incomplete of the coppers.",
            "OK, thank you.",
            "Is there any other question by the six people who have left or left in the room?",
            "Oh yeah, we have this one.",
            "That's great.",
            "Thank you very great.",
            "Do you have in your a model olanda parameter?",
            "Have you talked to Nick to estimate the value of the London parameter?",
            "Or you just choose whom are Italy one Valley?",
            "Yes, so actually we have about 3 parameters.",
            "One is the threshold for the linear combination of the temporal, the other is a threshold for temporal and the rest is the transfer on the topic, or so the mover distance things and we just do research.",
            "So we have three parameters, which is normal and do research.",
            "Getting the empirical estimation for the parameters.",
            "Yep.",
            "In the high balance between the two different values and you can choose yeah, so actually we also have one experiments about the sensitivity of the Parramatta 2 tunings, which I did not put here because of the lack of the time.",
            "So we do calculate the performance DMR for different parameter settings and we did not see much of the sensitivity of our system, at least compared with the baseline, and we see a few changing from the transfer, but not much so.",
            "Is it also means that our system actually gets more benefits from the Crown?",
            "Suzanne from from the, say the matrix rather than from the learning of the combination.",
            "OK, any final question was let's thank our speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you all for coming and my name is Tuan and now I'm going to give a talk which is slightly different in different directions from the other previous talks and it's about building recommended reading list entities for the entity recommendation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the motivation of the work?",
                    "label": 0
                },
                {
                    "sent": "Is that entity, semantic, relatedness, or to quantify how strongly two entities are semantically related to each other?",
                    "label": 1
                },
                {
                    "sent": "Is one of the crucial tasks for a variety of applications.",
                    "label": 0
                },
                {
                    "sent": "So from entity linking entity receiver to entity recommendation, yet this by a lot of areas works in these areas.",
                    "label": 0
                },
                {
                    "sent": "This thing remains challenging work questions and one of the difficulties is that.",
                    "label": 1
                },
                {
                    "sent": "Systematic region is in itself is a complex concept and take for example the rapid and Angelina Jolie.",
                    "label": 0
                },
                {
                    "sent": "This strength in there really nice can change and depends on the life of conditions and situations.",
                    "label": 0
                },
                {
                    "sent": "So they changed over times depending on the nature of their relationships from Co.",
                    "label": 0
                },
                {
                    "sent": "Starring in a movie to being exposed and then getting the voice and even during the high speak of their high pic of their redundancies.",
                    "label": 0
                },
                {
                    "sent": "It also be driven by a lot of context so.",
                    "label": 0
                },
                {
                    "sent": "Be it personal life or career or their social activities into some humanities, humanities works.",
                    "label": 0
                },
                {
                    "sent": "And so the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea in this role is that is insufficient to come up with the universe or matrix car for all contexts.",
                    "label": 0
                },
                {
                    "sent": "Instead, we rather take a look into a certain context and try to estimate how to entities are correlated related to each other and in what we called the context to a religionis.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can talk about how Brad Pitt and Angelina Jolie related to respect to career and with some certain time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm essentially all kind of the related measures comes with some optimism for validation scenarios in mind.",
                    "label": 0
                },
                {
                    "sent": "An in our work we try to optimize the entity recommended scenarios.",
                    "label": 0
                },
                {
                    "sent": "Namely, we assume that the context here reflects some sort of the user intentions when they want to explore the user.",
                    "label": 0
                },
                {
                    "sent": "The entity of interest.",
                    "label": 0
                },
                {
                    "sent": "So anybody looking into might be interested in either his personal life or his career.",
                    "label": 0
                },
                {
                    "sent": "And can look into different phase in his period of life and during each times a good semantic rated needs for entities will be can be used directly as the recommended score and can boost up the experience in general.",
                    "label": 0
                },
                {
                    "sent": "So for example here if we talk about the career aspect of Brad Pitt during to the F15 then it makes much more sense to boost the movie florita the effort in which is just be releases a few months back rather than some other.",
                    "label": 0
                },
                {
                    "sent": "Big movies of him.",
                    "label": 0
                },
                {
                    "sent": "First Mississippi's or legends of the fall.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, our approach can be summarized in one single sentence, so we want to learn the user navigation activities, which I'm going to expand along this talk.",
                    "label": 0
                },
                {
                    "sent": "So for support here, if we observe that under certain context that user decides to move from a profiler Brad Pitt to the movie flurry into the 14th, we can assume that there is some established Association between two entities under that context.",
                    "label": 0
                },
                {
                    "sent": "And as we collect the navigation behavior from.",
                    "label": 0
                },
                {
                    "sent": "Users this statistics statistic of entities from that context will give us a hint about how strongly related the entities are with respect to rapid under that context.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so before we go into the technical details, I'm going to introduce some basic concepts.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we assume that there are available Internet compass or the corpus where we have the hell or high quality entity mentions to the well disseminated entities.",
                    "label": 0
                },
                {
                    "sent": "So one obvious an popular examples can be Wikipedia where we have the links from the text to wiki pages or profile and entities or other.",
                    "label": 0
                },
                {
                    "sent": "Actually there are many other.",
                    "label": 0
                },
                {
                    "sent": "Examples nowadays due to the increase advance in the entity linking's, other examples can be the Freebase.",
                    "label": 0
                },
                {
                    "sent": "Stream purpose with the free education released by the researchers, and.",
                    "label": 0
                },
                {
                    "sent": "Each entity is also equipped with the entity profile or the text or description of the entities.",
                    "label": 1
                },
                {
                    "sent": "Can it be a home page of the researchers or the Wikipedia page of a political science?",
                    "label": 1
                },
                {
                    "sent": "For the context, in a work context consists of two dimensions, so the text or representations are we called the topic of the context and the time span or the interval in the daily granularity for the query we have an entity at hand and also the query context, which consists of the query terms at the time span for rapid or pics and the year 2016.",
                    "label": 0
                },
                {
                    "sent": "And disease can also be represented as a free text with the help of some dictionary or the text to entity mapping and all of the components at the time or topic here are completely optional, so in case of any missing context components, we can simply push them as the placeholder to say that anything would be acceptable.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now the next concept won't be the entity graph.",
                    "label": 1
                },
                {
                    "sent": "And how are we going to build context from that so entities in our case come from the terminal list of knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So at the Wikipedia or Freebase or some other linked data sources and the edges of entities can come from other links of their profile, or the facts in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Now we are going to build a context for entity graph and by that I mean contextual.",
                    "label": 0
                },
                {
                    "sent": "I bought the note and the edges for the graph.",
                    "label": 0
                },
                {
                    "sent": "For the nodes we.",
                    "label": 0
                },
                {
                    "sent": "We simply take Contacts from the entity profile and from the edges we calculate how we compute what is called the pseudo context.",
                    "label": 1
                },
                {
                    "sent": "Because there's no one to one mapping between the edge and the context is rather the context for any undirected NTP entity pairs.",
                    "label": 1
                },
                {
                    "sent": "So here, what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "We take a look into the coppers and get on kind of the bout text snippets.",
                    "label": 0
                },
                {
                    "sent": "In our case, will be sentences, and then we look into the presence of the entities in that.",
                    "label": 0
                },
                {
                    "sent": "Doing some temporal pattern matchings to detect the temporal expressions and then build the corpus of these two components.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we look into sentences from the Wikipedia attracted temporal patterns and if we don't find anything we put with a placeholder which simply says that anything from the early to the latest time spent possible in our knowledge base and then we build as context an accumulated for any pairs of two entities.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, now with that on that setup we have the next methods, which is one of the contribution of our work and in this work we propose the idea of what is called entity embedding and the idea is that we try to map the entities from the entity graph to the continuous representation of vectors which can significantly simplify and also improve the computation of the entity.",
                    "label": 0
                },
                {
                    "sent": "Related this and what we are doing is that we will learn simultaneously both the entities and the worse.",
                    "label": 0
                },
                {
                    "sent": "From the corpus and from the entity graphs.",
                    "label": 1
                },
                {
                    "sent": "So first we take a look into all kind of worse in the entity profile and then for each of these words we try to predict the distribute the context of the edges of the growth of that grows inside the energetic corpus.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we can look into the profile of the entity Jennifer Aniston's take one word and that and look into all Contacts that we have built for the entity graph.",
                    "label": 0
                },
                {
                    "sent": "Seeing how are the other words an entities are agency in that worse?",
                    "label": 0
                },
                {
                    "sent": "And then the pretty certain function would be this simple softmax function.",
                    "label": 0
                },
                {
                    "sent": "For because we learned from the context and from the profile rather than from the entire corpus, the sparsity is will become more severe, and for that we need some adapted negative sampling for the cost function.",
                    "label": 1
                },
                {
                    "sent": "So we negatively sample words from the profiles and also negatively sampled the entities from their corpus.",
                    "label": 0
                },
                {
                    "sent": "So taking some indicative papers, entities and was based on their frequencies.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, now we own that set up.",
                    "label": 0
                },
                {
                    "sent": "We go to.",
                    "label": 0
                },
                {
                    "sent": "The problem with this model or how well we're going to recommend the entities.",
                    "label": 0
                },
                {
                    "sent": "So in our work we model them in the generative settings where we try to model the context to a related list as the probability of user coming on one entities given one query and with a few of versions calculations we can't do, we can estimate them as a joint probability between the entities and the queries given the time.",
                    "label": 0
                },
                {
                    "sent": "Great, I'm at the query terms and also with some simple biasion inference tricks.",
                    "label": 0
                },
                {
                    "sent": "We can factorize this.",
                    "label": 0
                },
                {
                    "sent": "We can factorize these probabilities into two factors.",
                    "label": 0
                },
                {
                    "sent": "One is called temporalities, which is purely based on the time and the other is topical risk, which is also based on both query terms and times and the context.",
                    "label": 0
                },
                {
                    "sent": "Now for the estimates, and we do the linear models where for the temporary list we assume that there is a linear combination between call.",
                    "label": 0
                },
                {
                    "sent": "It is called static recentness and the dynamic release with some parameters for the control and for the topic or either this we will assume that there is a linear combination between the similarity models between any kind of single context and the query terms, and then we sum them up or or the context that we can extract from the two entities pairs.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the temporal region, this estimation, we apply two approaches for estimating these statically written lists.",
                    "label": 0
                },
                {
                    "sent": "The first approach is called mini wit and which is quite popular, and one of the very simple and effective metrics for calculating the rigidness between two entities.",
                    "label": 0
                },
                {
                    "sent": "So essentially the media we can calculate the Co occurrence between two entities and saying that based on their incoming links how closely they are ends the matrix.",
                    "label": 0
                },
                {
                    "sent": "Derived from the mutual information angle, it isn't.",
                    "label": 0
                },
                {
                    "sent": "We also calculate the static static release of 20s by based on their embedded vectors.",
                    "label": 0
                },
                {
                    "sent": "So we calculate the cosine similarity between two vectors and get this as a static written this for the dynamic rigidness.",
                    "label": 0
                },
                {
                    "sent": "We look into the activation functions of the entities, so we assume that there in any entities at any point in time will have some function to say how important they are.",
                    "label": 0
                },
                {
                    "sent": "And this function can be derived from some source or sources.",
                    "label": 0
                },
                {
                    "sent": "For example, the Wikipedia page views foreign entity or the edit history of the entities.",
                    "label": 0
                },
                {
                    "sent": "In our case, we work with away, extract the time series of the Wikipedia page.",
                    "label": 0
                },
                {
                    "sent": "Views is normalized them by.",
                    "label": 0
                },
                {
                    "sent": "Dropping out the expectation and standard deviation factors and then calculate the pickle code coherence between the two entities.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the topic or released estimations, we actually do two models.",
                    "label": 0
                },
                {
                    "sent": "The first model is called language models, where we assume that every topic or every text nip it in the context can be can be seen as the document and we see how we can generate the terms from the queries to that documents.",
                    "label": 0
                },
                {
                    "sent": "So then we assume that there is an independent with all documents or context and between the query terms the next.",
                    "label": 0
                },
                {
                    "sent": "Models for the topic of the estimates and East is also based on the event vectors of entities, and the idea is that now if we have all, this is actually the metric called word verse distance, and the idea can be illustrated from the pictures here, where we have two documents and for each documents we will see how we can move from any non stop words from one document to the closest one in the other documents and there's some distant roost will tell us about how closely two documents are in the.",
                    "label": 0
                },
                {
                    "sent": "Embedding space, and it turns out that is quite effective and one of the state of the art in calculating the documents similarities.",
                    "label": 0
                },
                {
                    "sent": "Also, we take into account the time factors here, saying that now for every document for every context and the query terms we also see how far they are in terms of the terms, and saying that there's a decay factor in the similarity.",
                    "label": 0
                },
                {
                    "sent": "So the closer they are, the more contributing the similarity metric come into the role.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now for datasets we experiment with.",
                    "label": 0
                },
                {
                    "sent": "The Wikipedia dumped alot from March to an 15 as annotated corpus and becausw.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia consists in complete sets of the links.",
                    "label": 0
                },
                {
                    "sent": "We also applied to these machine learning models.",
                    "label": 0
                },
                {
                    "sent": "We called 3 top level to enrich any links in the text or content that we keep our pages so that we can get more interested about the links so far support if you talk about the profile of a bat pages, not many links actually.",
                    "label": 0
                },
                {
                    "sent": "Not many message actually coming to other web pages because of the incomplete sets of the user editing process and so 3 W system here will help us in getting that complete set of annotations for the entity graph.",
                    "label": 0
                },
                {
                    "sent": "We get the Wikipedia pages and calculate the overlapping between them and the freebase entities and get in total about three millions and 100,000 entities.",
                    "label": 0
                },
                {
                    "sent": "And we actually answer about 100 and 108 million.",
                    "label": 0
                },
                {
                    "sent": "Context from the edges and for building the temporal dynamic context, we calculate the time story up Wikipedia page views downloads from February 12th till major and 16.",
                    "label": 0
                },
                {
                    "sent": "For extracting the context, the temporal patterns from the context we used for temporal parcel.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to build the query at the graduate automatically, we realize on the datasets which is called Wiki, Black Clickstream data sets.",
                    "label": 0
                },
                {
                    "sent": "It contains about 20 million different clicks, navigation click logs from one entity from Wikipedia page, the other pages within two months from February from January and February 2015, and we use fabric with the air fitting as one of our query time and for the query terms we extract or section Hector headings.",
                    "label": 0
                },
                {
                    "sent": "From the entities.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if user clicks from one entity of 1 wiki pages to the other wiki pages, and if that page if that entity only appears in uncertain situation principle, personal life or career that we assume that user has some interest in here interest in that aspect of that context of the two entities.",
                    "label": 0
                },
                {
                    "sent": "With that holistic we built about 102 hundred 200,000 queries and to get the correct oven entities for graduates, we get all entities which have at least five clicks and to get to 5 entities, burgundy queries in total we have about 200,000 queries and too many extents.",
                    "label": 0
                },
                {
                    "sent": "Our datasets is the 1st and largest of this guide.",
                    "label": 0
                },
                {
                    "sent": "We also publicly public datasets for the research of these similar.",
                    "label": 0
                },
                {
                    "sent": "9.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for the experiments we.",
                    "label": 0
                },
                {
                    "sent": "I'm compare our method against two.",
                    "label": 0
                },
                {
                    "sent": "To set up the ASK the first, everything else is middle written for the septic recently and the secret ones from the paper in ICIS, Lucifer and 16th last year actually get the best paper award and for our queryset we want to see the impact of times and what we do is that we categorize the queries into four groups based on the ratio of how people views one entities from one month compared to the previous month.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if there is a higher the resources, then there is a trending or there's some interest in this particular entity is for this particular month and we can see that.",
                    "label": 0
                },
                {
                    "sent": "Here for the entity with the higher ratio, meaning that there's some training interests, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a clear and significant improvement about our methods, and also the temporal and dynamic methods compared to the static one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of recall, we also varying the different cut off threshold.",
                    "label": 0
                },
                {
                    "sent": "So from five to five entities to the 50 entities and see how entities are clicked in terms of retrieved and recommended, and we can see that on our system we actually improve over 90% seven point 5 seven point, 9% to the statics and we together can get about 15 point.",
                    "label": 0
                },
                {
                    "sent": "2% of the improvements compared with static methods.",
                    "label": 0
                },
                {
                    "sent": "Come back with the temporal methods.",
                    "label": 0
                },
                {
                    "sent": "Week also gets a significant improvement, especially for the trending entities and we get about 14.3% improvement.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if we want to see the impact of topics, we will see the different categories of entities.",
                    "label": 1
                },
                {
                    "sent": "So we give the query entities into four groups, one above the person, one about location organizing and the others.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that there with the person are organizing group.",
                    "label": 0
                },
                {
                    "sent": "There are some other different aspect or topic or.",
                    "label": 0
                },
                {
                    "sent": "We did it through each entities of interest, while for Lucas and the terms will pay more important and then here you can see that for the person Zanfer organization group, we actually get a significant improvement over the temporal, so adding the topic here actually plays a much better role in fighting the highly recommended, highly relevant entities.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting is that for the organization group actually going for temporal or in static, this would not make so much difference.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude, in this work, we propose a new model for the context to release an.",
                    "label": 0
                },
                {
                    "sent": "It can improve the entity recommended scenarios in general cases, especially in the different entities or persons organizations we see the clear improvement of our topic aware system compared to the time where aware system from the SFC papers and also we can see that in different scenarios time.",
                    "label": 1
                },
                {
                    "sent": "Contacts are very well complements to each other and for the future directions we want to see how we can generalize the different query sets, so not just coming from the section headings, but also come from the more general query like system line from search engines and we also want to cover a different time spans an granularities.",
                    "label": 0
                },
                {
                    "sent": "We also want to see how this system our system performs in the presence of the context ambiguity so.",
                    "label": 0
                },
                {
                    "sent": "People talk about personal lines.",
                    "label": 0
                },
                {
                    "sent": "They can talk about some more granular context, or they want to talk about some other aspects.",
                    "label": 0
                },
                {
                    "sent": "So we want to see how our system can do it in a more flexible and efficient way.",
                    "label": 0
                },
                {
                    "sent": "And of course we also want to see how we can make a better way to interpret the topicals installed, not just coming from the section of the Wikipedia page.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rely on the fact that these entities are very well known and clicked on or whatever.",
                    "label": 0
                },
                {
                    "sent": "But if we look at the entities, for example that are on DB pedia, Wikipedia, whatever, there is a very long tail is not maintain, is not frequent.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you have to cope with these type of entities so you get noise, because for example it is not being updated.",
                    "label": 0
                },
                {
                    "sent": "The guy is no longer married with that person, but you assume that is married still by the person and therefore so just OK.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "It's a very good question.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "We acknowledge that the fact that we rely on the Wikipedia clickstreams actually very biased squares the heavy child towards the popular entities.",
                    "label": 0
                },
                {
                    "sent": "So and it's many of the entities in our graduates datasets actually come from the popular One becausw.",
                    "label": 0
                },
                {
                    "sent": "It simply means that if we look into the clickstreams, there can be a lot of hidden factors driving how people will go from one entities to the others, and.",
                    "label": 0
                },
                {
                    "sent": "As we rely on the statistique of this of such navigation, we.",
                    "label": 0
                },
                {
                    "sent": "We will not be able to.",
                    "label": 0
                },
                {
                    "sent": "So there will be some bias and errors errors in the long term entities and we wanted to improve this situations by building our applications and ask people for navigating into some control experiment for some other long-term entities.",
                    "label": 0
                },
                {
                    "sent": "But on it is speaking the graduates of the datasets that we have actually is not as scale and quality that we have from the classrooms, so it's been an opening questions for our future world so.",
                    "label": 0
                },
                {
                    "sent": "Does it mean that if by using an over simplification, the long tail is 20% of the entities, 20% of the queries?",
                    "label": 0
                },
                {
                    "sent": "Chris Webber 20% error on top of water.",
                    "label": 0
                },
                {
                    "sent": "You could have on the head so you have the error on the head plus the error on the 20%.",
                    "label": 0
                },
                {
                    "sent": "Is it possible is so catastrophic?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I would say no because.",
                    "label": 0
                },
                {
                    "sent": "So many of these currents there that in entity realness like Miller, Witten or even TR from the chaotic groups they actually get the same suffer from the launcher entities.",
                    "label": 0
                },
                {
                    "sent": "Here we build the generic frameworks with some machinery that can cope with the general case given that we have a better, say clickstream, our growth oriented coppers.",
                    "label": 0
                },
                {
                    "sent": "An such corpus.",
                    "label": 0
                },
                {
                    "sent": "Actually we tried our best with the enrichment from the Wikipedia and we also do some experiments.",
                    "label": 0
                },
                {
                    "sent": "With and without such enrichment, and because the idea is that with the enrichment for Wikipedia datasets, we can hopefully get more long-term tease into the into the place, and we see that there is some improvement, but not much for the event is in our in our experiments.",
                    "label": 0
                },
                {
                    "sent": "And then we postulate that actually what can improve is that we will simply see user clicks more from the launcher entities.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so that that's the one issue that we we have to investigate, but we don't see any clearly improvement with.",
                    "label": 0
                },
                {
                    "sent": "If we drop out the the management process, which means that the long term is actually cannot get that much problem affected by these incomplete of the coppers.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Is there any other question by the six people who have left or left in the room?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, we have this one.",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "Thank you very great.",
                    "label": 0
                },
                {
                    "sent": "Do you have in your a model olanda parameter?",
                    "label": 0
                },
                {
                    "sent": "Have you talked to Nick to estimate the value of the London parameter?",
                    "label": 0
                },
                {
                    "sent": "Or you just choose whom are Italy one Valley?",
                    "label": 0
                },
                {
                    "sent": "Yes, so actually we have about 3 parameters.",
                    "label": 0
                },
                {
                    "sent": "One is the threshold for the linear combination of the temporal, the other is a threshold for temporal and the rest is the transfer on the topic, or so the mover distance things and we just do research.",
                    "label": 0
                },
                {
                    "sent": "So we have three parameters, which is normal and do research.",
                    "label": 0
                },
                {
                    "sent": "Getting the empirical estimation for the parameters.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "In the high balance between the two different values and you can choose yeah, so actually we also have one experiments about the sensitivity of the Parramatta 2 tunings, which I did not put here because of the lack of the time.",
                    "label": 0
                },
                {
                    "sent": "So we do calculate the performance DMR for different parameter settings and we did not see much of the sensitivity of our system, at least compared with the baseline, and we see a few changing from the transfer, but not much so.",
                    "label": 0
                },
                {
                    "sent": "Is it also means that our system actually gets more benefits from the Crown?",
                    "label": 0
                },
                {
                    "sent": "Suzanne from from the, say the matrix rather than from the learning of the combination.",
                    "label": 0
                },
                {
                    "sent": "OK, any final question was let's thank our speaker again.",
                    "label": 0
                }
            ]
        }
    }
}