{
    "id": "5gdkq6smhav5osbiljyqyb7mcn3uyxjc",
    "title": "Pretrained Transformers for Simple Question Answering",
    "info": {
        "author": [
            "Denis Lukovnikov, University of Bonn"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_lukovnikov_simple_question_answering/",
    "segmentation": [
        [
            "Hi, my name is Dennis.",
            "Because I think another work on using pre train Transformers for answering simple questions on scrubs.",
            "The."
        ],
        [
            "So here we only consider simple questions which require."
        ],
        [
            "Only.",
            "A single entity in relation to be answered to be determined to answer the question, so the queries will be.",
            "A single triple pattern without any additional counts or.",
            "Reasons.",
            "So question answering is also hard for the simple questions because of."
        ],
        [
            "Many different ways in which you can express the.",
            "Questions for the same query.",
            "Yeah, so the.",
            "Wanted to see how."
        ],
        [
            "Oh well, this very model is actually working.",
            "Would work with simple questions and.",
            "First, some background."
        ],
        [
            "Formers so Transformers are not very different scenes for sequence processing they."
        ],
        [
            "Rely mostly on linear layers and attention mechanism and compared to other than CNN, stay explicitly attend from each word in the sequence to a different working sequence and do this over multiple layers so there's like a lot of many different tensions being computed, so it's kind of heavy.",
            "And compute in computation and.",
            "Maybe Wilson parameters so.",
            "So we.",
            "Work with the when you see how pretty Transformers work for simple questions.",
            "So we.",
            "Experiment with bird, which is basically transformer model with some small modifications.",
            "One difference between Burton, maybe some other works is that the work when with word, peace level Logans instead of word level tokens which allows us to keep a smaller vocabulary size and.",
            "Which should allow to better handle ultimately rewards.",
            "And versus pre trained on.",
            "A large amount of data on the mask Language, model, task and sentence."
        ],
        [
            "Diction task so much language models, basically.",
            "Tries to teach the model to predict some words that were left out when you had the words in it.",
            "So basically predicting words given its surroundings in the context and the sentence very task.",
            "Predicts whether one sentence holds in another sentence.",
            "And.",
            "Gordon's former retrained like this, they found that if you fine tune this model on some different tasks.",
            "You get very good results.",
            "So we wanted to see how well this works for simple questions.",
            "So in our approach, we've fallen approach that's similar to previous works, so."
        ],
        [
            "We have to spend reduction.",
            "We assume that only a single entity is mentioned in the question, so we can assume there is only a single span which is not interrupted by other words.",
            "And we also need to predict relation and given 90 to spend.",
            "We can generate any candidates simply based on string match with entity labels.",
            "And then finally we can take the risk."
        ],
        [
            "Notes from the previous steps and.",
            "Re ranking queries for removing trip by removing units do not exist in the Knowledge graph and also using some.",
            "Statistics too.",
            "Try to get.",
            "The best bear hire.",
            "So you actually do that.",
            "Both prediction tasks in one step.",
            "So we get the sequence tagging and sequence classification for relation prediction and anticipate reduction together."
        ],
        [
            "For so for this prediction prediction together we have.",
            "Simply, this model where we first encodes the question using the word.",
            "And then for the event prediction we use.",
            "Two classifiers, one for the start of the span.",
            "Sequence prediction one to determine the start of the sequence of the span and one for the end of this pen, and this classifier goes over the sequence length.",
            "And for the relation we simply use like a simple classifier over all possible relations.",
            "So.",
            "In some more detail we have."
        ],
        [
            "We add two parameters, parameter vectors start and end.",
            "And we just have a softmax over the sequence length.",
            "To produce a distribution over.",
            "Like to find out where the start and end are.",
            "So if you have a question who killed John F Kennedy?",
            "We encoded with words.",
            "We get the top level representations for each word and the distributions, and we take the.",
            "The maximum values and.",
            "They could spend predicted like this, so this is different from some previous works where they did like this full sequence tagging with.",
            "Will be classified each word as being in this gun in this manner not intended to spend.",
            "And this model is trained so this part of the model is trained on alignments automatically obtained.",
            "But comparing entity labels with the question."
        ],
        [
            "For relation prediction is a simple.",
            "It's a simple softmax classifier.",
            "Where we take the sealer slogan.",
            "From there it should contain like a summary of the whole sentence.",
            "And applying a suffix classifier and this can be simulated using the data from the data set.",
            "So everything together."
        ],
        [
            "We have the whole sentence.",
            "If you're using birds, you need to.",
            "Need to put the sealer spoken before the sentence in a separate token after the sentence and then we feed it through the model.",
            "We get the representations and we compute the entity spans from the over the representations of their real words and the relation classification is done using this representation from the CLS token.",
            "And so it's a very simple model and it requires only a single application of a model of the bird, because which is.",
            "Which is nice because it is expensive."
        ],
        [
            "So the next step is to generate candidates from the entity span.",
            "So if you have a question who wrote it and we were predicted to spend it.",
            "We just look in the.",
            "In the labels of entities and gets entities with the same label and there can be many different entities and it would be difficult to do."
        ],
        [
            "I agree.",
            "So the next step we need to somehow try to re rank the candidates to take into account the class structure.",
            "So to filter out some.",
            "Entity relation pairs that don't make sense.",
            "And.",
            "In some more detail, we do this by first.",
            "So we first discards the entity relation pairs that do not exist in the Knowledge Graph.",
            "And further, we rank first by string similarity of the entity with detected span.",
            "Then we.",
            "Break the ties by sorting by the prediction.",
            "Probability for the relation prediction.",
            "And then we sort by entity indegree.",
            "So basically trying to favor more popular entities."
        ],
        [
            "In our experiments, we."
        ],
        [
            "But with the simple questions data sets, which is a very big large data set over Freebase and we look at F1 and accuracy for entity spans.",
            "And accuracy for relation prediction.",
            "So we compare results.",
            "We compare model to baseline based on violence TM which is similar to one of the previous works.",
            "The truth is you can solve simple questions."
        ],
        [
            "Very well with very simple model.",
            "So this based on based on your license, nothing buildings and it uses 2 separate by list games.",
            "So one for relation classification.",
            "And one for certain classification.",
            "So we.",
            "Do the Yankees spend reduction similar to the grid based model and not like the previous work did?",
            "But no results.",
            "We can see that.",
            "This modification doesn't affect performance negatively."
        ],
        [
            "So the questions I wanted to answer experiments are how a very based model.",
            "Works on the subtasks individually.",
            "Also how the various model would work on the whole task in the end.",
            "And how both very based model in our baseline degrade?",
            "With fewer training data, and finally, we also wanted to see if the model actually learns.",
            "Anything like meaningful patterns."
        ],
        [
            "So for the first question.",
            "Or the subtask results.",
            "We can see that the entity spent reduction is sold pretty well and we get some improvement.",
            "By using the transformer, but it's not huge improvement.",
            "And.",
            "Like and also baseline is.",
            "Performs better than the one reported in the previous work.",
            "For division prediction we also get small improvement and.",
            "We also report the entity recall, so all these numbers are done validation set and if you look at entity call we can see that.",
            "With the bird based model we can get we get slightly higher equals."
        ],
        [
            "And this is this.",
            "A test results on the test set, just for some more information."
        ],
        [
            "So the end results of the whole model and approach, including density, ranking and degeneration.",
            "We did not manage to beat the best work that was there so far.",
            "But we can see that the ranking improves our relation accuracy.",
            "And.",
            "We did something very similar and then it says.",
            "And so that.",
            "And try to see.",
            "How the failures are distributed and we also checked.",
            "What may be the cause of entity errors and founded in 30% or something and it says?"
        ],
        [
            "So for the third question, how does it work with fewer training data?",
            "We took the training data and.",
            "Remove the fraction of the examples, so here we retain 75% of the examples.",
            "And when removing examples we try to remove examples with the most frequent relations.",
            "So that the coverage order relations is.",
            "So so that we minimize the number of unseen relations during training.",
            "And from the results we can see that the grids.",
            "Their base model degrades much better than the ballast model.",
            "So for example, for at some point is 10% difference.",
            "Here it's only 5.5% of the data retained.",
            "The spend prediction.",
            "Accuracy.",
            "Is so pretty good thing and we can see something similar for the relations.",
            "But you can also see that at the end and using all the data the difference in performance."
        ],
        [
            "Those degrees.",
            "So and finally we also looked at and we try to analyze the internal behavior so that simple analysis.",
            "And took the average of all the attention Maps for the Selous Token which is used for predicting the relation.",
            "And if we.",
            "Plots the old."
        ],
        [
            "Attention, so in this boat we have this token and.",
            "This role represents shows that average of the.",
            "All the attention distributions and all the layers in all their heads.",
            "Over the words that were used to computerization of cells in the different layers.",
            "And so before fine tuning.",
            "A tree like you can see that.",
            "There's some tendency to look at neighboring words, and also for some reason it's looking at the punctuation symbols.",
            "And after fine tuning we find that.",
            "The attention shifted towards more relation specific words, So what songs have little?",
            "Produced.",
            "Which is the word that is most characteristic of the relation.",
            "So which is nice and some more examples of this are here.",
            "So here is before training on simple questions."
        ],
        [
            "Do subterranean simple questions.",
            "And we can see that.",
            "In this case is.",
            "The internal attention distributions.",
            "Look at the more relation specific words, so this is an actual example from the data set.",
            "This is a question that we should be able to answer, which is another example from the data set.",
            "And this is not an example from another set who wanted in the words this year.",
            "So it's focusing on one in the world and then something.",
            "Also, not sure that's who let the dogs out.",
            "It's.",
            "Not focusing at the non relationships with specific words, I think.",
            "So."
        ],
        [
            "Things thank you for attention.",
            "Thanks for the talk.",
            "I have a few questions but I'll start with the first one there.",
            "In you were presenting results and you had 30% failure in the recognizing the entity.",
            "Right or they all wear yes no.",
            "The slide you just had where you show the breakdown by errors.",
            "So when you say entity wrong, do you know in how many cases it's the text, the span, the entity spend that is not predicted correctly versus the cases where spotting the actual entity was wrong or linking to the actual entity was wrong?",
            "I don't think we do that, sorry.",
            "I think we didn't do that analysis.",
            "You didn't do it.",
            "OK, good.",
            "Can I can I have another one?",
            "So now I wanted to ask the.",
            "I mean, it seems like this task is too easy.",
            "The simple question data set.",
            "My question is, do you think that?",
            "This is really a relevant task to work on becausw.",
            "I mean it's it's quite decoupled from being applicable in a in a real question answering setting becausw.",
            "I mean when you get when you get a question you don't know whether it's a simple question, right?",
            "So how would you?",
            "How would you apply this in in a real question answering?",
            "System and what do you think about the relevance of the task?",
            "I think it's.",
            "I think it's still relevant because like I'm not sure like another statistics but not sure that users would actually be asking very complex questions.",
            "Maybe if those systems show that they can answer more complex questions with more complex questions, but I think for people it's also.",
            "Very hard to express like a.",
            "A single large sentence with lots of constraints and counts and everything, so I think.",
            "I think.",
            "Like dialogue based question, answering systems with a sequence of more simple questions would be the most practical use case, but yeah.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Dennis.",
                    "label": 0
                },
                {
                    "sent": "Because I think another work on using pre train Transformers for answering simple questions on scrubs.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we only consider simple questions which require.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only.",
                    "label": 0
                },
                {
                    "sent": "A single entity in relation to be answered to be determined to answer the question, so the queries will be.",
                    "label": 0
                },
                {
                    "sent": "A single triple pattern without any additional counts or.",
                    "label": 0
                },
                {
                    "sent": "Reasons.",
                    "label": 0
                },
                {
                    "sent": "So question answering is also hard for the simple questions because of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many different ways in which you can express the.",
                    "label": 0
                },
                {
                    "sent": "Questions for the same query.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the.",
                    "label": 0
                },
                {
                    "sent": "Wanted to see how.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh well, this very model is actually working.",
                    "label": 0
                },
                {
                    "sent": "Would work with simple questions and.",
                    "label": 0
                },
                {
                    "sent": "First, some background.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formers so Transformers are not very different scenes for sequence processing they.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rely mostly on linear layers and attention mechanism and compared to other than CNN, stay explicitly attend from each word in the sequence to a different working sequence and do this over multiple layers so there's like a lot of many different tensions being computed, so it's kind of heavy.",
                    "label": 0
                },
                {
                    "sent": "And compute in computation and.",
                    "label": 0
                },
                {
                    "sent": "Maybe Wilson parameters so.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Work with the when you see how pretty Transformers work for simple questions.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Experiment with bird, which is basically transformer model with some small modifications.",
                    "label": 1
                },
                {
                    "sent": "One difference between Burton, maybe some other works is that the work when with word, peace level Logans instead of word level tokens which allows us to keep a smaller vocabulary size and.",
                    "label": 1
                },
                {
                    "sent": "Which should allow to better handle ultimately rewards.",
                    "label": 0
                },
                {
                    "sent": "And versus pre trained on.",
                    "label": 1
                },
                {
                    "sent": "A large amount of data on the mask Language, model, task and sentence.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diction task so much language models, basically.",
                    "label": 0
                },
                {
                    "sent": "Tries to teach the model to predict some words that were left out when you had the words in it.",
                    "label": 0
                },
                {
                    "sent": "So basically predicting words given its surroundings in the context and the sentence very task.",
                    "label": 0
                },
                {
                    "sent": "Predicts whether one sentence holds in another sentence.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Gordon's former retrained like this, they found that if you fine tune this model on some different tasks.",
                    "label": 0
                },
                {
                    "sent": "You get very good results.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to see how well this works for simple questions.",
                    "label": 0
                },
                {
                    "sent": "So in our approach, we've fallen approach that's similar to previous works, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have to spend reduction.",
                    "label": 0
                },
                {
                    "sent": "We assume that only a single entity is mentioned in the question, so we can assume there is only a single span which is not interrupted by other words.",
                    "label": 0
                },
                {
                    "sent": "And we also need to predict relation and given 90 to spend.",
                    "label": 1
                },
                {
                    "sent": "We can generate any candidates simply based on string match with entity labels.",
                    "label": 1
                },
                {
                    "sent": "And then finally we can take the risk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notes from the previous steps and.",
                    "label": 0
                },
                {
                    "sent": "Re ranking queries for removing trip by removing units do not exist in the Knowledge graph and also using some.",
                    "label": 0
                },
                {
                    "sent": "Statistics too.",
                    "label": 0
                },
                {
                    "sent": "Try to get.",
                    "label": 0
                },
                {
                    "sent": "The best bear hire.",
                    "label": 0
                },
                {
                    "sent": "So you actually do that.",
                    "label": 0
                },
                {
                    "sent": "Both prediction tasks in one step.",
                    "label": 0
                },
                {
                    "sent": "So we get the sequence tagging and sequence classification for relation prediction and anticipate reduction together.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For so for this prediction prediction together we have.",
                    "label": 0
                },
                {
                    "sent": "Simply, this model where we first encodes the question using the word.",
                    "label": 0
                },
                {
                    "sent": "And then for the event prediction we use.",
                    "label": 0
                },
                {
                    "sent": "Two classifiers, one for the start of the span.",
                    "label": 0
                },
                {
                    "sent": "Sequence prediction one to determine the start of the sequence of the span and one for the end of this pen, and this classifier goes over the sequence length.",
                    "label": 0
                },
                {
                    "sent": "And for the relation we simply use like a simple classifier over all possible relations.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In some more detail we have.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We add two parameters, parameter vectors start and end.",
                    "label": 0
                },
                {
                    "sent": "And we just have a softmax over the sequence length.",
                    "label": 0
                },
                {
                    "sent": "To produce a distribution over.",
                    "label": 0
                },
                {
                    "sent": "Like to find out where the start and end are.",
                    "label": 0
                },
                {
                    "sent": "So if you have a question who killed John F Kennedy?",
                    "label": 1
                },
                {
                    "sent": "We encoded with words.",
                    "label": 0
                },
                {
                    "sent": "We get the top level representations for each word and the distributions, and we take the.",
                    "label": 0
                },
                {
                    "sent": "The maximum values and.",
                    "label": 0
                },
                {
                    "sent": "They could spend predicted like this, so this is different from some previous works where they did like this full sequence tagging with.",
                    "label": 0
                },
                {
                    "sent": "Will be classified each word as being in this gun in this manner not intended to spend.",
                    "label": 0
                },
                {
                    "sent": "And this model is trained so this part of the model is trained on alignments automatically obtained.",
                    "label": 0
                },
                {
                    "sent": "But comparing entity labels with the question.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For relation prediction is a simple.",
                    "label": 1
                },
                {
                    "sent": "It's a simple softmax classifier.",
                    "label": 1
                },
                {
                    "sent": "Where we take the sealer slogan.",
                    "label": 0
                },
                {
                    "sent": "From there it should contain like a summary of the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "And applying a suffix classifier and this can be simulated using the data from the data set.",
                    "label": 0
                },
                {
                    "sent": "So everything together.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "If you're using birds, you need to.",
                    "label": 0
                },
                {
                    "sent": "Need to put the sealer spoken before the sentence in a separate token after the sentence and then we feed it through the model.",
                    "label": 0
                },
                {
                    "sent": "We get the representations and we compute the entity spans from the over the representations of their real words and the relation classification is done using this representation from the CLS token.",
                    "label": 0
                },
                {
                    "sent": "And so it's a very simple model and it requires only a single application of a model of the bird, because which is.",
                    "label": 0
                },
                {
                    "sent": "Which is nice because it is expensive.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next step is to generate candidates from the entity span.",
                    "label": 1
                },
                {
                    "sent": "So if you have a question who wrote it and we were predicted to spend it.",
                    "label": 1
                },
                {
                    "sent": "We just look in the.",
                    "label": 0
                },
                {
                    "sent": "In the labels of entities and gets entities with the same label and there can be many different entities and it would be difficult to do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I agree.",
                    "label": 0
                },
                {
                    "sent": "So the next step we need to somehow try to re rank the candidates to take into account the class structure.",
                    "label": 0
                },
                {
                    "sent": "So to filter out some.",
                    "label": 0
                },
                {
                    "sent": "Entity relation pairs that don't make sense.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In some more detail, we do this by first.",
                    "label": 0
                },
                {
                    "sent": "So we first discards the entity relation pairs that do not exist in the Knowledge Graph.",
                    "label": 1
                },
                {
                    "sent": "And further, we rank first by string similarity of the entity with detected span.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                },
                {
                    "sent": "Break the ties by sorting by the prediction.",
                    "label": 1
                },
                {
                    "sent": "Probability for the relation prediction.",
                    "label": 0
                },
                {
                    "sent": "And then we sort by entity indegree.",
                    "label": 0
                },
                {
                    "sent": "So basically trying to favor more popular entities.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our experiments, we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But with the simple questions data sets, which is a very big large data set over Freebase and we look at F1 and accuracy for entity spans.",
                    "label": 1
                },
                {
                    "sent": "And accuracy for relation prediction.",
                    "label": 0
                },
                {
                    "sent": "So we compare results.",
                    "label": 0
                },
                {
                    "sent": "We compare model to baseline based on violence TM which is similar to one of the previous works.",
                    "label": 0
                },
                {
                    "sent": "The truth is you can solve simple questions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very well with very simple model.",
                    "label": 0
                },
                {
                    "sent": "So this based on based on your license, nothing buildings and it uses 2 separate by list games.",
                    "label": 0
                },
                {
                    "sent": "So one for relation classification.",
                    "label": 1
                },
                {
                    "sent": "And one for certain classification.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 1
                },
                {
                    "sent": "Do the Yankees spend reduction similar to the grid based model and not like the previous work did?",
                    "label": 0
                },
                {
                    "sent": "But no results.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "This modification doesn't affect performance negatively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the questions I wanted to answer experiments are how a very based model.",
                    "label": 0
                },
                {
                    "sent": "Works on the subtasks individually.",
                    "label": 1
                },
                {
                    "sent": "Also how the various model would work on the whole task in the end.",
                    "label": 0
                },
                {
                    "sent": "And how both very based model in our baseline degrade?",
                    "label": 1
                },
                {
                    "sent": "With fewer training data, and finally, we also wanted to see if the model actually learns.",
                    "label": 0
                },
                {
                    "sent": "Anything like meaningful patterns.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the first question.",
                    "label": 0
                },
                {
                    "sent": "Or the subtask results.",
                    "label": 0
                },
                {
                    "sent": "We can see that the entity spent reduction is sold pretty well and we get some improvement.",
                    "label": 0
                },
                {
                    "sent": "By using the transformer, but it's not huge improvement.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Like and also baseline is.",
                    "label": 0
                },
                {
                    "sent": "Performs better than the one reported in the previous work.",
                    "label": 0
                },
                {
                    "sent": "For division prediction we also get small improvement and.",
                    "label": 0
                },
                {
                    "sent": "We also report the entity recall, so all these numbers are done validation set and if you look at entity call we can see that.",
                    "label": 0
                },
                {
                    "sent": "With the bird based model we can get we get slightly higher equals.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is this.",
                    "label": 0
                },
                {
                    "sent": "A test results on the test set, just for some more information.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the end results of the whole model and approach, including density, ranking and degeneration.",
                    "label": 1
                },
                {
                    "sent": "We did not manage to beat the best work that was there so far.",
                    "label": 0
                },
                {
                    "sent": "But we can see that the ranking improves our relation accuracy.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We did something very similar and then it says.",
                    "label": 0
                },
                {
                    "sent": "And so that.",
                    "label": 0
                },
                {
                    "sent": "And try to see.",
                    "label": 0
                },
                {
                    "sent": "How the failures are distributed and we also checked.",
                    "label": 0
                },
                {
                    "sent": "What may be the cause of entity errors and founded in 30% or something and it says?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the third question, how does it work with fewer training data?",
                    "label": 0
                },
                {
                    "sent": "We took the training data and.",
                    "label": 1
                },
                {
                    "sent": "Remove the fraction of the examples, so here we retain 75% of the examples.",
                    "label": 0
                },
                {
                    "sent": "And when removing examples we try to remove examples with the most frequent relations.",
                    "label": 0
                },
                {
                    "sent": "So that the coverage order relations is.",
                    "label": 1
                },
                {
                    "sent": "So so that we minimize the number of unseen relations during training.",
                    "label": 0
                },
                {
                    "sent": "And from the results we can see that the grids.",
                    "label": 1
                },
                {
                    "sent": "Their base model degrades much better than the ballast model.",
                    "label": 0
                },
                {
                    "sent": "So for example, for at some point is 10% difference.",
                    "label": 0
                },
                {
                    "sent": "Here it's only 5.5% of the data retained.",
                    "label": 0
                },
                {
                    "sent": "The spend prediction.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "Is so pretty good thing and we can see something similar for the relations.",
                    "label": 0
                },
                {
                    "sent": "But you can also see that at the end and using all the data the difference in performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those degrees.",
                    "label": 0
                },
                {
                    "sent": "So and finally we also looked at and we try to analyze the internal behavior so that simple analysis.",
                    "label": 1
                },
                {
                    "sent": "And took the average of all the attention Maps for the Selous Token which is used for predicting the relation.",
                    "label": 0
                },
                {
                    "sent": "And if we.",
                    "label": 0
                },
                {
                    "sent": "Plots the old.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention, so in this boat we have this token and.",
                    "label": 0
                },
                {
                    "sent": "This role represents shows that average of the.",
                    "label": 0
                },
                {
                    "sent": "All the attention distributions and all the layers in all their heads.",
                    "label": 0
                },
                {
                    "sent": "Over the words that were used to computerization of cells in the different layers.",
                    "label": 0
                },
                {
                    "sent": "And so before fine tuning.",
                    "label": 0
                },
                {
                    "sent": "A tree like you can see that.",
                    "label": 0
                },
                {
                    "sent": "There's some tendency to look at neighboring words, and also for some reason it's looking at the punctuation symbols.",
                    "label": 0
                },
                {
                    "sent": "And after fine tuning we find that.",
                    "label": 0
                },
                {
                    "sent": "The attention shifted towards more relation specific words, So what songs have little?",
                    "label": 0
                },
                {
                    "sent": "Produced.",
                    "label": 0
                },
                {
                    "sent": "Which is the word that is most characteristic of the relation.",
                    "label": 0
                },
                {
                    "sent": "So which is nice and some more examples of this are here.",
                    "label": 0
                },
                {
                    "sent": "So here is before training on simple questions.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do subterranean simple questions.",
                    "label": 0
                },
                {
                    "sent": "And we can see that.",
                    "label": 0
                },
                {
                    "sent": "In this case is.",
                    "label": 0
                },
                {
                    "sent": "The internal attention distributions.",
                    "label": 0
                },
                {
                    "sent": "Look at the more relation specific words, so this is an actual example from the data set.",
                    "label": 0
                },
                {
                    "sent": "This is a question that we should be able to answer, which is another example from the data set.",
                    "label": 0
                },
                {
                    "sent": "And this is not an example from another set who wanted in the words this year.",
                    "label": 0
                },
                {
                    "sent": "So it's focusing on one in the world and then something.",
                    "label": 0
                },
                {
                    "sent": "Also, not sure that's who let the dogs out.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Not focusing at the non relationships with specific words, I think.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I have a few questions but I'll start with the first one there.",
                    "label": 0
                },
                {
                    "sent": "In you were presenting results and you had 30% failure in the recognizing the entity.",
                    "label": 0
                },
                {
                    "sent": "Right or they all wear yes no.",
                    "label": 0
                },
                {
                    "sent": "The slide you just had where you show the breakdown by errors.",
                    "label": 0
                },
                {
                    "sent": "So when you say entity wrong, do you know in how many cases it's the text, the span, the entity spend that is not predicted correctly versus the cases where spotting the actual entity was wrong or linking to the actual entity was wrong?",
                    "label": 0
                },
                {
                    "sent": "I don't think we do that, sorry.",
                    "label": 0
                },
                {
                    "sent": "I think we didn't do that analysis.",
                    "label": 0
                },
                {
                    "sent": "You didn't do it.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "Can I can I have another one?",
                    "label": 0
                },
                {
                    "sent": "So now I wanted to ask the.",
                    "label": 0
                },
                {
                    "sent": "I mean, it seems like this task is too easy.",
                    "label": 0
                },
                {
                    "sent": "The simple question data set.",
                    "label": 0
                },
                {
                    "sent": "My question is, do you think that?",
                    "label": 0
                },
                {
                    "sent": "This is really a relevant task to work on becausw.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's quite decoupled from being applicable in a in a real question answering setting becausw.",
                    "label": 0
                },
                {
                    "sent": "I mean when you get when you get a question you don't know whether it's a simple question, right?",
                    "label": 0
                },
                {
                    "sent": "So how would you?",
                    "label": 0
                },
                {
                    "sent": "How would you apply this in in a real question answering?",
                    "label": 0
                },
                {
                    "sent": "System and what do you think about the relevance of the task?",
                    "label": 0
                },
                {
                    "sent": "I think it's.",
                    "label": 0
                },
                {
                    "sent": "I think it's still relevant because like I'm not sure like another statistics but not sure that users would actually be asking very complex questions.",
                    "label": 0
                },
                {
                    "sent": "Maybe if those systems show that they can answer more complex questions with more complex questions, but I think for people it's also.",
                    "label": 0
                },
                {
                    "sent": "Very hard to express like a.",
                    "label": 0
                },
                {
                    "sent": "A single large sentence with lots of constraints and counts and everything, so I think.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Like dialogue based question, answering systems with a sequence of more simple questions would be the most practical use case, but yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}