{
    "id": "q4nysqpor7pj4jcmshh56j3k4zx6kycd",
    "title": "Scaling Up Deep Learning",
    "info": {
        "author": [
            "Yoshua Bengio, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_bengio_deep_learning/",
    "segmentation": [
        [
            "So welcome to this second tutorial on deep learning.",
            "Anne.",
            "I suppose that many of you attended the morning tutorial by Russell.",
            "A good enough.",
            "So I will.",
            "There will be some overlap, but mostly the things I'll talk about will be different.",
            "Please.",
            "Do not hesitate to interrupt me in the middle and ask questions.",
            "We have plenty of time.",
            "And just raise your hand and I'll try to keep watching.",
            "So I've been embarked on this."
        ],
        [
            "It's crazy adventure.",
            "Because I want to understand intelligence, I want to understand the computational.",
            "And mathematical principles that could give rise to intelligence in animals and humans.",
            "And of course, in machines that we want to build."
        ],
        [
            "And I believe that it's central to this quest that we understand.",
            "How?",
            "Machines or living beings can acquire knowledge.",
            "And so I've been doing machine learning for a couple of decades.",
            "And since almost the same time."
        ],
        [
            "I've been working with neural Nets.",
            "And there's been a breakthrough in the last few years that people called deep learning that I'm going to tell you about today.",
            "And it's both machine learning algorithms that are inspired by brains brains, just like neural Nets have been.",
            "But with particular twists, which is this notion that we want to represent data and we want to do it using multiple levels of representation?",
            "And the idea is that these multiple levels of representation correspond to multiple levels of abstraction.",
            "And there are a number of theoretical.",
            "And practical reasons why we want to do that.",
            "That that hopefully you will.",
            "Be clear to you at the end of this tutorial."
        ],
        [
            "So deep learning.",
            "Is a particular form of representation learning.",
            "Representation learning is a way for.",
            "Machine learning systems too.",
            "Discover new representations of data they're going to be useful deep learning.",
            "Our representation, learning algorithms that have these multiple levels of representation.",
            "And of course, this is part of the bigger research on machine learning that I'm sure you know about which is itself part of AI."
        ],
        [
            "A deep learning has attracted a lot of attention in the recent two years, roughly.",
            "Although it really had a big breakthrough in 2006, so that's eight years ago.",
            "It has become used heavily in industry, mostly because it has almost revolutionized areas such as speech recognition and object recognition and computer vision.",
            "There's a lot more going on and coming.",
            "In other areas of application.",
            "Within computer vision, other applications besides object recognition, like detection.",
            "And parsing of scenes, but also in an area that may be more interesting from many people in this audience, which has to do with.",
            "Dealing with text, natural language processing dialogue.",
            "More recently, there's been some interesting advances in combining deep learning with reinforcement learning."
        ],
        [
            "Let me give you an example of how things have changed with the.",
            "Input of deep learning in speech recognition.",
            "So this these numbers come from Microsoft and the talk about performance on a switchboard data set, which has been longstanding benchmarks from the early 90s.",
            "So the X axis are yours and the Y axis is.",
            "Word error rate which one of go down things have been progressing well in the 90s?",
            "Even though the algorithms themselves haven't changed much in those years, I was actually working on speech recognition myself in the early 90s when I completed my PhD in 91.",
            "I'm.",
            "So there was mostly advances because of being able to train bigger systems, bigger models, faster with faster computers, much larger datasets, but there's only so much you can do with that an in the decade that followed, things have kind of flat and.",
            "And then progress started to accelerate again with the use of deep learning.",
            "So that's the kind of reason why people have been attracted to this area."
        ],
        [
            "Something similar has happened in object recognition at about the same time, a little bit later there was a breakthrough in one of the most interesting.",
            "Challenging tasks in object recognition.",
            "The Imagenet data set that has 1000 classes of objects and about a million training examples actually now 20,000,000.",
            "And you see, here.",
            "Outputs of this early system, which was getting around 17% correct in the sense of the having right answer on the 1st 5.",
            "I'm more modern systems that the latest systems have now are now down to five or six."
        ],
        [
            "Percent these were the numbers as they were in 2012.",
            "So on the right hand side you see the.",
            "Error rates obtained by by other systems competing in those days with the system put up by Alex Kryszewski.",
            "Yes, Karen, Jeff Hinton, but won this competition and brought it down from about 26% error to build 16% error.",
            "And they said in the.",
            "Your afterwards they brought it down to around 11 and this year is down to 6% or something like that.",
            "So there's been amazing progress thanks to these techniques."
        ],
        [
            "You can try it out for yourself.",
            "Russell probably showed this URL.",
            "This morning just deep learning.cs.toronto.edu you go there and you can upload a URL of an image.",
            "And it will use these 1000 categories to try to identify the main object in the image.",
            "So here it's saying convertible.",
            "I hear it's saying chimpanzee here it's saying sports car and I just picked those images without searching much using Google Image Search and pasted them.",
            "It doesn't.",
            "It's not always perfect, for example here it says Killer Whale.",
            "With 35% confidence and Tiger Shark with 19% confidence in scuba diver, so it's not always perfect, of course, and there's a lot more to be done.",
            "But this German Shepherd was perfectly recognized and.",
            "So.",
            "It's if you've tried computer vision and object recognition.",
            "In the past it was very brittle and these systems are just.",
            "Made a qualitative change by the way in which they are robust to all kinds of variations in illumination and background, and things like that."
        ],
        [
            "In my lab, we applied these techniques to emotion classification from movie clips from video using both the audio and images an we want to competition.",
            "Was held last December using using very fairly similar systems."
        ],
        [
            "So with these and other advances in 2013, the MIT Technology Review declared declining to be one of the 10 breakthrough technologies of the year."
        ],
        [
            "And then there's been a lot of.",
            "Journalists talking about it in New York Times and wired and others.",
            "An companies have started competing.",
            "For.",
            "For the technology and for the people working on it in particular, Google and Facebook, where in an arms race too.",
            "Acquire our companies or people one big event and this was the Google buying deep line for about 500 million dollars.",
            "Anne Anne now there's tons of startups in this in this area."
        ],
        [
            "OK, so that's all nice and there's a lot of hype.",
            "But there are some real research that remains to be done and.",
            "Some interesting challenges, so this tutorial is about these challenges.",
            "Also introducing you to some of the concepts behind deep learning.",
            "And I'll try to tell you about what I consider the major challenge, which is how to move the successes we've had with supervised learning.",
            "Most of these applications have been using supervised learning.",
            "Two unsupervised or semi supervised learning where you have tons of unlabeled or weakly labeled data and structured outputs which is tightly connected to the improvised learning question.",
            "Where the output of the system is not just a category or a few numbers, but it's something that's very high dimensional, like a sentence, an image, or some complicated data structure.",
            "The thing with these both supervised both these structured outputs and this provides learning scenarios.",
            "Is that probabilistic models quickly become intractable.",
            "Computation both from a computational point of view and from the statistical point of view.",
            "So the use of deep learning in these areas could have a fairly important impact.",
            "I'll tell you about the challenge of scaling up from a computational point of view, because what we've seen in recent years is that the better models are the bigger ones, and we're limited just by our ability to train bigger models and related to this, I'll tell you about some of the challenges arising in.",
            "Training deep Nets.",
            "So there's a numerical optimization issue.",
            "Even though the techniques we're using are doing amazingly well, we have some indication that we're not doing as well as we could, and I'll tell you about that and some ideas to get around these.",
            "Challenges."
        ],
        [
            "And if we can, if we can face these challenges, then it can be a much more impressive impact on AI.",
            "I think with computers that can see and hear and understand natural language understand humans.",
            "Anne Anne provide better service to humans and of course maybe even understand.",
            "Help us understand how humans and animals.",
            "Managed to solve difficult learning problems."
        ],
        [
            "My.",
            "So now let's go a bit more technical slowly.",
            "Anne.",
            "How do we build intelligent machines well?",
            "You need knowledge in order to take intelligent decisions and you need that knowledge to come from somewhere and part of it could be that we.",
            "Feed that knowledge directly to the machines, but what we've learned in the last few decades is that a large part of it has to be directly acquired by the machine, and that's what machine learning is about.",
            "And if you.",
            "Learn about machine learning and you try to figure out what are the core elements that you find in machine learning.",
            "Well, one core element is priors.",
            "In other words, no, there's no general purpose machine learning algorithm.",
            "You need some priors and price could be very specialized to your application.",
            "Or they could be very broad and in AI we're trying to look at very broad priors.",
            "But both both could be interesting.",
            "Then there is always an element of optimization or search where searching for a good solution to a problem in a large family of solution.",
            "Or in the case of inference, we're looking for searching for a good explanation to a particular input, like when you're doing machine translation or or speech recognition.",
            "There are many explanations to the thing we see and we're looking for the best one, so there's search or optimization comes up in these algorithms.",
            "And practically, from a computer science perspective, there's also an issue of efficiency of computation.",
            "We've seen a lot of progress in recent years due to GPU's that can paralyze a lot of that computation.",
            "So thinking about that aspect is also important.",
            "Now.",
            "The more fundamental question behind machine learning is generalization.",
            "How do we get?",
            "Our learner to provide the right answers for new cases.",
            "And the take that I'm trying to put forward on this generalization question is a geometrical One South.",
            "During the presentation you will see this come back when I'm thinking about generalization is how we take probability mass.",
            "That we know should be high on the training examples an spreading it in a right way.",
            "Guessing where it should concentrate.",
            "So you will see this notion come back.",
            "Now, unfortunately, in high dimensional spaces, guessing where to put probability mass is seems intractable, because there's a exponentially orange large number of configurations of places where you could put probability mass.",
            "So that's the curse of dimensionality.",
            "And how do we get around that?",
            "Well.",
            "Something I've been pushing in the last few years.",
            "Is this idea that?",
            "We can get around the curse of dimensionality.",
            "If our learning systems can discover and disentangle separate out.",
            "The underlying factors that explain the data.",
            "The underlying cause is basically making sense of the data, so it may seem like a hard thing to do.",
            "And of course it is, but I've been trying to.",
            "To push that as the agenda for making really serious progress towards AI."
        ],
        [
            "Alright, so now let's go back to machine learning 101.",
            "Let's consider this one dimensional supervised learning task where you have to predict Y given X, and if you're given these examples, these green stars.",
            "And of course there is a true underlying function that relates the wise in the axis, maybe with some noise.",
            "And you're trying to guess it using these examples.",
            "So that's the simple thing that we're trying to do, and here it will be very easy in one way to see why it will be very easy is that the I can't do that the.",
            "Examples are insufficient number.",
            "That they can cover the ups and downs of the function you want to learn.",
            "And because they can cover the ups and downs of the function you want to learn, you basically just need to interpolate."
        ],
        [
            "And this is shown in the next slide.",
            "So you can do it, you know, in very smart ways, and there's a lot of.",
            "Statistical machine learning that you know does that with very beautiful mathematics like kernel machines.",
            "That's nice and.",
            "What it does really is it exploits aprior.",
            "Remember I said one of the crucial ingredients in machine learning is the priors.",
            "What's the prior here?",
            "The prior is what people call the smoothness prior.",
            "It says the value of the function you want to learn.",
            "At Test point X.",
            "Should be close to.",
            "The value you observed in the neighborhood, so the function is smooth.",
            "It doesn't change rapidly, so that's why interpolation works.",
            "That's nice and we should use that prior, but it's not."
        ],
        [
            "Enough when you go into higher dimensional spaces where the set of configurations of interests becomes exponentially large.",
            "And maybe the functions you want to learn aren't that smooth.",
            "Then you get into trouble 'cause the number is the number of ups and downs of the function you'd like to cover or the number of configurations you'd like to separate.",
            "Could become much, much larger than the number of examples you could ever hope to get.",
            "So what do we do?",
            "One idea people had about 15 years ago is that we could solve that problem by reducing dimensionality."
        ],
        [
            "But we've shown with some theory that it's not really dimensionality.",
            "You can have a 1 dimensional function that could be very very hard to learn.",
            "What makes it really hard is the number of Upson Downs is the variability in the function you want to learn."
        ],
        [
            "And Furthermore, even if you were able to reduce dimensionality.",
            "It might not be enough.",
            "Because the functions maybe the manifold's near which is data concentrate very a lot and actually we do see this in AI applications.",
            "Now this idea of manifold learning however is very inspiring and it has been inspiring from me and for many people.",
            "And it relies on another prior, so I've already talked about one prior which was a smoothness and there is a second one here.",
            "It doesn't apply to every data set.",
            "Of course, like priors, they apply to some problems, but not all.",
            "And what it says here is the probability.",
            "Mass concentrates, in other words, if you get if you look at your data and you look at the variables of interests.",
            "You consider completely random, say, uniform configurations of your variables.",
            "These random configurations would be very unlikely you wouldn't.",
            "You wouldn't expect to see these in the data, so images where you pick the pixels in dependently you get these white noise and these are not at all like natural images.",
            "If you look at sequences of random characters, they don't look like English.",
            "What's the chance that you would produce a natural image by randomly picking pixels or what's the chance that you will pick?",
            "And a natural looking English sentence by picking characters randomly for long enough tiny.",
            "Exponentially tiny.",
            "So that says something about the kind of data that we're working with here.",
            "In many AI tasks, that data has this manifold structure.",
            "The probability concentrates and in continuous case we think of that concentration as regions.",
            "Of low probability of low dimensionality."
        ],
        [
            "So.",
            "So the geometrical view on machine learning is we're going to try to guess where this probability mass concentrates.",
            "And in order to face the challenge that there is an exponential large number of such places where we could put probably mass.",
            "We're going to go through this notion of representation learning.",
            "So the idea is going to change.",
            "We're going to do a change of coordinate.",
            "We're going to move to a new space where things are simpler and potentially of lower dimension.",
            "So in the figure we see this one dimensional manifold, which is twisted.",
            "If only we could map the points on the left.",
            "Through a space where the same manifold now corresponds to a nice flat line, even horizontal line, then the modeling problem would see would be much easier right on the right hand side you could just use a simple Gaussian and you can basically get it right.",
            "OK, so the key is going to be learning these representations that suddenly make the learning problem easy."
        ],
        [
            "So again to emphasize the same idea.",
            "Nobody gives US data.",
            "That's what we call the empirical distribution an from the general point of view, you can think of these points in space in the space of configurations.",
            "And at each of those points, we know that policy should be high.",
            "We know that if one of the axes X and the other is Y, then when X text these value the corresponding wisely.",
            "All right, that's a raw data, and what smoothness tells us is that neighborhood configurations were also likely.",
            "And that gives us a little bit of mileage, but not that much in high dimensional spaces.",
            "So instead, we'd like to figure out that these points are kind of aligned in some way.",
            "They have some structure.",
            "If we can find that structure, then we can generalize much more accurately than with this simpler spreading.",
            "The mass around idea of smoothing, but you see this, it's like a new representation where now the coordinate system would be the location on that curve.",
            "So that's what representation learning is about.",
            "It's not necessarily going to give us really a Euclidean coordinate system, but it's going to move the data into a new space where things become easy.",
            "At least that's going to be the goal."
        ],
        [
            "So that's what representation learning is about.",
            "It's about taking our data Ann transforming it.",
            "Extracting features that make the life of further machine learning easier.",
            "Now the traditional way of doing things in machine learning is to handcraft your features.",
            "Because humans are very good at that, they can understand the problem.",
            "They can think about it.",
            "They can guess good features, good transformations, and they're looking for features.",
            "They're going to be very informative for the task that they care about.",
            "But that's a lot of work.",
            "It's a lot of manual labor where somebody has to understand both machine learning and the application in industrial applications of machine learning that I was exposed to, it was usually more than 90% of the effort was in designing these features.",
            "And that makes the spread of machine learning kind of expensive and slow.",
            "What we would really like is to use computers to figure out those features, so that's what representation learning is about.",
            "Of course it doesn't prevent you from using your knowledge to provide the data in a good form, but if we could let the computers figure out good transformations of that data."
        ],
        [
            "Would be great.",
            "This figure illustrates the same notion.",
            "I'm with traditional.",
            "Well, it goes from rule based systems to deep learning.",
            "So on the left hand side you see the old way where there is not any machine learning.",
            "You handcraft a program that goes from inputs to outputs.",
            "Then we have the classic machine learning methods where you hand design your features and then you have machine learning that map those features to your output.",
            "So your classes.",
            "Now with representation learning, we introduce the notion that the features are going to be learned rather than hand designed.",
            "Of course, that doesn't you can.",
            "You can also think of it as adding an extra level where we take the hand design features and fold them by better learned features and with deep learning is just that those features are not going to come in a hierarchy of features, so we can have maybe low level similar features.",
            "And on top of the more complex features and you can have more than two levels of course."
        ],
        [
            "And when you do that, you can do all kinds of exciting things.",
            "The Google Image Search system is based on this idea and what it adds to this notion is not different.",
            "Modalities like texts and images can be associated through these representations that are learned.",
            "So, for example, what you see on the slide is.",
            "A system in which images are mapped with one function.",
            "Called by I.",
            "To a representation space, A vector space of 100 dimensions, and there's another function.",
            "Find W for words or queries that takes words or queries.",
            "So text Ann Maps them to the same space and those two functions are trained jointly together so that when somebody types dolphin and then clicks on that image.",
            "Um?",
            "The two objects end up close to each other in that space.",
            "This work was done by my brother Samy Bengio, by the way, and his collaborators, Jason Weston.",
            "Nicola losing you and others in a series of papers around 2010."
        ],
        [
            "Now, this idea of mapping words in language to a representation which is a vector.",
            "This was something that I put forward in 2000 and presented at NIPS.",
            "2000 follows up on earlier work from Geoff Hinton in representing symbols in the 80s.",
            "And in the work I did, we learned what's called a language model in which we do unsupervised learning on sequences of text, sequences of words, and as a side effect, each word gets mapped to a point in a. Vector space in those days was 50 dimension or 100 dimensions.",
            "You can't see those words, but we can zoom in.",
            "Well.",
            "First of all, you can see that there are these big big clusters.",
            "It turns out that these big clusters corresponds to correspond to part of speech, but if you zoom in, you start seeing sort of.",
            "Semantically grouped words, for example.",
            "Here we have countries or different verbs or variants of to be.",
            "So you get a sense that when you learn representations.",
            "Um?",
            "Objects that are semantically similar and are close to each other and that's connected.",
            "That's useful and connected to the earlier prior of smoothness.",
            "So words viewed as symbols are as far as each other as any pair of word is as far as each other as any other pair of word.",
            "But once you've mapped into that space, suddenly, if you've learned something about a word.",
            "Like take then that might generalize to nearby word in the same space like get.",
            "And actually, you can think of directions in that space.",
            "As attributes.",
            "That the system is learned.",
            "And."
        ],
        [
            "This idea was pushed in a very nice way by Thomas Mikolov and his collaborators.",
            "Um?",
            "Where they discovered that if you take these representations an you take the differences between words.",
            "Those differences have a meaning, and that meaning is preserved throughout different regions of space.",
            "So for example, if you take the word embedding for friends.",
            "And the word embedding for Paris.",
            "So these are points in 700 dimensional space and you subtract them.",
            "You get a vector, and if you take it's Lee minus Rome you get another vector, but these two vectors are almost the same.",
            "And so you can do algebra and find that King Minos Queen is almost the same as man minus woman.",
            "So you can use this to reason by analogy, and it it's something that comes up without having been trained for.",
            "It comes up of unsupervised learning of modeling sequences of words.",
            "It's not something that the system is being trained for it to look at pairs like this and make sure they match.",
            "So this is quite exciting.",
            "It brings us to the realm of semantics in a way that we couldn't imagine 10 or 15 years ago would be able to do."
        ],
        [
            "And these kinds of word embeddings recently of making starting to make breakthroughs in machine translation.",
            "One of the areas of AI that I consider really interesting because.",
            "To do a really good job in translation, you need to understand the sentence.",
            "Um?",
            "So we have a paper coming up in September, I think where we used these deep recurrent networks in order to do machine translation.",
            "And there's been in at the end of June at the ACL conference best paper award given to.",
            "Paper from BBN.",
            "Where they have achieved fairly amazing improvements in Bleu score, which is the metric used in machine translation.",
            "For some, for some tasks, I think they did it on Arabic, Arabic, English and Chinese English."
        ],
        [
            "In our system, the way we think about the machine translation problem and I give that as an example because it gives you a taste of.",
            "What it means to think of machine learning problem with representation learning as a core element?",
            "So what we do is we take a sentence like economy growth has slowed down in recent years an we have a machine which happens to be a recurrent net in this case.",
            "That not only represents the words in the sentence, but also represents the whole sentence as as a an object which.",
            "Here is a big vector.",
            "And then there's another machine.",
            "Which we call the decoder that takes that object an Maps it to a distribution over sequences in the other language.",
            "An if you sample from that, or if you look for the most likely translation, it might tell you something like lacrosse, economic ROTC danniels any which is a good translation for this.",
            "Sentence this actually was run on our system and even have a demo running.",
            "If you want to try it out.",
            "I'm.",
            "Alright.",
            "So this encoder decoder idea is actually something I'll come back to because these autoencoders come up as very useful ideas in deep learning."
        ],
        [
            "So let us go back to this notion of having multiple levels of representation.",
            "First of all, there is.",
            "All kinds of vertical results which point in the direction that representing functions.",
            "With sufficient levels of composition with sufficient.",
            "Composition of nonlinearities and combination of pieces.",
            "Um?",
            "Can give you an exponential gain.",
            "In other words that there are some functions that can be represented very efficiently.",
            "If you use what's called a deep circuit.",
            "Where you do some nonlinear computation.",
            "And you do it again and again.",
            "In different ways.",
            "Those are functions that can be represented very efficiently in this way, but that require an exponential amount of computation if you do it with a shallow circuit where you're only allowed to do, say, 2 levels of these operations.",
            "So these operations might be things like Boolean gates or or neurons, artificial neurons.",
            "Or rectifiers we've been playing with recently.",
            "There's also inspiration from.",
            "Brains, as I mentioned at the beginning of my presentation.",
            "When we look at.",
            "The visual cortex in particular, which is probably the best understood part of our brain.",
            "We see that the information coming from our retina gets transformed.",
            "In multiple stages going through different areas of visual cortex.",
            "And if we try to understand what the neurons do in these different areas, they seem to extract features.",
            "There are very sensible if you if you talk to computer vision people.",
            "These features make a lot of sense and the lower level features that computer vision people are used to extract, like edge detectors are extracted in the first of these areas and as you go higher up they detect more complicated shapes an.",
            "More Interestingly, the detect features that are more specialized, more invariant to all kinds of changes that can happen in the input.",
            "And eventually we have neurons that detect whole objects.",
            "There's also this very interesting observation that the cortex seems to be kind of general purpose learning machine that it's not specialized to a particular task, it is.",
            "But parts of the cortex parts of the cortex which are meant to do one job like vision could be doing another job.",
            "If, say, you get blind, or I'd say the part of the cortex which would be meant to do music.",
            "Could suddenly be larger if you're a musician, then if you're not a musician, so that means there are neurons that would, for someone else, have done another job.",
            "But because you're practicing using a lot, some neurons have been recruited to do that, which means they probably all use the same mechanism.",
            "So it looks like there is one simple mechanism that's used for learning all kinds of things in cortex.",
            "So that's that's very encouraging and sort of inspiring for for many of us.",
            "And then of course there is what we know about cognition and how humans solve problems.",
            "Which involves.",
            "Breaking problems into simpler problems and composing the solutions.",
            "Learning similar things first and then more complicated things on top and so on.",
            "So we want to have these these features as well."
        ],
        [
            "I'm going to skip this."
        ],
        [
            "So.",
            "This illustrates what I was telling you about regarding.",
            "Higher level features defined in terms of lower level features.",
            "So what you see in these circles are.",
            "What different artificial neurons like to see in the input and the first layer they like to detect edges?",
            "In the second layer they detect corners and contours and higher up they start detecting object parts and so on.",
            "And eventually objects."
        ],
        [
            "Do we write programs as one very long main program?",
            "No, we write."
        ],
        [
            "Programs that have structure and structure means.",
            "Recursion or reuse of functions, functions calling other functions.",
            "And that's something we want to have in machine learning as well.",
            "That's one one thing that deep learning is trying to bring to machine learning in general."
        ],
        [
            "Yes.",
            "I'm going."
        ],
        [
            "Skip that so as I mentioned.",
            "In 2000 something something happened.",
            "Though many of us had the intuition for many years that we would like to have deeper neural Nets, we hadn't been really successful in training deep neural Nets with back prop or other methods.",
            "But something happened which really started in Toronto with Jeff Hinton, and at almost the same time in my lab and yellow cones lab, where we discovered that we could train very deep Nets if we first used.",
            "Unsupervised learning to train in each layer one at a time in a kind of greedy way.",
            "And each layer would be trained in another, provides way to find a better representation on top of the representation of the lower level.",
            "So the initial work used are BMS restricted Boltzmann machines, which I'm sure Ross talked about this morning.",
            "Autoencoders, which I will talk about later and sparse coding which I want to talk about and the basic procedure."
        ],
        [
            "We used is this.",
            "We take the raw data in its initial form, like pixels."
        ],
        [
            "An we learn a set of features and the way we learn those features.",
            "For example with autoencoders.",
            "But it's."
        ],
        [
            "Almost the same with RBM's is we have a little neural net, which is fairly easy to train, for example here which tries to reconstruct the inputs with some some.",
            "Some regularization or something that prevents the neural net from just copying the input to the output.",
            "And once we've done that, we."
        ],
        [
            "Throw away this this reconstruction layer and use this new representation as input to."
        ],
        [
            "Train another autoencoder.",
            "And we can do it again."
        ],
        [
            "An after awhile we get more and more abstract features.",
            "Which we can use."
        ],
        [
            "As input to logistic regression.",
            "And just do classification with that.",
            "But we can also do what we call fine tuning where we just consider this as initializing very deep feedforward neural net.",
            "So this was a procedure we worked on in those in those days.",
            "There's been a lot of progress since then.",
            "In particular."
        ],
        [
            "The main progress is that we found ways to train deep supervised neural Nets without even using improvised learning simply by changing the way they are trained the way they are initialized and the architecture.",
            "So we found that the initialization of the parameters was quite important.",
            "Um?",
            "And the kind of nonlinearities we would use was quite important.",
            "In particular, we've been using what's called piecewise linear, non linearity.",
            "So instead of the traditional sigmoid or hyperbolic tangent.",
            "Things like rectifiers and the Max out which is related have been found to work quite a bit better for deep Nets especially, and we've also improved the ways of regularising.",
            "So in other words preventing overfitting and helping to generalize with a technique called dropout, which I will also mention later.",
            "The unsupervised pretraining remains important when you deal with rare classes.",
            "When your number of labeled examples maybe is not and is not large enough, or if you want to transfer to new domains or new classes or simply as an extra regularizer."
        ],
        [
            "OK, so now I'm going to do a little bit of.",
            "Machine learning 101.",
            "Your nuts.",
            "Tutorial.",
            "To explain some of the tools later.",
            "So what we want to do is to learn.",
            "A family of functions I call F Theta, where Theta represents tuneable parameters and the goal of learning is going to be changed.",
            "Those parameters so that we minimize some expected loss on you.",
            "Examples where these examples are to be coming from a data generating distribution.",
            "So I'm going to call these examples Ed here and they come from an unknown distribution P of Z.",
            "And we have a loss function L. That measures how well our function F Theta works.",
            "For the example zed.",
            "So for example, in the bottom we see the case the classical case of supervised learning, where Z is has an input part X and an output part Y, and we're trying to predict Y given X and then the loss has two arguments.",
            "The output of the function applied on X&Y.",
            "So if F of X is close to why we're happy?",
            "If not, we're not an.",
            "We measure our happiness and in this way.",
            "Typically we will also consider adding to the training criterion in addition to the average loss on the training set, a regularizer.",
            "The classical regularizer is some function of the parameters, like the smoothness I was talking about can be expressed as a regularizer.",
            "Sometimes the regularizer depends only on the data on the parameters, and sometimes it also depends on the data itself.",
            "Now the last ingredient we need to make an actual learning algorithm is a method to approximately minimize this average regularize loss on the data, and this approximate minimization.",
            "There are many ways of doing it, but I'll tell you about some of the simplest ways which are based on stochastic gradient descent and many match mini batch gradient descent.",
            "OK, so these are the."
        ],
        [
            "Some of the actors, one of the nice things about this framework is that it allows you to.",
            "To learn to estimate conditional probability and also probabilities in general.",
            "So in many supervised learning applications you can frame your problem in terms of estimating P of Y given X.",
            "So you want to predict Y given X.",
            "But Y could be something complicated.",
            "And instead of.",
            "Printing is expected value.",
            "Maybe we want to actually estimate its distribution in general.",
            "So keep in general we have this notion.",
            "We want to estimate P of Y given X.",
            "And the way in which we're going to stick our neural Nets in this is that we're going to declare that P of Y given X is parameterized.",
            "So we can imagine that P of Y, the distribution of Y for a given X is, say, a parametric distribution with parameters Omega.",
            "But those Omega, instead of being fixed like an unusual parametric distribution, are going to be a function of X, and that function is going to be the output of our neural net.",
            "So if we're going to be using our neural net to predict, say, probabilities over classes, then those probabilities are the parameters of our same multinomial distribution.",
            "An once we've set things in this way, we can define a natural loss function for these, which is the negative log like queuing.",
            "So in other words, given our input and given our function F data, we obtain parameters Omega and these parameters assign a probability P of Y to the observed wine and would like.",
            "That the observed Y given X gets the highest probability as possible, measured with its log and the log is because we want to add probability, add the loss over examples and we're assuming the examples are independent.",
            "If they weren't, then we can still fit that in by considering the joint an capturing that those dependencies.",
            "So as an example, 2 examples to classical examples of this is when the Y is the usual real number, which you might imagine represented with a Gaussian.",
            "So that's the Gaussian.",
            "Here isn't the marginal distribution.",
            "Why it's the conditional distribution and why so it means, given XY has maybe Gaussian distributed, so basically it has a typical value, but there is some air around it.",
            "And that's represented with just two parameters that mean and the variance, and so you can have that mean you and the variance Sigma B outputs of your neural net.",
            "Typically we only make the mean very an X an.",
            "We let Sigma be some parameter which doesn't depend on X, but you can also make Sigma the variance depend on X.",
            "So there are cases where.",
            "Only the input tells us about what, why should be, but also what uncertainty there should be about why.",
            "In that case you can make Sigma the output of your neural net as well.",
            "So in the in this simple case where Sigma doesn't depend on X, then your log loss just log Sigma plus the usual squared error weighted by Sigma squared.",
            "Similarly, if Y is a category, then.",
            "Our network could be outputing the probabilities for each of the classes.",
            "And we could use something like a softmax which takes a vector of numbers and converts them into a vector of normalized probabilities.",
            "These would be the Omega eyes.",
            "The parameters of the distribution over Y given X, and so the then again the negative log likelihood would just be minus log of the.",
            "Probability estimated by the model.",
            "Which would be the output for the category wine?"
        ],
        [
            "You can you can extend that to having not just one output variable, but a bunch, a bunch of variables, and if they are independent, it's just like having a neural net with multiple groups of outputs where each group corresponds to one of their random variables.",
            "If those variables are not independent, then you get into a very interesting realm of structured outputs.",
            "So for example, if you're predicting.",
            "Pixels in an image.",
            "For example, you're trying to label the pixels or you're trying to predict the complicated data structure, or you're trying to predict a sentence like in machine translation.",
            "Then the different things you want to predict are not independent of each other.",
            "Given the input, they have a complicated joint distribution.",
            "And capturing that joint distribution is often intractable, but we'll see that.",
            "There are some interesting things that can be done there."
        ],
        [
            "Alright, so in all of these cases we have either log blanket or some proxy or some other loss function and we'd like to minimize.",
            "And we have to think about how to do that, and we would typically do that by iterative optimization, where we start from some configuration of the parameters and we gradually change them.",
            "In order to hopefully reach a minimum, and as you know, you can have local minima.",
            "An if we're going to end up in the local minimum, we'd like to have one that is hopefully close to a global minimum in terms of error.",
            "And."
        ],
        [
            "The simplest way that you could go down the error is used by iterative changes.",
            "Use gradient based optimization where we have a loss, but we can also suppose that we can compute the derivative of our loss and the derivative.",
            "The derivative tells us.",
            "In what direction?",
            "I could make an infinitesimal change in my parameters.",
            "So as to make the loss increase or decrease.",
            "So if we move our parameters in that direction by some epsilon.",
            "Then we hope to make our loss change and improve.",
            "And of course there's vast literature on that question.",
            "But the algorithms that we use in deep learning for this are fairly simple and the simplest of them is called.",
            "Mini batch stochastic gradient descent?",
            "Well, it's not the simplest.",
            "The simplest stochastic gradient descent where we just take one example at a time.",
            "We measure its loss.",
            "We measure the gradient with respect to parameters and we do one step of gradient in that direction so it's called stochastic because we the real loss we care about is the average over the whole training set, and we're only taking one example or a group of example, which we call a mini batch in order to decide where to move.",
            "And so this is a kind of noisy estimate of the true gradient.",
            "But actually it has all kinds of nice properties, and more importantly, it converges very, very quickly."
        ],
        [
            "So now.",
            "Neural Nets.",
            "If you're familiar with logistic regression can be understood very easily by thinking of.",
            "A bunch of logistic regressions that you could be running at the same time if somebody told you a good representation for your problem.",
            "In other words, that kind of sub problem you'd like to solve.",
            "And you would know what the target values for these features would be given the inputs.",
            "Then you could just train a bunch of logistic regression to predict these."
        ],
        [
            "And then you could take these as input to your actual problem of interest.",
            "But the nice thing with units is that you don't have to tell the neural net with these intermediate values should be these intermediate features.",
            "It is going to look for them, of course, that makes the optimization problem harder, and that's the reason why people have preferred kernel methods for about more than a decade, because the optimization problem is now becomes non convex.",
            "But what we've got here is essentially we've learned the kernel.",
            "We've learned the feature space.",
            "Um, we were looking at it more difficult optimization problem but.",
            "But it it is, will see it has both computational and statistical advantages.",
            "And of course, once you understand this process, you can do it."
        ],
        [
            "Multiple times to have deeper neural Nets, so one of the."
        ],
        [
            "The ingredients in many of the techniques we use these days is backdrop, and that is that word is used in often the wrong sense in the sense that I would, I and others like Jeff Hinton would like people to retain, is back propagation algorithm to compute gradients.",
            "Now you can use those gradients as part of an optimization method like gradient descent, or say Hashem isation or conjugate gradients or albc.",
            "But but the way you compute the gradient is what backup is about.",
            "And what it relies on is called the chain rule.",
            "And it's something that makes that's really important when as soon as you start thinking about composing functions.",
            "So the simplest case of composition of functions is.",
            "Here we have X, which is transformed into Y by a function.",
            "G&Y gets transformed into Zed by function F, so zed is F of G of X.",
            "So if we want to know how to change.",
            "X So as to change said in a particular way, we need the ZX and the chain Rule tells us this is DZ y * D YDX."
        ],
        [
            "See why this is true?",
            "Think about this so we can.",
            "We can use this notation of flow graphs where each variable is a little circle.",
            "An represents a value.",
            "And the arrows tell us.",
            "What values computed based on what values?",
            "So we have partial derivatives now that we can attach attach to these these arcs.",
            "So DZDY tells us how a small change in Y makes a small change in Zen and similarly for dyd X.",
            "So.",
            "If we know that a small change Delta Y.",
            "When we multiply by diesel, DY tells us how zed oil change so it gives us a Delta zetten.",
            "We know this similar thing that Delta Y Delta X * D YDX gives us Delta Y.",
            "We just plug one of these in the other an we get that change in X. Delta X gets transformed into a change Delta set by multiplying Delta XY and ZYDYDX, right?",
            "So that's just very very simple.",
            "Algebra.",
            "And we get the chain rule."
        ],
        [
            "And if you had two paths to go from X to Z, you can make a similar argument for each of those paths, and you end up finding that the partial derivative is just a sum.",
            "Of the product of the partial derivatives along each of those paths."
        ],
        [
            "Right, and of course, if you have N paths, it's just the sum of the partial derivatives across all of these paths."
        ],
        [
            "And now you can make a general rule which tells you that if you have a graph of computation, which I call a flow graph where each node is one of these values.",
            "And you want to know the ZDX where Zen is the final nodes scaler.",
            "And X is any intermediate node in the middle.",
            "Then you can obtain it by looking at D. Zeddy Wiiware, why I is one is the one of the immediate descendants of X.",
            "So you look at all the descendants of X.",
            "We call them why I or the successors of X.",
            "And assuming you have already computed this ladies at the Why are you can compute the ZX by taking those?",
            "These are the wise and multiplying them by the partial derivatives relating each of those wise 2X the dyd X, and that's the most general form of the chain rule which which we take advantage of in back problem.",
            "You just plug it in your code actually."
        ],
        [
            "Design code around that and you have a very general purpose way of.",
            "Doing by proper in any kind of system.",
            "So in the regular neural net.",
            "What it means is you think of the neural net as this graph, where now the nodes include both your inputs, your hidden units, but also your parameters as well as your label annual loss.",
            "And for each of those nodes, there is a well defined computation as well as partial derivatives that you can compute.",
            "So for example, you can compute the partial derivative of the negative log likelihood.",
            "Here the final loss with respect to the output of the neural net.",
            "And then you can propagate that into.",
            "A derivative with respect to the parameters of the first layer W and with respect to the hidden unit.",
            "Outputs H. And so on.",
            "To get the gradients on the other set of parameters V. All."
        ],
        [
            "Right, and so that gives you a kind of general formula general algorithm for computing gradients in a flow graph.",
            "You have two parts to this.",
            "The F drop in the backdrop.",
            "The F prop visits the nodes in some order, which we call it topological order, so it creates a directed.",
            "Acyclic graph that goes from inputs to the final node and the final node in our problem is going to be a loss of scalar that we would like to minimize an average and then in the second phase the backdrop phase we are going to follow those those arcs backwards.",
            "And at each node we're going to compute the derivative of the final loss, said with respect to the values computed in the forward pass.",
            "So for the final node, these addys at is 1.",
            "But then we're going to compute the ZX for every internal node, and for each of those nodes, we're going to be using.",
            "Oh does that mean it's time for a break?",
            "We're going to be using the chain rule.",
            "As I showed earlier.",
            "Alright.",
            "And.",
            "I guess maybe now is a good time for the break anyways."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So welcome to this second tutorial on deep learning.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I suppose that many of you attended the morning tutorial by Russell.",
                    "label": 0
                },
                {
                    "sent": "A good enough.",
                    "label": 0
                },
                {
                    "sent": "So I will.",
                    "label": 0
                },
                {
                    "sent": "There will be some overlap, but mostly the things I'll talk about will be different.",
                    "label": 0
                },
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": "Do not hesitate to interrupt me in the middle and ask questions.",
                    "label": 0
                },
                {
                    "sent": "We have plenty of time.",
                    "label": 0
                },
                {
                    "sent": "And just raise your hand and I'll try to keep watching.",
                    "label": 0
                },
                {
                    "sent": "So I've been embarked on this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's crazy adventure.",
                    "label": 0
                },
                {
                    "sent": "Because I want to understand intelligence, I want to understand the computational.",
                    "label": 0
                },
                {
                    "sent": "And mathematical principles that could give rise to intelligence in animals and humans.",
                    "label": 0
                },
                {
                    "sent": "And of course, in machines that we want to build.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I believe that it's central to this quest that we understand.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Machines or living beings can acquire knowledge.",
                    "label": 0
                },
                {
                    "sent": "And so I've been doing machine learning for a couple of decades.",
                    "label": 0
                },
                {
                    "sent": "And since almost the same time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've been working with neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And there's been a breakthrough in the last few years that people called deep learning that I'm going to tell you about today.",
                    "label": 0
                },
                {
                    "sent": "And it's both machine learning algorithms that are inspired by brains brains, just like neural Nets have been.",
                    "label": 0
                },
                {
                    "sent": "But with particular twists, which is this notion that we want to represent data and we want to do it using multiple levels of representation?",
                    "label": 0
                },
                {
                    "sent": "And the idea is that these multiple levels of representation correspond to multiple levels of abstraction.",
                    "label": 0
                },
                {
                    "sent": "And there are a number of theoretical.",
                    "label": 0
                },
                {
                    "sent": "And practical reasons why we want to do that.",
                    "label": 0
                },
                {
                    "sent": "That that hopefully you will.",
                    "label": 0
                },
                {
                    "sent": "Be clear to you at the end of this tutorial.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So deep learning.",
                    "label": 0
                },
                {
                    "sent": "Is a particular form of representation learning.",
                    "label": 0
                },
                {
                    "sent": "Representation learning is a way for.",
                    "label": 1
                },
                {
                    "sent": "Machine learning systems too.",
                    "label": 1
                },
                {
                    "sent": "Discover new representations of data they're going to be useful deep learning.",
                    "label": 0
                },
                {
                    "sent": "Our representation, learning algorithms that have these multiple levels of representation.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is part of the bigger research on machine learning that I'm sure you know about which is itself part of AI.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A deep learning has attracted a lot of attention in the recent two years, roughly.",
                    "label": 0
                },
                {
                    "sent": "Although it really had a big breakthrough in 2006, so that's eight years ago.",
                    "label": 0
                },
                {
                    "sent": "It has become used heavily in industry, mostly because it has almost revolutionized areas such as speech recognition and object recognition and computer vision.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more going on and coming.",
                    "label": 0
                },
                {
                    "sent": "In other areas of application.",
                    "label": 0
                },
                {
                    "sent": "Within computer vision, other applications besides object recognition, like detection.",
                    "label": 0
                },
                {
                    "sent": "And parsing of scenes, but also in an area that may be more interesting from many people in this audience, which has to do with.",
                    "label": 0
                },
                {
                    "sent": "Dealing with text, natural language processing dialogue.",
                    "label": 0
                },
                {
                    "sent": "More recently, there's been some interesting advances in combining deep learning with reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me give you an example of how things have changed with the.",
                    "label": 0
                },
                {
                    "sent": "Input of deep learning in speech recognition.",
                    "label": 1
                },
                {
                    "sent": "So this these numbers come from Microsoft and the talk about performance on a switchboard data set, which has been longstanding benchmarks from the early 90s.",
                    "label": 0
                },
                {
                    "sent": "So the X axis are yours and the Y axis is.",
                    "label": 0
                },
                {
                    "sent": "Word error rate which one of go down things have been progressing well in the 90s?",
                    "label": 0
                },
                {
                    "sent": "Even though the algorithms themselves haven't changed much in those years, I was actually working on speech recognition myself in the early 90s when I completed my PhD in 91.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So there was mostly advances because of being able to train bigger systems, bigger models, faster with faster computers, much larger datasets, but there's only so much you can do with that an in the decade that followed, things have kind of flat and.",
                    "label": 0
                },
                {
                    "sent": "And then progress started to accelerate again with the use of deep learning.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of reason why people have been attracted to this area.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something similar has happened in object recognition at about the same time, a little bit later there was a breakthrough in one of the most interesting.",
                    "label": 0
                },
                {
                    "sent": "Challenging tasks in object recognition.",
                    "label": 0
                },
                {
                    "sent": "The Imagenet data set that has 1000 classes of objects and about a million training examples actually now 20,000,000.",
                    "label": 0
                },
                {
                    "sent": "And you see, here.",
                    "label": 0
                },
                {
                    "sent": "Outputs of this early system, which was getting around 17% correct in the sense of the having right answer on the 1st 5.",
                    "label": 0
                },
                {
                    "sent": "I'm more modern systems that the latest systems have now are now down to five or six.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Percent these were the numbers as they were in 2012.",
                    "label": 0
                },
                {
                    "sent": "So on the right hand side you see the.",
                    "label": 0
                },
                {
                    "sent": "Error rates obtained by by other systems competing in those days with the system put up by Alex Kryszewski.",
                    "label": 0
                },
                {
                    "sent": "Yes, Karen, Jeff Hinton, but won this competition and brought it down from about 26% error to build 16% error.",
                    "label": 0
                },
                {
                    "sent": "And they said in the.",
                    "label": 0
                },
                {
                    "sent": "Your afterwards they brought it down to around 11 and this year is down to 6% or something like that.",
                    "label": 0
                },
                {
                    "sent": "So there's been amazing progress thanks to these techniques.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can try it out for yourself.",
                    "label": 0
                },
                {
                    "sent": "Russell probably showed this URL.",
                    "label": 0
                },
                {
                    "sent": "This morning just deep learning.cs.toronto.edu you go there and you can upload a URL of an image.",
                    "label": 1
                },
                {
                    "sent": "And it will use these 1000 categories to try to identify the main object in the image.",
                    "label": 0
                },
                {
                    "sent": "So here it's saying convertible.",
                    "label": 0
                },
                {
                    "sent": "I hear it's saying chimpanzee here it's saying sports car and I just picked those images without searching much using Google Image Search and pasted them.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not always perfect, for example here it says Killer Whale.",
                    "label": 0
                },
                {
                    "sent": "With 35% confidence and Tiger Shark with 19% confidence in scuba diver, so it's not always perfect, of course, and there's a lot more to be done.",
                    "label": 0
                },
                {
                    "sent": "But this German Shepherd was perfectly recognized and.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "It's if you've tried computer vision and object recognition.",
                    "label": 0
                },
                {
                    "sent": "In the past it was very brittle and these systems are just.",
                    "label": 0
                },
                {
                    "sent": "Made a qualitative change by the way in which they are robust to all kinds of variations in illumination and background, and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my lab, we applied these techniques to emotion classification from movie clips from video using both the audio and images an we want to competition.",
                    "label": 0
                },
                {
                    "sent": "Was held last December using using very fairly similar systems.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with these and other advances in 2013, the MIT Technology Review declared declining to be one of the 10 breakthrough technologies of the year.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's been a lot of.",
                    "label": 0
                },
                {
                    "sent": "Journalists talking about it in New York Times and wired and others.",
                    "label": 0
                },
                {
                    "sent": "An companies have started competing.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For the technology and for the people working on it in particular, Google and Facebook, where in an arms race too.",
                    "label": 0
                },
                {
                    "sent": "Acquire our companies or people one big event and this was the Google buying deep line for about 500 million dollars.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne now there's tons of startups in this in this area.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's all nice and there's a lot of hype.",
                    "label": 0
                },
                {
                    "sent": "But there are some real research that remains to be done and.",
                    "label": 0
                },
                {
                    "sent": "Some interesting challenges, so this tutorial is about these challenges.",
                    "label": 0
                },
                {
                    "sent": "Also introducing you to some of the concepts behind deep learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll try to tell you about what I consider the major challenge, which is how to move the successes we've had with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Most of these applications have been using supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Two unsupervised or semi supervised learning where you have tons of unlabeled or weakly labeled data and structured outputs which is tightly connected to the improvised learning question.",
                    "label": 1
                },
                {
                    "sent": "Where the output of the system is not just a category or a few numbers, but it's something that's very high dimensional, like a sentence, an image, or some complicated data structure.",
                    "label": 0
                },
                {
                    "sent": "The thing with these both supervised both these structured outputs and this provides learning scenarios.",
                    "label": 1
                },
                {
                    "sent": "Is that probabilistic models quickly become intractable.",
                    "label": 0
                },
                {
                    "sent": "Computation both from a computational point of view and from the statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "So the use of deep learning in these areas could have a fairly important impact.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about the challenge of scaling up from a computational point of view, because what we've seen in recent years is that the better models are the bigger ones, and we're limited just by our ability to train bigger models and related to this, I'll tell you about some of the challenges arising in.",
                    "label": 0
                },
                {
                    "sent": "Training deep Nets.",
                    "label": 0
                },
                {
                    "sent": "So there's a numerical optimization issue.",
                    "label": 0
                },
                {
                    "sent": "Even though the techniques we're using are doing amazingly well, we have some indication that we're not doing as well as we could, and I'll tell you about that and some ideas to get around these.",
                    "label": 0
                },
                {
                    "sent": "Challenges.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we can, if we can face these challenges, then it can be a much more impressive impact on AI.",
                    "label": 0
                },
                {
                    "sent": "I think with computers that can see and hear and understand natural language understand humans.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne provide better service to humans and of course maybe even understand.",
                    "label": 0
                },
                {
                    "sent": "Help us understand how humans and animals.",
                    "label": 0
                },
                {
                    "sent": "Managed to solve difficult learning problems.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "So now let's go a bit more technical slowly.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "How do we build intelligent machines well?",
                    "label": 0
                },
                {
                    "sent": "You need knowledge in order to take intelligent decisions and you need that knowledge to come from somewhere and part of it could be that we.",
                    "label": 0
                },
                {
                    "sent": "Feed that knowledge directly to the machines, but what we've learned in the last few decades is that a large part of it has to be directly acquired by the machine, and that's what machine learning is about.",
                    "label": 0
                },
                {
                    "sent": "And if you.",
                    "label": 0
                },
                {
                    "sent": "Learn about machine learning and you try to figure out what are the core elements that you find in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Well, one core element is priors.",
                    "label": 0
                },
                {
                    "sent": "In other words, no, there's no general purpose machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You need some priors and price could be very specialized to your application.",
                    "label": 0
                },
                {
                    "sent": "Or they could be very broad and in AI we're trying to look at very broad priors.",
                    "label": 0
                },
                {
                    "sent": "But both both could be interesting.",
                    "label": 0
                },
                {
                    "sent": "Then there is always an element of optimization or search where searching for a good solution to a problem in a large family of solution.",
                    "label": 0
                },
                {
                    "sent": "Or in the case of inference, we're looking for searching for a good explanation to a particular input, like when you're doing machine translation or or speech recognition.",
                    "label": 0
                },
                {
                    "sent": "There are many explanations to the thing we see and we're looking for the best one, so there's search or optimization comes up in these algorithms.",
                    "label": 0
                },
                {
                    "sent": "And practically, from a computer science perspective, there's also an issue of efficiency of computation.",
                    "label": 0
                },
                {
                    "sent": "We've seen a lot of progress in recent years due to GPU's that can paralyze a lot of that computation.",
                    "label": 0
                },
                {
                    "sent": "So thinking about that aspect is also important.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The more fundamental question behind machine learning is generalization.",
                    "label": 0
                },
                {
                    "sent": "How do we get?",
                    "label": 0
                },
                {
                    "sent": "Our learner to provide the right answers for new cases.",
                    "label": 0
                },
                {
                    "sent": "And the take that I'm trying to put forward on this generalization question is a geometrical One South.",
                    "label": 0
                },
                {
                    "sent": "During the presentation you will see this come back when I'm thinking about generalization is how we take probability mass.",
                    "label": 0
                },
                {
                    "sent": "That we know should be high on the training examples an spreading it in a right way.",
                    "label": 0
                },
                {
                    "sent": "Guessing where it should concentrate.",
                    "label": 0
                },
                {
                    "sent": "So you will see this notion come back.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately, in high dimensional spaces, guessing where to put probability mass is seems intractable, because there's a exponentially orange large number of configurations of places where you could put probability mass.",
                    "label": 0
                },
                {
                    "sent": "So that's the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And how do we get around that?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Something I've been pushing in the last few years.",
                    "label": 0
                },
                {
                    "sent": "Is this idea that?",
                    "label": 0
                },
                {
                    "sent": "We can get around the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "If our learning systems can discover and disentangle separate out.",
                    "label": 0
                },
                {
                    "sent": "The underlying factors that explain the data.",
                    "label": 0
                },
                {
                    "sent": "The underlying cause is basically making sense of the data, so it may seem like a hard thing to do.",
                    "label": 0
                },
                {
                    "sent": "And of course it is, but I've been trying to.",
                    "label": 0
                },
                {
                    "sent": "To push that as the agenda for making really serious progress towards AI.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now let's go back to machine learning 101.",
                    "label": 0
                },
                {
                    "sent": "Let's consider this one dimensional supervised learning task where you have to predict Y given X, and if you're given these examples, these green stars.",
                    "label": 0
                },
                {
                    "sent": "And of course there is a true underlying function that relates the wise in the axis, maybe with some noise.",
                    "label": 0
                },
                {
                    "sent": "And you're trying to guess it using these examples.",
                    "label": 0
                },
                {
                    "sent": "So that's the simple thing that we're trying to do, and here it will be very easy in one way to see why it will be very easy is that the I can't do that the.",
                    "label": 0
                },
                {
                    "sent": "Examples are insufficient number.",
                    "label": 0
                },
                {
                    "sent": "That they can cover the ups and downs of the function you want to learn.",
                    "label": 0
                },
                {
                    "sent": "And because they can cover the ups and downs of the function you want to learn, you basically just need to interpolate.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is shown in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So you can do it, you know, in very smart ways, and there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Statistical machine learning that you know does that with very beautiful mathematics like kernel machines.",
                    "label": 0
                },
                {
                    "sent": "That's nice and.",
                    "label": 0
                },
                {
                    "sent": "What it does really is it exploits aprior.",
                    "label": 0
                },
                {
                    "sent": "Remember I said one of the crucial ingredients in machine learning is the priors.",
                    "label": 0
                },
                {
                    "sent": "What's the prior here?",
                    "label": 0
                },
                {
                    "sent": "The prior is what people call the smoothness prior.",
                    "label": 1
                },
                {
                    "sent": "It says the value of the function you want to learn.",
                    "label": 0
                },
                {
                    "sent": "At Test point X.",
                    "label": 0
                },
                {
                    "sent": "Should be close to.",
                    "label": 0
                },
                {
                    "sent": "The value you observed in the neighborhood, so the function is smooth.",
                    "label": 1
                },
                {
                    "sent": "It doesn't change rapidly, so that's why interpolation works.",
                    "label": 0
                },
                {
                    "sent": "That's nice and we should use that prior, but it's not.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enough when you go into higher dimensional spaces where the set of configurations of interests becomes exponentially large.",
                    "label": 0
                },
                {
                    "sent": "And maybe the functions you want to learn aren't that smooth.",
                    "label": 0
                },
                {
                    "sent": "Then you get into trouble 'cause the number is the number of ups and downs of the function you'd like to cover or the number of configurations you'd like to separate.",
                    "label": 0
                },
                {
                    "sent": "Could become much, much larger than the number of examples you could ever hope to get.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "One idea people had about 15 years ago is that we could solve that problem by reducing dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we've shown with some theory that it's not really dimensionality.",
                    "label": 0
                },
                {
                    "sent": "You can have a 1 dimensional function that could be very very hard to learn.",
                    "label": 0
                },
                {
                    "sent": "What makes it really hard is the number of Upson Downs is the variability in the function you want to learn.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Furthermore, even if you were able to reduce dimensionality.",
                    "label": 0
                },
                {
                    "sent": "It might not be enough.",
                    "label": 0
                },
                {
                    "sent": "Because the functions maybe the manifold's near which is data concentrate very a lot and actually we do see this in AI applications.",
                    "label": 0
                },
                {
                    "sent": "Now this idea of manifold learning however is very inspiring and it has been inspiring from me and for many people.",
                    "label": 0
                },
                {
                    "sent": "And it relies on another prior, so I've already talked about one prior which was a smoothness and there is a second one here.",
                    "label": 0
                },
                {
                    "sent": "It doesn't apply to every data set.",
                    "label": 0
                },
                {
                    "sent": "Of course, like priors, they apply to some problems, but not all.",
                    "label": 0
                },
                {
                    "sent": "And what it says here is the probability.",
                    "label": 0
                },
                {
                    "sent": "Mass concentrates, in other words, if you get if you look at your data and you look at the variables of interests.",
                    "label": 0
                },
                {
                    "sent": "You consider completely random, say, uniform configurations of your variables.",
                    "label": 0
                },
                {
                    "sent": "These random configurations would be very unlikely you wouldn't.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't expect to see these in the data, so images where you pick the pixels in dependently you get these white noise and these are not at all like natural images.",
                    "label": 0
                },
                {
                    "sent": "If you look at sequences of random characters, they don't look like English.",
                    "label": 0
                },
                {
                    "sent": "What's the chance that you would produce a natural image by randomly picking pixels or what's the chance that you will pick?",
                    "label": 0
                },
                {
                    "sent": "And a natural looking English sentence by picking characters randomly for long enough tiny.",
                    "label": 0
                },
                {
                    "sent": "Exponentially tiny.",
                    "label": 0
                },
                {
                    "sent": "So that says something about the kind of data that we're working with here.",
                    "label": 0
                },
                {
                    "sent": "In many AI tasks, that data has this manifold structure.",
                    "label": 1
                },
                {
                    "sent": "The probability concentrates and in continuous case we think of that concentration as regions.",
                    "label": 0
                },
                {
                    "sent": "Of low probability of low dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the geometrical view on machine learning is we're going to try to guess where this probability mass concentrates.",
                    "label": 1
                },
                {
                    "sent": "And in order to face the challenge that there is an exponential large number of such places where we could put probably mass.",
                    "label": 0
                },
                {
                    "sent": "We're going to go through this notion of representation learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea is going to change.",
                    "label": 0
                },
                {
                    "sent": "We're going to do a change of coordinate.",
                    "label": 0
                },
                {
                    "sent": "We're going to move to a new space where things are simpler and potentially of lower dimension.",
                    "label": 0
                },
                {
                    "sent": "So in the figure we see this one dimensional manifold, which is twisted.",
                    "label": 0
                },
                {
                    "sent": "If only we could map the points on the left.",
                    "label": 0
                },
                {
                    "sent": "Through a space where the same manifold now corresponds to a nice flat line, even horizontal line, then the modeling problem would see would be much easier right on the right hand side you could just use a simple Gaussian and you can basically get it right.",
                    "label": 0
                },
                {
                    "sent": "OK, so the key is going to be learning these representations that suddenly make the learning problem easy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again to emphasize the same idea.",
                    "label": 0
                },
                {
                    "sent": "Nobody gives US data.",
                    "label": 0
                },
                {
                    "sent": "That's what we call the empirical distribution an from the general point of view, you can think of these points in space in the space of configurations.",
                    "label": 0
                },
                {
                    "sent": "And at each of those points, we know that policy should be high.",
                    "label": 0
                },
                {
                    "sent": "We know that if one of the axes X and the other is Y, then when X text these value the corresponding wisely.",
                    "label": 0
                },
                {
                    "sent": "All right, that's a raw data, and what smoothness tells us is that neighborhood configurations were also likely.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a little bit of mileage, but not that much in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "So instead, we'd like to figure out that these points are kind of aligned in some way.",
                    "label": 0
                },
                {
                    "sent": "They have some structure.",
                    "label": 0
                },
                {
                    "sent": "If we can find that structure, then we can generalize much more accurately than with this simpler spreading.",
                    "label": 0
                },
                {
                    "sent": "The mass around idea of smoothing, but you see this, it's like a new representation where now the coordinate system would be the location on that curve.",
                    "label": 0
                },
                {
                    "sent": "So that's what representation learning is about.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily going to give us really a Euclidean coordinate system, but it's going to move the data into a new space where things become easy.",
                    "label": 0
                },
                {
                    "sent": "At least that's going to be the goal.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what representation learning is about.",
                    "label": 0
                },
                {
                    "sent": "It's about taking our data Ann transforming it.",
                    "label": 0
                },
                {
                    "sent": "Extracting features that make the life of further machine learning easier.",
                    "label": 0
                },
                {
                    "sent": "Now the traditional way of doing things in machine learning is to handcraft your features.",
                    "label": 0
                },
                {
                    "sent": "Because humans are very good at that, they can understand the problem.",
                    "label": 0
                },
                {
                    "sent": "They can think about it.",
                    "label": 0
                },
                {
                    "sent": "They can guess good features, good transformations, and they're looking for features.",
                    "label": 0
                },
                {
                    "sent": "They're going to be very informative for the task that they care about.",
                    "label": 0
                },
                {
                    "sent": "But that's a lot of work.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of manual labor where somebody has to understand both machine learning and the application in industrial applications of machine learning that I was exposed to, it was usually more than 90% of the effort was in designing these features.",
                    "label": 0
                },
                {
                    "sent": "And that makes the spread of machine learning kind of expensive and slow.",
                    "label": 0
                },
                {
                    "sent": "What we would really like is to use computers to figure out those features, so that's what representation learning is about.",
                    "label": 0
                },
                {
                    "sent": "Of course it doesn't prevent you from using your knowledge to provide the data in a good form, but if we could let the computers figure out good transformations of that data.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would be great.",
                    "label": 0
                },
                {
                    "sent": "This figure illustrates the same notion.",
                    "label": 0
                },
                {
                    "sent": "I'm with traditional.",
                    "label": 0
                },
                {
                    "sent": "Well, it goes from rule based systems to deep learning.",
                    "label": 1
                },
                {
                    "sent": "So on the left hand side you see the old way where there is not any machine learning.",
                    "label": 0
                },
                {
                    "sent": "You handcraft a program that goes from inputs to outputs.",
                    "label": 1
                },
                {
                    "sent": "Then we have the classic machine learning methods where you hand design your features and then you have machine learning that map those features to your output.",
                    "label": 0
                },
                {
                    "sent": "So your classes.",
                    "label": 0
                },
                {
                    "sent": "Now with representation learning, we introduce the notion that the features are going to be learned rather than hand designed.",
                    "label": 0
                },
                {
                    "sent": "Of course, that doesn't you can.",
                    "label": 1
                },
                {
                    "sent": "You can also think of it as adding an extra level where we take the hand design features and fold them by better learned features and with deep learning is just that those features are not going to come in a hierarchy of features, so we can have maybe low level similar features.",
                    "label": 0
                },
                {
                    "sent": "And on top of the more complex features and you can have more than two levels of course.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when you do that, you can do all kinds of exciting things.",
                    "label": 0
                },
                {
                    "sent": "The Google Image Search system is based on this idea and what it adds to this notion is not different.",
                    "label": 1
                },
                {
                    "sent": "Modalities like texts and images can be associated through these representations that are learned.",
                    "label": 0
                },
                {
                    "sent": "So, for example, what you see on the slide is.",
                    "label": 0
                },
                {
                    "sent": "A system in which images are mapped with one function.",
                    "label": 0
                },
                {
                    "sent": "Called by I.",
                    "label": 0
                },
                {
                    "sent": "To a representation space, A vector space of 100 dimensions, and there's another function.",
                    "label": 0
                },
                {
                    "sent": "Find W for words or queries that takes words or queries.",
                    "label": 1
                },
                {
                    "sent": "So text Ann Maps them to the same space and those two functions are trained jointly together so that when somebody types dolphin and then clicks on that image.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The two objects end up close to each other in that space.",
                    "label": 1
                },
                {
                    "sent": "This work was done by my brother Samy Bengio, by the way, and his collaborators, Jason Weston.",
                    "label": 0
                },
                {
                    "sent": "Nicola losing you and others in a series of papers around 2010.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, this idea of mapping words in language to a representation which is a vector.",
                    "label": 0
                },
                {
                    "sent": "This was something that I put forward in 2000 and presented at NIPS.",
                    "label": 1
                },
                {
                    "sent": "2000 follows up on earlier work from Geoff Hinton in representing symbols in the 80s.",
                    "label": 0
                },
                {
                    "sent": "And in the work I did, we learned what's called a language model in which we do unsupervised learning on sequences of text, sequences of words, and as a side effect, each word gets mapped to a point in a. Vector space in those days was 50 dimension or 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "You can't see those words, but we can zoom in.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "First of all, you can see that there are these big big clusters.",
                    "label": 0
                },
                {
                    "sent": "It turns out that these big clusters corresponds to correspond to part of speech, but if you zoom in, you start seeing sort of.",
                    "label": 0
                },
                {
                    "sent": "Semantically grouped words, for example.",
                    "label": 0
                },
                {
                    "sent": "Here we have countries or different verbs or variants of to be.",
                    "label": 0
                },
                {
                    "sent": "So you get a sense that when you learn representations.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Objects that are semantically similar and are close to each other and that's connected.",
                    "label": 0
                },
                {
                    "sent": "That's useful and connected to the earlier prior of smoothness.",
                    "label": 0
                },
                {
                    "sent": "So words viewed as symbols are as far as each other as any pair of word is as far as each other as any other pair of word.",
                    "label": 0
                },
                {
                    "sent": "But once you've mapped into that space, suddenly, if you've learned something about a word.",
                    "label": 0
                },
                {
                    "sent": "Like take then that might generalize to nearby word in the same space like get.",
                    "label": 0
                },
                {
                    "sent": "And actually, you can think of directions in that space.",
                    "label": 0
                },
                {
                    "sent": "As attributes.",
                    "label": 0
                },
                {
                    "sent": "That the system is learned.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This idea was pushed in a very nice way by Thomas Mikolov and his collaborators.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Where they discovered that if you take these representations an you take the differences between words.",
                    "label": 0
                },
                {
                    "sent": "Those differences have a meaning, and that meaning is preserved throughout different regions of space.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take the word embedding for friends.",
                    "label": 0
                },
                {
                    "sent": "And the word embedding for Paris.",
                    "label": 0
                },
                {
                    "sent": "So these are points in 700 dimensional space and you subtract them.",
                    "label": 0
                },
                {
                    "sent": "You get a vector, and if you take it's Lee minus Rome you get another vector, but these two vectors are almost the same.",
                    "label": 0
                },
                {
                    "sent": "And so you can do algebra and find that King Minos Queen is almost the same as man minus woman.",
                    "label": 0
                },
                {
                    "sent": "So you can use this to reason by analogy, and it it's something that comes up without having been trained for.",
                    "label": 0
                },
                {
                    "sent": "It comes up of unsupervised learning of modeling sequences of words.",
                    "label": 0
                },
                {
                    "sent": "It's not something that the system is being trained for it to look at pairs like this and make sure they match.",
                    "label": 0
                },
                {
                    "sent": "So this is quite exciting.",
                    "label": 0
                },
                {
                    "sent": "It brings us to the realm of semantics in a way that we couldn't imagine 10 or 15 years ago would be able to do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these kinds of word embeddings recently of making starting to make breakthroughs in machine translation.",
                    "label": 1
                },
                {
                    "sent": "One of the areas of AI that I consider really interesting because.",
                    "label": 0
                },
                {
                    "sent": "To do a really good job in translation, you need to understand the sentence.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we have a paper coming up in September, I think where we used these deep recurrent networks in order to do machine translation.",
                    "label": 1
                },
                {
                    "sent": "And there's been in at the end of June at the ACL conference best paper award given to.",
                    "label": 0
                },
                {
                    "sent": "Paper from BBN.",
                    "label": 1
                },
                {
                    "sent": "Where they have achieved fairly amazing improvements in Bleu score, which is the metric used in machine translation.",
                    "label": 0
                },
                {
                    "sent": "For some, for some tasks, I think they did it on Arabic, Arabic, English and Chinese English.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our system, the way we think about the machine translation problem and I give that as an example because it gives you a taste of.",
                    "label": 0
                },
                {
                    "sent": "What it means to think of machine learning problem with representation learning as a core element?",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take a sentence like economy growth has slowed down in recent years an we have a machine which happens to be a recurrent net in this case.",
                    "label": 0
                },
                {
                    "sent": "That not only represents the words in the sentence, but also represents the whole sentence as as a an object which.",
                    "label": 0
                },
                {
                    "sent": "Here is a big vector.",
                    "label": 0
                },
                {
                    "sent": "And then there's another machine.",
                    "label": 0
                },
                {
                    "sent": "Which we call the decoder that takes that object an Maps it to a distribution over sequences in the other language.",
                    "label": 0
                },
                {
                    "sent": "An if you sample from that, or if you look for the most likely translation, it might tell you something like lacrosse, economic ROTC danniels any which is a good translation for this.",
                    "label": 0
                },
                {
                    "sent": "Sentence this actually was run on our system and even have a demo running.",
                    "label": 0
                },
                {
                    "sent": "If you want to try it out.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So this encoder decoder idea is actually something I'll come back to because these autoencoders come up as very useful ideas in deep learning.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us go back to this notion of having multiple levels of representation.",
                    "label": 1
                },
                {
                    "sent": "First of all, there is.",
                    "label": 0
                },
                {
                    "sent": "All kinds of vertical results which point in the direction that representing functions.",
                    "label": 0
                },
                {
                    "sent": "With sufficient levels of composition with sufficient.",
                    "label": 0
                },
                {
                    "sent": "Composition of nonlinearities and combination of pieces.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Can give you an exponential gain.",
                    "label": 0
                },
                {
                    "sent": "In other words that there are some functions that can be represented very efficiently.",
                    "label": 0
                },
                {
                    "sent": "If you use what's called a deep circuit.",
                    "label": 0
                },
                {
                    "sent": "Where you do some nonlinear computation.",
                    "label": 0
                },
                {
                    "sent": "And you do it again and again.",
                    "label": 0
                },
                {
                    "sent": "In different ways.",
                    "label": 0
                },
                {
                    "sent": "Those are functions that can be represented very efficiently in this way, but that require an exponential amount of computation if you do it with a shallow circuit where you're only allowed to do, say, 2 levels of these operations.",
                    "label": 0
                },
                {
                    "sent": "So these operations might be things like Boolean gates or or neurons, artificial neurons.",
                    "label": 0
                },
                {
                    "sent": "Or rectifiers we've been playing with recently.",
                    "label": 0
                },
                {
                    "sent": "There's also inspiration from.",
                    "label": 0
                },
                {
                    "sent": "Brains, as I mentioned at the beginning of my presentation.",
                    "label": 0
                },
                {
                    "sent": "When we look at.",
                    "label": 0
                },
                {
                    "sent": "The visual cortex in particular, which is probably the best understood part of our brain.",
                    "label": 0
                },
                {
                    "sent": "We see that the information coming from our retina gets transformed.",
                    "label": 0
                },
                {
                    "sent": "In multiple stages going through different areas of visual cortex.",
                    "label": 0
                },
                {
                    "sent": "And if we try to understand what the neurons do in these different areas, they seem to extract features.",
                    "label": 1
                },
                {
                    "sent": "There are very sensible if you if you talk to computer vision people.",
                    "label": 0
                },
                {
                    "sent": "These features make a lot of sense and the lower level features that computer vision people are used to extract, like edge detectors are extracted in the first of these areas and as you go higher up they detect more complicated shapes an.",
                    "label": 0
                },
                {
                    "sent": "More Interestingly, the detect features that are more specialized, more invariant to all kinds of changes that can happen in the input.",
                    "label": 0
                },
                {
                    "sent": "And eventually we have neurons that detect whole objects.",
                    "label": 0
                },
                {
                    "sent": "There's also this very interesting observation that the cortex seems to be kind of general purpose learning machine that it's not specialized to a particular task, it is.",
                    "label": 0
                },
                {
                    "sent": "But parts of the cortex parts of the cortex which are meant to do one job like vision could be doing another job.",
                    "label": 0
                },
                {
                    "sent": "If, say, you get blind, or I'd say the part of the cortex which would be meant to do music.",
                    "label": 0
                },
                {
                    "sent": "Could suddenly be larger if you're a musician, then if you're not a musician, so that means there are neurons that would, for someone else, have done another job.",
                    "label": 0
                },
                {
                    "sent": "But because you're practicing using a lot, some neurons have been recruited to do that, which means they probably all use the same mechanism.",
                    "label": 0
                },
                {
                    "sent": "So it looks like there is one simple mechanism that's used for learning all kinds of things in cortex.",
                    "label": 0
                },
                {
                    "sent": "So that's that's very encouraging and sort of inspiring for for many of us.",
                    "label": 0
                },
                {
                    "sent": "And then of course there is what we know about cognition and how humans solve problems.",
                    "label": 0
                },
                {
                    "sent": "Which involves.",
                    "label": 0
                },
                {
                    "sent": "Breaking problems into simpler problems and composing the solutions.",
                    "label": 0
                },
                {
                    "sent": "Learning similar things first and then more complicated things on top and so on.",
                    "label": 0
                },
                {
                    "sent": "So we want to have these these features as well.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This illustrates what I was telling you about regarding.",
                    "label": 0
                },
                {
                    "sent": "Higher level features defined in terms of lower level features.",
                    "label": 0
                },
                {
                    "sent": "So what you see in these circles are.",
                    "label": 0
                },
                {
                    "sent": "What different artificial neurons like to see in the input and the first layer they like to detect edges?",
                    "label": 0
                },
                {
                    "sent": "In the second layer they detect corners and contours and higher up they start detecting object parts and so on.",
                    "label": 1
                },
                {
                    "sent": "And eventually objects.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we write programs as one very long main program?",
                    "label": 0
                },
                {
                    "sent": "No, we write.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Programs that have structure and structure means.",
                    "label": 0
                },
                {
                    "sent": "Recursion or reuse of functions, functions calling other functions.",
                    "label": 0
                },
                {
                    "sent": "And that's something we want to have in machine learning as well.",
                    "label": 0
                },
                {
                    "sent": "That's one one thing that deep learning is trying to bring to machine learning in general.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skip that so as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "In 2000 something something happened.",
                    "label": 0
                },
                {
                    "sent": "Though many of us had the intuition for many years that we would like to have deeper neural Nets, we hadn't been really successful in training deep neural Nets with back prop or other methods.",
                    "label": 0
                },
                {
                    "sent": "But something happened which really started in Toronto with Jeff Hinton, and at almost the same time in my lab and yellow cones lab, where we discovered that we could train very deep Nets if we first used.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning to train in each layer one at a time in a kind of greedy way.",
                    "label": 1
                },
                {
                    "sent": "And each layer would be trained in another, provides way to find a better representation on top of the representation of the lower level.",
                    "label": 0
                },
                {
                    "sent": "So the initial work used are BMS restricted Boltzmann machines, which I'm sure Ross talked about this morning.",
                    "label": 0
                },
                {
                    "sent": "Autoencoders, which I will talk about later and sparse coding which I want to talk about and the basic procedure.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We used is this.",
                    "label": 0
                },
                {
                    "sent": "We take the raw data in its initial form, like pixels.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An we learn a set of features and the way we learn those features.",
                    "label": 0
                },
                {
                    "sent": "For example with autoencoders.",
                    "label": 0
                },
                {
                    "sent": "But it's.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Almost the same with RBM's is we have a little neural net, which is fairly easy to train, for example here which tries to reconstruct the inputs with some some.",
                    "label": 0
                },
                {
                    "sent": "Some regularization or something that prevents the neural net from just copying the input to the output.",
                    "label": 0
                },
                {
                    "sent": "And once we've done that, we.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Throw away this this reconstruction layer and use this new representation as input to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Train another autoencoder.",
                    "label": 0
                },
                {
                    "sent": "And we can do it again.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An after awhile we get more and more abstract features.",
                    "label": 0
                },
                {
                    "sent": "Which we can use.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As input to logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And just do classification with that.",
                    "label": 0
                },
                {
                    "sent": "But we can also do what we call fine tuning where we just consider this as initializing very deep feedforward neural net.",
                    "label": 0
                },
                {
                    "sent": "So this was a procedure we worked on in those in those days.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of progress since then.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main progress is that we found ways to train deep supervised neural Nets without even using improvised learning simply by changing the way they are trained the way they are initialized and the architecture.",
                    "label": 0
                },
                {
                    "sent": "So we found that the initialization of the parameters was quite important.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the kind of nonlinearities we would use was quite important.",
                    "label": 0
                },
                {
                    "sent": "In particular, we've been using what's called piecewise linear, non linearity.",
                    "label": 0
                },
                {
                    "sent": "So instead of the traditional sigmoid or hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Things like rectifiers and the Max out which is related have been found to work quite a bit better for deep Nets especially, and we've also improved the ways of regularising.",
                    "label": 0
                },
                {
                    "sent": "So in other words preventing overfitting and helping to generalize with a technique called dropout, which I will also mention later.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised pretraining remains important when you deal with rare classes.",
                    "label": 0
                },
                {
                    "sent": "When your number of labeled examples maybe is not and is not large enough, or if you want to transfer to new domains or new classes or simply as an extra regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to do a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Machine learning 101.",
                    "label": 0
                },
                {
                    "sent": "Your nuts.",
                    "label": 0
                },
                {
                    "sent": "Tutorial.",
                    "label": 0
                },
                {
                    "sent": "To explain some of the tools later.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is to learn.",
                    "label": 0
                },
                {
                    "sent": "A family of functions I call F Theta, where Theta represents tuneable parameters and the goal of learning is going to be changed.",
                    "label": 0
                },
                {
                    "sent": "Those parameters so that we minimize some expected loss on you.",
                    "label": 0
                },
                {
                    "sent": "Examples where these examples are to be coming from a data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to call these examples Ed here and they come from an unknown distribution P of Z.",
                    "label": 0
                },
                {
                    "sent": "And we have a loss function L. That measures how well our function F Theta works.",
                    "label": 0
                },
                {
                    "sent": "For the example zed.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the bottom we see the case the classical case of supervised learning, where Z is has an input part X and an output part Y, and we're trying to predict Y given X and then the loss has two arguments.",
                    "label": 0
                },
                {
                    "sent": "The output of the function applied on X&Y.",
                    "label": 0
                },
                {
                    "sent": "So if F of X is close to why we're happy?",
                    "label": 0
                },
                {
                    "sent": "If not, we're not an.",
                    "label": 0
                },
                {
                    "sent": "We measure our happiness and in this way.",
                    "label": 0
                },
                {
                    "sent": "Typically we will also consider adding to the training criterion in addition to the average loss on the training set, a regularizer.",
                    "label": 0
                },
                {
                    "sent": "The classical regularizer is some function of the parameters, like the smoothness I was talking about can be expressed as a regularizer.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the regularizer depends only on the data on the parameters, and sometimes it also depends on the data itself.",
                    "label": 0
                },
                {
                    "sent": "Now the last ingredient we need to make an actual learning algorithm is a method to approximately minimize this average regularize loss on the data, and this approximate minimization.",
                    "label": 0
                },
                {
                    "sent": "There are many ways of doing it, but I'll tell you about some of the simplest ways which are based on stochastic gradient descent and many match mini batch gradient descent.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the actors, one of the nice things about this framework is that it allows you to.",
                    "label": 0
                },
                {
                    "sent": "To learn to estimate conditional probability and also probabilities in general.",
                    "label": 0
                },
                {
                    "sent": "So in many supervised learning applications you can frame your problem in terms of estimating P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So you want to predict Y given X.",
                    "label": 0
                },
                {
                    "sent": "But Y could be something complicated.",
                    "label": 0
                },
                {
                    "sent": "And instead of.",
                    "label": 0
                },
                {
                    "sent": "Printing is expected value.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to actually estimate its distribution in general.",
                    "label": 0
                },
                {
                    "sent": "So keep in general we have this notion.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "And the way in which we're going to stick our neural Nets in this is that we're going to declare that P of Y given X is parameterized.",
                    "label": 0
                },
                {
                    "sent": "So we can imagine that P of Y, the distribution of Y for a given X is, say, a parametric distribution with parameters Omega.",
                    "label": 0
                },
                {
                    "sent": "But those Omega, instead of being fixed like an unusual parametric distribution, are going to be a function of X, and that function is going to be the output of our neural net.",
                    "label": 0
                },
                {
                    "sent": "So if we're going to be using our neural net to predict, say, probabilities over classes, then those probabilities are the parameters of our same multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "An once we've set things in this way, we can define a natural loss function for these, which is the negative log like queuing.",
                    "label": 0
                },
                {
                    "sent": "So in other words, given our input and given our function F data, we obtain parameters Omega and these parameters assign a probability P of Y to the observed wine and would like.",
                    "label": 0
                },
                {
                    "sent": "That the observed Y given X gets the highest probability as possible, measured with its log and the log is because we want to add probability, add the loss over examples and we're assuming the examples are independent.",
                    "label": 0
                },
                {
                    "sent": "If they weren't, then we can still fit that in by considering the joint an capturing that those dependencies.",
                    "label": 0
                },
                {
                    "sent": "So as an example, 2 examples to classical examples of this is when the Y is the usual real number, which you might imagine represented with a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So that's the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Here isn't the marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "Why it's the conditional distribution and why so it means, given XY has maybe Gaussian distributed, so basically it has a typical value, but there is some air around it.",
                    "label": 0
                },
                {
                    "sent": "And that's represented with just two parameters that mean and the variance, and so you can have that mean you and the variance Sigma B outputs of your neural net.",
                    "label": 0
                },
                {
                    "sent": "Typically we only make the mean very an X an.",
                    "label": 0
                },
                {
                    "sent": "We let Sigma be some parameter which doesn't depend on X, but you can also make Sigma the variance depend on X.",
                    "label": 0
                },
                {
                    "sent": "So there are cases where.",
                    "label": 0
                },
                {
                    "sent": "Only the input tells us about what, why should be, but also what uncertainty there should be about why.",
                    "label": 0
                },
                {
                    "sent": "In that case you can make Sigma the output of your neural net as well.",
                    "label": 0
                },
                {
                    "sent": "So in the in this simple case where Sigma doesn't depend on X, then your log loss just log Sigma plus the usual squared error weighted by Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if Y is a category, then.",
                    "label": 0
                },
                {
                    "sent": "Our network could be outputing the probabilities for each of the classes.",
                    "label": 0
                },
                {
                    "sent": "And we could use something like a softmax which takes a vector of numbers and converts them into a vector of normalized probabilities.",
                    "label": 0
                },
                {
                    "sent": "These would be the Omega eyes.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the distribution over Y given X, and so the then again the negative log likelihood would just be minus log of the.",
                    "label": 0
                },
                {
                    "sent": "Probability estimated by the model.",
                    "label": 0
                },
                {
                    "sent": "Which would be the output for the category wine?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can you can extend that to having not just one output variable, but a bunch, a bunch of variables, and if they are independent, it's just like having a neural net with multiple groups of outputs where each group corresponds to one of their random variables.",
                    "label": 0
                },
                {
                    "sent": "If those variables are not independent, then you get into a very interesting realm of structured outputs.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're predicting.",
                    "label": 0
                },
                {
                    "sent": "Pixels in an image.",
                    "label": 0
                },
                {
                    "sent": "For example, you're trying to label the pixels or you're trying to predict the complicated data structure, or you're trying to predict a sentence like in machine translation.",
                    "label": 0
                },
                {
                    "sent": "Then the different things you want to predict are not independent of each other.",
                    "label": 0
                },
                {
                    "sent": "Given the input, they have a complicated joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And capturing that joint distribution is often intractable, but we'll see that.",
                    "label": 0
                },
                {
                    "sent": "There are some interesting things that can be done there.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so in all of these cases we have either log blanket or some proxy or some other loss function and we'd like to minimize.",
                    "label": 0
                },
                {
                    "sent": "And we have to think about how to do that, and we would typically do that by iterative optimization, where we start from some configuration of the parameters and we gradually change them.",
                    "label": 0
                },
                {
                    "sent": "In order to hopefully reach a minimum, and as you know, you can have local minima.",
                    "label": 0
                },
                {
                    "sent": "An if we're going to end up in the local minimum, we'd like to have one that is hopefully close to a global minimum in terms of error.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The simplest way that you could go down the error is used by iterative changes.",
                    "label": 0
                },
                {
                    "sent": "Use gradient based optimization where we have a loss, but we can also suppose that we can compute the derivative of our loss and the derivative.",
                    "label": 0
                },
                {
                    "sent": "The derivative tells us.",
                    "label": 0
                },
                {
                    "sent": "In what direction?",
                    "label": 0
                },
                {
                    "sent": "I could make an infinitesimal change in my parameters.",
                    "label": 0
                },
                {
                    "sent": "So as to make the loss increase or decrease.",
                    "label": 0
                },
                {
                    "sent": "So if we move our parameters in that direction by some epsilon.",
                    "label": 0
                },
                {
                    "sent": "Then we hope to make our loss change and improve.",
                    "label": 0
                },
                {
                    "sent": "And of course there's vast literature on that question.",
                    "label": 0
                },
                {
                    "sent": "But the algorithms that we use in deep learning for this are fairly simple and the simplest of them is called.",
                    "label": 0
                },
                {
                    "sent": "Mini batch stochastic gradient descent?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not the simplest.",
                    "label": 0
                },
                {
                    "sent": "The simplest stochastic gradient descent where we just take one example at a time.",
                    "label": 0
                },
                {
                    "sent": "We measure its loss.",
                    "label": 0
                },
                {
                    "sent": "We measure the gradient with respect to parameters and we do one step of gradient in that direction so it's called stochastic because we the real loss we care about is the average over the whole training set, and we're only taking one example or a group of example, which we call a mini batch in order to decide where to move.",
                    "label": 0
                },
                {
                    "sent": "And so this is a kind of noisy estimate of the true gradient.",
                    "label": 0
                },
                {
                    "sent": "But actually it has all kinds of nice properties, and more importantly, it converges very, very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Neural Nets.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with logistic regression can be understood very easily by thinking of.",
                    "label": 0
                },
                {
                    "sent": "A bunch of logistic regressions that you could be running at the same time if somebody told you a good representation for your problem.",
                    "label": 0
                },
                {
                    "sent": "In other words, that kind of sub problem you'd like to solve.",
                    "label": 0
                },
                {
                    "sent": "And you would know what the target values for these features would be given the inputs.",
                    "label": 0
                },
                {
                    "sent": "Then you could just train a bunch of logistic regression to predict these.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you could take these as input to your actual problem of interest.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing with units is that you don't have to tell the neural net with these intermediate values should be these intermediate features.",
                    "label": 0
                },
                {
                    "sent": "It is going to look for them, of course, that makes the optimization problem harder, and that's the reason why people have preferred kernel methods for about more than a decade, because the optimization problem is now becomes non convex.",
                    "label": 0
                },
                {
                    "sent": "But what we've got here is essentially we've learned the kernel.",
                    "label": 0
                },
                {
                    "sent": "We've learned the feature space.",
                    "label": 0
                },
                {
                    "sent": "Um, we were looking at it more difficult optimization problem but.",
                    "label": 0
                },
                {
                    "sent": "But it it is, will see it has both computational and statistical advantages.",
                    "label": 0
                },
                {
                    "sent": "And of course, once you understand this process, you can do it.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiple times to have deeper neural Nets, so one of the.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ingredients in many of the techniques we use these days is backdrop, and that is that word is used in often the wrong sense in the sense that I would, I and others like Jeff Hinton would like people to retain, is back propagation algorithm to compute gradients.",
                    "label": 0
                },
                {
                    "sent": "Now you can use those gradients as part of an optimization method like gradient descent, or say Hashem isation or conjugate gradients or albc.",
                    "label": 0
                },
                {
                    "sent": "But but the way you compute the gradient is what backup is about.",
                    "label": 0
                },
                {
                    "sent": "And what it relies on is called the chain rule.",
                    "label": 0
                },
                {
                    "sent": "And it's something that makes that's really important when as soon as you start thinking about composing functions.",
                    "label": 0
                },
                {
                    "sent": "So the simplest case of composition of functions is.",
                    "label": 0
                },
                {
                    "sent": "Here we have X, which is transformed into Y by a function.",
                    "label": 0
                },
                {
                    "sent": "G&Y gets transformed into Zed by function F, so zed is F of G of X.",
                    "label": 0
                },
                {
                    "sent": "So if we want to know how to change.",
                    "label": 0
                },
                {
                    "sent": "X So as to change said in a particular way, we need the ZX and the chain Rule tells us this is DZ y * D YDX.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See why this is true?",
                    "label": 0
                },
                {
                    "sent": "Think about this so we can.",
                    "label": 0
                },
                {
                    "sent": "We can use this notation of flow graphs where each variable is a little circle.",
                    "label": 0
                },
                {
                    "sent": "An represents a value.",
                    "label": 0
                },
                {
                    "sent": "And the arrows tell us.",
                    "label": 0
                },
                {
                    "sent": "What values computed based on what values?",
                    "label": 0
                },
                {
                    "sent": "So we have partial derivatives now that we can attach attach to these these arcs.",
                    "label": 0
                },
                {
                    "sent": "So DZDY tells us how a small change in Y makes a small change in Zen and similarly for dyd X.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we know that a small change Delta Y.",
                    "label": 0
                },
                {
                    "sent": "When we multiply by diesel, DY tells us how zed oil change so it gives us a Delta zetten.",
                    "label": 0
                },
                {
                    "sent": "We know this similar thing that Delta Y Delta X * D YDX gives us Delta Y.",
                    "label": 0
                },
                {
                    "sent": "We just plug one of these in the other an we get that change in X. Delta X gets transformed into a change Delta set by multiplying Delta XY and ZYDYDX, right?",
                    "label": 0
                },
                {
                    "sent": "So that's just very very simple.",
                    "label": 0
                },
                {
                    "sent": "Algebra.",
                    "label": 0
                },
                {
                    "sent": "And we get the chain rule.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you had two paths to go from X to Z, you can make a similar argument for each of those paths, and you end up finding that the partial derivative is just a sum.",
                    "label": 0
                },
                {
                    "sent": "Of the product of the partial derivatives along each of those paths.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and of course, if you have N paths, it's just the sum of the partial derivatives across all of these paths.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now you can make a general rule which tells you that if you have a graph of computation, which I call a flow graph where each node is one of these values.",
                    "label": 0
                },
                {
                    "sent": "And you want to know the ZDX where Zen is the final nodes scaler.",
                    "label": 0
                },
                {
                    "sent": "And X is any intermediate node in the middle.",
                    "label": 0
                },
                {
                    "sent": "Then you can obtain it by looking at D. Zeddy Wiiware, why I is one is the one of the immediate descendants of X.",
                    "label": 0
                },
                {
                    "sent": "So you look at all the descendants of X.",
                    "label": 0
                },
                {
                    "sent": "We call them why I or the successors of X.",
                    "label": 0
                },
                {
                    "sent": "And assuming you have already computed this ladies at the Why are you can compute the ZX by taking those?",
                    "label": 0
                },
                {
                    "sent": "These are the wise and multiplying them by the partial derivatives relating each of those wise 2X the dyd X, and that's the most general form of the chain rule which which we take advantage of in back problem.",
                    "label": 0
                },
                {
                    "sent": "You just plug it in your code actually.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Design code around that and you have a very general purpose way of.",
                    "label": 0
                },
                {
                    "sent": "Doing by proper in any kind of system.",
                    "label": 0
                },
                {
                    "sent": "So in the regular neural net.",
                    "label": 0
                },
                {
                    "sent": "What it means is you think of the neural net as this graph, where now the nodes include both your inputs, your hidden units, but also your parameters as well as your label annual loss.",
                    "label": 0
                },
                {
                    "sent": "And for each of those nodes, there is a well defined computation as well as partial derivatives that you can compute.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can compute the partial derivative of the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Here the final loss with respect to the output of the neural net.",
                    "label": 0
                },
                {
                    "sent": "And then you can propagate that into.",
                    "label": 0
                },
                {
                    "sent": "A derivative with respect to the parameters of the first layer W and with respect to the hidden unit.",
                    "label": 0
                },
                {
                    "sent": "Outputs H. And so on.",
                    "label": 0
                },
                {
                    "sent": "To get the gradients on the other set of parameters V. All.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and so that gives you a kind of general formula general algorithm for computing gradients in a flow graph.",
                    "label": 0
                },
                {
                    "sent": "You have two parts to this.",
                    "label": 0
                },
                {
                    "sent": "The F drop in the backdrop.",
                    "label": 0
                },
                {
                    "sent": "The F prop visits the nodes in some order, which we call it topological order, so it creates a directed.",
                    "label": 0
                },
                {
                    "sent": "Acyclic graph that goes from inputs to the final node and the final node in our problem is going to be a loss of scalar that we would like to minimize an average and then in the second phase the backdrop phase we are going to follow those those arcs backwards.",
                    "label": 0
                },
                {
                    "sent": "And at each node we're going to compute the derivative of the final loss, said with respect to the values computed in the forward pass.",
                    "label": 0
                },
                {
                    "sent": "So for the final node, these addys at is 1.",
                    "label": 0
                },
                {
                    "sent": "But then we're going to compute the ZX for every internal node, and for each of those nodes, we're going to be using.",
                    "label": 0
                },
                {
                    "sent": "Oh does that mean it's time for a break?",
                    "label": 0
                },
                {
                    "sent": "We're going to be using the chain rule.",
                    "label": 0
                },
                {
                    "sent": "As I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I guess maybe now is a good time for the break anyways.",
                    "label": 0
                }
            ]
        }
    }
}