{
    "id": "2xns7asydvc64riexinehwzkytwxq343",
    "title": "Learning Structured Outputs via Kernel Dependency Estimation and Stochastic Grammars",
    "info": {
        "author": [
            "Andrea Passerini, University of Firenze"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/oh06_passerini_lsovk/",
    "segmentation": [
        [
            "Dealing with structured outputs with structures which are undersea estimation and stochastic grammars."
        ],
        [
            "Yeah, so I will just.",
            "Give the idea in one slide because I think it's very simple, so we talk about sector output prediction and what we assume is that output structures are generated by a stochastic grammar.",
            "OK, but The thing is that the probabilities of this grammar are example dependent.",
            "OK, so you cannot use, let's say, an estimate of these probabilities on the entire data set in order to be able to parse.",
            "It's in the sequence because it won't work.",
            "That would mean soon that you have something like context sensitiveness in the grammar.",
            "And so the idea is that we would rally on the not proficient space, which is simply the space of all production rules of grammar, and we actually map the structure, that is, the parts of the sequence into the frequencies of single production rules.",
            "And once we have done that and we can do this quite easily with this vector output.",
            "In picture Matt estimation methods like.",
            "I'm doing the framework of canal density dependency estimation.",
            "Then you can do compute the preimage simply by inside.",
            "Outside procedure might be terribly procedure using the fact that you have example.",
            "You have estimated example dependent probabilities to attach to the grammar OK.",
            "So."
        ],
        [
            "I'm just playing it a little bit better.",
            "So like the framework is that of kind of dependency estimation, which is I think I would say, so it'll be the alternative to the idea of having like.",
            "Junk fishing space for input and output.",
            "In this case you have OK and input mapping with associated kernel.",
            "An output mapping from Y of tool FY with the associated output kernel and the process of structured output prediction goes into steps.",
            "The first one estimates the output features from the inputs with the function function G. And the second one is to compute the pre image from the output feature space to the output.",
            "And of course this one is not invertible usually so you have to estimated.",
            "So the fish."
        ],
        [
            "And the first step is quite simple.",
            "You have this examples XI side of why I and you have to estimate this this function from XI to Y and what we can do.",
            "What we did was simply to apply Kernel Ridge regression OK, but on the vector output, which is simply very simple extension of the individual session for this color output.",
            "Where here this one is the metrics of output features.",
            "Each column is a is the output feature mapping of a single Y in the training set, and this one is a Colonel in the input to plus organization, and this one is the metrics of the weights.",
            "That means that in the solution G of X is simply the sum of the support vectors of these columns of parameters.",
            "Times the kernel on the input and of course efficient additives exist.",
            "I would I would have talked about the maximum margin regression, but Santos made that you have to talk about myself much robots.",
            "But I mean I think it's an interesting idea.",
            "So."
        ],
        [
            "For the second step, which is the preimage calculation.",
            "It was proposed to typically was that of trying to find the way which minimized sort of distance between what you predicted and the image of the Y right?",
            "And it was shown that if you have G of X estimated using kernel integration, this problem doesn't force you to have explicit out feature mapping, but you can use the kernel.",
            "And so the problem is still to find the dark mean of this dysfunction.",
            "And what Karina Selena reason to ice email paper did was to use a graph theoretical algorithm when you had outputs which were swings, and you use the playground output characters on the output.",
            "And our idea was simply to use the customers that is.",
            "You have the stochastic grammar with.",
            "I would say with the probabilities on rolls."
        ],
        [
            "Which depends on the examples and of course you don't know them on contest examples.",
            "The output feature mapping will be a real vector which encodes such probability for a single example.",
            "And once you have estimated this album official mapping with the multiple regression, you can use the probabilistic parser in order to do that pretty much step that is found in most valuable parts.",
            "Given this grammar and estimated probabilities.",
            "OK. And to be more concrete, I would talk about."
        ],
        [
            "Acoustic context free grammars.",
            "Because we have parsers.",
            "And So what we do here, we have production rules on this form from a Cato Alpha L. OK, well finalist, union of Nonterminals, terminals and now and what you do is that probabilities associated to the rules of course have to sum to 1 four rules which share the same hand.",
            "OK, and what we do is use as Fisher Vector we.",
            "Musician Vector, which is related to these probabilities by the softmax and then we estimate this one.",
            "That means simply that here is something like we do a generalized linear model that allows us to have outputs which don't have to be like to sum to one and to be all in 01 course.",
            "So you don't have to put constraints on the output vector.",
            "And that's it.",
            "I mean, once you you have obtained this, you apply stochastic context free grammar parsers to compute the most probable parts given this probabilities.",
            "So why all this stuff could be interesting.",
            "What do we have to prove is that this can of course work better than a simple parser OK standard parser which has robot probabilities.",
            "And then how we try to show it with an artificial example?",
            "We tried to simulate the PP attachment ambiguity resolution problem which is classical in NLP and which easily explained with this example.",
            "If the sum of the way the fork with a fork is, let's say, characteristic of eating OK, so that means the preposition phrases directly attached to the verb while it is solid with tomatoes with tomatoes is attached to is a property of the salad and that means the prepositional phrase attached the noun phrase.",
            "And you cannot like discriminate between these."
        ],
        [
            "To simply with this debate, grammar OK, because there is ambiguity and so you need the lexicalization in order to resolve this ambiguity, OK?",
            "So."
        ],
        [
            "Wish we just very simple grammar.",
            "OK here.",
            "Alphabetic letters are nonterminals, then lowercase, R pre terminals and then digits are terminals.",
            "And what is I mean probabilities here are all uniform except for this one which is the sentence which goes with probability 0.2 in a conjunction conjunction of sentences or in the.",
            "0.8 in noun phrase and verb phrase.",
            "But what is interesting in this grammar is simply this rule OK, which says that phrase can go in in this case.",
            "OK, you have the first example it this summer with a fork.",
            "OK, so noun phrase and prepositional phrase are attached to.",
            "Like children of the same phrase or the verb phrase in the second case you are, you have the salad.",
            "The salad with tomatoes, OK?",
            "So we complicated things.",
            "Once we generated data set according to this grammar and this probabilities, we complicated things by collapsing 20 it terminals for the verbs BMW in a single one OK. And this way the standard parser, which had probabilities estimated on their time data set OK, couldn't really disambiguate within these two cases, right?",
            "And that's actually what what happens because?"
        ],
        [
            "OK, and how we generated the data set with this simple language generator program?",
            "OK, we randomly generated bunch of sentences.",
            "We post process them in two ways International, one with simply duplicate input sentences and the unique one.",
            "We made it more difficult because we filtered out sentences which had identical representation on the artificial space FY.",
            "And what happens?",
            "Is that OK, I run into the results, but I have to explain how it will."
        ],
        [
            "So we compared our solution which we called with this ugly name, Katie CFG with the standard parts are we use the spectrum kernel on the input on there, which is the sequence of terminals with K members of sites, two to five and we will be program in order to compute the bracketing F measure and the exact parts matches core.",
            "So the first Martin is more loose, the second one is really to get all the parts 3 correct.",
            "We randomly paid the data set in two sets and we use the first one for model selection during the five fold cross validation and the second one for performance evaluation with another 5 four cross validation.",
            "We get is that.",
            "In OK."
        ],
        [
            "This we we compared the results for the entire data set and for the short sequences only, so less than 35 terminals OK. And you know cases you have that the architecture is much better than the simpler parser, and these differences are really.",
            "Significant much more than 99.5% and that simply shows that you catch something like a little bit of context sensitiveness.",
            "You catch it on an example level, not on a like single production rule level on a single, let's say local level, but this surface is to be better than using global probabilities only, and so this is I think it's very simple idea and it's simply using.",
            "Known.",
            "I know you said it already.",
            "What was the difference with the unique?",
            "Why was unique so much right?",
            "The difference is that in the.",
            "A unique case you?",
            "We assured that in the data set you didn't have two sentences which were mapped to the same output feature from one?",
            "Yeah, yeah.",
            "So yeah, performances are much really has to generalize it.",
            "Yes?"
        ],
        [
            "So we I mean we are thinking of applying it more generally to any problems because we would like to see how how well it it's it works or real world data and further extensions we are thinking of is to use probabilistic IRP instead of stochastic country grammars and so.",
            "Programs which, let's say you learn the probabilities with each program, should explain target concept.",
            "And that's it.",
            "Cancel it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dealing with structured outputs with structures which are undersea estimation and stochastic grammars.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so I will just.",
                    "label": 0
                },
                {
                    "sent": "Give the idea in one slide because I think it's very simple, so we talk about sector output prediction and what we assume is that output structures are generated by a stochastic grammar.",
                    "label": 1
                },
                {
                    "sent": "OK, but The thing is that the probabilities of this grammar are example dependent.",
                    "label": 0
                },
                {
                    "sent": "OK, so you cannot use, let's say, an estimate of these probabilities on the entire data set in order to be able to parse.",
                    "label": 0
                },
                {
                    "sent": "It's in the sequence because it won't work.",
                    "label": 0
                },
                {
                    "sent": "That would mean soon that you have something like context sensitiveness in the grammar.",
                    "label": 1
                },
                {
                    "sent": "And so the idea is that we would rally on the not proficient space, which is simply the space of all production rules of grammar, and we actually map the structure, that is, the parts of the sequence into the frequencies of single production rules.",
                    "label": 0
                },
                {
                    "sent": "And once we have done that and we can do this quite easily with this vector output.",
                    "label": 0
                },
                {
                    "sent": "In picture Matt estimation methods like.",
                    "label": 0
                },
                {
                    "sent": "I'm doing the framework of canal density dependency estimation.",
                    "label": 0
                },
                {
                    "sent": "Then you can do compute the preimage simply by inside.",
                    "label": 0
                },
                {
                    "sent": "Outside procedure might be terribly procedure using the fact that you have example.",
                    "label": 0
                },
                {
                    "sent": "You have estimated example dependent probabilities to attach to the grammar OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm just playing it a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So like the framework is that of kind of dependency estimation, which is I think I would say, so it'll be the alternative to the idea of having like.",
                    "label": 0
                },
                {
                    "sent": "Junk fishing space for input and output.",
                    "label": 0
                },
                {
                    "sent": "In this case you have OK and input mapping with associated kernel.",
                    "label": 1
                },
                {
                    "sent": "An output mapping from Y of tool FY with the associated output kernel and the process of structured output prediction goes into steps.",
                    "label": 1
                },
                {
                    "sent": "The first one estimates the output features from the inputs with the function function G. And the second one is to compute the pre image from the output feature space to the output.",
                    "label": 0
                },
                {
                    "sent": "And of course this one is not invertible usually so you have to estimated.",
                    "label": 0
                },
                {
                    "sent": "So the fish.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the first step is quite simple.",
                    "label": 0
                },
                {
                    "sent": "You have this examples XI side of why I and you have to estimate this this function from XI to Y and what we can do.",
                    "label": 0
                },
                {
                    "sent": "What we did was simply to apply Kernel Ridge regression OK, but on the vector output, which is simply very simple extension of the individual session for this color output.",
                    "label": 1
                },
                {
                    "sent": "Where here this one is the metrics of output features.",
                    "label": 0
                },
                {
                    "sent": "Each column is a is the output feature mapping of a single Y in the training set, and this one is a Colonel in the input to plus organization, and this one is the metrics of the weights.",
                    "label": 0
                },
                {
                    "sent": "That means that in the solution G of X is simply the sum of the support vectors of these columns of parameters.",
                    "label": 0
                },
                {
                    "sent": "Times the kernel on the input and of course efficient additives exist.",
                    "label": 0
                },
                {
                    "sent": "I would I would have talked about the maximum margin regression, but Santos made that you have to talk about myself much robots.",
                    "label": 0
                },
                {
                    "sent": "But I mean I think it's an interesting idea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the second step, which is the preimage calculation.",
                    "label": 0
                },
                {
                    "sent": "It was proposed to typically was that of trying to find the way which minimized sort of distance between what you predicted and the image of the Y right?",
                    "label": 0
                },
                {
                    "sent": "And it was shown that if you have G of X estimated using kernel integration, this problem doesn't force you to have explicit out feature mapping, but you can use the kernel.",
                    "label": 1
                },
                {
                    "sent": "And so the problem is still to find the dark mean of this dysfunction.",
                    "label": 0
                },
                {
                    "sent": "And what Karina Selena reason to ice email paper did was to use a graph theoretical algorithm when you had outputs which were swings, and you use the playground output characters on the output.",
                    "label": 1
                },
                {
                    "sent": "And our idea was simply to use the customers that is.",
                    "label": 0
                },
                {
                    "sent": "You have the stochastic grammar with.",
                    "label": 0
                },
                {
                    "sent": "I would say with the probabilities on rolls.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which depends on the examples and of course you don't know them on contest examples.",
                    "label": 0
                },
                {
                    "sent": "The output feature mapping will be a real vector which encodes such probability for a single example.",
                    "label": 1
                },
                {
                    "sent": "And once you have estimated this album official mapping with the multiple regression, you can use the probabilistic parser in order to do that pretty much step that is found in most valuable parts.",
                    "label": 0
                },
                {
                    "sent": "Given this grammar and estimated probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK. And to be more concrete, I would talk about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acoustic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "Because we have parsers.",
                    "label": 0
                },
                {
                    "sent": "And So what we do here, we have production rules on this form from a Cato Alpha L. OK, well finalist, union of Nonterminals, terminals and now and what you do is that probabilities associated to the rules of course have to sum to 1 four rules which share the same hand.",
                    "label": 0
                },
                {
                    "sent": "OK, and what we do is use as Fisher Vector we.",
                    "label": 0
                },
                {
                    "sent": "Musician Vector, which is related to these probabilities by the softmax and then we estimate this one.",
                    "label": 1
                },
                {
                    "sent": "That means simply that here is something like we do a generalized linear model that allows us to have outputs which don't have to be like to sum to one and to be all in 01 course.",
                    "label": 0
                },
                {
                    "sent": "So you don't have to put constraints on the output vector.",
                    "label": 1
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "I mean, once you you have obtained this, you apply stochastic context free grammar parsers to compute the most probable parts given this probabilities.",
                    "label": 0
                },
                {
                    "sent": "So why all this stuff could be interesting.",
                    "label": 0
                },
                {
                    "sent": "What do we have to prove is that this can of course work better than a simple parser OK standard parser which has robot probabilities.",
                    "label": 0
                },
                {
                    "sent": "And then how we try to show it with an artificial example?",
                    "label": 0
                },
                {
                    "sent": "We tried to simulate the PP attachment ambiguity resolution problem which is classical in NLP and which easily explained with this example.",
                    "label": 0
                },
                {
                    "sent": "If the sum of the way the fork with a fork is, let's say, characteristic of eating OK, so that means the preposition phrases directly attached to the verb while it is solid with tomatoes with tomatoes is attached to is a property of the salad and that means the prepositional phrase attached the noun phrase.",
                    "label": 0
                },
                {
                    "sent": "And you cannot like discriminate between these.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To simply with this debate, grammar OK, because there is ambiguity and so you need the lexicalization in order to resolve this ambiguity, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wish we just very simple grammar.",
                    "label": 0
                },
                {
                    "sent": "OK here.",
                    "label": 0
                },
                {
                    "sent": "Alphabetic letters are nonterminals, then lowercase, R pre terminals and then digits are terminals.",
                    "label": 0
                },
                {
                    "sent": "And what is I mean probabilities here are all uniform except for this one which is the sentence which goes with probability 0.2 in a conjunction conjunction of sentences or in the.",
                    "label": 1
                },
                {
                    "sent": "0.8 in noun phrase and verb phrase.",
                    "label": 0
                },
                {
                    "sent": "But what is interesting in this grammar is simply this rule OK, which says that phrase can go in in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, you have the first example it this summer with a fork.",
                    "label": 0
                },
                {
                    "sent": "OK, so noun phrase and prepositional phrase are attached to.",
                    "label": 0
                },
                {
                    "sent": "Like children of the same phrase or the verb phrase in the second case you are, you have the salad.",
                    "label": 0
                },
                {
                    "sent": "The salad with tomatoes, OK?",
                    "label": 0
                },
                {
                    "sent": "So we complicated things.",
                    "label": 1
                },
                {
                    "sent": "Once we generated data set according to this grammar and this probabilities, we complicated things by collapsing 20 it terminals for the verbs BMW in a single one OK. And this way the standard parser, which had probabilities estimated on their time data set OK, couldn't really disambiguate within these two cases, right?",
                    "label": 0
                },
                {
                    "sent": "And that's actually what what happens because?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and how we generated the data set with this simple language generator program?",
                    "label": 1
                },
                {
                    "sent": "OK, we randomly generated bunch of sentences.",
                    "label": 0
                },
                {
                    "sent": "We post process them in two ways International, one with simply duplicate input sentences and the unique one.",
                    "label": 1
                },
                {
                    "sent": "We made it more difficult because we filtered out sentences which had identical representation on the artificial space FY.",
                    "label": 0
                },
                {
                    "sent": "And what happens?",
                    "label": 0
                },
                {
                    "sent": "Is that OK, I run into the results, but I have to explain how it will.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compared our solution which we called with this ugly name, Katie CFG with the standard parts are we use the spectrum kernel on the input on there, which is the sequence of terminals with K members of sites, two to five and we will be program in order to compute the bracketing F measure and the exact parts matches core.",
                    "label": 1
                },
                {
                    "sent": "So the first Martin is more loose, the second one is really to get all the parts 3 correct.",
                    "label": 0
                },
                {
                    "sent": "We randomly paid the data set in two sets and we use the first one for model selection during the five fold cross validation and the second one for performance evaluation with another 5 four cross validation.",
                    "label": 1
                },
                {
                    "sent": "We get is that.",
                    "label": 0
                },
                {
                    "sent": "In OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This we we compared the results for the entire data set and for the short sequences only, so less than 35 terminals OK. And you know cases you have that the architecture is much better than the simpler parser, and these differences are really.",
                    "label": 1
                },
                {
                    "sent": "Significant much more than 99.5% and that simply shows that you catch something like a little bit of context sensitiveness.",
                    "label": 0
                },
                {
                    "sent": "You catch it on an example level, not on a like single production rule level on a single, let's say local level, but this surface is to be better than using global probabilities only, and so this is I think it's very simple idea and it's simply using.",
                    "label": 0
                },
                {
                    "sent": "Known.",
                    "label": 0
                },
                {
                    "sent": "I know you said it already.",
                    "label": 0
                },
                {
                    "sent": "What was the difference with the unique?",
                    "label": 0
                },
                {
                    "sent": "Why was unique so much right?",
                    "label": 0
                },
                {
                    "sent": "The difference is that in the.",
                    "label": 0
                },
                {
                    "sent": "A unique case you?",
                    "label": 0
                },
                {
                    "sent": "We assured that in the data set you didn't have two sentences which were mapped to the same output feature from one?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, performances are much really has to generalize it.",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we I mean we are thinking of applying it more generally to any problems because we would like to see how how well it it's it works or real world data and further extensions we are thinking of is to use probabilistic IRP instead of stochastic country grammars and so.",
                    "label": 1
                },
                {
                    "sent": "Programs which, let's say you learn the probabilities with each program, should explain target concept.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Cancel it.",
                    "label": 0
                }
            ]
        }
    }
}