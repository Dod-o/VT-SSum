{
    "id": "k4zi5px4rfuml5ogm34quepgw7fqcxpv",
    "title": "Unsupervised Prediction of Citation Influences",
    "info": {
        "author": [
            "Laura Dietz, Department RG2: Machine Learning, Max Planck Institute for Computer Science, Max Planck Institute"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Network Analysis",
            "Top->Computer Science->Data Visualisation"
        ]
    },
    "url": "http://videolectures.net/icml07_dietz_upc/",
    "segmentation": [
        [
            "Supervised prediction of citation influences by Laura Dietz, Steven Bickell and OVS Shepherd.",
            "Good morning, well thank you.",
            "My name is Laura Dietz and today I present joint work with Stephen Pickle and to be a chef for about the application of topic models, probabilistic topic models through predicting citation influences."
        ],
        [
            "Well, I started with a motivation of the problem setting about, which is about visualizing citation graphs and especially creating meaningful pictures out of it, where one essential part part is to predict the links of citations, and so this is basically the rest of the talk and for the citation prediction I present two generative models, the copycat model, and the citation influence model, and I will.",
            "Do some experiments on the predictive performance and innovative evaluation."
        ],
        [
            "So when I was fresh PhD student reading onto machine learning, I went to my supervisor and he gave me a couple of seat publications to read on.",
            "For example this one.",
            "So after reading this, I went ahead.",
            "Well where should I go next to and what's going on around in this community?",
            "So I started by adding but looking at the citation of affinity, looking at the Brookside was cited an thanks to Google looking at publications that cited this work and looked at some more publications and then I already was left with too many publications, which I thought well I can never read all of them within reasonable time.",
            "So, um.",
            "One thing that I found is when you just randomly pick any of those nodes and you redeem them, you think, well it was related and the answer is probably no man.",
            "The reason for this is there are different reasons for citing publications, so the first one, and this is basically the one I was interested in.",
            "That time, is that the approach of fighting work was extended by the citing paper.",
            "You just get something like see value in evolution of scientific topic.",
            "But then there are also references to baselines and to other papers at work that was tackling the same problem, and there was already at least some influence for them or their citations pointing to basic literature for background reading.",
            "And this was already not the literature I was interested in that time.",
            "And Furthermore, there's related work that is just named for the purpose that in the next sentence you say why this is actually not helping you according to this problem statement.",
            "So there will be basically no very weak influence here and least last of all there is there always citations that are done just because everybody sites is.",
            "And there might be definitely OK in some cases there might be no influence on this one, so we thought it might be nice to have a measure of which of these edges actually influential, so it's easy to say we simply infers citation strings.",
            "So and this is actually not so easy to do and the remainder of the talk is actually about this one, and once we infer the citation strings, we can filter the edges by simple thresholding.",
            "And then get rid of isolated nodes and now the graph has shrinked a lot and."
        ],
        [
            "You know we layout this, we end up with something like this which is definitely more meaningful and you.",
            "Yeah, that's a more concise."
        ],
        [
            "So to formulate the problem statement, we are given a citation graph and text for the publications.",
            "This might be full text, but in this case I did some experiment with only an abstract.",
            "And now we want to predict how strong is the influence of a cited publication on citing publication.",
            "And if you're a PhD students, then it's definitely right that you do not have any strength levels.",
            "So here we are working with unsupervised methods.",
            "The idea is that we are proposing latently directly allocation like generative model and in contrast to latent directly allocation where the publications are seen as independent of one another.",
            "We want to capture the interactions between the publications an so basic idea is that we associate words of the abstract of the sighting publication to any of those sites.",
            "And the intuition is that the more words are associated, one of the sites are stronger.",
            "The influences here."
        ],
        [
            "So this is the generative process of the first of our two models, the copycat model and the plate diagram you see here is one of latent directly allocation, and it works as follows.",
            "So for each of the publications here to the left.",
            "It first draws a topic mixture for each of them, which is multinomial over topics drawn from.",
            "Additionally, and for each topic it was a typical characteristic word distribution for each of those.",
            "And then for all documents and for words inside these documents, we associate one topic to each of those words out of this topic mixture.",
            "So this is a standard and what we did.",
            "We extended this model by another plate for the citing publications.",
            "So in order to not confuse you, the errors are point.",
            "Here are not cited site links, but our potential influence influence links, so cited publication might influence citing publication here.",
            "So the process is as follows.",
            "First, we draw a mixture over citations.",
            "Indicating the strength of influence here and, for example, the third publication here has the strongest influence.",
            "Then for all the words be first associate word to one of the sites.",
            "For example, the first one to the first publication, and then for each word we draw the topic out of the corresponding topic mixture of the cited document.",
            "So we see in in the copycat model it's assumed that society publication something just a copy of different sites it has, so it's really a strong assumption we're making here.",
            "So and all all topics of all words are definitely inherited."
        ],
        [
            "So what I just told you is the topic Michelle cited.",
            "Publication is actually a shared topic mixture because it's influenced by the words of the site of society, publication as well as all the words that are associated with.",
            "So for example, we have another publication where all the words associated to publication #3.",
            "Then all those words inside publication #3 as well as those that are associated to publication #3 during the process influence the topic mixture.",
            "So this is actually a shared topic mixture, so the nice thing here is that we are able to capture, let's say the evolution of the vocabulary.",
            "For example, if in the original work the terminology was not as clear as in the follow up work, then.",
            "We can capture this an the influence process is actually also influenced by the new term which is not occurring in."
        ],
        [
            "In the original book.",
            "So we see in the copycat model we have a very tight coupling between citing and cited publications, as well as bibliographically coupled publications.",
            "However, there's actually 1 issue that is because over all words in the citing publication have to be associated to any of those sites.",
            "We may run into trouble if society publication contains words which do not fit to any of the sites.",
            "Then this will introduce noise in the shared topic mixture, so noise itself is not really an issue in the probabilistic models, it's simply handled in the natural way, but because of those accidentally associated words.",
            "Influence the Association process of other citing publications will be influenced by this, so we get sometimes very awkward prediction that do not fit the human intuition so.",
            "We saw like hey, well there might be a fixed and our fixes that we let some model decide to not decide some words.",
            "So just as something like an emergency exit."
        ],
        [
            "And this is what's part of the citation influence model which comes here.",
            "So this is the copycat model, and we add a bit more of complexity.",
            "And yeah, so the idea is that for each word in the sighting publication, we first flip a coin and the coin says whether the word is inherited, as in the copycat model, or whether it's not associated to any of those sites.",
            "So here is that the three words that are chosen to be not associated.",
            "So we also termed this innovation because all innovative words are worth talking about innovation and publication.",
            "But we probably not assigned to any of those sites.",
            "And then for the remaining words, we perform basically the same process as in the copycat model by choosing publications, cited publications and drawing words from those topic mixtures and for the innovative words, we draw an utopic mixture only capturing the innovation aspect and draw those words from this topic mixture.",
            "So to sum up.",
            "For each word in the site of inciting publication, you flip an unfair coin.",
            "And here, since we're basically only interested in inheritance process, the coin is very unfair in the sense that it prefers inheritance, and innovation is only the emergency exit if the coin sets inherit this word, then we draw this word from society.",
            "As in the copycat model, and if the words initial coin says in innovative, this word then withdraws from this own topic mixer special topic mixture."
        ],
        [
            "So from these generative models we can infer Gibbs samplers by writing up the joint distribution.",
            "The one you see here is 1 for the citation influence model an, since we're just using conjugate priors, we can do well black with black realization by integrating over other multinomial parameters.",
            "Overall, the multinomial's and yeah, so this is what we did for the copy cat and the citation influence model.",
            "And Furthermore we did some.",
            "Convergence monitoring based on the method of Brooks and Gilman, and here we are using the citation influence distribution as a scalar summary basis for the scalar summaries."
        ],
        [
            "Let's move on to the experiment section.",
            "In order to evaluate the predictive performance, we did a little survey.",
            "So we are some of our research friends to label some of publication, say road and labeling.",
            "Here means that we told them something about the data we had of their publications, namely the abstract and all the other citations we found on the web, especially here in society are 2004 data set.",
            "And so the researchers were asked to state for each of the sites how strong the influence was on their work.",
            "And here the range may be useful.",
            "Likert scale ranging from very strong influence to basically unrelated.",
            "Then the couples for our experiment consisted of those labeled publications along with society along with the sites ending up with a corpus of 100."
        ],
        [
            "29 documents we use.",
            "Some baselines in this experiment.",
            "So the first was based on Latin, directly allocation, and.",
            "And letting the reallocation we only have the topic mixture.",
            "We do not have any strength information, so we're using the answer Shannon Divergent for measuring the similarity of the two topic mixture of citing and cited publication and saying hey, when this is very similar, there is strong influence or the first baseline.",
            "The second baseline also is heuristic based on latent initially allocation, where we use Bayes rule and the chain rule to convert the topic mixture of the cited publication P of T given C to Pfc given T and then we were integrating overall topics for creating a similarity measure.",
            "The third baseline is simply the TF IDF cosine measure, and the last one is the page rank on the graph with three citation levels."
        ],
        [
            "The valuation measure we used here is something like area under the Roc curve.",
            "So for each single citing publication D, the citation is influence function.",
            "Gamma D is something like a decision function.",
            "And since AUC is basically just a measure for the binary case, we use three AUC values for all those three decision boundaries between very strong and strong influence.",
            "Strong and weak influence we can very weak influence and average over those AUC values.",
            "For the corpus right to build a corpus right evaluation measure, we simply used to mean and standard error over those average AUC's and perform paired T tests when necessary."
        ],
        [
            "So here's the result.",
            "And as you can see, the citation influence model, which is depicted by the dark blue line, is the best method on average, followed by the copycat model and then the two LDA baselines."
        ],
        [
            "And if we look here so especially for high number of topics.",
            "The citation influence model is significantly better than legendary allocation, which is probably because latent image reallocation is then about to degenerate and the measure for similarity does not work anymore."
        ],
        [
            "We also see that the situation influence models rather than sensitive to the number of topics."
        ],
        [
            "And we found that the TF IDF and the Pagerank baselines do not work at all in this setting, leading to AUC values about one 1.5.",
            "Which is basically random guessing."
        ],
        [
            "Then we were interested in the runtime.",
            "So and here we found that the citation influence model converges lot faster than the copycat model.",
            "So you see this here in the potential scale reduction picture.",
            "And this is actually quite interesting because the citation influence model is lot more complex.",
            "In the copycat model.",
            "But it seems and this is just a guess that this model model might be actually more suitable for capturing this domain.",
            "Then the copycat model, and it seems that the copycat model spends a lot of time on the noise.",
            "So we ended up with runtime in this experiment for citation influence.",
            "Being sick 46 minutes and the copycat being basically 3 hours."
        ],
        [
            "According to the Narative evaluation, we took the research paper later during the allocation, and this is citation augment augmentation.",
            "I showed you in the beginning of the talk and we found that this picture actually captures our intuitive notion that we very well.",
            "And with our method."
        ],
        [
            "We are Furthermore able to analyze the abstract seeing which words were typically associated to which other sites and what you see here is kind of interesting.",
            "So the public is cited the site with the strongest influences, the probabilistic latent semantic indexing, which actually matches my intuition, and we see that the words latent modeling, semantic and so associated to this one, whereas the second one which is about Dirichlet processes, got the word directly, mixture allocation and so on associated, and the last one which is about variational methods, which is the method that lie used for the inference.",
            "For the inference process versus associated variational inference algorithm, including so there's also some other words associated."
        ],
        [
            "So to sum up.",
            "We propose two generative model for unsupervised link strength prediction.",
            "And according to prediction, performance attention influences significantly better than later directly allocation.",
            "And so citation influences rather stable according to the number of topics.",
            "So although the infinite processes infinite letter directly stuff is very interesting and definitely step to go to if you're interested in the topic mixture, it seems that we can save the runtime in this special case.",
            "But just having somehow guess of the right number of topics.",
            "And Furthermore, we found that this is somehow intuitively useful for filtering citation graphs.",
            "In future we want to use this method for predicting the influence trends in social networks and web graphs, and you want to find out if this is better.",
            "If this can improve some other methods, for example, like for improve the authority ranking in web graph using those links strength, thank you.",
            "I have a question to start up.",
            "Have you looked at the so you have that parameter Lambda, which also tells you how innovative.",
            "Random words paper is if you looked at.",
            "At all, see whether it makes sense, and you could also ask.",
            "People I guess too.",
            "Regular papers by Helder if they are.",
            "Yeah, actually it's all about playing around with the Lambda.",
            "But yeah, so far we didn't fully analyze this, but yeah, so the idea is to use a Lambda parameter for global thresholding measure.",
            "I mean, so far citation graphs just locally threshold and so we will definitely look in on the Lambda.",
            "Yep.",
            "Model video got his own pocket.",
            "Pardon me, can you speak up, please?",
            "The citation influence model the document.",
            "Off topic only influences innovative orders.",
            "Yeah yeah.",
            "So the topic mixture of citing publication it's only for the innovative words so this is basically.",
            "Problem because then a document won't get credit for being about a topic that it's like another document.",
            "Be able to pass on that topic to future.",
            "Well, yeah, so actually it's yeah.",
            "So I have to confess that this generative model is not really generative, because we did some bipartite transformation.",
            "So the idea is that yeah.",
            "A fighting publication is only influenced by its direct sites and not by the grandparents, so to speak.",
            "So we did a conversion for still being able to infer this model where we duplicated nodes that were citing as well as cited.",
            "So there is no passing on anyway.",
            "So when they hear the topic models is the topic mixtures estimates separately?",
            "Employees.",
            "People.",
            "How?",
            "Appan means the second part of your question.",
            "Considering the authorship.",
            "Yeah, of course.",
            "I mean just some nice direction to move onto, and we're definitely considering this.",
            "One more question.",
            "Great alright thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supervised prediction of citation influences by Laura Dietz, Steven Bickell and OVS Shepherd.",
                    "label": 1
                },
                {
                    "sent": "Good morning, well thank you.",
                    "label": 0
                },
                {
                    "sent": "My name is Laura Dietz and today I present joint work with Stephen Pickle and to be a chef for about the application of topic models, probabilistic topic models through predicting citation influences.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I started with a motivation of the problem setting about, which is about visualizing citation graphs and especially creating meaningful pictures out of it, where one essential part part is to predict the links of citations, and so this is basically the rest of the talk and for the citation prediction I present two generative models, the copycat model, and the citation influence model, and I will.",
                    "label": 0
                },
                {
                    "sent": "Do some experiments on the predictive performance and innovative evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when I was fresh PhD student reading onto machine learning, I went to my supervisor and he gave me a couple of seat publications to read on.",
                    "label": 0
                },
                {
                    "sent": "For example this one.",
                    "label": 0
                },
                {
                    "sent": "So after reading this, I went ahead.",
                    "label": 0
                },
                {
                    "sent": "Well where should I go next to and what's going on around in this community?",
                    "label": 0
                },
                {
                    "sent": "So I started by adding but looking at the citation of affinity, looking at the Brookside was cited an thanks to Google looking at publications that cited this work and looked at some more publications and then I already was left with too many publications, which I thought well I can never read all of them within reasonable time.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "One thing that I found is when you just randomly pick any of those nodes and you redeem them, you think, well it was related and the answer is probably no man.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is there are different reasons for citing publications, so the first one, and this is basically the one I was interested in.",
                    "label": 1
                },
                {
                    "sent": "That time, is that the approach of fighting work was extended by the citing paper.",
                    "label": 1
                },
                {
                    "sent": "You just get something like see value in evolution of scientific topic.",
                    "label": 0
                },
                {
                    "sent": "But then there are also references to baselines and to other papers at work that was tackling the same problem, and there was already at least some influence for them or their citations pointing to basic literature for background reading.",
                    "label": 0
                },
                {
                    "sent": "And this was already not the literature I was interested in that time.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, there's related work that is just named for the purpose that in the next sentence you say why this is actually not helping you according to this problem statement.",
                    "label": 0
                },
                {
                    "sent": "So there will be basically no very weak influence here and least last of all there is there always citations that are done just because everybody sites is.",
                    "label": 1
                },
                {
                    "sent": "And there might be definitely OK in some cases there might be no influence on this one, so we thought it might be nice to have a measure of which of these edges actually influential, so it's easy to say we simply infers citation strings.",
                    "label": 0
                },
                {
                    "sent": "So and this is actually not so easy to do and the remainder of the talk is actually about this one, and once we infer the citation strings, we can filter the edges by simple thresholding.",
                    "label": 0
                },
                {
                    "sent": "And then get rid of isolated nodes and now the graph has shrinked a lot and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know we layout this, we end up with something like this which is definitely more meaningful and you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a more concise.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to formulate the problem statement, we are given a citation graph and text for the publications.",
                    "label": 1
                },
                {
                    "sent": "This might be full text, but in this case I did some experiment with only an abstract.",
                    "label": 0
                },
                {
                    "sent": "And now we want to predict how strong is the influence of a cited publication on citing publication.",
                    "label": 1
                },
                {
                    "sent": "And if you're a PhD students, then it's definitely right that you do not have any strength levels.",
                    "label": 0
                },
                {
                    "sent": "So here we are working with unsupervised methods.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we are proposing latently directly allocation like generative model and in contrast to latent directly allocation where the publications are seen as independent of one another.",
                    "label": 1
                },
                {
                    "sent": "We want to capture the interactions between the publications an so basic idea is that we associate words of the abstract of the sighting publication to any of those sites.",
                    "label": 0
                },
                {
                    "sent": "And the intuition is that the more words are associated, one of the sites are stronger.",
                    "label": 0
                },
                {
                    "sent": "The influences here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the generative process of the first of our two models, the copycat model and the plate diagram you see here is one of latent directly allocation, and it works as follows.",
                    "label": 0
                },
                {
                    "sent": "So for each of the publications here to the left.",
                    "label": 1
                },
                {
                    "sent": "It first draws a topic mixture for each of them, which is multinomial over topics drawn from.",
                    "label": 0
                },
                {
                    "sent": "Additionally, and for each topic it was a typical characteristic word distribution for each of those.",
                    "label": 1
                },
                {
                    "sent": "And then for all documents and for words inside these documents, we associate one topic to each of those words out of this topic mixture.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard and what we did.",
                    "label": 0
                },
                {
                    "sent": "We extended this model by another plate for the citing publications.",
                    "label": 0
                },
                {
                    "sent": "So in order to not confuse you, the errors are point.",
                    "label": 0
                },
                {
                    "sent": "Here are not cited site links, but our potential influence influence links, so cited publication might influence citing publication here.",
                    "label": 1
                },
                {
                    "sent": "So the process is as follows.",
                    "label": 0
                },
                {
                    "sent": "First, we draw a mixture over citations.",
                    "label": 0
                },
                {
                    "sent": "Indicating the strength of influence here and, for example, the third publication here has the strongest influence.",
                    "label": 0
                },
                {
                    "sent": "Then for all the words be first associate word to one of the sites.",
                    "label": 1
                },
                {
                    "sent": "For example, the first one to the first publication, and then for each word we draw the topic out of the corresponding topic mixture of the cited document.",
                    "label": 0
                },
                {
                    "sent": "So we see in in the copycat model it's assumed that society publication something just a copy of different sites it has, so it's really a strong assumption we're making here.",
                    "label": 0
                },
                {
                    "sent": "So and all all topics of all words are definitely inherited.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I just told you is the topic Michelle cited.",
                    "label": 0
                },
                {
                    "sent": "Publication is actually a shared topic mixture because it's influenced by the words of the site of society, publication as well as all the words that are associated with.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have another publication where all the words associated to publication #3.",
                    "label": 0
                },
                {
                    "sent": "Then all those words inside publication #3 as well as those that are associated to publication #3 during the process influence the topic mixture.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a shared topic mixture, so the nice thing here is that we are able to capture, let's say the evolution of the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "For example, if in the original work the terminology was not as clear as in the follow up work, then.",
                    "label": 0
                },
                {
                    "sent": "We can capture this an the influence process is actually also influenced by the new term which is not occurring in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the original book.",
                    "label": 0
                },
                {
                    "sent": "So we see in the copycat model we have a very tight coupling between citing and cited publications, as well as bibliographically coupled publications.",
                    "label": 1
                },
                {
                    "sent": "However, there's actually 1 issue that is because over all words in the citing publication have to be associated to any of those sites.",
                    "label": 0
                },
                {
                    "sent": "We may run into trouble if society publication contains words which do not fit to any of the sites.",
                    "label": 1
                },
                {
                    "sent": "Then this will introduce noise in the shared topic mixture, so noise itself is not really an issue in the probabilistic models, it's simply handled in the natural way, but because of those accidentally associated words.",
                    "label": 0
                },
                {
                    "sent": "Influence the Association process of other citing publications will be influenced by this, so we get sometimes very awkward prediction that do not fit the human intuition so.",
                    "label": 0
                },
                {
                    "sent": "We saw like hey, well there might be a fixed and our fixes that we let some model decide to not decide some words.",
                    "label": 0
                },
                {
                    "sent": "So just as something like an emergency exit.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what's part of the citation influence model which comes here.",
                    "label": 1
                },
                {
                    "sent": "So this is the copycat model, and we add a bit more of complexity.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so the idea is that for each word in the sighting publication, we first flip a coin and the coin says whether the word is inherited, as in the copycat model, or whether it's not associated to any of those sites.",
                    "label": 1
                },
                {
                    "sent": "So here is that the three words that are chosen to be not associated.",
                    "label": 0
                },
                {
                    "sent": "So we also termed this innovation because all innovative words are worth talking about innovation and publication.",
                    "label": 0
                },
                {
                    "sent": "But we probably not assigned to any of those sites.",
                    "label": 0
                },
                {
                    "sent": "And then for the remaining words, we perform basically the same process as in the copycat model by choosing publications, cited publications and drawing words from those topic mixtures and for the innovative words, we draw an utopic mixture only capturing the innovation aspect and draw those words from this topic mixture.",
                    "label": 0
                },
                {
                    "sent": "So to sum up.",
                    "label": 0
                },
                {
                    "sent": "For each word in the site of inciting publication, you flip an unfair coin.",
                    "label": 1
                },
                {
                    "sent": "And here, since we're basically only interested in inheritance process, the coin is very unfair in the sense that it prefers inheritance, and innovation is only the emergency exit if the coin sets inherit this word, then we draw this word from society.",
                    "label": 1
                },
                {
                    "sent": "As in the copycat model, and if the words initial coin says in innovative, this word then withdraws from this own topic mixer special topic mixture.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from these generative models we can infer Gibbs samplers by writing up the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "The one you see here is 1 for the citation influence model an, since we're just using conjugate priors, we can do well black with black realization by integrating over other multinomial parameters.",
                    "label": 0
                },
                {
                    "sent": "Overall, the multinomial's and yeah, so this is what we did for the copy cat and the citation influence model.",
                    "label": 1
                },
                {
                    "sent": "And Furthermore we did some.",
                    "label": 1
                },
                {
                    "sent": "Convergence monitoring based on the method of Brooks and Gilman, and here we are using the citation influence distribution as a scalar summary basis for the scalar summaries.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's move on to the experiment section.",
                    "label": 0
                },
                {
                    "sent": "In order to evaluate the predictive performance, we did a little survey.",
                    "label": 0
                },
                {
                    "sent": "So we are some of our research friends to label some of publication, say road and labeling.",
                    "label": 0
                },
                {
                    "sent": "Here means that we told them something about the data we had of their publications, namely the abstract and all the other citations we found on the web, especially here in society are 2004 data set.",
                    "label": 0
                },
                {
                    "sent": "And so the researchers were asked to state for each of the sites how strong the influence was on their work.",
                    "label": 0
                },
                {
                    "sent": "And here the range may be useful.",
                    "label": 0
                },
                {
                    "sent": "Likert scale ranging from very strong influence to basically unrelated.",
                    "label": 0
                },
                {
                    "sent": "Then the couples for our experiment consisted of those labeled publications along with society along with the sites ending up with a corpus of 100.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "29 documents we use.",
                    "label": 0
                },
                {
                    "sent": "Some baselines in this experiment.",
                    "label": 0
                },
                {
                    "sent": "So the first was based on Latin, directly allocation, and.",
                    "label": 0
                },
                {
                    "sent": "And letting the reallocation we only have the topic mixture.",
                    "label": 0
                },
                {
                    "sent": "We do not have any strength information, so we're using the answer Shannon Divergent for measuring the similarity of the two topic mixture of citing and cited publication and saying hey, when this is very similar, there is strong influence or the first baseline.",
                    "label": 0
                },
                {
                    "sent": "The second baseline also is heuristic based on latent initially allocation, where we use Bayes rule and the chain rule to convert the topic mixture of the cited publication P of T given C to Pfc given T and then we were integrating overall topics for creating a similarity measure.",
                    "label": 1
                },
                {
                    "sent": "The third baseline is simply the TF IDF cosine measure, and the last one is the page rank on the graph with three citation levels.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The valuation measure we used here is something like area under the Roc curve.",
                    "label": 1
                },
                {
                    "sent": "So for each single citing publication D, the citation is influence function.",
                    "label": 0
                },
                {
                    "sent": "Gamma D is something like a decision function.",
                    "label": 0
                },
                {
                    "sent": "And since AUC is basically just a measure for the binary case, we use three AUC values for all those three decision boundaries between very strong and strong influence.",
                    "label": 0
                },
                {
                    "sent": "Strong and weak influence we can very weak influence and average over those AUC values.",
                    "label": 0
                },
                {
                    "sent": "For the corpus right to build a corpus right evaluation measure, we simply used to mean and standard error over those average AUC's and perform paired T tests when necessary.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the result.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the citation influence model, which is depicted by the dark blue line, is the best method on average, followed by the copycat model and then the two LDA baselines.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look here so especially for high number of topics.",
                    "label": 0
                },
                {
                    "sent": "The citation influence model is significantly better than legendary allocation, which is probably because latent image reallocation is then about to degenerate and the measure for similarity does not work anymore.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also see that the situation influence models rather than sensitive to the number of topics.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we found that the TF IDF and the Pagerank baselines do not work at all in this setting, leading to AUC values about one 1.5.",
                    "label": 0
                },
                {
                    "sent": "Which is basically random guessing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we were interested in the runtime.",
                    "label": 0
                },
                {
                    "sent": "So and here we found that the citation influence model converges lot faster than the copycat model.",
                    "label": 1
                },
                {
                    "sent": "So you see this here in the potential scale reduction picture.",
                    "label": 1
                },
                {
                    "sent": "And this is actually quite interesting because the citation influence model is lot more complex.",
                    "label": 0
                },
                {
                    "sent": "In the copycat model.",
                    "label": 0
                },
                {
                    "sent": "But it seems and this is just a guess that this model model might be actually more suitable for capturing this domain.",
                    "label": 0
                },
                {
                    "sent": "Then the copycat model, and it seems that the copycat model spends a lot of time on the noise.",
                    "label": 1
                },
                {
                    "sent": "So we ended up with runtime in this experiment for citation influence.",
                    "label": 0
                },
                {
                    "sent": "Being sick 46 minutes and the copycat being basically 3 hours.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to the Narative evaluation, we took the research paper later during the allocation, and this is citation augment augmentation.",
                    "label": 0
                },
                {
                    "sent": "I showed you in the beginning of the talk and we found that this picture actually captures our intuitive notion that we very well.",
                    "label": 0
                },
                {
                    "sent": "And with our method.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are Furthermore able to analyze the abstract seeing which words were typically associated to which other sites and what you see here is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "So the public is cited the site with the strongest influences, the probabilistic latent semantic indexing, which actually matches my intuition, and we see that the words latent modeling, semantic and so associated to this one, whereas the second one which is about Dirichlet processes, got the word directly, mixture allocation and so on associated, and the last one which is about variational methods, which is the method that lie used for the inference.",
                    "label": 0
                },
                {
                    "sent": "For the inference process versus associated variational inference algorithm, including so there's also some other words associated.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum up.",
                    "label": 0
                },
                {
                    "sent": "We propose two generative model for unsupervised link strength prediction.",
                    "label": 1
                },
                {
                    "sent": "And according to prediction, performance attention influences significantly better than later directly allocation.",
                    "label": 0
                },
                {
                    "sent": "And so citation influences rather stable according to the number of topics.",
                    "label": 1
                },
                {
                    "sent": "So although the infinite processes infinite letter directly stuff is very interesting and definitely step to go to if you're interested in the topic mixture, it seems that we can save the runtime in this special case.",
                    "label": 1
                },
                {
                    "sent": "But just having somehow guess of the right number of topics.",
                    "label": 1
                },
                {
                    "sent": "And Furthermore, we found that this is somehow intuitively useful for filtering citation graphs.",
                    "label": 0
                },
                {
                    "sent": "In future we want to use this method for predicting the influence trends in social networks and web graphs, and you want to find out if this is better.",
                    "label": 0
                },
                {
                    "sent": "If this can improve some other methods, for example, like for improve the authority ranking in web graph using those links strength, thank you.",
                    "label": 0
                },
                {
                    "sent": "I have a question to start up.",
                    "label": 0
                },
                {
                    "sent": "Have you looked at the so you have that parameter Lambda, which also tells you how innovative.",
                    "label": 0
                },
                {
                    "sent": "Random words paper is if you looked at.",
                    "label": 0
                },
                {
                    "sent": "At all, see whether it makes sense, and you could also ask.",
                    "label": 0
                },
                {
                    "sent": "People I guess too.",
                    "label": 0
                },
                {
                    "sent": "Regular papers by Helder if they are.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually it's all about playing around with the Lambda.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so far we didn't fully analyze this, but yeah, so the idea is to use a Lambda parameter for global thresholding measure.",
                    "label": 0
                },
                {
                    "sent": "I mean, so far citation graphs just locally threshold and so we will definitely look in on the Lambda.",
                    "label": 1
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Model video got his own pocket.",
                    "label": 0
                },
                {
                    "sent": "Pardon me, can you speak up, please?",
                    "label": 0
                },
                {
                    "sent": "The citation influence model the document.",
                    "label": 0
                },
                {
                    "sent": "Off topic only influences innovative orders.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So the topic mixture of citing publication it's only for the innovative words so this is basically.",
                    "label": 0
                },
                {
                    "sent": "Problem because then a document won't get credit for being about a topic that it's like another document.",
                    "label": 0
                },
                {
                    "sent": "Be able to pass on that topic to future.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, so actually it's yeah.",
                    "label": 0
                },
                {
                    "sent": "So I have to confess that this generative model is not really generative, because we did some bipartite transformation.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that yeah.",
                    "label": 0
                },
                {
                    "sent": "A fighting publication is only influenced by its direct sites and not by the grandparents, so to speak.",
                    "label": 0
                },
                {
                    "sent": "So we did a conversion for still being able to infer this model where we duplicated nodes that were citing as well as cited.",
                    "label": 0
                },
                {
                    "sent": "So there is no passing on anyway.",
                    "label": 0
                },
                {
                    "sent": "So when they hear the topic models is the topic mixtures estimates separately?",
                    "label": 0
                },
                {
                    "sent": "Employees.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Appan means the second part of your question.",
                    "label": 0
                },
                {
                    "sent": "Considering the authorship.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                },
                {
                    "sent": "I mean just some nice direction to move onto, and we're definitely considering this.",
                    "label": 0
                },
                {
                    "sent": "One more question.",
                    "label": 0
                },
                {
                    "sent": "Great alright thanks.",
                    "label": 0
                }
            ]
        }
    }
}