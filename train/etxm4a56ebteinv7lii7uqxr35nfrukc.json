{
    "id": "etxm4a56ebteinv7lii7uqxr35nfrukc",
    "title": "Inferring Multiple Regulation Networks",
    "info": {
        "author": [
            "Yves Grandvalet, National Center for Scientific Research (CNRS)"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_grandvalet_imrn/",
    "segmentation": [
        [
            "OK thanks, so this is a joint work with Jakey and Crystal form words from the."
        ],
        [
            "Statistique engine or labyrint Neri?",
            "So the problem here is.",
            "To build regulation networks here it's on genome data, but it can be on any kind of biological data.",
            "You have a set of measurements and this measurements you will use these measurements in order to infer interactions.",
            "So the problem we have usually in this type of problem is that we have a few for safe genome data.",
            "We've got few arrays.",
            "This means that we've got a few examples.",
            "We've got a lot of jeans, so this means a lot of variables, high dimensional problems.",
            "And since we are inferring directions so it's high dimension to the to the squared.",
            "So it's a very high dimensional problem.",
            "So the only hope we have here is at nature should be sparse and that's this.",
            "Network should have a few fuelings.",
            "Few genes only should interact.",
            "Oh OK, so.",
            "So usually when you have a small amount of data.",
            "Well, usually what people try to do with the small amount of data they try to gather several.",
            "Experimental conditions in order to have slightly more data and then the from this pooled sets of these pools experiment to try to infer.",
            "Mean network, for example here.",
            "So here the approach we take is just we try to infer one network pair.",
            "Our experiments are conditioned, but we want to exchange to have some kind of.",
            "Change between inference in order to use a commonality's, because this is only experimental condition which changed, but we expect most of the network to be identical."
        ],
        [
            "In different experimental conditions.",
            "So I will present first the graph model we use.",
            "It's a simple Gaussian graphical model.",
            "Then I will.",
            "Recall how are stricter learning as being done in this type of model and the type of.",
            "Things you propose for inferring multiple.",
            "Several regulation network and these are based on group LASSO and a variant of group lasso that we propose here, which is a cooperative lasso where we ask for where we use something more than the group structure, which is a sign of interaction.",
            "And then I will briefly show what the type of algorithm algorithm we chose to implement here and I will finish by."
        ],
        [
            "I'll deal with experiments.",
            "So the graph model is.",
            "I."
        ],
        [
            "Model where each node are the variable, so it's the measurement.",
            "So here each node would be 1 gene.",
            "And we assume that these genes are Goshen variables, and so the set of the PP jeans here is a multi dimensional normal variable with covariance Sigma and hear what will will make many make use of the inverse of the covariance matrix of so called concentration metrics so."
        ],
        [
            "If we want to know if we when should we add a link an edge between X between 2:00?",
            "Two nodes, then we will add an edge if they interact and we will.",
            "This is formalized by the fact that there's a conditional dependency.",
            "So here that the probability.",
            "Of the distribution of the variable at these nodes.",
            "Knowing all the rest of the network is different from the distribution of this node.",
            "Knowing all the rest of the network except this.",
            "This variable this is gene.",
            "So we will draw a line here.",
            "We will have an edge in this graph provided.",
            "We've got a non zero partial correlation between these two variables and this means that the age I entry and the obviously IGN Asia entry are not and are in the.",
            "In the precision matrix, so in."
        ],
        [
            "Bring the graph amounts to infer the non zero entries of the matrix of the constant concentration matrix.",
            "OK, so."
        ],
        [
            "How do we learn the structure?",
            "The basis is maximum likelihood, so the empirical covariance S is a sufficient statistic in this case, because we assume that the genes that activity is 0 as a zero mean.",
            "So I will write that the likelihood function as a function of the empirical covariance matrix.",
            "This likelihood is standard Gaussian likelihood.",
            "Where you have a constant terms.",
            "Log determinant of the concentration metrics and projects that race product of the concentration metrics and the empirical covariance matrix.",
            "So the maximum likelihood is simply the inverse of the covariance matrix, so it's not defined if we have less examples on variables and it's not sparse, so it means that it will lead to a fully connected graph, so it's not interesting for graph in France.",
            "So."
        ],
        [
            "There have been two types of approach for this problem based on sparse.",
            "Sports penalties this one?",
            "The one maybe not in well.",
            "The approach based on maximum likelihood have been proposed by Bernie G and colleague and for the worked further extended by well.",
            "And some.",
            "Some some.",
            "Additional algorithm algorithmic tricks have been proposed by Friedman and correct, so the goal is here is just to maximize the log likelihood subject to a constraint on the metrics.",
            "So this is Norm here means that it's.",
            "The L1 norm on the coefficients of these metrics, Ann and the problem is well defined even for small sample size, so it's OK for biological application.",
            "It's sparse, so it will provide hopefully a sensible graph.",
            "But the problem is quite complex because it's a semidefinite program with order of P square variable an the contribution of Friedman and colleagues to solve this.",
            "Problem iteratively by solving smaller, smaller problems.",
            "So the other approach is based on what has been called neighborhood selection and the principle here is to select the edge the neighboring neighbors.",
            "So the edge going to node J according to the explanation that.",
            "So X slash J.",
            "Here is a set of variable of all variables except the J1 and we try to regress.",
            "These measurements of Noje, according to all the other ones and we will use the regression coefficient to estimate something which is related to the coefficient.",
            "The coefficient of the concentration metrics.",
            "So again, this is a sparse sparse problem problem.",
            "There are a few problems with these techniques that it's not symmetric, while the if you just stuck all the coefficient better G it will not form a symmetric matrix and it will not be positive definite, but it's very nice computationally because it's it's solve P independence lasso problem of size P -- 1.",
            "And we have also theoretical guarantees that show that.",
            "If the network is not too dense, we should recover it from.",
            "The solution of this problem?"
        ],
        [
            "So in fact, the neighborhood solution can be seen as a pseudo likelihood problem.",
            "It solves absolute likelihood problem where we do this approximation, so it's a usual trick where the joint distribution is approximated by the product of all the conditional distribution.",
            "And so with this pseudo likelihood, we've got this approximation of the likelihood, which is very close to the one which was that I gave before.",
            "So basically the main difference is that sometimes the diagonal.",
            "The D, which is a diagonal of the concentration matrix, replace the whole concentration matrix in the previous formula.",
            "OK, so neighborhood selection leads to the graph that maximize.",
            "So like lewd and the coefficients are related to the.",
            "To the.",
            "To the.",
            "Estimated concentration metric basis formula.",
            "OK, now for a multi task."
        ],
        [
            "Running, so we've got now.",
            "We've got the samples."
        ],
        [
            "See T experimental conditions from of the same variables, the same jeans and we want to use all these data set in order to infer network that should somewhat be close with the other ones.",
            "So each example here will be assumed to be drawn from a normal distribution with its own covariance matrix.",
            "But we bet that all the Koreans covariance matrix are similar and all the concentrations.",
            "Metrics are similar, so if we were solving this problem separately, we would have this type of.",
            "Of problem in the neighborhood selection framework.",
            "An the multi task learning will be simply to consist simply in solving that each task jointly.",
            "So there are two ways to couple the.",
            "Estimation tags by the fitting term.",
            "We could use some kind of empirical Bayes method or by the penalty term.",
            "Excuse me do you mean to infer a network for each sample?",
            "Yeah, one network, there are sample.",
            "No, I've got one setof.",
            "I sampled in the yeah yeah."
        ],
        [
            "So the usual way to group things in the multi task framework is in sparse method is to use a group lasso.",
            "So here we've got several several networks here safe."
        ],
        [
            "Or networks with the same variables and we will group the."
        ],
        [
            "Edges.",
            "We will provide group of variables by considering all the edges between the two same variables along the.",
            "Along the T networks, and that's what this formula says.",
            "We group across network the.",
            "The koi fish."
        ],
        [
            "We go from node 8, not I too."
        ],
        [
            "J so we can do that for."
        ],
        [
            "All the networks of these."
        ],
        [
            "This problem and we hope, well we know that with this type of penalty we will have a sparsity pattern that will be shared between graphs.",
            "And something which is less desirable.",
            "They that they will need two identical graphs across task, because there's no sparsity within groups.",
            "So it means that once.",
            "We have an edge which has been selected among one of these graphs that it will be for.",
            "It will exist for all graphs.",
            "So."
        ],
        [
            "So I will just remind what are the.",
            "What is the shape of the of the admissible set for the group lasso?",
            "Because I will it will be helpful to understand what.",
            "What are the difference with the Coop lasso?",
            "So we've got two tasks, 22 coefficients for tasks and so the task number is in paranthesis and we've got variable one and two for each task.",
            "And so here we've got the first task which correspond to beta 1 beta two in the in this plane and the.",
            "A projection of the Four 4 dimensional goal onto beta 2 = 0, which is here and if beta 2 for the second task is non zero, we've got this shape here.",
            "So this is just to see that the edges that we've got.",
            "In Group lasso tend to disappear when one of the variable is when one of the variable is not null.",
            "OK, so if we get this 3D bowl here on a 2D."
        ],
        [
            "So we've got the between task.",
            "Penalty so it looks like a lasso here and it's smoother here, meaning that there is no chance to have these beta 2 for task one, which is not nirbhik cause beta 2 for the second task is normal.",
            "An if we look for."
        ],
        [
            "All the values here, so we cut for a beta 1 two which is not null.",
            "We see that when none of the variables is not null for the second task, there will be no edges on the axis, meaning that there will be no zeros on the on the.",
            "On the 1st task."
        ],
        [
            "OK, so."
        ],
        [
            "The the other way to group penalty will be the cooperative law so that we want to ensure not only that the.",
            "The graphs are similar, but also that the type of interaction is similar.",
            "So if we have an inhibition in a graph, we want to find it all over the place and if we have an activation somewhere, we're going to find it all over the place.",
            "So we introduce in fact.",
            "The composition of each."
        ],
        [
            "Edge of the of the network into a positive positive or negative edge.",
            "And."
        ],
        [
            "We will use this for the multi task."
        ],
        [
            "Framework by coupling all the."
        ],
        [
            "But if they get."
        ],
        [
            "Give edge between any 2."
        ],
        [
            "Two edges, any two nodes?",
            "Sorry OK, so we believe that this thing is plausible for biological network in general, not only for genomic application and the good thing is that this this penalty will allow to have sparsity shared between graphs, but the graphs may differ.",
            "We may have different graphs.",
            "And that was one of our original goals."
        ],
        [
            "So that's the same.",
            "The same drawing as before for the Coop Lasso and what you should.",
            "The main difference that you should see on the 3D drawings is that you have additional edges and these edges will be new opportunity to have zeros in the.",
            "In the solution so."
        ],
        [
            "So just to compare."
        ],
        [
            "Have with the previous."
        ],
        [
            "Copla so.",
            "And here we see that in this situation previously we couldn't have zeros on beta 21.",
            "Here it's possible and even.",
            "In the situation."
        ],
        [
            "And where the old variables for the second task and non zero we see some discontinuity's on this.",
            "This figure, which means opportunity to have zeros and the second thing is that you see that this is not symmetrical anymore.",
            "So we've got positive values on the second task.",
            "Positive value will be.",
            "More likely in this."
        ],
        [
            "Second one OK."
        ],
        [
            "So the algorithm is based on."
        ],
        [
            "Decomposition strategy.",
            "Where we estimate all the the neighborhood of the T graph separately?",
            "I mean we estimate the neighbor each neighborhoods for each node we estimate the neighborhood separately, but we consider the graph junkie.",
            "And we just have the well, standard things that we will have a minimizer if zero is in the subdifferential of the function we have so."
        ],
        [
            "I want it."
        ],
        [
            "The further the."
        ],
        [
            "The algorithm, but it's really the."
        ],
        [
            "Classic where?"
        ],
        [
            "We optimize with respect to the active variable."
        ],
        [
            "Identify new zero variables or new non zero variable and we cycle until convergence."
        ],
        [
            "OK, so.",
            "I will just provide here a few."
        ],
        [
            "Experiments, so we try to do some experiments on data where that we could understand.",
            "So we generated data.",
            "By fixing the number of nodes, number of edges, number of examples.",
            "And.",
            "In fact, it's a it's a difficult problem because it's difficult to.",
            "To generate.",
            "Artificial data with a.",
            "It's difficult to figure out what is the difficulty of the problem.",
            "When we generate this data, some of them are really easy, some of them are really difficult and it's not that easy to understand why and how we can change things.",
            "OK.",
            "So basically, the processes that we generate an agency agency metrics from this agency metrics Laplacian.",
            "Then we just switch sign randomly.",
            "From this we've got a concentration metrics win versus and we generate data.",
            "So."
        ],
        [
            "So to have simulated to have similar task what we do is what we draw announced an ancestor graph and then we just perturb this guy."
        ],
        [
            "Have to have children graphs which are alike.",
            "OK, so that's two."
        ],
        [
            "Example with two perturbations and here I will."
        ],
        [
            "I'd results with recall and precision so."
        ],
        [
            "To make it very rapid, so in large sample sizes you don't gain anything in using the multitask framework.",
            "That's something that I guess you would expect.",
            "And if you're really wrong, I."
        ],
        [
            "If you've got a large discrepancy between networks, it may hurt.",
            "In the medium sample sizes so."
        ],
        [
            "I forget to see that in color, we've got several multitask.",
            "Recall precision curves and the Gray ones are either.",
            "Independent estimation of the.",
            "Of the network, or pooled estimation?",
            "Samples."
        ],
        [
            "What do you mean by sample size?",
            "Do you mean the number of observations for matrix sample that you have for the number of matrix samples that you have?",
            "The number of sample pair tasks?",
            "So I've got here 4 different tasks and I've got 50.",
            "Examples for each task.",
            "OK."
        ],
        [
            "For small sample sizes, then there's can be a really great difference, provided that you don't have two that the true network do not differ."
        ],
        [
            "Ouch."
        ],
        [
            "OK, so."
        ],
        [
            "I had a demo but I will skip it, but there's there's some code available, so just to sum up, I think that multi task is relevant often for network inference and maybe for other biological problem that it provides consistent improvement about upon the available baseline solution.",
            "If your assumptions are not incorrect.",
            "I mean if the user network are alike and.",
            "You we are.",
            "We would be delighted if you would.",
            "Like to be to test the TPR package which is available here.",
            "And what we are in the process of doing now is to explore model selection capabilities.",
            "How to select a point on these precision recall curves?",
            "An regarding theoretical theoretical analysis?",
            "We look at what are the conditions for the having the uniqueness of the solution and we are looking at the speed of the selection, consistency and the speed at which we can.",
            "Select the right.",
            "We can have the support of the of the non zero coefficients.",
            "Thanks."
        ],
        [
            "Sorry.",
            "But if the computer will have another next week or not, is it me?",
            "I'm not speaking in this.",
            "OK question."
        ],
        [
            "So I missed it when you talked about your simulation.",
            "So I mean, the lasso is based on this assumption.",
            "Everything has the same sign.",
            "Yeah, do you?",
            "Do you test how well your model does when you don't have that assumption?",
            "Or is that part is that encoded in your Delta?",
            "Well, when we when we in the Delta we just have.",
            "Yeah, we we.",
            "We just add or remove some some some links, some edges, but I think that well.",
            "For example the edges will remove well the edges we add may have the same.",
            "Sign or not, but on the other hand, I mean the ancestor graph, as is the same for everybody.",
            "So the the assumption that all the graphs have mostly the same all the edge of mostly the same.",
            "Sign is is correct, one is.",
            "Is reflected by your noise.",
            "Yeah yeah.",
            "Which can change side yeah.",
            "So there's somewhat robust to that.",
            "Breaking up that assumption, yes.",
            "What?",
            "In fact, it's it's.",
            "It's pretty much if if you've got.",
            "Assignment machine, it will really look like the group lasso.",
            "If you've got in the same once you activate one positive and one negative.",
            "Value then it's exactly like the group lasso in this group.",
            "I had a question ready to one of the questions that were asked is in the experiments you said you have four networks and then when you have enough sample storage networking forms, what happens if you don't have many samples per medical, but you have no networks?",
            "Will it work too?",
            "I mean will be consistent with.",
            "The number of tasks increases but not visible symbols that we didn't do the experiment, but yes, it's.",
            "Yeah, I believe that if there's a very small amount of perturbation between network, it should be OK, and otherwise I guess it will be completely messy.",
            "Becausw, in fact we all the network will be quite different if there's a lot of perturbations.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK thanks, so this is a joint work with Jakey and Crystal form words from the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statistique engine or labyrint Neri?",
                    "label": 0
                },
                {
                    "sent": "So the problem here is.",
                    "label": 0
                },
                {
                    "sent": "To build regulation networks here it's on genome data, but it can be on any kind of biological data.",
                    "label": 0
                },
                {
                    "sent": "You have a set of measurements and this measurements you will use these measurements in order to infer interactions.",
                    "label": 0
                },
                {
                    "sent": "So the problem we have usually in this type of problem is that we have a few for safe genome data.",
                    "label": 0
                },
                {
                    "sent": "We've got few arrays.",
                    "label": 0
                },
                {
                    "sent": "This means that we've got a few examples.",
                    "label": 0
                },
                {
                    "sent": "We've got a lot of jeans, so this means a lot of variables, high dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "And since we are inferring directions so it's high dimension to the to the squared.",
                    "label": 0
                },
                {
                    "sent": "So it's a very high dimensional problem.",
                    "label": 1
                },
                {
                    "sent": "So the only hope we have here is at nature should be sparse and that's this.",
                    "label": 0
                },
                {
                    "sent": "Network should have a few fuelings.",
                    "label": 1
                },
                {
                    "sent": "Few genes only should interact.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so.",
                    "label": 0
                },
                {
                    "sent": "So usually when you have a small amount of data.",
                    "label": 0
                },
                {
                    "sent": "Well, usually what people try to do with the small amount of data they try to gather several.",
                    "label": 0
                },
                {
                    "sent": "Experimental conditions in order to have slightly more data and then the from this pooled sets of these pools experiment to try to infer.",
                    "label": 0
                },
                {
                    "sent": "Mean network, for example here.",
                    "label": 0
                },
                {
                    "sent": "So here the approach we take is just we try to infer one network pair.",
                    "label": 0
                },
                {
                    "sent": "Our experiments are conditioned, but we want to exchange to have some kind of.",
                    "label": 0
                },
                {
                    "sent": "Change between inference in order to use a commonality's, because this is only experimental condition which changed, but we expect most of the network to be identical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In different experimental conditions.",
                    "label": 0
                },
                {
                    "sent": "So I will present first the graph model we use.",
                    "label": 1
                },
                {
                    "sent": "It's a simple Gaussian graphical model.",
                    "label": 0
                },
                {
                    "sent": "Then I will.",
                    "label": 0
                },
                {
                    "sent": "Recall how are stricter learning as being done in this type of model and the type of.",
                    "label": 0
                },
                {
                    "sent": "Things you propose for inferring multiple.",
                    "label": 1
                },
                {
                    "sent": "Several regulation network and these are based on group LASSO and a variant of group lasso that we propose here, which is a cooperative lasso where we ask for where we use something more than the group structure, which is a sign of interaction.",
                    "label": 0
                },
                {
                    "sent": "And then I will briefly show what the type of algorithm algorithm we chose to implement here and I will finish by.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll deal with experiments.",
                    "label": 0
                },
                {
                    "sent": "So the graph model is.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model where each node are the variable, so it's the measurement.",
                    "label": 0
                },
                {
                    "sent": "So here each node would be 1 gene.",
                    "label": 0
                },
                {
                    "sent": "And we assume that these genes are Goshen variables, and so the set of the PP jeans here is a multi dimensional normal variable with covariance Sigma and hear what will will make many make use of the inverse of the covariance matrix of so called concentration metrics so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we want to know if we when should we add a link an edge between X between 2:00?",
                    "label": 0
                },
                {
                    "sent": "Two nodes, then we will add an edge if they interact and we will.",
                    "label": 0
                },
                {
                    "sent": "This is formalized by the fact that there's a conditional dependency.",
                    "label": 1
                },
                {
                    "sent": "So here that the probability.",
                    "label": 0
                },
                {
                    "sent": "Of the distribution of the variable at these nodes.",
                    "label": 1
                },
                {
                    "sent": "Knowing all the rest of the network is different from the distribution of this node.",
                    "label": 0
                },
                {
                    "sent": "Knowing all the rest of the network except this.",
                    "label": 0
                },
                {
                    "sent": "This variable this is gene.",
                    "label": 0
                },
                {
                    "sent": "So we will draw a line here.",
                    "label": 0
                },
                {
                    "sent": "We will have an edge in this graph provided.",
                    "label": 0
                },
                {
                    "sent": "We've got a non zero partial correlation between these two variables and this means that the age I entry and the obviously IGN Asia entry are not and are in the.",
                    "label": 1
                },
                {
                    "sent": "In the precision matrix, so in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring the graph amounts to infer the non zero entries of the matrix of the constant concentration matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we learn the structure?",
                    "label": 0
                },
                {
                    "sent": "The basis is maximum likelihood, so the empirical covariance S is a sufficient statistic in this case, because we assume that the genes that activity is 0 as a zero mean.",
                    "label": 1
                },
                {
                    "sent": "So I will write that the likelihood function as a function of the empirical covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This likelihood is standard Gaussian likelihood.",
                    "label": 0
                },
                {
                    "sent": "Where you have a constant terms.",
                    "label": 0
                },
                {
                    "sent": "Log determinant of the concentration metrics and projects that race product of the concentration metrics and the empirical covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So the maximum likelihood is simply the inverse of the covariance matrix, so it's not defined if we have less examples on variables and it's not sparse, so it means that it will lead to a fully connected graph, so it's not interesting for graph in France.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There have been two types of approach for this problem based on sparse.",
                    "label": 0
                },
                {
                    "sent": "Sports penalties this one?",
                    "label": 0
                },
                {
                    "sent": "The one maybe not in well.",
                    "label": 0
                },
                {
                    "sent": "The approach based on maximum likelihood have been proposed by Bernie G and colleague and for the worked further extended by well.",
                    "label": 0
                },
                {
                    "sent": "And some.",
                    "label": 0
                },
                {
                    "sent": "Some some.",
                    "label": 0
                },
                {
                    "sent": "Additional algorithm algorithmic tricks have been proposed by Friedman and correct, so the goal is here is just to maximize the log likelihood subject to a constraint on the metrics.",
                    "label": 0
                },
                {
                    "sent": "So this is Norm here means that it's.",
                    "label": 0
                },
                {
                    "sent": "The L1 norm on the coefficients of these metrics, Ann and the problem is well defined even for small sample size, so it's OK for biological application.",
                    "label": 0
                },
                {
                    "sent": "It's sparse, so it will provide hopefully a sensible graph.",
                    "label": 0
                },
                {
                    "sent": "But the problem is quite complex because it's a semidefinite program with order of P square variable an the contribution of Friedman and colleagues to solve this.",
                    "label": 0
                },
                {
                    "sent": "Problem iteratively by solving smaller, smaller problems.",
                    "label": 0
                },
                {
                    "sent": "So the other approach is based on what has been called neighborhood selection and the principle here is to select the edge the neighboring neighbors.",
                    "label": 0
                },
                {
                    "sent": "So the edge going to node J according to the explanation that.",
                    "label": 1
                },
                {
                    "sent": "So X slash J.",
                    "label": 0
                },
                {
                    "sent": "Here is a set of variable of all variables except the J1 and we try to regress.",
                    "label": 0
                },
                {
                    "sent": "These measurements of Noje, according to all the other ones and we will use the regression coefficient to estimate something which is related to the coefficient.",
                    "label": 0
                },
                {
                    "sent": "The coefficient of the concentration metrics.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a sparse sparse problem problem.",
                    "label": 1
                },
                {
                    "sent": "There are a few problems with these techniques that it's not symmetric, while the if you just stuck all the coefficient better G it will not form a symmetric matrix and it will not be positive definite, but it's very nice computationally because it's it's solve P independence lasso problem of size P -- 1.",
                    "label": 0
                },
                {
                    "sent": "And we have also theoretical guarantees that show that.",
                    "label": 0
                },
                {
                    "sent": "If the network is not too dense, we should recover it from.",
                    "label": 0
                },
                {
                    "sent": "The solution of this problem?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in fact, the neighborhood solution can be seen as a pseudo likelihood problem.",
                    "label": 0
                },
                {
                    "sent": "It solves absolute likelihood problem where we do this approximation, so it's a usual trick where the joint distribution is approximated by the product of all the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And so with this pseudo likelihood, we've got this approximation of the likelihood, which is very close to the one which was that I gave before.",
                    "label": 0
                },
                {
                    "sent": "So basically the main difference is that sometimes the diagonal.",
                    "label": 0
                },
                {
                    "sent": "The D, which is a diagonal of the concentration matrix, replace the whole concentration matrix in the previous formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so neighborhood selection leads to the graph that maximize.",
                    "label": 1
                },
                {
                    "sent": "So like lewd and the coefficients are related to the.",
                    "label": 0
                },
                {
                    "sent": "To the.",
                    "label": 0
                },
                {
                    "sent": "To the.",
                    "label": 0
                },
                {
                    "sent": "Estimated concentration metric basis formula.",
                    "label": 0
                },
                {
                    "sent": "OK, now for a multi task.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Running, so we've got now.",
                    "label": 0
                },
                {
                    "sent": "We've got the samples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See T experimental conditions from of the same variables, the same jeans and we want to use all these data set in order to infer network that should somewhat be close with the other ones.",
                    "label": 1
                },
                {
                    "sent": "So each example here will be assumed to be drawn from a normal distribution with its own covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "But we bet that all the Koreans covariance matrix are similar and all the concentrations.",
                    "label": 0
                },
                {
                    "sent": "Metrics are similar, so if we were solving this problem separately, we would have this type of.",
                    "label": 0
                },
                {
                    "sent": "Of problem in the neighborhood selection framework.",
                    "label": 1
                },
                {
                    "sent": "An the multi task learning will be simply to consist simply in solving that each task jointly.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways to couple the.",
                    "label": 1
                },
                {
                    "sent": "Estimation tags by the fitting term.",
                    "label": 1
                },
                {
                    "sent": "We could use some kind of empirical Bayes method or by the penalty term.",
                    "label": 0
                },
                {
                    "sent": "Excuse me do you mean to infer a network for each sample?",
                    "label": 0
                },
                {
                    "sent": "Yeah, one network, there are sample.",
                    "label": 0
                },
                {
                    "sent": "No, I've got one setof.",
                    "label": 0
                },
                {
                    "sent": "I sampled in the yeah yeah.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the usual way to group things in the multi task framework is in sparse method is to use a group lasso.",
                    "label": 0
                },
                {
                    "sent": "So here we've got several several networks here safe.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or networks with the same variables and we will group the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edges.",
                    "label": 0
                },
                {
                    "sent": "We will provide group of variables by considering all the edges between the two same variables along the.",
                    "label": 0
                },
                {
                    "sent": "Along the T networks, and that's what this formula says.",
                    "label": 0
                },
                {
                    "sent": "We group across network the.",
                    "label": 0
                },
                {
                    "sent": "The koi fish.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go from node 8, not I too.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "J so we can do that for.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the networks of these.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem and we hope, well we know that with this type of penalty we will have a sparsity pattern that will be shared between graphs.",
                    "label": 0
                },
                {
                    "sent": "And something which is less desirable.",
                    "label": 0
                },
                {
                    "sent": "They that they will need two identical graphs across task, because there's no sparsity within groups.",
                    "label": 0
                },
                {
                    "sent": "So it means that once.",
                    "label": 0
                },
                {
                    "sent": "We have an edge which has been selected among one of these graphs that it will be for.",
                    "label": 0
                },
                {
                    "sent": "It will exist for all graphs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will just remind what are the.",
                    "label": 0
                },
                {
                    "sent": "What is the shape of the of the admissible set for the group lasso?",
                    "label": 0
                },
                {
                    "sent": "Because I will it will be helpful to understand what.",
                    "label": 0
                },
                {
                    "sent": "What are the difference with the Coop lasso?",
                    "label": 0
                },
                {
                    "sent": "So we've got two tasks, 22 coefficients for tasks and so the task number is in paranthesis and we've got variable one and two for each task.",
                    "label": 0
                },
                {
                    "sent": "And so here we've got the first task which correspond to beta 1 beta two in the in this plane and the.",
                    "label": 0
                },
                {
                    "sent": "A projection of the Four 4 dimensional goal onto beta 2 = 0, which is here and if beta 2 for the second task is non zero, we've got this shape here.",
                    "label": 0
                },
                {
                    "sent": "So this is just to see that the edges that we've got.",
                    "label": 0
                },
                {
                    "sent": "In Group lasso tend to disappear when one of the variable is when one of the variable is not null.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we get this 3D bowl here on a 2D.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've got the between task.",
                    "label": 0
                },
                {
                    "sent": "Penalty so it looks like a lasso here and it's smoother here, meaning that there is no chance to have these beta 2 for task one, which is not nirbhik cause beta 2 for the second task is normal.",
                    "label": 0
                },
                {
                    "sent": "An if we look for.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the values here, so we cut for a beta 1 two which is not null.",
                    "label": 0
                },
                {
                    "sent": "We see that when none of the variables is not null for the second task, there will be no edges on the axis, meaning that there will be no zeros on the on the.",
                    "label": 0
                },
                {
                    "sent": "On the 1st task.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the other way to group penalty will be the cooperative law so that we want to ensure not only that the.",
                    "label": 0
                },
                {
                    "sent": "The graphs are similar, but also that the type of interaction is similar.",
                    "label": 0
                },
                {
                    "sent": "So if we have an inhibition in a graph, we want to find it all over the place and if we have an activation somewhere, we're going to find it all over the place.",
                    "label": 0
                },
                {
                    "sent": "So we introduce in fact.",
                    "label": 0
                },
                {
                    "sent": "The composition of each.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edge of the of the network into a positive positive or negative edge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will use this for the multi task.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Framework by coupling all the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if they get.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give edge between any 2.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two edges, any two nodes?",
                    "label": 0
                },
                {
                    "sent": "Sorry OK, so we believe that this thing is plausible for biological network in general, not only for genomic application and the good thing is that this this penalty will allow to have sparsity shared between graphs, but the graphs may differ.",
                    "label": 1
                },
                {
                    "sent": "We may have different graphs.",
                    "label": 0
                },
                {
                    "sent": "And that was one of our original goals.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the same.",
                    "label": 0
                },
                {
                    "sent": "The same drawing as before for the Coop Lasso and what you should.",
                    "label": 0
                },
                {
                    "sent": "The main difference that you should see on the 3D drawings is that you have additional edges and these edges will be new opportunity to have zeros in the.",
                    "label": 0
                },
                {
                    "sent": "In the solution so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to compare.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have with the previous.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Copla so.",
                    "label": 0
                },
                {
                    "sent": "And here we see that in this situation previously we couldn't have zeros on beta 21.",
                    "label": 0
                },
                {
                    "sent": "Here it's possible and even.",
                    "label": 0
                },
                {
                    "sent": "In the situation.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And where the old variables for the second task and non zero we see some discontinuity's on this.",
                    "label": 0
                },
                {
                    "sent": "This figure, which means opportunity to have zeros and the second thing is that you see that this is not symmetrical anymore.",
                    "label": 0
                },
                {
                    "sent": "So we've got positive values on the second task.",
                    "label": 0
                },
                {
                    "sent": "Positive value will be.",
                    "label": 0
                },
                {
                    "sent": "More likely in this.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second one OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm is based on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decomposition strategy.",
                    "label": 0
                },
                {
                    "sent": "Where we estimate all the the neighborhood of the T graph separately?",
                    "label": 1
                },
                {
                    "sent": "I mean we estimate the neighbor each neighborhoods for each node we estimate the neighborhood separately, but we consider the graph junkie.",
                    "label": 0
                },
                {
                    "sent": "And we just have the well, standard things that we will have a minimizer if zero is in the subdifferential of the function we have so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want it.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The further the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm, but it's really the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classic where?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We optimize with respect to the active variable.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identify new zero variables or new non zero variable and we cycle until convergence.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I will just provide here a few.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments, so we try to do some experiments on data where that we could understand.",
                    "label": 0
                },
                {
                    "sent": "So we generated data.",
                    "label": 0
                },
                {
                    "sent": "By fixing the number of nodes, number of edges, number of examples.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's a it's a difficult problem because it's difficult to.",
                    "label": 0
                },
                {
                    "sent": "To generate.",
                    "label": 0
                },
                {
                    "sent": "Artificial data with a.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to figure out what is the difficulty of the problem.",
                    "label": 0
                },
                {
                    "sent": "When we generate this data, some of them are really easy, some of them are really difficult and it's not that easy to understand why and how we can change things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically, the processes that we generate an agency agency metrics from this agency metrics Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Then we just switch sign randomly.",
                    "label": 0
                },
                {
                    "sent": "From this we've got a concentration metrics win versus and we generate data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to have simulated to have similar task what we do is what we draw announced an ancestor graph and then we just perturb this guy.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have to have children graphs which are alike.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's two.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example with two perturbations and here I will.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd results with recall and precision so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To make it very rapid, so in large sample sizes you don't gain anything in using the multitask framework.",
                    "label": 0
                },
                {
                    "sent": "That's something that I guess you would expect.",
                    "label": 0
                },
                {
                    "sent": "And if you're really wrong, I.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you've got a large discrepancy between networks, it may hurt.",
                    "label": 0
                },
                {
                    "sent": "In the medium sample sizes so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I forget to see that in color, we've got several multitask.",
                    "label": 0
                },
                {
                    "sent": "Recall precision curves and the Gray ones are either.",
                    "label": 0
                },
                {
                    "sent": "Independent estimation of the.",
                    "label": 0
                },
                {
                    "sent": "Of the network, or pooled estimation?",
                    "label": 0
                },
                {
                    "sent": "Samples.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do you mean by sample size?",
                    "label": 0
                },
                {
                    "sent": "Do you mean the number of observations for matrix sample that you have for the number of matrix samples that you have?",
                    "label": 0
                },
                {
                    "sent": "The number of sample pair tasks?",
                    "label": 0
                },
                {
                    "sent": "So I've got here 4 different tasks and I've got 50.",
                    "label": 0
                },
                {
                    "sent": "Examples for each task.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For small sample sizes, then there's can be a really great difference, provided that you don't have two that the true network do not differ.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ouch.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had a demo but I will skip it, but there's there's some code available, so just to sum up, I think that multi task is relevant often for network inference and maybe for other biological problem that it provides consistent improvement about upon the available baseline solution.",
                    "label": 0
                },
                {
                    "sent": "If your assumptions are not incorrect.",
                    "label": 0
                },
                {
                    "sent": "I mean if the user network are alike and.",
                    "label": 0
                },
                {
                    "sent": "You we are.",
                    "label": 0
                },
                {
                    "sent": "We would be delighted if you would.",
                    "label": 0
                },
                {
                    "sent": "Like to be to test the TPR package which is available here.",
                    "label": 0
                },
                {
                    "sent": "And what we are in the process of doing now is to explore model selection capabilities.",
                    "label": 0
                },
                {
                    "sent": "How to select a point on these precision recall curves?",
                    "label": 0
                },
                {
                    "sent": "An regarding theoretical theoretical analysis?",
                    "label": 0
                },
                {
                    "sent": "We look at what are the conditions for the having the uniqueness of the solution and we are looking at the speed of the selection, consistency and the speed at which we can.",
                    "label": 0
                },
                {
                    "sent": "Select the right.",
                    "label": 0
                },
                {
                    "sent": "We can have the support of the of the non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "But if the computer will have another next week or not, is it me?",
                    "label": 0
                },
                {
                    "sent": "I'm not speaking in this.",
                    "label": 0
                },
                {
                    "sent": "OK question.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I missed it when you talked about your simulation.",
                    "label": 0
                },
                {
                    "sent": "So I mean, the lasso is based on this assumption.",
                    "label": 0
                },
                {
                    "sent": "Everything has the same sign.",
                    "label": 0
                },
                {
                    "sent": "Yeah, do you?",
                    "label": 0
                },
                {
                    "sent": "Do you test how well your model does when you don't have that assumption?",
                    "label": 0
                },
                {
                    "sent": "Or is that part is that encoded in your Delta?",
                    "label": 0
                },
                {
                    "sent": "Well, when we when we in the Delta we just have.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we.",
                    "label": 0
                },
                {
                    "sent": "We just add or remove some some some links, some edges, but I think that well.",
                    "label": 0
                },
                {
                    "sent": "For example the edges will remove well the edges we add may have the same.",
                    "label": 0
                },
                {
                    "sent": "Sign or not, but on the other hand, I mean the ancestor graph, as is the same for everybody.",
                    "label": 0
                },
                {
                    "sent": "So the the assumption that all the graphs have mostly the same all the edge of mostly the same.",
                    "label": 0
                },
                {
                    "sent": "Sign is is correct, one is.",
                    "label": 0
                },
                {
                    "sent": "Is reflected by your noise.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Which can change side yeah.",
                    "label": 0
                },
                {
                    "sent": "So there's somewhat robust to that.",
                    "label": 0
                },
                {
                    "sent": "Breaking up that assumption, yes.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "In fact, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much if if you've got.",
                    "label": 0
                },
                {
                    "sent": "Assignment machine, it will really look like the group lasso.",
                    "label": 0
                },
                {
                    "sent": "If you've got in the same once you activate one positive and one negative.",
                    "label": 0
                },
                {
                    "sent": "Value then it's exactly like the group lasso in this group.",
                    "label": 0
                },
                {
                    "sent": "I had a question ready to one of the questions that were asked is in the experiments you said you have four networks and then when you have enough sample storage networking forms, what happens if you don't have many samples per medical, but you have no networks?",
                    "label": 0
                },
                {
                    "sent": "Will it work too?",
                    "label": 0
                },
                {
                    "sent": "I mean will be consistent with.",
                    "label": 0
                },
                {
                    "sent": "The number of tasks increases but not visible symbols that we didn't do the experiment, but yes, it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I believe that if there's a very small amount of perturbation between network, it should be OK, and otherwise I guess it will be completely messy.",
                    "label": 0
                },
                {
                    "sent": "Becausw, in fact we all the network will be quite different if there's a lot of perturbations.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}