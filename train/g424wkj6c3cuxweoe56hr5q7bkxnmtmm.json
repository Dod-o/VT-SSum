{
    "id": "g424wkj6c3cuxweoe56hr5q7bkxnmtmm",
    "title": "Beyond String Search: Fast and Accurate Retrieval of Entities and Dependencies",
    "info": {
        "author": [
            "Hugo Zaragoza, Yahoo! Research"
        ],
        "published": "Jan. 28, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Semantic Web->Annotation->Tagging",
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/ecml07_zaragoza_bss/",
    "segmentation": [
        [
            "Last time.",
            "Previously at Microsoft Research Lab in Cambridge, better slowly.",
            "Paris.",
            "I made some pleasure for me.",
            "He's going to talk about today.",
            "Thanks mining and information retrieval.",
            "Beyond string search.",
            "Fast and accurate retrieval of entities and developers.",
            "Thank you, what time should I finish the talk, you think?",
            "OK 20.",
            "So it's 20 just OK. Alright so.",
            "Not this one.",
            "This one here.",
            "OK, so I'll be talking about this.",
            "It's a bit like cheating.",
            "Looks like Afghan this.",
            "I've done fast and accurate retrieval of entities and dependencies.",
            "Where it says that is what I want to do.",
            "I want to have fast and accurate retrieval Avengers and dependencies.",
            "What?",
            "OK. Really OK.",
            "I don't know.",
            "Hello.",
            "Hello.",
            "Maybe if I speak louder can you hear me at the back, I mean.",
            "The song yeah OK. Feel like Britney Spears.",
            "No aspect in a very limited way, but OK.",
            "So because this talk is in the enterprise track.",
            "I thought I want uses excuse to present my work even though it wasn't submitted.",
            "An peer reviewed an, so I'm going to do something different when I'm going to try to show you is actually a view of some of the problems that we're dealing in Yahoo Research Barcelona and specifically some of the data that we're using because it is.",
            "It is our hope that people who know a lot about machine learning, like the people sitting in this room here, will take this data and do things with it that we couldn't even think was possible to do so.",
            "Instead of showing you.",
            "Particular results or algorithms?",
            "I will show you data that we're trying to build and make available to people to do experiments and problems that are maybe.",
            "Interesting to deal with.",
            "OK, this is work with the people working at Yahoo Research Lab in Barcelona as well as you separate, RDN hanging rotate.",
            "Rd is a intern and you said they spent a year with us in Barcelona."
        ],
        [
            "So I have two goals.",
            "I will tell you whether a research goes, why are we trying to do this and then give you ideas of how you can go about representing text that goes beyond bags of words or matrices, representations OK. And then I want.",
            "I don't think I have any more time, but if I have more time, I'll go into actually solutions that we are producing in Barcelona."
        ],
        [
            "So this is the document understanding cartoon why are we doing these things?",
            "It is our feeling that this is how the document understanding worlds looks like at the left you have grab those who don't know who is crap is just a search string.",
            "Search verifies unique string search OK and on the right you have this ultimate goal of a domain expert at your fingertips.",
            "Somebody who knows the topics very very well and can talk to you knows you, knows what you've read OK and so.",
            "Search engines today are dangerously close to grep there really, just very fast, very large scale grep OK, except for a few features are coming out in the last few years like you type a hotel room and then you actually get the price of the night.",
            "These things are actually going beyond grip and understanding the query, but for the most part we're here Now this question answering a question answering.",
            "It's a bit of a proof of concept.",
            "There are people using NLP to do fairly incredible things like answering questions that looked impossible to answer before, so there is this idea that we could.",
            "Certainly move beyond search engines and beyond grab towards more NLP or understanding of text OK and semantic Web is Mary somewhere depending who you are, you would put it somewhere in that line.",
            "So we're just trying to move that a little bit to the right.",
            "That's all we try to do, OK?"
        ],
        [
            "This is another way of looking at things, so this is a sentence by TS Eliot in the room.",
            "The woman come and go talking of Michelangelo.",
            "That's TS Eliot there.",
            "This is Karen Spark Jones and this is how she started the representing this sentence.",
            "She said, well, let's look at every word and just put in a vector.",
            "And it's a vector of zeros everywhere and every time I see the word in the sentence and put a one.",
            "So as part of the presentation of that vector would be that their room woman can go talking.",
            "Michelangelo and the reason she, the reason she did that is because she did that in the 70s.",
            "And, you know, machines were very small.",
            "Data was very small but never in her wildest dreams.",
            "She thought that 30 years later would still be doing that, but.",
            "That's what we're doing.",
            "So what we're trying to do, and not as a lot of people, have been trying to do this in the last 20 years, and there's a new revival of this and this to actually go beyond this type of representation in text.",
            "So here are examples of things we could do.",
            "We talkin about semantic tagging.",
            "We actually are capable to a fairly reasonable degree of accuracy.",
            "To know that a room is an artifact woman are people come and go is a verbal motion.",
            "Talking is a verbal communication, and Michelangelo is a person.",
            "So maybe we can move away from this pure token based representation to more semantic representations.",
            "In fact, I'll show you in a minute we can go even beyond that.",
            "We can start establishing dependencies, for example, for a firm large number of sentences, we can determine that actually.",
            "The room is a modifier of the verb come and go, and woman is actually the subject of that verb.",
            "OK, and there's a second sentence going on that says that there is an object, Michelangelo, that is being talked about.",
            "So this is not the subject but the object.",
            "OK, and you can think of any combination of that, so you could, if you're an engineer, and you get this and you have to solve a particular task, you may actually choose a number of features in here and say, well, I'm interested in representations of text that have all the proper names in there, because proper names are useful.",
            "On any couple that has some people motion, couple Anna communication person couple so this would be an interesting representation of that.",
            "Text is not more correct or less correct than the others, but perhaps for particular application can be extremely useful WHI because maybe this allows me now too much other people emotion.",
            "OK, certainly that doesn't allow me to match all the people in motion, but this could or maybe other people who are being communicated about and then make a larger could be using some way of context to specify the type of sentences that you're looking for.",
            "OK, so there are several tasks to do this.",
            "One is to actually do this mapping and that's what NLP works.",
            "Very hard to do today and I think to some degree we can already use open source technology that is there, we can download it from the web and it gives us accuracy that is reasonable to this kind of mapping.",
            "Alot of that relies on NLP on it.",
            "Artistically based tagger.",
            "So again machine learning plays a central role.",
            "In that step.",
            "The second step is defining our presentation and that has to do with what is the task that you want to solve.",
            "It doesn't make sense to solve to find the right representation unless you know what it is that you want to do.",
            "OK. And the third step.",
            "Which is something machine learning community sometimes is lazy about or tends to forget is that we have to do this very fast on a very large scale because otherwise is not very useful.",
            "OK.",
            "So all these applications of that is anything that has to do with matching text.",
            "We're basically going beyond matching based on vectors to a more semantic based matching."
        ],
        [
            "So this is another picture and this is maybe the picture."
        ],
        [
            "In the future, we don't have to stop here.",
            "Of course this is.",
            "This is kind of what we can do today, but if you're doing a PhD and you're dreaming of what you should be doing in five years time, this shouldn't be the dream."
        ],
        [
            "We should go beyond that, so you can think of very fancy representations in which sentence is actually connected to many other sentences that have other meanings about those different words and have other relations.",
            "And when I kind of have my own private, wildest dreams, I actually never think of this.",
            "I think of very large continuous spaces.",
            "I think of a sentence or some kind of path in a very complex continuous space, and there's many manifolds describing different meanings.",
            "And at the end of the day, if we eventually move here, we can use all or continuous space machine learning technology to actually understand Texan.",
            "That will be.",
            "Really wonderful.",
            "Oak."
        ],
        [
            "So one of the things that stops us from doing that is this problem of domain dependence.",
            "What is doesn't mean domain dependence and so this is another cartoon of the world in which we have the access here means that dependence on domain and what you see is that for the largest part of what we do in search on the web, it's over here.",
            "It's very large capture.",
            "This would be so large that it wouldn't fit in this building, and it stinks like World Wide Web RSS feeds and blogs.",
            "OK, this is most of the content on the web.",
            "How do we search that?",
            "Basically we use grep to search this OK. We have a few techniques like string matching, so grab plus some waiting like TF IDF.",
            "We have some tricks like hyperlink popularity that tells us a priority what is the importance of a text and then we have a couple of algorithms for data structures like the inverted index that allows us to do this very fast.",
            "And that's where it ends.",
            "So what can we do with this?",
            "Basically we can find relevant strings, so sorry we can find string matches.",
            "That's how we can do on the other side of this domain dependence you have sites like Facebook or Ryan Air which actually have a dialogue with you.",
            "You can explain to Ryan or that you want to go to Paris and you don't want to pay a lot and you like to leave on Monday morning and you explain this to this site and the site find you the right price.",
            "Of course it's so domain dependent.",
            "This explanation this dialogue is done through an interface, it can be completely deconstructed into a set of menus, so then it's fairly easy to do.",
            "OK so again.",
            "We have to make a decision right away.",
            "Do we want to be domain independent and then we don't really need to do NLP or anything like that.",
            "We just need to model the problem.",
            "Do you want to be domain independent?",
            "But if we are independent then can we do anything else besides this?",
            "OK, so again, we're trying to move a little bit to the right."
        ],
        [
            "OK, I had some demos.",
            "Prepare of things that are already happening out there on the web.",
            "For example, there are a number of sites think the most significant significant if that is DB life.",
            "I used to random researchers from the database community.",
            "So this is a site where you can type people in the database community an it on the fly generates a page.",
            "So this page has not been generated by human.",
            "He was generated on the fly.",
            "What it does basically in monitors.",
            "Currently all the news wires, all the lists, all the announcements of the important websites and it uses handcrafted.",
            "Template matching rules to extract information so you can, for example, find out which are the most important things happening about the latest a story happening about Ricardo.",
            "OK, and you may find out that he's actually participated in a particular page.",
            "The presentation in August 6 or things like that.",
            "You can also find related people and this is done by automatically building ranking lists of entities on the graph of connections of people.",
            "OK and the related topics etc.",
            "This can be done because it's been handcrafted.",
            "You basically write a lot of templates and then you hang up these rules.",
            "So this kind of websites cannot take off until we manage to take the machine learning technology to actually generate these kinds of pages for any topic.",
            "For any person without having to write hundreds of hours of template extraction rules OK, but they're already being built, so if we don't do it using machine learning and NLP, somebody will do it by basically writing Perl scripts and lots of rules an that will be the end of the story.",
            "So can we generalize this to a better way to do it?",
            "Another example of this, for example.",
            "Let me see I had to change from my laptop to this computer so I don't control completely.",
            "OK, this is another collection of things happening.",
            "So this is Yahoo answers here.",
            "People are asking questions to each other, so they're asking questions and then somebody loses the questions as well.",
            "I know the answer and answers this question.",
            "This looks ridiculous, but to me Wikipedia looked ridiculous when they explain it to me, they said oh people are going to run it and it's going to be good and I said yeah, whatever.",
            "So this is exactly the same idea people are actually writing very complicated questions and answering each other.",
            "Very complicated questions, you know.",
            "Very high accuracy, very long kind of answers of course is a lot of rubbish questions as well and a lot of rubbish answers, but this kind of data here is available in it's waiting to be given some kind of semantic way of searching.",
            "All we can do today is to hope that people tag the right category in the right question because we beyond string matching we cannot do anything else OK. OK, so let me go back to this."
        ],
        [
            "So let me give you some examples of that extra playing with and we're trying to put out in the public so that other people."
        ],
        [
            "Come play with so this is the road map.",
            "Basically we're starting with tokens and limits and part of speech, and that's fairly well known, so I won't explain what it is and then we are extending that with semantic tags and dependency relations.",
            "Basically, part of speech is the basic syntactic unit that you can get, and this has been there for a long time, but it's actually not very informative semantically, and many people have tried to use it effectively for search but hasn't really gotten anywhere, and the hope is that by moving to more complex structures.",
            "Linguistic perspective, we can get more leverage out of them.",
            "OK, so we took some hypothesis from the beginning.",
            "The first one is that whatever we do, I mean this list could go on and on and on because linguists have been very busy in the last years all the way to some pragmatic model or something like that.",
            "But we said, well, let's do things that we can do linearly in time with respect to text.",
            "That's perhaps an arbitrary decision, but it allows us to actually really do things on large collections.",
            "OK, so for us linear and fast means basically something you can do 5000 words.",
            "Per machine per second.",
            "Just to get an idea of what we're talking about, this is a very rough number, but OK. Actually has a cost and areas that you going to have error print output you cannot hope for clean output.",
            "OK, so from the beginning we said OK doesn't matter.",
            "Let's assume that there's going to be a lot of errors, so a lot of errors is somewhere between 60 and 90% of the tax.",
            "OK, so if you get 99% of the text right then you probably not in that range anymore.",
            "So how do you make up all this information?",
            "That is that you need to do tasks that will encounter as output highly redundant information, because if you rely only on the NLP tagger to find the answer because you have so much error, you will very often get mistakes.",
            "So you have to do something that will encounter lots of information that will allow you to clean the noise and I'll give you some examples of this."
        ],
        [
            "OK.",
            "So this is an example of semantic tagging and this is something that is being done being done for quite awhile in the NLP community, and now there's we have fairly standard statistical Tigers that produce fairly high outputs on particular collection, so I'll give you an example.",
            "So you want to identify instances of categories or relations.",
            "So categories are traditional categories that people are familiar with, things like name entities like a person to know that, that is, we know that that is a noun.",
            "We know it's a proper noun because it starts with capitals, but that's where part of speech ends.",
            "We want to know if it's the noun that refers to a place or a person or a date.",
            "What is it OK and so you can find with fairly high accuracy depending on the collection.",
            "Again on the task, whatever things like persons, locations, quantities and things like that one step beyond that, in some cases, for example in the bio biological domain, you can have very specific for a particular domain tags.",
            "For example here I know that.",
            "Something is a cell, for example, and we're not so much interested in that because as I said, we want to remain in a fairly domain independent world, so you can argue that person is a domain dependent thing because not all domains may care about persons, but at the same time, person, location, date, these are things that probably are useful for a very large number of domains, so that's what we call to some degree domain independent.",
            "OK, then there's things like relations and I'll show you more specific examples, but you can find out relations between entities, so you know that this is a cell.",
            "You know this RNA, but.",
            "It is the relation that links them.",
            "OK, so I'll show you an example of dependency relations that we can accurately get.",
            "Um, so this is back to the domain depends, so we're interested in things like person, substance, artifact communication, motion.",
            "These are things that are priority look like for many domains may be useful.",
            "Maybe you're doing politics or you trying to tag sports pages, or you may be doing.",
            "Pop singers, but these tags actually participate in most of not very sort of technical domains."
        ],
        [
            "So these things are done.",
            "There's different ways to do them, and the way we do them is.",
            "We use statistical taggers.",
            "Basically, this is a standard sequence learning task in which you're trying to map the worse.",
            "To its labels, except there's a bracketing task, so you actually have to know where the label starts and where it ends.",
            "So it's not just the token to token marketing, but it's a basically a bracketed sequence matching problem and.",
            "The kind of models you can use that work fairly well.",
            "There's there are many, and you probably there are people in this room and a lot more about this than me, but the kind of models we're using right now are basically HMM models, or like models that are learned discriminately with average perceptions and things like that.",
            "That kind of features that we're using our focus themselves as part of speech.",
            "The word shape.",
            "So does it start with a capital?",
            "Doesn't end with an S. You can write a number of rules that extract features and their people who know about morphology can help you a lot.",
            "Also, most frequent labels, So what is what is the problem that you're trying to learn?",
            "What?",
            "What are names of people?",
            "If you have a list of names of people, then it should be very easy to just match the names in the text and you're done.",
            "But ambiguity is a huge problem, so if you have a large list of people and you have a large list of places, there will be lots of overlaps.",
            "There will be cities are called Paul.",
            "There will be people that are called Paris.",
            "An ambiguity basically kills the performance to a degree that goes beyond the 60% is so so much noise that you cannot use it.",
            "So the task of this kind of models is to actually disambiguate on the one hand, try to choose the right tag given the right context, and on the other hand gets the label for words that are not in the dictionary at all.",
            "So the first time you see a name like I gave my, I shook hands with Mr. Rhue.",
            "Maybe Drew is not in your list of people, but given the content you should be able to figure out that it is a person.",
            "So these are the two main task here is to disambiguate taranta guess for new words, OK?"
        ],
        [
            "So.",
            "What kind of tags are interesting?",
            "Because there's this name.",
            "Entity tags like person, time, and location that are interesting.",
            "They've been used for quite awhile, but it turns out there's a larger number of tags that are not used very often that are available to us and they have to do with more semantic meanings.",
            "For example, something being an animal or an artifact or not tribute of body, or even more emotional things like something a feeling or communication.",
            "These are tags that are available.",
            "There are collections that have been manually tag so we can learn this kind of sequence models.",
            "OK.",
            "In particular, people tend to concentrate a lot of nouns, but actually, verbs also have tags, and you can actually map verbs through this kind of tax, so you may want to know the verbs available communication.",
            "Because if you have a sentence where somebody communicates something about Paris, you could ask yourself, are there other people communicating things about Paris?",
            "So the only way to do this kind of queries to map from the text to the tag and then do the search?",
            "How much time, OK?",
            "Alright."
        ],
        [
            "So one more step of these are some, so you get some idea.",
            "This is work by the massive baramita in my team using this HMM, average precision, average perceptron model.",
            "And so if you take some first approach you could do is well.",
            "Let's take a word and for all possible meanings that I know off.",
            "Let's take one at random.",
            "OK, that gives you a recall and precision of 42 and 38.",
            "OK, so recall is how many did I find correctly and precision is of those that I said where a particular tag, how many were really that tag?",
            "So this is clearly too low.",
            "One thing you can do, which we call here baseline, is you can say, well, let's take always the most frequent.",
            "So if I see Paris, I ask worn it or my particular ontology or my thesaurus.",
            "OK, what are things that can be?",
            "Paris can be, a city, can be a person.",
            "Well, what's the frequency of Parisa city?",
            "What's the frequency of parties as a person?",
            "So forget about context.",
            "Just look at the frequency that gives you.",
            "Actually reasonable performance is already in the 60 range, OK?",
            "Which is quite quite nice because it means for many things with you may not even have learning data.",
            "You could actually get away with it now if you use this kind of calling sparser techniques in this particular collection, you get.",
            "These are typical results.",
            "So somewhere between 60 and 80, depending on the quality of the tasks that you're using, etc.",
            "OK, so of course fairly noisy, and this is remember on the same collection that was trained well on the test set, but the same input distribution, the same collection.",
            "So if you're moving that to another collection like I'll show you.",
            "Will get worse."
        ],
        [
            "OK, the other level that we are into the representation of the data is dependency parsing and so and there's many types of dependency parsing relations that you can draw, and there's a number of collections and very large training data sets in English, and a number of other languages in which you can try to learn the dependencies between words.",
            "So for example, here I have the sentence John hit the ball with the butt and you have hit as a verb, and you know that John is a noun.",
            "An bullets are now, but you want to know that generally happens to be the subject and ball is the object, so it was joined at hip and not the ball that hit John.",
            "And that may be interesting if you're asking questions like show me companies that were purchased by IBM for example, or companies that are actually purchasing IBM.",
            "These are very different questions and you may be able to use this kind of trees to be simulator.",
            "OK, there's many issues about dependency parsing, and finally another kind of models that you can use to learn this.",
            "This looks like a harder task because it's a tree mapping task, but if you're careful and you only consider trees where you have only terminals in the nodes and you have none projection, so you never jump into forward across the link, then you can use statistical learning model similar to the calling parser where you basically using a stack and you're trying to learn whether award attaches to the right or to the left.",
            "Basically goes to this time.",
            "So again, fairly well known statistical techniques can be used to run this kind of dependency.",
            "Parses on data."
        ],
        [
            "OK, so here's a publicity break, so we realize that.",
            "People want to use this stuff.",
            "Some of our friends want to let's use that, but of course nobody is going to go through the trouble of building all these tools, or at least compiling them because they're open source ANAN.",
            "Running it through lots of data.",
            "So one thing we've done over the last year is we've actually taken all of Wikipedia and we run all these tools one after the other and we made publicly available the output.",
            "OK, so that's why it's a publicity break.",
            "It's just it's not.",
            "A paper is a site in which you can go and download this information an so I can show you.",
            "Yeah.",
            "OK, if I have time and then I'll show you the pages on my laptop, but it's not on this computer, but in there you have information about its size, the number of cells is the number of tokens, etc.",
            "It's all of Wikipedia as of last year and it has all the information that I talked about.",
            "So name entities I think I have a picture here.",
            "OK, in a second.",
            "OK, no picture.",
            "So he has named entities of different kinds like person, location, entity from The Wall Street Journal, semantic tags from Vernon, Supersense Dependency Relations and that kind of thing.",
            "It's very early release.",
            "In future releases, we proved to actually improve the performance an actually provide more linguistically oriented data representations for that acts like an adding another resolution for those who know what is relations, etc.",
            "OK.",
            "So we have this text and we said this very noisy.",
            "So how can we deal with this problem of noisy and there isn't that the way we've been doing this in Barcelona is we've only tackled problems that we knew the answer would be very redundant.",
            "OK, so one of these.",
            "Representations that we used to do that, and it's also available in to download on this website is this thing."
        ],
        [
            "So we've called them entity graphs or anything."
        ],
        [
            "Containment graphs and the idea is very simple is that well, we have a sentence that is like this and it says that graph gives a lecture on entity ranking together with somebody else.",
            "And we've tagged because the collection is that we know that these entities are people.",
            "So we can represent as a bipartite graph.",
            "All the sentences are all the documents and all the entities and connect an entity to a document.",
            "If it's been mentioned it's being contained.",
            "So we have a containment graph, bipartite graph, and the next thing of these types of graphs.",
            "So this is an example of that.",
            "Best public because I get a lot of sentences and three of them are shown here and all the entities and all the dates are shown as type nodes in this graph and they're connected.",
            "If they were mentioned and it's not surprising the public Picasso has an indegree that is higher than any other any other entity, because every sentence is actually trying to mention or is about, and I would guess that many other types of graphs that you can try to build and then we have a paper CMO 7 discussing different graphs.",
            "But the nice thing about this is that is big, so this is the Pablo Picasso graph that I show you a while ago, and this big fat notes here.",
            "Tend to represent very important concepts of Pablo Picasso, and this is not surprising.",
            "OK, so now a lot of the node ranking or relational learning techniques that you may want to try.",
            "A machine learning.",
            "This is, in our opinion, is an interesting data set because you can try a lot of these algorithms and theories on language rather than trying them on the web graph on the publication connection sites here or something like that.",
            "This is text, so this stuff that comes out some degree is more striking or small, closer to knowledge sinoway.",
            "OK, our age is also labeled by anything.",
            "So in the graph that we're providing right now, they're not label.",
            "We just saying contains.",
            "But remember that dependency parsing could give you labels, so you could have subject and object relations etc which you can build yourself from that data.",
            "OK, so I guess what 5 minutes?",
            "Or a woman?",
            "Yeah, OK. Alright."
        ],
        [
            "So this is the information available, but there's a lot more information available on the web and I think it's important that we realize this information is there because if we keep using on the same data sets that people have given us, the number of problems we work on becomes very small and everybody is trying to improve the 0.01% out of their particular task.",
            "And there's a lot of data out there, and it's not surprising that people who are actually building these data.",
            "They look at us in machine learning or information retrieval, and they say, well, you guys, yeah, stuff you do is very complicated, but why don't you use this data?",
            "Why don't you use this data?",
            "For example, you tried to classify pages on Wikipedia and it's very fascinating, but it actually has a category label already.",
            "Provided by people, so to some degree is just a mathematical exercise.",
            "It doesn't really respond to need OK, so these are some examples of data that we're trying to use to either improve our Tigers or improve the representation of the text.",
            "So in the case of Wikipedia, and it's also the case for some other websites using semantic web technology like RDF.",
            "Besides the actual text you have a database like representation of the text.",
            "It has some information with a particular schema.",
            "OK, so this is a page of Alan Turing.",
            "This is the text and the text explains when he was born and what is he but he.",
            "This is natural language.",
            "At the same time, there is a what's called micro formatting Wikipedia, which is basically a relational set of facts, so we know that it's nationalities England, even though it said in the text, they realized the computer is not going to find it, so let's write it as a database.",
            "OK, so you can actually not this back to some degree and actually use these to label or semi label, because there will be mistakes of course to label the Texan now improve your taggers out into the domain we learn, evaluate, do a number of things OK and this data is very easy to achieve."
        ],
        [
            "Fire.",
            "You also have categories, for example for every page which can give you a lot of information about the type of person.",
            "So for example, if you want to know what is the Alan Turing type?",
            "Well if you look at the categories, you can probably learn that it is a person just because of the type of categories there is.",
            "There is not trivial because this is real data by written by humans who don't have a PhD in ontology creation.",
            "So they're just writing stuff.",
            "OK, so the categories of volunteering are very many like English mathematician, but also all sugar runions.",
            "I don't even know what that is and.",
            "English essay, so there's a lot of stuff that has to be cleaned up, but that's what machine learning should be useful for.",
            "We should be able to take this kind of redundancy and extract the information that we need."
        ],
        [
            "OK. OK, one more example of how can we use the Web and API's on the web?",
            "To to do to solve more interesting problems in machine learning, and one of them is to actually use search engines as part of our retrieval as part of our function.",
            "Whatever function you want to compute, you have to think that well, maybe part of that function is an API call to particular search engine, so this is an example again in the same paper where imagine that you want computer similarity in a query in an entity, and one way to do this is to build a graph that I just mentioned to run patron overhead and do something complicated like that.",
            "Very simple, stupid way to do this.",
            "Well, it's not stupid.",
            "Different from how I would do it is to actually.",
            "Hey, it's the actually well take the query an issue, a query to the search engine and then record the set of URLs it comes back.",
            "Now take the entity issue.",
            "The entity to search engine really look at the resource account back.",
            "So in this case the query was life of Pablo Picasso and we want to know general stain is related or not to this OK and so there are a number of.",
            "If you want to set overlap measures that work very well, because this is the web, these are not set there actually rank set.",
            "So if you take into account the ranks, for example by computing the average or reciprocal average rank in this list of entities in this set, you will find that you get a measure that ranks incredibly well.",
            "The entities with respect to the query life of Pablo Picasso.",
            "So if you need to do that, maybe you can do that in an API call to a search engine and then move on to whatever else you wanted to do so.",
            "This is another example."
        ],
        [
            "OK.",
            "So that's all I want to say about textual presentations that can be useful to you.",
            "To create more interesting experiments or moving to new areas of application.",
            "I'll just give you a very quick idea because I have 5 minutes left.",
            "I'll give you a quick idea of one of the issues that we've dealt with.",
            "One of the problems, of course, is the size of it.",
            "You have a lot of data and not only data is now not just sequences, but multiple overlapping bracketed sequences.",
            "An little trees which are this dependency thing.",
            "So what can you do with this?",
            "To do experiments fast?",
            "And it turns out.",
            "You can, you can do a fair amount."
        ],
        [
            "Things so this is typically how the data looks like if you download it from the side.",
            "It just looks like a bunch of sequences like for example the Tories won this election you have the power of speech labels, you have the lemmas, you have the sum of the Tigers.",
            "We have a number of them disguise the ER.",
            "It tells you that this is the beginning of a person beginning of a group of competition.",
            "Then you have dependency relation.",
            "These are the trees remember so it tells you that these are modifier of something that is in position #2 which happens to be Tories Tories is a subject with reposition three.",
            "So position three is 1.",
            "Here it tells you that election is the object of something in position 3, so it's the object of 1.",
            "Basically we serialize the tree here OK, but in this serialized representation is not very useful to do things right."
        ],
        [
            "So.",
            "What people tend to do in the past or there's still doing is they.",
            "They use this inverted index like Lucina.",
            "You take any search engine that you like or lemon for example in the machine learning community and you use that to find the interesting documents that are potentially interesting and then you go and build a representation that is useful for you and run the experiments.",
            "So basically it's like issuing a query to Google getting the top 10 results and then doing experiments on those top 10.",
            "Something that we're often tempted to do.",
            "OK now this is the only way you can do it if the collection is the web but the collection.",
            "Wikipedia that fits in a laptop.",
            "Then you shouldn't be doing this, but you should be doing is actually creating much better indices because the problem of this is that there are two possibilities.",
            "Either the problem was very easy and then the inverted index could solve it by itself, and then this set here contains lots of good answers, or the problem was very hard, and if there's not very good answers here, then no matter what you do, you're not going to find the good answers.",
            "OK, this makes sense.",
            "So what we've been trying to do is to build better very."
        ],
        [
            "Dirty indexes and just to give you an example of that, an inverted index is just a way to encode a sparse matrix.",
            "OK, and typically what you have information retrieval is you have this, you have a document, you have a term and you want to know the score of that term for that document.",
            "So that's a valuable xig and it's just a big matrix, but it's so big that you cannot fit in memory.",
            "So inverted index is just a very cheap way to find all elements of this matrix for which the value is not zero.",
            "That's easy enough, but more Interestingly, it's a very cheap way to find combinations.",
            "So if you want to find any two objects which have those features, non 0.",
            "Then this operation with an operation is extremely cheap, so you can do this.",
            "This is how you type Britney Spears music in Google.",
            "You can get the answer incredibly fast.",
            "Obviously that's a very expensive thing to do because you have to look at every document and find those that don't contain.",
            "Britney don't contain music, and then throw them away.",
            "That could not be linear in the size of the web.",
            "Otherwise we'll never get it back.",
            "It's actually very, very cheap to do it, and it turns out if you use position information then you can do fairly amazing things.",
            "You can actually say.",
            "Well, I have a sparse matrix and for every element that sparse measures have a vector of positions.",
            "So it tells me the word how many times it appears that also in which position in the document appears an inverted index.",
            "Algorithms can actually give you.",
            "Again, you can find 2 features for which the values are non zero an.",
            "There are positions of those two features that are within some range, so that's how you would find all the documents that contain Britney Spears music at a distance of less than 10, for example.",
            "And this you can do not as fast as before, but fairly fast as well.",
            "So actually this is very useful because you can start thinking of you cannot trees into position strings and try to do operations on trees that are very cheap, so."
        ],
        [
            "And with that, the reason you can do that is if you can do this kind of algorithms like the sassy algorithm that can, if you think of all the positions in which word appears, it is a document indexes another position in which Webb Pierce.",
            "Then this algorithm is what they do is they basically have a clever way of skipping until they find a match.",
            "So here 40, because we were at 8:00, and then we have a 40 and there's no point looking at 10:25 because they couldn't be an end here.",
            "So eventually we can find a match in a time that is much smaller than the size of the actual collection or even the size of the possibilities.",
            "And you can use position tricks as well in here to actually find the ranges."
        ],
        [
            "So the way we coded this data is we basically thought of this is undocumented and inverted index and this is sparse matrix that contains these features.",
            "the Tories won this election with their position 12345, but as well as that we create fake words in that position.",
            "So in position one of these documents are in position zero of this document.",
            "Yeah, in possession of this document, I say that there are two words.",
            "There's the, but there's also DT.",
            "And then I say there's another word.",
            "There are three words when it's called Bond.",
            "And then the next word I said their stories and then appears.",
            "But there is also a subject work, so this is going to allow me later to say final documents in which there is a word subject.",
            "There is a word Tori and they are in the same position.",
            "This equivalent to say find it or is there is a subject.",
            "And if you rewrite your tree using position tricks like this, you can actually find you can compute very restricted limited number of tree distances on very large scale very very fast."
        ],
        [
            "And just to end show you an example of that.",
            "Oh, I can't because it's in the laptop and."
        ],
        [
            "OK, but I think it's."
        ],
        [
            "Time to win anyway, so I'll stop there.",
            "Question.",
            "Yes, I'm still not very clear how those semantic tags could be used for the search engine.",
            "I mean, user was still type short keywords unless user type, natural language questions or are you expecting user too?",
            "Ask natural language questions.",
            "Savior tax will actually help you to search better, so I think if I type Britney Spears music into search engine, the problem is being solved and we should not try to use semantics.",
            "The problem is solved.",
            "OK, so I'm not interested in three world queries in our search engine.",
            "I'm interesting in other problems that go beyond that, so I tried to give you some ideas of that.",
            "This is one of them.",
            "For example, how do I find related people to a query?",
            "If you're in Wikipedia, you may be able to write a fairly long query like you know the you know.",
            "US Civil War cannon usage.",
            "You're in a particular domain you're willing to write long queries or longest queries, or maybe short queries, But you're going to interact by clicking or doing something, so that's the kind of problems I'm looking at.",
            "The other example I gave is in Yahoo answers, for example.",
            "People are spending hours answering questions, so maybe they're willing to spend minutes finding them so they will have much richer representations of text.",
            "Another one that I didn't talk about is Yahoo Pipes so."
        ],
        [
            "We go back here, but it doesn't matter.",
            "I never think of users writing queries that are very complex, but maybe programmers, for example, could be writing queries that are very complex.",
            "So when you write a website, some parts of your website may actually be very complex queries that are running against the server.",
            "Example finding related people or imagine that you have a site about.",
            "Particular tennis player and you say, well, I like to go and get all the relevant places where this tennis player has been and then find pictures in Flickr so I can decorate my website.",
            "This kind of thing human will never do better.",
            "Programmer is willing to learn enough to do it, hopefully.",
            "That's that's the hope.",
            "Say something about a problem.",
            "I notice things like context, automatic summarization and that was if you have cover message to to produce a nice hot.",
            "And if you don't have a good message evaluation methods you have kind of mismatch.",
            "So if you use syntactic measures or simple minded measures and evaluation then you may not actually notice the difference between good or bad output or satisfactory answer.",
            "I suspect this also arises here so.",
            "Yeah, so that's a huge problem.",
            "In fact, the first thing we did was we develop.",
            "We did this task, we thought, OK, let's do a particular very difficult task and it was the idea.",
            "The problem of ranking important dates with the spectral query.",
            "So what are the most important dates with respect to query?",
            "And we thought this is a very difficult task.",
            "So first you have to find a difficult task, because if it's easy, as you say, very simple algorithms will do it.",
            "But then you have to evaluate it and that's quite hard.",
            "I don't think there's been any advances there.",
            "You basically have to do the work you have to find a difficult task.",
            "You have to ask people to evaluate manually.",
            "So in our lab we regularly ask people to manually evaluate things.",
            "Ann and there you have to hope that the task is hard enough and evaluation is good enough so that you can show improvement.",
            "It's hard and I don't think.",
            "There's a way around it, but you have to go through it, yeah?",
            "Question.",
            "OK, thank you very much.",
            "There's a. I can't hear you.",
            "Example with scriptions of people right by the fictional figure out that actually other pictures on Flickr, Esther.",
            "So what's the question?",
            "ID.",
            "I think you can use it in many many ways and so it could be that that's a very good way to use it, I think, yeah.",
            "Well, I think we have to stop because we have to sit on the so thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last time.",
                    "label": 0
                },
                {
                    "sent": "Previously at Microsoft Research Lab in Cambridge, better slowly.",
                    "label": 0
                },
                {
                    "sent": "Paris.",
                    "label": 0
                },
                {
                    "sent": "I made some pleasure for me.",
                    "label": 0
                },
                {
                    "sent": "He's going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "Thanks mining and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Beyond string search.",
                    "label": 0
                },
                {
                    "sent": "Fast and accurate retrieval of entities and developers.",
                    "label": 0
                },
                {
                    "sent": "Thank you, what time should I finish the talk, you think?",
                    "label": 0
                },
                {
                    "sent": "OK 20.",
                    "label": 0
                },
                {
                    "sent": "So it's 20 just OK. Alright so.",
                    "label": 0
                },
                {
                    "sent": "Not this one.",
                    "label": 0
                },
                {
                    "sent": "This one here.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll be talking about this.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like cheating.",
                    "label": 0
                },
                {
                    "sent": "Looks like Afghan this.",
                    "label": 0
                },
                {
                    "sent": "I've done fast and accurate retrieval of entities and dependencies.",
                    "label": 1
                },
                {
                    "sent": "Where it says that is what I want to do.",
                    "label": 0
                },
                {
                    "sent": "I want to have fast and accurate retrieval Avengers and dependencies.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "OK. Really OK.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Maybe if I speak louder can you hear me at the back, I mean.",
                    "label": 0
                },
                {
                    "sent": "The song yeah OK. Feel like Britney Spears.",
                    "label": 0
                },
                {
                    "sent": "No aspect in a very limited way, but OK.",
                    "label": 0
                },
                {
                    "sent": "So because this talk is in the enterprise track.",
                    "label": 0
                },
                {
                    "sent": "I thought I want uses excuse to present my work even though it wasn't submitted.",
                    "label": 0
                },
                {
                    "sent": "An peer reviewed an, so I'm going to do something different when I'm going to try to show you is actually a view of some of the problems that we're dealing in Yahoo Research Barcelona and specifically some of the data that we're using because it is.",
                    "label": 0
                },
                {
                    "sent": "It is our hope that people who know a lot about machine learning, like the people sitting in this room here, will take this data and do things with it that we couldn't even think was possible to do so.",
                    "label": 0
                },
                {
                    "sent": "Instead of showing you.",
                    "label": 0
                },
                {
                    "sent": "Particular results or algorithms?",
                    "label": 0
                },
                {
                    "sent": "I will show you data that we're trying to build and make available to people to do experiments and problems that are maybe.",
                    "label": 0
                },
                {
                    "sent": "Interesting to deal with.",
                    "label": 0
                },
                {
                    "sent": "OK, this is work with the people working at Yahoo Research Lab in Barcelona as well as you separate, RDN hanging rotate.",
                    "label": 0
                },
                {
                    "sent": "Rd is a intern and you said they spent a year with us in Barcelona.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have two goals.",
                    "label": 0
                },
                {
                    "sent": "I will tell you whether a research goes, why are we trying to do this and then give you ideas of how you can go about representing text that goes beyond bags of words or matrices, representations OK. And then I want.",
                    "label": 0
                },
                {
                    "sent": "I don't think I have any more time, but if I have more time, I'll go into actually solutions that we are producing in Barcelona.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the document understanding cartoon why are we doing these things?",
                    "label": 1
                },
                {
                    "sent": "It is our feeling that this is how the document understanding worlds looks like at the left you have grab those who don't know who is crap is just a search string.",
                    "label": 0
                },
                {
                    "sent": "Search verifies unique string search OK and on the right you have this ultimate goal of a domain expert at your fingertips.",
                    "label": 0
                },
                {
                    "sent": "Somebody who knows the topics very very well and can talk to you knows you, knows what you've read OK and so.",
                    "label": 0
                },
                {
                    "sent": "Search engines today are dangerously close to grep there really, just very fast, very large scale grep OK, except for a few features are coming out in the last few years like you type a hotel room and then you actually get the price of the night.",
                    "label": 0
                },
                {
                    "sent": "These things are actually going beyond grip and understanding the query, but for the most part we're here Now this question answering a question answering.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a proof of concept.",
                    "label": 0
                },
                {
                    "sent": "There are people using NLP to do fairly incredible things like answering questions that looked impossible to answer before, so there is this idea that we could.",
                    "label": 0
                },
                {
                    "sent": "Certainly move beyond search engines and beyond grab towards more NLP or understanding of text OK and semantic Web is Mary somewhere depending who you are, you would put it somewhere in that line.",
                    "label": 0
                },
                {
                    "sent": "So we're just trying to move that a little bit to the right.",
                    "label": 0
                },
                {
                    "sent": "That's all we try to do, OK?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is another way of looking at things, so this is a sentence by TS Eliot in the room.",
                    "label": 0
                },
                {
                    "sent": "The woman come and go talking of Michelangelo.",
                    "label": 1
                },
                {
                    "sent": "That's TS Eliot there.",
                    "label": 0
                },
                {
                    "sent": "This is Karen Spark Jones and this is how she started the representing this sentence.",
                    "label": 0
                },
                {
                    "sent": "She said, well, let's look at every word and just put in a vector.",
                    "label": 0
                },
                {
                    "sent": "And it's a vector of zeros everywhere and every time I see the word in the sentence and put a one.",
                    "label": 0
                },
                {
                    "sent": "So as part of the presentation of that vector would be that their room woman can go talking.",
                    "label": 0
                },
                {
                    "sent": "Michelangelo and the reason she, the reason she did that is because she did that in the 70s.",
                    "label": 0
                },
                {
                    "sent": "And, you know, machines were very small.",
                    "label": 0
                },
                {
                    "sent": "Data was very small but never in her wildest dreams.",
                    "label": 0
                },
                {
                    "sent": "She thought that 30 years later would still be doing that, but.",
                    "label": 0
                },
                {
                    "sent": "That's what we're doing.",
                    "label": 0
                },
                {
                    "sent": "So what we're trying to do, and not as a lot of people, have been trying to do this in the last 20 years, and there's a new revival of this and this to actually go beyond this type of representation in text.",
                    "label": 0
                },
                {
                    "sent": "So here are examples of things we could do.",
                    "label": 0
                },
                {
                    "sent": "We talkin about semantic tagging.",
                    "label": 0
                },
                {
                    "sent": "We actually are capable to a fairly reasonable degree of accuracy.",
                    "label": 0
                },
                {
                    "sent": "To know that a room is an artifact woman are people come and go is a verbal motion.",
                    "label": 0
                },
                {
                    "sent": "Talking is a verbal communication, and Michelangelo is a person.",
                    "label": 0
                },
                {
                    "sent": "So maybe we can move away from this pure token based representation to more semantic representations.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'll show you in a minute we can go even beyond that.",
                    "label": 0
                },
                {
                    "sent": "We can start establishing dependencies, for example, for a firm large number of sentences, we can determine that actually.",
                    "label": 1
                },
                {
                    "sent": "The room is a modifier of the verb come and go, and woman is actually the subject of that verb.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's a second sentence going on that says that there is an object, Michelangelo, that is being talked about.",
                    "label": 0
                },
                {
                    "sent": "So this is not the subject but the object.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can think of any combination of that, so you could, if you're an engineer, and you get this and you have to solve a particular task, you may actually choose a number of features in here and say, well, I'm interested in representations of text that have all the proper names in there, because proper names are useful.",
                    "label": 0
                },
                {
                    "sent": "On any couple that has some people motion, couple Anna communication person couple so this would be an interesting representation of that.",
                    "label": 0
                },
                {
                    "sent": "Text is not more correct or less correct than the others, but perhaps for particular application can be extremely useful WHI because maybe this allows me now too much other people emotion.",
                    "label": 0
                },
                {
                    "sent": "OK, certainly that doesn't allow me to match all the people in motion, but this could or maybe other people who are being communicated about and then make a larger could be using some way of context to specify the type of sentences that you're looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are several tasks to do this.",
                    "label": 0
                },
                {
                    "sent": "One is to actually do this mapping and that's what NLP works.",
                    "label": 0
                },
                {
                    "sent": "Very hard to do today and I think to some degree we can already use open source technology that is there, we can download it from the web and it gives us accuracy that is reasonable to this kind of mapping.",
                    "label": 0
                },
                {
                    "sent": "Alot of that relies on NLP on it.",
                    "label": 0
                },
                {
                    "sent": "Artistically based tagger.",
                    "label": 0
                },
                {
                    "sent": "So again machine learning plays a central role.",
                    "label": 0
                },
                {
                    "sent": "In that step.",
                    "label": 0
                },
                {
                    "sent": "The second step is defining our presentation and that has to do with what is the task that you want to solve.",
                    "label": 0
                },
                {
                    "sent": "It doesn't make sense to solve to find the right representation unless you know what it is that you want to do.",
                    "label": 0
                },
                {
                    "sent": "OK. And the third step.",
                    "label": 0
                },
                {
                    "sent": "Which is something machine learning community sometimes is lazy about or tends to forget is that we have to do this very fast on a very large scale because otherwise is not very useful.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So all these applications of that is anything that has to do with matching text.",
                    "label": 0
                },
                {
                    "sent": "We're basically going beyond matching based on vectors to a more semantic based matching.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is another picture and this is maybe the picture.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the future, we don't have to stop here.",
                    "label": 0
                },
                {
                    "sent": "Of course this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of what we can do today, but if you're doing a PhD and you're dreaming of what you should be doing in five years time, this shouldn't be the dream.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We should go beyond that, so you can think of very fancy representations in which sentence is actually connected to many other sentences that have other meanings about those different words and have other relations.",
                    "label": 0
                },
                {
                    "sent": "And when I kind of have my own private, wildest dreams, I actually never think of this.",
                    "label": 0
                },
                {
                    "sent": "I think of very large continuous spaces.",
                    "label": 0
                },
                {
                    "sent": "I think of a sentence or some kind of path in a very complex continuous space, and there's many manifolds describing different meanings.",
                    "label": 0
                },
                {
                    "sent": "And at the end of the day, if we eventually move here, we can use all or continuous space machine learning technology to actually understand Texan.",
                    "label": 0
                },
                {
                    "sent": "That will be.",
                    "label": 0
                },
                {
                    "sent": "Really wonderful.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the things that stops us from doing that is this problem of domain dependence.",
                    "label": 0
                },
                {
                    "sent": "What is doesn't mean domain dependence and so this is another cartoon of the world in which we have the access here means that dependence on domain and what you see is that for the largest part of what we do in search on the web, it's over here.",
                    "label": 0
                },
                {
                    "sent": "It's very large capture.",
                    "label": 0
                },
                {
                    "sent": "This would be so large that it wouldn't fit in this building, and it stinks like World Wide Web RSS feeds and blogs.",
                    "label": 1
                },
                {
                    "sent": "OK, this is most of the content on the web.",
                    "label": 0
                },
                {
                    "sent": "How do we search that?",
                    "label": 0
                },
                {
                    "sent": "Basically we use grep to search this OK. We have a few techniques like string matching, so grab plus some waiting like TF IDF.",
                    "label": 0
                },
                {
                    "sent": "We have some tricks like hyperlink popularity that tells us a priority what is the importance of a text and then we have a couple of algorithms for data structures like the inverted index that allows us to do this very fast.",
                    "label": 0
                },
                {
                    "sent": "And that's where it ends.",
                    "label": 0
                },
                {
                    "sent": "So what can we do with this?",
                    "label": 0
                },
                {
                    "sent": "Basically we can find relevant strings, so sorry we can find string matches.",
                    "label": 1
                },
                {
                    "sent": "That's how we can do on the other side of this domain dependence you have sites like Facebook or Ryan Air which actually have a dialogue with you.",
                    "label": 0
                },
                {
                    "sent": "You can explain to Ryan or that you want to go to Paris and you don't want to pay a lot and you like to leave on Monday morning and you explain this to this site and the site find you the right price.",
                    "label": 0
                },
                {
                    "sent": "Of course it's so domain dependent.",
                    "label": 1
                },
                {
                    "sent": "This explanation this dialogue is done through an interface, it can be completely deconstructed into a set of menus, so then it's fairly easy to do.",
                    "label": 0
                },
                {
                    "sent": "OK so again.",
                    "label": 0
                },
                {
                    "sent": "We have to make a decision right away.",
                    "label": 0
                },
                {
                    "sent": "Do we want to be domain independent and then we don't really need to do NLP or anything like that.",
                    "label": 0
                },
                {
                    "sent": "We just need to model the problem.",
                    "label": 1
                },
                {
                    "sent": "Do you want to be domain independent?",
                    "label": 0
                },
                {
                    "sent": "But if we are independent then can we do anything else besides this?",
                    "label": 0
                },
                {
                    "sent": "OK, so again, we're trying to move a little bit to the right.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I had some demos.",
                    "label": 0
                },
                {
                    "sent": "Prepare of things that are already happening out there on the web.",
                    "label": 0
                },
                {
                    "sent": "For example, there are a number of sites think the most significant significant if that is DB life.",
                    "label": 0
                },
                {
                    "sent": "I used to random researchers from the database community.",
                    "label": 0
                },
                {
                    "sent": "So this is a site where you can type people in the database community an it on the fly generates a page.",
                    "label": 0
                },
                {
                    "sent": "So this page has not been generated by human.",
                    "label": 0
                },
                {
                    "sent": "He was generated on the fly.",
                    "label": 0
                },
                {
                    "sent": "What it does basically in monitors.",
                    "label": 0
                },
                {
                    "sent": "Currently all the news wires, all the lists, all the announcements of the important websites and it uses handcrafted.",
                    "label": 0
                },
                {
                    "sent": "Template matching rules to extract information so you can, for example, find out which are the most important things happening about the latest a story happening about Ricardo.",
                    "label": 0
                },
                {
                    "sent": "OK, and you may find out that he's actually participated in a particular page.",
                    "label": 0
                },
                {
                    "sent": "The presentation in August 6 or things like that.",
                    "label": 0
                },
                {
                    "sent": "You can also find related people and this is done by automatically building ranking lists of entities on the graph of connections of people.",
                    "label": 0
                },
                {
                    "sent": "OK and the related topics etc.",
                    "label": 0
                },
                {
                    "sent": "This can be done because it's been handcrafted.",
                    "label": 0
                },
                {
                    "sent": "You basically write a lot of templates and then you hang up these rules.",
                    "label": 0
                },
                {
                    "sent": "So this kind of websites cannot take off until we manage to take the machine learning technology to actually generate these kinds of pages for any topic.",
                    "label": 0
                },
                {
                    "sent": "For any person without having to write hundreds of hours of template extraction rules OK, but they're already being built, so if we don't do it using machine learning and NLP, somebody will do it by basically writing Perl scripts and lots of rules an that will be the end of the story.",
                    "label": 0
                },
                {
                    "sent": "So can we generalize this to a better way to do it?",
                    "label": 0
                },
                {
                    "sent": "Another example of this, for example.",
                    "label": 0
                },
                {
                    "sent": "Let me see I had to change from my laptop to this computer so I don't control completely.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another collection of things happening.",
                    "label": 0
                },
                {
                    "sent": "So this is Yahoo answers here.",
                    "label": 1
                },
                {
                    "sent": "People are asking questions to each other, so they're asking questions and then somebody loses the questions as well.",
                    "label": 0
                },
                {
                    "sent": "I know the answer and answers this question.",
                    "label": 0
                },
                {
                    "sent": "This looks ridiculous, but to me Wikipedia looked ridiculous when they explain it to me, they said oh people are going to run it and it's going to be good and I said yeah, whatever.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the same idea people are actually writing very complicated questions and answering each other.",
                    "label": 0
                },
                {
                    "sent": "Very complicated questions, you know.",
                    "label": 0
                },
                {
                    "sent": "Very high accuracy, very long kind of answers of course is a lot of rubbish questions as well and a lot of rubbish answers, but this kind of data here is available in it's waiting to be given some kind of semantic way of searching.",
                    "label": 0
                },
                {
                    "sent": "All we can do today is to hope that people tag the right category in the right question because we beyond string matching we cannot do anything else OK. OK, so let me go back to this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give you some examples of that extra playing with and we're trying to put out in the public so that other people.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come play with so this is the road map.",
                    "label": 0
                },
                {
                    "sent": "Basically we're starting with tokens and limits and part of speech, and that's fairly well known, so I won't explain what it is and then we are extending that with semantic tags and dependency relations.",
                    "label": 1
                },
                {
                    "sent": "Basically, part of speech is the basic syntactic unit that you can get, and this has been there for a long time, but it's actually not very informative semantically, and many people have tried to use it effectively for search but hasn't really gotten anywhere, and the hope is that by moving to more complex structures.",
                    "label": 0
                },
                {
                    "sent": "Linguistic perspective, we can get more leverage out of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so we took some hypothesis from the beginning.",
                    "label": 0
                },
                {
                    "sent": "The first one is that whatever we do, I mean this list could go on and on and on because linguists have been very busy in the last years all the way to some pragmatic model or something like that.",
                    "label": 0
                },
                {
                    "sent": "But we said, well, let's do things that we can do linearly in time with respect to text.",
                    "label": 0
                },
                {
                    "sent": "That's perhaps an arbitrary decision, but it allows us to actually really do things on large collections.",
                    "label": 0
                },
                {
                    "sent": "OK, so for us linear and fast means basically something you can do 5000 words.",
                    "label": 0
                },
                {
                    "sent": "Per machine per second.",
                    "label": 0
                },
                {
                    "sent": "Just to get an idea of what we're talking about, this is a very rough number, but OK. Actually has a cost and areas that you going to have error print output you cannot hope for clean output.",
                    "label": 0
                },
                {
                    "sent": "OK, so from the beginning we said OK doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that there's going to be a lot of errors, so a lot of errors is somewhere between 60 and 90% of the tax.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you get 99% of the text right then you probably not in that range anymore.",
                    "label": 0
                },
                {
                    "sent": "So how do you make up all this information?",
                    "label": 1
                },
                {
                    "sent": "That is that you need to do tasks that will encounter as output highly redundant information, because if you rely only on the NLP tagger to find the answer because you have so much error, you will very often get mistakes.",
                    "label": 0
                },
                {
                    "sent": "So you have to do something that will encounter lots of information that will allow you to clean the noise and I'll give you some examples of this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of semantic tagging and this is something that is being done being done for quite awhile in the NLP community, and now there's we have fairly standard statistical Tigers that produce fairly high outputs on particular collection, so I'll give you an example.",
                    "label": 1
                },
                {
                    "sent": "So you want to identify instances of categories or relations.",
                    "label": 1
                },
                {
                    "sent": "So categories are traditional categories that people are familiar with, things like name entities like a person to know that, that is, we know that that is a noun.",
                    "label": 0
                },
                {
                    "sent": "We know it's a proper noun because it starts with capitals, but that's where part of speech ends.",
                    "label": 0
                },
                {
                    "sent": "We want to know if it's the noun that refers to a place or a person or a date.",
                    "label": 0
                },
                {
                    "sent": "What is it OK and so you can find with fairly high accuracy depending on the collection.",
                    "label": 0
                },
                {
                    "sent": "Again on the task, whatever things like persons, locations, quantities and things like that one step beyond that, in some cases, for example in the bio biological domain, you can have very specific for a particular domain tags.",
                    "label": 0
                },
                {
                    "sent": "For example here I know that.",
                    "label": 0
                },
                {
                    "sent": "Something is a cell, for example, and we're not so much interested in that because as I said, we want to remain in a fairly domain independent world, so you can argue that person is a domain dependent thing because not all domains may care about persons, but at the same time, person, location, date, these are things that probably are useful for a very large number of domains, so that's what we call to some degree domain independent.",
                    "label": 0
                },
                {
                    "sent": "OK, then there's things like relations and I'll show you more specific examples, but you can find out relations between entities, so you know that this is a cell.",
                    "label": 0
                },
                {
                    "sent": "You know this RNA, but.",
                    "label": 0
                },
                {
                    "sent": "It is the relation that links them.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll show you an example of dependency relations that we can accurately get.",
                    "label": 0
                },
                {
                    "sent": "Um, so this is back to the domain depends, so we're interested in things like person, substance, artifact communication, motion.",
                    "label": 0
                },
                {
                    "sent": "These are things that are priority look like for many domains may be useful.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're doing politics or you trying to tag sports pages, or you may be doing.",
                    "label": 0
                },
                {
                    "sent": "Pop singers, but these tags actually participate in most of not very sort of technical domains.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these things are done.",
                    "label": 0
                },
                {
                    "sent": "There's different ways to do them, and the way we do them is.",
                    "label": 0
                },
                {
                    "sent": "We use statistical taggers.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is a standard sequence learning task in which you're trying to map the worse.",
                    "label": 0
                },
                {
                    "sent": "To its labels, except there's a bracketing task, so you actually have to know where the label starts and where it ends.",
                    "label": 0
                },
                {
                    "sent": "So it's not just the token to token marketing, but it's a basically a bracketed sequence matching problem and.",
                    "label": 0
                },
                {
                    "sent": "The kind of models you can use that work fairly well.",
                    "label": 0
                },
                {
                    "sent": "There's there are many, and you probably there are people in this room and a lot more about this than me, but the kind of models we're using right now are basically HMM models, or like models that are learned discriminately with average perceptions and things like that.",
                    "label": 0
                },
                {
                    "sent": "That kind of features that we're using our focus themselves as part of speech.",
                    "label": 0
                },
                {
                    "sent": "The word shape.",
                    "label": 0
                },
                {
                    "sent": "So does it start with a capital?",
                    "label": 0
                },
                {
                    "sent": "Doesn't end with an S. You can write a number of rules that extract features and their people who know about morphology can help you a lot.",
                    "label": 0
                },
                {
                    "sent": "Also, most frequent labels, So what is what is the problem that you're trying to learn?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What are names of people?",
                    "label": 0
                },
                {
                    "sent": "If you have a list of names of people, then it should be very easy to just match the names in the text and you're done.",
                    "label": 0
                },
                {
                    "sent": "But ambiguity is a huge problem, so if you have a large list of people and you have a large list of places, there will be lots of overlaps.",
                    "label": 0
                },
                {
                    "sent": "There will be cities are called Paul.",
                    "label": 0
                },
                {
                    "sent": "There will be people that are called Paris.",
                    "label": 0
                },
                {
                    "sent": "An ambiguity basically kills the performance to a degree that goes beyond the 60% is so so much noise that you cannot use it.",
                    "label": 0
                },
                {
                    "sent": "So the task of this kind of models is to actually disambiguate on the one hand, try to choose the right tag given the right context, and on the other hand gets the label for words that are not in the dictionary at all.",
                    "label": 0
                },
                {
                    "sent": "So the first time you see a name like I gave my, I shook hands with Mr. Rhue.",
                    "label": 0
                },
                {
                    "sent": "Maybe Drew is not in your list of people, but given the content you should be able to figure out that it is a person.",
                    "label": 0
                },
                {
                    "sent": "So these are the two main task here is to disambiguate taranta guess for new words, OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What kind of tags are interesting?",
                    "label": 0
                },
                {
                    "sent": "Because there's this name.",
                    "label": 0
                },
                {
                    "sent": "Entity tags like person, time, and location that are interesting.",
                    "label": 0
                },
                {
                    "sent": "They've been used for quite awhile, but it turns out there's a larger number of tags that are not used very often that are available to us and they have to do with more semantic meanings.",
                    "label": 0
                },
                {
                    "sent": "For example, something being an animal or an artifact or not tribute of body, or even more emotional things like something a feeling or communication.",
                    "label": 0
                },
                {
                    "sent": "These are tags that are available.",
                    "label": 0
                },
                {
                    "sent": "There are collections that have been manually tag so we can learn this kind of sequence models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In particular, people tend to concentrate a lot of nouns, but actually, verbs also have tags, and you can actually map verbs through this kind of tax, so you may want to know the verbs available communication.",
                    "label": 0
                },
                {
                    "sent": "Because if you have a sentence where somebody communicates something about Paris, you could ask yourself, are there other people communicating things about Paris?",
                    "label": 0
                },
                {
                    "sent": "So the only way to do this kind of queries to map from the text to the tag and then do the search?",
                    "label": 0
                },
                {
                    "sent": "How much time, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one more step of these are some, so you get some idea.",
                    "label": 0
                },
                {
                    "sent": "This is work by the massive baramita in my team using this HMM, average precision, average perceptron model.",
                    "label": 0
                },
                {
                    "sent": "And so if you take some first approach you could do is well.",
                    "label": 0
                },
                {
                    "sent": "Let's take a word and for all possible meanings that I know off.",
                    "label": 0
                },
                {
                    "sent": "Let's take one at random.",
                    "label": 0
                },
                {
                    "sent": "OK, that gives you a recall and precision of 42 and 38.",
                    "label": 0
                },
                {
                    "sent": "OK, so recall is how many did I find correctly and precision is of those that I said where a particular tag, how many were really that tag?",
                    "label": 0
                },
                {
                    "sent": "So this is clearly too low.",
                    "label": 0
                },
                {
                    "sent": "One thing you can do, which we call here baseline, is you can say, well, let's take always the most frequent.",
                    "label": 0
                },
                {
                    "sent": "So if I see Paris, I ask worn it or my particular ontology or my thesaurus.",
                    "label": 0
                },
                {
                    "sent": "OK, what are things that can be?",
                    "label": 0
                },
                {
                    "sent": "Paris can be, a city, can be a person.",
                    "label": 0
                },
                {
                    "sent": "Well, what's the frequency of Parisa city?",
                    "label": 0
                },
                {
                    "sent": "What's the frequency of parties as a person?",
                    "label": 0
                },
                {
                    "sent": "So forget about context.",
                    "label": 0
                },
                {
                    "sent": "Just look at the frequency that gives you.",
                    "label": 0
                },
                {
                    "sent": "Actually reasonable performance is already in the 60 range, OK?",
                    "label": 0
                },
                {
                    "sent": "Which is quite quite nice because it means for many things with you may not even have learning data.",
                    "label": 0
                },
                {
                    "sent": "You could actually get away with it now if you use this kind of calling sparser techniques in this particular collection, you get.",
                    "label": 0
                },
                {
                    "sent": "These are typical results.",
                    "label": 0
                },
                {
                    "sent": "So somewhere between 60 and 80, depending on the quality of the tasks that you're using, etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course fairly noisy, and this is remember on the same collection that was trained well on the test set, but the same input distribution, the same collection.",
                    "label": 0
                },
                {
                    "sent": "So if you're moving that to another collection like I'll show you.",
                    "label": 0
                },
                {
                    "sent": "Will get worse.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the other level that we are into the representation of the data is dependency parsing and so and there's many types of dependency parsing relations that you can draw, and there's a number of collections and very large training data sets in English, and a number of other languages in which you can try to learn the dependencies between words.",
                    "label": 0
                },
                {
                    "sent": "So for example, here I have the sentence John hit the ball with the butt and you have hit as a verb, and you know that John is a noun.",
                    "label": 1
                },
                {
                    "sent": "An bullets are now, but you want to know that generally happens to be the subject and ball is the object, so it was joined at hip and not the ball that hit John.",
                    "label": 0
                },
                {
                    "sent": "And that may be interesting if you're asking questions like show me companies that were purchased by IBM for example, or companies that are actually purchasing IBM.",
                    "label": 0
                },
                {
                    "sent": "These are very different questions and you may be able to use this kind of trees to be simulator.",
                    "label": 0
                },
                {
                    "sent": "OK, there's many issues about dependency parsing, and finally another kind of models that you can use to learn this.",
                    "label": 0
                },
                {
                    "sent": "This looks like a harder task because it's a tree mapping task, but if you're careful and you only consider trees where you have only terminals in the nodes and you have none projection, so you never jump into forward across the link, then you can use statistical learning model similar to the calling parser where you basically using a stack and you're trying to learn whether award attaches to the right or to the left.",
                    "label": 0
                },
                {
                    "sent": "Basically goes to this time.",
                    "label": 0
                },
                {
                    "sent": "So again, fairly well known statistical techniques can be used to run this kind of dependency.",
                    "label": 0
                },
                {
                    "sent": "Parses on data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's a publicity break, so we realize that.",
                    "label": 0
                },
                {
                    "sent": "People want to use this stuff.",
                    "label": 0
                },
                {
                    "sent": "Some of our friends want to let's use that, but of course nobody is going to go through the trouble of building all these tools, or at least compiling them because they're open source ANAN.",
                    "label": 1
                },
                {
                    "sent": "Running it through lots of data.",
                    "label": 0
                },
                {
                    "sent": "So one thing we've done over the last year is we've actually taken all of Wikipedia and we run all these tools one after the other and we made publicly available the output.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why it's a publicity break.",
                    "label": 1
                },
                {
                    "sent": "It's just it's not.",
                    "label": 0
                },
                {
                    "sent": "A paper is a site in which you can go and download this information an so I can show you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, if I have time and then I'll show you the pages on my laptop, but it's not on this computer, but in there you have information about its size, the number of cells is the number of tokens, etc.",
                    "label": 0
                },
                {
                    "sent": "It's all of Wikipedia as of last year and it has all the information that I talked about.",
                    "label": 0
                },
                {
                    "sent": "So name entities I think I have a picture here.",
                    "label": 0
                },
                {
                    "sent": "OK, in a second.",
                    "label": 0
                },
                {
                    "sent": "OK, no picture.",
                    "label": 0
                },
                {
                    "sent": "So he has named entities of different kinds like person, location, entity from The Wall Street Journal, semantic tags from Vernon, Supersense Dependency Relations and that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "It's very early release.",
                    "label": 1
                },
                {
                    "sent": "In future releases, we proved to actually improve the performance an actually provide more linguistically oriented data representations for that acts like an adding another resolution for those who know what is relations, etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have this text and we said this very noisy.",
                    "label": 0
                },
                {
                    "sent": "So how can we deal with this problem of noisy and there isn't that the way we've been doing this in Barcelona is we've only tackled problems that we knew the answer would be very redundant.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of these.",
                    "label": 0
                },
                {
                    "sent": "Representations that we used to do that, and it's also available in to download on this website is this thing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've called them entity graphs or anything.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Containment graphs and the idea is very simple is that well, we have a sentence that is like this and it says that graph gives a lecture on entity ranking together with somebody else.",
                    "label": 0
                },
                {
                    "sent": "And we've tagged because the collection is that we know that these entities are people.",
                    "label": 0
                },
                {
                    "sent": "So we can represent as a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "All the sentences are all the documents and all the entities and connect an entity to a document.",
                    "label": 0
                },
                {
                    "sent": "If it's been mentioned it's being contained.",
                    "label": 0
                },
                {
                    "sent": "So we have a containment graph, bipartite graph, and the next thing of these types of graphs.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of that.",
                    "label": 0
                },
                {
                    "sent": "Best public because I get a lot of sentences and three of them are shown here and all the entities and all the dates are shown as type nodes in this graph and they're connected.",
                    "label": 0
                },
                {
                    "sent": "If they were mentioned and it's not surprising the public Picasso has an indegree that is higher than any other any other entity, because every sentence is actually trying to mention or is about, and I would guess that many other types of graphs that you can try to build and then we have a paper CMO 7 discussing different graphs.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about this is that is big, so this is the Pablo Picasso graph that I show you a while ago, and this big fat notes here.",
                    "label": 0
                },
                {
                    "sent": "Tend to represent very important concepts of Pablo Picasso, and this is not surprising.",
                    "label": 0
                },
                {
                    "sent": "OK, so now a lot of the node ranking or relational learning techniques that you may want to try.",
                    "label": 0
                },
                {
                    "sent": "A machine learning.",
                    "label": 0
                },
                {
                    "sent": "This is, in our opinion, is an interesting data set because you can try a lot of these algorithms and theories on language rather than trying them on the web graph on the publication connection sites here or something like that.",
                    "label": 0
                },
                {
                    "sent": "This is text, so this stuff that comes out some degree is more striking or small, closer to knowledge sinoway.",
                    "label": 0
                },
                {
                    "sent": "OK, our age is also labeled by anything.",
                    "label": 0
                },
                {
                    "sent": "So in the graph that we're providing right now, they're not label.",
                    "label": 0
                },
                {
                    "sent": "We just saying contains.",
                    "label": 0
                },
                {
                    "sent": "But remember that dependency parsing could give you labels, so you could have subject and object relations etc which you can build yourself from that data.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess what 5 minutes?",
                    "label": 0
                },
                {
                    "sent": "Or a woman?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the information available, but there's a lot more information available on the web and I think it's important that we realize this information is there because if we keep using on the same data sets that people have given us, the number of problems we work on becomes very small and everybody is trying to improve the 0.01% out of their particular task.",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of data out there, and it's not surprising that people who are actually building these data.",
                    "label": 0
                },
                {
                    "sent": "They look at us in machine learning or information retrieval, and they say, well, you guys, yeah, stuff you do is very complicated, but why don't you use this data?",
                    "label": 0
                },
                {
                    "sent": "Why don't you use this data?",
                    "label": 0
                },
                {
                    "sent": "For example, you tried to classify pages on Wikipedia and it's very fascinating, but it actually has a category label already.",
                    "label": 0
                },
                {
                    "sent": "Provided by people, so to some degree is just a mathematical exercise.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really respond to need OK, so these are some examples of data that we're trying to use to either improve our Tigers or improve the representation of the text.",
                    "label": 0
                },
                {
                    "sent": "So in the case of Wikipedia, and it's also the case for some other websites using semantic web technology like RDF.",
                    "label": 1
                },
                {
                    "sent": "Besides the actual text you have a database like representation of the text.",
                    "label": 0
                },
                {
                    "sent": "It has some information with a particular schema.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a page of Alan Turing.",
                    "label": 0
                },
                {
                    "sent": "This is the text and the text explains when he was born and what is he but he.",
                    "label": 0
                },
                {
                    "sent": "This is natural language.",
                    "label": 0
                },
                {
                    "sent": "At the same time, there is a what's called micro formatting Wikipedia, which is basically a relational set of facts, so we know that it's nationalities England, even though it said in the text, they realized the computer is not going to find it, so let's write it as a database.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can actually not this back to some degree and actually use these to label or semi label, because there will be mistakes of course to label the Texan now improve your taggers out into the domain we learn, evaluate, do a number of things OK and this data is very easy to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "You also have categories, for example for every page which can give you a lot of information about the type of person.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to know what is the Alan Turing type?",
                    "label": 0
                },
                {
                    "sent": "Well if you look at the categories, you can probably learn that it is a person just because of the type of categories there is.",
                    "label": 0
                },
                {
                    "sent": "There is not trivial because this is real data by written by humans who don't have a PhD in ontology creation.",
                    "label": 0
                },
                {
                    "sent": "So they're just writing stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so the categories of volunteering are very many like English mathematician, but also all sugar runions.",
                    "label": 0
                },
                {
                    "sent": "I don't even know what that is and.",
                    "label": 0
                },
                {
                    "sent": "English essay, so there's a lot of stuff that has to be cleaned up, but that's what machine learning should be useful for.",
                    "label": 0
                },
                {
                    "sent": "We should be able to take this kind of redundancy and extract the information that we need.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, one more example of how can we use the Web and API's on the web?",
                    "label": 0
                },
                {
                    "sent": "To to do to solve more interesting problems in machine learning, and one of them is to actually use search engines as part of our retrieval as part of our function.",
                    "label": 0
                },
                {
                    "sent": "Whatever function you want to compute, you have to think that well, maybe part of that function is an API call to particular search engine, so this is an example again in the same paper where imagine that you want computer similarity in a query in an entity, and one way to do this is to build a graph that I just mentioned to run patron overhead and do something complicated like that.",
                    "label": 0
                },
                {
                    "sent": "Very simple, stupid way to do this.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not stupid.",
                    "label": 0
                },
                {
                    "sent": "Different from how I would do it is to actually.",
                    "label": 0
                },
                {
                    "sent": "Hey, it's the actually well take the query an issue, a query to the search engine and then record the set of URLs it comes back.",
                    "label": 0
                },
                {
                    "sent": "Now take the entity issue.",
                    "label": 0
                },
                {
                    "sent": "The entity to search engine really look at the resource account back.",
                    "label": 0
                },
                {
                    "sent": "So in this case the query was life of Pablo Picasso and we want to know general stain is related or not to this OK and so there are a number of.",
                    "label": 0
                },
                {
                    "sent": "If you want to set overlap measures that work very well, because this is the web, these are not set there actually rank set.",
                    "label": 0
                },
                {
                    "sent": "So if you take into account the ranks, for example by computing the average or reciprocal average rank in this list of entities in this set, you will find that you get a measure that ranks incredibly well.",
                    "label": 0
                },
                {
                    "sent": "The entities with respect to the query life of Pablo Picasso.",
                    "label": 0
                },
                {
                    "sent": "So if you need to do that, maybe you can do that in an API call to a search engine and then move on to whatever else you wanted to do so.",
                    "label": 0
                },
                {
                    "sent": "This is another example.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's all I want to say about textual presentations that can be useful to you.",
                    "label": 0
                },
                {
                    "sent": "To create more interesting experiments or moving to new areas of application.",
                    "label": 0
                },
                {
                    "sent": "I'll just give you a very quick idea because I have 5 minutes left.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a quick idea of one of the issues that we've dealt with.",
                    "label": 0
                },
                {
                    "sent": "One of the problems, of course, is the size of it.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of data and not only data is now not just sequences, but multiple overlapping bracketed sequences.",
                    "label": 0
                },
                {
                    "sent": "An little trees which are this dependency thing.",
                    "label": 0
                },
                {
                    "sent": "So what can you do with this?",
                    "label": 0
                },
                {
                    "sent": "To do experiments fast?",
                    "label": 0
                },
                {
                    "sent": "And it turns out.",
                    "label": 0
                },
                {
                    "sent": "You can, you can do a fair amount.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things so this is typically how the data looks like if you download it from the side.",
                    "label": 0
                },
                {
                    "sent": "It just looks like a bunch of sequences like for example the Tories won this election you have the power of speech labels, you have the lemmas, you have the sum of the Tigers.",
                    "label": 1
                },
                {
                    "sent": "We have a number of them disguise the ER.",
                    "label": 0
                },
                {
                    "sent": "It tells you that this is the beginning of a person beginning of a group of competition.",
                    "label": 0
                },
                {
                    "sent": "Then you have dependency relation.",
                    "label": 0
                },
                {
                    "sent": "These are the trees remember so it tells you that these are modifier of something that is in position #2 which happens to be Tories Tories is a subject with reposition three.",
                    "label": 0
                },
                {
                    "sent": "So position three is 1.",
                    "label": 0
                },
                {
                    "sent": "Here it tells you that election is the object of something in position 3, so it's the object of 1.",
                    "label": 0
                },
                {
                    "sent": "Basically we serialize the tree here OK, but in this serialized representation is not very useful to do things right.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What people tend to do in the past or there's still doing is they.",
                    "label": 0
                },
                {
                    "sent": "They use this inverted index like Lucina.",
                    "label": 0
                },
                {
                    "sent": "You take any search engine that you like or lemon for example in the machine learning community and you use that to find the interesting documents that are potentially interesting and then you go and build a representation that is useful for you and run the experiments.",
                    "label": 0
                },
                {
                    "sent": "So basically it's like issuing a query to Google getting the top 10 results and then doing experiments on those top 10.",
                    "label": 0
                },
                {
                    "sent": "Something that we're often tempted to do.",
                    "label": 0
                },
                {
                    "sent": "OK now this is the only way you can do it if the collection is the web but the collection.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia that fits in a laptop.",
                    "label": 0
                },
                {
                    "sent": "Then you shouldn't be doing this, but you should be doing is actually creating much better indices because the problem of this is that there are two possibilities.",
                    "label": 0
                },
                {
                    "sent": "Either the problem was very easy and then the inverted index could solve it by itself, and then this set here contains lots of good answers, or the problem was very hard, and if there's not very good answers here, then no matter what you do, you're not going to find the good answers.",
                    "label": 0
                },
                {
                    "sent": "OK, this makes sense.",
                    "label": 0
                },
                {
                    "sent": "So what we've been trying to do is to build better very.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dirty indexes and just to give you an example of that, an inverted index is just a way to encode a sparse matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, and typically what you have information retrieval is you have this, you have a document, you have a term and you want to know the score of that term for that document.",
                    "label": 0
                },
                {
                    "sent": "So that's a valuable xig and it's just a big matrix, but it's so big that you cannot fit in memory.",
                    "label": 0
                },
                {
                    "sent": "So inverted index is just a very cheap way to find all elements of this matrix for which the value is not zero.",
                    "label": 0
                },
                {
                    "sent": "That's easy enough, but more Interestingly, it's a very cheap way to find combinations.",
                    "label": 0
                },
                {
                    "sent": "So if you want to find any two objects which have those features, non 0.",
                    "label": 0
                },
                {
                    "sent": "Then this operation with an operation is extremely cheap, so you can do this.",
                    "label": 1
                },
                {
                    "sent": "This is how you type Britney Spears music in Google.",
                    "label": 0
                },
                {
                    "sent": "You can get the answer incredibly fast.",
                    "label": 0
                },
                {
                    "sent": "Obviously that's a very expensive thing to do because you have to look at every document and find those that don't contain.",
                    "label": 0
                },
                {
                    "sent": "Britney don't contain music, and then throw them away.",
                    "label": 0
                },
                {
                    "sent": "That could not be linear in the size of the web.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we'll never get it back.",
                    "label": 0
                },
                {
                    "sent": "It's actually very, very cheap to do it, and it turns out if you use position information then you can do fairly amazing things.",
                    "label": 0
                },
                {
                    "sent": "You can actually say.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a sparse matrix and for every element that sparse measures have a vector of positions.",
                    "label": 0
                },
                {
                    "sent": "So it tells me the word how many times it appears that also in which position in the document appears an inverted index.",
                    "label": 0
                },
                {
                    "sent": "Algorithms can actually give you.",
                    "label": 0
                },
                {
                    "sent": "Again, you can find 2 features for which the values are non zero an.",
                    "label": 0
                },
                {
                    "sent": "There are positions of those two features that are within some range, so that's how you would find all the documents that contain Britney Spears music at a distance of less than 10, for example.",
                    "label": 0
                },
                {
                    "sent": "And this you can do not as fast as before, but fairly fast as well.",
                    "label": 0
                },
                {
                    "sent": "So actually this is very useful because you can start thinking of you cannot trees into position strings and try to do operations on trees that are very cheap, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with that, the reason you can do that is if you can do this kind of algorithms like the sassy algorithm that can, if you think of all the positions in which word appears, it is a document indexes another position in which Webb Pierce.",
                    "label": 0
                },
                {
                    "sent": "Then this algorithm is what they do is they basically have a clever way of skipping until they find a match.",
                    "label": 0
                },
                {
                    "sent": "So here 40, because we were at 8:00, and then we have a 40 and there's no point looking at 10:25 because they couldn't be an end here.",
                    "label": 0
                },
                {
                    "sent": "So eventually we can find a match in a time that is much smaller than the size of the actual collection or even the size of the possibilities.",
                    "label": 0
                },
                {
                    "sent": "And you can use position tricks as well in here to actually find the ranges.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we coded this data is we basically thought of this is undocumented and inverted index and this is sparse matrix that contains these features.",
                    "label": 0
                },
                {
                    "sent": "the Tories won this election with their position 12345, but as well as that we create fake words in that position.",
                    "label": 1
                },
                {
                    "sent": "So in position one of these documents are in position zero of this document.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in possession of this document, I say that there are two words.",
                    "label": 0
                },
                {
                    "sent": "There's the, but there's also DT.",
                    "label": 0
                },
                {
                    "sent": "And then I say there's another word.",
                    "label": 0
                },
                {
                    "sent": "There are three words when it's called Bond.",
                    "label": 0
                },
                {
                    "sent": "And then the next word I said their stories and then appears.",
                    "label": 0
                },
                {
                    "sent": "But there is also a subject work, so this is going to allow me later to say final documents in which there is a word subject.",
                    "label": 0
                },
                {
                    "sent": "There is a word Tori and they are in the same position.",
                    "label": 0
                },
                {
                    "sent": "This equivalent to say find it or is there is a subject.",
                    "label": 0
                },
                {
                    "sent": "And if you rewrite your tree using position tricks like this, you can actually find you can compute very restricted limited number of tree distances on very large scale very very fast.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to end show you an example of that.",
                    "label": 0
                },
                {
                    "sent": "Oh, I can't because it's in the laptop and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but I think it's.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time to win anyway, so I'll stop there.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm still not very clear how those semantic tags could be used for the search engine.",
                    "label": 0
                },
                {
                    "sent": "I mean, user was still type short keywords unless user type, natural language questions or are you expecting user too?",
                    "label": 0
                },
                {
                    "sent": "Ask natural language questions.",
                    "label": 0
                },
                {
                    "sent": "Savior tax will actually help you to search better, so I think if I type Britney Spears music into search engine, the problem is being solved and we should not try to use semantics.",
                    "label": 0
                },
                {
                    "sent": "The problem is solved.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm not interested in three world queries in our search engine.",
                    "label": 0
                },
                {
                    "sent": "I'm interesting in other problems that go beyond that, so I tried to give you some ideas of that.",
                    "label": 0
                },
                {
                    "sent": "This is one of them.",
                    "label": 0
                },
                {
                    "sent": "For example, how do I find related people to a query?",
                    "label": 0
                },
                {
                    "sent": "If you're in Wikipedia, you may be able to write a fairly long query like you know the you know.",
                    "label": 0
                },
                {
                    "sent": "US Civil War cannon usage.",
                    "label": 0
                },
                {
                    "sent": "You're in a particular domain you're willing to write long queries or longest queries, or maybe short queries, But you're going to interact by clicking or doing something, so that's the kind of problems I'm looking at.",
                    "label": 0
                },
                {
                    "sent": "The other example I gave is in Yahoo answers, for example.",
                    "label": 0
                },
                {
                    "sent": "People are spending hours answering questions, so maybe they're willing to spend minutes finding them so they will have much richer representations of text.",
                    "label": 0
                },
                {
                    "sent": "Another one that I didn't talk about is Yahoo Pipes so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go back here, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "I never think of users writing queries that are very complex, but maybe programmers, for example, could be writing queries that are very complex.",
                    "label": 0
                },
                {
                    "sent": "So when you write a website, some parts of your website may actually be very complex queries that are running against the server.",
                    "label": 0
                },
                {
                    "sent": "Example finding related people or imagine that you have a site about.",
                    "label": 0
                },
                {
                    "sent": "Particular tennis player and you say, well, I like to go and get all the relevant places where this tennis player has been and then find pictures in Flickr so I can decorate my website.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing human will never do better.",
                    "label": 0
                },
                {
                    "sent": "Programmer is willing to learn enough to do it, hopefully.",
                    "label": 0
                },
                {
                    "sent": "That's that's the hope.",
                    "label": 0
                },
                {
                    "sent": "Say something about a problem.",
                    "label": 0
                },
                {
                    "sent": "I notice things like context, automatic summarization and that was if you have cover message to to produce a nice hot.",
                    "label": 0
                },
                {
                    "sent": "And if you don't have a good message evaluation methods you have kind of mismatch.",
                    "label": 0
                },
                {
                    "sent": "So if you use syntactic measures or simple minded measures and evaluation then you may not actually notice the difference between good or bad output or satisfactory answer.",
                    "label": 0
                },
                {
                    "sent": "I suspect this also arises here so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's a huge problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, the first thing we did was we develop.",
                    "label": 0
                },
                {
                    "sent": "We did this task, we thought, OK, let's do a particular very difficult task and it was the idea.",
                    "label": 0
                },
                {
                    "sent": "The problem of ranking important dates with the spectral query.",
                    "label": 0
                },
                {
                    "sent": "So what are the most important dates with respect to query?",
                    "label": 0
                },
                {
                    "sent": "And we thought this is a very difficult task.",
                    "label": 0
                },
                {
                    "sent": "So first you have to find a difficult task, because if it's easy, as you say, very simple algorithms will do it.",
                    "label": 0
                },
                {
                    "sent": "But then you have to evaluate it and that's quite hard.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's been any advances there.",
                    "label": 0
                },
                {
                    "sent": "You basically have to do the work you have to find a difficult task.",
                    "label": 0
                },
                {
                    "sent": "You have to ask people to evaluate manually.",
                    "label": 0
                },
                {
                    "sent": "So in our lab we regularly ask people to manually evaluate things.",
                    "label": 0
                },
                {
                    "sent": "Ann and there you have to hope that the task is hard enough and evaluation is good enough so that you can show improvement.",
                    "label": 0
                },
                {
                    "sent": "It's hard and I don't think.",
                    "label": 0
                },
                {
                    "sent": "There's a way around it, but you have to go through it, yeah?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "There's a. I can't hear you.",
                    "label": 0
                },
                {
                    "sent": "Example with scriptions of people right by the fictional figure out that actually other pictures on Flickr, Esther.",
                    "label": 0
                },
                {
                    "sent": "So what's the question?",
                    "label": 0
                },
                {
                    "sent": "ID.",
                    "label": 0
                },
                {
                    "sent": "I think you can use it in many many ways and so it could be that that's a very good way to use it, I think, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, I think we have to stop because we have to sit on the so thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}