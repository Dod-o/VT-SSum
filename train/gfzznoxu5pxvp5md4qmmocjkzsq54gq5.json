{
    "id": "gfzznoxu5pxvp5md4qmmocjkzsq54gq5",
    "title": "Kernel Tricks, Means and Ends",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "June 21, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml07_scholkopf_thok/",
    "segmentation": [
        [
            "Thank you very much swimming for this flattering introduction and thank you all the organizers for inviting me here.",
            "It's actually quite special for me to speak here in Oregon because I spent some time in Oregon as a student going to school here so it was the first time that I really got to know America.",
            "So now I feel a bit like a student again here in Oregon so I slightly change the title of my talk and will talk about four things.",
            "Basically kernels, some applications of kernels.",
            "I'll talk about something called.",
            "Kernel means it will become later.",
            "It will become clear later what I mean by this.",
            "And finally I'll try to put things a little bit into perspective.",
            "Most of you probably heard about this thing called the kernel trick by talking about how this kernel trick has evolved over the years, how we thought of it at the beginning, how we think of it now.",
            "And it's also a little bit related to the actual content of this talk.",
            "So part of this talk will be well known to most of you, and I'll try to cut that very short.",
            "It would be maybe the first 5 or 10 minutes.",
            "So if you're still sleepy, that's that's your chance.",
            "Afterwards, I'll talk about some applications that some of you may have seen.",
            "Others may not have seen.",
            "And finally then I'll talk about something that I hope maybe the majority of you hasn't seen yet.",
            "So these are the first 5 or 10 minutes now, so I'll start with an example of a very simple pattern recognition algorithm where we are given two classes of data.",
            "The classes in the circles and we tried to classify a new point X based on whether it's closer to the mean of the one class denoted mu plus.",
            "So this is the mean of the one class, or closer to the mean of the other class.",
            "The noted mu minus, which is over here.",
            "So obviously this induces.",
            "A separating hyperplane as a decision function with the normal vector.",
            "That's the vector connecting these two class means.",
            "But of course, this only solves problems that are linearly separable and the kernel approaches then to pre process the inputs with some mapping that takes our input domain into a dot product space and then we work in that space.",
            "And why does that help?",
            "Or why can that help?",
            "Here's an example 2 dimensional classification problem.",
            "Separating these crosses from the circles we assume the true decision boundary.",
            "This is this ellipse, but we don't know if we only have the data.",
            "Now if we pre process our points by computing all features of order 2.",
            "Or product of order two, then this problem becomes linearly separable, which you can see by writing the ellipse equation in these new coordinates.",
            "Now the trick comes in when we see that if we take two such points X&X prime to 2 dimensional points, we map them into that 3 dimensional space using this prescription.",
            "When we then take the DOT product, we see that this gives a simple binomial formula.",
            "We can simplify it and write it as the square of the dot product in the original coordinates.",
            "So that's an example of a simple kernel.",
            "We can compute the dot product without going into this higher dimensional space.",
            "Same thing works for N dimensional inputs and products of order T. This case we end up with this polynomial kernel and basically something similar works for all positive definite kernels.",
            "So that's a notion well known in mathematics, and whenever we have such a positive definite kernel, we can prove there exists some mapping into another space.",
            "So mapping fire such that the kernel computes the dot product in that space.",
            "And one way of visualizing that mapping is by thinking of it as a mapping into a function space.",
            "So we define the mapping to take a input point X into the kernel centered on that point X.",
            "So for instance, if we use a Gaussian kernel, then at this point X would be mapped into a Gaussian function sitting on that point X.",
            "Another point X prime would be mapped into another Gaussian, and our space will be defined to be the linear Hull complete completion of the linear.",
            "All of these kinds of mapped functions, and then we have to define what's the dot product in that space, and we have to define it such that.",
            "It satisfies this equality because we want the kernel to compute the dot product.",
            "It turns out this works.",
            "We can define it this way.",
            "It's well defined.",
            "We can extend it linearly and we then have a kernel that computes the DOT product in this function space.",
            "And one of the nice things about of this kernel is if you substitute K in here and you do the same thing for linear combinations, you will see that the kernel represents PT evaluation on these functions.",
            "So by this I mean if I have a function in that space, so that's at.",
            "This point is just an abstract element of a Hilbert space.",
            "If I want to know the value of this function at some point X, the only thing I have to do is take this other element of the Hilbert space, which is the kernel sitting on X, compute the dot product between the two and I will get my PT evaluation.",
            "So this is a particular kind of Hilbert space.",
            "It's a Hilbert space web.",
            "PT evaluation is a continuous function and can therefore be represented as a dot product.",
            "So we'll come back.",
            "We'll use these later on.",
            "This kind of PT evaluation expression.",
            "Now let's go back to this example from before and do the same thing in the feature space.",
            "So now we want to classify points mapped into the feature space again by.",
            "Computing distances between a test point and the mean of the positive class in the middle of the negative class.",
            "If we do this and we work it out again.",
            "So this is a separating hyperplane.",
            "This time in the feature space in the properties in kernel Hilbert space, and if we work it out, it turns out this is a nice expression in terms of kernels.",
            "The decision function will be thresholded version of a kernel expansion, with kernels sitting on the positive negative points, and it will be such that here this thing.",
            "If K is a density, for instance a Gaussian, we can think of this as 1000 Windows estimate of the positive class, and we can think of this as a passing Windows estimate of the negative class.",
            "So this is a passing windows.",
            "Classifier based on plugging estimates from 1000 windows of the two classes.",
            "And the support vector machine is quite similar.",
            "It's also a separating hyperplane in such a feature space, which solves nonlinear classification problem in the input domain.",
            "And depending on what kind of kernel we use, if we turn up and down linearity, the decision boundary in the input space will become increasingly nonlinear.",
            "So you can see the behavior is a little bit similar to these behaviors that you get if you change the regularization constant for some of you who know this kind of work.",
            "So, so far so good.",
            "Let's talk about some applications, so hopefully this will get a little bit more interesting for you.",
            "So the application that I want to focus on in the next 10 minutes or so is our applications in computer graphics.",
            "Starting with the problem of implicit surface fitting.",
            "So here we have a sampling of a surface is a surface typically living in our three, and sometimes we have corresponding soft surface normals and would like to construct a function, a kernel expansion whose zero level approximates the surface.",
            "So wherever the surface is, the function is supposed to take the value 0.",
            "Inside of the object which is delimited by the surface, the function should be negative.",
            "Outside it should be positive.",
            "And the number of support vector approaches have been developed for this problem, some of them building on one class support vector machine.",
            "Someone building a support vector regression, and it's possible to approximate objects with this kind of approach and also to estimate what's called the signed distance function.",
            "So the scientists function is a particular such function with the additional property that the value of F should always be equal to the distance from the surface multiplied with the minus one.",
            "If we inside the object.",
            "And once we have such implicit representations, we can do funny things like intersections of objects and things like that, and one can solve such problems also in fairly large scale.",
            "So this is some work of our PhD student Christian Waldo, who has solved, for instance, this approximated this object, which is a standard benchmarking computer graphics that has 14 million training points, and you can solve this pretty fast in a few hours.",
            "And he developed a multi scale approach where you don't just have a single scale of kernel with but you have multiple scales.",
            "So you can.",
            "You can do things like fill holes where you don't have data.",
            "If you adapt the scales accordingly and so on.",
            "So yeah, there's another few examples, so all these things are.",
            "With with a grain of salt, hyperplanes in some feature space associated with the kernel, and then the input space, they look like these complex objects.",
            "Now if you have tried to apply this to videos, the first thing that comes to mind is you take a video and you'll fit such an implicit surface representation frame wise.",
            "And if you do that and you get this kind of solutions, so you see each picture is reasonable, but from picture to picture there's certain jumps, which is because you do each of them separately and you don't regularize across time.",
            "And if you extend this approach and do it in for the you get a solution which is now smoother in time and which also has some interesting properties in terms of interpolation with Christian tried here is he.",
            "Deleted some frames in this video that shown in red, so wherever you see something red, there were no training points and the red thing scene is purely filled in by the system.",
            "Now one could come up with the idea of trying to use this to interpolate between different shapes, not just the same person at different times, but really different shapes for like for instance, this monkey in this human.",
            "But it turns out this doesn't work very well.",
            "So the deformation states are pretty ugly and I don't want to claim that the final human is beautiful, but the things in between are probably even worse.",
            "So we we try to think of something different and some different approach that more directly tackles this problem of morphing as people in computer graphics call it, so was the morphing problem.",
            "We have two objects, maybe from somehow the similar class or similar class of objects, but not not too similar.",
            "And we try to compute something in between.",
            "And obviously if we just take a linear combination of images, that's not going to work.",
            "We somehow have to be able to combine corresponding features onto the on these two objects, so we need to know correspondence we need to do this kind of correspondence field, or sometimes it's called a warp field.",
            "And once we have that, we can interpolate and extrapolate images, almost like in a linear space.",
            "So how do we estimate this correspondence field?",
            "Well, let's assume our objects live in some joint domain.",
            "Then this warp or this correspondence field is a mapping from that domain onto the domain, and we assume that as training points were given surface points of 1 objects and surface points of the other object.",
            "Now if we knew if these points were in correspondence, so we knew which X point corresponds to which set point, we could just do regression.",
            "That would be straightforward, but what do we do if they're not in correspondence?",
            "It turns out that a fairly simple idea can already solve this problem relatively well, and the idea is that this warp should be such that relative to a starting point X, the first object should somehow.",
            "Locally it looks similar to the second object as seen from the warp point.",
            "So we formalize this as a cost function that depends on all these terms and the simplest cost function that we use is something that will build on these implicit surface embeddings that are shown you before.",
            "So remember we were estimating the scientists and function of objects, which means given an object we estimated function which is 0 on the surface of the object and whose value is equal to the distance from the object elsewhere.",
            "So let's consider this as a feature to construct.",
            "Request function, so think of these functions F1F2 as the signed distance functions of our two objects.",
            "Then we can use regularizers of the type like this one or cost functions at this type.",
            "So the signed distance function of the first object or this is the distance of point X to the surface of the first object.",
            "We want this to be similar to the distance of the warp point to the second object.",
            "Of course, this doesn't completely constrain the mapping, but it turns out if you Additionally use standard support vector stagger style regularizer so it's a protector regression approach it already more or less constraints the mapping and sometimes we can do better by also using higher order properties of these scientists functions.",
            "So the kind of optimization problems that we solve are just similar or are generalizations of support vector regression where we fit component functions in the modification is here that we have this additional.",
            "Integral or approximate integral over this locational cost, and here this is just in case we if we have some labeled training data X set input output, we can also use this term, but we do not necessarily have that, so let's look first.",
            "Edit a simple toy example and then some realistic ones.",
            "So here we have an example in 2D.",
            "This is the first object.",
            "This is the second object.",
            "Obviously if you take a linear combination of these two pictures you won't get anything good.",
            "If you use the scientists cost function, you already get this kind of solution, so this is not a 5050 mix of these tools.",
            "Yeah, take the Warfield that Maps this to this and sort of apply 50% of that Warfield to deform this object and what you can see here.",
            "This is the war field.",
            "What it does it?",
            "It shrinks this protrusion up here and it grows a new one down here.",
            "Doesn't work perfectly as you can see here if we Additionally use the normals in our cost function, then it works pretty well and the reason is that if you use the normals then it costs too much, for instance toward this point on this one because they have different normals.",
            "So then the Warfield will actually move down this protrusion and then you get a fairly good solution.",
            "And this is all this is automatic, so nobody was clicking on points here.",
            "Here's another example, which is also fully automatic, so the input is this object is Queen.",
            "This object here, which is a pawn, and we use the scientist since function cost function no landmark points, no color information in these objects in between there are completely synthetic.",
            "Coming back to our original problem of moving the heads, we have the same kind of approach here.",
            "This time we use normals in addition to moving the head turns out to be a little bit harder, and if we use the enormous it gets a bit better.",
            "So here this time we have this male head and the female head over here and these three heads are synthetic, computed by the algorithm.",
            "This is a magnified view of this 5050 head, which is relatively good.",
            "We can also use it to do things like texture mapping, so we have one object without a text, so we have another object from sort of the same class, but actually quite different shape because all this stuff up here is missing and we can use this approach to map this texture onto this fast.",
            "If we Additionally click on some points XY corresponding to each other, we can also move more complex up to our more diverse objects into each other, and I think I have another example.",
            "In the next slide, which is some data that we generated for our Department of Physiology, they wanted to do experiments with this kind of stimuli.",
            "When we move some humans into monkeys.",
            "So for these applications in now I want to spend the rest of the main part of the talk on kernel means, which is something that came up during the last years and I think it's it's quite an intriguing direction of kernel machines research.",
            "For this I have to move into the world of Latech.",
            "And I should say this is joint work with the people listed here.",
            "So let me first return to this example that I showed you at the beginning of my talk.",
            "So I was showing this this simple example of a classifier."
        ],
        [
            "So again, here where you take some positive class and negative class, you map them into the feature space and compute the means in those spaces denoted by mu of X&Y are so this time I'll call the positive class of this sample here, so in positive points I'll call it X, the negative points I'll call them Y.",
            "And I'm assuming that my input domain is compact or satisfy certain conditions Apple and have a positive definite kernel on this domain and all this is happening in their properties in kernel Hilbert space associated with that kernel.",
            "So I take my positive Class X, map it into the feature space.",
            "This is the mean.",
            "Do the same for the negative class.",
            "So these are my means down here where I've used this representation K of my mapping into the feature space.",
            "Remember, I told you that we can think of it as a space of functions.",
            "And then.",
            "Let's keep it a bit more exciting, and obviously this kind of approach.",
            "So I told you before separating surface is the hyperplane with normal vector, which is the vector connecting these two class means.",
            "Now obviously if this vector and actually we came up with this example when we were writing the introductory chapter of our book, and we thought this is nice, but it's nice that it turns out to be a passing windows based classifier, but we didn't.",
            "Continuing this direction and only recently, we notice that this is actually much more interesting than we thought.",
            "So if you look at this thing so this vector, obviously if it's a normal vector hyperplane only makes sense if it's non 0.",
            "Sounds like a technical condition, but it turns out if you think about what happens if it's zero, then this can be the starting point of something more interesting.",
            "And if you do think about this, well, obviously if you do any input space, it just means that these two samples have the same mean, So what?",
            "If you do it in the space associated with the polynomial kernel of degree D. So remember, the polynomial kernel computes products of D input coordinates.",
            "Now, this mean just computes the mean over the whole training set.",
            "So the mean of products of the coordinates.",
            "It just means that these directions are basically monomials expectations of our means of monomials.",
            "In other words, there are empirical moments of order, the.",
            "Therefore, if we use a polynomial kernel of order D and mu of X is mu is equal to mu of Y.",
            "So this means coincide.",
            "It means that for these two distributions on moments up to already or empirical moments up to ALDI coincide.",
            "Now the next question is, what if we use a kernel that's even more high dimensional than a death order polynomial kernel?",
            "What if we use a kernel which is strictly positive definite, in which Case No matter how many training points you map into the feature space, there will always be linearly independent.",
            "That means it's potentially infinite dimensional feature space.",
            "If you think about this, it's actually fairly straightforward to show that if we take a strictly positive definite kernel."
        ],
        [
            "For instance, a Gaussian.",
            "Then the means coincide if and only if the datasets coincide.",
            "So all the points have to be the same.",
            "So this somehow means that the mean of the set of points in the feature space remembers each point that has contributed to the mean.",
            "And actually, this is also a point that we had seen at some point before.",
            "I remember during preparing this talk we had a workshop from 10 years ago in Japan where we were trying to come up with a clustering algorithm in the feature space and we notice this property that if you take such linear combinations of points, they somehow remember each point that contribute contributes to them.",
            "And this was I remember this was in discussions with John Platt, quits Watkins and Nello Cristianini, and we were trying to come up with a name for this algorithm which would have the acronym FUGU, which was very difficult, especially with the use.",
            "I think one of the user at the Institute for Heuristics.",
            "But we never published this algorithm.",
            "But anyway, so I remembered it now.",
            "And let's take a look at this mean map a little bit more closely.",
            "So the map just remember recall.",
            "It took this image sample.",
            "This M training points and map them into the mean of the images in the feature space.",
            "If you take this point, mu of X in the feature space and I'll take the dot product with an arbitrary function of that space.",
            "So I just substituted in here.",
            "I can use the linearity of the dot product and then I can use the fact that the kernel represents point evaluation of the function.",
            "So here we get the function values, which means we get the mean of the function on that sample.",
            "So this mean in the feature space represents the operation of taking the sample mean of a function.",
            "And another thing that's interesting to workout is if the distance between these two means, which is the quantity that we started with.",
            "We can rewrite this thing.",
            "So that's just a vector in that Hilbert space.",
            "This difference here, and we can rewrite the length of this vector is the same as the maximum dot product that we get by taking the product between this vector and vectors from the unit sphere.",
            "So let's just koshish routes.",
            "Inequality is just linear algebra.",
            "So now we have rewritten it like this and now we can use this effect.",
            "That's the main represents taking averages and rewrite things like this.",
            "Now we have this supremum over the unit sphere of the difference of these two averages.",
            "The average of the function on the example and the function on the wise sample.",
            "So this distance between these two means in the Dark Ages is actually in the way that geometrically computing the solution of a pretty high dimensional optimization problem over the unit ball in infinite dimensional.",
            "Hilbert space and we'll come back to this point later.",
            "It's also interesting to look at the The witness function of this quantity, so the function which actually realises the maximum value for this thing.",
            "Actually, this is a maximum in this case, and this function is just the function that's parallel to this thing here, but normalized to length one, so we can write it like this."
        ],
        [
            "And if we want to plot it, we have to point evaluated by taking DOT products with the kernel and here's an example.",
            "If we have samples X&Y drawn from a Gaussian in Laplacian in both of them, zero mean and unit variance.",
            "Then this witness is the red function, so that goes into Laplacian 1, iron, black, dashed and dotted.",
            "In the read function, is this witness so you can see the witness function is positive whenever the Laplacian exceeds the Gaussian, it's negative whenever the Gaussian exceeds the Laplacian.",
            "So somehow this function will will best detect differences between Gaussian interpolations from the unit ball of this arc AHS, which in this case was a Gaussian our cages.",
            "So it's a reasonably smooth function and the smoothness will depend on what our cages you choose.",
            "But it's a function that tries to detect these differences."
        ],
        [
            "So that's nice, but it actually becomes even more interesting when you do the same thing for measures if you, rather than mapping samples into that space, you can map measures into that Hilbert space in the construction is exactly the same, so there's really nothing surprising coming now, so we have some technical conditions that certain expectations have to exist.",
            "These are fairly benign, for instance, that's sufficient if.",
            "The image of the input data under the feature map is contained in some ball like, which is the case for Gaussian and it goes in case the unit ball.",
            "But let's assume these conditions hold true.",
            "Then we can map such a measure into our particular Hilbert space by taking the expectation of our feature map.",
            "So think of it as this kernel map, the expectation where X is drawn according to this measure P. So this quantity will exist if this above conditions were satisfied, and so this is not the expectation of the mapped measure in the feature space.",
            "And as before we can workout what happens if we take the DOT product between this quantity and some arbitrary function of our space and it will turn out.",
            "This will give us the expectation of that function.",
            "Likewise, we can compute the difference between 2 means in that space, and we can rewrite this also as a supremum over the unit ball of the difference between two expectations with respect to these two different distributions P&Q at the same function.",
            "Now remember that in the finite sample case we had this surprising property that the mean remembered all the points that were contributed to it.",
            "So that means we're only identically if the point sets were identical.",
            "In other words, this mapping mu was one to one.",
            "So how about now for the measures and it turns out something similar holds.",
            "And to see this we first have to appeal to a classic result from probability theory, which deals with the quantity very similar to this one.",
            "The only difference being that now rather than having the unit sphere of a universal or unit sphere of representing Hilbert space, we have the set of all continuous bounded functions on our domain.",
            "And this theorem tells tells us that.",
            "This quantity here with the Supreme over all continuous functions, is 0 if and only if the two distributions are identical.",
            "So clearly if they are identical then this thing will always be 0.",
            "So the interesting direction is from here to here.",
            "And this right here and get stronger the more functions we use here.",
            "So if this function class is large enough, the theorem says if this function is large enough.",
            "For instance, if it's all continuous function, then actually the converse is true.",
            "So there will always be a continuous function that detects the difference between P&Q.",
            "By taking these means.",
            "Now we can convert this into a result for our setting."
        ],
        [
            "Simply by considering our first by considering a repetition kernel Hilbert space which is dense in the set of continuous functions.",
            "And then taking the unit ball in that repetition with space for taking the unit ball doesn't make a difference.",
            "The scaling doesn't hurt us 'cause this is an equality here, and revenues in kernel Hilbert spaces that are dense in the set of continuous functions have been studied before in the context of consistency of support vector machines and they have been called universal kernel space time, but.",
            "An example of universal kernel is a Gaussian.",
            "So if we do this.",
            "We get this result here.",
            "If the kernel is universal.",
            "Two distributions are the same if and only if their images in the feature space are the same.",
            "So also this mapping is 1 to one and therefore invertible on its image, and this image has in a slightly different context, being called the marginal polytope.",
            "It's also interesting to note that this concept of the mean mapping generalizes the moment generating function, which you can define for a random variable X with distribution P. So the moment generating function you just take this quantity the exponential of X dot product with an open argument and take the expectation of this.",
            "And from this you can by taking derivatives you can reconstruct all moments of that random variable.",
            "And if you recall, or you may or may not know.",
            "If not, I'll tell you now that this this thing, here this exponential thing is actually a universal kernel.",
            "This explain.",
            "This is the exponential kernel.",
            "When can show this is a so called conformal modification of a Gaussian kernel, which is also.",
            "Universal, so it's Hilbert space is dense in the set of continuous functions it satisfies the conditions of this result.",
            "Here, therefore, everything that I said before holds true for this kind of thing, but we can also plug in other universal kernels in here.",
            "No, before I've shown you several times how to convert such different vectors into quantities like this, which suggests that we can also handle them using methods of statistical learning theory.",
            "And indeed if you take here the difference vector between distribution and sample from that distribution.",
            "You can rewrite it like this so we know it's an expectation of a function, and here we have the mean of the same function over some function class.",
            "This is exactly the kind of quantities that are studied by VC theory.",
            "It will pack theory and when combined this kind of quantity using uniform convergence methods when needs certain deliberately more complicated.",
            "But if basically if the volume averages of that unit ball.",
            "With the given distribution are well behaved and this thing will go to zero like one over square root of him.",
            "OK, so let's not not talk about some.",
            "Well before I talk about applications, maybe just one word about one more word about this.",
            "What does it mean if this thing here goes to zero?",
            "It's not quite the same as saying we are estimating this measure because it's certainly not saying that we can estimate a measure in a general setting independent of the problem, because that's an impossible problem.",
            "So what this is this is just saying is that if we look at this problem through the unit ball of our cages and then at some point we cannot detect differences anymore.",
            "So our unit ball which we have to choose ourselves, we have to choose the arcade chess by specifying what kind of differences we are interested in at some point.",
            "Maybe that unit problem will not contain functions that are sufficiently non smooth such as to be able to detect differences between these anymore.",
            "And the whole thing is a little bit similar to.",
            "Publix concept of weak convergence of risks.",
            "So remember in the VC bonds you don't study convergence.",
            "Convergence of the empirical measure to the true measure.",
            "You study convergence of risks, so you have a risk which depends on the empirical measure and that will measure you and you prove that this kind of risk goes to 0, but that's a weaker condition than the actual measures conversion."
        ],
        [
            "OK, so let's look at some applications of this kind of stuff and I probably I don't have time to go through all of them, but I want to cover at least a few of them.",
            "One of them is the two sample problem, and some of you might have seen this because Arthur Gretton gave a talk about some of this at Nips last year.",
            "But let me just spend 2 minutes and say a little bit about it.",
            "So we in this case we assume we have two samples X&Y, drawn from distributions P&Q respectively, and based on these samples we want to decide whether P is equal to Q or not.",
            "So if we workout this difference of the vectors, we can work it out in terms of kernels.",
            "It's an expression that only depends on kernel.",
            "Is these three sum of three expectations, which you can we can write as a single expectations over this quantity here, which is by the way also positive definite kernel.",
            "But also it's the kernel of a so called you statistics.",
            "But don't worry bout that is this quantity with which these kind of pairwise comparisons and in terms of this thing we can write our difference.",
            "M. And actually will let me call it D squared.",
            "This difference.",
            "We can write it as the.",
            "Expectation over this kernel H and we can give an estimator the hat sample based estimator which is summing over these ages only, excluding the terms whereis equal today in order to avoid and bias.",
            "And one can prove that this isn't that an unbiased estimator of this discrepancy between the two means.",
            "And one of the nice things if you look at this formula, because this is what one actually has to compute to perform such a 2 sample test is just.",
            "It's trivial to implement, it's just a double for loop.",
            "So we can do it relatively fast.",
            "And also we can do it basically for any kind of data where one can define kernels.",
            "So we can also do it for strings and graphs and other kind of stuff.",
            "This thing can."
        ],
        [
            "Shown so these empirical estimate can be shown to converge to the true quantity in probability with a relatively fast rate.",
            "So this is again uniform convergence style bound and one could use this as the basis for a test.",
            "But actually it doesn't work very well, which is because as most of us know, uniform convergence bonds are often very loose and pessimistic.",
            "So in practice, I have to admit that actually classical statistics works better in this case if we study the asymptotic distribution of this quantity here, so this discrepancy between the estimated difference in the means and the true difference in the means.",
            "Then one can study what this converters to both in the case P non equal to Q and in the case P equal to Q in the one case we get a Gaussian.",
            "In the other case we get something more complicated and I don't want to go into details on it.",
            "But based on this one can devise a test and this test will use a null hypothesis that P is equal to Q.",
            "So maybe just to give you the idea.",
            "So P is equal to Q, then this thing would be 0.",
            "That's our null hypothesis.",
            "If we then observe based on a sample, if we have a relatively large sample and we notice that the head is significantly different from zero and we know something about the distribution of the hat, then we can sort of say how unlikely it is that in fact the null hypothesis Wister, or how likely it is that the null hypothesis was true or unlikely was rejected in building on that, one can test things."
        ],
        [
            "Second application is in the field of dependence measures.",
            "So some of you might have heard of methods have methods like Colonel ICA and kernel constraint covariance and then the Hilbert Schmidt independence criterion and all of them treat this problem of independence.",
            "So you have two random variables.",
            "X&Y are drawn from some joint distribution PFPXY with these marginals and we want to know whether this distribution factorizes.",
            "And the main idea in these methods that work very well.",
            "It's very, very similar to the idea that I showed you before when we were rewriting a distance as an optimization problem over the unit sphere.",
            "The main idea is that.",
            "2 random variables X&Y are independent if and only if for all continuous functions FG the covariance of F of X&G of wire vanish is.",
            "So again, this is like a.",
            "It's a simple linear criterion, but it's enforced over a large class of nonlinear functions.",
            "And it turns out again, we can do this over the unit sphere of a reproducing kernel Hilbert space and then get independent criteria that work very well.",
            "But now I want to.",
            "Show I'm not really sure, but tell you that we can recover one of these algorithms exactly using the kind of argument that I talked about in the last 10 minutes, and to do this we just have to map the joint distribution into our Hilbert space and we have to map the product of the marginals into our Hilbert space and then we use this quantity.",
            "This difference vector the norm of this difference vector as a measure of dependence.",
            "So we just check how different are these two distributions."
        ],
        [
            "And if we then use a kernel now we need a kernel on X&Y.",
            "If we unit could use a kernel that factorizes into two kernels on the X&Y.",
            "We can show that.",
            "In this case this quantity.",
            "The square of this quantity is equal to the so called Hilbert Schmidt norm of the covariance operator between the two arcade.",
            "Yes, it's a generalization of the approval for menus Norm.",
            "And if we look at the empirical estimate of this quantity, we get this quote this thing here.",
            "This trace, which is exactly the same thing that has been studied before and called the Hilbert Schmidt independence criterion.",
            "So can we arrive at it from somewhere different point of view and we can also look at the witness function of the equivalent optimize."
        ],
        [
            "Asian problem, but let me not go into that now.",
            "Let me just point out that there's another talk here at Ice Email by Jean High student on.",
            "Saturday before lunch.",
            "Who uses the conditional cross covariance operator in the context of learning causal structures for graphical models?",
            "So the third application is in the field of covariate shift correction and local learning in this case.",
            "So with this problem of comparative correction, we assume we have a training set drawn from a distribution P. We have a test set drawn from a different distribution P prime, but we assume that the conditional distributions of Y given X identical.",
            "So only the input shift.",
            "And a number of approaches.",
            "One of them, one of Shimura, have suggested that in such a situation one should re wait the training set.",
            "So do some kind of important sampling.",
            "This code for kind of idea also Maps well into our framework.",
            "What we basically will do is we use the mean of the test points test inputs.",
            "Is all reference and we tried to re wait the training points.",
            "We put these bitter eyes on the training points such that the re weighted mean of the training points becomes as similar as possible to the mean of the test points.",
            "So remember, if the means are the same, then statistically these two distributions are not distinguishable.",
            "So we try to move the training points to mimic the distribution of the test points and this leads us to an optimization problem subject to these constraints.",
            "Of course practice we regularize this problem.",
            "And it turns out that this, specially if the situation is underspecified.",
            "So for instance, the kernel widths are too large for the problem.",
            "This can help a lot.",
            "In a special case where here this set of test points only contains one test point.",
            "This actually leads to a local sample weighting scheme, so it's a method for local learning in that case.",
            "Last application.",
            "Is again related to the previous ones, but maybe a little bit more general and this is something that we've only just started looking at is in the field of measure estimation."
        ],
        [
            "And there's a special case, datasets quashing.",
            "And there have been some related approaches out there.",
            "And in this case, the idea is we try to minimize this discrepancy between the mean of the sample and the mean of the distribution, where we consider an A priori specified class of distributions.",
            "So we take this convex combination of distributions with these constraints and we minimize this quantity and we can rewrite this as a convex for quadratic program with this objective function where this term here is constant.",
            "And it's interesting to look at these other quantities so the difficult ones are these first 2.",
            "Is our expectations of the currents with respect to these measures that we pre specify, But it's interesting to think about what kind of measures.",
            "What kind of combinations of kernels in measures lead to quantities that can be empirically and analytically calculated.",
            "In one example is of course, if both this Gaussian then this is just an integral over several Gaussians which we can do so they can be Gaussians of different widths.",
            "We could also use Dirac measures for the distribution."
        ],
        [
            "In which case we can either try to approximate the training set by I've waited subset of the training set.",
            "This problem has been called datasets squashing before, or we can try to.",
            "Or we can also recover the covariate shift correction algorithm where we try to approximate the test set by measures on the training points.",
            "So that's the algorithm they just talked about before we can recover that as well.",
            "So, so in a way in all these approaches we try to choose a Hilbert space to reflect those properties of our estimates that we're interested in all those properties that we want to accurately estimate.",
            "So in such a measure estimation problem, for instance, we might be only interested in correct."
        ],
        [
            "Estimating the first N moments or or similar things and then we could choose the corresponding Hilbert space to reflect this kind of prior knowledge and thereby maybe make a problem solvable."
        ],
        [
            "Which otherwise wouldn't be solvable.",
            "So let me move back to a PowerPoint and wrap up the talk so we still have some."
        ],
        [
            "Discussion left.",
            "So the last few slides will be about the kernel trick in how people thought about the kernel trick in this embedding into the Hilbert space over these last years, and the whole thing started probably in 1904.",
            "When positive definite kernels were first, to my knowledge, first used and proposed by Hilbert.",
            "In the 60s when they were used in Russia to prove the convergence of the so called potential function method.",
            "So this was already about pattern recognition and it happened at the same Institute in approximately the same time as the development of the hyperplane classifier.",
            "The generalized portray method of African children keys.",
            "But surprisingly, these things were not combined at that time.",
            "And indeed, they were known, but they were sort of considered.",
            "It seems they were considered a little bit useless, as reflected by this quotation from the Duda and Hart book.",
            "Where they say about these kernels that they use is often suggested for the construction of potential functions, but these suggestions are more appealing for their mathematical beauty than their practical usefulness.",
            "I should also mention that in approximation theory and in statistics, Grace Wahba have been working on positive definite kernels for a pretty long time already in parallel, and there have been other communities such as the one of creaking where they also used positive different kernels for a long time.",
            "In machine learning kernels experience, there are any source?",
            "Starting with their use in optimal margin classifiers.",
            "So the 1st paper about optimal margin classifiers was the Wonder Forza go in public in 90, two at Colt and I thought it might be interesting to ask them how that paper was received at the time.",
            "So I talked to them recently in which was quite funny because Ben had Bowser told me that let me read this to do him justice.",
            "He said all I remember is that the paper on Legos got much more attention at Cult, so apparently there was a paper about Lego.",
            "And it's a big yawn, told me that she before she went to to give a practice talk to Santa Cruz to the Group of David Hosler and she said they asked us what flooding may have contributed to this paper because it was obviously weak on the theory side.",
            "It also, she said that they felt the paper was accepted because he was considered an application paper by cult and called wanted to push more application papers.",
            "Then later on could, in accordance with your PhD on support vector machines with bladimir.",
            "They extended this to allow for training errors.",
            "They called it the soft margin classifier originally, but that paper also had a hard time getting accepted at the Machine Learning Journal, and Coconut told me they even had to change the title to be more aligned with what was fashionable at the time, and they changed it into support vector networks.",
            "But a little bit later in the same year I remember we had a discussion in Latimer next office, with Chris Burgess and Vladimir and especially Vladimir felt very strongly that this should not sound like it's related to a neural network, because neural networks, if you wanted to kill neural networks.",
            "So support vector machines number called support vector networks.",
            "And this is where this term was born.",
            "A few years later, we worked on Kernel PCA and in Network also pointed out that one can use this kind of trick to kernel eyes.",
            "Any product algorithm which is sort of.",
            "Sort of obvious, especially in retrospect, although that email and a lot of other people at the time didn't see it, and it's interesting to think about why they didn't see it yesterday.",
            "I had a discussion with the number 2, and somehow he felt that maybe it was because reading was a pure theoretician who just thought of this as some kind of mathematics.",
            "Where is anybody who implemented an SVM's?",
            "So that's including these people listed here who did see it to such people.",
            "It must have been obvious that this is just a subroutine.",
            "The kernel you called the kernel instead of calling a dot product, and you can do it.",
            "From any algorithm in Lyon told me that he actually is also funny.",
            "He at that time he was at 18 to before he went back to France.",
            "He submitted the grant proposal in this grant proposal he suggested to kernelized various algorithms.",
            "But the grant proposal was rejected.",
            "So in the following years whole kernel industry started the wide dissemination dissemination of this idea started.",
            "The first NIPS kernel workshop.",
            "There was the public domain software.",
            "By tossing your hips, there was Chris Burges tutorial paper.",
            "There was the book of Yellow and on the website about kernel machines and also it was notice that the input domain on which the kernels are defined need not be a vector space.",
            "We can also use.",
            "Other kinds of data, and this led to quite some successes, especially in the field of bioinformatics.",
            "Now the modern period in which we may be still are started in my mind or from my point of view, with the development of kernel ICA buyback in Jordan and others.",
            "And interesting thing about this was that this was the first example where kernels were used to solve an optimization problem over a large class with only their functions.",
            "So over the unit ball in reproducing kernel Hilbert space and in this third option you several examples of such.",
            "Problems in such users of the kernel trick and.",
            "Maybe you can all rephrase the kernel trick as saying that if you prefer to work with expectations rather than higher order moments, just map your data into an arcade, chess or map your distribution into that space and then work in this space.",
            "And of course you can ask what comes after modernity.",
            "This may be a postmodern.",
            "Where we will start just to Colonel eyes, ourselves and the whole thing will become self referencial and Colonel papers referring to Colonel Paper said.",
            "Everything will become more more and more irrelevant to reality.",
            "And I don't know whether we're going to enter that.",
            "But maybe there's something for discussion I think thank you very much.",
            "Time for a few questions.",
            "So.",
            "In which kind of problem in general or.",
            "Yeah, so there's no.",
            "Sorry asking how to choose the bandwidth parameter in the answer is as usual in kernel methods.",
            "There's no really convincing methods for this kind of approach, so if somehow the curse in the blessing of kernel methods at the same time, so I mean, there are some ideas, but there's nothing convincing for that.",
            "I have a question actually.",
            "So do you think kernel machines were successful at killing off neural networks or are we post modern era at the danger of a revival of neural networks?",
            "Yes, so certainly there's there's a danger looming on the horizon, and I was actually expecting you're sure to ask something is here.",
            "Yeah, sure, OK, so soon had to ask the author's question.",
            "No, I mean from my point of view, we have an addition to the toolkit and of course I could say now we also do multilayer kernel machines.",
            "And actually when I prepare this talk I look through some of the old papers and I notice that in a very old technical report I had written something about multilayer support vector machines, but I guess at the time I found it too embarrassing to put into a real paper.",
            "Certainly there are problems that can be much better represented by our multilayer system.",
            "It's just that.",
            "But I like about kernel machines.",
            "Is the underlying mathematics and everything works out nicely and is more elegant and I don't yet see something corresponding for neural network.",
            "Certainly you can solve problems will, but we all have our subjective reasons for choosing certain approaches and choosing what to work on.",
            "And certainly if something comes up, or if I come up with an idea of doing something comperable for multilayer system, then why not?",
            "That would be very interesting but.",
            "I'm not sure what will be this kind of thing.",
            "Well, it will be something something starting from heuristic neural networks or something starting from something different, and certainly we have more tools Now at our disposal.",
            "I think kernels are a great tool and maybe they're not limited to one layer or to shallow systems.",
            "Maybe they can be used in deeper systems, and I mean in a way you could say backpropagation has has brought the chain rule to machine learning and people are still using the chain rule even when they're not using backpropagation.",
            "So people will stop using support vector machines probably, although I was pleased to see that one of the.",
            "Best student papers by Unfound.",
            "But this is a real support vector machine paper, so people will stop using support vector machines.",
            "But I don't think they would stop using functional analysis in kernels and stuff like that.",
            "Technology fundamentals.",
            "Yeah, so did you ask?",
            "Is it a or the fundamental theory?",
            "Is your opinion?",
            "Let's say it's a fundamental element of the theory of pattern recognition, but it's certainly not the theory because I mean, as you've seen before in the question of Jean Yves, it's not even a complete theory.",
            "There's alot of things that are open that we cannot answer.",
            "There's a lot of problems that we cannot solve.",
            "Maybe some of them we cannot solve in principle, but some of them we should be able to solve.",
            "So I think actually overall our field is relatively relatively undeveloped.",
            "So far, so turns the theory.",
            "There's still a lot of work to do.",
            "When I started.",
            "One drink.",
            "Join now.",
            "the Bible students have to know about functional analysis to comments.",
            "Machine Learning is a barrier to some people coming into the field or you think it stills.",
            "Still getting.",
            "Yeah, it's it's an interesting point.",
            "I mean certainly if you look at the students now with what they start with, it's different from a few years ago and probably it will be a barrier to some students.",
            "But at the same time it will be more inviting to other students.",
            "So I think we will now see maybe more students from mathematics getting into machine learning, because if they look at it, maybe they get the feeling there's something that they can contribute, whereas a few years ago it didn't look like it was so interesting from a mathematical point of view, although.",
            "I mean, I guess it always depends on what kind of machine learning you did.",
            "If you looked at VC theory, you probably already had lots of mathematics to do 20 years ago in machine learning, but certainly if you're looking at machine learning algorithms, I completely agree with you that now the threshold is a bit higher, not just in terms of kernel methods.",
            "Also, graphical models and nonparametric Bayesian statistics and all that stuff.",
            "It's getting all the little bit more technical than it was ten years ago, but I guess it's a curse and a blessing.",
            "For our field.",
            "We have time for maybe one more question.",
            "Account with you or logical tea and the people particularly.",
            "Are fairly disjoint.",
            "Please keep them urging.",
            "Thanks.",
            "Is there a Christmas?",
            "Will logical AI and statistically I'm merge, so it's a.",
            "It's a tricky question because I'm someone from very much from the statistical side, so I I came into AI because I'm interested in in intelligence, but to me at the time what I saw in logical AI was not satisfying and I sort of went the statistical followed the statistical branch.",
            "But now the statistical branch, especially in kernel methods and for me, if you look at the program with this conference, you will see a lot of work on relational data structure data and things like that.",
            "And maybe there are some possibilities for making contact again, so I wouldn't.",
            "I wouldn't exclude that these contact is is going to be made again, but I don't see it as something immediate.",
            "Let's put it this way.",
            "Great, alright, well let's play."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much swimming for this flattering introduction and thank you all the organizers for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite special for me to speak here in Oregon because I spent some time in Oregon as a student going to school here so it was the first time that I really got to know America.",
                    "label": 0
                },
                {
                    "sent": "So now I feel a bit like a student again here in Oregon so I slightly change the title of my talk and will talk about four things.",
                    "label": 0
                },
                {
                    "sent": "Basically kernels, some applications of kernels.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about something called.",
                    "label": 0
                },
                {
                    "sent": "Kernel means it will become later.",
                    "label": 1
                },
                {
                    "sent": "It will become clear later what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "And finally I'll try to put things a little bit into perspective.",
                    "label": 0
                },
                {
                    "sent": "Most of you probably heard about this thing called the kernel trick by talking about how this kernel trick has evolved over the years, how we thought of it at the beginning, how we think of it now.",
                    "label": 0
                },
                {
                    "sent": "And it's also a little bit related to the actual content of this talk.",
                    "label": 0
                },
                {
                    "sent": "So part of this talk will be well known to most of you, and I'll try to cut that very short.",
                    "label": 0
                },
                {
                    "sent": "It would be maybe the first 5 or 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "So if you're still sleepy, that's that's your chance.",
                    "label": 0
                },
                {
                    "sent": "Afterwards, I'll talk about some applications that some of you may have seen.",
                    "label": 0
                },
                {
                    "sent": "Others may not have seen.",
                    "label": 0
                },
                {
                    "sent": "And finally then I'll talk about something that I hope maybe the majority of you hasn't seen yet.",
                    "label": 0
                },
                {
                    "sent": "So these are the first 5 or 10 minutes now, so I'll start with an example of a very simple pattern recognition algorithm where we are given two classes of data.",
                    "label": 0
                },
                {
                    "sent": "The classes in the circles and we tried to classify a new point X based on whether it's closer to the mean of the one class denoted mu plus.",
                    "label": 0
                },
                {
                    "sent": "So this is the mean of the one class, or closer to the mean of the other class.",
                    "label": 0
                },
                {
                    "sent": "The noted mu minus, which is over here.",
                    "label": 0
                },
                {
                    "sent": "So obviously this induces.",
                    "label": 0
                },
                {
                    "sent": "A separating hyperplane as a decision function with the normal vector.",
                    "label": 0
                },
                {
                    "sent": "That's the vector connecting these two class means.",
                    "label": 0
                },
                {
                    "sent": "But of course, this only solves problems that are linearly separable and the kernel approaches then to pre process the inputs with some mapping that takes our input domain into a dot product space and then we work in that space.",
                    "label": 0
                },
                {
                    "sent": "And why does that help?",
                    "label": 0
                },
                {
                    "sent": "Or why can that help?",
                    "label": 0
                },
                {
                    "sent": "Here's an example 2 dimensional classification problem.",
                    "label": 0
                },
                {
                    "sent": "Separating these crosses from the circles we assume the true decision boundary.",
                    "label": 0
                },
                {
                    "sent": "This is this ellipse, but we don't know if we only have the data.",
                    "label": 0
                },
                {
                    "sent": "Now if we pre process our points by computing all features of order 2.",
                    "label": 0
                },
                {
                    "sent": "Or product of order two, then this problem becomes linearly separable, which you can see by writing the ellipse equation in these new coordinates.",
                    "label": 0
                },
                {
                    "sent": "Now the trick comes in when we see that if we take two such points X&X prime to 2 dimensional points, we map them into that 3 dimensional space using this prescription.",
                    "label": 0
                },
                {
                    "sent": "When we then take the DOT product, we see that this gives a simple binomial formula.",
                    "label": 0
                },
                {
                    "sent": "We can simplify it and write it as the square of the dot product in the original coordinates.",
                    "label": 0
                },
                {
                    "sent": "So that's an example of a simple kernel.",
                    "label": 0
                },
                {
                    "sent": "We can compute the dot product without going into this higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Same thing works for N dimensional inputs and products of order T. This case we end up with this polynomial kernel and basically something similar works for all positive definite kernels.",
                    "label": 0
                },
                {
                    "sent": "So that's a notion well known in mathematics, and whenever we have such a positive definite kernel, we can prove there exists some mapping into another space.",
                    "label": 0
                },
                {
                    "sent": "So mapping fire such that the kernel computes the dot product in that space.",
                    "label": 0
                },
                {
                    "sent": "And one way of visualizing that mapping is by thinking of it as a mapping into a function space.",
                    "label": 0
                },
                {
                    "sent": "So we define the mapping to take a input point X into the kernel centered on that point X.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we use a Gaussian kernel, then at this point X would be mapped into a Gaussian function sitting on that point X.",
                    "label": 0
                },
                {
                    "sent": "Another point X prime would be mapped into another Gaussian, and our space will be defined to be the linear Hull complete completion of the linear.",
                    "label": 0
                },
                {
                    "sent": "All of these kinds of mapped functions, and then we have to define what's the dot product in that space, and we have to define it such that.",
                    "label": 0
                },
                {
                    "sent": "It satisfies this equality because we want the kernel to compute the dot product.",
                    "label": 0
                },
                {
                    "sent": "It turns out this works.",
                    "label": 0
                },
                {
                    "sent": "We can define it this way.",
                    "label": 0
                },
                {
                    "sent": "It's well defined.",
                    "label": 0
                },
                {
                    "sent": "We can extend it linearly and we then have a kernel that computes the DOT product in this function space.",
                    "label": 0
                },
                {
                    "sent": "And one of the nice things about of this kernel is if you substitute K in here and you do the same thing for linear combinations, you will see that the kernel represents PT evaluation on these functions.",
                    "label": 0
                },
                {
                    "sent": "So by this I mean if I have a function in that space, so that's at.",
                    "label": 0
                },
                {
                    "sent": "This point is just an abstract element of a Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "If I want to know the value of this function at some point X, the only thing I have to do is take this other element of the Hilbert space, which is the kernel sitting on X, compute the dot product between the two and I will get my PT evaluation.",
                    "label": 0
                },
                {
                    "sent": "So this is a particular kind of Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "It's a Hilbert space web.",
                    "label": 0
                },
                {
                    "sent": "PT evaluation is a continuous function and can therefore be represented as a dot product.",
                    "label": 0
                },
                {
                    "sent": "So we'll come back.",
                    "label": 0
                },
                {
                    "sent": "We'll use these later on.",
                    "label": 0
                },
                {
                    "sent": "This kind of PT evaluation expression.",
                    "label": 0
                },
                {
                    "sent": "Now let's go back to this example from before and do the same thing in the feature space.",
                    "label": 0
                },
                {
                    "sent": "So now we want to classify points mapped into the feature space again by.",
                    "label": 0
                },
                {
                    "sent": "Computing distances between a test point and the mean of the positive class in the middle of the negative class.",
                    "label": 0
                },
                {
                    "sent": "If we do this and we work it out again.",
                    "label": 0
                },
                {
                    "sent": "So this is a separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "This time in the feature space in the properties in kernel Hilbert space, and if we work it out, it turns out this is a nice expression in terms of kernels.",
                    "label": 0
                },
                {
                    "sent": "The decision function will be thresholded version of a kernel expansion, with kernels sitting on the positive negative points, and it will be such that here this thing.",
                    "label": 0
                },
                {
                    "sent": "If K is a density, for instance a Gaussian, we can think of this as 1000 Windows estimate of the positive class, and we can think of this as a passing Windows estimate of the negative class.",
                    "label": 0
                },
                {
                    "sent": "So this is a passing windows.",
                    "label": 0
                },
                {
                    "sent": "Classifier based on plugging estimates from 1000 windows of the two classes.",
                    "label": 0
                },
                {
                    "sent": "And the support vector machine is quite similar.",
                    "label": 0
                },
                {
                    "sent": "It's also a separating hyperplane in such a feature space, which solves nonlinear classification problem in the input domain.",
                    "label": 0
                },
                {
                    "sent": "And depending on what kind of kernel we use, if we turn up and down linearity, the decision boundary in the input space will become increasingly nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So you can see the behavior is a little bit similar to these behaviors that you get if you change the regularization constant for some of you who know this kind of work.",
                    "label": 0
                },
                {
                    "sent": "So, so far so good.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about some applications, so hopefully this will get a little bit more interesting for you.",
                    "label": 0
                },
                {
                    "sent": "So the application that I want to focus on in the next 10 minutes or so is our applications in computer graphics.",
                    "label": 0
                },
                {
                    "sent": "Starting with the problem of implicit surface fitting.",
                    "label": 0
                },
                {
                    "sent": "So here we have a sampling of a surface is a surface typically living in our three, and sometimes we have corresponding soft surface normals and would like to construct a function, a kernel expansion whose zero level approximates the surface.",
                    "label": 0
                },
                {
                    "sent": "So wherever the surface is, the function is supposed to take the value 0.",
                    "label": 0
                },
                {
                    "sent": "Inside of the object which is delimited by the surface, the function should be negative.",
                    "label": 0
                },
                {
                    "sent": "Outside it should be positive.",
                    "label": 0
                },
                {
                    "sent": "And the number of support vector approaches have been developed for this problem, some of them building on one class support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Someone building a support vector regression, and it's possible to approximate objects with this kind of approach and also to estimate what's called the signed distance function.",
                    "label": 0
                },
                {
                    "sent": "So the scientists function is a particular such function with the additional property that the value of F should always be equal to the distance from the surface multiplied with the minus one.",
                    "label": 0
                },
                {
                    "sent": "If we inside the object.",
                    "label": 0
                },
                {
                    "sent": "And once we have such implicit representations, we can do funny things like intersections of objects and things like that, and one can solve such problems also in fairly large scale.",
                    "label": 0
                },
                {
                    "sent": "So this is some work of our PhD student Christian Waldo, who has solved, for instance, this approximated this object, which is a standard benchmarking computer graphics that has 14 million training points, and you can solve this pretty fast in a few hours.",
                    "label": 0
                },
                {
                    "sent": "And he developed a multi scale approach where you don't just have a single scale of kernel with but you have multiple scales.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can do things like fill holes where you don't have data.",
                    "label": 0
                },
                {
                    "sent": "If you adapt the scales accordingly and so on.",
                    "label": 0
                },
                {
                    "sent": "So yeah, there's another few examples, so all these things are.",
                    "label": 0
                },
                {
                    "sent": "With with a grain of salt, hyperplanes in some feature space associated with the kernel, and then the input space, they look like these complex objects.",
                    "label": 0
                },
                {
                    "sent": "Now if you have tried to apply this to videos, the first thing that comes to mind is you take a video and you'll fit such an implicit surface representation frame wise.",
                    "label": 0
                },
                {
                    "sent": "And if you do that and you get this kind of solutions, so you see each picture is reasonable, but from picture to picture there's certain jumps, which is because you do each of them separately and you don't regularize across time.",
                    "label": 0
                },
                {
                    "sent": "And if you extend this approach and do it in for the you get a solution which is now smoother in time and which also has some interesting properties in terms of interpolation with Christian tried here is he.",
                    "label": 0
                },
                {
                    "sent": "Deleted some frames in this video that shown in red, so wherever you see something red, there were no training points and the red thing scene is purely filled in by the system.",
                    "label": 0
                },
                {
                    "sent": "Now one could come up with the idea of trying to use this to interpolate between different shapes, not just the same person at different times, but really different shapes for like for instance, this monkey in this human.",
                    "label": 0
                },
                {
                    "sent": "But it turns out this doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "So the deformation states are pretty ugly and I don't want to claim that the final human is beautiful, but the things in between are probably even worse.",
                    "label": 0
                },
                {
                    "sent": "So we we try to think of something different and some different approach that more directly tackles this problem of morphing as people in computer graphics call it, so was the morphing problem.",
                    "label": 0
                },
                {
                    "sent": "We have two objects, maybe from somehow the similar class or similar class of objects, but not not too similar.",
                    "label": 0
                },
                {
                    "sent": "And we try to compute something in between.",
                    "label": 0
                },
                {
                    "sent": "And obviously if we just take a linear combination of images, that's not going to work.",
                    "label": 0
                },
                {
                    "sent": "We somehow have to be able to combine corresponding features onto the on these two objects, so we need to know correspondence we need to do this kind of correspondence field, or sometimes it's called a warp field.",
                    "label": 0
                },
                {
                    "sent": "And once we have that, we can interpolate and extrapolate images, almost like in a linear space.",
                    "label": 0
                },
                {
                    "sent": "So how do we estimate this correspondence field?",
                    "label": 0
                },
                {
                    "sent": "Well, let's assume our objects live in some joint domain.",
                    "label": 0
                },
                {
                    "sent": "Then this warp or this correspondence field is a mapping from that domain onto the domain, and we assume that as training points were given surface points of 1 objects and surface points of the other object.",
                    "label": 0
                },
                {
                    "sent": "Now if we knew if these points were in correspondence, so we knew which X point corresponds to which set point, we could just do regression.",
                    "label": 0
                },
                {
                    "sent": "That would be straightforward, but what do we do if they're not in correspondence?",
                    "label": 0
                },
                {
                    "sent": "It turns out that a fairly simple idea can already solve this problem relatively well, and the idea is that this warp should be such that relative to a starting point X, the first object should somehow.",
                    "label": 0
                },
                {
                    "sent": "Locally it looks similar to the second object as seen from the warp point.",
                    "label": 0
                },
                {
                    "sent": "So we formalize this as a cost function that depends on all these terms and the simplest cost function that we use is something that will build on these implicit surface embeddings that are shown you before.",
                    "label": 0
                },
                {
                    "sent": "So remember we were estimating the scientists and function of objects, which means given an object we estimated function which is 0 on the surface of the object and whose value is equal to the distance from the object elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So let's consider this as a feature to construct.",
                    "label": 0
                },
                {
                    "sent": "Request function, so think of these functions F1F2 as the signed distance functions of our two objects.",
                    "label": 0
                },
                {
                    "sent": "Then we can use regularizers of the type like this one or cost functions at this type.",
                    "label": 0
                },
                {
                    "sent": "So the signed distance function of the first object or this is the distance of point X to the surface of the first object.",
                    "label": 0
                },
                {
                    "sent": "We want this to be similar to the distance of the warp point to the second object.",
                    "label": 0
                },
                {
                    "sent": "Of course, this doesn't completely constrain the mapping, but it turns out if you Additionally use standard support vector stagger style regularizer so it's a protector regression approach it already more or less constraints the mapping and sometimes we can do better by also using higher order properties of these scientists functions.",
                    "label": 0
                },
                {
                    "sent": "So the kind of optimization problems that we solve are just similar or are generalizations of support vector regression where we fit component functions in the modification is here that we have this additional.",
                    "label": 0
                },
                {
                    "sent": "Integral or approximate integral over this locational cost, and here this is just in case we if we have some labeled training data X set input output, we can also use this term, but we do not necessarily have that, so let's look first.",
                    "label": 0
                },
                {
                    "sent": "Edit a simple toy example and then some realistic ones.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example in 2D.",
                    "label": 0
                },
                {
                    "sent": "This is the first object.",
                    "label": 0
                },
                {
                    "sent": "This is the second object.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you take a linear combination of these two pictures you won't get anything good.",
                    "label": 0
                },
                {
                    "sent": "If you use the scientists cost function, you already get this kind of solution, so this is not a 5050 mix of these tools.",
                    "label": 0
                },
                {
                    "sent": "Yeah, take the Warfield that Maps this to this and sort of apply 50% of that Warfield to deform this object and what you can see here.",
                    "label": 0
                },
                {
                    "sent": "This is the war field.",
                    "label": 0
                },
                {
                    "sent": "What it does it?",
                    "label": 0
                },
                {
                    "sent": "It shrinks this protrusion up here and it grows a new one down here.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work perfectly as you can see here if we Additionally use the normals in our cost function, then it works pretty well and the reason is that if you use the normals then it costs too much, for instance toward this point on this one because they have different normals.",
                    "label": 0
                },
                {
                    "sent": "So then the Warfield will actually move down this protrusion and then you get a fairly good solution.",
                    "label": 0
                },
                {
                    "sent": "And this is all this is automatic, so nobody was clicking on points here.",
                    "label": 0
                },
                {
                    "sent": "Here's another example, which is also fully automatic, so the input is this object is Queen.",
                    "label": 0
                },
                {
                    "sent": "This object here, which is a pawn, and we use the scientist since function cost function no landmark points, no color information in these objects in between there are completely synthetic.",
                    "label": 0
                },
                {
                    "sent": "Coming back to our original problem of moving the heads, we have the same kind of approach here.",
                    "label": 0
                },
                {
                    "sent": "This time we use normals in addition to moving the head turns out to be a little bit harder, and if we use the enormous it gets a bit better.",
                    "label": 0
                },
                {
                    "sent": "So here this time we have this male head and the female head over here and these three heads are synthetic, computed by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is a magnified view of this 5050 head, which is relatively good.",
                    "label": 0
                },
                {
                    "sent": "We can also use it to do things like texture mapping, so we have one object without a text, so we have another object from sort of the same class, but actually quite different shape because all this stuff up here is missing and we can use this approach to map this texture onto this fast.",
                    "label": 0
                },
                {
                    "sent": "If we Additionally click on some points XY corresponding to each other, we can also move more complex up to our more diverse objects into each other, and I think I have another example.",
                    "label": 0
                },
                {
                    "sent": "In the next slide, which is some data that we generated for our Department of Physiology, they wanted to do experiments with this kind of stimuli.",
                    "label": 0
                },
                {
                    "sent": "When we move some humans into monkeys.",
                    "label": 0
                },
                {
                    "sent": "So for these applications in now I want to spend the rest of the main part of the talk on kernel means, which is something that came up during the last years and I think it's it's quite an intriguing direction of kernel machines research.",
                    "label": 0
                },
                {
                    "sent": "For this I have to move into the world of Latech.",
                    "label": 0
                },
                {
                    "sent": "And I should say this is joint work with the people listed here.",
                    "label": 1
                },
                {
                    "sent": "So let me first return to this example that I showed you at the beginning of my talk.",
                    "label": 0
                },
                {
                    "sent": "So I was showing this this simple example of a classifier.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, here where you take some positive class and negative class, you map them into the feature space and compute the means in those spaces denoted by mu of X&Y are so this time I'll call the positive class of this sample here, so in positive points I'll call it X, the negative points I'll call them Y.",
                    "label": 0
                },
                {
                    "sent": "And I'm assuming that my input domain is compact or satisfy certain conditions Apple and have a positive definite kernel on this domain and all this is happening in their properties in kernel Hilbert space associated with that kernel.",
                    "label": 0
                },
                {
                    "sent": "So I take my positive Class X, map it into the feature space.",
                    "label": 0
                },
                {
                    "sent": "This is the mean.",
                    "label": 0
                },
                {
                    "sent": "Do the same for the negative class.",
                    "label": 0
                },
                {
                    "sent": "So these are my means down here where I've used this representation K of my mapping into the feature space.",
                    "label": 0
                },
                {
                    "sent": "Remember, I told you that we can think of it as a space of functions.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Let's keep it a bit more exciting, and obviously this kind of approach.",
                    "label": 0
                },
                {
                    "sent": "So I told you before separating surface is the hyperplane with normal vector, which is the vector connecting these two class means.",
                    "label": 0
                },
                {
                    "sent": "Now obviously if this vector and actually we came up with this example when we were writing the introductory chapter of our book, and we thought this is nice, but it's nice that it turns out to be a passing windows based classifier, but we didn't.",
                    "label": 0
                },
                {
                    "sent": "Continuing this direction and only recently, we notice that this is actually much more interesting than we thought.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this thing so this vector, obviously if it's a normal vector hyperplane only makes sense if it's non 0.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a technical condition, but it turns out if you think about what happens if it's zero, then this can be the starting point of something more interesting.",
                    "label": 0
                },
                {
                    "sent": "And if you do think about this, well, obviously if you do any input space, it just means that these two samples have the same mean, So what?",
                    "label": 0
                },
                {
                    "sent": "If you do it in the space associated with the polynomial kernel of degree D. So remember, the polynomial kernel computes products of D input coordinates.",
                    "label": 0
                },
                {
                    "sent": "Now, this mean just computes the mean over the whole training set.",
                    "label": 0
                },
                {
                    "sent": "So the mean of products of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "It just means that these directions are basically monomials expectations of our means of monomials.",
                    "label": 0
                },
                {
                    "sent": "In other words, there are empirical moments of order, the.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if we use a polynomial kernel of order D and mu of X is mu is equal to mu of Y.",
                    "label": 0
                },
                {
                    "sent": "So this means coincide.",
                    "label": 0
                },
                {
                    "sent": "It means that for these two distributions on moments up to already or empirical moments up to ALDI coincide.",
                    "label": 0
                },
                {
                    "sent": "Now the next question is, what if we use a kernel that's even more high dimensional than a death order polynomial kernel?",
                    "label": 0
                },
                {
                    "sent": "What if we use a kernel which is strictly positive definite, in which Case No matter how many training points you map into the feature space, there will always be linearly independent.",
                    "label": 0
                },
                {
                    "sent": "That means it's potentially infinite dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "If you think about this, it's actually fairly straightforward to show that if we take a strictly positive definite kernel.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance, a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then the means coincide if and only if the datasets coincide.",
                    "label": 0
                },
                {
                    "sent": "So all the points have to be the same.",
                    "label": 0
                },
                {
                    "sent": "So this somehow means that the mean of the set of points in the feature space remembers each point that has contributed to the mean.",
                    "label": 0
                },
                {
                    "sent": "And actually, this is also a point that we had seen at some point before.",
                    "label": 0
                },
                {
                    "sent": "I remember during preparing this talk we had a workshop from 10 years ago in Japan where we were trying to come up with a clustering algorithm in the feature space and we notice this property that if you take such linear combinations of points, they somehow remember each point that contribute contributes to them.",
                    "label": 0
                },
                {
                    "sent": "And this was I remember this was in discussions with John Platt, quits Watkins and Nello Cristianini, and we were trying to come up with a name for this algorithm which would have the acronym FUGU, which was very difficult, especially with the use.",
                    "label": 0
                },
                {
                    "sent": "I think one of the user at the Institute for Heuristics.",
                    "label": 0
                },
                {
                    "sent": "But we never published this algorithm.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so I remembered it now.",
                    "label": 0
                },
                {
                    "sent": "And let's take a look at this mean map a little bit more closely.",
                    "label": 0
                },
                {
                    "sent": "So the map just remember recall.",
                    "label": 0
                },
                {
                    "sent": "It took this image sample.",
                    "label": 0
                },
                {
                    "sent": "This M training points and map them into the mean of the images in the feature space.",
                    "label": 0
                },
                {
                    "sent": "If you take this point, mu of X in the feature space and I'll take the dot product with an arbitrary function of that space.",
                    "label": 0
                },
                {
                    "sent": "So I just substituted in here.",
                    "label": 0
                },
                {
                    "sent": "I can use the linearity of the dot product and then I can use the fact that the kernel represents point evaluation of the function.",
                    "label": 0
                },
                {
                    "sent": "So here we get the function values, which means we get the mean of the function on that sample.",
                    "label": 0
                },
                {
                    "sent": "So this mean in the feature space represents the operation of taking the sample mean of a function.",
                    "label": 1
                },
                {
                    "sent": "And another thing that's interesting to workout is if the distance between these two means, which is the quantity that we started with.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this thing.",
                    "label": 0
                },
                {
                    "sent": "So that's just a vector in that Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "This difference here, and we can rewrite the length of this vector is the same as the maximum dot product that we get by taking the product between this vector and vectors from the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "So let's just koshish routes.",
                    "label": 0
                },
                {
                    "sent": "Inequality is just linear algebra.",
                    "label": 0
                },
                {
                    "sent": "So now we have rewritten it like this and now we can use this effect.",
                    "label": 0
                },
                {
                    "sent": "That's the main represents taking averages and rewrite things like this.",
                    "label": 0
                },
                {
                    "sent": "Now we have this supremum over the unit sphere of the difference of these two averages.",
                    "label": 0
                },
                {
                    "sent": "The average of the function on the example and the function on the wise sample.",
                    "label": 0
                },
                {
                    "sent": "So this distance between these two means in the Dark Ages is actually in the way that geometrically computing the solution of a pretty high dimensional optimization problem over the unit ball in infinite dimensional.",
                    "label": 1
                },
                {
                    "sent": "Hilbert space and we'll come back to this point later.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting to look at the The witness function of this quantity, so the function which actually realises the maximum value for this thing.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is a maximum in this case, and this function is just the function that's parallel to this thing here, but normalized to length one, so we can write it like this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we want to plot it, we have to point evaluated by taking DOT products with the kernel and here's an example.",
                    "label": 0
                },
                {
                    "sent": "If we have samples X&Y drawn from a Gaussian in Laplacian in both of them, zero mean and unit variance.",
                    "label": 0
                },
                {
                    "sent": "Then this witness is the red function, so that goes into Laplacian 1, iron, black, dashed and dotted.",
                    "label": 0
                },
                {
                    "sent": "In the read function, is this witness so you can see the witness function is positive whenever the Laplacian exceeds the Gaussian, it's negative whenever the Gaussian exceeds the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So somehow this function will will best detect differences between Gaussian interpolations from the unit ball of this arc AHS, which in this case was a Gaussian our cages.",
                    "label": 0
                },
                {
                    "sent": "So it's a reasonably smooth function and the smoothness will depend on what our cages you choose.",
                    "label": 0
                },
                {
                    "sent": "But it's a function that tries to detect these differences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's nice, but it actually becomes even more interesting when you do the same thing for measures if you, rather than mapping samples into that space, you can map measures into that Hilbert space in the construction is exactly the same, so there's really nothing surprising coming now, so we have some technical conditions that certain expectations have to exist.",
                    "label": 0
                },
                {
                    "sent": "These are fairly benign, for instance, that's sufficient if.",
                    "label": 0
                },
                {
                    "sent": "The image of the input data under the feature map is contained in some ball like, which is the case for Gaussian and it goes in case the unit ball.",
                    "label": 0
                },
                {
                    "sent": "But let's assume these conditions hold true.",
                    "label": 0
                },
                {
                    "sent": "Then we can map such a measure into our particular Hilbert space by taking the expectation of our feature map.",
                    "label": 0
                },
                {
                    "sent": "So think of it as this kernel map, the expectation where X is drawn according to this measure P. So this quantity will exist if this above conditions were satisfied, and so this is not the expectation of the mapped measure in the feature space.",
                    "label": 1
                },
                {
                    "sent": "And as before we can workout what happens if we take the DOT product between this quantity and some arbitrary function of our space and it will turn out.",
                    "label": 0
                },
                {
                    "sent": "This will give us the expectation of that function.",
                    "label": 0
                },
                {
                    "sent": "Likewise, we can compute the difference between 2 means in that space, and we can rewrite this also as a supremum over the unit ball of the difference between two expectations with respect to these two different distributions P&Q at the same function.",
                    "label": 1
                },
                {
                    "sent": "Now remember that in the finite sample case we had this surprising property that the mean remembered all the points that were contributed to it.",
                    "label": 0
                },
                {
                    "sent": "So that means we're only identically if the point sets were identical.",
                    "label": 0
                },
                {
                    "sent": "In other words, this mapping mu was one to one.",
                    "label": 0
                },
                {
                    "sent": "So how about now for the measures and it turns out something similar holds.",
                    "label": 0
                },
                {
                    "sent": "And to see this we first have to appeal to a classic result from probability theory, which deals with the quantity very similar to this one.",
                    "label": 0
                },
                {
                    "sent": "The only difference being that now rather than having the unit sphere of a universal or unit sphere of representing Hilbert space, we have the set of all continuous bounded functions on our domain.",
                    "label": 0
                },
                {
                    "sent": "And this theorem tells tells us that.",
                    "label": 0
                },
                {
                    "sent": "This quantity here with the Supreme over all continuous functions, is 0 if and only if the two distributions are identical.",
                    "label": 0
                },
                {
                    "sent": "So clearly if they are identical then this thing will always be 0.",
                    "label": 0
                },
                {
                    "sent": "So the interesting direction is from here to here.",
                    "label": 0
                },
                {
                    "sent": "And this right here and get stronger the more functions we use here.",
                    "label": 0
                },
                {
                    "sent": "So if this function class is large enough, the theorem says if this function is large enough.",
                    "label": 1
                },
                {
                    "sent": "For instance, if it's all continuous function, then actually the converse is true.",
                    "label": 0
                },
                {
                    "sent": "So there will always be a continuous function that detects the difference between P&Q.",
                    "label": 0
                },
                {
                    "sent": "By taking these means.",
                    "label": 0
                },
                {
                    "sent": "Now we can convert this into a result for our setting.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simply by considering our first by considering a repetition kernel Hilbert space which is dense in the set of continuous functions.",
                    "label": 0
                },
                {
                    "sent": "And then taking the unit ball in that repetition with space for taking the unit ball doesn't make a difference.",
                    "label": 0
                },
                {
                    "sent": "The scaling doesn't hurt us 'cause this is an equality here, and revenues in kernel Hilbert spaces that are dense in the set of continuous functions have been studied before in the context of consistency of support vector machines and they have been called universal kernel space time, but.",
                    "label": 0
                },
                {
                    "sent": "An example of universal kernel is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So if we do this.",
                    "label": 0
                },
                {
                    "sent": "We get this result here.",
                    "label": 0
                },
                {
                    "sent": "If the kernel is universal.",
                    "label": 0
                },
                {
                    "sent": "Two distributions are the same if and only if their images in the feature space are the same.",
                    "label": 0
                },
                {
                    "sent": "So also this mapping is 1 to one and therefore invertible on its image, and this image has in a slightly different context, being called the marginal polytope.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting to note that this concept of the mean mapping generalizes the moment generating function, which you can define for a random variable X with distribution P. So the moment generating function you just take this quantity the exponential of X dot product with an open argument and take the expectation of this.",
                    "label": 0
                },
                {
                    "sent": "And from this you can by taking derivatives you can reconstruct all moments of that random variable.",
                    "label": 0
                },
                {
                    "sent": "And if you recall, or you may or may not know.",
                    "label": 0
                },
                {
                    "sent": "If not, I'll tell you now that this this thing, here this exponential thing is actually a universal kernel.",
                    "label": 0
                },
                {
                    "sent": "This explain.",
                    "label": 0
                },
                {
                    "sent": "This is the exponential kernel.",
                    "label": 1
                },
                {
                    "sent": "When can show this is a so called conformal modification of a Gaussian kernel, which is also.",
                    "label": 0
                },
                {
                    "sent": "Universal, so it's Hilbert space is dense in the set of continuous functions it satisfies the conditions of this result.",
                    "label": 1
                },
                {
                    "sent": "Here, therefore, everything that I said before holds true for this kind of thing, but we can also plug in other universal kernels in here.",
                    "label": 0
                },
                {
                    "sent": "No, before I've shown you several times how to convert such different vectors into quantities like this, which suggests that we can also handle them using methods of statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "And indeed if you take here the difference vector between distribution and sample from that distribution.",
                    "label": 0
                },
                {
                    "sent": "You can rewrite it like this so we know it's an expectation of a function, and here we have the mean of the same function over some function class.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the kind of quantities that are studied by VC theory.",
                    "label": 0
                },
                {
                    "sent": "It will pack theory and when combined this kind of quantity using uniform convergence methods when needs certain deliberately more complicated.",
                    "label": 0
                },
                {
                    "sent": "But if basically if the volume averages of that unit ball.",
                    "label": 0
                },
                {
                    "sent": "With the given distribution are well behaved and this thing will go to zero like one over square root of him.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's not not talk about some.",
                    "label": 0
                },
                {
                    "sent": "Well before I talk about applications, maybe just one word about one more word about this.",
                    "label": 0
                },
                {
                    "sent": "What does it mean if this thing here goes to zero?",
                    "label": 0
                },
                {
                    "sent": "It's not quite the same as saying we are estimating this measure because it's certainly not saying that we can estimate a measure in a general setting independent of the problem, because that's an impossible problem.",
                    "label": 0
                },
                {
                    "sent": "So what this is this is just saying is that if we look at this problem through the unit ball of our cages and then at some point we cannot detect differences anymore.",
                    "label": 0
                },
                {
                    "sent": "So our unit ball which we have to choose ourselves, we have to choose the arcade chess by specifying what kind of differences we are interested in at some point.",
                    "label": 0
                },
                {
                    "sent": "Maybe that unit problem will not contain functions that are sufficiently non smooth such as to be able to detect differences between these anymore.",
                    "label": 0
                },
                {
                    "sent": "And the whole thing is a little bit similar to.",
                    "label": 0
                },
                {
                    "sent": "Publix concept of weak convergence of risks.",
                    "label": 0
                },
                {
                    "sent": "So remember in the VC bonds you don't study convergence.",
                    "label": 0
                },
                {
                    "sent": "Convergence of the empirical measure to the true measure.",
                    "label": 0
                },
                {
                    "sent": "You study convergence of risks, so you have a risk which depends on the empirical measure and that will measure you and you prove that this kind of risk goes to 0, but that's a weaker condition than the actual measures conversion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at some applications of this kind of stuff and I probably I don't have time to go through all of them, but I want to cover at least a few of them.",
                    "label": 0
                },
                {
                    "sent": "One of them is the two sample problem, and some of you might have seen this because Arthur Gretton gave a talk about some of this at Nips last year.",
                    "label": 0
                },
                {
                    "sent": "But let me just spend 2 minutes and say a little bit about it.",
                    "label": 0
                },
                {
                    "sent": "So we in this case we assume we have two samples X&Y, drawn from distributions P&Q respectively, and based on these samples we want to decide whether P is equal to Q or not.",
                    "label": 0
                },
                {
                    "sent": "So if we workout this difference of the vectors, we can work it out in terms of kernels.",
                    "label": 0
                },
                {
                    "sent": "It's an expression that only depends on kernel.",
                    "label": 0
                },
                {
                    "sent": "Is these three sum of three expectations, which you can we can write as a single expectations over this quantity here, which is by the way also positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "But also it's the kernel of a so called you statistics.",
                    "label": 0
                },
                {
                    "sent": "But don't worry bout that is this quantity with which these kind of pairwise comparisons and in terms of this thing we can write our difference.",
                    "label": 0
                },
                {
                    "sent": "M. And actually will let me call it D squared.",
                    "label": 0
                },
                {
                    "sent": "This difference.",
                    "label": 0
                },
                {
                    "sent": "We can write it as the.",
                    "label": 0
                },
                {
                    "sent": "Expectation over this kernel H and we can give an estimator the hat sample based estimator which is summing over these ages only, excluding the terms whereis equal today in order to avoid and bias.",
                    "label": 0
                },
                {
                    "sent": "And one can prove that this isn't that an unbiased estimator of this discrepancy between the two means.",
                    "label": 0
                },
                {
                    "sent": "And one of the nice things if you look at this formula, because this is what one actually has to compute to perform such a 2 sample test is just.",
                    "label": 0
                },
                {
                    "sent": "It's trivial to implement, it's just a double for loop.",
                    "label": 0
                },
                {
                    "sent": "So we can do it relatively fast.",
                    "label": 0
                },
                {
                    "sent": "And also we can do it basically for any kind of data where one can define kernels.",
                    "label": 0
                },
                {
                    "sent": "So we can also do it for strings and graphs and other kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "This thing can.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shown so these empirical estimate can be shown to converge to the true quantity in probability with a relatively fast rate.",
                    "label": 1
                },
                {
                    "sent": "So this is again uniform convergence style bound and one could use this as the basis for a test.",
                    "label": 1
                },
                {
                    "sent": "But actually it doesn't work very well, which is because as most of us know, uniform convergence bonds are often very loose and pessimistic.",
                    "label": 0
                },
                {
                    "sent": "So in practice, I have to admit that actually classical statistics works better in this case if we study the asymptotic distribution of this quantity here, so this discrepancy between the estimated difference in the means and the true difference in the means.",
                    "label": 0
                },
                {
                    "sent": "Then one can study what this converters to both in the case P non equal to Q and in the case P equal to Q in the one case we get a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In the other case we get something more complicated and I don't want to go into details on it.",
                    "label": 0
                },
                {
                    "sent": "But based on this one can devise a test and this test will use a null hypothesis that P is equal to Q.",
                    "label": 0
                },
                {
                    "sent": "So maybe just to give you the idea.",
                    "label": 0
                },
                {
                    "sent": "So P is equal to Q, then this thing would be 0.",
                    "label": 0
                },
                {
                    "sent": "That's our null hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If we then observe based on a sample, if we have a relatively large sample and we notice that the head is significantly different from zero and we know something about the distribution of the hat, then we can sort of say how unlikely it is that in fact the null hypothesis Wister, or how likely it is that the null hypothesis was true or unlikely was rejected in building on that, one can test things.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second application is in the field of dependence measures.",
                    "label": 0
                },
                {
                    "sent": "So some of you might have heard of methods have methods like Colonel ICA and kernel constraint covariance and then the Hilbert Schmidt independence criterion and all of them treat this problem of independence.",
                    "label": 0
                },
                {
                    "sent": "So you have two random variables.",
                    "label": 0
                },
                {
                    "sent": "X&Y are drawn from some joint distribution PFPXY with these marginals and we want to know whether this distribution factorizes.",
                    "label": 0
                },
                {
                    "sent": "And the main idea in these methods that work very well.",
                    "label": 0
                },
                {
                    "sent": "It's very, very similar to the idea that I showed you before when we were rewriting a distance as an optimization problem over the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "The main idea is that.",
                    "label": 0
                },
                {
                    "sent": "2 random variables X&Y are independent if and only if for all continuous functions FG the covariance of F of X&G of wire vanish is.",
                    "label": 0
                },
                {
                    "sent": "So again, this is like a.",
                    "label": 0
                },
                {
                    "sent": "It's a simple linear criterion, but it's enforced over a large class of nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "And it turns out again, we can do this over the unit sphere of a reproducing kernel Hilbert space and then get independent criteria that work very well.",
                    "label": 0
                },
                {
                    "sent": "But now I want to.",
                    "label": 0
                },
                {
                    "sent": "Show I'm not really sure, but tell you that we can recover one of these algorithms exactly using the kind of argument that I talked about in the last 10 minutes, and to do this we just have to map the joint distribution into our Hilbert space and we have to map the product of the marginals into our Hilbert space and then we use this quantity.",
                    "label": 0
                },
                {
                    "sent": "This difference vector the norm of this difference vector as a measure of dependence.",
                    "label": 0
                },
                {
                    "sent": "So we just check how different are these two distributions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we then use a kernel now we need a kernel on X&Y.",
                    "label": 1
                },
                {
                    "sent": "If we unit could use a kernel that factorizes into two kernels on the X&Y.",
                    "label": 0
                },
                {
                    "sent": "We can show that.",
                    "label": 0
                },
                {
                    "sent": "In this case this quantity.",
                    "label": 0
                },
                {
                    "sent": "The square of this quantity is equal to the so called Hilbert Schmidt norm of the covariance operator between the two arcade.",
                    "label": 1
                },
                {
                    "sent": "Yes, it's a generalization of the approval for menus Norm.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the empirical estimate of this quantity, we get this quote this thing here.",
                    "label": 0
                },
                {
                    "sent": "This trace, which is exactly the same thing that has been studied before and called the Hilbert Schmidt independence criterion.",
                    "label": 0
                },
                {
                    "sent": "So can we arrive at it from somewhere different point of view and we can also look at the witness function of the equivalent optimize.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian problem, but let me not go into that now.",
                    "label": 0
                },
                {
                    "sent": "Let me just point out that there's another talk here at Ice Email by Jean High student on.",
                    "label": 0
                },
                {
                    "sent": "Saturday before lunch.",
                    "label": 0
                },
                {
                    "sent": "Who uses the conditional cross covariance operator in the context of learning causal structures for graphical models?",
                    "label": 1
                },
                {
                    "sent": "So the third application is in the field of covariate shift correction and local learning in this case.",
                    "label": 0
                },
                {
                    "sent": "So with this problem of comparative correction, we assume we have a training set drawn from a distribution P. We have a test set drawn from a different distribution P prime, but we assume that the conditional distributions of Y given X identical.",
                    "label": 0
                },
                {
                    "sent": "So only the input shift.",
                    "label": 0
                },
                {
                    "sent": "And a number of approaches.",
                    "label": 0
                },
                {
                    "sent": "One of them, one of Shimura, have suggested that in such a situation one should re wait the training set.",
                    "label": 0
                },
                {
                    "sent": "So do some kind of important sampling.",
                    "label": 0
                },
                {
                    "sent": "This code for kind of idea also Maps well into our framework.",
                    "label": 0
                },
                {
                    "sent": "What we basically will do is we use the mean of the test points test inputs.",
                    "label": 0
                },
                {
                    "sent": "Is all reference and we tried to re wait the training points.",
                    "label": 0
                },
                {
                    "sent": "We put these bitter eyes on the training points such that the re weighted mean of the training points becomes as similar as possible to the mean of the test points.",
                    "label": 0
                },
                {
                    "sent": "So remember, if the means are the same, then statistically these two distributions are not distinguishable.",
                    "label": 0
                },
                {
                    "sent": "So we try to move the training points to mimic the distribution of the test points and this leads us to an optimization problem subject to these constraints.",
                    "label": 0
                },
                {
                    "sent": "Of course practice we regularize this problem.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this, specially if the situation is underspecified.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the kernel widths are too large for the problem.",
                    "label": 0
                },
                {
                    "sent": "This can help a lot.",
                    "label": 0
                },
                {
                    "sent": "In a special case where here this set of test points only contains one test point.",
                    "label": 0
                },
                {
                    "sent": "This actually leads to a local sample weighting scheme, so it's a method for local learning in that case.",
                    "label": 0
                },
                {
                    "sent": "Last application.",
                    "label": 0
                },
                {
                    "sent": "Is again related to the previous ones, but maybe a little bit more general and this is something that we've only just started looking at is in the field of measure estimation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a special case, datasets quashing.",
                    "label": 0
                },
                {
                    "sent": "And there have been some related approaches out there.",
                    "label": 0
                },
                {
                    "sent": "And in this case, the idea is we try to minimize this discrepancy between the mean of the sample and the mean of the distribution, where we consider an A priori specified class of distributions.",
                    "label": 0
                },
                {
                    "sent": "So we take this convex combination of distributions with these constraints and we minimize this quantity and we can rewrite this as a convex for quadratic program with this objective function where this term here is constant.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting to look at these other quantities so the difficult ones are these first 2.",
                    "label": 0
                },
                {
                    "sent": "Is our expectations of the currents with respect to these measures that we pre specify, But it's interesting to think about what kind of measures.",
                    "label": 0
                },
                {
                    "sent": "What kind of combinations of kernels in measures lead to quantities that can be empirically and analytically calculated.",
                    "label": 0
                },
                {
                    "sent": "In one example is of course, if both this Gaussian then this is just an integral over several Gaussians which we can do so they can be Gaussians of different widths.",
                    "label": 0
                },
                {
                    "sent": "We could also use Dirac measures for the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which case we can either try to approximate the training set by I've waited subset of the training set.",
                    "label": 0
                },
                {
                    "sent": "This problem has been called datasets squashing before, or we can try to.",
                    "label": 0
                },
                {
                    "sent": "Or we can also recover the covariate shift correction algorithm where we try to approximate the test set by measures on the training points.",
                    "label": 0
                },
                {
                    "sent": "So that's the algorithm they just talked about before we can recover that as well.",
                    "label": 0
                },
                {
                    "sent": "So, so in a way in all these approaches we try to choose a Hilbert space to reflect those properties of our estimates that we're interested in all those properties that we want to accurately estimate.",
                    "label": 0
                },
                {
                    "sent": "So in such a measure estimation problem, for instance, we might be only interested in correct.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimating the first N moments or or similar things and then we could choose the corresponding Hilbert space to reflect this kind of prior knowledge and thereby maybe make a problem solvable.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which otherwise wouldn't be solvable.",
                    "label": 0
                },
                {
                    "sent": "So let me move back to a PowerPoint and wrap up the talk so we still have some.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discussion left.",
                    "label": 0
                },
                {
                    "sent": "So the last few slides will be about the kernel trick in how people thought about the kernel trick in this embedding into the Hilbert space over these last years, and the whole thing started probably in 1904.",
                    "label": 0
                },
                {
                    "sent": "When positive definite kernels were first, to my knowledge, first used and proposed by Hilbert.",
                    "label": 0
                },
                {
                    "sent": "In the 60s when they were used in Russia to prove the convergence of the so called potential function method.",
                    "label": 0
                },
                {
                    "sent": "So this was already about pattern recognition and it happened at the same Institute in approximately the same time as the development of the hyperplane classifier.",
                    "label": 0
                },
                {
                    "sent": "The generalized portray method of African children keys.",
                    "label": 0
                },
                {
                    "sent": "But surprisingly, these things were not combined at that time.",
                    "label": 0
                },
                {
                    "sent": "And indeed, they were known, but they were sort of considered.",
                    "label": 0
                },
                {
                    "sent": "It seems they were considered a little bit useless, as reflected by this quotation from the Duda and Hart book.",
                    "label": 0
                },
                {
                    "sent": "Where they say about these kernels that they use is often suggested for the construction of potential functions, but these suggestions are more appealing for their mathematical beauty than their practical usefulness.",
                    "label": 0
                },
                {
                    "sent": "I should also mention that in approximation theory and in statistics, Grace Wahba have been working on positive definite kernels for a pretty long time already in parallel, and there have been other communities such as the one of creaking where they also used positive different kernels for a long time.",
                    "label": 0
                },
                {
                    "sent": "In machine learning kernels experience, there are any source?",
                    "label": 0
                },
                {
                    "sent": "Starting with their use in optimal margin classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the 1st paper about optimal margin classifiers was the Wonder Forza go in public in 90, two at Colt and I thought it might be interesting to ask them how that paper was received at the time.",
                    "label": 0
                },
                {
                    "sent": "So I talked to them recently in which was quite funny because Ben had Bowser told me that let me read this to do him justice.",
                    "label": 0
                },
                {
                    "sent": "He said all I remember is that the paper on Legos got much more attention at Cult, so apparently there was a paper about Lego.",
                    "label": 0
                },
                {
                    "sent": "And it's a big yawn, told me that she before she went to to give a practice talk to Santa Cruz to the Group of David Hosler and she said they asked us what flooding may have contributed to this paper because it was obviously weak on the theory side.",
                    "label": 0
                },
                {
                    "sent": "It also, she said that they felt the paper was accepted because he was considered an application paper by cult and called wanted to push more application papers.",
                    "label": 0
                },
                {
                    "sent": "Then later on could, in accordance with your PhD on support vector machines with bladimir.",
                    "label": 0
                },
                {
                    "sent": "They extended this to allow for training errors.",
                    "label": 0
                },
                {
                    "sent": "They called it the soft margin classifier originally, but that paper also had a hard time getting accepted at the Machine Learning Journal, and Coconut told me they even had to change the title to be more aligned with what was fashionable at the time, and they changed it into support vector networks.",
                    "label": 0
                },
                {
                    "sent": "But a little bit later in the same year I remember we had a discussion in Latimer next office, with Chris Burgess and Vladimir and especially Vladimir felt very strongly that this should not sound like it's related to a neural network, because neural networks, if you wanted to kill neural networks.",
                    "label": 0
                },
                {
                    "sent": "So support vector machines number called support vector networks.",
                    "label": 0
                },
                {
                    "sent": "And this is where this term was born.",
                    "label": 0
                },
                {
                    "sent": "A few years later, we worked on Kernel PCA and in Network also pointed out that one can use this kind of trick to kernel eyes.",
                    "label": 0
                },
                {
                    "sent": "Any product algorithm which is sort of.",
                    "label": 0
                },
                {
                    "sent": "Sort of obvious, especially in retrospect, although that email and a lot of other people at the time didn't see it, and it's interesting to think about why they didn't see it yesterday.",
                    "label": 0
                },
                {
                    "sent": "I had a discussion with the number 2, and somehow he felt that maybe it was because reading was a pure theoretician who just thought of this as some kind of mathematics.",
                    "label": 0
                },
                {
                    "sent": "Where is anybody who implemented an SVM's?",
                    "label": 0
                },
                {
                    "sent": "So that's including these people listed here who did see it to such people.",
                    "label": 0
                },
                {
                    "sent": "It must have been obvious that this is just a subroutine.",
                    "label": 0
                },
                {
                    "sent": "The kernel you called the kernel instead of calling a dot product, and you can do it.",
                    "label": 0
                },
                {
                    "sent": "From any algorithm in Lyon told me that he actually is also funny.",
                    "label": 0
                },
                {
                    "sent": "He at that time he was at 18 to before he went back to France.",
                    "label": 0
                },
                {
                    "sent": "He submitted the grant proposal in this grant proposal he suggested to kernelized various algorithms.",
                    "label": 0
                },
                {
                    "sent": "But the grant proposal was rejected.",
                    "label": 0
                },
                {
                    "sent": "So in the following years whole kernel industry started the wide dissemination dissemination of this idea started.",
                    "label": 0
                },
                {
                    "sent": "The first NIPS kernel workshop.",
                    "label": 0
                },
                {
                    "sent": "There was the public domain software.",
                    "label": 0
                },
                {
                    "sent": "By tossing your hips, there was Chris Burges tutorial paper.",
                    "label": 0
                },
                {
                    "sent": "There was the book of Yellow and on the website about kernel machines and also it was notice that the input domain on which the kernels are defined need not be a vector space.",
                    "label": 0
                },
                {
                    "sent": "We can also use.",
                    "label": 0
                },
                {
                    "sent": "Other kinds of data, and this led to quite some successes, especially in the field of bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Now the modern period in which we may be still are started in my mind or from my point of view, with the development of kernel ICA buyback in Jordan and others.",
                    "label": 0
                },
                {
                    "sent": "And interesting thing about this was that this was the first example where kernels were used to solve an optimization problem over a large class with only their functions.",
                    "label": 0
                },
                {
                    "sent": "So over the unit ball in reproducing kernel Hilbert space and in this third option you several examples of such.",
                    "label": 0
                },
                {
                    "sent": "Problems in such users of the kernel trick and.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can all rephrase the kernel trick as saying that if you prefer to work with expectations rather than higher order moments, just map your data into an arcade, chess or map your distribution into that space and then work in this space.",
                    "label": 0
                },
                {
                    "sent": "And of course you can ask what comes after modernity.",
                    "label": 0
                },
                {
                    "sent": "This may be a postmodern.",
                    "label": 0
                },
                {
                    "sent": "Where we will start just to Colonel eyes, ourselves and the whole thing will become self referencial and Colonel papers referring to Colonel Paper said.",
                    "label": 0
                },
                {
                    "sent": "Everything will become more more and more irrelevant to reality.",
                    "label": 0
                },
                {
                    "sent": "And I don't know whether we're going to enter that.",
                    "label": 0
                },
                {
                    "sent": "But maybe there's something for discussion I think thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In which kind of problem in general or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's no.",
                    "label": 0
                },
                {
                    "sent": "Sorry asking how to choose the bandwidth parameter in the answer is as usual in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "There's no really convincing methods for this kind of approach, so if somehow the curse in the blessing of kernel methods at the same time, so I mean, there are some ideas, but there's nothing convincing for that.",
                    "label": 0
                },
                {
                    "sent": "I have a question actually.",
                    "label": 0
                },
                {
                    "sent": "So do you think kernel machines were successful at killing off neural networks or are we post modern era at the danger of a revival of neural networks?",
                    "label": 0
                },
                {
                    "sent": "Yes, so certainly there's there's a danger looming on the horizon, and I was actually expecting you're sure to ask something is here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure, OK, so soon had to ask the author's question.",
                    "label": 0
                },
                {
                    "sent": "No, I mean from my point of view, we have an addition to the toolkit and of course I could say now we also do multilayer kernel machines.",
                    "label": 0
                },
                {
                    "sent": "And actually when I prepare this talk I look through some of the old papers and I notice that in a very old technical report I had written something about multilayer support vector machines, but I guess at the time I found it too embarrassing to put into a real paper.",
                    "label": 0
                },
                {
                    "sent": "Certainly there are problems that can be much better represented by our multilayer system.",
                    "label": 0
                },
                {
                    "sent": "It's just that.",
                    "label": 0
                },
                {
                    "sent": "But I like about kernel machines.",
                    "label": 0
                },
                {
                    "sent": "Is the underlying mathematics and everything works out nicely and is more elegant and I don't yet see something corresponding for neural network.",
                    "label": 0
                },
                {
                    "sent": "Certainly you can solve problems will, but we all have our subjective reasons for choosing certain approaches and choosing what to work on.",
                    "label": 0
                },
                {
                    "sent": "And certainly if something comes up, or if I come up with an idea of doing something comperable for multilayer system, then why not?",
                    "label": 0
                },
                {
                    "sent": "That would be very interesting but.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what will be this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Well, it will be something something starting from heuristic neural networks or something starting from something different, and certainly we have more tools Now at our disposal.",
                    "label": 0
                },
                {
                    "sent": "I think kernels are a great tool and maybe they're not limited to one layer or to shallow systems.",
                    "label": 0
                },
                {
                    "sent": "Maybe they can be used in deeper systems, and I mean in a way you could say backpropagation has has brought the chain rule to machine learning and people are still using the chain rule even when they're not using backpropagation.",
                    "label": 0
                },
                {
                    "sent": "So people will stop using support vector machines probably, although I was pleased to see that one of the.",
                    "label": 0
                },
                {
                    "sent": "Best student papers by Unfound.",
                    "label": 0
                },
                {
                    "sent": "But this is a real support vector machine paper, so people will stop using support vector machines.",
                    "label": 0
                },
                {
                    "sent": "But I don't think they would stop using functional analysis in kernels and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Technology fundamentals.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so did you ask?",
                    "label": 0
                },
                {
                    "sent": "Is it a or the fundamental theory?",
                    "label": 0
                },
                {
                    "sent": "Is your opinion?",
                    "label": 0
                },
                {
                    "sent": "Let's say it's a fundamental element of the theory of pattern recognition, but it's certainly not the theory because I mean, as you've seen before in the question of Jean Yves, it's not even a complete theory.",
                    "label": 0
                },
                {
                    "sent": "There's alot of things that are open that we cannot answer.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of problems that we cannot solve.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of them we cannot solve in principle, but some of them we should be able to solve.",
                    "label": 0
                },
                {
                    "sent": "So I think actually overall our field is relatively relatively undeveloped.",
                    "label": 0
                },
                {
                    "sent": "So far, so turns the theory.",
                    "label": 0
                },
                {
                    "sent": "There's still a lot of work to do.",
                    "label": 0
                },
                {
                    "sent": "When I started.",
                    "label": 0
                },
                {
                    "sent": "One drink.",
                    "label": 0
                },
                {
                    "sent": "Join now.",
                    "label": 0
                },
                {
                    "sent": "the Bible students have to know about functional analysis to comments.",
                    "label": 0
                },
                {
                    "sent": "Machine Learning is a barrier to some people coming into the field or you think it stills.",
                    "label": 0
                },
                {
                    "sent": "Still getting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's an interesting point.",
                    "label": 0
                },
                {
                    "sent": "I mean certainly if you look at the students now with what they start with, it's different from a few years ago and probably it will be a barrier to some students.",
                    "label": 0
                },
                {
                    "sent": "But at the same time it will be more inviting to other students.",
                    "label": 0
                },
                {
                    "sent": "So I think we will now see maybe more students from mathematics getting into machine learning, because if they look at it, maybe they get the feeling there's something that they can contribute, whereas a few years ago it didn't look like it was so interesting from a mathematical point of view, although.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess it always depends on what kind of machine learning you did.",
                    "label": 0
                },
                {
                    "sent": "If you looked at VC theory, you probably already had lots of mathematics to do 20 years ago in machine learning, but certainly if you're looking at machine learning algorithms, I completely agree with you that now the threshold is a bit higher, not just in terms of kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Also, graphical models and nonparametric Bayesian statistics and all that stuff.",
                    "label": 0
                },
                {
                    "sent": "It's getting all the little bit more technical than it was ten years ago, but I guess it's a curse and a blessing.",
                    "label": 0
                },
                {
                    "sent": "For our field.",
                    "label": 0
                },
                {
                    "sent": "We have time for maybe one more question.",
                    "label": 0
                },
                {
                    "sent": "Account with you or logical tea and the people particularly.",
                    "label": 0
                },
                {
                    "sent": "Are fairly disjoint.",
                    "label": 0
                },
                {
                    "sent": "Please keep them urging.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Is there a Christmas?",
                    "label": 0
                },
                {
                    "sent": "Will logical AI and statistically I'm merge, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a tricky question because I'm someone from very much from the statistical side, so I I came into AI because I'm interested in in intelligence, but to me at the time what I saw in logical AI was not satisfying and I sort of went the statistical followed the statistical branch.",
                    "label": 0
                },
                {
                    "sent": "But now the statistical branch, especially in kernel methods and for me, if you look at the program with this conference, you will see a lot of work on relational data structure data and things like that.",
                    "label": 0
                },
                {
                    "sent": "And maybe there are some possibilities for making contact again, so I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't exclude that these contact is is going to be made again, but I don't see it as something immediate.",
                    "label": 0
                },
                {
                    "sent": "Let's put it this way.",
                    "label": 0
                },
                {
                    "sent": "Great, alright, well let's play.",
                    "label": 0
                }
            ]
        }
    }
}