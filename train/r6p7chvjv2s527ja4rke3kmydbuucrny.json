{
    "id": "r6p7chvjv2s527ja4rke3kmydbuucrny",
    "title": "Online Learning in Non-Stationary Markov Decision Processes",
    "info": {
        "author": [
            "Gergely Neu, SequeL, INRIA Lille - Nord Europe"
        ],
        "published": "Aug. 6, 2013",
        "recorded": "April 2013",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/machine_neu_online_learning/",
    "segmentation": [
        [
            "Hi, I'm I'm going now and I work in with the 1st and I work with the two other Hungarian guys.",
            "Troubles, obituary, another church who already moved to Canada.",
            "So we are mainly interested in reinforcement learning problems and online learning problems, and this work that I'm presenting here is a mixture of these two settings, so it's essentially it's a reinforcement learning problem where the reward functions change overtime and they can change without making any assumptions about the nature of the change.",
            "So it's kind of an online learning problem, so let me describe just vaguely how this process looks like, so there's a player.",
            "Interacting with an environment, so in each round of this sequential decision process at the player selects an action and based on this action and the complete state of the environment, it receives some reward and its goal is to maximize this reward.",
            "So we are interested in some specific structures where the environment can be decomposed into two parts.",
            "One part that is controlled and has a good stochastic model, and this is a Markovian dynamics.",
            "So this is the stochastic dynamics here, and there's another part of the environment that you have no model about and you cannot even hope to model model it.",
            "So it this is where this online learning component comes in.",
            "Can you give me the next slide?"
        ],
        [
            "So so and so in this setting, when you make no assumption about the sequence of reward functions, it's traditional too.",
            "To try and minimize the regret so they regret is is the difference between the performance of your algorithm.",
            "So the total reward that your algorithm gather.",
            "So this is the this is our happy in this case, so these are the total rewards that you receive.",
            "And the other thing that you compare it with is RT of \u03c0, which is, which is the.",
            "Which is the total reward of the best possible policy given the sequence of actions.",
            "So, so essentially we are interested in in algorithms where you can theoretically guarantee that this this regret this gap between the best performance and your own performance is small and you can look at this problem for a number of perspectives so we can make a number of assumptions about the structure of the stochastic dynamics.",
            "For example, you can assume that it's it's an episodic structure, so in a setting and used in each, each episode is start from the starting state and.",
            "An episode terminates when you see that many reach an end state.",
            "But you could also assume that there's a continuing task that you that, well, it never ends, but in each time step the reward function is allowed to change.",
            "So there are also.",
            "Some assumptions that you can make about the feedback given to the learner.",
            "So the first column considers the case when after, after each episode is terminated.",
            "For example, you can observe the whole reward function for that episode, so this is called a full information setting and there is a much more complicated setting when they learn only observes the rewards that it received during that episode, and this is called the bandwidth setting and also on the columns you can see two different versions.",
            "When, when the agent is assumed to know the transition probabilities so it doesn't have to estimate them, but here's a more difficult problem when it is unknown and you have to estimate it from samples and so in this setting you have a very interesting combination of stochastic learning problems and adversarial learning problems.",
            "So we have a number of results."
        ],
        [
            "For all of these settings, but as you can see, the last most complicated one is still not sold, but well.",
            "The other things are also pretty much involved, so that's what my PhD thesis is about.",
            "It's about 150 pages and all and all we have there is these proofs.",
            "So if you want to hear some details, then come to my post and check it out."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I'm I'm going now and I work in with the 1st and I work with the two other Hungarian guys.",
                    "label": 0
                },
                {
                    "sent": "Troubles, obituary, another church who already moved to Canada.",
                    "label": 0
                },
                {
                    "sent": "So we are mainly interested in reinforcement learning problems and online learning problems, and this work that I'm presenting here is a mixture of these two settings, so it's essentially it's a reinforcement learning problem where the reward functions change overtime and they can change without making any assumptions about the nature of the change.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of an online learning problem, so let me describe just vaguely how this process looks like, so there's a player.",
                    "label": 0
                },
                {
                    "sent": "Interacting with an environment, so in each round of this sequential decision process at the player selects an action and based on this action and the complete state of the environment, it receives some reward and its goal is to maximize this reward.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in some specific structures where the environment can be decomposed into two parts.",
                    "label": 0
                },
                {
                    "sent": "One part that is controlled and has a good stochastic model, and this is a Markovian dynamics.",
                    "label": 0
                },
                {
                    "sent": "So this is the stochastic dynamics here, and there's another part of the environment that you have no model about and you cannot even hope to model model it.",
                    "label": 0
                },
                {
                    "sent": "So it this is where this online learning component comes in.",
                    "label": 1
                },
                {
                    "sent": "Can you give me the next slide?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so and so in this setting, when you make no assumption about the sequence of reward functions, it's traditional too.",
                    "label": 0
                },
                {
                    "sent": "To try and minimize the regret so they regret is is the difference between the performance of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the total reward that your algorithm gather.",
                    "label": 1
                },
                {
                    "sent": "So this is the this is our happy in this case, so these are the total rewards that you receive.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that you compare it with is RT of \u03c0, which is, which is the.",
                    "label": 0
                },
                {
                    "sent": "Which is the total reward of the best possible policy given the sequence of actions.",
                    "label": 1
                },
                {
                    "sent": "So, so essentially we are interested in in algorithms where you can theoretically guarantee that this this regret this gap between the best performance and your own performance is small and you can look at this problem for a number of perspectives so we can make a number of assumptions about the structure of the stochastic dynamics.",
                    "label": 0
                },
                {
                    "sent": "For example, you can assume that it's it's an episodic structure, so in a setting and used in each, each episode is start from the starting state and.",
                    "label": 0
                },
                {
                    "sent": "An episode terminates when you see that many reach an end state.",
                    "label": 0
                },
                {
                    "sent": "But you could also assume that there's a continuing task that you that, well, it never ends, but in each time step the reward function is allowed to change.",
                    "label": 0
                },
                {
                    "sent": "So there are also.",
                    "label": 0
                },
                {
                    "sent": "Some assumptions that you can make about the feedback given to the learner.",
                    "label": 0
                },
                {
                    "sent": "So the first column considers the case when after, after each episode is terminated.",
                    "label": 0
                },
                {
                    "sent": "For example, you can observe the whole reward function for that episode, so this is called a full information setting and there is a much more complicated setting when they learn only observes the rewards that it received during that episode, and this is called the bandwidth setting and also on the columns you can see two different versions.",
                    "label": 0
                },
                {
                    "sent": "When, when the agent is assumed to know the transition probabilities so it doesn't have to estimate them, but here's a more difficult problem when it is unknown and you have to estimate it from samples and so in this setting you have a very interesting combination of stochastic learning problems and adversarial learning problems.",
                    "label": 0
                },
                {
                    "sent": "So we have a number of results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all of these settings, but as you can see, the last most complicated one is still not sold, but well.",
                    "label": 0
                },
                {
                    "sent": "The other things are also pretty much involved, so that's what my PhD thesis is about.",
                    "label": 0
                },
                {
                    "sent": "It's about 150 pages and all and all we have there is these proofs.",
                    "label": 0
                },
                {
                    "sent": "So if you want to hear some details, then come to my post and check it out.",
                    "label": 0
                }
            ]
        }
    }
}